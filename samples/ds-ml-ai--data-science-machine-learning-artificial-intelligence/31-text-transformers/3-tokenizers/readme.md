# Tokenizers

*   language models

    *   process text using tokens, which are common sequences of characters found in a set of text

    *   models learn to understand the statistical relationships between these tokens
    
    *   producing the next token in a sequence of tokens.

*   https://platform.openai.com/tokenizer

*   token 

    *   generally corresponds to ~4 characters of text for common English text. 
    
    *   translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).

*   programmatic interface for tokenizing text

    *   tiktoken package for Python

        *   https://github.com/openai/tiktoken

        *   https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb

*   HuggingFace

    *   https://github.com/huggingface/tokenizers

*   https://nebius.com/blog/posts/how-tokenizers-work-in-ai-models

*   https://www.datacamp.com/blog/what-is-tokenization

## Implementations


### C#

#### ML.NET

*   https://learn.microsoft.com/en-us/dotnet/api/microsoft.ml.tokenizers.tokenizer


