# Ollama Python `generate`

*   https://github.com/ollama/ollama-python

*   https://pypi.org/project/ollama-python/

*   https://apidog.com/blog/how-to-use-ollama/

*   https://www.cohorte.co/blog/using-ollama-with-python-step-by-step-guide

*   https://www.medium.anujonthemove.com/simplifying-ollama-model-management-with-a-python-utility-wrapper-05898b98ea57

```
osascript -e 'tell app "Ollama" to quit'
```


lmstudio

https://lmstudio.ai/docs/python



vllm


https://pypi.org/project/vllm/

https://docs.vllm.ai/en/latest/getting_started/quickstart.html#openai-compatible-server

https://github.com/vllm-project

Koboldcpp 

    https://github.com/LostRuins/koboldcpp
    
    no model hot swapping, 
    not crazy fast
    
*   vllm

    faster than koboldcpp

    for batched responses and many users

*   Aphrodite

    https://github.com/aphrodite-engine/aphrodite-engine

    for batched responses and many users

    more compatible with different hardware

*   Exllamav2

    good for raw speed

TabbyAPI

    openai server for it
    
    


https://www.reddit.com/r/LocalLLaMA/comments/1et810q/ollama_constantly_hangs_in_production_after_some/

error after killing processes:

```
httpx.RemoteProtocolError: Server disconnected without sending a response.
```
