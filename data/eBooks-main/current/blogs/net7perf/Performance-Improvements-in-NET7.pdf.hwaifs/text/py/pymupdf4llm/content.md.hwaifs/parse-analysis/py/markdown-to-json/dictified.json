{"1": {"Setup": "[The microbenchmarks throughout this post utilize benchmarkdotnet. To make it easy for you to follow](https://github.com/dotnet/benchmarkdotnet)\nalong with your own validation, I have a very simple setup for the benchmarks I use. Create a new C#\nproject:\n\nand the contents of `Program.cs` with this:\n\n1 CHAPTER 1 | Setup\n\nFor each benchmark included in this write-up, you can then just copy and paste the code into this test\nclass, and run the benchmarks. For example, to run a benchmark comparing performance on .NET 6\nand .NET 7, do:\n\n```\ndotnet run -c Release -f net6.0 --filter '**' --runtimes net6.0 net7.0\n\n```\n\nThis command says \u201cbuild the benchmarks in release configuration targeting the .NET 6 surface area,\nand then run all of the benchmarks on both .NET 6 and .NET 7.\u201d Or to run just on .NET 7:\n\n```\ndotnet run -c Release -f net7.0 --filter '**' --runtimes net7.0\n\n```\n\nwhich instead builds targeting the .NET 7 surface area and then only runs once against .NET 7. You\ncan do this on any of Windows, Linux, or macOS. Unless otherwise called out (e.g. where the\nimprovements are specific to Unix and I run the benchmarks on Linux), the results I share were\nrecorded on Windows 11 64-bit but aren\u2019t Windows-specific and should show similar relative\ndifferences on the other operating systems as well.\n\nThe release of the first .NET 7 release candidate is right around the corner. All of the measurements in\n[this post were gathered with a recent daily build](https://github.com/dotnet/installer/blob/main/README.md#installers-and-binaries) of .NET 7 RC1.\n\nAlso, my standard caveat: These are microbenchmarks. It is expected that different hardware, different\nversions of operating systems, and the way in which the wind is currently blowing can affect the\nnumbers involved. Your mileage may vary.\n\n2 CHAPTER 1 | Setup\n\n**CHAPTER**"}, "2": {"JIT": "I\u2019d like to kick off a discussion of performance improvements in the Just-In-Time (JIT) compiler by\ntalking about something that itself isn\u2019t actually a performance improvement. Being able to\nunderstand exactly what assembly code is generated by the JIT is critical when fine-tuning lower-level,\nperformance-sensitive code. There are multiple ways to get at that assembly code. The online tool\n[sharplab.io](https://sharplab.io/#v2:EYLgxg9gTgpgtADwGwBYA0AXEBDAzgWwB8ABAJgEYBYAKGIGYACMhgYRoG8aHuGAHKAJYA3bBhgMBAOwwMA+tgYBeBilJo5wJQwAcKANxce9CdIYBZKQAoAlEoB850QAsAdBcmX562cGsHqAL5AA) is _incredibly useful_ for this (thanks to [@ashmind](https://github.com/ashmind) for this\ntool); however it currently only targets a single release, so as I write this I\u2019m only able to see the\n[output for .NET 6, which makes it difficult to use for A/B comparisons. godbolt.org](https://godbolt.org/z/4v33asW6z) is also valuable for\n[this, with C# support added in compiler-explorer/compiler-explorer#3168](https://github.com/compiler-explorer/compiler-explorer/pull/3168) from\n\n[@hez2010](https://github.com/hez2010), with similar limitations. The most flexible solutions involve\ngetting at that assembly code locally, as it enables comparing whatever versions or local builds you\ndesire with whatever configurations and switches set that you need.\n\nOne common approach is to use the `[DisassemblyDiagnoser]` in benchmarkdotnet. Simply slap the\n\n`[DisassemblyDiagnoser]` attribute onto your test class: benchmarkdotnet will find the assembly code\ngenerated for your tests and some depth of functions they call, and dump out the found assembly\ncode in a human-readable form. For example, if I run this test:\n\nwith:\n\n```\ndotnet run -c Release -f net7.0 --filter '**'\n\n```\n\nin addition to doing all of its normal test execution and timing, benchmarkdotnet also outputs a\n\n`Program-asm.md` file that contains this:\n\n3 CHAPTER 2 | JIT\n\n[Pretty neat. This support was recently improved further in dotnet/benchmarkdotnet#2072, which](https://github.com/dotnet/BenchmarkDotNet/pull/2072)\nallows passing a filter list on the command-line to benchmarkdotnet to tell it exactly which methods\u2019\nassembly code should be dumped.\n\nIf you can get your hands on a \u201cdebug\u201d or \u201cchecked\u201d build of the .NET runtime (\u201cchecked\u201d is a build\nthat has optimizations enabled but also still includes asserts), and specifically of clrjit.dll, another\nvaluable approach is to set an environment variable that causes the JIT itself to spit out a humanreadable description of all of the assembly code it emits. This can be used with any kind of\napplication, as it\u2019s part of the JIT itself rather than part of any specific tool or other environment, it\nsupports showing the code the JIT generates each time it generates code (e.g. if it first compiles a\nmethod without optimization and then later recompiles it with optimization), and overall it\u2019s the most\naccurate picture of the assembly code as it comes \u201cstraight from the horses mouth,\u201d as it were. The\n(big) downside of course is that it requires a non-release build of the runtime, which typically means\n[you need to build it yourself from the sources in the dotnet/runtime](https://github.com/dotnet/runtime) repo.\n\n[\u2026 until .NET 7, that is. As of dotnet/runtime#73365, this assembly dumping support is now available in](https://github.com/dotnet/runtime/pull/73365)\nrelease builds as well, which means it\u2019s simply part of .NET 7 and you don\u2019t need anything special to\nuse it. To see this, try creating a simple \u201chello world\u201d app like:\n\nand building it (e.g. `dotnet build -c Release` ). Then, set the `DOTNET_JitDisasm` environment\nvariable to the name of the method we care about, in this case \u201cMain\u201d (the exact syntax allowed is\nmore permissive and allows for some use of wildcards, optional namespace and class names, etc.). As\nI\u2019m using PowerShell, that means:\n\n```\n$env:DOTNET_JitDisasm=\"Main\"\n\n```\n\nand then running the app. You should see code like this output to the console:\n\n4 CHAPTER 2 | JIT\n\nThis is immeasurably helpful for performance analysis and tuning, even for questions as simple as \u201cdid\nmy function get inlined\u201d or \u201cis this code I expected to be optimized away actually getting optimized\naway.\u201d Throughout the rest of this post, I\u2019ll include assembly snippets generated by one of these two\nmechanisms, in order to help exemplify concepts.\n\nNote that it can sometimes be a little confusing figuring out what name to specify as the value for\n\n`DOTNET_JitDisasm`, especially when the method you care about is one that the C# compiler names or\nname mangles (since the JIT only sees the IL and metadata, not the original C#), e.g. the name of the\nentry point method for a program with top-level statements, the names of local functions, etc. To\nboth help with this and to provide a really valuable top-level view of the work the JIT is doing, .NET 7\nalso supports the new `DOTNET_JitDisasmSummary` environment variable (introduced in\n[dotnet/runtime#74090). Set that to \u201c1\u201d, and it\u2019ll result in the JIT emitting a line every time it compiles a](https://github.com/dotnet/runtime/pull/74090)\nmethod, including the name of that method which is copy/pasteable with `DOTNET_JitDisasm` . This\nfeature is useful in-and-of-itself, however, as it can quickly highlight for you what\u2019s being compiled,\nwhen, and with what settings. For example, if I set the environment variable and then run a \u201chello,\nworld\u201d console app, I get this output:\n\nWe can see for \u201chello, world\u201d there\u2019s only 5 methods that actually get JIT compiled. There are of\ncourse many more methods that get executed as part of a simple \u201chello, world,\u201d but almost all of\n\nbecause they explicitly opted-out of R2R via use of the\n\n`[MethodImpl(MethodImplOptions.AggressiveOptimization)]` attribute (despite the name, this\nattribute should almost never be used, and is only used for very specific reasons in a few very specific\nplaces in the core libraries). Then there\u2019s our `Main` method. And lastly there\u2019s the `NarrowUtf16ToAscii`\n\n5 CHAPTER 2 | JIT\n\nmethod, which doesn\u2019t have R2R code, either, due to using the variable-width `Vector<T>` (more on\nthat later). Every other method that\u2019s run doesn\u2019t require JIT\u2019ing. If we instead first set the\n\n`DOTNET_ReadyToRun` environment variable to `0`, the list is much longer, and gives you a very good\nsense of what the JIT needs to do on startup (and why technologies like R2R are important for startup\ntime). Note how many methods get compiled before \u201chello, world\u201d is output:\n\n6 CHAPTER 2 | JIT\n\n7 CHAPTER 2 | JIT\n\n8 CHAPTER 2 | JIT\n\n9 CHAPTER 2 | JIT\n\n10 CHAPTER 2 | JIT\n\n11 CHAPTER 2 | JIT\n\n12 CHAPTER 2 | JIT\n\nWith that out of the way, let\u2019s move on to actual performance improvements, starting with on-stack\nreplacement.\n\n**On-Stack Replacement**\n\nOn-stack replacement (OSR) is one of the coolest features to hit the JIT in .NET 7. But to really\nunderstand OSR, we first need to understand tiered compilation, so a quick recap\u2026\n\nOne of the issues a managed environment with a JIT compiler has to deal with is tradeoffs between\nstartup and throughput. Historically, the job of an optimizing compiler is to, well, optimize, in order to\nenable the best possible throughput of the application or service once running. But such optimization\ntakes analysis, takes time, and performing all of that work then leads to increased startup time, as all\nof the code on the startup path (e.g. all of the code that needs to be run before a web server can\nserve the first request) needs to be compiled. So a JIT compiler needs to make tradeoffs: better\nthroughput at the expense of longer startup time, or better startup time at the expense of decreased\nthroughput. For some kinds of apps and services, the tradeoff is an easy call, e.g. if your service starts\nup once and then runs for days, several extra seconds of startup time doesn\u2019t matter, or if you\u2019re a\nconsole application that\u2019s going to do a quick computation and exit, startup time is all that matters.\nBut how can the JIT know which scenario it\u2019s in, and do we really want every developer having to\nknow about these kinds of settings and tradeoffs and configure every one of their applications\naccordingly? One answer to this has been ahead-of-time compilation, which has taken various forms\nin .NET. For example, all of the core libraries are \u201ccrossgen\u201d\u2019d, meaning they\u2019ve been run through a\ntool that produces the previously mentioned R2R format, yielding binaries that contain assembly code\nthat needs only minor tweaks to actually execute; not every method can have code generated for it,\nbut enough that it significantly reduces startup time. Of course, such approaches have their own\ndownsides, e.g. one of the promises of a JIT compiler is it can take advantage of knowledge of the\ncurrent machine / process in order to best optimize, so for example the R2R images have to assume a\n\n13 CHAPTER 2 | JIT\n\ncertain baseline instruction set (e.g. what vectorizing instructions are available) whereas the JIT can see\nwhat\u2019s actually available and use the best. \u201cTiered compilation\u201d provides another answer, one that\u2019s\nusable with or without these other ahead-of-time (AOT) compilation solutions.\n\nTiered compilation enables the JIT to have its proverbial cake and eat it, too. The idea is simple: allow\nthe JIT to compile the same code multiple times. The first time, the JIT can use as a few optimizations\nas make sense (a handful of optimizations can actually make the JIT\u2019s own throughput faster, so those\nstill make sense to apply), producing fairly unoptimized assembly code but doing so really quickly.\nAnd when it does so, it can add some instrumentation into the assembly to track how often the\nmethods are called. As it turns out, many functions used on a startup path are invoked once or maybe\nonly a handful of times, and it would take more time to optimize them than it does to just execute\nthem unoptimized. Then, when the method\u2019s instrumentation triggers some threshold, for example a\nmethod having been executed 30 times, a work item gets queued to recompile that method, but this\ntime with all the optimizations the JIT can throw at it. This is lovingly referred to as \u201ctiering up.\u201d Once\nthat recompilation has completed, call sites to the method are patched with the address of the newly\nhighly optimized assembly code, and future invocations will then take the fast path. So, we get faster\nstartup _and_ faster sustained throughput. At least, that\u2019s the hope.\n\nA problem, however, is methods that don\u2019t fit this mold. While it\u2019s certainly the case that many\nperformance-sensitive methods are relatively quick and executed many, many, many times, there\u2019s\nalso a large number of performance-sensitive methods that are executed just a handful of times, or\nmaybe even only once, but that take a very long time to execute, maybe even the duration of the\nwhole process: methods with loops. As a result, by default tiered compilation hasn\u2019t applied to loops,\nthough it can be enabled by setting the `DOTNET_TC_QuickJitForLoops` environment variable to `1` . We\ncan see the effect of this by trying this simple console app with .NET 6. With the default settings, run\nthis app:\n\nI get numbers printed out like:\n\n14 CHAPTER 2 | JIT\n\nNow, try setting `DOTNET_TC_QuickJitForLoops` to `1` . When I then run it again, I get numbers like this:\n\nIn other words, with `DOTNET_TC_QuickJitForLoops` enabled, it\u2019s taking 2.5x as long as without (the\ndefault in .NET 6). That\u2019s because this main function never gets optimizations applied to it. By setting\n\n`DOTNET_TC_QuickJitForLoops` to `1`, we\u2019re saying \u201cJIT, please apply tiering to methods with loops as\nwell,\u201d but this method with a loop is only ever invoked once, so for the duration of the process it ends\nup remaining at \u201ctier-0,\u201d aka unoptimized. Now, let\u2019s try the same thing with .NET 7. Regardless of\nwhether that environment variable is set, I again get numbers like this:\n\nbut importantly, this method was still participating in tiering. In fact, we can get confirmation of that\nby using the aforementioned `DOTNET_JitDisasmSummary=1` environment variable. When I set that and\nrun again, I see these lines in the output:\n\nhighlighting that `Main` was indeed compiled twice. How is that possible? On-stack replacement.\n\nThe idea behind on-stack replacement is a method can be replaced not just between invocations but\neven while it\u2019s executing, while it\u2019s \u201con the stack.\u201d In addition to the tier-0 code being instrumented\nfor call counts, loops are also instrumented for iteration counts. When the iterations surpass a certain\nlimit, the JIT compiles a new highly optimized version of that method, transfers all the local/register\nstate from the current invocation to the new invocation, and then jumps to the appropriate location in\nthe new method. We can see this in action by using the previously discussed `DOTNET_JitDisasm`\nenvironment variable. Set that to `Program:*` in order to see the assembly code generated for all of the\nmethods in the `Program` class, and then run the app again. You should see output like the following:\n\n15 CHAPTER 2 | JIT\n\n16 CHAPTER 2 | JIT\n\nA few relevant things to notice here. First, the comments at the top highlight how this code was\ncompiled:\n\nSo, we know this is the initial version (\u201cTier-0\u201d) of the method compiled with minimal optimization\n(\u201cMinOpts\u201d). Second, note this line of the assembly:\n\n```\nFF152DD40B00     call   [Program:<Main>g__IsAsciiDigit|0_0(ushort):bool]\n\n```\n\nOur `IsAsciiDigit` helper method is trivially inlineable, but it\u2019s not getting inlined; instead, the\nassembly has a call to it, and indeed we can see below the generated code (also \u201cMinOpts\u201d) for\n\n`IsAsciiDigit` . Why? Because inlining is an optimization (a really important one) that\u2019s disabled as\npart of tier-0 (because the analysis for doing inlining well is also quite costly). Third, we can see the\ncode the JIT is outputting to instrument this method. This is a bit more involved, but I\u2019ll point out the\nrelevant parts. First, we see:\n\n17 CHAPTER 2 | JIT\n\n```\nC745A8E8030000    mov   dword ptr [rbp-58H], 0x3E8\n\n```\n\nThat `0x3E8` is the hex value for the decimal 1,000, which is the default number of iterations a loop\nneeds to iterate before the JIT will generate the optimized version of the method (this is configurable\nvia the `DOTNET_TC_OnStackReplacement_InitialCounter` environment variable). So we see 1,000\nbeing stored into this stack location. Then a bit later in the method we see this:\n\nThe generated code is loading that counter into the `ecx` register, decrementing it, storing it back, and\nthen seeing whether the counter dropped to 0. If it didn\u2019t, the code skips to `G_M000_IG05`, which is the\nlabel for the actual code in the rest of the loop. But if the counter did drop to 0, the JIT proceeds to\nstore relevant state into the the `rcx` and `edx` registers and then calls the `CORINFO_HELP_PATCHPOINT`\nhelper method. That helper is responsible for triggering the creation of the optimized method if it\ndoesn\u2019t yet exist, fixing up all appropriate tracking state, and jumping to the new method. And indeed,\nif you look again at your console output from running the program, you\u2019ll see yet another output for\nthe `Main` method:\n\n18 CHAPTER 2 | JIT\n\n19 CHAPTER 2 | JIT\n\nHere, again, we notice a few interesting things. First, in the header we see this:\n\nso we know this is both optimized \u201ctier-1\u201d code and is the \u201cOSR variant\u201d for this method. Second,\nnotice there\u2019s no longer a call to the `IsAsciiDigit` helper. Instead, where that call would have been,\nwe see this:\n\nimplementation ( `(uint)(c - '0') <= 9` ), doesn\u2019t it? That\u2019s because it is. The helper was successfully\ninlined in this now-optimized code.\n\nGreat, so now in .NET 7, we can largely avoid the tradeoffs between startup and throughput, as OSR\nenables tiered compilation to apply to all methods, even those that are long-running. A multitude of\nPRs went into enabling this, including many over the last few years, but all of the functionality was\n[disabled in the shipping bits. Thanks to improvements like dotnet/runtime#62831](https://github.com/dotnet/runtime/pull/62831) which implemented\nsupport for OSR on Arm64 (previously only x64 support was implemented), and\n[dotnet/runtime#63406](https://github.com/dotnet/runtime/pull/63406) [and dotnet/runtime#65609](https://github.com/dotnet/runtime/pull/65609) which revised how OSR imports and epilogs are\n[handled, dotnet/runtime#65675 enables OSR (and as a result](https://github.com/dotnet/runtime/pull/65675) `DOTNET_TC_QuickJitForLoops` ) by\ndefault.\n\n20 CHAPTER 2 | JIT\n\nBut, tiered compilation and OSR aren\u2019t just about startup (though they\u2019re of course very valuable\nthere). They\u2019re also about further improving throughput. Even though tiered compilation was\noriginally envisioned as a way to optimize startup while not hurting throughput, it\u2019s become much\nmore than that. There are various things the JIT can learn about a method during tier-0 that it can\n\nonly have been initialized by the time the tier-1 code executes but their values won\u2019t ever change. And\nthat in turn means that any readonly statics of primitive types (e.g. `bool`, `int`, etc.) can be treated like\n\nWhen I do so, I get this output:\n\n21 CHAPTER 2 | JIT\n\nthe \u201cTier-1\u201d code, where all of that overhead has vanished and is instead replaced simply by `mov eax,`\n\n`1` . Since the \u201cTier-0\u201d code had to have executed in order for it to tier up, the \u201cTier-1\u201d code was\n\nThis is so useful that components are now written with tiering in mind. Consider the new `Regex` source\ngenerator, which is discussed later in this post (Roslyn source generators were introduced a couple of\nyears ago; just as how Roslyn analyzers are able to plug into the compiler and surface additional\ndiagnostics based on all of the data the compiler learns from the source code, Roslyn source\ngenerators are able to analyze that same data and then further augment the compilation unit with\n\ninstances that don\u2019t explicitly set a timeout. That means, even though it\u2019s super rare for such a\nprocess-wide timeout to be set, the `Regex` source generator still needs to output timeout-related code\njust in case it\u2019s needed. It does so by outputting some helpers like this:\n\nwhich it then uses at call sites like this:\n\nIn tier-0, these checks will still be emitted in the assembly code, but in tier-1 where throughput\nmatters, if the relevant `AppContext` switch hasn\u2019t been set, then `s_defaultTimeout` will be\n\n22 CHAPTER 2 | JIT\n\nassembly code entirely as dead code.\n\nBut, this is somewhat old news. The JIT has been able to do such an optimization since tiered\ncompilation was introduced in .NET Core 3.0. Now in .NET 7, though, with OSR it\u2019s also able to do so\nby default for methods with loops (and thus enable cases like the regex one). However, the real magic\nof OSR comes into play when combined with another exciting feature: dynamic PGO.\n\n**PGO**\n\n[I wrote about profile-guided optimization (PGO) in my Performance Improvements in .NET 6 post, but](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6)\nI\u2019ll cover it again here as it\u2019s seen a multitude of improvements for .NET 7.\n\nPGO has been around for a long time, in any number of languages and compilers. The basic idea is\nyou compile your app, asking the compiler to inject instrumentation into the application to track\nvarious pieces of interesting information. You then put your app through its paces, running through\nvarious common scenarios, causing that instrumentation to \u201cprofile\u201d what happens when the app is\nexecuted, and the results of that are then saved out. The app is then recompiled, feeding those\ninstrumentation results back into the compiler, and allowing it to optimize the app for exactly how it\u2019s\nexpected to be used. This approach to PGO is referred to as \u201cstatic PGO,\u201d as the information is all\ngleaned ahead of actual deployment, and it\u2019s something .NET has been doing in various forms for\nyears. From my perspective, though, the really interesting development in .NET is \u201cdynamic PGO,\u201d\nwhich was introduced in .NET 6, but off by default.\n\nDynamic PGO takes advantage of tiered compilation. I noted that the JIT instruments the tier-0 code\nto track how many times the method is called, or in the case of loops, how many times the loop\nexecutes. It can instrument it for other things as well. For example, it can track exactly which concrete\ntypes are used as the target of an interface dispatch, and then in tier-1 specialize the code to expect\nthe most common types (this is referred to as \u201cguarded devirtualization,\u201d or GDV). You can see this in\nthis little example. Set the `DOTNET_TieredPGO` environment variable to `1`, and then run this on .NET 7:\n\n23 CHAPTER 2 | JIT\n\nThe tier-0 code for `DoWork` ends up looking like this:\n\n24 CHAPTER 2 | JIT\n\nThat first block is checking the concrete type of the `IPrinter` (stored in `rdi` ) and comparing it against\nthe known type for `Printer` ( `0x7FFC3F1B2D98` ). If they\u2019re different, it just jumps to the same interface\ndispatch it was doing in the unoptimized version. But if they\u2019re the same, it then jumps directly to an\ninlined version of `Printer.PrintIfTrue` (you can see the call to `Console:WriteLine` right there in\nthis method). Thus, the common case (the only case in this example) is super efficient at the expense\nof a single comparison and branch.\n\nThat all existed in .NET 6, so why are we talking about it now? Several things have improved. First,\n[PGO now works with OSR, thanks to improvements like dotnet/runtime#61453. That\u2019s a big deal, as it](https://github.com/dotnet/runtime/pull/61453)\nmeans hot long-running methods that do this kind of interface dispatch (which are fairly common)\ncan get these kinds of devirtualization/inlining optimizations. Second, while PGO isn\u2019t currently\n\nenabling dynamic PGO (note that it _doesn\u2019t_ disable use of R2R images, so if you want the entirety of\nthe core libraries also employing dynamic PGO, you\u2019ll also need to set `DOTNET_ReadyToRun=0` ). Third,\nhowever, is dynamic PGO has been taught how to instrument and optimize additional things.\n\nPGO already knew how to instrument virtual dispatch. Now in .NET 7, thanks in large part to\n[dotnet/runtime#68703, it can do so for delegates as well (at least for delegates to instance methods).](https://github.com/dotnet/runtime/pull/68703)\nConsider this simple console app:\n\nWithout PGO enabled, I get generated optimized assembly like this:\n\n25 CHAPTER 2 | JIT\n\nNote the `call [rsi+18H]Func'2:Invoke(int):int:this` in there that\u2019s invoking the delegate. Now\nwith PGO enabled:\n\n26 CHAPTER 2 | JIT\n\nI chose the `42` constant in `i => i * 42` to make it easy to see in the assembly, and sure enough, there\nit is:\n\n27 CHAPTER 2 | JIT\n\nThis is loading the target address from the delegate into `r8` and is loading the address of the\nexpected target into `rax` . If they\u2019re the same, it then simply performs the inlined operation ( `imul`\n\n`r15d, edx, 42` ), and otherwise it jumps to G_M000_IG07 which calls to the function in `r8` . The effect\nof this is obvious if we run this as a benchmark:\n\nWith PGO disabled, we get the same performance throughput for .NET 6 and .NET 7:\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|DelegatePGO|.NET 6.0|1.665 us|1.00|\n|DelegatePGO|.NET 7.0|1.659 us|1.00|\n\nBut the picture changes when we enable dynamic PGO ( `DOTNET_TieredPGO=1` ). .NET 6 gets ~14%\nfaster, but .NET 7 gets ~3x faster!\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|DelegatePGO|.NET 6.0|1,427.7 ns|1.00|\n|DelegatePGO|.NET 7.0|539.0 ns|0.38|\n\n[dotnet/runtime#70377](https://github.com/dotnet/runtime/pull/70377) is another valuable improvement with dynamic PGO, which enables PGO to\nplay nicely with loop cloning and invariant hoisting. To understand this better, a brief digression into\nwhat those are. Loop cloning is a mechanism the JIT employs to avoid various overheads in the fast\npath of a loop. Consider the `Test` method in this example:\n\n28 CHAPTER 2 | JIT\n\nThe JIT doesn\u2019t know whether the passed in array is of sufficient length that all accesses to `array[i]`\ninside the loop will be in bounds, and thus it would need to inject bounds checks for every access.\nWhile it\u2019d be nice to simply do the length check up front and simply throw an exception early if it\nwasn\u2019t long enough, doing so could also change behavior (imagine the method were writing into the\narray as it went, or otherwise mutating some shared state). Instead, the JIT employs \u201cloop cloning.\u201d It\nessentially rewrites this `Test` method to be more like this:\n\nThat way, at the expense of some code duplication, we get our fast loop without bounds checks and\nonly pay for the bounds checks in the slow path. You can see this in the generated assembly (if you\ncan\u2019t already tell, `DOTNET_JitDisasm` is one of my favorite features in .NET 7):\n\n29 CHAPTER 2 | JIT\n\nThat G_M000_IG02 section is doing the null check and the length check, jumping to the G_M000_IG05\nblock if either fails. If both succeed, it\u2019s then executing the loop (block G_M000_IG03) without bounds\nchecks:\n\n30 CHAPTER 2 | JIT\n\nwith the bounds checks only showing up in the slow-path block:\n\nThat\u2019s \u201cloop cloning.\u201d What about \u201cinvariant hoisting\u201d? Hoisting means pulling something out of a\nloop to be before the loop, and invariants are things that don\u2019t change. Thus invariant hoisting is\npulling something out of a loop to before the loop in order to avoid recomputing every iteration of\nthe loop an answer that won\u2019t change. Effectively, the previous example already showed invariant\nhoisting, in that the bounds check is moved to be before the loop rather than in the loop, but a more\nconcrete example would be something like this:\n\nNote that the value of `array.Length - 42` doesn\u2019t change on each iteration of the loop, so it\u2019s\n\u201cinvariant\u201d to the loop iteration and can be lifted out, which the generated code does:\n\n31 CHAPTER 2 | JIT\n\n`r8d`, we then see this up-front block subtracting 42 from the length ( `add r8d, -42` ), and that\u2019s before\nwe continue into the fast-path loop in the G_M000_IG03 block. This keeps that additional set of\noperations out of the loop, thereby avoiding the overhead of recomputing the value per iteration.\n\nOk, so how does this apply to dynamic PGO? Remember that with the interface/virtual dispatch\navoidance PGO is able to do, it does so by doing a type check to see whether the type in use is the\nmost common type; if it is, it uses a fast path that calls directly to that type\u2019s method (and in doing so\nthat call is then potentially inlined), and if it isn\u2019t, it falls back to normal interface/virtual dispatch. That\ncheck can be invariant to a loop. So when a method is tiered up and PGO kicks in, the type check can\nnow be hoisted out of the loop, making it even cheaper to handle the common case. Consider this\nvariation of our original example:\n\nWhen we look at the optimized assembly generated for this with dynamic PGO enabled, we see this:\n\n32 CHAPTER 2 | JIT\n\n33 CHAPTER 2 | JIT\n\nInterestingly, improvements like this can bring with them their own challenges. PGO leads to a\nsignificant increase in the number of type checks, since call sites that specialize for a given type need\nto compare against that type. However, common subexpression elimination (CSE) hasn\u2019t historically\nworked for such type handles (CSE is a compiler optimization where duplicate expressions are\neliminated by computing the result once and then storing it for subsequent use rather than\n[recomputing it each time). dotnet/runtime#70580 fixes this by enabling CSE for such constant](https://github.com/dotnet/runtime/pull/70580)\nhandles. For example, consider this method:\n\nOn .NET 6, the JIT produced this assembly code:\n\nNote the C# has four tests for `string` and the assembly code has four loads with `mov rax,offset`\n\n`MT_System.String` . Now on .NET 7, the load is performed just once:\n\n34 CHAPTER 2 | JIT\n\n**Bounds Check Elimination**\n\nOne of the things that makes .NET attractive is its safety. The runtime guards access to arrays, strings,\nand spans such that you can\u2019t accidentally corrupt memory by walking off either end; if you do, rather\nthan reading/writing arbitrary memory, you\u2019ll get exceptions. Of course, that\u2019s not magic; it\u2019s done by\nthe JIT inserting bounds checks every time one of these data structures is indexed. For example, this:\n\nresults in:\n\nThe array is passed into this method in the `rcx` register, pointing to the method table pointer in the\nobject, and the length of an array is stored in the object just after that method table pointer (which is\n8 bytes in a 64-bit process). Thus the `cmp dword ptr [rcx+08H], 0` instruction is reading the length\n\n35 CHAPTER 2 | JIT\n\nof the array and comparing the length to 0; that makes sense, since the length can\u2019t be negative, and\nwe\u2019re trying to access the 0th element, so as long as the length isn\u2019t 0, the array has enough elements\nfor us to access its 0th element. In the event that the length was 0, the code jumps to the end of the\n\n`ptr [rcx+10H]` ).\n\nWhile these bounds checks in and of themselves aren\u2019t super expensive, do a lot of them and their\ncosts add up. So while the JIT needs to ensure that \u201csafe\u201d accesses don\u2019t go out of bounds, it also tries\nto prove that certain accesses won\u2019t, in which case it needn\u2019t emit the bounds check that it knows will\nbe superfluous. In every release of .NET, more and more cases have been added to find places these\nbounds checks can be eliminated, and .NET 7 is no exception.\n\n[For example, dotnet/runtime#61662 from [@anthonycanino](https://github.com/anthonycanino)](https://github.com/dotnet/runtime/pull/61662)\nenabled the JIT to understand various forms of binary operations as part of range checks. Consider\nthis method:\n\nIt\u2019s validating that the input span is 16 bytes long and then creating a `new ushort[8]` where each\n\n`ushort` in the array combines two of the input bytes. To do that, it\u2019s looping over the output array,\nand indexing into the bytes array using `i * 2` and `i * 2 + 1` as the indices. On .NET 6, each of those\nindexing operations would result in a bounds check, with assembly like:\n\nwhere that G_M000_IG04 is the `call CORINFO_HELP_RNGCHKFAIL` we\u2019re now familiar with. But on .NET\n7, we get this assembly for the method:\n\n36 CHAPTER 2 | JIT\n\nNo bounds checks, which is most easily seen by the lack of the telltale `call`\n\n`CORINFO_HELP_RNGCHKFAIL` at the end of the method. With this PR, the JIT is able to understand the\nimpact of certain multiplication and shift operations and their relationships to the bounds of the data\nstructure. Since it can see that the result array\u2019s length is 8 and the loop is iterating from 0 to that\n\nto prove that the bounds checks aren\u2019t needed.\n\n[dotnet/runtime#61569](https://github.com/dotnet/runtime/pull/61569) [and dotnet/runtime#62864](https://github.com/dotnet/runtime/pull/62864) also help to eliminate bounds checks when dealing\nwith constant strings and spans initialized from RVA statics (\u201cRelative Virtual Address\u201d static fields,\nbasically a static field that lives in a module\u2019s data section). For example, consider this benchmark:\n\n37 CHAPTER 2 | JIT\n\nOn .NET 6, we get this assembly:\n\nThe beginning of this makes sense: the JIT was obviously able to see that the length of `Text` is 5, so\nit\u2019s implementing the `(uint)i < Text.Length` check by doing `cmp rax,5`, and if `i` as an unsigned\nvalue is greater than or equal to 5, it\u2019s then zero\u2019ing out the return value (to return the `'\\0'` ) and\nexiting. If the length is less than 5 (in which case it\u2019s also at least 0 due to the unsigned comparison), it\nthen jumps to M00_L00 to read the value from the string\u2026 but we then see another `cmp` against 5, this\ntime as part of a range check. So even though the JIT knew the index was in bounds, it wasn\u2019t able to\nremove the bounds check. Now it is; in .NET 7, we get this:\n\nSo much nicer.\n\n[dotnet/runtime#67141](https://github.com/dotnet/runtime/pull/67141) is a great example of how evolving ecosystem needs drives specific\noptimizations into the JIT. The `Regex` compiler and source generator handle some cases of regular\nexpression character classes by using a bitmap lookup stored in strings. For example, to determine\nwhether a `char c` is in the character class `\"[A-Za-z0-9_]\"` (which will match an underscore or any\nASCII letter or digit), the implementation ends up generating an expression like the body of the\nfollowing method:\n\n38 CHAPTER 2 | JIT\n\nThe implementation is treating an 8-character string as a 128-bit lookup table. If the character is\nknown to be in range (such that it\u2019s effectively a 7-bit value), it\u2019s then using the top 3 bits of the value\nto index into the 8 elements of the string, and the bottom 4 bits to select one of the 16 bits in that\nelement, giving us an answer as to whether this input character is in the set or not. In .NET 6, even\nthough we know the character is in range of the string, the JIT couldn\u2019t see through either the length\ncomparison or the bit shift.\n\nThe previously mentioned PR takes care of the length check. And this PR takes care of the bit shift. So\nin .NET 7, we get this loveliness:\n\n39 CHAPTER 2 | JIT\n\nNote the distinct lack of a `call CORINFO_HELP_RNGCHKFAIL` . And as you might guess, this check can\nhappen _a lot_ in a `Regex`, making this a very useful addition.\n\nBounds checks are an obvious source of overhead when talking about array access, but they\u2019re not the\nonly ones. There\u2019s also the need to use the cheapest instructions possible. In .NET 6, with a method\nlike:\n\nassembly code like the following would be generated:\n\nThis should look fairly familiar from our previous discussion; the JIT is loading the array\u2019s length\n( `[rcx+8]` ) and comparing that with the value of `i` (in `edx` ), and then jumping to the end to throw an\n\nof moving it, it\u2019s sign-extending it; that\u2019s the \u201csxd\u201d part of the instruction name (sign-extending means\nthe upper 32 bits of the new 64-bit value will be set to the value of the upper bit of the 32-bit value,\n\nalso know that `i` is non-negative. That makes such sign-extension useless, since the upper bit is\nguaranteed to be 0. Since the `mov` instruction that zero-extends is a tad cheaper than `movsxd`, we can\n[simply use that instead. And that\u2019s exactly what dotnet/runtime#57970](https://github.com/dotnet/runtime/pull/57970) from\n\n[[@pentp](https://github.com/pentp) does for both arrays and spans (dotnet/runtime#70884 also](https://github.com/dotnet/runtime/pull/70884)\nsimilarly avoids some signed casts in other situations). Now on .NET 7, we get this:\n\n40 CHAPTER 2 | JIT\n\nThat\u2019s not the only source of overhead with array access, though. In fact, there\u2019s a very large category\nof array access overhead that\u2019s been there forever, but that\u2019s so well known there are even old FxCop\nrules and newer Roslyn analyzers that warn against it: multidimensional array accesses. The overhead\nin the case of a multidimensional array isn\u2019t just an extra branch on every indexing operation, or\nadditional math required to compute the location of the element, but rather that they currently pass\n[through the JIT\u2019s optimization phases largely unmodified. dotnet/runtime#70271](https://github.com/dotnet/runtime/pull/70271) improves the state\nof the world here by doing an expansion of a multidimensional array access early in the JIT\u2019s pipeline,\nsuch that later optimization phases can improve multidimensional accesses as they would other code,\nincluding CSE and loop invariant hoisting. The impact of this is visible in a simple benchmark that\nsums all the elements of a multidimensional array.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Sum|.NET 6.0|964.1 us|1.00|\n|Sum|.NET 7.0|674.7 us|0.70|\n\nThis previous example assumes you know the size of each dimension of the multidimensional array\n(it\u2019s referring to the `Size` directly in the loops). That\u2019s obviously not always (or maybe even rarely) the\ncase. In such situations, you\u2019d be more likely to use the `Array.GetUpperBound` method, and because\n\n41 CHAPTER 2 | JIT\n\nmultidimensional arrays can have a non-zero lower bound, `Array.GetLowerBound` . That would lead to\ncode like this:\n\n[In .NET 7, thanks to dotnet/runtime#60816, those](https://github.com/dotnet/runtime/pull/60816) `GetLowerBound` and `GetUpperBound` calls become\nJIT intrinsics. An \u201cintrinsic\u201d to a compiler is something the compiler has intrinsic knowledge of, such\nthat rather than relying solely on a method\u2019s defined implementation (if it even has one), the compiler\ncan substitute in something it considers to be better. There are literally thousands of methods in .NET\n\nsubstitute the necessary assembly instructions to read directly from the memory location that houses\nthe bounds. Here\u2019s what the assembly code for this benchmark looked like with .NET 6; the main thing\nto see here are all of the `call` s out to `GetLowerBound` and `GetUpperBound` :\n\n42 CHAPTER 2 | JIT\n\n43 CHAPTER 2 | JIT\n\nNow here\u2019s what it is for .NET 7:\n\n44 CHAPTER 2 | JIT\n\n```\ncall   System.Array.GetUpperBound(Int32)\n\n```\n\nwe get:\n\nand it ends up being much faster:\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Sum|.NET 6.0|2,657.5 us|1.00|\n|Sum|.NET 7.0|676.3 us|0.25|\n\n**Loop Hoisting and Cloning**\n\nWe previously saw how PGO interacts with loop hoisting and cloning, and those optimizations have\nseen other improvements, as well.\n\nHistorically, the JIT\u2019s support for hoisting has been limited to lifting an invariant out one level.\nConsider this example:\n\n45 CHAPTER 2 | JIT\n\nAt first glance, you might look at this and say \u201cwhat could be hoisted, the computation of `n` requires\nall of the loop inputs, and all of that computation is in `ComputeNumber` .\u201d But from a compiler\u2019s\n\ndifferent levels, e.g. the tens computation can be hoisted out one level, the hundreds out two levels,\nand the thousands out three levels. Here\u2019s what `[DisassemblyDiagnoser]` outputs for .NET 6:\n\n46 CHAPTER 2 | JIT\n\nWe can see that _some_ hoisting has happened here. After all, the inner most loop (tagged M00_L03) is\n\nall of the unnecessary computation out of the inner loop, being left only with adding the ones\nposition to the rest of the number. Let\u2019s go out a level. M00_L02 is the label for the tens loop. What\n\noperations which could have been hoisted out further were left stuck in the next-to-innermost loop.\n[Now, here\u2019s what we get for .NET 7, where this was improved in dotnet/runtime#68061:](https://github.com/dotnet/runtime/issues/68061)\n\n47 CHAPTER 2 | JIT\n\nNotice now where those `imul` instructions live. There are four labels, each one corresponding to one\n\nhighlighting that these computations were hoisted out to the appropriate level (the tens and ones\ncomputation are still in the right places).\n\nMore improvements have gone in on the cloning side. Previously, loop cloning would only apply for\n[loops iterating by 1 from a low to a high value. With dotnet/runtime#60148, the comparison against](https://github.com/dotnet/runtime/pull/60148)\nthe upper value can be `<=` rather than just `<` [. And with dotnet/runtime#67930, loops that iterate](https://github.com/dotnet/runtime/pull/67930)\ndownward can also be cloned, as can loops that have increments and decrements larger than 1.\nConsider this benchmark:\n\nWithout loop cloning, the JIT can\u2019t assume that `offset` through `offset+count` are in range, and thus\nevery access to the array needs to be bounds checked. With loop cloning, the JIT could generate one\nversion of the loop without bounds checks and only use that when it knows all accesses will be valid.\nThat\u2019s exactly what happens now in .NET 7. Here\u2019s what we got with .NET 6:\n\n48 CHAPTER 2 | JIT\n\nNotice how in the core loop, at label M00_L00, there\u2019s a bounds check ( `cmp eax,r9d` and `jae short`\n\n`M00_L03`, which jumps to a `call CORINFO_HELP_RNGCHKFAIL` ). And here\u2019s what we get with .NET 7:\n\nNotice how the code size is larger, and how there are now two variations of the loop: one at M00_L00\nand one at M00_L01. The second one, M00_L01, has a branch to that same `call`\n\nbounds.\n\n[Other changes also improved loop cloning. dotnet/runtime#59886](https://github.com/dotnet/runtime/pull/59886) enables the JIT to choose different\nforms for how to emit the the conditions for choosing the fast or slow loop path, e.g. whether to emit\n\n49 CHAPTER 2 | JIT\n\n`slowPath` [). dotnet/runtime#66257 enables loop cloning to kick in when the loop variable is initialized](https://github.com/dotnet/runtime/pull/66257)\nto more kinds of expressions (e.g. `for (int fromindex = lastIndex - lengthToClear; ...)` ). And\n[dotnet/runtime#70232](https://github.com/dotnet/runtime/pull/70232) increases the JIT\u2019s willingness to clone loops with bodies that do a broader set\nof operations.\n\n**Folding, propagation, and substitution**\n\nConstant folding is an optimization where a compiler computes the value of an expression involving\nonly constants at compile-time rather than generating the code to compute the value at run-time.\nThere are multiple levels of constant folding in .NET, with some constant folding performed by the C#\ncompiler and some constant folding performed by the JIT compiler. For example, given the C# code:\n\nthe C# compiler will generate IL for these methods like the following:\n\n`2;`, highlighting that the constant folding performed by the C# compiler was intramethod only. Now\nhere\u2019s what the JIT generates:\n\n50 CHAPTER 2 | JIT\n\nThe assembly for method `A` isn\u2019t particularly interesting; it\u2019s just returning that same value 23 (hex\n\n0x2e). Constant propagation is intricately linked to constant folding and is essentially just the idea that\nyou can substitute a constant value (typically one computed via constant folding) into further\nexpressions, at which point they may also be able to be folded.\n\nThe JIT has long performed constant folding, but it improves further in .NET 7. One of the ways\nconstant folding can improve is by exposing more values to be folded, which often means more\n\nshould consider being more aggressive about inlining, since exposing that constant to the body of the\ncallee can potentially significantly reduce the amount of code required to implement the callee. The\nJIT might have previously inlined such a method anyway, but when it comes to inlining, the JIT is all\nabout heuristics and generating enough evidence that it\u2019s worthwhile to inline something; this\ncontributes to that evidence. This pattern shows up, for example, in the various `FromXx` methods on\n\n`TimeSpan` . For example, `TimeSpan.FromSeconds` is implemented as:\n\nand, eschewing argument validation for the purposes of this example, `Interval` is:\n\nwhich if everything gets inlined means `FromSeconds` is essentially:\n\n```\nreturn new TimeSpan(50_000_000);\n\n```\n\nI\u2019ll spare you the .NET 6 assembly for this, but on .NET 7 with a benchmark like:\n\nwe now get the simple and clean:\n\n51 CHAPTER 2 | JIT\n\n[Another change improving constant folding included dotnet/runtime#57726](https://github.com/dotnet/runtime/pull/57726) from\n\n[@SingleAccretion](https://github.com/SingleAccretion), which unblocked constant folding in a\nparticular scenario that sometimes manifests when doing field-by-field assignment of structs being\nreturned from method calls. As a small example, consider this trivial property, which access the\n\n`Color.DarkOrange` property, which in turn does `new Color(KnownColor.DarkOrange)` :\n\nIn .NET 6, the JIT generated this:\n\nwith direct assignment of these constant values into their destination locations ( `mov word ptr`\n\n`[rdx+12],1` and `mov word ptr [rdx+10],39` ). Other changes contributing to constant folding\n[included dotnet/runtime#58171 from [@SingleAccretion](https://github.com/SingleAccretion) and](https://github.com/dotnet/runtime/pull/58171)\n[dotnet/runtime#57605](https://github.com/dotnet/runtime/pull/57605) from [@SingleAccretion](https://github.com/SingleAccretion).\n\nHowever, a large category of improvement came from an optimization related to propagation, that of\nforward substitution. Consider this silly benchmark:\n\n52 CHAPTER 2 | JIT\n\nIf we look at the assembly code generated for `Compute1` on .NET 6, it looks like what we\u2019d hope for.\nWe\u2019re adding `Value` 5 times, `Value` is trivially inlined and returns a constant value 16, and so we\u2019d\nhope that the assembly code generated for `Compute1` would effectively just be returning the value 80\n(hex 0x50), which is exactly what happens:\n\nwith this assembly code:\n\nRather than a single `mov eax, 50` to put the value 0x50 into the return register, we have 5 separate\n\n`add eax, 10` to build up that same 0x50 (80) value. That\u2019s\u2026 not ideal.\n\nIt turns out that many of the JIT\u2019s optimizations operate on the tree data structures created as part of\nparsing the IL. In some cases, optimizations can do better when they\u2019re exposed to more of the\nprogram, in other words when the tree they\u2019re operating on is larger and contains more to be\nanalyzed. However, various operations can break up these trees into smaller, individual ones, such as\nwith temporary variables created as part of inlining, and in doing so can inhibit these operations.\nSomething is needed in order to effectively stitch these trees back together, and that\u2019s forward\nsubstitution. You can think of forward substitution almost like an inverse of CSE; rather than trying to\nfind duplicate expressions and eliminate them by computing the value once and storing it into a\ntemporary, forward substitution eliminates that temporary and effectively moves the expression tree\ninto its use site. Obviously you don\u2019t want to do this if it would then negate CSE and result in\nduplicate work, but for expressions that are defined once and used once, this kind of forward\n[propagation is valuable. dotnet/runtime#61023 added an initial limited version of forward](https://github.com/dotnet/runtime/pull/61023)\n[substitution, and then dotnet/runtime#63720 added a more robust generalized implementation.](https://github.com/dotnet/runtime/pull/63720)\n[Subsequently, dotnet/runtime#70587 expanded it to also cover some SIMD vectors, and then](https://github.com/dotnet/runtime/pull/70587)\n[dotnet/runtime#71161](https://github.com/dotnet/runtime/pull/71161) improved it further to enable substitutions into more places (in this case into\ncall arguments). And with those, our silly benchmark now produces the following on .NET 7:\n\n53 CHAPTER 2 | JIT\n\n**Vectorization**\n\nSIMD, or Single Instruction Multiple Data, is a kind of processing in which one instruction applies to\nmultiple pieces of data at the same time. You\u2019ve got a list of numbers and you want to find the index\nof a particular value? You could walk the list comparing one element at a time, and that would be fine\nfunctionally. But what if in the same amount of time it takes you to read and compare one element,\nyou could instead read and compare two elements, or four elements, or 32 elements? That\u2019s SIMD,\nand the art of utilizing SIMD instructions is lovingly referred to as \u201cvectorization,\u201d where operations\nare applied to all of the elements in a \u201cvector\u201d at the same time.\n\n.NET has long had support for vectorization in the form of `Vector<T>`, which is an easy-to-use type\nwith first-class JIT support to enable a developer to write vectorized implementations. One of\n\n`Vector<T>` \u2019s greatest strengths is also one of its greatest weaknesses. The type is designed to adapt to\nwhatever width vector instructions are available in your hardware. If the machine supports 256-bit\n\ntoday; for example, the operations you can perform on a `Vector<T>` end up needing to be agnostic to\nthe width of the vectors used, since the width is variable based on the hardware on which the code\nactually runs. And that means the operations that can be exposed on `Vector<T>` are limited, which in\nturn limits the kinds of operations that can be vectorized with it. Also, because it\u2019s only ever a single\nsize in a given process, some data set sizes that fall in between 128 bits and 256 bits might not be\nprocessed as well as you\u2019d hope. You write your `Vector<byte>` -based algorithm, and you run it on a\nmachine with support for 256-bit vectors, which means it can process 32 bytes at a time, but then you\nfeed it an input with 31 bytes. Had `Vector<T>` mapped to 128-bit vectors, it could have been used to\nimprove the processing of that input, but as its vector size is larger than the input data size, the\nimplementation ends up falling back to one that\u2019s not accelerated. There are also issues related to\nR2R and Native AOT, since ahead-of-time compilation needs to know in advance what instructions\n\nmethods that was JIT compiled in a \u201chello, world\u201d console app, and that this was because it lacked R2R\ncode due to its use of `Vector<T>` .\n\nStarting in .NET Core 3.0, .NET gained literally thousands of new \u201chardware intrinsics\u201d methods, most\nof which are .NET APIs that map down to one of these SIMD instructions. These intrinsics enable an\nexpert to write an implementation tuned to a specific instruction set, and if done well, get the best\npossible performance, but it also requires the developer to understand each instruction set and to\nimplement their algorithm for each instruction set that might be relevant, e.g. an AVX2\nimplementation if it\u2019s supported, or an SSE2 implementation if it\u2019s supported, or an ArmBase\nimplementation if it\u2019s supported, and so on.\n\n.NET 7 has introduced a middle ground. Previous releases saw the introduction of the `Vector128<T>`\nand `Vector256<T>` types, but purely as the vehicle by which data moved in and out of the hardware\nintrinsics, since they\u2019re all tied to specific width vectors. Now in .NET 7, exposed via\n\n54 CHAPTER 2 | JIT\n\n[dotnet/runtime#53450,](https://github.com/dotnet/runtime/pull/53450) [dotnet/runtime#63414, dotnet/runtime#60094, and dotnet/runtime#68559, a](https://github.com/dotnet/runtime/pull/63414)\nvery large set of cross-platform operations is defined over these types as well, e.g.\n\ntarget one or more of these two types. Typically this would amount to a developer writing one code\npath based on `Vector128<T>`, as that has the broadest reach and achieves a significant amount of the\ngains from vectorization, and then if is motivated to do so can add a second path for `Vector256<T>` in\norder to potentially double throughput further on platforms that have 256-bit width vectors. Think of\nthese types and methods as a platform-abstraction layer: you code to these methods, and then the JIT\ntranslates them into the most appropriate instructions for the underlying platform. Consider this\nsimple code as an example:\n\nhere\u2019s what the optimized tier-1 code for these looks like on my x64 Windows machine:\n\n55 CHAPTER 2 | JIT\n\nNotice anything? The code for the two methods is identical, both resulting in a `vpmovmskb` (Move Byte\nMask) instruction. Yet the former code will only work on a platform that supports SSE2 whereas the\nlatter code will work on any platform with support for 128-bit vectors, including Arm64 and WASM\n(and any future platforms on-boarded that also support SIMD); it\u2019ll just result in different instructions\nbeing emitted on those platforms.\n\nTo explore this a bit more, let\u2019s take a simple example and vectorize it. We\u2019ll implement a `Contains`\nmethod, where we want to search a span of bytes for a specific value and return whether it was found:\n\nHow would we vectorize this with `Vector<T>` ? First things first, we need to check whether it\u2019s even\nsupported, and fall back to our existing implementation if it\u2019s not ( `Vector.IsHardwareAccelerated` ).\nWe also need to fall back if the length of the input is less than the size of a vector\n( `Vector<byte>.Count` ).\n\n56 CHAPTER 2 | JIT\n\nNow that we know we have enough data, we can get to coding our vectorized loop. In this loop, we\u2019ll\nbe searching for the `needle`, which means we need a vector that contains that value for every element;\nthe `Vector<T>` \u2019s constructor provides that ( `new Vector<byte>(needle)` ). And we need to be able to\nslice off a vector\u2019s width of data at a time; for a bit more efficiency, I\u2019ll use pointers. We need a current\niteration pointer, and we need to iterate until the point where we couldn\u2019t form another vector\nbecause we\u2019re too close to the end, and a straightforward way to do that is to get a pointer that\u2019s\nexactly one vector\u2019s width from the end; that way, we can just iterate until our current pointer is equal\nto or greater than that threshold. And finally, in our loop body, we need to compare our current\nvector with the target vector to see if any elements are the same ( `Vector.EqualsAny` ), if any is\nreturning true, and if not bumping our current pointer to the next location. At this point we have:\n\nAnd we\u2019re almost done. The last issue to handle is we may still have a few elements at the end we\nhaven\u2019t searched. There are a couple of ways we could handle that. One would be to just continue\nwith our fall back implementation and process each of the remaining elements one at a time. Another\nwould be to employ a trick that\u2019s common when vectorizing idempotent operations. Our operation\nisn\u2019t mutating anything, which means it doesn\u2019t matter if we compare the same element multiple\ntimes, which means we can just do one final vector compare for the last vector in the search space;\n\n57 CHAPTER 2 | JIT\n\nthat might or might not overlap with elements we\u2019ve already looked at, but it won\u2019t hurt anything if it\ndoes. And with that, our implementation is complete:\n\nCongratulations, we\u2019ve vectorized this operation, and fairly decently at that. We can throw this into\nbenchmarkdotnet and see really nice speedups:\n\n58 CHAPTER 2 | JIT\n\n|Method|Mean|Ratio|\n|---|---|---|\n|Find|484.05 ns|1.00|\n|FindVectorized|20.21 ns|0.04|\n\nA 24x speedup! Woo hoo, victory, all your performance are belong to us!\n\nYou deploy this in your service, and you see `Contains` being called on your hot path, but you don\u2019t\nsee the improvements you were expecting. You dig in a little more, and you discover that while you\ntested this with an input array with 1000 elements, typical inputs had more like 30 elements. What\nhappens if we change our benchmark to have just 30 elements? That\u2019s not long enough to form a\nvector, so we fall back to the one-at-a-time path, and we don\u2019t get any speedups at all.\n\nOne thing we can now do is switch from using `Vector<T>` to `Vector128<T>` . That will then lower the\nthreshold from 32 bytes to 16 bytes, such that inputs in that range will still have some amount of\nvectorization applied. As these `Vector128<T>` and `Vector256<T>` types have been designed very\nrecently, they also utilize all the cool new toys, and thus we can use `ref` s instead of pointers. Other\n\npointer arithmetic on the span we fixed.\n\n59 CHAPTER 2 | JIT\n\nWith that in hand, we can now try it on our smaller 30 element data set:\n\n|Method|Mean|Ratio|\n|---|---|---|\n|Find|15.388 ns|1.00|\n|FindVectorized|1.747 ns|0.11|\n\nWoo hoo, victory, all your performance are belong to us\u2026 again!\n\nWhat about on the larger data set again? Previously with `Vector<T>` we had a 24x speedup, but now:\n\n|Method|Mean|Ratio|\n|---|---|---|\n|Find|484.25 ns|1.00|\n|FindVectorized|32.92 ns|0.07|\n\n\u2026 closer to 15x. Nothing to sneeze at, but it\u2019s not the 24x we previously saw. What if we want to have\n\nare enough elements to utilize it.\n\n60 CHAPTER 2 | JIT\n\nAnd, boom, we\u2019re back:\n\n|Method|Mean|Ratio|\n|---|---|---|\n|Find|484.53 ns|1.00|\n|FindVectorized|20.08 ns|0.04|\n\nWe now have an implementation that is vectorized on any platform with either 128-bit or 256-bit\nvector instructions (x86, x64, Arm64, WASM, etc.), that can use either based on the input length, and\nthat can be included in an R2R image if that\u2019s of interest.\n\nThere are many factors that impact which path you go down, and I expect we\u2019ll have guidance\nforthcoming to help navigate all the factors and approaches. But the capabilities are all there, and\nwhether you choose to use `Vector<T>`, `Vector128<T>` and/or `Vector256<T>`, or the hardware\nintrinsics directly, there are some amazing performance opportunities ready for the taking.\n\n61 CHAPTER 2 | JIT\n\nI already mentioned several PRs that exposed the new cross-platform vector support, but that only\nscratches the surface of the work done to actually enable these operations and to enable them to\nproduce high-quality code. As just one example of a category of such work, a set of changes went in\n\nthat add first-class knowledge of vector constants to the JIT\u2019s intermediate representation; and\n[dotnet/runtime#62933,](https://github.com/dotnet/runtime/pull/62933) [dotnet/runtime#65632, dotnet/runtime#55875, dotnet/runtime#67502, and](https://github.com/dotnet/runtime/pull/65632)\n[dotnet/runtime#64783](https://github.com/dotnet/runtime/pull/64783) that all improve the code quality of instructions generated for zero vector\ncomparisons.\n\n**Inlining**\n\nInlining is one of the most important optimizations the JIT can do. The concept is simple: instead of\nmaking a call to some method, take the code from that method and bake it into the call site. This has\nthe obvious advantage of avoiding the overhead of a method call, but except for really small methods\non really hot paths, that\u2019s often on the smaller side of the wins inlining brings. The bigger wins are\ndue to the callee\u2019s code being exposed to the caller\u2019s code, and vice versa. So, for example, if the\ncaller is passing a constant as an argument to the callee, if the method isn\u2019t inlined, the compilation of\nthe callee has no knowledge of that constant, but if the callee is inlined, all of the code in the callee is\nthen aware of its argument being a constant value, and can do all of the optimizations possible with\nsuch a constant, like dead code elimination, branch elimination, constant folding and propagation,\nand so on. Of course, if it were all rainbows and unicorns, everything possible to be inlined would be\ninlined, and that\u2019s obviously not happening. Inlining brings with it the cost of potentially increased\nbinary size. If the code being inlined would result in the same amount or less assembly code in the\ncaller than it takes to call the callee (and if the JIT can quickly determine that), then inlining is a nobrainer. But if the code being inlined would increase the size of the callee non-trivially, now the JIT\nneeds to weigh that increase in code size against the throughput benefits that could come from it.\nThat code size increase can itself result in throughput regressions, due to increasing the number of\ndistinct instructions to be executed and thereby putting more pressure on the instruction cache. As\nwith any cache, the more times you need to read from memory to populate it, the less effective the\ncache will be. If you have a function that gets inlined into 100 different call sites, every one of those\ncall sites\u2019 copies of the callee\u2019s instructions are unique, and calling each of those 100 functions could\nend up thrashing the instruction cache; in contrast, if all of those 100 functions \u201cshared\u201d the same\ninstructions by simply calling the single instance of the callee, it\u2019s likely the instruction cache would be\nmuch more effective and lead to fewer trips to memory.\n\nAll that is to say, inlining is _really_ important, it\u2019s important that the \u201cright\u201d things be inlined and that it\nnot overinline, and as such every release of .NET in recent memory has seen nice improvements\naround inlining. .NET 7 is no exception.\n\n[One really interesting improvement around inlining is dotnet/runtime#64521, and it might be](https://github.com/dotnet/runtime/pull/64521)\nsurprising. Consider the `Boolean.ToString` method; here\u2019s its full implementation:\n\n62 CHAPTER 2 | JIT\n\nPretty simple, right? You\u2019d expect something this trivial to be inlined. Alas, on .NET 6, this benchmark:\n\nproduces this assembly code:\n\nNote the `call System.Boolean.ToString()` . The reason for this is, historically, the JIT has been\nunable to inline methods across assembly boundaries if those methods contain string literals (like the\n\n`\"False\"` and `\"True\"` in that `Boolean.ToString` implementation). This restriction had to do with string\ninterning and the possibility that such inlining could lead to visible behavioral differences. Those\nconcerns are no longer valid, and so this PR removes the restriction. As a result, that same benchmark\non .NET 7 now produces this:\n\nNo more `call System.Boolean.ToString()` .\n\n[dotnet/runtime#61408](https://github.com/dotnet/runtime/pull/61408) made two changes related to inlining. First, it taught the inliner how to better\nsee the what methods were being called in an inlining candidate, and in particular when tiered\ncompilation is disabled or when a method would bypass tier-0 (such as a method with loops before\nOSR existed or with OSR disabled); by understanding what methods are being called, it can better\nunderstand the cost of the method, e.g. if those method calls are actually hardware intrinsics with a\nvery low cost. Second, it enabled CSE in more cases with SIMD vectors.\n\n[dotnet/runtime#71778](https://github.com/dotnet/runtime/pull/71778) also impacted inlining, and in particular in situations where a `typeof()` could\nbe propagated to the callee (e.g. via a method argument). In previous releases of .NET, various\n\n63 CHAPTER 2 | JIT\n\nmembers on `Type` like `IsValueType` were turned into JIT intrinsics, such that the JIT could substitute a\nconstant value for calls where it could compute the answer at compile time. For example, this:\n\nresults in this assembly code on .NET 6:\n\nHowever, change the benchmark slightly:\n\nand it\u2019s no longer as simple:\n\nEffectively, as part of inlining the JIT loses the notion that the argument is a constant and fails to\npropagate it. This PR fixes that, such that on .NET 7, we now get what we expect:\n\n**Arm64**\n\nA huge amount of effort in .NET 7 went into making code gen for Arm64 as good or better than its\nx64 counterpart. I\u2019ve already discussed a bunch of PRs that are relevant regardless of architecture, and\nothers that are specific to Arm, but there are plenty more. To rattle off some of them:\n\n['**Addressing modes** . \u201cAddressing mode\u201d is the term used to refer to how the operand of\\ninstructions are specified. It could be the actual value, it could be the address from where a value\\nshould be loaded, it could be the register containing the value, and so on. Arm supports a\\n\u201cscaled\u201d addressing mode, typically used for indexing into an array, where the size of each\\nelement is supplied and the instruction \u201cscales\u201d the provided offset by the specified scale.\\n[dotnet/runtime#60808 enables the JIT to utilize this addressing mode. More generally,](https://github.com/dotnet/runtime/pull/60808)\\n[dotnet/runtime#70749 enables the JIT to use addressing modes when accessing elements of](https://github.com/dotnet/runtime/pull/70749)']\n\n64 CHAPTER 2 | JIT\n\n[And dotnet/runtime#67490 implements addressing modes for SIMD vectors, specifically for](https://github.com/dotnet/runtime/pull/67490)\nloads with unscaled indices.\n\n['**Better instruction selection** . Various techniques go into ensuring that the best instructions are\\n[selected to represent input code. dotnet/runtime#61037](https://github.com/dotnet/runtime/pull/61037) teaches the JIT how to recognize the']\n\nenables the JIT to recognize certain constant bit shift operations (either explicit in the code or\nimplicit to various forms of managed array access) and emit `sbfiz` / `ubfiz` instructions.\n\n[@SeanWoo](https://github.com/SeanWoo) removes an unnecessary `movi` emitted as part of\n[setting a dereferenced pointer to a constant value. dotnet/runtime#57926 from](https://github.com/dotnet/runtime/pull/57926)\n\n[@SingleAccretion](https://github.com/SingleAccretion) enables computing a 64-bit result as the\n\n`uxtw` / `sxtw` / `lsl` [, while dotnet/runtime#62630](https://github.com/dotnet/runtime/pull/62630) drops redundant zero extensions after a `ldr`\ninstruction.\n\n['**Zeroing** . Lots of operations require state to be set to zero, such as initializing all reference locals\\nin a method to zero as part of the method\u2019s prologue (so that the GC doesn\u2019t see and try to\\nfollow garbage references). While such functionality was previously vectorized,\\n[dotnet/runtime#63422 enables this to be implemented using 128-bit width vector instructions](https://github.com/dotnet/runtime/pull/63422)\\n[on Arm. And dotnet/runtime#64481 changes the instruction sequences used for zeroing in order](https://github.com/dotnet/runtime/pull/64481)\\nto avoid unnecessary zeroing, free up additional registers, and enable the CPU to recognize\\nvarious instruction sequences and better optimize.', '**Memory Model** [. dotnet/runtime#62895 enables store barriers to be used wherever possible](https://github.com/dotnet/runtime/pull/62895)']\n\n[dotnet/runtime#64354 uses a cheaper instruction sequence to handle volatile indirections.](https://github.com/dotnet/runtime/pull/64354)\n[There\u2019s dotnet/runtime#70600, which enables LSE Atomics to be used for](https://github.com/dotnet/runtime/pull/70600) `Interlocked`\n[operations; dotnet/runtime#71512, which enables using the](https://github.com/dotnet/runtime/pull/71512) `atomics` instruction on Unix\n[machines; and dotnet/runtime#70921, which enables the same but on Windows.](https://github.com/dotnet/runtime/pull/70921)\n\n**JIT helpers**\n\nWhile logically part of the runtime, the JIT is actually isolated from the rest of the runtime, only\ninteracting with it through an interface that enables communication between the JIT and the rest of\nthe VM (Virtual Machine). There\u2019s a large amount of VM functionality then that the JIT relies on for\ngood performance.\n\n[dotnet/runtime#65738](https://github.com/dotnet/runtime/pull/65738) rewrote various \u201cstubs\u201d to be more efficient. Stubs are tiny bits of code that\nserve to perform some check and then redirect execution somewhere else. For example, when an\ninterface dispatch call site is expected to only ever be used with a single implementation of that\ninterface, the JIT might employ a \u201cdispatch stub\u201d that compares the type of the object against the\n\n65 CHAPTER 2 | JIT\n\nsingle one it\u2019s cached, and if they\u2019re equal simply jumps to the right target. You know you\u2019re in the\ncorest of the core areas of the runtime when a PR contains lots of assembly code for every\narchitecture the runtime targets. And it paid off; there\u2019s a virtual group of folks from around .NET that\nreview performance improvements and regressions in our automated performance test suites, and\nattribute these back to the PRs likely to be the cause (this is mostly automated but requires some\nhuman oversight). It\u2019s always nice then when a few days after a PR is merged and performance\ninformation has stabilized that you see a rash of comments like there were on this PR:\n\nFor anyone familiar with generics and interested in performance, you may have heard the refrain that\ngeneric virtual methods are relatively expensive. They are, comparatively. For example on .NET 6, this\ncode:\n\nresults in:\n\n66 CHAPTER 2 | JIT\n\n|Method|Mean|Ratio|\n|---|---|---|\n|GenericNonVirtual|0.4866<br>ns|1.00|\n|GenericVirtual|6.4552<br>ns|13.28|\n\n[dotnet/runtime#65926](https://github.com/dotnet/runtime/pull/65926) eases the pain a tad. Some of the cost comes from looking up some cached\ninformation in a hash table in the runtime, and as is the case with many map implementations, this\none involves computing a hash code and using a mod operation to map to the right bucket. Other\n[hash table implementations around dotnet/runtime, including](https://github.com/dotnet/runtime) `Dictionary<,>`, `HashSet<,>`, and\n\n|employed:|Col2|Col3|Col4|\n|---|---|---|---|\n|**Method**|**Runtime**|**Mean**|**Ratio**|\n|GenericVirtual|.NET 6.0|6.475 ns|1.00|\n|GenericVirtual|.NET 7.0|6.119 ns|0.95|\n\nNot enough of an improvement for us to start recommending people use them, but a 5%\nimprovement takes a bit of the edge off the sting.\n\n**Grab Bag**\n\nIt\u2019s near impossible to cover every performance change that goes into the JIT, and I\u2019m not going to\ntry. But there were so many more PRs, I couldn\u2019t just leave them all unsung, so here\u2019s a few more\nquickies:\n\n['[dotnet/runtime#58196 from [@benjamin-hodgson](https://github.com/benjamin-hodgson).](https://github.com/dotnet/runtime/pull/58196)']\n\n67 CHAPTER 2 | JIT\n\n['[dotnet/runtime#69003 from [@SkiFoD](https://github.com/SkiFoD). The pattern](https://github.com/dotnet/runtime/pull/69003) `~x + 1` can be\\nchanged into a two\u2019s-complement negation.']\n\nactually expressible without a following `!= 0` in C#).\n\n68 CHAPTER 2 | JIT\n\n['[dotnet/runtime#62394.](https://github.com/dotnet/runtime/pull/62394) `/` and `%` by a vector\u2019s `.Count` wasn\u2019t recognizing that `Count` can be\\nunsigned, but doing so leads to better code gen.']\n\n69 CHAPTER 2 | JIT\n\n['[dotnet/runtime#60787. Loop alignment in .NET 6](https://github.com/dotnet/runtime/pull/60787) provides a very nice exploration of why and\\nhow the JIT handles loop alignment. This PR extends that further by trying to \u201chide\u201d an emitted\\n`align` instruction behind an unconditional `jmp` that might already exist, in order to minimize the\\nimpact of the processor having to fetch and decode `nop` s.']\n\n70 CHAPTER 2 | JIT\n\n**CHAPTER**"}, "3": {"GC": "\u201cRegions\u201d is a feature of the garbage collector (GC) that\u2019s been in the works for multiple years. It\u2019s\n[enabled by default in 64-bit processes in .NET 7 as of dotnet/runtime#64688, but as with other multi-](https://github.com/dotnet/runtime/pull/64688)\nyear features, a multitude of PRs went into making it a reality. At a 30,000 foot level, \u201cregions\u201d replaces\nthe current \u201csegments\u201d approach to managing memory on the GC heap; rather than having a few\ngigantic segments of memory (e.g. each 1GB), often associated 1:1 with a generation, the GC instead\nmaintains many, many smaller regions (e.g. each 4MB) as their own entity. This enables the GC to be\nmore agile with regards to operations like repurposing regions of memory from one generation to\n[another. For more information on regions, the blog post Put a DPAD on that GC!](https://devblogs.microsoft.com/dotnet/put-a-dpad-on-that-gc) from the primary\ndeveloper on the GC is still the best resource.\n\n71 CHAPTER 3 | GC\n\n**CHAPTER**"}, "4": {"Native AOT": "To many people, the word \u201cperformance\u201d in the context of software is about throughput. How fast\ndoes something execute? How much data per second can it process? How many requests per second\ncan it process? And so on. But there are many other facets to performance. How much memory does\nit consume? How fast does it start up and get to the point of doing something useful? How much\nspace does it consume on disk? How long would it take to download? And then there are related\nconcerns. In order to achieve these goals, what dependencies are required? What kinds of operations\ndoes it need to perform to achieve these goals, and are all of those operations permitted in the target\nenvironment? If any of this paragraph resonates with you, you are the target audience for the Native\nAOT support now shipping in .NET 7.\n\n.NET has long had support for AOT code generation. For example, .NET Framework had it in the form\nof `ngen`, and .NET Core has it in the form of `crossgen` . Both of those solutions involve a standard .NET\nexecutable that has some of its IL already compiled to assembly code, but not all methods will have\nassembly code generated for them, various things can invalidate the assembly code that was\ngenerated, external .NET assemblies without any native assembly code can be loaded, and so on, and\nin all of those cases, the runtime continues to utilize a JIT compiler. Native AOT is different. It\u2019s an\nevolution of CoreRT, which itself was an evolution of .NET Native, and it\u2019s entirely free of a JIT. The\nbinary that results from publishing a build is a completely standalone executable in the target\nplatform\u2019s platform-specific file format (e.g. COFF on Windows, ELF on Linux, Mach-O on macOS) with\nno external dependencies other than ones standard to that platform (e.g. libc). And it\u2019s entirely native:\nno IL in sight, no JIT, no nothing. All required code is compiled and/or linked in to the executable,\nincluding the same GC that\u2019s used with standard .NET apps and services, and a minimal runtime that\nprovides services around threading and the like. All of that brings great benefits: super fast startup\ntime, small and entirely-self contained deployment, and ability to run in places JIT compilers aren\u2019t\nallowed (e.g. because memory pages that were writable can\u2019t then be executable). It also brings\nlimitations: no JIT means no dynamic loading of arbitrary assemblies (e.g. `Assembly.LoadFile` ) and no\nreflection emit (e.g. `DynamicMethod` ), everything compiled and linked in to the app means the more\nfunctionality that\u2019s used (or might be used) the larger is your deployment, etc. Even with those\nlimitations, for a certain class of application, Native AOT is an incredibly exciting and welcome\naddition to .NET 7.\n\nToo many PRs to mention have gone into bringing up the Native AOT stack, in part because it\u2019s been\n[in the works for years (as part of the archived dotnet/corert](https://github.com/dotnet/corert) project and then as part of\n[dotnet/runtimelab/feature/NativeAOT) and in part because there have been over a hundred PRs just](https://github.com/dotnet/runtimelab/tree/feature/NativeAOT)\n[in dotnet/runtime](https://github.com/dotnet/runtime) that have gone into bringing Native AOT up to a shippable state since the code was\n[originally brought over from dotnet/runtimelab](https://github.com/dotnet/runtimelab) [in dotnet/runtime#62563](https://github.com/dotnet/runtime/pull/62563) [and dotnet/runtime#62563.](https://github.com/dotnet/runtime/pull/62611)\nBetween that and there not being a previous version to compare its performance to, instead of\nfocusing PR by PR on improvements, let\u2019s just look at how to use it and the benefits it brings.\n\n72 CHAPTER 4 | Native AOT\n\nToday, Native AOT is focused on console applications, so let\u2019s create a console app:\n\n```\ndotnet new console -o nativeaotexample\n\n<PublishAot>true</PublishAot>\n\n```\n\nAnd then\u2026 actually, that\u2019s it. Our app is now fully configured to be able to target Native AOT. All that\u2019s\nleft is to publish. As I\u2019m currently writing this on my Windows x64 machine, I\u2019ll target that:\n\n```\ndotnet publish -r win-x64 -c Release\n\n```\n\nI now have my generated executable in the output publish directory:\n\nso 2M instead of 3.5MB. Of course, for that significant reduction I\u2019ve given up some things:\n\n['Setting `InvariantGlobalization` to true means I\u2019m now not respecting culture information and\\nam instead using a set of invariant data for most globalization operations.']\n\nshould I need to debug an exception.\n\n['Setting `DebuggerSupport` to false\u2026 good luck debugging things.', '\u2026 you get the idea.']\n\nOne of the potentially mind-boggling aspects of Native AOT for a developer used to .NET is that, as it\nsays on the tin, it really is native. After publishing the app, there is no IL involved, and there\u2019s no JIT\nthat could even process it. This makes some of the other investments in .NET 7 all the more valuable,\nfor example everywhere investments are happening in source generators. Code that previously relied\non reflection emit for good performance will need another scheme. We can see that, for example, with\n\nimplementation of the specified pattern. But if you look at the implementation of the `Regex`\nconstructor, you\u2019ll find this nugget:\n\nWith the JIT, `IsDynamicCodeCompiled` is true. But with Native AOT, it\u2019s false. Thus, with Native AOT\n\n`[GeneratedRegex(...)]`, which, along with the new regex source generator shipping in the .NET 7\n\n73 CHAPTER 4 | Native AOT\n\nSDK, emits C# code into the assembly using it. That C# code takes the place of the reflection emit that\nwould have happened at run-time, and is thus able to work successfully with Native AOT.\n\n|Method|Mean|Ratio|\n|---|---|---|\n|Interpreter|9,036.7 us|1.00|\n|Compiled|9,064.8 us|1.00|\n|SourceGenerator|426.1 us|0.05|\n\nSo, yes, there are some constraints associated with Native AOT, but there are also solutions for\nworking with those constraints. And further, those constraints can actually bring further benefits.\n[Consider dotnet/runtime#64497. Remember how we talked about \u201cguarded devirtualization\u201d in](https://github.com/dotnet/runtime/pull/64497)\ndynamic PGO, where via instrumentation the JIT can determine the most likely type to be used at a\ngiven call site and special-case it? With Native AOT, the entirety of the program is known at compile\ntime, with no support for `Assembly.LoadFrom` or the like. That means at compile time, the compiler\ncan do whole-program analysis to determine what types implement what interfaces. If a given\ninterface only has a single type that implements it, then every call site through that interface can be\nunconditionally devirtualized, without any type-check guards.\n\nThis is a really exciting space, one we expect to see flourish in coming releases.\n\n74 CHAPTER 4 | Native AOT\n\n**CHAPTER**"}, "5": {"Mono": "Up until now I\u2019ve referred to \u201cthe JIT,\u201d \u201cthe GC,\u201d and \u201cthe runtime,\u201d but in reality there are actually\nmultiple runtimes in .NET. I\u2019ve been talking about \u201ccoreclr,\u201d which is the runtime that\u2019s recommended\nfor use on Linux, macOS, and Windows. However, there\u2019s also \u201cmono,\u201d which powers Blazor wasm\napplications, Android apps, and iOS apps. It\u2019s also seen significant improvements in .NET 7.\n\nJust as with coreclr (which can JIT compile, AOT compile partially with JIT fallback, and fully Native\nAOT compile), mono has multiple ways of actually executing code. One of those ways is an interpreter,\nwhich enables mono to execute .NET code in environments that don\u2019t permit JIT\u2019ing and without\nrequiring ahead-of-time compilation or incurring any limitations it may bring. Interestingly, though,\nthe interpreter is itself almost a full-fledged compiler, parsing the IL, generating its own intermediate\nrepresentation (IR) for it, and doing one or more optimization passes over that IR; it\u2019s just that at the\nend of the pipeline when a compiler would normally emit code, the interpreter instead saves off that\ndata for it to interpret when the time comes to run. As such, the interpreter has a very similar\nconundrum to the one we discussed with coreclr\u2019s JIT: the time it takes to optimize vs the desire to\nstart up quickly. And in .NET 7, the interpreter employs a similar solution: tiered compilation.\n[dotnet/runtime#68823](https://github.com/dotnet/runtime/pull/68823) adds the ability for the interpreter to initially compile with minimal\noptimization of that IR, and then once a certain threshold of call counts has been hit, then take the\ntime to do as much optimization on the IR as possible for all future invocations of that method. This\nyields the same benefits as it does for coreclr: improved startup time while also having efficient\nsustained throughput. When this merged, we saw improvements in Blazor wasm app startup time\nimprove by 10-20%. Here\u2019s one example from an app being tracked in our benchmarking system:\n\n75 CHAPTER 5 | Mono\n\nThe interpreter isn\u2019t just used for entire apps, though. Just as how coreclr can use the JIT when an R2R\nimage doesn\u2019t contain code for a method, mono can use the interpreter when there\u2019s no AOT code\nfor a method. Once such case that occurred on mono was with generic delegate invocation, where the\npresence of a generic delegate being invoked would trigger falling back to the interpreter; for .NET 7,\n[that gap was addressed with dotnet/runtime#70653. A more impactful case, however, is](https://github.com/dotnet/runtime/pull/70653)\n[dotnet/runtime#64867. Previously, any methods with](https://github.com/dotnet/runtime/pull/64867) `catch` or `filter` exception handling clauses\ncouldn\u2019t be AOT compiled and would fall back to being interpreted. With this PR, the method is now\nable to be AOT compiled, and it only falls back to using the interpreter when an exception actually\noccurs, switching over to the interpreter for the remainder of that method call\u2019s execution. Since many\nmethods contain such clauses, this can make a big difference in throughput and CPU consumption. In\n\ninterpreted.\n\nBeyond such backend improvements, another class of improvement came from further unification\nbetween coreclr and mono. Years ago, coreclr and mono had their own entire library stack built on\ntop of them. Over time, as .NET was open sourced, portions of mono\u2019s stack got replaced by shared\ncomponents, bit by bit. Fast forward to today, all of the core .NET libraries above\n\n(these statements means that the vast majority of the performance improvements discussed in the\nrest of this post apply equally whether running on mono and coreclr). Even so, every release now we\ntry to chip away at that few remaining percent, for reasons of maintainability, but also because the\nsource used for coreclr\u2019s `CoreLib` has generally had more attention paid to it from a performance\n[perspective. dotnet/runtime#71325, for example, moves mono\u2019s array and span sorting generic](https://github.com/dotnet/runtime/pull/71325)\nsorting utility class over to the more efficient implementation used by coreclr.\n\nOne of the biggest categories of improvements, however, is in vectorization. This comes in two pieces.\nFirst, `Vector<T>` and `Vector128<T>` are now fully accelerated on both x64 and Arm64, thanks to PRs\n[like dotnet/runtime#64961, dotnet/runtime#65086,](https://github.com/dotnet/runtime/pull/64961) [dotnet/runtime#65128,](https://github.com/dotnet/runtime/pull/65128) [dotnet/runtime#66317,](https://github.com/dotnet/runtime/pull/66317)\n\n76 CHAPTER 5 | Mono\n\n[dotnet/runtime#66391,](https://github.com/dotnet/runtime/pull/66391) [dotnet/runtime#66409, dotnet/runtime#66512, dotnet/runtime#66586,](https://github.com/dotnet/runtime/pull/66409)\n[dotnet/runtime#66589,](https://github.com/dotnet/runtime/pull/66589) [dotnet/runtime#66597, dotnet/runtime#66476, and dotnet/runtime#67125;](https://github.com/dotnet/runtime/pull/66597)\nthat significant amount of work means all that code that gets vectorized using these abstractions will\n[light-up on mono and coreclr alike. Second, thanks primarily to dotnet/runtime#70086, mono now](https://github.com/dotnet/runtime/pull/70086)\n\nanywhere else WASM might be executed.\n\n77 CHAPTER 5 | Mono\n\n**CHAPTER**"}, "6": {"Reflection": "Reflection is one of those areas you either love or hate (I find it a bit humorous to be writing this\nsection immediately after writing the Native AOT section). It\u2019s immensely powerful, providing the\nability to query all of the metadata for code in your process and for arbitrary assemblies you might\nencounter, to invoke arbitrary functionality dynamically, and even to emit dynamically-generated IL at\nrun-time. It\u2019s also difficult to handle well in the face of tooling like a linker or a solution like Native\nAOT that needs to be able to determine at build time exactly what code will be executed, and it\u2019s\ngenerally quite expensive at run-time; thus it\u2019s both something we strive to avoid when possible but\nalso invest in reducing the costs of, as it\u2019s so popular in so many different kinds of applications\nbecause it is incredibly useful. As with most releases, it\u2019s seen some nice improvements in .NET 7.\n\nOne of the most impacted areas is reflection invoke. Available via `MethodBase.Invoke`, this\nfunctionality let\u2019s you take a `MethodBase` (e.g. `MethodInfo` ) object that represents some method for\nwhich the caller previously queried, and call it, with arbitrary arguments that the runtime needs to\nmarshal through to the callee, and with an arbitrary return value that needs to be marshaled back. If\nyou know the signature of the method ahead of time, the best way to optimize invocation speed is to\ncreate a delegate from the `MethodBase` via `CreateDelegate<T>` and then use that delegate for all\nfuture invocations. But in some circumstances, you don\u2019t know the signature at compile time, and thus\ncan\u2019t easily rely on delegates with known matching signatures. To address this, some libraries have\ntaken to using reflection emit to generate code at run-time specific to the target method. This is\nextremely complicated and it\u2019s not something we want apps to have to do. Instead, in .NET 7 via\n\ngives developers most of the performance benefits of a custom reflection emit-based implementation\nbut without having the complexity or challenges of such an implementation in their own code base.\n\n78 CHAPTER 6 | Reflection\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|MethodInfoInvoke|.NET 6.0|43.846 ns|1.00|\n|MethodInfoInvoke|.NET 7.0|8.078 ns|0.18|\n\nReflection also involves lots of manipulation of objects that represent types, methods, properties, and\nso on, and tweaks here and there can add up to a measurable difference when using these APIs. For\nexample, I\u2019ve talked in past performance posts about how, potentially counterintuitively, one of the\nways we\u2019ve achieved performance boosts is by porting native code from the runtime back into\nmanaged C#. There are a variety of ways in which doing so can help performance, but one is that\nthere is some overhead associated with calling from managed code into the runtime, and eliminating\n[such hops avoids that overhead. This can be seen in full effect in dotnet/runtime#71873, which moves](https://github.com/dotnet/runtime/pull/71873)\nseveral of these \u201cFCalls\u201d related to `Type`, `RuntimeType` (the `Type` -derived class used by the runtime to\nrepresent its types), and `Enum` out of native into managed.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|GetUnderlyingType|.NET 6.0|27.413 ns|1.00|\n|GetUnderlyingType|.NET 7.0|5.115 ns|0.19|\n\n[Another example of this phenomenon comes in dotnet/runtime#62866, which moved much of the](https://github.com/dotnet/runtime/pull/62866)\nunderlying support for `AssemblyName` out of native runtime code into managed code in CoreLib. That\nin turn has an impact on anything that uses it, such as when using `Activator.CreateInstance`\noverloads that take assembly names that need to be parsed.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|CreateInstance|.NET 6.0|3.827 us|1.00|\n|CreateInstance|.NET 7.0|2.276 us|0.60|\n\nOther changes contributed to `Activator.CreateInstance` improvements as well.\n\nresulting in less allocation and faster throughput.\n\n79 CHAPTER 6 | Reflection\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|CreateInstance|.NET 6.0|167.8 ns|1.00|320 B|1.00|\n|CreateInstance|.NET 7.0|143.4 ns|0.85|200 B|0.62|\n\nAnd since we were talking about `AssemblyName`, other PRs improved it in other ways as well.\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Names|.NET 6.0|3.423 us|1.00|9.14 KB|1.00|\n|Names|.NET 7.0|2.010 us|0.59|2.43 KB|0.27|\n\nMore reflection-related operations have also been turned into JIT intrinsics, as discussed earlier\nenabling the JIT to compute answers to various questions at JIT compile time rather than at run-time.\nThis was done, for example, for `Type.IsByRefLike` [in dotnet/runtime#67852.](https://github.com/dotnet/runtime/pull/67852)\n\n|Method|Runtime|Mean|Ratio|Code Size|\n|---|---|---|---|---|\n|IsByRefLike|.NET 6.0|2.1322 ns|1.000|31 B|\n|IsByRefLike|.NET 7.0|0.0000 ns|0.000|6 B|\n\nThat the .NET 7 version is so close to zero is called out in a warning by benchmarkdotnet:\n\nand it\u2019s so indistinguishable from an empty method because that\u2019s effectively what it is, as we can see\nfrom the disassembly:\n\n80 CHAPTER 6 | Reflection\n\nThere are also improvements that are hard to see but that remove overheads as part of populating\nreflection\u2019s caches, which end up reducing the work done typically on startup paths, helping apps to\n[launch faster. dotnet/runtime#66825, dotnet/runtime#66912, and dotnet/runtime#67149 all fall into](https://github.com/dotnet/runtime/pull/66825)\nthis category by removing unnecessary or duplicative array allocations as part of gathering data on\nparameters, properties, and events.\n\n81 CHAPTER 6 | Reflection\n\n**CHAPTER**"}, "7": {"Interop": ".NET has long had great support for interop, enabling .NET applications to consume huge amounts of\nfunctionality written in other languages and/or exposed by the underlying operating system. The\nbedrock of this support has been \u201cPlatform Invoke,\u201d or \u201cP/Invoke,\u201d represented in code by\n\n`[DllImport(...)]` applied to methods. The `DllImportAttribute` enables declaring a method that\ncan be called like any other .NET method but that actually represents some external method that the\nruntime should call when this managed method is invoked. The DllImport specifies details about in\nwhat library the function lives, what its actual name is in the exports from that library, high-level\ndetails about marshalling of input arguments and return values, and so on, and the runtime ensures\nall the right things happen. This mechanism works on all operating systems. For example, Windows\nhas a method `CreatePipe` for creating an anonymous pipe:\n\nIf I want to call this function from C#, I can declare a `[DllImport(...)]` counterpart to it which I can\nthen invoke as I can any other managed method:\n\nThere are several interesting things to note here. Several of the arguments are directly blittable with\n\ncalling this managed method can\u2019t just directly invoke the native function somehow, as there needs to\n\nit\u2019s no longer being used). Some logic needs to take the output handles generated by the native\n\n82 CHAPTER 7 | Interop\n\nand some code somewhere needs to take any error produced by this method and ensure it\u2019s available\nfor consumption via a subsequent `GetLastPInvokeError()` .\n\nIf there\u2019s no marshalling logic required, such that the managed signature and native signature are for\nall intents and purposes the same, all arguments blittable, all return values blittable, no additional\nlogic required around the invocation of the method, etc., then a `[DllImport(...)]` ends up being a\nsimple passthrough with the runtime needing to do very little work to implement it. If, however, the\n\n`[DllImport(...)]` involves any of this marshalling work, the runtime needs to generate a \u201cstub,\u201d\ncreating a dedicated method that\u2019s called when the `[DllImport(...)]` is called, that handles fixing\nup all inputs, that delegates to the actual native function, and that fixes up all of the outputs. That\nstub is generated at execution time, with the runtime effectively doing reflection emit, generating IL\ndynamically that\u2019s then JIT\u2019d.\n\nThere are a variety of downsides to this. First, it takes time to generate all that marshalling code, time\nwhich can then negatively impact user experience for things like startup. Second, the nature of its\nimplementation inhibits various optimizations, such as inlining. Third, there are platforms that don\u2019t\nallow for JIT\u2019ing due to the security exposure of allowing for dynamically generated code to then be\nexecuted (or in the case of Native AOT, where there isn\u2019t a JIT at all). And fourth, it\u2019s all hidden away\nmaking it more challenging for a developer to really understand what\u2019s going on.\n\nBut what if that logic could all be generated at build time rather than at run time? The cost of\ngenerating the code would be incurred only at build time and not on every execution. The code would\neffectively just end up being user code that has all of the C# compiler\u2019s and runtime\u2019s optimizations\navailable to it. The code, which then would just be part of the app, would be able to be ahead-of-time\ncompiled using whatever AOT system is desirable, whether it be crossgen or Native AOT or some\nother system. And the code would be inspectable, viewable by users to understand exactly what work\nis being done on their behalf. Sounds pretty desirable. Sounds magical. Sounds like a job for a Roslyn\nsource generator, mentioned earlier.\n\n.NET 6 included several source generators in the .NET SDK, and .NET 7 doubles down on this effort\nincluding several more. One of these is the brand new LibraryImport generator, which provides exactly\nthe magical, desirable solution we were just discussing.\n\nNow if you\u2019re following along at home in Visual Studio, try right-clicking on CreatePipe and selecting\nGo to Definition. That might seem a little strange. \u201cGo to Definition? Isn\u2019t this the definition?\u201d This is a\npartial method, which is a way of declaring something that another partial definition fills in, and in this\ncase, a source generator in .NET 7 SDK has noticed this method with the `[LibraryImport]` attribute\nand fully generated the entire marshalling stub code in C# that\u2019s built directly into the assembly.\nWhile by default that code isn\u2019t persisted, Visual Studio still enables you to browse it (and you can\n\n83 CHAPTER 7 | Interop\n\nopt-in to having it persisted on disk by adding a\n\n`<EmitCompilerGeneratedFiles>true</EmitCompilerGeneratedFiles>` property into your .csproj).\nHere\u2019s what it currently looks like for that method:\n\n84 CHAPTER 7 | Interop\n\nWith this, you can read exactly the marshalling work that\u2019s being performed. Two `SafeHandle`\ninstances are being allocated and then later after the native function completes, the\n\n`Marshal.InitHandle` method is used to store the resulting handles into these instances (the\nallocations happen before the native function call, as performing them after the native handles have\n\nany stub to be generated by the runtime, as all that work has been handled in this C# code.\n\n[A sheer ton of work went in to enabling this. I touched on some of it last year in Performance](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6/)\n[Improvements in .NET 6, but a significant amount of additional effort has gone into .NET 7 to polish](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6/)\n[the design and make the implementation robust, roll it out across all of dotnet/runtime and beyond,](https://github.com/dotnet/runtime)\nand expose the functionality for all C# developers to use:\n\n['[The LibraryImport generator started its life as an experiment in dotnet/runtimelab. When it was](https://github.com/dotnet/runtimelab)\\n[ready, dotnet/runtime#59579 brought 180 commits spanning years of effort into the](https://github.com/dotnet/runtime/pull/59579)\\n[dotnet/runtime](https://github.com/dotnet/runtime) main branch.', 'In .NET 6, there were almost 3000 `[DllImport]` uses throughout the core .NET libraries. As of my\\nwriting this, in .NET 7 there are\u2026 let me search\u2026 wait for it\u2026 7 (I was hoping I could say 0, but\\nthere are just a few stragglers, mostly related to COM interop, still remaining). That\u2019s not a\\ntransformation that happens over night. A multitude of PRs went library by library converting\\n[from the old to the new, such as dotnet/runtime#62295](https://github.com/dotnet/runtime/pull/62295) [and dotnet/runtime#61640 for](https://github.com/dotnet/runtime/pull/61640)\\n[System.Private.CoreLib, dotnet/runtime#61742](https://github.com/dotnet/runtime/pull/61742) [and dotnet/runtime#62309 for the cryptography](https://github.com/dotnet/runtime/pull/62309)\\n[libraries, dotnet/runtime#61765 for networking, dotnet/runtime#61996](https://github.com/dotnet/runtime/pull/61765) and\\n[dotnet/runtime#61638 for most of the other I/O-related libraries, and a long-tail of additional](https://github.com/dotnet/runtime/pull/61638)\\n[porting in dotnet/runtime#61975, dotnet/runtime#61389,](https://github.com/dotnet/runtime/pull/61975) [dotnet/runtime#62353,](https://github.com/dotnet/runtime/pull/62353)\\n[dotnet/runtime#61990, dotnet/runtime#61949,](https://github.com/dotnet/runtime/pull/61990) [dotnet/runtime#61805, dotnet/runtime#61741,](https://github.com/dotnet/runtime/pull/61805)\\n[dotnet/runtime#61184, dotnet/runtime#54290,](https://github.com/dotnet/runtime/pull/61184) [dotnet/runtime#62365, dotnet/runtime#61609,](https://github.com/dotnet/runtime/pull/62365)\\n[dotnet/runtime#61532, and dotnet/runtime#54236.](https://github.com/dotnet/runtime/pull/61532)', 'Such porting is significantly easier when there\u2019s a tool to help automate it.\\n[dotnet/runtime#72819 enables the analyzer and fixer for performing these transformations.](https://github.com/dotnet/runtime/pull/72819)\\n:::{custom-style=Figure}']\n\n85 CHAPTER 7 | Interop\n\n:::\n\nThere were plenty of other PRs that went into making the LibraryImport generator a reality for .NET 7.\n\nthe runtime\u2019s built-in marshalling; at that point, the only marshalling performed as part of interop is\nthe marshaling done in the user\u2019s code, e.g. that which is generated by `[LibraryImport]` . Other PRs\n\nmarshaling is performed (the generator is pattern-based and allows for customization of marshalling\nby providing types that implement the right shape, which these types do in support of the most\n\n[signatures, just as arrays can be (examples in dotnet/runtime](https://github.com/dotnet/runtime) [are available in dotnet/runtime#73256).](https://github.com/dotnet/runtime/pull/73256)\n\nOne more category of interop-related changes that I think are worth talking about are to do with\n\n`SafeHandle` cleanup. As a reminder, `SafeHandle` exists to mitigate various issues around managing\nnative handles and file descriptors. A native handle or file descriptor is just a memory address or\nnumber that refers to some owned resource and which must be cleaned up / closed when done with\nit. A `SafeHandle` at its core is just a managed object that wraps such a value and provides a `Dispose`\n\nits finalizer eventually run. `SafeHandle` then also provides some synchronization around that closure,\ntrying to minimize the possibility that the resource is closed while it\u2019s still in use. It provides\n\n`SafeHandle` into a P/Invoke, the generated code for that P/Invoke handles calling `DangerousAddRef`\n\n86 CHAPTER 7 | Interop\n\nand `DangerousRelease` (and due to the wonders of LibraryImport I\u2019ve already extolled, you can easily\nsee that being done, such as in the previous generated code example). Our code tries hard to clean\nup after `SafeHandle` s deterministically, but it\u2019s quite easy to accidentally leave some for finalization.\n\n[dotnet/runtime#71854](https://github.com/dotnet/runtime/pull/71991) added some debug-only tracking code to `SafeHandle` to make it easier for\n[developers working in dotnet/runtime (or more specifically, developers using a checked build of the](https://github.com/dotnet/runtime)\nruntime) to find such issues. When the `SafeHandle` is constructed, it captures the current stack trace,\n\nensure they\u2019re being disposed of. As is probably evident from that PR touching over 150 files and\nalmost 1000 lines of code, there were quite a few places that benefited from clean up. Now to be fair,\nmany of these are on exceptional code paths. For example, consider a hypothetical P/Invoke like:\n\nand code that uses it like:\n\nSeems straightforward enough. Except this code will actually leave a `SafeHandle` for finalization on\nthe failure path. It doesn\u2019t matter that `SafeHandle` has an invalid handle in it, it\u2019s still a finalizable\nobject. To deal with that, this code would have been more robustly written as:\n\nThat way, this `SafeHandle` won\u2019t create finalization pressure even in the case of failure. Note, as well,\n\nclose the resource, which typically involves making another P/Invoke. And if that P/Invoke has\n\n`SetLastError=true` on it, it can overwrite the very error code for which we\u2019re about to throw. Hence,\nwe access and store the last error immediately after the interop call once we know it failed, then clean\nup, and only then throw. All that said, there were a myriad of places in that PR where `SafeHandle` s\nwere being left for finalization even on the success path. And that PR wasn\u2019t alone.\n[dotnet/runtime#71991,](https://github.com/dotnet/runtime/pull/71991) [dotnet/runtime#71854, dotnet/runtime#72116, dotnet/runtime#72189,](https://github.com/dotnet/runtime/pull/71854)\n\nthe earlier mentioned PR).\n\n87 CHAPTER 7 | Interop\n\n[Other PRs also accrued to improved interop performance. dotnet/runtime#70000](https://github.com/dotnet/runtime/pull/70000) from\n\n[@huoyaoyuan](https://github.com/huoyaoyuan) rewrote several delegate-related \u201cFCalls\u201d from being\nimplemented in native code to instead being managed, resulting in less overhead when invoking\nthese operations that are commonly involved in scenarios involving\n\n`Marshal.GetDelegateForFunctionPointer` [. dotnet/runtime#68694](https://github.com/dotnet/runtime/pull/68694) also moved some trivial\nfunctionality from native to managed, as part of relaxing argument validation on the use of pinning\nhandles. This in turn measurably reduced the overhead involved with using `GCHandle.Alloc` for such\npinning handles:\n\n|Method|Runtime|Mean|Ratio|Code Size|\n|---|---|---|---|---|\n|PinUnpin|.NET 6.0|37.11 ns|1.00|353 B|\n|PinUnpin|.NET 7.0|32.17 ns|0.87|232 B|\n\n88 CHAPTER 7 | Interop\n\n**CHAPTER**"}, "8": {"Threading": "Threading is one of those cross-cutting concerns that impacts every application, such that changes in\nthe threading space can have a wide-spread impact. This release sees two very substantial changes to\nthe `ThreadPool` [itself; dotnet/runtime#64834 switches the \u201cIO pool\u201d over to using an entirely](https://github.com/dotnet/runtime/pull/64834)\nmanaged implementation (whereas previously the IO pool was still in native code even though the\n[worker pool had been moved entirely to managed in previous releases), and dotnet/runtime#71864](https://github.com/dotnet/runtime/pull/71864)\nsimilarly switches the timer implementation from one based in native to one entirely in managed\ncode. Those two changes can impact performance, and the former was demonstrated to on larger\nhardware, but for the most part that wasn\u2019t their primary goal. Instead, other PRs have been focused\non improving throughput.\n\n[One in particular is dotnet/runtime#69386. The](https://github.com/dotnet/runtime/pull/69386) `ThreadPool` has a \u201cglobal queue\u201d that any thread can\nqueue work into, and then each thread in the pool has its own \u201clocal queue\u201d (which any thread can\ndequeue from but only the owning thread can enqueue into). When a worker needs another piece of\nwork to process, it first checks its own local queue, then it checks the global queue, and then only if it\ncouldn\u2019t find work in either of those two places, it goes and checks all of the other threads\u2019 local\nqueues to see if it can help lighten their load. As machines scale up to have more and more cores, and\nmore and more threads, there\u2019s more and more contention on these shared queues, and in particular\non the global queue. This PR addresses this for such larger machines by introducing additional global\nqueues once the machine reaches a certain threshold (32 processors today). This helps to partition\naccesses across multiple queues, thereby decreasing contention.\n\n[Another is dotnet/runtime#57885. In order to coordinate threads, when work items were enqueued](https://github.com/dotnet/runtime/pull/57885)\nand dequeued, the pool was issuing requests to its threads to let them know that there was work\navailable to do. This, however, often resulted in oversubscription, where more threads than necessary\nwould race to try to get work items, especially when the system wasn\u2019t at full load. That in turn would\nmanifest as a throughput regression. This change overhauls how threads are requested, such that only\none additional thread is requested at a time, and after that thread has dequeued its first work item, it\ncan issue a request for an additional thread if there\u2019s work remaining, and then that one can issue an\nadditional request, and so on. Here\u2019s one of our performance tests in our performance test suite (I\u2019ve\nsimplified it down to remove a bunch of configuration options from the test, but it\u2019s still accurately\none of those configurations). At first glance you might think, \u201chey, this is a performance test about\n\n`ArrayPool`, why is it showing up in a threading discussion?\u201d And, you\u2019d be right, this is a performance\n\ndoing \u201creal work\u201d that competes for CPU cycles with thread pool threads all racing to get their next\ntask, it shows a measurable improvement when moving to .NET 7.\n\n89 CHAPTER 8 | Threading\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|MultipleSerial|.NET 6.0|14.340 us|1.00|\n|MultipleSerial|.NET 7.0|9.262 us|0.65|\n\nreturning whatever data it finds, or in the case of a setter, creating a new `ExecutionContext` with an\nupdated dictionary and publishing that back. This dictionary thus needs to be very efficient for reads\nand writes, as developers expect `AsyncLocal<T>` access to be as fast as possible, often treating it as if\nit were any other local. So, to optimize these lookups, the representation of that dictionary changes\nbased on how many `AsyncLocal<T>` s are represented in this context. For up to three items, dedicated\nimplementations with fields for each of the three keys and values were used. Above that up to around\n\ninstances is really common, especially in ASP.NET where ASP.NET infrastructure itself uses a couple.\n\n90 CHAPTER 8 | Threading\n\nSo, this PR took the complexity hit to add a dedicated type for four key/value pairs, in order to\noptimize from one to four of them rather than one to three. While this improves throughput a bit, its\nmain intent was to improve allocation, which is does over .NET 6 by ~20%.\n\n|Method|Runtime|Mean|Ratio|Code Size|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|---|\n|Update|.NET 6.0|61.96 ns|1.00|1,272 B|176 B|1.00|\n|Update|.NET 7.0|61.92 ns|1.00|1,832 B|144 B|0.82|\n\n[Another valuable fix comes for locking in dotnet/runtime#70165. This particular improvement is a bit](https://github.com/dotnet/runtime/pull/70165)\nharder to demonstrate with benchmarkdotnet, so just try running this program, first on .NET 6 and\nthen on .NET 7:\n\n91 CHAPTER 8 | Threading\n\nThis is simply spinning up 100 tasks, each of which enters and exits a read-write lock, waits for them\nall, and then does the process over again, for 10 seconds. It also times how long it takes to enter and\nexit the lock, and writes a warning if it had to wait for at least 15ms. When I run this on .NET 6, I get\n~100 occurrences of it taking >= 10 ms to enter/exit the lock. On .NET 7, I get 0 occurrences. Why the\ndifference? The implementation of `ReaderWriterLockSlim` has its own spin loop implementation, and\nthat spin loop tries to mix together various things to do as it spins, ranging from calling\n\n`Thread.SpinWait` to `Thread.Sleep(0)` to `Thread.Sleep(1)` . The issue lies in the `Thread.Sleep(1)` .\nThat\u2019s saying \u201cput this thread to sleep for 1 millisecond\u201d; however, the operating system has the\nultimate say on such timings, and on Windows, by default that sleep is going to be closer to 15\nmilliseconds (on Linux it\u2019s a bit lower but still quite high). Thus, every time there was enough\ncontention on the lock to force it to call `Thread.Sleep(1)`, we\u2019d incur a delay of at least 15\nmilliseconds, if not more. The aforementioned PR fixed this by eliminating use of `Thread.Sleep(1)` .\n\n[One final threading-related change to call out: dotnet/runtime#68639. This one is Windows specific.](https://github.com/dotnet/runtime/pull/68639)\nWindows has the concept of processor groups, each of which can have up to 64 cores in it, and by\ndefault when a process runs, it\u2019s assigned a specific processor group and can only use the cores in\nthat group. With .NET 7, the runtime flips its default so that by default it will try to use all processor\ngroups if possible.\n\n92 CHAPTER 8 | Threading\n\n**CHAPTER**"}, "9": {"Primitive Types and Numerics": "We\u2019ve looked at code generation and GC, at threading and vectorization, at interop\u2026 let\u2019s turn our\nattention to some of the fundamental types in the system. Primitives like `int` and `bool` and `double`,\ncore types like `Guid` and `DateTime`, they form the backbone on which everything is built, and every\nrelease it\u2019s exciting to see the improvements that find their way into these types.\n\n[values. This is particularly neat because it\u2019s based on some relatively recent research from](https://lemire.me/blog/2021/02/22/parsing-floating-point-numbers-really-fast-in-c)\n\n[@lemire](https://github.com/lemire) and [@CarlVerret](https://github.com/CarlVerret), who used C#\nwith .NET 5 to implement a very fast implementation for parsing floating-point numbers, and that\nimplementation how now found its way into .NET 7!\n\n93 CHAPTER 9 | Primitive Types and Numerics\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|ParseAll|.NET 6.0|26.84 ms|1.00|\n|ParseAll|.NET 7.0|12.63 ms|0.47|\n\nof `TryFormat` writing out \u201cTrue\u201d by doing:\n\nwhich requires four writes, it can instead implement the same operation in a single write by doing:\n\nThat `0x65007500720054` is the numerical value of the four characters in memory as a single `ulong` . You\ncan see the impact of these changes with a microbenchmark:\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|ParseTrue|.NET 6.0|7.347 ns|1.00|\n|ParseTrue|.NET 7.0|2.327 ns|0.32|\n|||||\n|FormatTrue|.NET 6.0|3.030 ns|1.00|\n|FormatTrue|.NET 7.0|1.997 ns|0.66|\n\nvalues defined on the enum. This cache includes the string name and the value for every defined\nenumeration in the `Enum` . It\u2019s also sorted by value in an array, so when one of these operations is\nperformed, the code uses `Array.BinarySearch` to find the index of the relevant entry. The issue with\nthat is one of overheads. When it comes to algorithmic complexity, a binary search is faster than a\nlinear search; after all, a binary search is `O(log N)` whereas a linear search is `O(N)` . However, there\u2019s\nalso less overhead for every step of the algorithm in a linear search, and so for smaller values of `N`, it\n[can be much faster to simply do the simple thing. That\u2019s what dotnet/runtime#57973 does for enums.](https://github.com/dotnet/runtime/pull/57973)\nFor enums with less than or equal to 32 defined values, the implementation now just does a linear\nsearch via the internal `SpanHelpers.IndexOf` (the worker routine behind `IndexOf` on spans, strings,\n\n94 CHAPTER 9 | Primitive Types and Numerics\n\nand arrays), and for enums with more than that, it does a `SpanHelpers.BinarySearch` (which is the\nimplementation for `Array.BinarySearch` ).\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|AllDefined|.NET 6.0|159.28 ns|1.00|\n|AllDefined|.NET 7.0|94.86 ns|0.60|\n\nimplementation choosing from a multitude of different internal implementations, for example a\n\nplay nicely with enums. The results highlight just how much unnecessary overhead there was\npreviously.\n\n95 CHAPTER 9 | Primitive Types and Numerics\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|FindEnum|.NET 6.0|421.608 ns|1.00|\n|FindEnum|.NET 7.0|5.466 ns|0.01|\n\nfour 32-bit values and performed 4 `int` comparisons. With this change, if the current hardware has\n128-bit SIMD support, the implementation loads the data from the two guids as two vectors and\nsimply does a single comparison.\n\n|Method|Runtime|Mean|Ratio|Code Size|\n|---|---|---|---|---|\n|GuidEquals|.NET 6.0|2.119 ns|1.00|90 B|\n|GuidEquals|.NET 7.0|1.354 ns|0.64|78 B|\n\nof the bits store a ticks offset from 1/1/0001 12:00am and where each tick is 100 nanoseconds, and\n\ndifference in such tiny operations, but you can see it simply from the number of instructions involved,\nwhere on .NET 6 this produces:\n\nand on .NET 7 this produces:\n\n96 CHAPTER 9 | Primitive Types and Numerics\n\nso instead of a `mov`, `and`, `and`, and `cmp`, we get just an `xor` and a `shl` .\n\nOther operations on `DateTime` [also become more efficient, thanks to dotnet/runtime#72712 from](https://github.com/dotnet/runtime/pull/72712)\n\n[[@SergeiPavlov](https://github.com/SergeiPavlov) and dotnet/runtime#73277](https://github.com/dotnet/runtime/pull/73277) from\n\n[@SergeiPavlov](https://github.com/SergeiPavlov). In another case of .NET benefiting from recent\nadvancements in research, these PRs implemented the algorithm from Neri and Schneider\u2019s\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Day|.NET 6.0|5.2080 ns|1.00|\n|Day|.NET 7.0|2.0549 ns|0.39|\n|||||\n|Month|.NET 6.0|4.1186 ns|1.00|\n|Month|.NET 7.0|2.0945 ns|0.51|\n|||||\n|Year|.NET 6.0|3.1422 ns|1.00|\n|Year|.NET 7.0|0.8200 ns|0.26|\n|||||\n|TryFormat|.NET 6.0|27.6259 ns|1.00|\n|TryFormat|.NET 7.0|25.9848 ns|0.94|\n\nSo, we\u2019ve touched on improvements to a few types, but the pi\u00e8ce de r\u00e9sistance around primitive types\nin this release is \u201cgeneric math,\u201d which impacts almost every primitive type in .NET. There are\nsignificant improvements here, some which have been in the making for literally over a decade.\n\n[There\u2019s an excellent blog post from June dedicated just to generic math, so I won\u2019t go into much](https://devblogs.microsoft.com/dotnet/dotnet-7-generic-math)\ndepth here. At a high level, however, there are now over 30 new interfaces that utilize the new C# 11\nstatic abstract interface methods functionality, exposing wide-ranging operations from exponentiation\nfunctions to trigonometric functions to standard numerical operators, all available via generics, such\nthat you can write one implementation that operates over these interfaces generically and have your\n\n97 CHAPTER 9 | Primitive Types and Numerics\n\ncode applied to any types that implement the interfaces\u2026 which all of the numerical types in .NET 7\ndo (including not just the primitives but also, for example, `BigInteger` and `Complex` ). A preview\nversion of this feature, including necessary runtime support, language syntax, C# compiler support,\ngeneric interfaces, and interface implementations all shipped in .NET 6 and C# 10, but it wasn\u2019t\nsupported for production use, and you had to download an experimental reference assembly in order\n[to get access. With dotnet/runtime#65731, all of this support moved into .NET 7 as supported](https://github.com/dotnet/runtime/pull/65731)\n[functionality. dotnet/runtime#66748, dotnet/runtime#67453,](https://github.com/dotnet/runtime/pull/66748) [dotnet/runtime#69391,](https://github.com/dotnet/runtime/pull/69391)\n[dotnet/runtime#69582,](https://github.com/dotnet/runtime/pull/69582) [dotnet/runtime#69756, and dotnet/runtime#71800 all updated the design](https://github.com/dotnet/runtime/pull/69756)\nand implementation based on feedback from usage in .NET 6 and .NET 7 previews as well as a proper\nAPI review with our API review team (a process every new API in .NET goes through before it\u2019s\n\n[dotnet/runtime#67939,](https://github.com/dotnet/runtime/pull/67939) [dotnet/runtime#73274, dotnet/runtime#71033, dotnet/runtime#71010,](https://github.com/dotnet/runtime/pull/73274)\n[dotnet/runtime#68251,](https://github.com/dotnet/runtime/pull/68251) [dotnet/runtime#68217, and dotnet/runtime#68094 all added large swaths of](https://github.com/dotnet/runtime/pull/68217)\nnew public surface area for various operations, all with highly-efficient managed implementations, in\n[many cases based on the open source AMD Math Library.](https://github.com/amd/aocl-libm-ose)\n\nWhile this support is all primarily intended for external consumers, the core libraries do consume\nsome of it internally. You can see how these APIs clean up consuming code even while maintaining\n\ndiffs tells the story on how much code was able to be deleted:\n\nAnother simple example comes from the new `System.Formats.Tar` library in .NET 7, which as the\n[name suggests is used for reading and writing archives in any of multiple tar file formats. The tar file](https://en.wikipedia.org/wiki/Tar_(computing)#File_format)\nformats include integer values in octal representation, so the `TarReader` class needs to parse octal\nvalues. Some of these values are 32-bit integers, and some are 64-bit integers. Rather than have two\n\n`INumber<T>` . The implementation is then entirely in terms of `T` and can be used for either of these\ntypes (plus any other types meeting the constraints, should that ever be needed). What\u2019s particularly\n\ninterface contains these methods:\n\n98 CHAPTER 9 | Primitive Types and Numerics\n\nand the compiler will pick the appropriate one based on the context.\n\nIn addition to all the existing types that get these interfaces, there are also new types.\n[dotnet/runtime#69204](https://github.com/dotnet/runtime/pull/69204) adds the new `Int128` and `UInt128` types. As these types implement all of the\nrelevant generic math interfaces, they come complete with a huge number of methods, over 100 each,\nall of which are implemented efficiently in managed code. In the future, the aim is that some set of\nthese will be optimized further by the JIT and to take advantage of hardware acceleration.\n\nSeveral PRs moved native implementations of these kinds of math operations to managed code.\n[dotnet/runtime#63881](https://github.com/dotnet/runtime/pull/63881) from [@am11](https://github.com/am11) did so for `Math.Abs` and `Math.AbsF`\n[(absolute value), and dotnet/runtime#56236 from](https://github.com/dotnet/runtime/pull/56236)\n\n[@alexcovington](https://github.com/alexcovington) did so for `Math.ILogB` and `MathF.ILogB` (base 2\ninteger logarithm). The latter\u2019s implementation is based on the MUSL libc implementation of the same\nalgorithm, and in addition to improving performance (in part by avoiding the transition between\nmanaged and native code, in part by the actual algorithm employed), it also enabled deleting two\ndistinct implementations from native code, one from the coreclr side and one from the mono side,\nwhich is always a nice win from a maintainability perspective.\n\n|Method|Runtime|arg|Mean|Ratio|\n|---|---|---|---|---|\n|ILogB|.NET 6.0|12345.6789|4.056 ns|1.00|\n|ILogB|.NET 7.0|12345.6789|1.059 ns|0.26|\n\nOther math operations were also improved in various ways. `Math{F}.Truncate` was improved in\n[dotnet/runtime#65014](https://github.com/dotnet/runtime/pull/65014) from [@MichalPetryka](https://github.com/MichalPetryka) by making it into a\nJIT intrinsic, such that on Arm64 the JIT could directly emit a `frintz` instruction.\n\n[dotnet/runtime#71567](https://github.com/dotnet/runtime/pull/71567) in order to enable better code generation in some generic math scenarios.\n\nwhile a larger algorithmic complexity than we\u2019d normally like, it has a low constant overhead and so is\nstill reasonable for reasonably-sized values. In contrast, an alternative algorithm is available that runs\nin `O(N * (log N)^2)` time, but with a much higher constant factor involved. That makes is so that it\u2019s\nreally only worth switching for really big numbers. Which is what this PR does. It implements the\nalternative algorithm and switches over to it when the input is at least 20,000 digits (so, yes, big). But\nfor such large numbers, it makes a significant difference.\n\n99 CHAPTER 9 | Primitive Types and Numerics\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Parse|.NET 6.0|3.474 s|1.00|\n|Parse|.NET 7.0|1.672 s|0.48|\n\nspans rather than arrays. That in turn enabled a fair amount of use of stack allocation and slicing to\navoid allocation overheads, while also improving reliability and safety by moving some code away\nfrom unsafe pointers to safe spans. The primary performance impact is visible in allocation numbers,\nand in particular for operations related to division.\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|ModPow|.NET 6.0|1.527 ms|1.00|706 B|1.00|\n|ModPow|.NET 7.0|1.589 ms|1.04|50 B|0.07|\n\n100 CHAPTER 9 | Primitive Types and Numerics\n\n**CHAPTER**"}, "10": {"Arrays, Strings, and Spans": "While there are many forms of computation that can consume resources in applications, some of the\nmost common include processing of data stored in arrays, strings, and now spans. Thus you see a\nfocus in every .NET release on removing as much overhead as possible from such scenarios, while also\nfinding ways to further optimize the concrete operations developers are commonly performing.\n\nLet\u2019s start with some new APIs that can help make writing more efficient code easier. When examining\nstring parsing/processing code, it\u2019s very common to see characters examined for their inclusion in\nvarious sets. For example, you might see a loop looking for characters that are ASCII digits:\n\nor that are ASCII letters:\n\nor other such groups. Interestingly, there\u2019s wide-spread variation in how such checks are coded, often\ndepending on how much effort a developer put in to optimizing them, or in some cases likely not\neven recognizing that some amount of performance was being left on the table. For example, that\nsame ASCII letter check could instead be written as:\n\nwhich while more \u201cintense\u201d is also much more concise and more efficient. It\u2019s taking advantage of a\nfew tricks. First, rather than having two comparisons to determine whether the character is greater\nthan or equal to the lower bound and less than or equal to the upper bound, it\u2019s doing a single\n\n101 CHAPTER 10 | Arrays, Strings, and Spans\n\nthan `'a'`, then `'c' - 'a'` will be negative, and casting it to `uint` will then cause it to wrap around to a\nmassive number, also larger than 25, again causing the comparison to fail. Thus, we\u2019re able to pay a\nsingle additional subtraction to avoid an entire additional comparison and branch, which is _almost_\n\nlowercase, such that for the low cost of a bit twiddle, we can achieve both the lowercase and\nuppercase range checks. Of course, those tricks aren\u2019t something we want every developer to have to\nknow and write on each use. Instead, .NET 7 exposes a bunch of new helpers on `System.Char` to\nencapsulate these common checks, done in an efficient manner. `char` already had methods like\n\n`IsDigit` and `IsLetter`, which provided the more comprehensive Unicode meaning of those monikers\n(e.g. there are ~320 Unicode characters categorized as \u201cdigits\u201d). Now in .NET 7, there are also these\nhelpers:\n\n['IsAsciiDigit', 'IsAsciiHexDigit', 'IsAsciiHexDigitLower', 'IsAsciiHexDigitUpper', 'IsAsciiLetter', 'IsAsciiLetterLower', 'IsAsciiLetterUpper', 'IsAsciiLetterOrDigit']\n\n[These methods were added by dotnet/runtime#69318, which also employed them in dozens of](https://github.com/dotnet/runtime/pull/69318)\n[locations where such checks were being performed across dotnet/runtime](https://github.com/dotnet/runtime) (many of them using lessefficient approaches).\n\nAnother new API focused on encapsulating a common pattern is the new\n\neach input span. This is useful when you want to know the first place that two inputs differ.\n[dotnet/runtime#68210](https://github.com/dotnet/runtime/pull/68210) from [@gfoidl](https://github.com/gfoidl) then utilized the new `Vector128`\nfunctionality to provide a basic vectorization of the implementation. As it\u2019s comparing two sequences\nand looking for the first place they differ, this implementation uses a neat trick, which is to have a\nsingle method implemented to compare the sequences as bytes. If the `T` being compared is bitwiseequatable and no custom equality comparer is supplied, then it reinterpret-casts the refs from the\nspans as `byte` refs, and uses the single shared implementation.\n\nYet another new set of APIs are the `IndexOfAnyExcept` and `LastIndexOfAnyExcept` methods,\n[introduced by dotnet/runtime#67941 and used in a variety of additional call sites by](https://github.com/dotnet/runtime/pull/67941)\n[dotnet/runtime#71146](https://github.com/dotnet/runtime/pull/71146) [and dotnet/runtime#71278. While somewhat of a mouthful, these methods](https://github.com/dotnet/runtime/pull/71278)\nare quite handy. They do what their name suggests: whereas `IndexOf(T value)` searches for the first\n\n102 CHAPTER 10 | Arrays, Strings, and Spans\n\nYou can now write that as:\n\n```\nbool allZero = array.AsSpan().IndexOfAnyExcept(0) < 0;\n\n```\n\n[dotnet/runtime#73488](https://github.com/dotnet/runtime/pull/73488) vectorizes this overload, as well.\n\n|Method|Mean|Ratio|\n|---|---|---|\n|OpenCoded|370.47 ns|1.00|\n|IndexOfAnyExcept|23.84 ns|0.06|\n\nOf course, while new \u201cindex of\u201d variations are helpful, we already have a bunch of such methods, and\nit\u2019s important that they are as efficient as possible. These core `IndexOf{Any}` methods are used in\nhuge numbers of places, many of which are performance-sensitive, and so every release they get\n[additional tender-loving care. While PRs like dotnet/runtime#67811](https://github.com/dotnet/runtime/pull/67811) got gains by paying very close\nattention to the assembly code being generated (in this case, tweaking some of the checks used on\nArm64 in `IndexOf` and `IndexOfAny` to achieve better utilization), the biggest improvements here come\nin places where either vectorization was added and none was previously employed, or where the\n\ndo the equivalent of repeatedly searching for the \u2018h\u2019, and when an \u2018h\u2019 was found, then performing a\n\n`SequenceEqual` to match the remainder. As you can imagine, however, it\u2019s very easy to run into cases\nwhere the first character being searched for is very common, such that you frequently have to break\nout of the vectorized loop in order to do the full string comparison. Instead, the PR implements an\n[algorithm based on SIMD-friendly algorithms for substring searching. Rather than just searching for](http://0x80.pl/articles/simd-strfind.html#algorithm-1-generic-simd)\nthe first character, it can instead vectorize a search for both the first and last character at appropriate\ndistances from each other. In our \u201chello\u201d example, in any given input, it\u2019s much more likely to find an\n\n103 CHAPTER 10 | Arrays, Strings, and Spans\n\n\u2018h\u2019 than it is to find an \u2018h\u2019 followed four characters later by an \u2018o\u2019, and thus this implementation is able\nto stay within the vectorized loop a lot longer, garnering many fewer false positives that force it down\nthe `SequenceEqual` route. The implementation also handles cases where the two characters selected\nare equal, in which case it\u2019ll quickly look for another character that\u2019s not equal in order to maximize\nthe efficiency of the search. We can see the impact of all of this with a couple of examples:\n\nThis is pulling down the text to \u201cThe Adventures of Sherlock Holmes\u201d from Project Gutenberg and\nthen benchmarking using `IndexOf` to count the occurrences of \u201cSherlock\u201d and \u201celementary\u201d in the\ntext. On my machine, I get results like this:\n\n|Method|Runtime|needle|Mean|Ratio|\n|---|---|---|---|---|\n|Count|.NET 6.0|Sherlock|43.68 us|1.00|\n|Count|.NET 7.0|Sherlock|48.33 us|1.11|\n||||||\n|Count|.NET 6.0|elementary|1,063.67 us|1.00|\n|Count|.NET 7.0|elementary|56.04 us|0.05|\n\nFor \u201cSherlock\u201d, the performance is actually a bit worse in .NET 7 than in .NET 6; not much, but a\nmeasurable 10%. That\u2019s because there are very few capital `'S'` characters in the source text, 841 to be\nexact, out of 593,836 characters in the document. At only 0.1% density of the starting character, the\nnew algorithm doesn\u2019t bring much benefit, as the existing algorithm that searched for the first\ncharacter alone captures pretty much all of the possible vectorization gains to be had, and we do pay\n\n10% of the source. In that case, .NET 7 is 20x faster than .NET 6, taking 53us on .NET 7 to count all the\n\n`'e'` \u2019s vs 1084us on .NET 6. In this case, the new scheme yields immense gains, by vectorizing a search\nfor both the `'e'` and a `'y'` at the specific distance away, a combination that is much, much less\nfrequent. This is one of those situations where overall there are on average huge observed gains even\nthough we can see small regressions for some specific inputs.\n\n104 CHAPTER 10 | Arrays, Strings, and Spans\n\n`StringComparison.OrdinalIgnoreCase)` . Previously, this operation was implemented with a fairly\ntypical substring search, walking the input string and at every location doing an inner loop to\ncompare the target string, except performing a `ToUpper` on every character in order to do it in a caseinsensitive manner. Now with this PR, which is based on approaches previously used by `Regex`, if the\ntarget string begins with an ASCII character, the implementation can use `IndexOf` (if the character isn\u2019t\nan ASCII letter) or `IndexOfAny` (if the character is an ASCII letter) to quickly jump ahead to the first\npossible location of a match. Let\u2019s take the exact same benchmark as we just looked at, but tweaked\nto use `OrdinalIgnoreCase` :\n\nHere, both words are about 4x faster on .NET 7 than they were on .NET 6:\n\n|Method|Runtime|needle|Mean|Ratio|\n|---|---|---|---|---|\n|Count|.NET 6.0|Sherlock|2,113.1 us|1.00|\n|Count|.NET 7.0|Sherlock|467.3 us|0.22|\n||||||\n|Count|.NET 6.0|elementary|2,325.6 us|1.00|\n|Count|.NET 7.0|elementary|638.8 us|0.27|\n\nas we\u2019re now doing a vectorized `IndexOfAny('S', 's')` or `IndexOfAny('E', 'e')` rather than\n\nleftover elements at the end of vectorized operation: process one last vector\u2019s worth of data, even if it\nmeans duplicating some work already done. This particularly helps for smaller inputs where the\nprocessing time might otherwise be dominated by the serial handling of those leftovers.\n\n105 CHAPTER 10 | Arrays, Strings, and Spans\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Contains|.NET 6.0|15.115 ns|1.00|\n|Contains|.NET 7.0|2.557 ns|0.17|\n\ntypes, but this PR extends it as well to four and eight-byte sized primitives. As with most of the other\nvectorized implementations, it checks whether the `T` is bitwise-equatable, which is important for the\nvectorization as it\u2019s only looking at the bits in memory and not paying attention to any `Equals`\nimplementation that might be defined on the type. In practice today, that means this is limited to just\na handful of types of which the runtime has intimate knowledge ( `Boolean`, `Byte`, `SByte`, `UInt16`, `Int16`,\n\n`Char`, `UInt32`, `Int32`, `UInt64`, `Int64`, `UIntPtr`, `IntPtr`, `Rune`, and enums), but in theory it could be\nextended in the future.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|IndexOf|.NET 6.0|252.17 ns|1.00|\n|IndexOf|.NET 7.0|78.82 ns|0.31|\n\nadded to provide extension methods for spans and friends, including such\n\n[map,\u201d essentially a Bloom filter. It creates a 256-bit table, and quickly sets bits in that table based on](https://en.wikipedia.org/wiki/Bloom_filter)\nthe values being searched for (essentially hashing them, but with a trivial hash function). Then it\niterates through the input, and rather than checking every input character against every one of the\ntarget values, it instead first looks up the input character in the table. If the corresponding bit isn\u2019t set,\nit knows the input character doesn\u2019t match any of the target values. If the corresponding bit is set,\nthen it proceeds to compare the input character against each of the target values, with a high\n\n106 CHAPTER 10 | Arrays, Strings, and Spans\n\n[dotnet/runtime#63817, all of these are now unified, such that both](https://github.com/dotnet/runtime/pull/63817) `string` and `MemoryExtensions`\nget the best of what the other had.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|LastIndexOfAny|.NET 6.0|443.29 ns|1.00|\n|LastIndexOfAny|.NET 7.0|31.79 ns|0.07|\n|||||\n|CountLines|.NET 6.0|1,689.66 ns|1.00|\n|CountLines|.NET 7.0|1,461.64 ns|0.86|\n\nThat same PR also cleans up uses of the `IndexOf` family, and in particular around uses that are\nchecking for containment rather than the actual index of a result. The `IndexOf` family of methods\nreturn a non-negative value when an element is found, and otherwise return -1. That means when\n\ngenerated for comparisons against 0 is ever so slightly more efficient than comparisons generated\nagainst -1, and this isn\u2019t something the JIT can itself substitute without the `IndexOf` methods being\nintrinsics such that the JIT can understand the semantics of the return value. Thus, for consistency and\na small perf gain, all relevant call sites were switched to compare against 0 instead of against -1.\n\n107 CHAPTER 10 | Arrays, Strings, and Spans\n\nSpeaking of call sites, one of the great things about having highly optimized `IndexOf` methods is\nusing them in all the places that can benefit, removing the maintenance impact of open-coded\n[replacements while also reaping the perf wins. dotnet/runtime#63913](https://github.com/dotnet/runtime/pull/63913) used `IndexOf` inside of\n\n`StringBuilder.Replace` to speed up the search for the next character to be replaced:\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Replace|.NET 6.0|1,563.69 ns|1.00|\n|Replace|.NET 7.0|70.84 ns|0.04|\n\nsubstantial throughput gains even with the allocation and copy that is inherent to the method\u2019s\ndesign:\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|ReadAllLines|.NET 6.0|947.8 ns|1.00|\n|ReadAllLines|.NET 7.0|385.7 ns|0.41|\n\n[And dotnet/runtime#70176 cleaned up a plethora of additional uses.](https://github.com/dotnet/runtime/pull/70176)\n\nFinally on the `IndexOf` front, as noted, a lot of time and energy over the years has gone into\noptimizing these methods. In previous releases, some of that energy has been in the form of using\nhardware intrinsics directly, e.g. having an SSE2 code path and an AVX2 code path and an AdvSimd\ncode path. Now that we have `Vector128<T>` and `Vector256<T>`, many such uses can be simplified\n(e.g. avoiding the duplication between an SSE2 implementation and an AdvSimd implementation)\nwhile still maintaining as good or even better performance and while automatically supporting\n[vectorization on other platforms with their own intrinsics, like WebAssembly. dotnet/runtime#73481,](https://github.com/dotnet/runtime/pull/73481)\n[dotnet/runtime#73556,](https://github.com/dotnet/runtime/pull/73556) [dotnet/runtime#73368, dotnet/runtime#73364, dotnet/runtime#73064, and](https://github.com/dotnet/runtime/pull/73368)\n[dotnet/runtime#73469](https://github.com/dotnet/runtime/pull/73469) all contributed here, in some cases incurring meaningful throughput gains:\n\n108 CHAPTER 10 | Arrays, Strings, and Spans\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|IndexOfAny|.NET 6.0|52.29 ns|1.00|\n|IndexOfAny|.NET 7.0|40.17 ns|0.77|\n\nrecognized by the JIT, which can now automatically unroll the comparison and compare more than\none char at a time, e.g. doing a single read of four chars as a `long` and a single comparison of that\n\n`long` against the expected combination of those four chars. The result is beautiful. Making it even\n\nemploys the same trick as part of this unrolling, so if you do that same\n\nand will OR in the appropriate mask on both the comparison constant and on the read data from the\ninput in order to perform the comparison in a case-insensitive manner.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|IsHttps_Ordinal|.NET 6.0|4.5634 ns|1.00|\n|IsHttps_Ordinal|.NET 7.0|0.4873 ns|0.11|\n|||||\n|IsHttps_OrdinalIgnoreCase|.NET 6.0|6.5654 ns|1.00|\n|IsHttps_OrdinalIgnoreCase|.NET 7.0|0.5577 ns|0.08|\n\nInterestingly, since .NET 5 the code generated by `RegexOptions.Compiled` would perform similar\nunrolling when comparing sequences of multiple characters, and when the source generator was\nadded in .NET 7, it also learned how to do this. However, the source generator has problems with such\nan optimization, due to endianness. The constants being compared against are subject to byte\nordering issues, such that the source generator would need to emit code that could handle running\non either little-endian or big-endian machines. The JIT has no such problem, as it\u2019s generating the\ncode on the same machine on which the code will execute (and in scenarios where it\u2019s being used to\ngenerate code ahead of time, the entirety of that code is already tied to a particular architecture). By\nmoving this optimization into the JIT, the corresponding code could be deleted from\n\n`RegexOptions.Compiled` and the regex source generator, which then also benefits from producing\n\n109 CHAPTER 10 | Arrays, Strings, and Spans\n\nregex being compiled.)\n\n`StartsWith` and `EndsWith` [have improved in other ways. dotnet/runtime#63734](https://github.com/dotnet/runtime/pull/63734) (improved further by\n[dotnet/runtime#64530) added another really interesting JIT-based optimization, but to understand it,](https://github.com/dotnet/runtime/pull/64530)\nwe need to understand `string` \u2019s internal layout. `string` is essentially represented in memory as an\n\n`_firstChar` indeed lines up with the first character of the string, or the null terminator if the string is\nempty. Internally in System.Private.CoreLib, and in particular in methods on `string` itself, code will\n\nstring\u2019s length generally needn\u2019t be consulted. Now, consider a method like `public bool`\n\n`StartsWith(char value)` on `string` . In .NET 6, the implementation was:\n\n```\nreturn Length != 0 && _firstChar == value;\n\n```\n\nwhich given what I just described makes sense: if the `Length` is 0, then the string doesn\u2019t begin with\n\nfine\u2026 unless the target character is itself `'\\0'`, in which case we could get false positives on the result.\nNow to this PR. The PR introduces an internal JIT intrinsinc `RuntimeHelpers.IsKnownConstant`, which\nthe JIT will substitute with `true` if the containing method is inlined and the argument passed to\n\n`IsKnownConstant` is then seen to be a constant. In such cases, the implementation can rely on other\nJIT optimizations kicking in and optimizing various code in the method, effectively enabling a\ndeveloper to write two different implementations, one when the argument is known to be a constant\nand one when not. With that in hand, the PR is able to optimize `StartsWith` as follows:\n\nIf the `value` parameter isn\u2019t a constant, then `IsKnownConstant` will be substituted with `false`, the\nentire starting `if` block will be eliminated, and the method will be left exactly was it was before. But, if\nthis method gets inlined and the `value` was actually a constant, then the `value != '\\0'` condition will\nalso be evaluatable at JIT-compile-time. If the `value` is in fact `'\\0'`, well, again that whole `if` block will\nbe eliminated and we\u2019re no worse off. But in the common case where the `value` isn\u2019t null, the entire\nmethod will end up being compiled as if it were:\n\n```\nreturn _firstChar == ConstantValue;\n\n```\n\n110 CHAPTER 10 | Arrays, Strings, and Spans\n\nand we\u2019ve saved ourselves a read of the string\u2019s length, a comparison, and a branch.\n[dotnet/runtime#69038](https://github.com/dotnet/runtime/pull/69038) then employs a similar technique for `EndsWith` .\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|StartsWith|.NET 6.0|8.130 ns|1.00|\n|StartsWith|.NET 7.0|1.653 ns|0.20|\n\nexplicitly specify the enum value as a constant, which then allows the JIT to specialize the code\ngeneration for the method to the specific mode being used; that in turn, for example, enables a\n\n`Math.Round(..., MidpointRounding.AwayFromZero)` call on Arm64 to be lowered to a single `frinta`\ninstruction.)\n\nhelper method was used to implement this method, taking advantage of one that is sufficient for the\nneeds of this method and that has lower overheads.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|EndsWith|.NET 6.0|10.861 ns|1.00|\n|EndsWith|.NET 7.0|5.385 ns|0.50|\n\nbut here for `SequenceEqual` and `SequenceCompareTo`, respectively.\n\n111 CHAPTER 10 | Arrays, Strings, and Spans\n\nAnother method that\u2019s seem some attention in .NET 7 is `MemoryExtensions.Reverse` (and\n\n`Array.Reverse` as it shares the same implementation), which performs an in-place reversal of the\n[target span. dotnet/runtime#64412 from [@alexcovington](https://github.com/alexcovington)](https://github.com/dotnet/runtime/pull/64412)\nprovides a vectorized implementation via direct use of AVX2 and SSSE3 hardware intrinsics, with\n[dotnet/runtime#72780](https://github.com/dotnet/runtime/pull/72780) from [@SwapnilGaikwad](https://github.com/SwapnilGaikwad) following up to\nadd an AdvSimd intrinsics implementation for Arm64. (There was an unintended regression\n[introduced by the original vectorization change, but that was fixed by dotnet/runtime#70650.)](https://github.com/dotnet/runtime/pull/70650)\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Reverse|.NET 6.0|21.352 ns|1.00|\n|Reverse|.NET 7.0|9.536 ns|0.45|\n\n`String.Split` [also saw vectorization improvements in dotnet/runtime#64899](https://github.com/dotnet/runtime/pull/64899) from\n\n[@yesmey](https://github.com/yesmey). As with some of the previously discussed PRs, it switched the\nexisting usage of SSE2 and SSSE3 hardware intrinsics over to the new `Vector128<T>` helpers, which\nimproved upon the existing implementation while also implicitly adding vectorization support for\nArm64.\n\nConverting various formats of strings is something many applications and services do, whether that\u2019s\nconverting from UTF8 bytes to and from `string` or formatting and parsing hex values. Such\n[operations have also improved in a variety of ways in .NET 7. Base64-encoding, for example, is a way](https://en.wikipedia.org/wiki/Base64)\nof representing arbitrary binary data (think `byte[]` ) across mediums that only support text, encoding\nbytes into one of 64 different ASCII characters. Multiple APIs in .NET implement this encoding. For\n\n[but they were further improved in .NET 7 via dotnet/runtime#70654 from](https://github.com/dotnet/runtime/pull/70654)\n\n[@a74nh](https://github.com/a74nh), which converted the SSSE3-based implementation to use\n\n`Vector128<T>` (which in turn implicitly enabled vectorization on Arm64). However, for converting\n\nThen, then they \u201cwiden\u201d those bytes into chars (remember, Base64-encoded data is a set of ASCII\nchars, so going from these bytes to chars entails adding just a `0` byte onto each element). That\nwidening can itself easily be done in a vectorized manner. The other interesting thing about this\nlayering is it doesn\u2019t actually require separate intermediate storage for the encoded bytes. The\nimplementation can perfectly compute the number of resulting characters for encoding X bytes into Y\n\n112 CHAPTER 10 | Arrays, Strings, and Spans\n\nBase64 characters (there\u2019s a formula), and the implementation can either allocate that final space\n(e.g. in the case of ToBase64CharArray) or ensure the provided space is sufficient (e.g. in the case of\nTryToBase64Chars). And since we know the initial encoding will require exactly half as many bytes, we\ncan encode into that same space (with the destination span reinterpreted as a `byte` span rather than\n\n`char` span), and then widen \u201cin place\u201d: walk from the end of the bytes and the end of the char space,\ncopying the bytes into the destination.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|TryToBase64Chars|.NET 6.0|623.25 ns|1.00|\n|TryToBase64Chars|.NET 7.0|81.82 ns|0.13|\n\nJust as widening can be used to go from bytes to chars, narrowing can be used to go from chars to\nbytes, in particular if the chars are actually ASCII and thus have a 0 upper byte. Such narrowing can be\n\n[fast-path utilized SSE2 and thus didn\u2019t apply to Arm64; thanks to dotnet/runtime#70080 from](https://github.com/dotnet/runtime/pull/70080)\n\n[@SwapnilGaikwad](https://github.com/SwapnilGaikwad), that path was changed over to be based on\nthe cross-platform `Vector128<T>`, enabling the same level of optimization across supported\n[platforms. Similarly, dotnet/runtime#71637 from](https://github.com/dotnet/runtime/pull/71637)\n\n[@SwapnilGaikwad](https://github.com/SwapnilGaikwad) adds Arm64 vectorization to the\n\nautomatically providing an Arm64 implementation.)\n\ncalls.\n\n113 CHAPTER 10 | Arrays, Strings, and Spans\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|GetMaxByteCount|.NET 6.0|1.7442 ns|1.00|\n|GetMaxByteCount|.NET 7.0|0.4746 ns|0.27|\n\nArguably the biggest improvement around UTF8 in .NET 7 is the new C# 11 support for UTF8 literals.\n[Initially implemented in the C# compiler in dotnet/roslyn#58991, with follow-on work in](https://github.com/dotnet/roslyn/pull/58991)\n[dotnet/roslyn#59390,](https://github.com/dotnet/roslyn/pull/59390) [dotnet/roslyn#61532, and dotnet/roslyn#62044, UTF8 literals enables the](https://github.com/dotnet/roslyn/pull/61532)\ncompiler to perform the UTF8 encoding into bytes at compile-time. Rather than writing a normal\n\n```\npublic static ReadOnlySpan<byte> Text => \"hello\"u8;\n\n```\n\nthe C# compiler will compile that equivalent to if you wrote:\n\nIn other words, the compiler is doing the equivalent of `Encoding.UTF8.GetBytes` at compile-time and\nhardcoding the resulting bytes, saving the cost of performing that encoding at run-time. Of course, at\nfirst glance, that array allocation might look terribly inefficient. However, looks can be deceiving, and\nare in this case. For several releases now, when the C# compiler sees a `byte[]` (or `sbyte[]` or `bool[]` )\nbeing initialized with a constant length and constant values and immediately cast to or used to\nconstruct a `ReadOnlySpan<byte>`, it optimizes away the `byte[]` allocation. Instead, it blits the data for\nthat span into the assembly\u2019s data section, and then constructs a span that points directly to that data\nin the loaded assembly. This is the actual generated IL for the above property:\n\nThis means we not only save on the encoding costs at run-time, and we not only avoid whatever\nmanaged allocations might be required to store the resulting data, we also benefit from the JIT being\nable to see information about the encoded data, like it\u2019s length, enabling knock-on optimizations. You\ncan see this clearly by examining the assembly generated for a method like:\n\n```\npublic static int M() => Text.Length;\n\n```\n\nfor which the JIT produces:\n\n114 CHAPTER 10 | Arrays, Strings, and Spans\n\nThe JIT inlines the property access, sees that the span is being constructed with a length of `5`, and so\nrather than emitting any array allocations or span constructions or anything even resembling that, it\nsimply outputs `mov eax, 5` to return the known length of the span.\n\n[Thanks primarily to dotnet/runtime#70568, dotnet/runtime#69995,](https://github.com/dotnet/runtime/pull/70568) [dotnet/runtime#70894,](https://github.com/dotnet/runtime/pull/70894)\n[dotnet/runtime#71417](https://github.com/dotnet/runtime/pull/71417) [from [@am11](https://github.com/am11), dotnet/runtime#71292,](https://github.com/dotnet/runtime/pull/71292)\n[dotnet/runtime#70513, and dotnet/runtime#71992,](https://github.com/dotnet/runtime/pull/70513) `u8` is now used more than 2100 times throughout\n[dotnet/runtime. Hardly a fair comparison, but the following benchmark demonstrates how little work](https://github.com/dotnet/runtime)\nis actually being performed for `u8` at execution time:\n\n|Method|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|\n|WithEncoding|17.3347 ns|1.000|32 B|1.00|\n|Withu8|0.0060 ns|0.000|-|0.00|\n\nLike I said, not fair, but it proves the point :)\n\n`Encoding` is of course just one mechanism for creating `string` instances. Others have also improved in\n.NET 7. Take the super common `long.ToString`, for example. Previous releases improved\n\n`int.ToString`, but there were enough differences between the 32-bit and 64-bit algorithms that `long`\n[didn\u2019t see all of the same gains. Now thanks to dotnet/runtime#68795, the 64-bit formatting code](https://github.com/dotnet/runtime/pull/68795)\npaths are made much more similar to the 32-bit, resulting in faster performance.\n\nthe next interpolation hole that needs to be filled in, and if the non-hole-character to hole ratio is high\n(e.g. long format string with few holes), it can be way faster than before.\n\n115 CHAPTER 10 | Arrays, Strings, and Spans\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|AppendFormat|.NET 6.0|338.23 ns|1.00|\n|AppendFormat|.NET 7.0|49.15 ns|0.15|\n\nthings. The first was to remove pinning as part of formatting operations. As an example,\n\n`int count)` method was essentially implemented as:\n\nThat `fixed` statement translates into a \u201cpinning pointer.\u201d Normally the GC is free to move managed\nobjects around on the heap, which it might do in order to compact the heap (to, for example, avoid\nsmall, unusuable fragments of memory between objects). But if the GC can move objects around, a\nnormal native pointer into that memory would be terribly unsafe and unreliable, as without notice the\ndata being pointed to could move and your pointer could now be pointing to garbage or to some\nother object that was shifted to this location. There are two ways for dealing with this. The first is a\n\u201cmanaged pointer,\u201d otherwise known as a \u201creference\u201d or \u201cref,\u201d as that\u2019s exactly what you get when you\nhave the \u201cref\u201d keyword in C#; it\u2019s a pointer that the runtime will update with the correct value when it\nmoves the object being pointed into. The second is to prevent the pointed-to object from being\nmoved, \u201cpinning\u201d it in place. And that\u2019s what the \u201cfixed\u201d keyword does, pinning the referenced object\nfor the duration of the `fixed` block, during which time it\u2019s safe to use the supplied pointer. Thankfully,\npinning is cheap when no GC occurs; when a GC does occur, however, pinned objects aren\u2019t able to be\nmoved around, and thus pinning can have a global impact on the performance of the application (and\non GCs themselves). There are also various optimizations inhibited by pinning. With all of the advents\nin C# around being able to use `ref` in many more places (e.g. ref locals, ref returns, and now in C# 11,\nref fields), and with all of the new APIs in .NET for manipulating refs (e.g. `Unsafe.Add`,\n\n`Unsafe.AreSame` ), it\u2019s now possible to rewrite code that was using pinning pointers to instead use\nmanaged pointers, thereby avoiding the problems that come from pinning. Which is what this PR did.\n\nnow akin to\n\n```\nAppend( ref Unsafe.Add( ref value.GetRawStringData(), startIndex), count);\n\n```\n\nbounds checking and the like can continue to do so, but now also does so without pinning all of the\ninputs.\n\n116 CHAPTER 10 | Arrays, Strings, and Spans\n\nput in place to optimize for this input and specifically for the case where there\u2019s already enough room\n\nhelper, such that it not only helps out `string` but any other type that also calls into the same helper.\nThe effects of this are visible in a simple microbenchmark:\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|AppendSpan|.NET 6.0|35.98 ns|1.00|\n|AppendSpan|.NET 7.0|17.59 ns|0.49|\n\nOne of the great things about improving things low in the stack is they have a multiplicative effect;\nthey not only help improve the performance of user code that directly relies on the improved\nfunctionality, they can also help improve the performance of other code in the core libraries, which\nthen further helps dependent apps and services. You can see this, for example, with\n\n`DateTimeOffset.ToString`, which depends on `StringBuilder` :\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|DateTimeOffsetToString|.NET 6.0|340.4 ns|1.00|\n|DateTimeOffsetToString|.NET 7.0|289.4 ns|0.85|\n\nif there\u2019s not enough room remaining do they then take the old path as a fallback. `Insert` wasn\u2019t\nimproved in this way at the time, because it can\u2019t just format into the space at the end of the builder;\nthe insert location could be anywhere in the builder. This PR addresses that by formatting into some\ntemporary stack space, and then delegating to the existing internal ref-based helper from the\n\n117 CHAPTER 10 | Arrays, Strings, and Spans\n\npreviously discussed PR to insert the resulting characters at the right location (it also falls back to\n\n`ToString` when there\u2019s not enough stack space for the `ISpanFormattable.TryFormat`, but that only\nhappens in incredibly corner cases, like a floating-point value that formats to hundreds of digits).\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Insert|.NET 6.0|30.02 ns|1.00|32 B|1.00|\n|Insert|.NET 7.0|25.53 ns|0.85|-|0.00|\n\npossible implementation for that operation (and if we ever found a better way, that method would be\nimproved according). So we can just use that:\n\nNote that I\u2019ve expressed that concatenation via an interpolated string, but the C# compiler will \u201clower\u201d\nthis interpolated string to a call to `string.Concat`, so the IL for this is indistinguishable from if I\u2019d\ninstead written:\n\n118 CHAPTER 10 | Arrays, Strings, and Spans\n\nAs an aside, the expanded `string.Concat` version highlights that this method could have been\nwritten to result in a bit less IL if it were instead written as:\n\nbut this doesn\u2019t meaningfully affect performance and here clarity and maintainability was more\nimportant than shaving off a few bytes.\n\n|Method|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|\n|WithStringBuilder|68.34 ns|1.00|272 B|1.00|\n|WithConcat|20.78 ns|0.31|64 B|0.24|\n\nThere are also places where `StringBuilder` was still applicable, but it was being used on hot-enough\npaths that previous releases of .NET saw the `StringBuilder` instance being cached. Several of the\ncore libraries, including System.Private.CoreLib, have an internal `StringBuilderCache` type which\ncaches a `StringBuilder` instance in a `[ThreadStatic]`, meaning every thread could end up having\nsuch an instance. There are several issues with this, including that the buffers employed by\n\ncached; attempts to cache ones longer than that result in them being thrown away. It\u2019d be better\ninstead to use cached arrays that aren\u2019t length-limited and that everyone has access to for sharing.\nMany of the core .NET libraries have an internal `ValueStringBuilder` type for this purpose, a `ref`\n\n119 CHAPTER 10 | Arrays, Strings, and Spans\n\nIn the same vein of not doing unnecessary work, there\u2019s a fairly common pattern that shows up with\nmethods like `string.Substring` and `span.Slice` :\n\n```\nspan = span.Slice(offset, str.Length - offset);\n\n```\n\nThe relevant thing to recognize here is these methods have overloads that take just the starting offset.\nSince the length being specified is the remainder after the specified offset, the call could instead be\nsimplified to:\n\n```\nspan = span.Slice(offset);\n\n```\n\nwhich is not only more readable and maintainable, it has some small efficiency benefits, e.g. on 64-bit\n\n[code maintenance and for performance to simplify these calls, which dotnet/runtime#68937 does for](https://github.com/dotnet/runtime/pull/68937)\n[all found occurrences of that pattern. This is then made more impactful by dotnet/runtime#73882,](https://github.com/dotnet/runtime/pull/73882)\nwhich streamlines `string.Substring` to remove unnecessary overheads, e.g. it condenses four\nargument validation checks down to a single fast-path comparison (in 64-bit processes).\n\ndiscussed how they\u2019re essentially managed pointers, i.e. pointers that the runtime can update at any\ntime due to the object it references getting moved on the heap. These references can point to the\nbeginning of an object, or they can point somewhere inside the object, in which case they\u2019re referred\nto as \u201cinterior pointers.\u201d `ref` has existed in C# since 1.0, but at that time it was primarily about passing\nby reference to method calls, e.g.\n\nLater versions of C# added the ability to have local `ref` s, e.g.\n\nand even to have `ref` returns, e.g.\n\n120 CHAPTER 10 | Arrays, Strings, and Spans\n\nThese facilities are more advanced, but they\u2019re used liberally throughout higher-performance code\nbases, and many of the optimizations in .NET in recent years are possible in large part due to these\n\n`ref` -related capabilities.\n\n`Span<T>` and `ReadOnlySpan<T>` themselves are heavily-based on `ref` s. For example, the indexer on\nmany older collection types is implemented as a get/set property, e.g.\n\nBut not span. `Span<T>` \u2019s indexer looks more like this:\n\nNote there\u2019s only a getter and no setter; that\u2019s because it returns a `ref T` to the actual storage\nlocation. It\u2019s a writable ref, so you can assign to it, e.g. you can write:\n\n```\nspan[i] = value;\n\n```\n\nbut rather than that being equivalent to calling some setter:\n\n```\nspan.set_Item(i, value);\n\n```\n\nit\u2019s actually equivalent to using the getter to retrieve the `ref` and then writing a value through that\n\n`ref`, e.g.\n\nThat\u2019s all well and good, but what\u2019s that `_reference` in the getter definition? Well, `Span<T>` is really\njust a tuple of two fields: a reference (to the start of the memory being referred to) and a length (how\nmany elements from that reference are included in the span). In the past, the runtime had to hack this\n\ndefined as follows:\n\n121 CHAPTER 10 | Arrays, Strings, and Spans\n\nThe rollout of `ref` [fields throughout dotnet/runtime](https://github.com/dotnet/runtime) [was done in dotnet/runtime#71498, following the](https://github.com/dotnet/runtime/pull/71498)\n[C# language gaining this support primarily in dotnet/roslyn#62155, which itself was the culmination](https://github.com/dotnet/roslyn/pull/62155)\nof many PRs first into a feature branch. `ref` fields alone doesn\u2019t itself automatically improve\nperformance, but it does simplify code significantly, and it allows for both new custom code that uses\n\n`ref` fields as well as new APIs that take advantage of them, both of which can help with performance\n(and specifically performance without sacrificing potential safety). One such example of a new API is\nnew constructors on `ReadOnlySpan<T>` and `Span<T>` :\n\n[added in dotnet/runtime#67447 (and then made public and used more broadly in](https://github.com/dotnet/runtime/pull/67447)\n\nequivalent to calling those methods with a length of 1. The answer is: safety.\n\nImagine if you could willy-nilly call this constructor. You\u2019d be able to write code like this:\n\nAt this point the caller of this method is handed a span that refers to garbage; that\u2019s bad in code\nthat\u2019s intended to be safe. You can already accomplish the same thing by using pointers:\n\nbut at that point you\u2019ve taken on the risk of using unsafe code and pointers and any resulting\nproblems are on you. With C# 11, if you now try to write the above code using the `ref` -based\nconstructor, you\u2019ll be greeted with an error like this:\n\n122 CHAPTER 10 | Arrays, Strings, and Spans\n\nof the method, which is bad. Hence how this relates to `ref` fields: because `ref` fields are now a thing,\n\nAs is often the case, addressing one issue kicks the can down the road a bit and exposes another. The\n\n`ref structs` ), but what if we don\u2019t want that? What if we want to be able to say \u201cthis `ref` is not\nstorable and should not escape the calling scope\u201d? From a caller\u2019s perspective, we want the compiler\nto allow passing in such `ref` s without it complaining about potential extension of lifetime, and from a\ncallee\u2019s perspective, we want the compiler to prevent the method from doing what it\u2019s not supposed\nto do. Enter `scoped` . The new C# keyword does exactly what we just wished for: put it on a `ref` or `ref`\n\n`struct` parameter, and the compiler both will guarantee (short of using unsafe code) that the method\ncan\u2019t stash away the argument and will then enable the caller to write code that relies on that\nguarantee. For example, consider this program:\n\nWe have a `ref struct SpanWriter` that takes a `Span<char>` to its constructor and allows for writing\nto it by copying in additional content and then updating the stored length. The `Write` method accepts\n\nStraightforward. Except, this doesn\u2019t compile:\n\n123 CHAPTER 10 | Arrays, Strings, and Spans\n\n```\npublic void Write(scoped ReadOnlySpan<char> value)\n\n```\n\nIf `Write` were then to try to store `value`, the compiler would balk:\n\nBut as it\u2019s not trying to do so, everything now compiles successfully. You can see examples of how this\n[is utilized in the aforementioned dotnet/runtime#71589.](https://github.com/dotnet/runtime/pull/71589)\n\nThere\u2019s also the other direction: there are some things that are implicitly `scoped`, like the `this`\nreference on a struct. Consider this code:\n\nThis produces a compiler error:\n\n```\nerror CS8170: Struct members cannot return 'this' or other instance members by reference\n\n```\n\nEffectively, that\u2019s because `this` is implicitly `scoped` (even though that keyword wasn\u2019t previously\navailable). What if we want to enable such an item to be returned? Enter `[UnscopedRef]` . This is rare\nenough in need that it doesn\u2019t get its own C# language keyword, but the C# compiler does recognize\nthe new `[UnscopedRef]` attribute. It can be put onto relevant parameters but also onto methods and\nproperties, in which case it applies to the `this` reference for that member. As such, we can modify our\nprevious code example to be:\n\nand now the code will compile successfully. Of course, this also places demands on callers of this\nmethod. For a call site, the compiler sees the `[UnscopedRef]` on the member being invoked, and then\nknows that the returned `ref` might reference something from that struct, and thus assigns to the\nreturned `ref` the same lifetime as that struct. So, if that struct were a local living on the stack, the `ref`\nwould also be limited to that same method.\n\n124 CHAPTER 10 | Arrays, Strings, and Spans\n\nimplementation for producing high-quality hash codes. In its current incarnation, it incorporates a\nrandom process-wide seed and is an implementation of the xxHash32 non-cryptographic hash\nalgorithm. In a previous release, `HashCode` saw the addition of an `AddBytes` methods, which accepts a\n\nxxHash32 algorithm works by accumulating 4 32-bit unsigned integers and then combining them\ntogether into the hash code; thus if you call `HashCode.Add(int)`, the first three times you call it you\u2019re\njust storing the values separately into the instance, and then the fourth time you call it all of those\nvalues are combined into the hash code (and there\u2019s a separate process that incorporates any\nremaining values if the number of 32-bit values added wasn\u2019t an exact multiple of 4). Thus, previously\n\nto deal with the possibility that previous calls to `Add` may have left some state queued, which means\n(with the current implementation at least), if there are multiple pieces of state to include in the hash\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|AddBytes|.NET 6.0|159.11 ns|1.00|\n|AddBytes|.NET 7.0|42.11 ns|0.26|\n\n[Another span-related change, dotnet/runtime#72727](https://github.com/dotnet/runtime/pull/72727) refactored a bunch of code paths to eliminate\nsome cached arrays. Why avoid cached arrays? After all, isn\u2019t it desirable to cache an array once and\nreuse it over and over again? It is, if that\u2019s the best option, but sometimes there are better options. For\nexample, one of the changes took code like:\n\nand replaced it with code like:\n\n```\nint index = value.AsSpan().IndexOfAny(@\":\\/?#\");\n\n```\n\n125 CHAPTER 10 | Arrays, Strings, and Spans\n\nThis has a variety of benefits. There\u2019s the usability benefit of keeping the tokens being searched close\nto the use site, and the usability benefit of the list being immutable such that some code somewhere\nwon\u2019t accidentally replace a value in the array. But there are also performance benefits. We don\u2019t need\nan extra field to store the array. We don\u2019t need to allocate the array as part of this type\u2019s static\nconstructor. And loading/using the string is slightly faster.\n\n|Method|Mean|Ratio|\n|---|---|---|\n|WithArray|8.601 ns|1.00|\n|WithString|6.949 ns|0.81|\n\nAnother example from that PR took code along the lines of:\n\nand replaced it with code like:\n\nIn this case, not only have we avoided the `char[]`, but if the text did require any trimming of\nwhitespaces, the new version (which trims a span instead of the original string) will save an allocation\nfor the trimmed string. This is taking advantage of the new C# 11 feature that supports switching on\n\n`ReadOnlySpan<char>` s just as you can switch on `string` [s, added in dotnet/roslyn#44388 from](https://github.com/dotnet/roslyn/pull/44388)\n\n[[@YairHalberstadt](https://github.com/YairHalberstadt). dotnet/runtime#68831](https://github.com/dotnet/runtime/pull/68831) also took advantage\nof this in several additional places.\n\nOf course, in some cases the arrays are entirely unnecessary. In that same PR, there were several cases\nlike this:\n\n126 CHAPTER 10 | Arrays, Strings, and Spans\n\nBy switching to use spans, again, we can instead write it like this:\n\n`MemoryExtensions.IndexOfAny` has a dedicated overload for two and three arguments, at which\npoint we don\u2019t need the array at all (these overloads also happen to be faster; when passing an array\nof two chars, the implementation would extract the two chars from the array and pass them off to the\nsame two-argument implementation). Multiple other PRs similarly removed array allocations.\n\n[Finally, dotnet/runtime#59670 from [@NewellClark](https://github.com/NewellClark) got rid of even](https://github.com/dotnet/runtime/pull/59670)\n\n[As I discussed in the .NET 6 post, this avoids even the one-time array allocation you\u2019d get for a cached](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6)\narray, results in much more efficient access, and supplies to the JIT compiler more information that\nenables it to more heavily optimize\u2026 goodness all around. This PR removed even more arrays in this\n[manner, as did dotnet/runtime#60411, dotnet/runtime#72743,](https://github.com/dotnet/runtime/pull/60411) [dotnet/runtime#73115 from](https://github.com/dotnet/runtime/pull/73115)\n\n[[@vcsjones](https://github.com/vcsjones), and dotnet/runtime#70665.](https://github.com/dotnet/runtime/pull/70665)\n\n127 CHAPTER 10 | Arrays, Strings, and Spans\n\n**CHAPTER**"}, "11": {"Regex": "[Back in May, I shared a fairly detailed post about the improvements coming to Regular Expressions in](https://devblogs.microsoft.com/dotnet/regular-expression-improvements-in-dotnet-7)\n[.NET 7. As a recap, prior to .NET 5,](https://devblogs.microsoft.com/dotnet/regular-expression-improvements-in-dotnet-7) `Regex` \u2019s implementation had largely been untouched for quite\nsome time. In .NET 5, we brought it back up to be on par with or better than multiple other industry\nimplementations from a performance perspective. .NET 7 takes some significant leaps forward from\nthat. If you haven\u2019t read the post yet, please go ahead and do so now; I\u2019ll wait\u2026\n\nWelcome back. With that context, I\u2019ll avoid duplicating content here, and instead focus on how exactly\nthese improvements came about and the PRs that did so.\n\n**RegexOptions.NonBacktracking**\n\nprocessing of `Regex` over to using a new engine based in finite automata. It has two primary modes of\nexecution, one that relies on DFAs (deterministic finite automata) and one that relies on NFAs (nondeterministic finite automata). Both implementations provide a very valuable guarantee: processing\ntime is linear in the length of the input. Whereas a backtracking engine (which is what `Regex` uses if\n\n`NonBacktracking` isn\u2019t specified) can hit a situation known as \u201ccatastrophic backtracking,\u201d where\nproblematic expressions combined with problematic input can result in exponential processing in the\nlength of the input, `NonBacktracking` guarantees it\u2019ll only ever do an ammortized-constant amount\nof work per character in the input. In the case of a DFA, that constant is very small. With an NFA, that\nconstant can be much larger, based on the complexity of the pattern, but for any given pattern the\nwork is still linear in the length of the input.\n\nA significant number of years of development went into the `NonBacktracking` implementation, which\n[was initially added into dotnet/runtime in dotnet/runtime#60607. However, the original research and](https://github.com/dotnet/runtime)\nimplementation for it actually came from Microsoft Research (MSR), and was available as an\nexperimental package in the form of the Symbolic Regex Matcher (SRM) library published by MSR.\nYou can still see vestiges of this in the current code now in .NET 7, but it\u2019s evolved significantly, in\ntight collaboration between developers on the .NET team and the researchers at MSR (prior to being\n[integrated in dotnet/runtime, it was incubated for over a year in dotnet/runtimelab, where the original](https://github.com/dotnet/runtime)\n[SRM code was brought in via dotnet/runtimelab#588](https://github.com/dotnet/runtimelab/pull/588) from [@veanes](https://github.com/veanes)).\n\nThis implementation is based on the notion of regular expression derivatives, a concept that\u2019s been\naround for decades (the term was originally coined in a paper by Janusz Brzozowski in the 1960s) and\nwhich has been significantly advanced for this implementation. Regex derivatives form the basis for\nhow the automata (think \u201cgraph\u201d) used to process input are constructed. The idea at its core is fairly\nsimple: take a regex and process a single character\u2026 what is the new regex you get to describe what\nremains after processing that one character? That\u2019s the derivative. For example, given the regex `\\w{3}`\n\n128 CHAPTER 11 | Regex\n\nto match three word characters, if you apply this to the next input character \u2018a\u2019, well, that will strip off\n\nderivative would be nothing, which we\u2019ll express here as an empty character class, giving us\n\n`.*(the|he)|he|[]` . Of course, as part of an alternation, that \u201cnothing\u201d at the end is a nop, and so we\ncan simplify the whole derivative to just `.*(the|he)|he` \u2026 done. That was all when applying the\n\nor `h` ), and so we just end up with that same subexpression. But against the right side of the\nalternation, `e` matches `e`, leaving us with the empty string `()` : `.*(the|he)|()` . At the point where a\npattern is \u201cnullable\u201d (it can match the empty string), that can be considered a match. We can visualize\nthis whole thing as a graph, with transitions for every input character to the derivative that comes\nfrom applying it.\n\n129 CHAPTER 11 | Regex\n\n130 CHAPTER 11 | Regex\n\nLooks an awful lot like a DFA, doesn\u2019t it? It should. And that\u2019s exactly how `NonBacktracking`\nconstructs the DFAs it uses to process input. For every regex construct (concatenations, alternations,\nloops, etc.) the engine knows how to derive the next regex based on the character being evaluated.\nThis application is done lazily, so we have an initial starting state (the original pattern), and then when\nwe evaluate the next character in the input, it looks to see whether there\u2019s already a derivative\navailable for that transition: if there is, it follows it, and if there isn\u2019t, it dynamically/lazily derives the\nnext node in the graph. At its core, that\u2019s how it works.\n\nOf course, the devil is in the details and there\u2019s a ton of complication and engineering smarts that go\ninto making the engine efficient. One such example is a tradeoff between memory consumption and\nthroughput. Given the ability to have any `char` as input, you could have effectively ~65K transitions\nout of every node (e.g. every node could need a ~65K element table); that would significantly increase\nmemory consumption. However, if you actually had that many transitions, it\u2019s very likely a significant\nmajority of them would point to the same target node. Thus, `NonBacktracking` maintains its own\ngroupings of characters into what it calls \u201cminterms.\u201d If two characters will have exactly the same\ntransition, they\u2019re part of the same minterm. The transitions are then constructed in terms of\nminterms, with at most one transition per minterm out of a given node. When the next input character\nis read, it maps that to a minterm ID, and then finds the appropriate transition for that ID; one\nadditional level of indirection in order to save a potentially huge amount of memory. That mapping is\n[handled via an array bitmap for ASCII and an efficient data structure known as a Binary Decision](https://en.wikipedia.org/wiki/Binary_decision_diagram)\n[Diagram (BDD)](https://en.wikipedia.org/wiki/Binary_decision_diagram) for everything above 0x7F.\n\nAs noted, the non-backtracking engine is linear in the length of the input. But that doesn\u2019t mean it\nalways looks at each input character exactly once. If you call `Regex.IsMatch`, it does; after all, `IsMatch`\nonly needs to determine whether there is a match and doesn\u2019t need to compute any additional\ninformation, such as where the match actual starts or ends, any information on captures, etc. Thus, the\nengine can simply employ its automata to walk along the input, transitioning from node to node in\nthe graph until it comes to a final state or runs out of input. Other operations, however, do require it\nto gather more information. `Regex.Match` needs to compute everything, and that can actually entail\nmultiple walks over the input. In the initial implementation, the equivalent of `Match` would always take\nthree passes: match forwards to find the end of _a_ match, then match a reversed-copy of the pattern in\nreverse from that ending location in order to find where the match actually starts, and then once more\nwalk forwards from that known starting position to find the actual ending position. However, with\n[dotnet/runtime#68199](https://github.com/dotnet/runtime/pull/68199) from [@olsaarik](https://github.com/olsaarik), unless captures are required, it\ncan now be done in only two passes: once forward to find the guaranteed ending location of the\n[match, and then once in reverse to find its starting location. And dotnet/runtime#65129 from](https://github.com/dotnet/runtime/pull/65129)\n\n[@olsaarik](https://github.com/olsaarik) added captures support, which the original implementation\nalso didn\u2019t have. This captures support adds back a third pass, such that once the bounds of the\nmatch are known, the engine runs the forward pass one more time, but this time with an NFA-based\n\u201csimulation\u201d that is able to record \u201ccapture effects\u201d on transitions. All of this enables the nonbacktracking implementation to have the exact same semantics as the backtracking engines, always\nproducing the same matches in the same order with the same capture information. The only\ndifference in this regard is, whereas with the backtracking engines capture groups inside of loops will\nstore all values captured in every iteration of the loop, only the last iteration is stored with the nonbacktracking implementation. On top of that, there are a few constructs the non-backtracking\n\n131 CHAPTER 11 | Regex\n\nimplementation simply doesn\u2019t support, such that attempting to use any of those will fail when trying\nto construct the `Regex`, e.g. backreferences and lookarounds.\n\nEven after its progress as a standalone library from MSR, more than 100 PRs went into making\n\n`RegexOptions.NonBacktracking` what it is now in .NET 7, including optimizations like\n[dotnet/runtime#70217](https://github.com/dotnet/runtime/pull/70217) from [@olsaarik](https://github.com/olsaarik) that tries to streamline the tight\ninner matching loop at the heart of the DFA (e.g. read the next input character, find the appropriate\ntransition to take, move to the next node, and check information about the node like whether it\u2019s a\n[final state) and optimizations like dotnet/runtime#65637](https://github.com/dotnet/runtime/pull/65637) from [@veanes](https://github.com/veanes)\nthat optimized the NFA mode to avoid superfluous allocations, caching and reusing list and set\nobjects to make the handling of the lists of states ammortized allocation-free.\n\nThere\u2019s one more set of PRs of performance interest for `NonBacktracking` . The `Regex` implementation\nfor taking patterns and turning them into something processable, regardless of which of the multiple\nengines is being used, is essentially a compiler, and as with many compilers, it naturally lends itself to\nrecursive algorithms. In the case of `Regex`, those algorithms involve walking around trees of regular\nexpression constructs. Recursion ends up being a very handy way of expressing these algorithms, but\nrecursion also suffers from the possibility of stack overflow; essentially it\u2019s using stack space as scratch\nspace, and if it ends up using too much, things go badly. One common approach to dealing with this\nis turning the recursive algorithm into an iterative one, which typically involves using an explicit stack\nof state rather than the implicit one. The nice thing about this is the amount of state you can store is\nlimited only by how much memory you have, as opposed to being limited by your thread\u2019s stack\nspace. The downsides, however, are that it\u2019s typically much less natural to write the algorithms in this\nmanner, and it typically requires allocating heap space for the stack, which then leads to additional\ncomplications if you want to avoid that allocation, such as various kinds of pooling.\n[dotnet/runtime#60385](https://github.com/dotnet/runtime/pull/60385) introduces a different approach for `Regex`, which is then used by\n\nthe recursive algorithm as well as being able to use stack space and thus avoid additional allocation in\nthe most common cases, but then to avoid stack overflows, it issues explicit checks to ensure we\u2019re\nnot too deep on the stack (.NET has long provided the helpers\n\non the stack, it forks off continued execution into another thread. Hitting this condition is expensive,\nbut it\u2019s very rarely if ever actually hit in practice (e.g. the only time it\u2019s hit in our vast functional tests\nare in the tests explicitly written to stress it), it keeps the code simple, and it keeps the typical cases\n[fast. A similar approach is used in other areas of dotnet/runtime, such as in System.Linq.Expressions.](https://github.com/dotnet/runtime)\n\nAs was mentioned in my previous blog post about regular expressions, both the backtracking\nimplementations and the non-backtracking implementation have their place. The main benefit of the\nnon-backtracking implementation is predictability: because of the linear processing guarantee, once\nyou\u2019ve constructed the regex, you don\u2019t need to worry about malicious inputs causing worst-case\nbehavior in the processing of your potentially susceptible expressions. This doesn\u2019t mean\n\n`RegexOptions.NonBacktracking` is always the fastest; in fact, it\u2019s frequently not. In exchange for\nreduced best-case performance, it provides the best worst-case performance, and for some kinds of\napplications, that\u2019s a really worthwhile and valuable tradeoff.\n\n132 CHAPTER 11 | Regex\n\n**New APIs**\n\n`Regex` gets several new methods in .NET 7, all of which enable improved performance. The simplicity\nof the new APIs likely also misrepresents how much work was necessary to enable them, in particular\nbecause the new APIs all support `ReadOnlySpan<char>` inputs into the regex engines.\n\nrelied on in .NET Framework ( `CompileToAssembly` is now obsoleted and has never been functional in\n.NET Core). One subtly that relies on the nature of `string` as the input is how match information is\n\nchallenge to support spans, but the problem is even more deeply rooted. All of the regex engines rely\n\nfunctionally correct, but would have completely defeated the purpose of accepting spans, and worse,\nwould have been so unexpected as to likely cause consuming apps to be worse performing than they\nwould have without the APIs. Instead, we needed a new approach and new APIs.\n\nFirst, we made `FindFirstChar` and `Go` virtual instead of abstract. The design that splits these methods\nis largely antiquated, and in particular the forced separation between a stage of processing where you\nfind the next possible location of a match and then a stage where you actually perform the match at\nthat location doesn\u2019t align well with all engines, like the one used by `NonBacktracking` (which initially\nimplemented `FindFirstChar` as a nop and had all its logic in `Go` ). Then we added a new virtual `Scan`\nmethod which, importantly, takes a `ReadOnlySpan<char>` as a parameter; the span can\u2019t be exposed\nfrom the base `RegexRunner` and must be passed in. We then implemented `FindFirstChar` and `Go` in\nterms of `Scan`, and made them \u201cjust work.\u201d Then, all of the engines are implemented in terms of that\n\n133 CHAPTER 11 | Regex\n\nspan; they no longer need to access the protected `RegexRunner.runtext`, `RegexRunner.runtextbeg`,\nand `RegexRunner.runtextend` members that surface the input; they\u2019re just handed the span, already\nsliced to the input region, and process that. One of the neat things about this from a performance\nperspective is it enables the JIT to do a better job at shaving off various overheads, in particular\naround bounds checking. When the logic is implemented in terms of `string`, in addition to the input\nstring itself the engine is also handed the beginning and end of the region of the input to process\n(since the developer could have called a method like `Regex.Match(string input, int beginning,`\n\n`int length)` in order to only process a substring). Obviously the engine matching logic is way more\ncomplicated than this, but simplifying, imagine the entirety of the engine was just a loop over the\ninput. With the input, beginning, and length, that would look like:\n\nThat will result in the JIT generating assembly code along the lines of this:\n\nIn contrast, if we\u2019re dealing with a span, which already factors in the bounds, then we can write a more\ncanonical loop like this:\n\n134 CHAPTER 11 | Regex\n\nAnd when it comes to compilers, something in a canonical form is really good, because the more\ncommon the shape of the code, the more likely it is to be heavily optimized:\n\nSo even without all the other benefits that come from operating in terms of span, we immediately get\nlow-level code generation benefits from performing all the logic in terms of spans. While the above\nexample was made up (obviously the matching logic does more than a simple for loop), here\u2019s a real\n\ncurrent position is a word character and whether the character before it is a word character (factoring\n\nand here\u2019s what the span version looks like:\n\n135 CHAPTER 11 | Regex\n\nAnd here\u2019s the resulting assembly:\n\n136 CHAPTER 11 | Regex\n\nThe most interesting thing to notice here is the:\n\nat the end of the first version that doesn\u2019t exist at the end of the second. As we saw earlier, this is\nwhat the generated assembly looks like when the JIT is emitting the code to throw an index out of\nrange exception for an array, string, or span. It\u2019s at the end because it\u2019s considered to be \u201ccold,\u201d rarely\nexecuted. It exists in the first because the JIT can\u2019t prove based on local analysis of that function that\n\nspan accesses are always in bound. As such, it doesn\u2019t need to emit any bounds checks in the method,\nand the method then lacks the tell-tale signature of the index out of range throw. You can see more\nexamples of taking advantage of spans now being at the heart of the all of the engines in\n\n137 CHAPTER 11 | Regex\n\nOk, so the engines are now able to be handed span inputs and process them, great, what can we do\nwith that? Well, `Regex.IsMatch` is easy: it\u2019s not encumbered by needing to perform multiple matches,\nand thus doesn\u2019t need to worry about how to store that input `ReadOnlySpan<char>` for the next\nmatch. Similarly, the new `Regex.Count`, which provides an optimized implementation for counting\n\nadditional information into the engines to let them know how much information they actually need to\ncompute. For example, I noted previously that `NonBacktracking` is fairly pay-for-play in how much\nwork it needs to do relative to what information it needs to gather. It\u2019s cheapest to just determine\nwhether there is a match, as it can do that in a single forward pass through the input. If it also needs\nto compute the actual starting and ending bounds, that requires another reverse pass through some\nof the input. And if it then also needs to compute capture information, that requires yet another\nforward pass based on an NFA (even if the other two were DFA-based). `Count` needs the bounds\ninformation, as it needs to know where to start looking for the next match, but it doesn\u2019t need the\ncapture information, since none of that capture information is handed back to the caller.\n\nSo, `IsMatch` and `Count` can work with spans. But we still don\u2019t have a method that lets you actually get\n\nBeing a `ref struct`, the enumerator is able to store a reference to the input span, and is thus able to\niterate through matches, which are represented by the `ValueMatch` ref struct. Notably, today\n\n`ValueMatch` doesn\u2019t provide capture information, which also enables it to partake in the optimizations\npreviously mentioned for `Count` . Even if you have an input `string`, `EnumerateMatches` is thus a way to\nhave ammortized allocation-free enumeration of all matches in the input. In .NET 7, though, there isn\u2019t\na way to have such allocation-free enumeration if you also need all the capture data. That\u2019s something\nwe\u2019ll investigate designing in the future if/as needed.\n\n**TryFindNextPossibleStartingPosition**\n\nAs noted earlier, the core of all of the engines is a `Scan(ReadOnlySpan<char>)` method that accepts\nthe input text to match, combines that with positional information from the base instance, and exits\n\n138 CHAPTER 11 | Regex\n\nwhen it either finds the location of the next match or exhausts the input without finding another. For\nthe backtracking engines, the implementation of that method is logically as follows:\n\nWe try to match the input at the current position, and if we\u2019re successful in doing so, that\u2019s it, we exit.\nIf the current position doesn\u2019t match, however, then if there\u2019s any input remaining we \u201cbump\u201d the\nposition and start the process over. In regex engine terminology, this is often referred to as a\n\u201cbumpalong loop.\u201d However, if we actually ran the full matching process at every input character, that\ncould be unnecessarily slow. For many patterns, there\u2019s something about the pattern that would\nenable us to be more thoughtful about where we perform full matches, quickly skipping past locations\nthat couldn\u2019t possibly match, and only spending our time and resources on locations that have a real\nchance of matching. To elevate that concept to a first-class one, the backtracking engines\u2019\n\u201cbumpalong loop\u201d is typically more like the following (I say \u201ctypically\u201d because in some cases the\ncompiled and source generated regexes are able to generate something even better).\n\nAs with `FindFirstChar` previously, that `TryFindNextPossibleStartingPosition` has the\nresponsibility of searching as quickly as possible for the next place to match (or determining that\nnothing else could possibly match, in which case it would return `false` and the loop would exit). As\n\nbe fast.\n\nIn .NET 6, the interpreter engine had effectively two ways of implementing\n\n`TryFindNextPossibleStartingPosition` : a Boyer-Moore substring search if the pattern began with a\nstring (potentially case-insensitive) of at least two characters, and a linear scan for a character class\nknown to be the set of all possible chars that could begin a match. For the latter case, the interpreter\nhad eight different implementations for matching, based on a combination of whether\n\n`RegexOptions.RightToLeft` was set or not, whether the character class required case-insensitive\ncomparison or not, and whether the character class contained only a single character or more than\none character. Some of these were more optimized than others, e.g. a left-to-right, case-sensitive,\nsingle-char search would use an `IndexOf(char)` to search for the next location, an optimization\nadded in .NET 5. However, every time this operation was performed, the engine would need to\n[recompute which case it would be. dotnet/runtime#60822 improved this, introducing an internal](https://github.com/dotnet/runtime/pull/60822)\n\n139 CHAPTER 11 | Regex\n\nand precomputing which strategy to use when the interpreter was constructed. This not only made\nthe interpreter\u2019s implementation at match time faster, it made it effectively free (in terms of runtime\noverhead at match time) to add additional strategies.\n\nbeing significantly better than Boyer-Moore in all but the most corner of corner cases. So this PR\nenables a new `IndexOf(ReadOnlySpan<char>)` strategy to be used to search for a prefix string in the\ncase where the string is case-sensitive.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Count|.NET 6.0|377.32 us|1.00|\n|Count|.NET 7.0|55.44 us|0.15|\n\n[dotnet/runtime#61490](https://github.com/dotnet/runtime/pull/61490) then removed Boyer-Moore entirely. This wasn\u2019t done in the previously\nmentioned PR because of lack of a good way to handle case-insensitive matches. However, this PR\nalso special-cased ASCII letters to teach the optimizer how to turn an ASCII case-insensitive match\ninto a set of both casings of that letter (excluding the few known to be a problem, like `i` and `k`, which\ncan both be impacted by the employed culture and which might map case-insensitively to more than\ntwo values). With enough of the common cases covered, rather than use Boyer-Moore to perform a\n\nimplementation handily in real-world cases. This PR goes further than that, such that it doesn\u2019t just\ndiscover the \u201cstarting set,\u201d but is able to find all of the character classes that could match a pattern a\nfixed-offset from the beginning; that then gives the analyzer the ability to choose the set that\u2019s\nexpected to be least common and issue a search for it instead of whatever happens to be at the\nbeginning. The PR goes even further, too, motivated in large part by the non-backtracking engine. The\nnon-backtracking engine\u2019s prototype implementation also used `IndexOfAny(char, char, ...)` when\nit arrived at a starting state and was thus able to quickly skip through input text that wouldn\u2019t have a\nchance of pushing it to the next state. We wanted all of the engines to share as much logic as\npossible, in particular around this speed ahead, and so this PR unified the interpreter with the nonbacktracking engine to have them share the exact same `TryFindNextPossibleStartingPosition`\nroutine (which the non-backtracking engine just calls at an appropriate place in its graph traversal\nloop). Since the non-backtracking engine was already using `IndexOfAny` in this manner, initially not\ndoing so popped as a significant regression on a variety of patterns we measure, and this caused us to\ninvest in using it everywhere. This PR also introduced the first special-casing for case-insensitive\ncomparisons into the compiled engine, e.g. if we found a set that was `[Ee]`, rather than emitting a\n\n140 CHAPTER 11 | Regex\n\ncheck akin to `c == 'E' || c == 'e'`, we\u2019d instead emit a check akin to `(c | 0x20) == 'e'` (those\nfun ASCII tricks discussed earlier coming into play again).\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Count|.NET 6.0|499.3 us|1.00|\n|Count|.NET 7.0|177.7 us|0.35|\n\nThe previous PR started turning `IgnoreCase` pattern text into sets, in particular for ASCII, e.g. `(?i)a`\nwould become `[Aa]` . That PR hacked in the support for ASCII knowing that something more complete\n[would be coming along, as it did in dotnet/runtime#67184. Rather than hardcoding the case-](https://github.com/dotnet/runtime/pull/67184)\ninsensitive sets that just the ASCII characters map to, this PR essentially hardcodes the sets for every\npossible char. Once that\u2019s done, we no longer need to know about case-insensitivity at match time\nand can instead just double-down on efficiently matching sets, which we already need to be able to\ndo well. Now, I said it encodes the sets for every possible char; that\u2019s not entirely true. If it were true,\nthat would take up a large amount of memory, and in fact, most of that memory would be wasted\nbecause the vast majority of characters don\u2019t participate in case conversion\u2026 there are only ~2,000\ncharacters that we need to handle. As such, the implementation employs a three-tier table scheme.\nThe first table has 64 elements, dividing the full range of `chars` into 64 groupings; of those 64 groups,\n54 of them have no characters that participate in case conversion, so if we hit one of those entries, we\ncan immediately stop the search. For the remaining 10 that do have at least one character in their\nrange participating, the character and the value from the first table are used to compute an index into\nthe second table; there, too, the majority of entries say that nothing participates in case conversion.\nIt\u2019s only if we get a legitimate hit in the second table does that give us an index into the third table, at\nwhich location we can find all of the characters considered case-equivalent with the first.\n\n[dotnet/runtime#63477](https://github.com/dotnet/runtime/pull/63477) [(and then later improved in dotnet/runtime#66572) proceeded to add another](https://github.com/dotnet/runtime/pull/66572)\n[searching strategy, this one inspired by nim-regex\u2019s literal optimizations. There are a multitude of](https://nitely.github.io/2020/11/30/regex-literals-optimization.html)\nregexes we track from a performance perspective to ensure we\u2019re not regressing in common cases\n[and to help guide investments. One is the set of patterns in mariomka/regex-benchmark languages](https://github.com/mariomka/regex-benchmark)\n\nenabled strategies for finding a next good location, as it\u2019s guaranteed to begin with a \u201cword\ncharacter\u201d ( `\\w` ), which includes ~50,000 of the ~65,000 possible characters; we don\u2019t have a good way\nof vectorizing a search for such a character class. However, this pattern is interesting in that it begins\nwith a loop, and not only that, it\u2019s an upper-unbounded loop which our analysis will determine is\natomic, because the character guaranteed to immediately follow the loop is a `':'`, which is itself not a\nword character, and thus there\u2019s nothing the loop could match and give up as part of backtracking\n\n141 CHAPTER 11 | Regex\n\nfind it, we can match backwards through as many `[\\w]` s as we can find; in this case, the only\nconstraint is we need to match at least one. This PR added that strategy, for a literal after an atomic\nloop, to all of the engines.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|IsMatch|.NET 6.0|4,291.77 us|1.000|\n|IsMatch|.NET 7.0|42.40 us|0.010|\n\nOf course, as has been talked about elsewhere, the best optimizations aren\u2019t ones that make\nsomething faster but rather ones that make something entirely unnecessary. That\u2019s what\n[dotnet/runtime#64177](https://github.com/dotnet/runtime/pull/64177) does, in particular in relation to anchors. The .NET regex implementation has\nlong had optimizations for patterns with a starting anchor: if the pattern begins with `^`, for example\n(and `RegexOptions.Multiline` wasn\u2019t specified), the pattern is rooted to the beginning, meaning it\ncan\u2019t possibly match at any position other than `0` ; as such, with such an anchor,\n\n`TryFindNextPossibleStartingPosition` won\u2019t do any searching at all. The key here, though, is being\nable to detect whether the pattern begins with such an anchor. In some cases, like `^abc$`, that\u2019s trivial.\n\nthat a pattern has an ending anchor like `$` . If the analysis engine can determine a maximum number of\ncharacters for any possible match, and it has such an anchor, then it can simply jump to that distance\nfrom the end of the string, and bypass even looking at anything before then.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|IsMatch|.NET 6.0|867,890.56 ns|1.000|\n|IsMatch|.NET 7.0|33.55 ns|0.000|\n\n[dotnet/runtime#67732](https://github.com/dotnet/runtime/pull/67732) is another PR related to improving anchor handling. It\u2019s always fun when a bug\nfix or code simplification refactoring turns into a performance improvement. The PR\u2019s primary purpose\nwas to simplify some complicated code that was computing the set of characters that could possibly\nstart a match. It turns out that complication was hiding a logic bug which manifested in it missing\nsome opportunities to report valid starting character classes, the impact of which is that some\nsearches which could have been vectorized weren\u2019t. By simplifying the implementation, the bug was\nfixed, exposing more performance opportunities.\n\n142 CHAPTER 11 | Regex\n\nBy this point, the engines are able to use `IndexOf(ReadOnlySpan<char>)` to find a substring at the\nbeginning of a pattern. But sometimes the most valuable substring isn\u2019t at the beginning, but\nsomewhere in the middle or even at the end. As long as it\u2019s at a fixed-offset from the beginning of the\npattern, we can search for it, and then just back-off by the offset to the position we should actually try\n[running the match. dotnet/runtime#67907 does exactly that.](https://github.com/dotnet/runtime/pull/67907)\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Count|.NET 6.0|444.2 us|1.00|\n|Count|.NET 7.0|122.6 us|0.28|\n\n**Loops and Backtracking**\n\nLoop handling in the compiled and source generated engines has been significantly improved, both\nwith respect to processing them faster and with respect to backtracking less.\n\nWith regular greedy loops (e.g. `c*` ), there are two directions to be concerned about: how quickly can\nwe consume all the elements that match the loop, and how quickly can we give back elements that\nmight be necessary as part of backtracking for the remainder of the expression to match. And with\nlazy loops, we\u2019re primarily concerned with backtracking, which is the forward direction (since lazy\nloops consume as part of backtracking rather than giving back as part of backtracking). With PRs\n[dotnet/runtime#63428,](https://github.com/dotnet/runtime/pull/63428) [dotnet/runtime#68400, dotnet/runtime#64254, and dotnet/runtime#73910, in](https://github.com/dotnet/runtime/pull/68400)\nboth the compiler and source generator we now make full use of effectively all of the variants of\n\nthe forward direction of that loop entails consuming every character until the next newline, which we\n\npossibly match the remainder of the pattern. Or for example, in a pattern like `[^a-c]*def`, the loop\nwill initially greedily consume everything other than `'a'`, `'b'`, or `'c'`, so we can use\n\n`IndexOfAnyExcept('a', 'b', 'c')` to find the initial end of the loop. And so on. This can yield huge\nperformance gains, and with the source generator, also makes the generated code more idiomatic\nand easier to understand.\n\n143 CHAPTER 11 | Regex\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Count|.NET 6.0|3,369.5 us|1.00|\n|Count|.NET 7.0|430.2 us|0.13|\n\n[Sometimes optimizations are well-intended but slightly miss the mark. dotnet/runtime#63398 fixes](https://github.com/dotnet/runtime/pull/63398)\nsuch an issue with an optimization introduced in .NET 5; the optimization was valuable but only for a\nsubset of the scenarios it was intended to cover. While `TryFindNextPossibleStartingPosition` \u2019s\nprimary raison d\u2019\u00eatre is to update the bumpalong position, it\u2019s also possible for\n\n`TryMatchAtCurrentPosition` to do so. One of the occasions in which it\u2019ll do so is when the pattern\nbegins with an upper-unbounded single-character greedy loop. Since processing starts with the loop\nhaving fully consumed everything it could possibly match, subsequent trips through the scan loop\ndon\u2019t need to reconsider any starting position within that loop; doing so would just be duplicating\nwork done in a previous iteration of the scan loop. And as such, `TryMatchAtCurrentPosition` can\nupdate the bumpalong position to the end of the loop. The optimization added in .NET 5 was dutifully\ndoing this, and it did so in a way that fully handled atomic loops. But with greedy loops, the updated\nposition was getting updated every time we backtracked, meaning it started going backwards, when it\nshould have remained at the end of the loop. This PR fixes that, yielding significant savings in the\nadditional covered cases.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Count|.NET 6.0|103,962.8 us|1.000|\n|Count|.NET 7.0|336.9 us|0.003|\n\nAs mentioned elsewhere, the best optimizations are those that make work entirely vanish rather than\n[just making work faster. dotnet/runtime#68989, dotnet/runtime#63299, and dotnet/runtime#63518](https://github.com/dotnet/runtime/pull/68989)\ndo exactly that by improving the pattern analyzers ability to find and eliminate more unnecessary\nbacktracking, a process the analyzer refers to as \u201cauto-atomicity\u201d (automatically making loops atomic).\n\ntry to match `\"ab\"` . It won\u2019t match so we\u2019ll backtrack into the lazy loop and try to match `\"aab\"` . And so\non, until we\u2019ve consumed all the `'a'` s such that the rest of the pattern has a chance of matching the\nrest of the input. That\u2019s exactly what an atomic greedy loop does, so we can transform the pattern\n\n`a*?b` into `(?>a*)b`, which is much more efficiently processed. In fact, we can see exactly how it\u2019s\nprocessed just by looking at the source-generated implementation of this pattern:\n\n144 CHAPTER 11 | Regex\n\n(Note that those comments aren\u2019t ones I added for this blog post; the source generator itself is\nemitting commented code.)\n\nWhen a regular expression is input, it\u2019s parsed into a tree-based form. The \u201cauto-atomicity\u201d analysis\ndiscussed in the previous PR is one form of analysis that walks around this tree looking for\nopportunities to transform portions of the tree into a behaviorally equivalent alternative that will be\nmore efficient to execute. Several PRs introduced additional such transformations.\n[dotnet/runtime#63695, for example, looks for \u201cempty\u201d and \u201cnothing\u201d nodes in the tree that can be](https://github.com/dotnet/runtime/pull/63695)\nremoved. An \u201cempty\u201d node is something that matches the empty string, so for example in the\nalternation `abc|def||ghi`, the third branch of that alternation is empty. A \u201cnothing\u201d node is\nsomething that can\u2019t match anything, so for example in the concatenation `abc(?!)def`, that `(?!)` in\nmiddle is a negative lookahead around an empty, which can\u2019t possibly match anything, as it\u2019s saying\nthe expression won\u2019t match if it\u2019s followed by an empty string, which everything is. These constructs\noften arise as a result of other transformations rather than being something a developer typically\nwrites by hand, just as there are optimizations in the JIT where you might look at them and say \u201cwhy\non earth is that something a developer would write\u201d but it ends up being a valuable optimization\nanyways because inlining might transform perfectly reasonable code into something that matches the\ntarget pattern. Thus, for example, if you did have `abc(?!)def`, since that concatenation requires the\n\n`(?!)` to match in order to be successful, the concatenation itself can simply be replaced by a\n\u201cnothing.\u201d You can see this easily if you try this with the source generator:\n\n```\n[GeneratedRegex(@\"abc(?!)def\")]\n\n```\n\n145 CHAPTER 11 | Regex\n\nas it will produce a `Scan` method like this (comment and all):\n\n[Another set of transformations was introduced in dotnet/runtime#59903, specifically around](https://github.com/dotnet/runtime/pull/59903)\nalternations (which beyond loops are the other source of backtracking). This introduced two main\noptimizations. First, it enables rewriting alternations into alternations of alternations, e.g. transforming\n\n`axy|axz|bxy|bxz` into `ax(?:y|z)|bx(?:y|z)`, which is then further reduced into `ax[yz]|bx[yz]` . This\ncan enable the backtracking engines to more efficiently process alternations due to fewer branches\nand thus less potential backtracking. The PR also enabled limited reordering of branches in an\nalternation. Generally branches can\u2019t be reordered, as the order can impact exactly what\u2019s matched\nand what\u2019s captured, but if the engine can prove there\u2019s no effect on ordering, then it\u2019s free to\nreorder. One key place that ordering isn\u2019t a factor is if the alternation is atomic due to it being\nwrapped in an atomic group (and the auto-atomicity analysis will add such groups implicitly in some\nsituations). Reordering the branches then enables other optimizations, like the one previously\nmentioned from this PR. And then once those optimizations have kicked in, if we\u2019re left with an atomic\nalternation where every branch begins with a different letter, than can enable further optimizations in\nterms of how the alternation is lowered; this PR teaches the source generator how to emit a `switch`\nstatement, which leads to both more efficient and more readable code. (The detection of whether\nnodes in the tree are atomic, and other such properties such as performing captures or introducing\n[backtracking, turned out to be valuable enough that dotnet/runtime#65734](https://github.com/dotnet/runtime/pull/65734) added dedicated support\nfor this.)\n\n**Code generation**\n\nThe .NET 7 regex implementation has no fewer than four engines: the interpreter (what you get if you\n\nengine don\u2019t require any kind of code generation; they\u2019re both based on creating in-memory data\nstructures that represent how to match input against the pattern. The other two, though, both\ngenerate code specific to the pattern; the generated code is code attempting to mimick what you\nmight write if you weren\u2019t using `Regex` at all and were instead writing code to perform a similar match\ndirectly. The source generator spits out C# that\u2019s compiled directly into your assembly, and the\ncompiler spits out IL at run-time via reflection emit. The fact that these are generating code specific to\nthe pattern means there\u2019s a ton of opportunity to optimize.\n\n[dotnet/runtime#59186](https://github.com/dotnet/runtime/pull/59186) provided the initial implementation of the source generator. This was a direct\nport of the compiler, effectively a line-by-line translation of IL into C#; the result is C# akin to what\n[you\u2019d get if you were to run the generated IL through a decompiler like ILSpy. A bunch of PRs then](https://github.com/icsharpcode/ILSpy)\nproceeded to iterate on and tweak the source generator, but the biggest improvements came from\nchanges that changed the compiler and the source generator together. Prior to .NET 5, the compiler\nspit out IL that was very similar to what the interpreter would do. The interpreter is handed a series of\ninstructions that it walks through one by one and interprets, and the compiler, handed that same\n\n146 CHAPTER 11 | Regex\n\nseries of instructions, would just emit the IL for processing each. It had some opportunity for being\nmore efficient, e.g. loop unrolling, but a lot of value was left on the table. In .NET 5, an alternate path\nwas added in support of patterns without backtracking; this code path was based on being handed\nthe parsed node tree rather than being based on the series of instructions, and that higher-level form\nenabled the compiler to derive more insights about the pattern that it could then use to generate\nmore efficient code. In .NET 7, support for all regex features were incrementally added in, over the\n[course of multiple PRs, in particular dotnet/runtime#60385](https://github.com/dotnet/runtime/pull/60385) for backtracking single char loops,\n[dotnet/runtime#61698](https://github.com/dotnet/runtime/pull/61698) [for backtracking single char lazy loops, dotnet/runtime#61784 for other](https://github.com/dotnet/runtime/pull/61784)\n[backtracking lazy loops, and dotnet/runtime#61906](https://github.com/dotnet/runtime/pull/61906) for other backtracking loops as well as back\nreferences and conditionals. At that point, the only features missing were support for\n\n`RegexOptions.RightToLeft` and lookbehinds (which are implemented in terms of right-to-left), and\nwe decided based on relatively little use of these features that we needn\u2019t keep around the old\n[compiler code just to enable them. So, dotnet/runtime#62318](https://github.com/dotnet/runtime/pull/62318) deleted the old implementation. But,\neven though these features are relatively rare, it\u2019s a lot easier to tell a story that \u201call patterns are\n[supported\u201d than one that requires special callouts and exceptions, so dotnet/runtime#66127 and](https://github.com/dotnet/runtime/pull/66127)\n[dotnet/runtime#66280](https://github.com/dotnet/runtime/pull/66280) added full lookbehind and `RightToLeft` support such that there were no\ntakebacks. At this point, both the compiler and source generator now supported everything the\ncompiler previously did, but now with the more modernized code generation. This code generation is\nin turn what enables many of the optimizations previously discussed, e.g. it provides the opportunity\nto use APIs like `LastIndexOf` as part of backtracking, which would have been near impossible with the\nprevious approach.\n\nOne of the great things about the source generator emitting idiomatic C# is it makes it easy to iterate.\nEvery time you put in a pattern and see what the generator emits, it\u2019s like being asked to do a code\nreview of someone else\u2019s code, and you very frequently see something \u201cnew\u201d worthy of comment, or\nin this case, improving the generator to address the issue. And so a bunch of PRs were originated\nbased on reviewing what the generator emitted and then tweaking the generator to do better (and\nsince the compiler was effectively entirely rewritten along with the source generator, they maintain the\nsame structure, and it\u2019s easy to port improvements from one to the other). For example,\n[dotnet/runtime#68846](https://github.com/dotnet/runtime/pull/68846) [and dotnet/runtime#69198](https://github.com/dotnet/runtime/pull/69198) tweaked how some comparisons were being\nperformed in order for them to convey enough information to the JIT that it can eliminate some\n[subsequent bounds checking, and dotnet/runtime#68490](https://github.com/dotnet/runtime/pull/68490) recognized a variety of conditions being\nemitted that could never happen in some situations observable statically and was able to elide all that\ncode gen. It also became obvious that some patterns didn\u2019t need the full expressivity of the scan loop,\nand a more compact and customized `Scan` [implementation could be used. dotnet/runtime#68560](https://github.com/dotnet/runtime/pull/68560)\ndoes that, such that, for example, a simple pattern like `hello` won\u2019t emit a loop at all and will instead\nhave a simpler `Scan` implementation like:\n\n147 CHAPTER 11 | Regex\n\nThe compiler and source generator were also updated to take advantage of newer features.\n[dotnet/runtime#63277, for example, teaches the source generator how to determine if](https://github.com/dotnet/runtime/pull/63277) `unsafe` code is\n\nzero\u2019ing being necessary. Then there\u2019s the issue of where the code is generated; we want helper\nfunctions (like the `\\w` `IsWordChar` [helper introduced in dotnet/runtime#62620) that can be shared](https://github.com/dotnet/runtime/pull/62620)\namongst multiple generated regexes, and we want to be able to share the exact same regex\nimplementation if the same pattern/options/timeout combination are used in multiple places in the\n[same assembly (dotnet/runtime#66747), but doing so then exposes this implementation detail to user](https://github.com/dotnet/runtime/pull/66747)\ncode in the same assembly. To still be able to get the perf benefits of such code sharing while\n\nOne last and interesting code generation aspect is in optimizations around character class matching.\nMatching character classes, whether ones explicitly written by the developer or ones implicitly created\nby the engine (e.g. as part of finding the set of all characters that can begin the expression), can be\none of the more time-consuming aspects of matching; if you imagine having to evaluate this logic for\nevery character in the input, then how many instructions needs to be executed as part of matching a\ncharacter class directly correlates to how long it takes to perform the overall match. We thus spend\nsome time trying to ensure we generate optimal matching code for as many categories of character\n[classes as possible. dotnet/runtime#67365, for example, improved a bunch of cases found to be](https://github.com/dotnet/runtime/pull/67365)\n\noptimizations around the handling of \u201cmatch anything\u201d can kick in.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Match|.NET 6.0|1,934,393.69 ns|1.000|\n|Match|.NET 7.0|91.80 ns|0.000|\n\nabout, in the generated output; for example this:\n\nnow produces this in the core matching logic emitted by the source generator:\n\n148 CHAPTER 11 | Regex\n\n[Other changes impacting character class code generation included dotnet/runtime#72328, which](https://github.com/dotnet/runtime/pull/72328)\nimproved the handling of character classes that involve character class subtraction;\n[dotnet/runtime#72317](https://github.com/dotnet/runtime/pull/72317) from [@teo-tsirpanis](https://github.com/teo-tsirpanis), which enabled\n[additional cases where the generator could avoid emitting a bitmap lookup; dotnet/runtime#67133,](https://github.com/dotnet/runtime/pull/67133)\nwhich added a tighter bounds check when it does emit such a lookup table; and\n[dotnet/runtime#61562, which enables better normalization of character classes in the engine\u2019s](https://github.com/dotnet/runtime/pull/61562)\ninternal representation, thus leading to downstream optimizations better recognizing more character\nclasses.\n\nFinally, with all of these improvements to `Regex`, a multitude of PRs fixed up regexes being used\n[across dotnet/runtime, in various ways. dotnet/runtime#66142,](https://github.com/dotnet/runtime) [dotnet/runtime#66179 from](https://github.com/dotnet/runtime/pull/66179)\n\nexpensive phases in the non-backtracking engine to compute exact bounds and capture information.\nThe PR also replaced some `Match` / `Match.MoveNext` usage with `EnumerateMatches`, in order to avoid\nneeding `Match` object allocations. The PR also entirely removed at least one regex usage that was just\n\nin a way as to try to make it as trimmmer friendly as possible. If you only ever do `new`\n\n`Regex(pattern)`, we\u2019d really like to be able to statically determine that the compiler and nonbacktracking implementations aren\u2019t needed such that the trimmer can remove it without having a\nvisible and meaningful negative impact. However, the trimmer analysis isn\u2019t yet sophisticated enough\nto see exactly which options are used and only keep the additional engines linked in if\n\noptions, we increase the chances that no code in the app is using this constructor, which would in turn\nenable this constructor, the compiler, and the non-backtracking implementation to be trimmed away.\n\n149 CHAPTER 11 | Regex\n\n**CHAPTER**"}, "12": {"Collections": "`System.Collections` hasn\u2019t seen as much investment in .NET 7 as it has in previous releases, though\nmany of the lower-level improvements have a trickle-up effect into collections as well. For example,\n\n`Dictionary<,>` \u2019s code hasn\u2019t changed between .NET 6 and .NET 7, but even so, this benchmark\nfocused on dictionary lookups:\n\nshows a measurable improvement in throughput between .NET 6 and .NET 7:\n\n|Method|Runtime|Mean|Ratio|Code Size|\n|---|---|---|---|---|\n|Sum|.NET 6.0|51.18 us|1.00|431 B|\n|Sum|.NET 7.0|43.44 us|0.85|413 B|\n\nthat array can\u2019t be mutated, but if there are mutable reference types stored in the array, those\ninstances themselves may still have their data mutated). As a result, `ImmutableArray<T>` also has an\nassociated \u201cbuilder\u201d type, which does support mutation: you create the builder, populate it, and then\ntransfer that contents to an `ImmutableArray<T>` which is frozen forevermore. In\n\n150 CHAPTER 12 | Collections\n\nallocation, while also speeding up the sort itself by removing several layers of indirection from every\ncomparison.\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Sort|.NET 6.0|86.28 us|1.00|\n|Sort|.NET 7.0|67.17 us|0.78|\n\nperspective because it means if you have your data in a span, you can get it into an\n\n`ImmutableArray<T>` without incurring additional allocations beyond the one the `ImmutableArray<T>`\n[itself will create. dotnet/runtime#66550 from [@RaymondHuy](https://github.com/RaymondHuy) also](https://github.com/dotnet/runtime/pull/66550)\nadds a bunch of new methods to the immutable collection builders, which provide efficient\nimplementations for operations like replacing elements and adding, inserting, and removing ranges.\n\n`SortedSet<T>` also saw some improvements in .NET 7. For example, `SortedSet<T>` internally uses a\n[red/black tree](https://en.wikipedia.org/wiki/Red%E2%80%93black_tree) as its internal data structure, and it uses a `Log2` operation to determine the maximum\ndepth the tree could be for a given node count. Previously, that operation was implemented as a loop.\n[But thanks to dotnet/runtime#58793 from [@teo-tsirpanis](https://github.com/teo-tsirpanis) that](https://github.com/dotnet/runtime/pull/58793)\nimplementation is now simply a call to `BitOperations.Log2`, which is in turn implemented trivially in\nterms of one of multiple hardware intrinsics if they\u2019re supported (e.g. `Lzcnt.LeadingZeroCount`,\n\nstreamlining how the iteration through the nodes in the tree is handled.\n\n151 CHAPTER 12 | Collections\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|SortedSetCopy|.NET 6.0|2.397 us|1.00|\n|SortedSetCopy|.NET 7.0|2.090 us|0.87|\n\n[One last PR to look at in collections: dotnet/runtime#67923.](https://github.com/dotnet/runtime/pull/67923) `ConditionalWeakTable<TKey, TValue>`\nis a collection most developers haven\u2019t used, but when you need it, you need it. It\u2019s used primarily for\ntwo purposes: to associate additional state with some object, and to maintain a weak collection of\nobjects. Essentially, it\u2019s a thread-safe dictionary that doesn\u2019t maintain strong references to anything it\nstores but ensures that the value associated with a key will remain rooted as long as the associated\nkey is rooted. It exposes many of the same APIs as `ConcurrentDictionary<,>`, but for adding items\nto the collection, it\u2019s historically only had an `Add` method. That means if the design of the consuming\ncode entailed trying to use the collection as a set, where duplicates were common, it would also be\ncommon to experience exceptions when trying to `Add` an item that already existed in the collection.\nNow in .NET 7, it has a `TryAdd` method, which enables such usage without potentially incurring the\ncosts of such exceptions (and without needing to add `try/catch` blocks to defend against them).\n\n152 CHAPTER 12 | Collections\n\n**CHAPTER**"}, "13": {"LINQ": "Let\u2019s move on to Language-Integrated Query (LINQ). LINQ is a productivity feature that practically\nevery .NET developer uses. It enables otherwise complicated operations to be trivially expressed,\nwhether via language-integrated query comprehension syntax or via direct use of methods on\n\n`System.Linq.Enumerable` . That productivity and expressivity, however, comes at a bit of an overhead\ncost. In the vast majority of situations, those costs (such as delegate and closure allocations, delegate\ninvocations, use of interface methods on arbitrary enumerables vs direct access to indexers and\n\n`Length` / `Count` properties, etc.) don\u2019t have a significant impact, but for really hot paths, they can and\ndo show up in a meaningful way. This leads some folks to declare LINQ as being broadly off-limits in\ntheir codebases. From my perspective, that\u2019s misguided; LINQ is extremely useful and has its place. In\n.NET itself, we use LINQ, we\u2019re just practical and thoughtful about where, avoiding it in code paths\nwe\u2019ve optimized to be lightweight and fast due to expectations that such code paths could matter to\nconsumers. And as such, while LINQ itself may not perform as fast as a hand-rolled solution, we still\ncare a lot about the performance of LINQ\u2019s implementation, so that it can be used in more and more\nplaces, and so that where it\u2019s used there\u2019s as little overhead as possible. There are also differences\nbetween operations in LINQ; with over 200 overloads providing various kinds of functionality, some of\nthese overloads benefit from more performance tuning than do others, based on their expected\nusage.\n\narrays, but still improved performance even for short arrays (because the implementation is now able\nto access the array directly rather than going through the enumerable, leading to less allocation and\ninterface dispatch and more applicable optimizations like inlining).\n\n153 CHAPTER 13 | LINQ\n\n|Method|Runtime|Length|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|---|\n|Min|.NET 6.0|4|26.167 ns|1.00|32 B|1.00|\n|Min|.NET 7.0|4|4.788 ns|0.18|-|0.00|\n||||||||\n|Max|.NET 6.0|4|25.236 ns|1.00|32 B|1.00|\n|Max|.NET 7.0|4|4.234 ns|0.17|-|0.00|\n||||||||\n|Min|.NET 6.0|1024|3,987.102 ns|1.00|32 B|1.00|\n|Min|.NET 7.0|1024|101.830 ns|0.03|-|0.00|\n||||||||\n|Max|.NET 6.0|1024|3,798.069 ns|1.00|32 B|1.00|\n|Max|.NET 7.0|1024|100.279 ns|0.03|-|0.00|\n\nOne of the more interesting aspects of the PR, however, is one line that\u2019s meant to help with the nonarray cases. In performance optimization, and in particular when adding \u201cfast paths\u201d to better handle\ncertain cases, there\u2019s almost always a winner and a loser: the winner is the case the optimization is\nintended to help, and the loser is every other case that\u2019s penalized by whatever checks are necessary\nto determine whether to take the improved path. An optimization that special-cases arrays might\nnormally look like:\n\nHowever, if you look at the PR, you\u2019ll see the `if` condition is actually:\n\n```\nif (source.GetType() == typeof (int[]))\n\n```\n\nHow come? Well at this point in the code flow, we know that source isn\u2019t null, so we don\u2019t need the\nextra null check that `is` will bring. However, that\u2019s minor compared to the real impact here, that of\n\nlanguage disagree on aspects of the type system. If you change that\n\nwork as part of the type check than just a simple comparison against the known type identity of\n\n154 CHAPTER 13 | LINQ\n\n`int[]` . We can see this by looking at the assembly generated for these two methods (the latter\nassumes we\u2019ve already null-checked the input, which is the case in these LINQ methods):\n\nThis results in:\n\nNote the former involves a method call to the JIT\u2019s `CastHelpers.IsInstanceOfAny` helper method,\nand that it\u2019s not inlined. That in turn impacts performance:\n\n|Method|Mean|Ratio|Code Size|\n|---|---|---|---|\n|WithIs|1.9246 ns|1.000|215 B|\n|WithTypeCheck|0.0013 ns|0.001|24 B|\n\nOf course, these two operations aren\u2019t semantically equivalent, so if this was for something that\nrequired the semantics of the former, we couldn\u2019t use the latter. But in the case of this LINQ\nperformance optimization, we can choose to only optimize the `int[]` case, forego the super rare case\n\n155 CHAPTER 13 | LINQ\n\n[This improvement was built upon further in dotnet/runtime#64624, which expands the input types](https://github.com/dotnet/runtime/pull/64624)\nsupported and the operations that take advantage. First, it introduced a private helper for extracting a\n\nform to avoid significantly penalizing other inputs. Both of these types enable extracting a\n\ncan do a few interesting things. This PR:\n\nThe effect of that is evident in microbenchmarks, e.g.\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Sum|.NET 6.0|39.067 us|1.00|32 B|1.00|\n|Sum|.NET 7.0|14.349 us|0.37|-|0.00|\n|||||||\n|Average|.NET 6.0|41.232 us|1.00|32 B|1.00|\n\n156 CHAPTER 13 | LINQ\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Average|.NET 7.0|14.378 us|0.35|-|0.00|\n|||||||\n|Min|.NET 6.0|45.522 us|1.00|32 B|1.00|\n|Min|.NET 7.0|9.668 us|0.21|-|0.00|\n|||||||\n|Max|.NET 6.0|41.178 us|1.00|32 B|1.00|\n|Max|.NET 7.0|9.210 us|0.22|-|0.00|\n\nThe previous LINQ PRs were examples from making existing operations faster. But sometimes\nperformance improvements come about from new APIs that can be used in place of previous ones in\ncertain situations to further improve performance. One such example of that comes from new APIs\n[introduced in dotnet/runtime#70525 from [@deeprobin](https://github.com/deeprobin) which were](https://github.com/dotnet/runtime/pull/70525)\n\nspirit of pairs like `Distinct` and `DistinctBy`, perform that same sorting operation, just with an implicit\n\n`x => x` done on behalf of the caller. But beyond performance, a nice benefit of this is the\nimplementation then knows that the keys will all be the same as the inputs, and it no longer needs to\ninvoke the callback for each item to retrieve its key nor allocate a new array to store those keys. Thus\nif you find yourself using LINQ and reaching for `OrderBy(x => x)`, consider instead using `Order()`\nand reaping the (primarily allocation) benefits:\n\n157 CHAPTER 13 | LINQ\n\n|Method|Length|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|OrderBy|1024|68.74 us|1.00|12.3 KB|1.00|\n|Order|1024|66.24 us|0.96|8.28 KB|0.67|\n\n158 CHAPTER 13 | LINQ\n\n**CHAPTER**"}, "14": {"File I/O": ".NET 6 saw some huge file I/O improvements, in particular a complete rewrite of `FileStream` . While\n.NET 7 doesn\u2019t have any single changes on that scale, it does have a significant number of\nimprovements that measurably \u201cmove the needle,\u201d and in variety of ways.\n\nOne form of performance improvement that also masquerades as a reliability improvement is\nincreasing responsiveness to cancellation requests. The faster something can be canceled, the sooner\nthe system is able to give back valuable resources in use, and the sooner things waiting for that\noperation to complete are able to be unblocked. There have been several improvements of this ilk in\n.NET 7.\n\nIn some cases, it comes from adding cancelable overloads where things weren\u2019t previously cancelable\n\n[satisfies the required types of both). You can see this put to good use in dotnet/runtime#66492 from](https://github.com/dotnet/runtime/pull/66492)\n\n[@lateapexearlyspeed](https://github.com/lateapexearlyspeed), which adds a new\n\nfully cancelable.\n\nFrom my perspective, though, a more interesting form of this is when an existing overload is\n\nup-front check for cancellation, such that if cancellation was requested prior to the call being made, it\nwill be immediately canceled, but after that check the supplied `CancellationToken` is effectively\nignored. Over time we\u2019ve tried to stamp out all remaining such cases, but a few stragglers have\nremained. One pernicious case has been with pipes. For this discussion, there are two relevant kinds of\npipes, anonymous and named, which are represented in .NET as pairs of streams:\n\nbetween handles opened for synchronous I/O from handles opened for overlapped I/O (aka\nasynchronous I/O), and this is reflected in the .NET API: you can open a named pipe for synchronous\nor overlapped I/O based on the `PipeOptions.Asynchronous` option specified at construction. And, on\n\n159 CHAPTER 14 | File I/O\n\nUnix, named pipes, contrary to their naming, are actually implemented on top of Unix domain sockets.\nNow some history:\n\n['.NET Framework 4.8: No cancellation support. The pipe `Stream` -derived types didn\u2019t even\\noverride `ReadAsync` or `WriteAsync`, so all they got was the default up-front check for\\ncancellation and then the token was ignored.', '.NET Core 1.0: On Windows, with a named pipe opened for asynchronous I/O, cancellation was']\n\nasynchronous operation. On Unix, with named pipes implemented in terms of sockets, if the pipe\n\ntoken, and then looping around to do it again until either the `Poll` indicated the operation\nwould succeed or cancellation was requested. On both Windows and Unix, other than a named\npipe opened with `Asynchronous`, after the operation was initated, cancellation was a nop.\n\n['.NET Core 2.1: On Unix, the implementation was improved to avoid the polling loop, but it still\\nlacked a truly cancelable `Socket.ReceiveAsync` / `Socket.SendAsync` . Instead, by this point']\n\nactually consuming it. The Unix implementation for asynchronous named pipe streams then\nchanged to issue zero-byte reads, and would `await` a `Task.WhenAny` of both that operation\u2019s\ntask and a task that would be completed when cancellation was requested. Better, but still far\nfrom ideal.\n\n['.NET Core 3.0: On Unix, `Socket` got truly cancelable `ReceiveAsync` and `SendAsync` methods,\\nwhich asynchronous named pipes were updated to utilize. At this point, the Windows and Unix\\nimplementations were effectively on par with regards to cancellation; both good for\\nasynchronous named pipes, and just posing for everything else.']\n\nanonymous and named pipes, regardless of how they were opened.\n\nSo by .NET 5, the problem was addressed on Unix, but still an issue on Windows. Until now. In .NET 7,\nwe\u2019ve made the rest of the operations fully cancelable on Windows as well, thanks to\n[dotnet/runtime#72503](https://github.com/dotnet/runtime/pull/72503) [(and a subsequent tweak in dotnet/runtime#72612). Windows doesn\u2019t support](https://github.com/dotnet/runtime/pull/72612)\noverlapped I/O for anonymous pipes today, so for anonymous pipes and for named pipes opened for\nsynchronous I/O, the Windows implementation would just delegate to the base `Stream`\nimplementation, which would queue a work item to the `ThreadPool` to invoke the synchronous\ncounterpart, just on another thread. Instead, the implementations now queue that work item, but\ninstead of just calling the synchronous method, it does some pre- and post- work that registers for\ncancellation, passing in the thread ID of the thread that\u2019s about to perform the I/O. If cancellation is\nrequested, the implementation then uses `CancelSynchronousIo` to interrupt it. There\u2019s a race\ncondition here, in that the moment the thread registers for cancellation, cancellation could be\nrequested, such that `CancelSynchronousIo` could be called before the operation is actually initiated.\n\n160 CHAPTER 14 | File I/O\n\nSo, there\u2019s a small spin loop employed, where if cancellation is requested between the time\nregistration occurs and the time the synchronous I/O is actually performed, the cancellation thread\nwill spin until the I/O is initiated, but this condition is expected to be exceedingly rare. There\u2019s also a\nrace condition on the other side, that of `CancelSynchronousIo` being requested after the I/O has\nalready completed; to address that race, the implementation relies on the guarantees made by\n\n`CancellationTokenRegistration.Dispose`, which promises that the associated callback will either\nnever be invoked or will already have fully completed executing by the time `Dispose` returns. Not only\ndoes this implementation complete the puzzle such that all asynchronous read/write operations on\nboth anonymous and named pipes on both Windows and Unix are cancelable, it also actually\nimproves normal throughput.\n\n161 CHAPTER 14 | File I/O\n\n|Method|Runtime|Cancelable|Named|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|---|---|\n|ReadWriteAsync|.NET 6.0|False|False|22.08 us|1.00|400 B|1.00|\n|ReadWriteAsync|.NET 7.0|False|False|12.61 us|0.76|192 B|0.48|\n|||||||||\n|ReadWriteAsync|.NET 6.0|False|True|38.45 us|1.00|400 B|1.00|\n|ReadWriteAsync|.NET 7.0|False|True|32.16 us|0.84|220 B|0.55|\n|||||||||\n|ReadWriteAsync|.NET 6.0|True|False|27.11 us|1.00|400 B|1.00|\n|ReadWriteAsync|.NET 7.0|True|False|13.29 us|0.52|193 B|0.48|\n|||||||||\n|ReadWriteAsync|.NET 6.0|True|True|38.57 us|1.00|400 B|1.00|\n|ReadWriteAsync|.NET 7.0|True|True|33.07 us|0.86|214 B|0.54|\n\nThe rest of the performance-focused changes around I/O in .NET 7 were primarily focused on one of\ntwo things: reducing syscalls, and reducing allocation.\n\nSeveral PRs went into reducing syscalls on Unix as part of copying files, e.g. `File.Copy` and\n\n`FileInfo.CopyTo` [. dotnet/runtime#59695 from [@tmds](https://github.com/tmds) reduced overheads](https://github.com/dotnet/runtime/pull/59695)\nin several ways. The code had been performing a `stat` call in order to determine up front whether the\nsource was actually a directory, in which case the operation would error out. Instead, the PR simply\ntries to open the source file, which it would need to do anyway for the copy operation, and then it\n\nnow also extracts from the results of that single `fstat` the source file size, which it then threads\nthrough to the core copy routine, which itself is then able to avoid an `fstat` syscall it had been\nperforming in order to get the size. Saving those syscalls is great, in particular for very small files\nwhere the overhead of setting up the copy can actually be more expensive than the actual copy of the\nbytes. But the biggest benefit of this PR is that it takes advantage of `IOCTL-FICLONERANGE` on Linux.\nSome Linux file systems, like XFS and Btrfs, support \u201ccopy-on-write,\u201d which means that rather than\ncopying all of the data to a new file, the file system simply notes that there are two different files\npointing to the same data, sharing the underlying storage. This makes the \u201ccopy\u201d super fast, since\nnothing actually needs to be copied and instead the file system just needs to update some\nbookkeeping; plus, less space is consumed on disk, since there\u2019s just a single store of the data. The file\nsystem then only needs to actually copy data that\u2019s overwritten in one of the files. This PR uses `ioctl`\nand `FICLONE` to perform the copy as copy-on-write if the source and destination file system are the\n\n`copy_file_range` on Linux if it\u2019s supported (and only if it\u2019s a new enough kernel that it addresses\n\n162 CHAPTER 14 | File I/O\n\nsome issues the function had in previous releases). Unlike a typical read/write loop that reads the data\nfrom the source and then writes it to the destination, `copy_file_range` is implemented to stay\nentirely in kernel mode, without having to transition to user space for each read and write.\n\ndata from a file, and hints to OS caching to expect data to be read from the file sequentially rather\nthan randomly. However, these write/append methods don\u2019t read, they only write, and the\n\nbenefiting from it. This situation is akin to the famous Henny Youngman joke: \u201cThe patient says,\n\u2018Doctor, it hurts when I do this\u2019; the doctor says, \u2018Then don\u2019t do that!\u2019.\u201d Here, too, the answer is \u201cdon\u2019t\n\nDirectory handling has seen reduced syscalls across the directory lifecycle, especially on Unix.\n[dotnet/runtime#58799](https://github.com/dotnet/runtime/pull/58799) from [@tmds](https://github.com/tmds) speeds up directory creation on Unix.\nPreviously, the implementation of directory creation would first check to see if the directory already\nexisted, which involves a syscall. In the expected minority case where it already existed the code could\nearly exit out. But in the expected more common case where the directory didn\u2019t exist, it would then\nparse the file path to find all of the directories in it, walk up the directory list until it found one that\ndid exist, and then try to create all of the subdirectories back down through the target one. However,\nthe expected most common case is the parent directories already exist and the child directory doesn\u2019t,\nin which case we\u2019re still paying for all that parsing when we could have just created the target\ndirectory. This PR addresses that by changing the up-front existence check to instead simply try to\n\n`mkdir` the target directory; if it succeeds, great, we\u2019re done, and if it fails, the error code from the\nfailure can be used instead of the existence check to know whether `mkdir` failed because it had no\n[work to do. dotnet/runtime#61777 then takes this a step further and avoids string allocations while](https://github.com/dotnet/runtime/pull/61777)\ncreating directories by using stack memory for the paths temporarily needed to pass to `mkdir` .\n\n[dotnet/runtime#63675](https://github.com/dotnet/runtime/pull/63675) then improves the performance of moving directories, on both Unix and\nWindows, removing several syscalls. The shared code for `Directory.Move` and `DirectorInfo.MoveTo`\nwas doing explicit directory existence checks for the source and destination locations, but on\nWindows the Win32 API called to perform the move does such checks itself, so they\u2019re not needed\npreemptively. On Unix, we can similarly avoid the existence check for the source directory, as the\n\n`rename` function called will similarly simply fail if the source doesn\u2019t exist (with an appropriate error\nthat let\u2019s us deduce what went wrong so the right exception can be thrown), and for the destination,\nthe code had been issuing separate existence checks for whether the destination existed as a directory\nor as a file, but a single `stat` call suffices for both.\n\n163 CHAPTER 14 | File I/O\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Move|.NET 6.0|31.70 us|1.00|256 B|1.00|\n|Move|.NET 7.0|26.31 us|0.83|-|0.00|\n\n[And then also on Unix, dotnet/runtime#59520 from [@tmds](https://github.com/tmds) improves](https://github.com/dotnet/runtime/pull/59520)\ndirectory deletion, and in particular recursive deletion (deleting a directory and everything it contains\nand everything they contain and so on), by utilizing the information already provided by the file\nsystem enumeration to avoid a secondary existence check.\n\nto determine whether the specified file already exists; that\u2019s because later in the method as part of\ndealing with errors and exceptions, the implementation needs to know whether to delete the file that\n\nthan `Open` or `CreateNew`, which means we can trivially avoid the extra system call in the majority case.\n[dotnet/runtime#63790](https://github.com/dotnet/runtime/pull/63790) also helps here, in two ways. First, throughout the `CreateFromFile` operation,\nthe implementation might access the `FileStream` \u2019s `Length` multiple times, but each call results in a\nsyscall to read the underlying length of the file. We can instead read it once and use that one value for\n\nthe superfluous `FileStream` and its supporting state. This helps to reduce allocations.\n\nbut the practical implication of this is that closing either needn\u2019t bother flushing, since that view\ncouldn\u2019t have changed any data in the implementation, and flushing a view can be relatively\nexpensive, especially for larger views. Thus, a simple change to avoid flushing if the view isn\u2019t writable\ncan yield a measurable improvement to `MemoryMappedViewAccessor` / `MemoryMappedviewStream` \u2019s\n\n`Dispose` .\n\n164 CHAPTER 14 | File I/O\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|MMF|.NET 6.0|315.7 us|1.00|488 B|1.00|\n|MMF|.NET 7.0|227.1 us|0.68|336 B|0.69|\n\nBeyond system calls, there have also been a plethora of improvements around reducing allocation.\n\nthings: one, that these operations are common enough that it\u2019s worth avoiding the small-butmeasurable overhead of going through a `FileStream` and instead just going directly to the\nunderlying `SafeFileHandle`, and, two, that since the methods are passed the entirety of the payload\nto output, the implementation can use that knowledge (in particular for length) to do better than the\n\n`StreamWriter` that was previously employed. In doing so, the implementation avoids the overheads\n(primarily in allocation) of the streams and writers and temporary buffers.\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|WriteAllText|.NET 6.0|488.5 us|1.00|9944 B|1.00|\n|WriteAllText|.NET 7.0|482.9 us|0.99|392 B|0.04|\n\n165 CHAPTER 14 | File I/O\n\nadditional syscall can measure up to 10% of the total cost (and for larger files, modern kernels are\npretty good about caching even without a sequentiality hint, so there\u2019s little downside measured\nthere).\n\nwrapped each string in a span and delegated to the span-based overloads. However, in the event that\nthe join operation is a nop (e.g. there are two path segments and the second is empty so the join\nshould just return the first), the span-based implementation still needs to create a new string (there\u2019s\nno way for the `ReadOnlySpan<char>` -based overloads to extract a string from the span). As such, the\n\n`string` -based overloads can do a little bit better in the case of one of them being null or empty; they\ncan do the same thing the `Path.Combine` overloads do, which is to have the M argument overload\ndelegate to the M-1 argument overload, filtering out a null or empty, and in the base case of the\noverload with two arguments, if a segment is null or empty, the other (or empty) can just be returned\ndirectly.\n\n[Beyond that, there are a multitude of allocation-focused PRs, such as dotnet/runtime#69335 from](https://github.com/dotnet/runtime/pull/69335)\n\n[@pedrobsaila](https://github.com/pedrobsaila) which adds a fast-path based on stack allocation to\nthe internal `ReadLink` helper that\u2019s used on Unix anywhere we need to follow symlinks, or\n\n[But my personal favorite improvement in this area come from dotnet/runtime#69272, which adds a](https://github.com/dotnet/runtime/pull/69272)\nfew new helpers to `Stream` :\n\nIn fairness, these are more about usability than they are about performance, but in this case there\u2019s a\ntight correlation between the two. It\u2019s very common to write these helpers one\u2019s self (the\naforementioned PR deleted many open-coded loops for this functionality from across the core\nlibraries) as the functionality is greatly needed, and it\u2019s unfortunately easy to get them wrong in ways\n\n166 CHAPTER 14 | File I/O\n\nthat negatively impact performance, such as by using a `Stream.ReadAsync` overload that needs to\nallocate a returned `Task<int>` or reading fewer bytes than is allowed as part of a read call. These\nimplementations are correct and efficient.\n\n167 CHAPTER 14 | File I/O\n\n**CHAPTER**"}, "15": {"Compression": "on top of. For the most part, these types just provide wrappers around a native C implementation\n[from google/brotli, and so while the .NET layer has the opportunity to improve how data is moved](https://github.com/google/brotli)\naround, managed allocation, and so on, the speed and quality of the compression itself are largely at\nthe mercy of the C implementation and the intricacies of the Brotli algorithm.\n\nAs with many compression algorithms, Brotli provides a knob that allows for a quintessential tradeoff\nto be made between compression speed (how fast data can be compressed) and compression\nquality/ratio (how small can the compressed output be made). The hand-wavy idea is the more time\nthe algorithm spends looking for opportunity, the more space can be saved. Many algorithms expose\nthis as a numerical dial, in Brotli\u2019s case going from 0 (fastest speed, least compression) to 11 (spend as\nmuch time as is needed to minimize the output size). But while `BrotliEncoder` surfaces that same\n\nvalue when one is.\n\nFor better or worse (and I\u2019m about to argue \u201cmuch worse\u201d), the native C implementation itself defines\n\nvalue is poorly named. It\u2019s intended to represent a good default that\u2019s a balanced tradeoff between\nspeed and quality; that\u2019s exactly what it means for `DeflateStream`, `GZipStream`, and `ZLibStream` . But\nfor `BrotliStream`, as the default it similarly got translated to mean the underlying native library\u2019s\n\ndefault, you\u2019re getting the dial turned all the way up to 11.\n\nIs that so bad? Maybe compression quality is the most important thing? For example, reducing the\nsize of data can make it faster to then transmit it over a wire, and with a slow connection, size then\nmeaningfully translates into end-to-end throughput.\n\nThe problem is just how much this extra effort costs. Compression speed and ratio are highly\ndependent on the data being compressed, so take this example with a small grain of salt as it\u2019s not\nentirely representative of all use, but it\u2019s good enough for our purposes. Consider this code, which\n\n168 CHAPTER 15 | Compression\n\nuses `BrotliEncoder` [to compress the The Complete Works of William Shakespeare from Project](https://www.gutenberg.org/cache/epub/100/pg100.txt)\n[Gutenberg](https://www.gutenberg.org/cache/epub/100/pg100.txt) at varying levels of compression:\n\n169 CHAPTER 15 | Compression\n\nThe code is measuring how long it takes to compress the input data at each of the levels (doing a\nwarmup and then averaging several iterations), timing how long it takes and capturing the resulting\ncompressed data size. For the size, I get values like this:\n\n|Level|Size (bytes)|\n|---|---|\n|0|2,512,855.00|\n|1|2,315,466.00|\n|2|2,224,638.00|\n|3|2,218,328.00|\n|4|2,027,153.00|\n|5|1,964,810.00|\n|6|1,923,456.00|\n|7|1,889,927.00|\n|8|1,863,988.00|\n|9|1,846,685.00|\n|10|1,741,561.00|\n|11|1,702,214.00|\n\nThat\u2019s a fairly liner progression from least to most compression. That\u2019s not the problem. This is the\nproblem:\n\n170 CHAPTER 15 | Compression\n\n|Level|Time<br>(ms)|\n|---|---|\n|0|24.11|\n|1|36.67|\n|2|64.13|\n|3|73.72|\n|4|146.41|\n|5|257.12|\n|6|328.54|\n|7|492.81|\n|8|702.38|\n|9|892.08|\n|10|4,830.32|\n|11|10,634.88|\n\nThis chart shows an almost exponential increase in processing time as we near the upper end of the\ndial, with quality level 11 compressing ~33% better than quality level 0 but taking ~440x as long to\n\nrepresent a fairly balanced trade-off between size and speed.\n\n171 CHAPTER 15 | Compression\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Compress|.NET 6.0|9,807.0 ms|1.00|\n|Compress|.NET 7.0|133.1 ms|0.01|\n\n`System.IO.Packaging` [, and dotnet/runtime#73082 updating the zlib implementations shipped as part](https://github.com/dotnet/runtime/pull/73082)\nof .NET from v1.2.11 (which was released in January 2017) to v1.2.12 (which was released in March\n2022).\n\n172 CHAPTER 15 | Compression\n\n**CHAPTER**"}, "16": {"Networking": "Networking is the life-blood of almost every service, with performance being critical to success. In\nprevious releases, a lot of effort was focused on the lower layers of the networking stack, e.g. .NET 5\nsaw a significant investment in improving the performance of sockets on Linux. In .NET 7, much of the\neffort is above sockets.\n\nThat said, there were some interesting performance improvements in sockets itself for .NET 7. One of\n\ncomplicated to use well, it\u2019s relatively inefficient, resulting in allocation for every single operation\nperformed (at a minimum for the `IAsyncResult` object that\u2019s returned from the `BeginXx` method). To\nhelp make networking operations more efficient, `SocketsAsyncEventArgs` was introduced.\n\n`SocketsAsyncEventArgs` is a reusable class you allocate to hold all of the state associated with\nasynchronous operations: allocate one, pass it to various async methods (e.g. `ReceiveAsync` ), and\nthen completion events are raised on the `SocketAsyncEventArgs` instance when the operation\ncompletes. It can be quite efficient when used correctly, but it\u2019s also complicated to use correctly. In\n\ncovers, and so while most code these days isn\u2019t written to use `SocketAsyncEventArgs` directly, it\u2019s still\nvery relevant from a performance perspective.\n\n`SocketAsyncEventArgs` on Windows is implemented to use winsock and overlapped I/O. When you\n\nwas introduced in earlier releases of .NET Core, and it enables a significant number of socket\noperations, in particular sends and receives, to complete synchronously, which in turn saves\nunnecessary trips through the thread pool, unnecessary unwinding of async state machines, and so\non. But it also causes a condundrum. There are some operations we want to perform associated with\nasynchronous operation but that have additional overhead, such as registering for the cancellation of\nthose operations, and we don\u2019t want to pay the cost of doing them if the operation is going to\ncomplete synchronously. That means we really want to delay performing such registration until after\nwe\u2019ve made the native call and discovered the operation didn\u2019t complete synchronously\u2026 but at that\npoint we\u2019ve already initiated the operation, so if it _doesn\u2019t_ complete synchronously, then we\u2019re now in\n\n173 CHAPTER 16 | Networking\n\na potential race condition, where our code that\u2019s still setting up the asynchronous operation is racing\nwith it potentially completing in a callback on another thread. Fun. `SocketAsyncEventArgs` handled\nthis race condition with a spin lock; the theory was that contention would be incredibly rare, as the\nvast majority cases would either be the operation completing synchronously (in which case there\u2019s no\nother thread involved) or asynchronously with enough of a delay that the small amount of additional\nwork performed by the initiating thread would have long ago completed by the time the\nasynchronous operation completed. And for the most part, that was true. However, it turns out that\nit\u2019s actually much more common than expected for certain kinds of operations, like Accepts. Accepts\nend up almost always completing asynchronously, but if there\u2019s already a pending connection,\ncompleting asynchronously almost immediately, which then induces this race condition to happen\nmore frequently and results in more contention on the spin locks. Contention on a spin lock is\nsomething you really want to avoid. And in fact, for a particular benchmark, this spin lock showed up\nas the cause for an almost 300% slowdown in requests-per-second (RPS) for a benchmark that used a\ndedicated connection per request (e.g. with every response setting \u201cConnection: close\u201d).\n[dotnet/runtime#64770](https://github.com/dotnet/runtime/pull/64770) changed the synchronization mechanism to no longer involve a spin lock;\ninstead, it maintains a simple gate implemented as an `Interlocked.CompareExchange` . If the initiating\nthread gets to the gate first, from that point on the operation is considered asynchronous and any\nadditional work is handled by the completing callback. Conversely, if the callback gets to the gate first,\nthe initiating thread treats the operation as if it completed synchronously. This not only avoids one of\nthe threads spinning while waiting for the other to make forward progress, it also increases the\nnumber of operations that end up being handled as synchronous, which in turn reduces other costs\n(e.g. the code `await` ing the task returned from this operation doesn\u2019t need to hook up a callback and\nexit, and can instead itself continue executing synchronously). The impact of this is difficult to come\nup with a microbenchmark for, but it can have meaningful impact for loaded Windows servers that\nend up accepting significant numbers of connections in steady state.\n\ncomparing spans, doing it byte-by-byte is also much less efficient than the vectorized implementation\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|Equals_Same|.NET 6.0|57.659 ns|1.00|\n|Equals_Same|.NET 7.0|4.435 ns|0.08|\n\n174 CHAPTER 16 | Networking\n\nLet\u2019s move up to some more interesting changes in the layers above `Sockets`, starting with\n\n`SslStream` .\n\nOne of the more impactful changes to `SslStream` on .NET 7 is in support for TLS resumption on Linux.\nWhen a TLS connection is established, the client and server engage in a handshake protocol where\nthey collaborate to decide on a TLS version and cipher suites to use, authenticate and validate each\nother\u2019s identity, and create symmetric encryption keys for use after the handshake. This represents a\nsignificant portion of the time required to establish a new connection. For a client that might\ndisconnect from a server and then reconnect later, as is fairly common in distributed applications, TLS\nresumption allows a client and server to essentially pick up where they left off, with the client and/or\nserver storing some amount of information about recent connections and using that information to\nresume. Windows SChannel provides default support for TLS resumption, and thus the Windows\nimplementation of `SslStream` (which is built on SChannel) has long had support for TLS resumption.\nBut OpenSSL\u2019s model requires additional code to enable TLS resumption, and such code wasn\u2019t\npresent in the Linux implementation of `SslStream` [. With dotnet/runtime#57079](https://github.com/dotnet/runtime/pull/57079) and\n[dotnet/runtime#63030, .NET 7 adds server-side support for TLS resumption (using the variant that](https://github.com/dotnet/runtime/pull/63030)\n_doesn\u2019t_ [require storing recent connection state on the server), and with dotnet/runtime#64369, .NET 7](https://github.com/dotnet/runtime/pull/64369)\nadds client-side support (which _does_ require storing additional state). The effect of this is significant, in\nparticular for a benchmark that opens and closes lots of connections between clients.\n\n175 CHAPTER 16 | Networking\n\n176 CHAPTER 16 | Networking\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Handshake|.NET 6.0|4.647 ms|1.00|19.27 KB|1.00|\n|Handshake|.NET 7.0|2.314 ms|0.50|9.56 KB|0.50|\n\nAnother significant improvement for `SslStream` in .NET 7 is support for OCSP stapling. When a client\nhandshakes with the server and the server shares its certificate, a client that cares about validating it\u2019s\ntalking to exactly who it intended to talk to needs to validate that certificate. In the days of yore, such\nvalidation was done with certificate revocation lists (CRL), where periodically the client would\ndownload a giant list of certificates known to be revoked. Online Certificate Status Protocol (OCSP) is\na newer protocol and mechanism that enables a client to get real-time information about a certificate;\nwhile the client handshakes with the server and the server sends the client its certificate, the client\nthen connects to an \u201cOCSP responder\u201d and sends it a request to determine whether the certificate is\nconsidered good. OCSP has multiple issues of its own, however. In particular, it places a significant\nload on these OCSP responder servers, with every client making a real-time request to it about every\ncertificate encountered, and also potentially significantly increasing the time it takes the client to\nestablish a connection. OCSP stapling offers a solution to this. Rather than a client issuing a request to\n\n177 CHAPTER 16 | Networking\n\nthe OCSP responder, the server itself contacts the OCSP responder and gets a signed ticket from the\nOCSP responder stating that the server\u2019s certificate is good and will be for some period of time. When\na client handshakes with the server, the server can then \u201cstaple\u201d (include) this signed ticket as part of\nits response to the client, giving the validation to the client directly rather than the client needing to\nmake a separate roundtrip to the OCSP responder. This reduces overheads for everyone involved.\n[dotnet/runtime#67011](https://github.com/dotnet/runtime/pull/67011) adds support for OCSP stapling to `SslStream` client usage on Linux, with\n[dotnet/runtime#69833](https://github.com/dotnet/runtime/pull/69833) [adding the Linux server-side counterpart, and dotnet/runtime#71570 adds](https://github.com/dotnet/runtime/pull/71570)\nclient-side support for Windows.\n\nThe aforementioned changes are primarily about the performance of opening a connection.\n[Additional work has been done to improve that further in other ways. dotnet/runtime#69527 gets rid](https://github.com/dotnet/runtime/pull/69527)\nof allocations associated with several `SafeHandle` instances that were being created unnecessarily on\nLinux as part of establishing a TLS connection. This highlights the benefits of doing profiling on\nmultiple platforms, as while these `SafeHandle` s were necessary in the Windows implementation, they\nwere fairly meaningless in the Linux implementation (due to differences between SChannel and\nOpenSSL), and were only brought along for the ride because of how the platform-abstraction layer\n(PAL) was defined to reuse most of the `SslStream` [code across platforms. And dotnet/runtime#68188](https://github.com/dotnet/runtime/pull/68188)\navoids several collections allocated as part of the TLS handshake. This one is particularly interesting as\nit\u2019s come up multiple times in the past in various libraries. Imagine you have a lazily initialized\nproperty like this:\n\nAnd then some code in the same implementation comes along and wants to read the contents of\nthese items. That code might look like:\n\n```\nif (Items.Count > 0) { ... }\n\n```\n\nbut the very act of accessing `Items` just to check its count forces the collection into existence (with a 0\n\n`Count` ). If the code instead checks:\n\n```\nif (_items is List<T> items && items.Count > 0) { ... }\n\n```\n\nIt can save that unnecessary collection allocation. The approach is made even simpler with C# pattern\nmatching:\n\n```\nif (_items is { Count: > 0 }) items) { ... }\n\n```\n\nThis is one of those things that\u2019s incredibly obvious once you \u201csee\u201d it and realize what\u2019s happening,\nbut you often miss until it jumps out at you in a profiler.\n\n[dotnet/runtime#69098](https://github.com/dotnet/runtime/pull/69098) is another good example of how profiling can lead to insights about\nallocations that can be removed. Application-Layer Protocol Negotation (ALPN) allows code\nestablishing a TLS connection to piggy-back on the roundtrips that are being used for the TLS\nhandshake anyway to negotiate some higher-level protocol that will end up being used as well. A very\ncommon use-case, for example, is for an HTTPS client/server to negotiate which version of HTTP\n\n178 CHAPTER 16 | Networking\n\nby far the most common byte sequences are equivalent to \u201chttp/1.1\u201d for HTTP/1.1, \u201ch2\u201d for HTTP/2,\nand \u201ch3\u201d for HTTP/3. Thus, it makes sense to special-case those values and use a reusable cached\n\n`byte[]` singleton when one of those values is needed. If `SslApplicationProtocol` exposed the\nunderlying `byte[]` directly to consumers, we\u2019d be hesitant to use such singletons, as doing so would\nmean that if code wrote into the `byte[]` it would potentially be changing the value for other\nconsumers in the same process. However, `SslApplicationProtocol` exposes it as a\n\n`SslStream` . It uses stack memory instead of an array allocation for protocols up to 256 bytes in length,\nwhich is way larger than any in known use, and thus doesn\u2019t bother to do anything fancy for the\n[fallback path, which will never be used in practice. And dotnet/runtime#69103](https://github.com/dotnet/runtime/pull/69103) further avoids ALPNrelated allocations and work on Windows by entirely skipping some unnecessary code paths: various\nmethods can be invoked multiple times during a TLS handshake, but even though the ALPN-related\nwork only needed to happen once the first time, the code wasn\u2019t special-casing it and was instead\nrepeating the work over and over.\n\nEverything discussed thus far was about establishing connections. What about the performance of\nreading and writing on that connection? Improvements have been made there, too, in particular\naround memory management and asynchrony. But first we need some context.\n\nWhen `async/await` were first introduced, `Task` and `Task<TResult>` were the only game in town; while\nthe pattern-based mechanism the compiler supports for arbitrary \u201ctask-like\u201d types enabled `async`\nmethods to return other types, in practice it was only tasks (which also followed our guidance). We\nsoon realized, however, that a significant number of calls to a significant number of commonly-used\nasync APIs would actually complete synchronously. Consider, for example, a method like\n\n`MemoryStream.ReadAsync` : `MemoryStream` is backed entirely by an in-memory buffer, so even though\nthe operation is \u201casync,\u201d every call to it completes synchronously, as the operation can be performed\n\nactual native call with its own much larger buffer, which by default is 4K. The first time you issue your\n16-byte read, actual I/O will be required and the operation is likely to complete asynchronously. But\nthe next 255 calls you make could simply end up draining the remainder of the data read into that 4K\nbuffer, in which case 255 of the 256 \u201casync\u201d operations actually complete synchronously. If the\n\ndevised to minimize this, e.g. if the `int` is one of a few well-known values (e.g. -1 through 8), then the\nasync method infrastructure will hand back a pre-allocated and cached `Task<int>` instance for that\nvalue, and various stream implementations (including `FileStream` ) would cache the previouslyreturned `Task<int>` and hand it back for the next call as well if the next call yielded exactly the same\nnumber of bytes. But those optimizations don\u2019t fully mitigate the issue. Instead, we introduced the\n\n`ValueTask<TResult>` struct and provided the necessary \u201cbuilder\u201d to allow `async` methods to return\n\n179 CHAPTER 16 | Networking\n\nthem. `ValueTask<TResult>` was simply a discrimated union between a `TResult` and `Task<TResult>` . If\nan async method completed asynchronously (or if it failed synchronously), well, it would simply\n\nallocation overhead for the synchronously-completing case. Yay, everyone\u2019s happy. Well, almost\neveryone. For really hot paths, especially those lower down in the stack that many other code paths\nbuild on top of, it can also be beneficial to avoid the allocations even for the asynchronously\ncompleting case. To address that, .NET Core 2.1 saw the introduction of the\n\ncan implement this interface with whatever behaviors they want, although we codified the typical\nimplementation of the core async logic into the `ManualResetValueTaskSourceCore` helper struct,\nwhich is typically embedded into some object, with the interface methods delegating to\ncorresponding helpers on the struct. Why would someone want to do this? Most commonly, it\u2019s to be\nable to reuse the same instance implementing this interface over and over and over. So, for example,\n\nallocation-free (there is another instance used for `SendAsync`, such that you can have a concurrent\nread and write on the socket and still avoid allocations). However, implementing this support is still\nnon-trivial, and can be super hard when dealing with an operation that\u2019s composed of multiple\nsuboperations, which is exactly where `async/await` shine. Thus, C# 10 added support for overriding\nthe default builder that\u2019s used on an individual async method (e.g. such that someone could provide\ntheir own builder for a `ValueTask<int>` -returning method instead of the one that allocates `Task<int>`\ninstances for asynchronous completion) and .NET 6 included the new\n\n```\npublic async ValueTask<int> ReadAsync(Memory<byte> buffer) { ... }\n\n```\n\ncan be changed to be:\n\nwhich will cause the C# compiler to emit the implementation of this method using\n\n_most_ of the allocation asynchronous completion would otherwise experience (I say \u201cmost\u201d because\nthe pooling by design tries to balance all the various costs involved and may still sometimes allocate),\nand makes it easy for methods implemented with `async` / `await` to reap those benefits. So, if this was\nall introduced in the last release, why am I talking about it now? Pooling isn\u2019t free. There are various\n\n180 CHAPTER 16 | Networking\n\ntradeoffs involved in its usage, and while it can make microbenchmarks look really good, it can also\nnegatively impact real-world usage, e.g. by increasing the cost of garbage collections that do occur by\nincreasing the number of Gen2 to Gen0 references that exist. As such, while the functionality is\nvaluable, we\u2019ve been methodical in where and how we use it, choosing to do so more slowly and only\nemploying it after sufficient analysis deems it\u2019s worthwhile.\n\nSuch is the case with `SslStream` [. With dotnet/runtime#69418, two core and hot](https://github.com/dotnet/runtime/pull/69418) `async` methods on\n\n`SslStream` \u2019s read path were annotated to use pooling. A microbenchmark shows what I mean when I\nwrote this can make microbenchmarks look really good (focus on the allocation columns). This\nbenchmark is repeatedly issuing a read (that will be forced to complete asynchronously because\nthere\u2019s no available data to satisfy it), then issuing a write to enable that read to complete, and then\n\n`await` ing the read\u2019s completion; every read thus completes asynchronously.\n\n181 CHAPTER 16 | Networking\n\n|Method|Runtime|Mean|Ratio|Code Size|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|---|\n|ReadWriteAsync|.NET 6.0|68.34 ms|1.00|510 B|336404 B|1.000|\n|ReadWriteAsync|.NET 7.0|69.60 ms|1.02|514 B|995 B|0.003|\n\nOne final change related to reading and writing performance on an `SslStream` . I find this one\nparticularly interesting, as it highlights a new and powerful C# 11 and .NET 7 feature: static abstract\nmembers in interfaces. `SslStream`, as with every `Stream`, exposes both synchronous and asynchronous\nmethods for reading and writing. And as you may be aware, the code within `SslStream` for\nimplementing reads and writes is not particularly small. Thus, we really want to avoid having to\nduplicate all of the code paths, once for synchronous work and once for asynchronous work, when in\nreality the only place that bifurcation is needed is at the leaves where calls into the underlying `Stream`\nare made to perform the actual I/O. Historically, we\u2019ve had two different mechanisms we\u2019ve employed\n\nat the leaves, e.g.\n\nThis way most of the logic and code is shared, and when useAsync is false, everything completes\nsynchronously and so we don\u2019t pay for allocation that might otherwise be associated with the `async` ness. The other approach is similar in spirit, but instead of a `bool` parameter, taking advantage of\ngeneric specialization and interface-implementing structs. Consider an interface like:\n\n182 CHAPTER 16 | Networking\n\nWe can then declare two implementations of this interface:\n\nThen we can redeclare our earlier example as:\n\nNote that the generic constraint on the `TReader` parameter here allows the implementation to invoke\nthe interface methods, and passing the structs as a generic avoids boxing. One code path supporting\nboth sync and async implementations.\n\nThis latter generic approach is how `SslStream` has historically handled the unification of its sync and\nasync implementations. It gets better in .NET 7 with C# 11 now that we have static abstract methods\nin interfaces. We can instead declare our interface as (note the `static abstract` addition):\n\nour types as (note the `static` addition):\n\n183 CHAPTER 16 | Networking\n\nand our consuming methods as (note the removal of the parameter and the switch to calling static\nmethods on the type parameter):\n\nNot only is this cleaner, but from a performance perspective we no longer need to pass around the\ndummy generic parameter, which is general goodness, but for an async method it\u2019s particularly\nbeneficial because the state machine type ends up storing all parameters as fields, which means every\nparameter can increase the amount of allocation incurred by an async method if the method ends up\n[completing asynchronously. dotnet/runtime#65239](https://github.com/dotnet/runtime/pull/65239) flipped `SslStream` (and `NegotiateStream` ) to\nfollow this approach. It\u2019s also used in multiple other places now throughout dotnet/runtime.\n\nway to abstract over DFA and NFA-based operations using the same code (this technique was since\nfurther utilized in `NonBacktracking` [, such as by dotnet/runtime#71234](https://github.com/dotnet/runtime/pull/71234) from\n\nfunctionality: searching forward or backwards, and with equality or inequality. While more challenging\nto try to unify the directional aspect, this PR utilized this same kind of generic specialization to hide\nbehind an interface the ability to negate the comparison; the core implementations of these methods\n\noptimizations of the non- `Except` varieties, but also the goal of trying to make everything consistent\nresulted in finding places where we were missing optimization opportunities in existing methods\n(gaps that the PR also rectified).\n\n184 CHAPTER 16 | Networking\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|LastIndexOfAny|.NET 6.0|9.977 us|1.00|\n|LastIndexOfAny|.NET 7.0|1.172 us|0.12|\n\nLet\u2019s move up the stack to HTTP. Most of the folks focusing on networking in .NET 7 were focused on\ntaking the preview support for HTTP/3 that shipped in .NET 6 and making it a first-class supported\nfeature in .NET 7. That included functional improvements, reliability and correctness fixes, and\nperformance improvements, such that HTTP/3 can now be used via `HttpClient` on both Windows\nand Linux (it depends on an underlying QUIC implementation in the msquic component, which isn\u2019t\ncurrently available for macOS). However, there were significant improvements throughout the HTTP\nstack, beyond HTTP/3.\n\nOne aspect of `HttpClient` that cuts across all versions of HTTP is support for handling and\nrepresenting headers. While significant improvements went into previous releases to trim down the\nsize of the data structures used to store header information, further work on this front was done for\n\nthat can be sent with an HTTP request or response (though in order to mitigate possible denial of\nservice attacks, the implementation has a configurable limit for how many bytes of headers are\naccepted from the server), and thus it needs to be able to handle an arbitrary number of them and to\ndo so with efficient access. As such, for the longest time `HttpHeaders` has used a `Dictionary<,>` to\nprovide `O(1)` lookup into these headers. However, while it\u2019s valid to have large numbers of headers,\nit\u2019s most common to only have a handful, and for only a few items, the overheads involved in a hash\ntable like `Dictionary<>` can be more than just storing the elements in an array and doing an `O(N)`\nlookup by doing a linear search through all the elements (algorithmic complexity ignores the\n\u201cconstants\u201d involved, so for a small `N`, an `O(N)` algorithm might be much faster and lighterweight than\nan `O(1)` ). This PR takes advantage of that and teaches `HttpHeaders` how to use either an array or a\ndictionary; for common numbers of headers (the current threshold is 64), it just uses an array, and in\nthe rare case where that threshold is exceeded, it graduates into a dictionary. This reduces the\nallocation in `HttpHeader` in all but the most niche cases while also making it faster for lookups.\n\n[Another header-related size reduction comes in dotnet/runtime#64105. The internal representation of](https://github.com/dotnet/runtime/pull/64105)\nheaders involves a `HeaderDescriptor` that enables \u201cknown headers\u201d (headers defined in the HTTP\nspecifications or that we\u2019re otherwise aware of and want to optimize) to share common data, e.g. if a\nresponse header matches one of these known headers, we can just use the header name string\nsingleton rather than allocating a new string for that header each time we receive it. This\n\n`HeaderDescriptor` accomodated both known headers and custom headers by having two fields, one\nfor known header data (which would be null for custom headers) and one for the header name.\nInstead, this PR employs a relatively-common technique of having a single `object` field that then\nstores either the known header information or the name, since the known header information itself\nincludes the name, and thus we don\u2019t need the duplication. At the expense of a type check when we\n\n185 CHAPTER 16 | Networking\n\nneed to look up information from that field, we cut the number of fields in half. And while this\n\n`HeaderDescriptor` is itself a struct, it\u2019s stored in header collections, and thus by cutting the size of the\n\n`HeaderDescriptor` in half, we can significantly reduce the size of those collections, especially when\nmany custom headers are involved.\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|GetHeaders|.NET 6.0|334.4 ns|1.00|664 B|1.00|\n|GetHeaders|.NET 7.0|213.9 ns|0.64|360 B|0.54|\n\nimplementation of this type were overly flexible, with a mechanism for custom validation of values,\nwhich entailed multiple fields for storing things like an `Action<>` callback to use for validation. But as\nit turns out in practice, that validation was only used for one specific consumer, and so rather than\nmaking everyone pay for the extra space that wasn\u2019t typically used, the validation was instead\nextracted out to just the call sites it was required.\n\nparsed host information, but only some of the callers needed this. Rather than making everyone pay\nfor something that not everyone needed, this logic was moved into only the call sites that needed it.\n\nOther small allocation improvements were also made outside of headers. For example, when new\nHTTP/1 and HTTP/2 connections are created, the implementation queues a work item to the thread\npool to handle the actual creation, primarily to escape locks that might be held higher in the call\n\njust be eaten, and the lambda being passed to `Task.Run` was closing over `this` and a local, which\nmeans the C# compiler will have generated code to allocate both a \u201cdisplay class\u201d (an object to store\nthe state being passed in) for the closure and then also a delegate to a method on that display class.\n\nboth superfluous allocations.\n\n186 CHAPTER 16 | Networking\n\nFolks using HTTP often need to go through a proxy server, and in .NET the ability to go through an\nHTTP proxy is represented via the `IWebProxy` interface; it has three members, `GetProxy` for getting the\n\nbypassed is determined by two things (assuming a non-null proxy `Uri` was provided): did the\nconstructor of the `WebProxy` specify that \u201clocal\u201d destinations should be bypassed (and if so, is this\ndestination local), or does this destination address match any of any number of regular expressions\nprovided. As it turns out, this latter aspect has been relatively slow and allocation-heavy in all previous\nreleases of .NET, for two reasons: every call to check whether an address was bypassed was recreating\na `Regex` instance for every supplied regular expression, and every call to check whether an address\nwas bypassed was deriving a new `string` from the `Uri` to use to match against the `Regex` . In .NET 7,\nboth of those issues have been fixed, yielding significant improvements if you rely on this regular\n[expression functionality. dotnet/runtime#73803 from](https://github.com/dotnet/runtime/pull/73803)\n\n[@onehourlate](https://github.com/onehourlate) changed the handling of the collection of these\n\n`Regex` instances. The problem was that `WebProxy` exposes an `ArrayList` (this type goes back to the\nbeginning of .NET and was created pre-generics), which the consumer could modify, and so `WebProxy`\nhad to assume the collection was modified between uses and addressed that by simply creating new\n\n`Regex` instances on every use; not good. Instead, this PR creates a custom `ArrayList` -derived type\nthat can track all relevant mutations, and then only if the collection is changed (which is incredibly\nrare, bordering on never) do the `Regex` [instances need to be recreated. And dotnet/runtime#73807](https://github.com/dotnet/runtime/pull/73807)\ntakes advantage of stack allocation and the `MemoryExtensions.TryWrite` method with string\ninterpolation to format the text into stack memory, avoiding the string allocation. This, combined with\n\nimprovements:\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|IsBypassed|.NET 6.0|5,343.2 ns|1.00|7528 B|1.00|\n|IsBypassed|.NET 7.0|205.5 ns|0.04|-|0.00|\n\nAlso related to HTTP, `WebUtility` \u2019s `HtmlDecode` method has improved for .NET 7. The implementation\nhad been manually iterating through each character in the input looking for a `'&'` to be unescaped.\nAny time you see such an open-coded loop looking for one or more specific characters, it\u2019s a red flag\nthat `IndexOf` [should be strongly considered. dotnet/runtime#70700](https://github.com/dotnet/runtime/pull/70700) deletes the entire searching\n\n187 CHAPTER 16 | Networking\n\n`IndexOfAny` in `HttpListener` \u2019s `HandleAuthentication` to search a header for certain kinds of\nwhitespace):\n\n|Method|Runtime|Mean|Ratio|\n|---|---|---|---|\n|HtmlDecode|.NET 6.0|245.54 ns|1.00|\n|HtmlDecode|.NET 7.0|19.66 ns|0.08|\n\nThere have been a myriad of other performance-related improvements in networking as well, such as\n[dotnet/runtime#67881](https://github.com/dotnet/runtime/pull/67881) which removed the use of `TcpClient` from `FtpWebRequest` ;\n[dotnet/runtime#68745](https://github.com/dotnet/runtime/pull/68745) in `WebSocket` which removed a parameter from one of the core async methods\n(and since parameters end up on the state machine, if the async method yields this results in fewer\n\nused and the runtime needs to be involved in the conversion, it\u2019s also much more heavyweight than\njust casting, which can be done when the native and managed layouts are bit-for-bit compatible. As\nwith the `u8` example earlier, this comparison is hardly fair, but that\u2019s exactly the point:\n\n188 CHAPTER 16 | Networking\n\n|Method|Mean|Ratio|\n|---|---|---|\n|PtrToStructure|26.6593 ns|1.000|\n|Cast|0.0736 ns|0.003|\n\nFor folks using `NegotiateStream` [, dotnet/runtime#71280](https://github.com/dotnet/runtime/pull/71280) from\n\n[@filipnavara](https://github.com/filipnavara) will also be very welcome (this comes as part of a larger\n[effort, primarily in dotnet/runtime#71777 from [@filipnavara](https://github.com/filipnavara) and](https://github.com/dotnet/runtime/pull/71777)\n\nhandshake by reusing a buffer rather than reallocating a new buffer for each of multiple phases of the\nhandshake:\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|Handshake|.NET 6.0|1.905 ms|1.00|240.5 KB|1.00|\n|Handshake|.NET 7.0|1.913 ms|1.00|99.28 KB|0.41|\n\n189 CHAPTER 16 | Networking\n\n**CHAPTER**"}, "17": {"JSON": "`System.Text.Json` was introduced in .NET Core 3.0, and has seen a significant amount of investment\n[in each release since. .NET 7 is no exception. New features in .NET 7 include support for customizing](https://github.com/dotnet/runtime/issues/63686)\n[contracts,](https://github.com/dotnet/runtime/issues/63686) [polymorphic serialization, support for required members,](https://github.com/dotnet/runtime/pull/67961) [support for DateOnly / TimeOnly,](https://github.com/dotnet/runtime/pull/69160)\n[support for IAsyncEnumerable<T> and JsonDocument](https://github.com/dotnet/runtime/pull/68985) [in source generation, and support for](https://github.com/dotnet/runtime/issues/44947)\n[configuring MaxDepth in JsonWriterOptions. However, there have also been new features specifically](https://github.com/dotnet/runtime/issues/44947)\nfocused on performance, and other changes about improving performance of JSON handling in a\nvariety of scenarios.\n\nOne of the biggest performance pitfalls we\u2019ve seen developers face with `System.Text.Json` has to do\nwith how the library caches data. In order to achieve good serialization and deserialization\nperformance when the source generator isn\u2019t used, `System.Text.Json` uses reflection emit to\ngenerate custom code for reading/writing members of the types being processed. Instead of then\nhaving to pay reflection invoke costs on every access, the library incurs a much larger one-time cost\nper type to perform this code generation, but then all subsequent handling of these types is very\nfast\u2026 assuming the generated code is available for use. These generated handlers need to be stored\nsomewhere, and the location that\u2019s used for storing them is them is `JsonSerializerOptions` . The\nidea was intended to be that developers would instantiate an options instance once and pass it\naround to all of their serialization/deserialization calls; thus, state like these generated handlers could\nbe cached on them. And that works well when developers follow the recommended model. But when\nthey don\u2019t, performance falls off a cliff, and hard. Instead of \u201cjust\u201d paying for the reflection invoke\ncosts, each use of a new `JsonSerializerOptions` ends up re-generating via reflection emit those\nhandlers, skyrocketing the cost of serialization and deserialization. A super simple benchmark makes\nthis obvious:\n\n190 CHAPTER 17 | JSON\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|ImplicitOptions|.NET 6.0|170.3 ns|1.00|200 B|1.00|\n|WithCached|.NET 6.0|163.8 ns|0.96|200 B|1.00|\n|WithoutCached|.NET 6.0|100,440.6 ns|592.48|7393 B|36.97|\n\n[In .NET 7, this was fixed in dotnet/runtime#64646](https://github.com/dotnet/runtime/pull/64646) (and subsequently tweaked in\n\nreflection emit, those are also cached at the global level (with appropriate removal when no longer\nused in order to avoid unbounded leaks).\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|ImplicitOptions|.NET 6.0|170.3 ns|1.00|200 B|1.00|\n|ImplicitOptions|.NET 7.0|166.8 ns|0.98|48 B|0.24|\n|WithCached|.NET 6.0|163.8 ns|0.96|200 B|1.00|\n|WithCached|.NET 7.0|168.3 ns|0.99|48 B|0.24|\n|WithoutCached|.NET 6.0|100,440.6 ns|592.48|7393 B|36.97|\n|WithoutCached|.NET 7.0|590.1 ns|3.47|337 B|1.69|\n\nAs can be seen here, it\u2019s still more expensive to create a new `JsonSerializerOptions` instance on\neach call, and the recommended approach is \u201cdon\u2019t do that.\u201d But if someone does do it, in this\nexample they\u2019re only paying 3.6x the cost rather than 621x the cost, a huge improvement.\n[dotnet/runtime#61434](https://github.com/dotnet/runtime/pull/61434) also now exposes the `JsonSerializerOptions.Default` instance that\u2019s used\nby default if no options are explicitly provided.\n\nAnother change to `JsonSerializer` [came in dotnet/runtime#72510, which slightly improved the](https://github.com/dotnet/runtime/issues/72510)\nperformance of serialization when using the source generator. The source generator emits helpers for\nperforming the serialization/deserialization work, and these are then invoked by `JsonSerializer` via\ndelegates (as part of abstracting away all the different implementation strategies for how to get and\nset members on the types being serialized and deserialized). Previously, these helpers were being\nemitted as static methods, which in turn meant that the delegates were being created to static\nmethods. Delegates to instance methods are a bit faster to invoke than delegates to static methods,\nso this PR made a simple few-line change for the source generator to emit these as instance methods\ninstead.\n\nfew extra objects and an extra couple of hundred bytes just for these helper data structures. To\naddress that, this PR takes advantage of `[ThreadStatic]`, which can be put onto static fields to make\nthem per-thread rather than per-process. From whatever thread is performing the (synchronous)\n\n191 CHAPTER 17 | JSON\n\n`Serialize` operation, it then ensures the current thread has a `Utf8JsonWriter` and `IBufferWriter`\ninstance it can use, and uses them; for the most part this is straightforward, but it needs to ensure that\nthe serialization operation itself doesn\u2019t try to recursively serialize, in which case these objects could\nend up being used erroneously while already in use. It also needs to make sure that the pooled\n\nmaking use of the pool, not sequestered off in this cached `IBufferWriter` implementation. This\noptimization is also only really meaningful for small object graphs being serialized, and only applies to\nthe synchronous operations (asynchronous operations would require a more complicated pooling\nmechanism, since the operation isn\u2019t tied to a specific thread, and the overhead of such complication\nwould likely outweigh the modest gain this optimization provides).\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|SerializeToString|.NET 6.0|146.4 ns|1.00|200 B|1.00|\n|SerializeToString|.NET 7.0|137.5 ns|0.94|48 B|0.24|\n\n`Utf8JsonWriter` and `Utf8JsonReader` [also saw several improvements directly. dotnet/runtime#69580](https://github.com/dotnet/runtime/pull/69580)\nadds a few new performance-focused members, the `ValueIsEscaped` property (which exposes already\ntracked information and enables consumers to avoid the expense of re-checking) and the `CopyString`\nmethod (which provides a non-allocating mechanism to get access to a string value from the reader).\nIt then also uses the added support internally to speed up certain operations on `Utf8JsonReader` . And\n[dotnet/runtime#63863,](https://github.com/dotnet/runtime/pull/63863) [dotnet/runtime#71534, and dotnet/runtime#61746 fix how some exception](https://github.com/dotnet/runtime/pull/71534)\nchecks and throws were being handled so as to not slow down the non-exceptional fast paths.\n\n192 CHAPTER 17 | JSON\n\n**CHAPTER**"}, "18": {"XML": "System.Xml is used by a huge number of applications and services, but ever since JSON hit the scene\nand has been all the rage, XML has taken a back seat and thus hasn\u2019t seen a lot of investment from\neither a functionality or performance perspective. Thankfully, System.Xml gets a bit of performance\nlove in .NET 7, in particular around reducing allocation on some commonly used code paths.\n\nSometimes a performance fix is as easy as changing a single number. That\u2019s the case with\n[dotnet/runtime#63459](https://github.com/dotnet/runtime/pull/63459) from [@chrisdcmoore](https://github.com/chrisdcmoore), which addresses a\nlong-standing issue with the asynchronous methods on the popular `XmlReader` . When `XmlReader` was\noriginally written, whoever developed it chose a fairly common buffer size to be used for read\noperations, namely 4K or 8K `char` s depending on various conditions. When `XmlReader` later gained\nasynchronous reading functionality, for whatever reason a much, much larger buffer size of 64K `char` s\nwas selected (presumably in hopes of minimizing the number of asynchronous operations that would\nneed to be employed, but the actual rationale is lost to history). A key problem with such a buffer size,\nbeyond it leading to a lot of allocation, is the allocation it produces typically ends up on the Large\nObject Heap (LOH). By default, under the expectation that really large objects are long-lived, objects\ngreater than 85K bytes are allocated into the LOH, which is treated as part of Gen 2, and that makes\nsuch allocation if _not_ long-lived even more expensive in terms of overall impact on the system. Well,\n\nmuch memory needs to be zero\u2019d, etc). While it\u2019s still a very large allocation, and in the future we\ncould look at pooling the buffer or employing a smaller one (e.g. no different from what\u2019s done for\nthe synchronous APIs), this simple one-number change alone makes a substantial difference for\nshorter input documents (while not perceivably negatively impacting larger ones).\n\n193 CHAPTER 18 | XML\n\n|Method|Runtime|ItemCount|Mean|Ratio|Allocated|Alloc<br>Ratio|\n|---|---|---|---|---|---|---|\n|XmlReader_ReadAsync|.NET 6.0|10|42.344 us|1.00|195.94 KB|1.00|\n|XmlReader_ReadAsync|.NET 7.0|10|9.992 us|0.23|99.94 KB|0.51|\n||||||||\n|XmlReader_ReadAsync|.NET 6.0|1000000|340,382.953<br>us|1.00|101790.34<br>KB|1.00|\n|XmlReader_ReadAsync|.NET 7.0|1000000|333,417.347<br>us|0.98|101804.45<br>KB|1.00|\n\nthese cases were stylistic, converting something like `string1 + \":\" + string2` into\n\n`$\"{string1}:{string2}\"` ; I say stylistic here because the C# compiler will generate the exact same\ncode for both of those, a call to `string.Concat(string1, \":\", string2)`, given that there\u2019s a\n\n`Concat` overload that accepts three strings. However, some of the changes do impact allocation. For\nexample, the private `XmlTextWriter.GeneratePrefix` method had the code:\n\nwhere `_top` and `temp` are both `int` s. This will result in allocating two temporary strings and then\nconcatenating those with the two constant strings. Instead, the PR changed it to:\n\n```\nreturn string.Create(CultureInfo.InvariantCulture, $\"d{_top:d}p{temp:d}\");\n\n```\n\nwhich while shorter is also more efficient, avoiding the intermediate string allocations, as the custom\ninterpolated string handler used by `string.Create` will format those into a pooled buffer rather than\nallocating intermediate temporaries.\n\nusing reflection emit to dynamically generate IL at run-time that are tuned to the specific shape of the\n[types being serialized/deserialized, and the XML Serializer Generator Tool](https://docs.microsoft.com/dotnet/standard/serialization/xml-serializer-generator-tool-sgen-exe) (sgen), which generates a\n.dll containing the same support, just ahead-of-time (a sort-of precursor to the Roslyn source\n\n194 CHAPTER 18 | XML\n\ngenerators we love today). In both cases, when deserializing, the generated code wants to track which\nproperties of the object being deserialized have already been set, and to do that, it uses a `bool[]` as a\nbit array. Every time an object is deserialized, it allocates a `bool[]` with enough elements to track\nevery member of the type. But in common usage, the vast majority of types being deserialized only\nhave a relatively small number of properties, which means we can easily use stack memory to track\n\ninto a `Span<bool>` so that the subsequent code paths simply use a span instead of an array). You can\nsee this quite easily in the .NET Object Allocation Tracking tool in Visual Studio. For this console app\n(which, as an aside, shows how lovely the new raw string literals feature in C# is for working with\nXML):\n\nHere\u2019s what I see when I run this under .NET 6:\n\n195 CHAPTER 18 | XML\n\nWe\u2019re running a thousand deserializations, each of which will deserialize 10 `Release` instances, and so\nwe expect to see 10,000 `Release` objects being allocated, which we do\u2026 but we also see 10,000\n\n`bool[]` being allocated. Now with .NET 7 (note the distinct lack of the per-object `bool[]` ):\n\nOther allocation reduction went into the creation of the serializer/deserializer itself, such as with\n[dotnet/runtime#68738](https://github.com/dotnet/runtime/pull/68738) avoiding allocating strings to escape text that didn\u2019t actually need escaping,\n\n[cache of serializers previously created, dotnet/runtime#67001](https://github.com/dotnet/runtime/pull/67001) from\n\n[@TrayanZapryanov](https://github.com/TrayanZapryanov) caching an array used with `string.Split`,\n[and dotnet/runtime#67002 from [@TrayanZapryanov](https://github.com/TrayanZapryanov) that](https://github.com/dotnet/runtime/pull/67002)\nchanged some parsing code to avoid a `string.ToCharArray` invocation.\n\nthis is a fairly common performance-focused replacement to do, but you need to be careful as\n\n`Hashtable` has a few behavioral differences from `Dictionary<,>` ; beyond the obvious difference of\n\n[@TrayanZapryanov](https://github.com/TrayanZapryanov) uses stack-based memory and pooling to\n\n196 CHAPTER 18 | XML\n\navoid temporary string allocation in the implementation of the internal `XsdDateTime` and\n\n`XsdDuration` types, which are used by the public `XmlConvert` .\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|XmlConvertToString|.NET 6.0|90.70 ns|1.00|184 B|1.00|\n|XmlConvertToString|.NET 7.0|59.21 ns|0.65|40 B|0.22|\n\nXML pops up in other areas as well, as in the `XmlWriterTraceListener` type. While the\n\n`System.Diagnostics.Trace` type isn\u2019t the recommended tracing mechanism for new code, it\u2019s widely\nused in existing applications, and `XmlWriterTraceListener` let\u2019s you plug in to that mechanism to\n[write out XML logs for traced information. dotnet/runtime#66762](https://github.com/dotnet/runtime/pull/66762) avoids a bunch of string allocation\noccurring as part of this tracing, by formatting much of the header information into a span and then\nwriting that out rather than `ToString()` \u2019ing each individual piece of data.\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|TraceWrite|.NET 6.0|961.9 ns|1.00|288 B|1.00|\n|TraceWrite|.NET 7.0|772.2 ns|0.80|64 B|0.22|\n\n197 CHAPTER 18 | XML\n\n**CHAPTER**"}, "19": {"Cryptography": "Some fairly significant new features came to `System.Security.Cryptography` in .NET 7, including the\n[support necessary to enable the previously discussed OCSP stapling and support for building](https://github.com/dotnet/runtime/pull/72708)\n[certificate revocation lists, but there was also a fair amount of effort put into making existing support](https://github.com/dotnet/runtime/pull/72708)\nfaster and more lightweight.\n\n[One fairly substantial change in .NET 7 is split across dotnet/runtime#61025,](https://github.com/dotnet/runtime/pull/61025) [dotnet/runtime#61137,](https://github.com/dotnet/runtime/pull/61137)\n[and dotnet/runtime#64307. These PRs don\u2019t change any code materially, but instead consolidate all of](https://github.com/dotnet/runtime/pull/64307)\nthe various cryptography-related assemblies in the core libraries into a single\n\n`System.Security.Cryptography` assembly. When .NET Core was first envisioned, a goal was to make\nit extremely modular, and large swaths of code were teased apart to create many smaller assemblies.\nFor example, cryptographic functionality was split between\n\nshared framework folder for a previous release, e.g. here\u2019s mine for .NET 6:\n\nThese PRs move all of that code into a single `System.Security.Cryptography.dll` assembly. This has\nseveral benefits. First, crypto is used in a huge number of applications, and most apps would end up\n\n198 CHAPTER 19 | Cryptography\n\nrequiring multiple (or even most) of these assemblies. Every assembly that\u2019s loaded adds overhead.\nSecond, a variety of helper files had to be compiled into each assembly, leading to overall larger\namount of compiled code to be distributed. And third, we weren\u2019t able to implement everything as\noptimal as we\u2019d have otherwise liked due to functionality in one assembly not exposed to another\n(and we avoid using `InternalsVisibleTo` as it hampers maintainability and impedes other analysis\nand optimizations). Now in .NET 7, the shared framework looks more like this:\n\nInteresting, you still see a bunch of assemblies there, but all except for\n\n`System.Security.Cryptography.dll` are tiny; that\u2019s because these are simple facades. Because we\nneed to support binaries built for .NET 6 and earlier running on .NET 7, we need to be able to handle\nbinaries that refer to types in these assemblies, but in .NET 7, those types actually live in\n\n[X, it now lives over there.\u201d And if you crack open one of these assemblies in a tool like ILSpy, you can](https://github.com/icsharpcode/ILSpy)\nsee they\u2019re essentially empty except for a bunch of these attributes:\n\n199 CHAPTER 19 | Cryptography\n\nIn addition to the startup and maintenance wins this provides, this has also enabled further\nsubsequent optimization. For example, there\u2019s a lot of object cloning that goes on in the innards of\nthis library. Various objects are used to wrap native handles to OS cryptographic resources, and to\nhandle lifetime semantics and ownership appropriately, there are many cases where a native handle is\nduplicated and then wrapped in one or more new managed objects. In some cases, however, the\noriginal resource is then destroyed because it\u2019s no longer needed, and the whole operation could\nhave been made more efficient if the original resource just had its ownership transferred to the new\nobjects rather than being duplicated and destroyed. This kind of ownership transfer typically is hard to\ndo between assemblies as it generally requires public API that\u2019s not focused on such usage patterns,\n\npublic types.\n\nIn terms of actual code improvements, there are many. One category of improvements is around\n\u201cone-shot\u201d operations. With many forms of data processing, all of the data needn\u2019t be processed in\none operation. A block of data can be processed, then another, then another, until finally there\u2019s no\nmore data to be processed. In such usage, there\u2019s often some kind of state carried over from the\nprocessing of one block to the processing of the next, and then the processing of the last block is\nspecial as it needn\u2019t carry over anything and instead needs to perform whatever work is required to\nend the whole operation, e.g. outputting any final footer or checksum that might be required as part\nof the format. Thus, APIs that are able to handle arbitrary number of blocks of data are often a bit\nmore expensive in one way, shape, or form than APIs that only support a single input; this latter\ncategory is known as \u201cone shot\u201d operations, because they do everything in \u201cone shot.\u201d In some cases,\none-shot operations can be significantly cheaper, and in other cases they merely avoid some\nallocations that would have been necessary to transfer state from the processing of one block of data\n[to the next. dotnet/runtime#58270 from [@vcsjones](https://github.com/vcsjones) and](https://github.com/dotnet/runtime/pull/58270)\n[dotnet/runtime#65725](https://github.com/dotnet/runtime/pull/65725) from [@vcsjones](https://github.com/vcsjones) both improved the\nperformance of various one-shot operations on \u201csymmetric\u201d cryptograhic algorithms (algorithms that\nuse the same key information to both encrypt and decrypt), like AES. The former does so by\nrefactoring the implementations to avoid some reset work that\u2019s not necessary in the case of oneshots because the relevant state is about to go away, anyway, and that in turns also allows the\nimplementation to store less of certain kinds of state. The latter does so for decryption one-shots by\ndecrypting directly into the destination buffer whenever possible, using stack space if possible when\ngoing directly into the user\u2019s buffer isn\u2019t feasible, etc.\n\n200 CHAPTER 19 | Cryptography\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|OneShot|.NET 6.0|1.828 us|1.00|336 B|1.00|\n|OneShot|.NET 7.0|1.770 us|0.97|184 B|0.55|\n\nIn addition to making one-shots lighterweight, other PRs have then used these one-shot operations in\nmore places in order to simplify their code and benefit from the increased performance, e.g.\n[dotnet/runtime#70639](https://github.com/dotnet/runtime/pull/70639) [from [@vcsjones](https://github.com/vcsjones), dotnet/runtime#70857 from](https://github.com/dotnet/runtime/pull/70857)\n\n[[@vcsjones](https://github.com/vcsjones), dotnet/runtime#64005](https://github.com/dotnet/runtime/pull/64005) from\n\n[[@vcsjones](https://github.com/vcsjones), and dotnet/runtime#64174 from](https://github.com/dotnet/runtime/pull/64174)\n\n[@vcsjones](https://github.com/vcsjones).\n\nThere\u2019s also a large number of PRs that have focused on removing allocations from around the crypto\nstack:\n\n['**Stack allocation** . As has been seen in many other PRs referenced throughout this post, using\\n`stackalloc` is a very effective way to get rid of array allocations in many situations. It\u2019s used\\neffectively in multiple crypto PRs to avoid either temporary or pooled array allocations, such as\\n[in dotnet/runtime#64584 from [@vcsjones](https://github.com/vcsjones), dotnet/runtime#69831](https://github.com/dotnet/runtime/pull/64584)\\n[from [@vcsjones](https://github.com/vcsjones), dotnet/runtime#70173 from](https://github.com/dotnet/runtime/pull/70173)']\n\n[[@vcsjones](https://github.com/vcsjones), dotnet/runtime#69812](https://github.com/dotnet/runtime/pull/69812) from\n\n[[@vcsjones](https://github.com/vcsjones), and dotnet/runtime#69448 from](https://github.com/dotnet/runtime/pull/69448)\n\n[@vcsjones](https://github.com/vcsjones). Sometimes this is used when calling an API that has\nmultiple overloads, including one taking an array and one taking a span. Othertimes it\u2019s used\nwith P/Invokes that often just pass out a small amount of data. Sometimes it\u2019s used to avoid\ntemporary array allocations, and sometimes it\u2019s used in places where pooling was used\npreviously, but the data is often small enough to avoid even the overheads of pooling.\n\n['**Avoiding double copies** . Most of the crypto APIs that accept `byte[]` s and store them end up\\nmaking defensive copies of those arrays rather than storing the original. This is fairly common\\nthroughout .NET, but it\u2019s especially common in the crypto stack, where the ability to trust the\\ndata is as you expect it (and validate it) is paramount. In some cases, though, code ends up\\nallocating a temporary `byte[]` just to pass data into one of these APIs that copies and re[allocates, anyway. dotnet/runtime#71102 from [@vcsjones](https://github.com/vcsjones),](https://github.com/dotnet/runtime/pull/71102)\\n[dotnet/runtime#69024 from [@vcsjones](https://github.com/vcsjones), dotnet/runtime#71015](https://github.com/dotnet/runtime/pull/69024)\\n[from [@vcsjones](https://github.com/vcsjones), and dotnet/runtime#69534 from](https://github.com/dotnet/runtime/pull/69534)']\n\n[@vcsjones](https://github.com/vcsjones) deal with that duplication in some cases by extracting\na span to the original data instead of creating a temporary `byte[]` ; when that span is passed into\nthe target API, the target API still makes a copy, but we\u2019ve avoided the first one and thus cut the\n[array allocation for these operations effectively in half. dotnet/runtime#71888](https://github.com/dotnet/runtime/pull/71888) from\n\n[@vcsjones](https://github.com/vcsjones) is a variation on this theme, improving the internals of\n\n201 CHAPTER 19 | Cryptography\n\n`Rfc2898DeriveBytes` to supports spans such that its constructors that accept spans can then do\nthe more efficient thing.\n\ncomplexity is misleading. Yes, these provide very efficient searching, but there\u2019s still overhead\nassociated with computing a hash code, mapping that hash code to a location in the data\n\nstructure may not even be needed at all: the search can just be open-coded as a waterfall of\n[if/elseif/else constructs. That\u2019s the case in a PR like dotnet/runtime#71341](https://github.com/dotnet/runtime/pull/71341) from\n\n[@vcsjones](https://github.com/vcsjones), where the 99.999% case involves just five strings\n(names of hash algorithms); it\u2019s cheaper to just compare against each than it is do a\n`HashSet<>.Contains`, especially since the JIT now unrolls and vectorizes the comparison against\nthe constant string names.\n\n['**Simply avoiding unnecessary work** . The best optimizations are ones where you simply stop\\n[doing work you don\u2019t have to do. dotnet/runtime#68553](https://github.com/dotnet/runtime/pull/68553) from']\n\n[@vcsjones](https://github.com/vcsjones) is a good example of this. This code was performing a\nhash of some data in order to determine the length of resulting hashes for that particular\nconfiguration, but we actually know ahead of time exactly how long a hash for a given algorithm\nis going to be, and we already have in this code a cascading if/elseif/else that\u2019s checking for each\n[known algorithm, so we can instead just hardcode the length for each. dotnet/runtime#70589](https://github.com/dotnet/runtime/pull/70589)\nfrom [@vcsjones](https://github.com/vcsjones) is another good example, in the same spirit of\nthe ownership transfer example mentioned earlier (but this one didn\u2019t previously span assembly\nboundaries). Rather than in several places taking an `X509Extension`, serializing it to a `byte[]`,\nand passing that temporary `byte[]` to something else that in turn makes a defensive copy, we\ncan instead provide an internal pathway for ownership transfer, bypassing all of the middle\n[stages. Another good one is dotnet/runtime#70618](https://github.com/dotnet/runtime/pull/70618) from\n\n[@vcsjones](https://github.com/vcsjones), as it\u2019s an example of how it pays to really understand\nyour dependencies. The implementation of symmetric encryption on macOS uses the\nCommonCrypto library. One of the functions it exposes is `CCCryptorFinal`, which is used at the\nend of the encryption/decryption process. However, there are several cases called out in the\ndocs where it\u2019s unnecessary (\u201csuperfluous,\u201d according to the docs), and so our dutifully calling it\neven in those situations is wasteful. The fix? Stop doing unnecessary work.\n\n['**New APIs** . A bunch of new APIs were introduced for cryptography in .NET 7. Most are focused']\n\n[some are focused squarely on performance. dotnet/runtime#57835](https://github.com/dotnet/runtime/pull/57835) from\n\ncall (again a defensive copy to avoid having to deal with the possiblity that the consumer\n\nenables accessing this data freely without additional allocation.\n\n202 CHAPTER 19 | Cryptography\n\n**CHAPTER**"}, "20": {"Diagnostics": "Let\u2019s turn our attention to System.Diagnostics, which encompasses types ranging from process\nmanagement to tracing.\n\nThe `Process` class is used for a variety of purposes, including querying information about running\nprocesses, interacting with other processes (e.g. being notified of their exiting), and launching\nprocesses. The performance of querying for information in particular had some notable improvements\n\nfor each. It turns out that previous releases of .NET were loading the full information (e.g. all of its\nthreads) about every `Process` on the machine in order to filter down to just those with the target\n[name. dotnet/runtime#68705 fixes that by only loading the name for a process rather than all of the](https://github.com/dotnet/runtime/pull/68705)\ninformation for it. While this helps a bit with throughput, it helps a ton with allocation:\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|GetProcessesByName|.NET 6.0|2.287 ms|1.00|447.86 KB|1.000|\n|GetProcessesByName|.NET 7.0|2.086 ms|0.90|2.14 KB|0.005|\n\na single member of one of those lazy-loaded `Process` instances triggers loading all of the data for it,\nas the information is all available as part of the same native operation, e.g. on Windows using\n\n`NtQuerySystemInformation` and on Linux reading from `/proc/pid/stat` and `/proc/pid/status` . But\nin some cases we can be more fine-grained about it, using APIs that serve up a subset of the data\n\n203 CHAPTER 20 | Diagnostics\n\nprocess\u2019 name, it\u2019s a huge boost in throughput, and even if you subsequently go on to read additional\nstate from the `Process` and force it to load everything else, accessing the process name is so fast that\nit doesn\u2019t add meaningful overhead to the all-up operation. This is visible in this benchmark:\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|GetCurrentProcessName|.NET 6.0|3,070.54 us|1.00|3954 B|1.00|\n|GetCurrentProcessName|.NET 7.0|32.30 us|0.01|456 B|0.12|\n|||||||\n|GetCurrentProcessNameAndWorkingSet|.NET 6.0|3,055.70 us|1.00|4010 B|1.00|\n|GetCurrentProcessNameAndWorkingSet|.NET 7.0|3,149.92 us|1.03|4186 B|1.04|\n\nInterestingly, this PR had a small deficiency we didn\u2019t initially catch, which is that the\n\n`QueryFullProcessImageName` API we switched to didn\u2019t work in the case of elevated/privileged\n[processes. To accomodate those, dotnet/runtime#70073 from](https://github.com/dotnet/runtime/pull/70073)\n\n[@schuettecarsten](https://github.com/schuettecarsten) updated the code to keep both the new and\nold implementations, starting with the new one and then only falling back to the old if operating on\nan incompatible process.\n\n[directly into the native memory already being allocated. dotnet/runtime#71136 simplifies and](https://github.com/dotnet/runtime/pull/71136)\nstreamlines the code involved in getting the \u201cshort name\u201d of a process on Windows for use in\n[comparing process names. And dotnet/runtime#45690](https://github.com/dotnet/runtime/pull/45690) replaces a custom cache with use of\n\n`ArrayPool` in the Windows implementation of getting all process information, enabling effective reuse\nof the array that ends up being used rather than having it sequestered off in the `Process`\nimplementation forever.\n\nAnother area of performance investment has been in `DiagnosticSource`, and in particular around\nenumerating through data from `Activity` instances. This work translates into faster integration and\n[interoperability via OpenTelemetry, in order to be able to export data from .NET](https://devblogs.microsoft.com/dotnet/opentelemetry-net-reaches-v1-0) `Activity` information\n[faster. dotnet/runtime#67012 from [@CodeBlanch](https://github.com/CodeBlanch), for example,](https://github.com/dotnet/runtime/pull/67012)\nimproved the performance of the internal `DiagLinkedList<T>.DiagEnumerator` type that\u2019s the\n\n204 CHAPTER 20 | Diagnostics\n\nenumerator returned when enumerating `Activity.Links` and `Activity.Events` by avoiding a copy\nof each `T` value:\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|EnumerateActivityLinks|.NET 6.0|19.62 us|1.00|64 B|1.00|\n|EnumerateActivityLinks|.NET 7.0|13.72 us|0.70|32 B|0.50|\n\n[Then dotnet/runtime#67920 from [@CodeBlanch](https://github.com/CodeBlanch) and](https://github.com/dotnet/runtime/pull/67920)\n\n205 CHAPTER 20 | Diagnostics\n\n|Method|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|\n|EnumerateActivityLinks_Old|13.655 us|1.00|32 B|1.00|\n|EnumerateActivityLinks_New|2.380 us|0.17|-|0.00|\n\nOf course, when it comes to diagnostics, anyone who\u2019s ever done anything with regards to timing and\nmeasurements is likely familiar with good ol\u2019 `Stopwatch` . `Stopwatch` is a simple type that\u2019s very handy\nfor getting precise measurements and is thus used all over the place. But for folks that are really costsensitive, the fact that `Stopwatch` is a class can be prohibitive, e.g. writing:\n\nis easy, but allocates a new object just to measure. To address this, `Stopwatch` has for years exposed\n\nthat last mile of performance can write:\n\nwhich avoids the allocation and saves a few cycles:\n\n206 CHAPTER 20 | Diagnostics\n\n|Method|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|\n|Old|32.90 ns|1.00|40 B|1.00|\n|New|26.30 ns|0.80|-|0.00|\n\n207 CHAPTER 20 | Diagnostics\n\n**CHAPTER**"}, "21": {"Exceptions": "It might be odd to see the subject of \u201cexceptions\u201d in a post on performance improvements. After all,\nexceptions are by their very nature meant to be \u201cexceptional\u201d (in the \u201crare\u201d sense), and thus wouldn\u2019t\ntypically contribute to fast-path performance. Which is a good thing, because fast-paths that throw\nexceptions in the common case are no longer fast: throwing exceptions is quite expensive.\n\nInstead, one of the things we _do_ concern ourselves with is how to minimize the impact of checking for\nexceptional conditions: the actual exception throwing may be unexpected and slow, but it\u2019s super\ncommon to need to check for those unexpected conditions, and that checking should be very fast. We\nalso want such checking to minimally impact binary size, especially if we\u2019re going to have many such\nchecks all over the place, in generic code for which we end up with many copies due to generic\nspecialization, in functions that might be inlined, and so on. Further, we don\u2019t want such checks to\nimpede other optimizations; for example, if I have a small function that wants to do some argument\nvalidation and would otherwise be inlineable, I likely don\u2019t want the presence of exception throwing to\ninvalidate the possibility of inlining.\n\nBecause of all of that, high-performance libraries often come up with custom \u201cthrow helpers\u201d they use\nto achieve their goals. There are a variety of patterns for this. Sometimes a library will just define its\nown static method that handles constructing and throwing an exception, and then call sites do the\ncondition check and delegate to the method if throwing is needed:\n\nThis keeps the IL associated with the throwing out of the calling function, minimizing the impact of\nthe throw. That\u2019s particularly valuable when additional work is needed to construct the exception, e.g.\n\nOther times, libraries will encapsulate both the checking and throwing. This is exactly what the\nArgumentNullException.ThrowIfNull method that was added in .NET 6 does:\n\n208 CHAPTER 21 | Exceptions\n\nWith that, callers benefit from the concise call site:\n\nthe IL remains concise, and the assembly generated for the JIT will include the streamlined condition\ncheck from the inlined `ThrowIfNull` but won\u2019t inline the `Throw` helper, resulting in effectively the same\ncode as if you\u2019d written the previously shown manual version with `ThrowArgumentNullException`\nyourself. Nice.\n\nWhenever we introduce new public APIs in .NET, I\u2019m particularly keen on seeing them used as widely\nas possible. Doing so serves multiple purposes, including helping to validate that the new API is\nusable and fully addresses the intended scenarios, and including the rest of the codebase benefiting\nfrom whatever that API is meant to provide, whether it be a performance improvement or just a\nreduction in routinely written code. In the case of `ArgumentNullException.ThrowIfNull`, however, I\npurposefully put on the brakes. We used it in .NET 6 in several dozen call sites, but primarily just in\nplace of custom `ThrowIfNull` -like helpers that had sprung up in various libraries around the runtime,\neffectively deduplicating them. What we didn\u2019t do, however, was replace the literally thousands of null\nchecks we have with calls to `ArgumentNullException.ThrowIfNull` . Why? Because the new `!!` C#\nfeature was right around the corner, destined for C# 11.\n\nFor those unaware, the `!!` feature enabled putting `!!` onto parameter names in member signatures,\ne.g.\n\nThe C# compiler then compiled that as equivalent to:\n\n(albeit using its own `ThrowIfNull` helper injected as internal into the assembly). Armed with the new\n\nfrom Kung Fu Panda, \u201cOne often meets his destiny on the road he takes to avoid it\u201d? The presence of\nthat initial PR kicked off an unprecedented debate about the `!!` feature, with many folks liking the\nconcept but a myriad of different opinions about exactly how it should be exposed, and in the end,\n\n209 CHAPTER 21 | Exceptions\n\nAPIs internally. Interestingly, while we expected a peanut-buttery effect of slight perf improvements in\nmany places, our performance auto-analysis system flagged several performance improvements (e.g.\n[dotnet/perf-autofiling-issues#3531) as stemming from these changes, in particular because it enabled](https://github.com/dotnet/perf-autofiling-issues/issues/3531)\nthe JIT\u2019s inlining heuristics to flag more methods for inlining.\n\nWith the success of `ArgumentNullException.ThrowIfNull` and along with its significant roll-out in\n[.NET 7, .NET 7 also sees the introduction of several more such throw helpers. dotnet/runtime#61633,](https://github.com/dotnet/runtime/pull/61633)\n\n[(tweaked by dotnet/runtime#71544 to help ensure it\u2019s inlineable), which is then used at over a](https://github.com/dotnet/runtime/pull/71544)\n[hundred additional call sites by dotnet/runtime#71546.](https://github.com/dotnet/runtime/pull/71546)\n\n210 CHAPTER 21 | Exceptions\n\n**CHAPTER**"}, "22": {"Registry": "On Windows, the Registry is a database provided by the OS for applications and the system itself to\nload and store configuration settings. Practically every application accesses the registry. I just tried a\nsimple console app:\n\n```\nConsole.WriteLine(\"Hello, world\");\n\n```\n\nbuilt it as release, and then ran the resulting .exe. That execution alone triggered 64 `RegQueryValue`\n[operations (as visible via SysInternals\u2019 Process Monitor](https://docs.microsoft.com/sysinternals/downloads/procmon) tool). The core .NET libraries even access the\nregistry for a variety of purposes, such as for gathering data for `TimeZoneInfo`, gathering data for\n\nIt\u2019s thus beneficial to streamline access to registry data on Windows, in particular for reducing\n\nrequired in order to store the value for the key. The implementation would then allocate a buffer of\nthe appropriate size and call `RegQueryValueEx` again, and for values that are to be returned as strings,\nwould then allocate a string based on the data in that buffer. This PR instead recognizes that the vast\n\nwas sufficiently large, we no longer have to make a second system call to retrieve the actual data: we\nalready got it. If the buffer was too small, we rent an `ArrayPool` buffer of sufficient size and use that\npooled buffer for the subsequent `RegQueryValueEx` call. Except in situations where we actually need\n\nthe resulting string.\n\n211 CHAPTER 22 | Registry\n\n|Method|Runtime|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|---|\n|RegSz|.NET 6.0|6.266 us|1.00|200 B|1.00|\n|RegSz|.NET 7.0|3.182 us|0.51|96 B|0.48|\n\n212 CHAPTER 22 | Registry\n\n**CHAPTER**"}, "23": {"Analyzers": "The ability to easily plug custom code, whether for analyzers or source generators, into the Roslyn\ncompiler is one of my favorite features in all of C#. It means the developers working on C# don\u2019t need\nto be solely responsible for highlighting every possible thing you might want to diagnose in your\ncode. Instead, library authors can write their own analyzers, ship them either in dedicated nuget\npackages or as side-by-side in nuget packages with APIs, and those analyzers augment the compiler\u2019s\nown analysis to help developers write better code. We ship a large number of analyzer rules in the\n.NET SDK, many of which are focused on performance, and we augment that set with more and more\nanalyzers every release. We also work to apply more and more of those rules against our own\ncodebases in every release. .NET 7 is no exception.\n\n[One of my favorite new analyzers was added in dotnet/roslyn-analyzers#5594](https://github.com/dotnet/roslyn-analyzers/pull/5594) from\n\n[[@NewellClark](https://github.com/NewellClark) (and tweaked in dotnet/roslyn-analyzers#5972). In](https://github.com/dotnet/roslyn-analyzers/pull/5972)\n[my .NET 6 performance](https://devblogs.microsoft.com/dotnet/performance-improvements-in-net-6) post, I talked about some of the overheads possible when types aren\u2019t sealed:\n\n['Virtual calls are more expensive than regular non-virtual invocation and generally can\u2019t be\\ninlined, since the JIT doesn\u2019t know what is the actual type of the instance and thus the actual\\ntarget of the invocation (at least not without assistance from PGO). But if the JIT can see that a\\nvirtual method is being invoked on a sealed type, it can devirtualize the call and potentially even\\ninline it.']\n\nhelper method in Corelib that not only checks for null and for direct equality with the specified\ntype, but also linearly walks the parent hierarchy of the type of the object being tested to see if it\nmight derive from the specified type.\n\nadditional validation may need to be performed to ensure the reference being written is\ncompatible with the concrete type of the array in question. But, if `A` in this example were sealed,\nnothing could derive from it, so storing objects into it doesn\u2019t require such covariance checks.\n\n['Spans shift this covariance check to their constructor; rather than performing the covariance\\ncheck on every write into the array, the check is performed when a span is being constructed\\nfrom an array, such that if you try to create a `new Span<A>(bArray)`, the ctor will throw an\\nexception. If `A` is sealed, the JIT is able to elide such a check as well.']\n\n213 CHAPTER 23 | Analyzers\n\nIt effectively would be impossible for an analyzer to be able to safely recommend sealing public types.\nAfter all, it has no knowledge of the type\u2019s purpose, how it\u2019s intended to be used, and whether anyone\noutside of the assembly containing the type actually derives from it. But internal and private types are\nanother story. An analyzer _can_ actually see every possible type that could be deriving from a private\ntype, since the analyzer has access to the whole compilation unit containing that type, and it needn\u2019t\nworry about compatibility because anything that could derive from such a type necessarily must also\nbe non-public and would be recompiled right along with the base type. Further, with the exception of\nassemblies annotated as InternalsVisibleTo, an analyzer can have the same insight into internal types.\nThus, this PR adds CA1852, an analyzer that flags in non-InternalsVisibleTo assemblies all private and\ninternal types that aren\u2019t sealed and that have no types deriving from them and recommends they be\nsealed. (Due to some current limitations in the infrastructure around fixers and how this analyzer had\nto be written in order to be able to see all of the types in the assembly, the analyzer for CA1852\ndoesn\u2019t show up in Visual Studio. It can, however, be applied using the `dotnet format` tool. And if\nyou bump up the level of the rule from info to warning or error, it\u2019ll show up as part of builds as well.)\n\nIn .NET 6, we sealed over 2300 types, but even with that, this analyzer ended up finding more to seal.\n[dotnet/runtime#59941](https://github.com/dotnet/runtime/pull/59941) from [@NewellClark](https://github.com/NewellClark) sealed another ~70\n[types, and dotnet/runtime#68268 which enabled the rule as an warning in dotnet/runtime (which](https://github.com/dotnet/runtime/pull/68268)\nbuilds with warnings-as-errors) sealed another ~100 types. As a larger example of the rule in use,\nASP.NET hadn\u2019t done much in the way of sealing types in previous releases, but with CA1852 now in\n[the .NET SDK, dotnet/aspnetcore#41457 enabled the analyzer and sealed more than ~1100 types.](https://github.com/dotnet/aspnetcore/pull/41457)\n\n[Another new analyzer, CA1854, was added in dotnet/roslyn-analyzers#4851](https://github.com/dotnet/roslyn-analyzers/pull/4851) from\n\n`ContainsKey` is used to determine whether a dictionary contains a particular entry, and then if it does,\nthe dictionary\u2019s indexer is used to retrieve the value associated with the key, e.g.\n\nDictionary\u2019s `TryGetValue` method already combines both of these operations, both looking up the\nkey and retrieving its value if it exists, doing so as a single operation:\n\nA benefit of this, in addition to arguably being simpler, is that it\u2019s also faster. While `Dictionary<TKey,`\n\n`TValue>` provides very fast lookups, and while the performance of those lookups has gotten faster\nover time, doing fast work is still more expensive than doing no work, and if we can do one lookup\ninstead of two, that can result in a meaningful performance boost, in particular if it\u2019s being performed\non a fast path. And we can see from this simple benchmark that looks up a word in a dictionary that,\nfor this operation, making distinct calls to `ContainsKey` and the indexer does indeed double the cost\nof using the dictionary, almost exactly:\n\n214 CHAPTER 23 | Analyzers\n\n|Method|Mean|Ratio|\n|---|---|---|\n|Lookup1|28.20 ns|1.00|\n|Lookup2|14.12 ns|0.50|\n\nSomewhat ironically, even as I write this example, the analyzer and its auto-fixer are helpfully trying to\nget me to change my benchmark code:\n\n215 CHAPTER 23 | Analyzers\n\nit\u2019s fairly natural for developers to only call `Remove` on a dictionary once they\u2019re sure the dictionary\ncontains the thing being removed; maybe they think `Remove` will throw an exception if the specified\nkey doesn\u2019t exist. However, `Remove` actually allows this as a first-class scenario, with its return `Boolean`\nvalue indicating whether the key was in the dictionary (and thus successfully removed) or not. An\n\nwhich the analyzer flagged and which it\u2019s auto-fixer replaced with just:\n\n```\nm_subscriptions.Remove(listener);\n\n```\n\nNice and simple. And faster, since as with the TryGetValue case, this is now doing a single dictionary\nlookup rather than two. :::{custom-style=Figure}\n\n216 CHAPTER 23 | Analyzers\n\n:::\n\n[Another nice analyzer added in dotnet/roslyn-analyzers#5907](https://github.com/dotnet/roslyn-analyzers/pull/5907) [and dotnet/roslyn-analyzers#5910 is](https://github.com/dotnet/roslyn-analyzers/pull/5910)\nCA1851, which looks for code that iterates through some kinds of enumerables multiple times.\nEnumerating an enumerator, whether directly or via helper methods like those in LINQ, can have nontrivial cost. Calling GetEnumerator typically allocates an enumerator object, and every item yielded\ntypically involves two interface calls, one to MoveNext and one to Current. If something can be done\nvia a single pass over the enumerable rather than multiple passes, that can save such costs. In some\ncases, seeing places this analyzer fires can also inspire changes that avoid any use of enumerators. For\n[example, dotnet/runtime#67292 enabled CA1851 for dotnet/runtime, and in doing so, it fixed several](https://github.com/dotnet/runtime/pull/67292)\ndiagnostics issued by the analyzer (even in a code base that\u2019s already fairly stringent about\nenumerator and LINQ usage). As an example, this is a function in\nSystem.ComponentModel.Composition that was flagged by the analyzer:\n\nThe method\u2019s purpose is to convert the enumerable into an array to be stored, but also to validate\nthat the contents are all non-null and non-\u201cReflectionOnly.\u201d To achieve that, the method is first using\na foreach to iterate through the enumerable, validating each element along the way, and then once\nit\u2019s done so, it calls ToArray() to convert the enumerable into an array. There are multiple problems\n\n217 CHAPTER 23 | Analyzers\n\nwith this. First, it\u2019s incurring the expense of interating through the enumerable twice, once for the\n\nactually ensuring there aren\u2019t nulls in the resulting array, for example. Since the expectation of the\nmethod is that all inputs are valid and we don\u2019t need to optimize for the failure cases, the better\napproach is to _first_ call `ToArray()` and then validate the contents of that array, which is exactly what\nthat PR fixes it to do:\n\nWith that, we only ever iterate it once (and possibly 0 times if `ToArray` can special-case it, and bonus,\nwe validate on the copy rather than on the mutable original.\n\n[Yet another helpful analyzer is the new CA1850 introduced in dotnet/roslyn-analyzers#4797 from](https://github.com/dotnet/roslyn-analyzers/pull/4797)\n\n[@wzchua](https://github.com/wzchua). It used to be that if you wanted to cryptographically hash\nsome data in .NET, you would create an instance of a hash algorithm and call its ComputeHash\nmethod, e.g.\n\nHowever, .NET 5 introduced new \u201cone-shot\u201d hashing methods, which obviates the need to create a\nnew `HashAlgorithm` instance, providing a static method that performs the whole operation.\n\nCA1850 finds occurrences of the former pattern and recommends changing them to the latter.\n\n218 CHAPTER 23 | Analyzers\n\nThe result is not only simpler, it\u2019s also faster:\n\n|Method|Mean|Ratio|Allocated|Alloc Ratio|\n|---|---|---|---|---|\n|Hash1|1,212.9 ns|1.00|240 B|1.00|\n|Hash2|950.8 ns|0.78|56 B|0.23|\n\nThe .NET 7 SDK also includes new analyzers around `[GeneratedRegex(...)]` [(dotnet/runtime#68976)](https://github.com/dotnet/runtime/pull/68976)\nand the already mentioned ones for LibraryImport, all of which help to move your code forwards to\nmore modern patterns that have better performance characteristics.\n\n219 CHAPTER 23 | Analyzers\n\n[This release also saw dotnet/runtime turn on a bunch of additional IDEXXXX code style rules and](https://github.com/dotnet/runtime)\nmake a huge number of code changes in response. Most of the resulting changes are purely about\nsimplifying the code, but in almost every case some portion of the changes also have a functional and\nperformance impact.\n\nLet\u2019s start with IDE0200, which is about removing unnecessary lambdas. Consider a setup like this:\n\npasses it off to some static functionality. For this code, the C# compiler is going to generate\nsomething along the lines of this:\n\nthe lambda and store it into that field. For all subsequent invocations, however, it\u2019ll find the field is\n\n220 CHAPTER 23 | Analyzers\n\nnon-null and will just reuse the same delegate. Thus, this lambda only ever results in a single\nallocation for the whole process (ignoring any race conditions on the initial lazy initialization such that\nmultiple threads all racing to initialize the field might end up producing a few additional unnecessary\nallocations). It\u2019s important to recognize this caching only happens because the lambda doesn\u2019t access\nany instance state and doesn\u2019t close over any locals; if it did either of those things, such caching\nwouldn\u2019t happen. Secondarily, it\u2019s interesting to note the pattern the compiler uses for the lambda\nitself. Note that generated `<CallSite>b__0_0` method is generated as an instance method, and the\ncall site refers to that method of a singleton instance that\u2019s used to initialize a `<>9` field. That\u2019s done\nbecause delegates to static methods use something called a \u201cshuffle thunk\u201d to move arguments into\nthe right place for the target method invocation, making delegates to statics ever so slightly more\nexpensive to invoke than delegates to instance methods.\n\n|Method|Mean|Ratio|\n|---|---|---|\n|InvokeInstance|0.8858 ns|1.00|\n|InvokeStatic|1.3979 ns|1.58|\n\nSo, the compiler is able to cache references to lambdas, great. What about method groups, i.e. where\nyou just name the method directly? Previously, if changed my code to:\n\nthe compiler would generate the equivalent of:\n\n221 CHAPTER 23 | Analyzers\n\nwhich has the unfortunate effect of allocating a new delegate on every invocation, even though we\u2019re\n[still dealing with the exact same static method. Thanks to dotnet/roslyn#58288](https://github.com/dotnet/roslyn/pull/58288) from\n\n[@pawchen](https://github.com/pawchen), the compiler will now generate the equivalent of:\n\nNote we again have a caching field that\u2019s used to enable allocating the delegate once and caching it.\nThat means that places where code was using a lambda to enable this caching can now switch back to\nthe cleaner and simpler method group way of expressing the desired functionality. There is the\ninteresting difference to be cognizant of that since we don\u2019t have a lambda which required the\ncompiler emitting a new method for, we\u2019re still creating a delegate directly to the static method.\nHowever, the minor difference in thunk overhead is typically made up for by the fact that we don\u2019t\nhave a second method to invoke; in the common case where the static helper being invoked isn\u2019t\ninlinable (because it\u2019s not super tiny, because it has exception handling, etc.), we previously would\nhave incurred the cost of the delegate invocation plus the non-inlinable method call, and now we just\nhave the cost of an ever-so-slightly more expensive delegate invocation; on the whole, it\u2019s typically a\nwash.\n\nAnd that brings us to IDE0200, which recognizes lambda expressions that can be removed.\n[dotnet/runtime#71011](https://github.com/dotnet/runtime/pull/71011) [enabled the analyzer for dotnet/runtime, resulting in more than 100 call sites](https://github.com/dotnet/runtime)\nchanging accordingly. However, IDE0200 does more than just this mostly stylistic change. It also\nrecognizes some patterns that can make a more substantial impact. Consider this code that was\nchanged as part of that PR:\n\nThat delegate closes over the `disposable` local, which means this method needs to allocate a display\nclass. But IDE0200 recognizes that instead of closing over `disposable`, we can create the delegate\ndirectly to the Dispose method:\n\n222 CHAPTER 23 | Analyzers\n\nWe still get a delegate allocation, but we avoid the display class allocation, and as a bonus we save on\nthe additional metadata required for the synthesized display class and method generated for the\nlambda.\n\nIDE0020 is another good example of an analyzer that is primarily focused on making code cleaner,\nmore maintainable, more modern, but that can also lead to removing overhead from many different\nplaces. The analyzer looks for code performing unnecessary duplicative casts and recommends using\n[C# pattern matching syntax instead. For example, dotnet/runtime#70523 enabled the analyzer and](https://github.com/dotnet/runtime/pull/70523)\nswitched more than 250 locations from code like:\n\nto instead be like:\n\nIn addition to being cleaner, this ends up saving a cast operation, which can add measurable\noverhead if the JIT is unable to remove it:\n\n|Method|Mean|Ratio|\n|---|---|---|\n|WithCast|2.602 ns|1.00|\n|WithPattern|1.886 ns|0.73|\n\nThen there\u2019s IDE0031, which promotes using null propagation features of C#. This analyzer typically\nmanifests as recommending changing snippets like:\n\n```\nreturn _value != null ? _value.Property : null ;\n\n```\n\ninto code that\u2019s instead like:\n\n223 CHAPTER 23 | Analyzers\n\n```\nreturn _value?.Property;\n\n```\n\nNice, concise, and primarily about cleaning up the code and making it simpler and more maintainable\nby utilizing newer C# syntax. However, there is also a small performance advantage in some situations\nas well. For example, consider this snippet:\n\nThe C# compiler lowers these expressions to the equivalent of this:\n\nfor which the JIT then generates:\n\n224 CHAPTER 23 | Analyzers\n\nthe local means the JIT doesn\u2019t need to add an additional null check on the second use of the local,\n[since nothing could have changed it. dotnet/runtime#70965](https://github.com/dotnet/runtime/pull/70965) rolled out additional use of the null\npropagation operator via auto-fixing IDE0031, resulting in ~120 uses being improved.\n\nAnother interesting example is IDE0060, which finds unused parameters and recommends removing\n[them. This was done for non-public members in System.Private.CoreLib in dotnet/runtime#63015. As](https://github.com/dotnet/runtime/pull/63015)\nwith some of the other mentioned rules, it\u2019s primarily about good hygiene. There can be some small\nadditional cost associated with passing additional parameters (the overhead of reading the values at\nthe call site, putting them into the right register or stack location, etc., and also the metadata size\nassociated with the additional parameter information), but the larger benefit comes from auditing all\nof the cited violations and finding places where work is simply being performed unnecessarily. For\nexample, that PR made some updates to the `TimeZoneInfo` type\u2019s implementation for Unix. In that\nimplementation is a `TZif_ParseRaw` method, which is used to extract some information from a time\nzone data file. Amongst many input and output parameters, it had `out bool[] StandardTime, out`\n\n`bool[] GmtTime`, which the implementation was dutifully filling in by allocating and populating new\n\n`TZif_ParseRaw` to no longer need to allocate and populate those arrays at all, which obviously yields\na much larger gain.\n\nOne final example of peanut-buttery performance improvements from applying an analyzer comes\n[from dotnet/runtime#70896 and dotnet/runtime#71361, which applied IDE0029 across](https://github.com/dotnet/runtime/pull/70896)\ndotnet/runtime. IDE0029 flags cases where null coalescing can be used, e.g. flagging:\n\n```\nreturn message != null ? message : string.Empty;\n\n```\n\nand recommending it be converted to:\n\n```\nreturn message ?? string.Empty;\n\n```\n\nAs with some of the previous rules discussed, that in and of itself doesn\u2019t make a meaningful\nperformance improvement, and rather is about clarity and simplicity. However, in various cases it can.\nFor example, the aforementioned PRs contained an example like:\n\n```\nnull != foundColumns[i] ? foundColumns[i] : DBNull.Value;\n\n```\n\nwhich is rewritten to:\n\n```\nfoundColumns[i] ?? DBNull.Value\n\n```\n\nThis avoids an unnecessary re-access to an array. Or again from those PRs the expression:\n\n225 CHAPTER 23 | Analyzers\n\n```\nentry.GetKey(_thisCollection) != null ? entry.GetKey(_thisCollection) : \"key\"\n\n```\n\nbeing changed to:\n\n```\nentry.GetKey(_thisCollection) ?? \"key\"\n\n```\n\nand avoiding an unnecessary table lookup.\n\n226 CHAPTER 23 | Analyzers\n\n**CHAPTER**"}, "24": {"What\u2019s Next?": "Whew! That was a lot. Congrats on getting through it all.\n\nThe next step is on you. Download the latest .NET 7 bits and take them for a spin. Upgrade your apps.\nWrite and share your own benchmarks. Provide feedback, positive and critical. Find something you\nthink can be better? Open an issue, or better yet, submit a PR with the fix. We\u2019re excited to work with\nyou to polish .NET 7 to be the best .NET release yet; meanwhile, we\u2019re getting going on .NET 8 :)\n\nUntil next time\u2026\n\nHappy coding!\n\n227 CHAPTER 24 | What\u2019s Next?"}}