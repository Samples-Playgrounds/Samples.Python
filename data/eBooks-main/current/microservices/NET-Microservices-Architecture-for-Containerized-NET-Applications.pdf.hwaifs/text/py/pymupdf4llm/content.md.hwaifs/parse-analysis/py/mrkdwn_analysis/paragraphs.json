{'Paragraph': ['**EDITION v7.0** - Updated to ASP.NET Core 7.0', '[Refer changelog](https://aka.ms/MicroservicesEbookChangelog) for the book updates and community contributions.', 'This guide is an introduction to developing microservices-based applications and managing them\nusing containers. It discusses architectural design and implementation approaches using .NET and\nDocker containers.', 'To make it easier to get started, the guide focuses on a reference containerized and microservicebased application that you can explore. The reference application is available at the\n[eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers) GitHub repo.', 'Enterprises are increasingly realizing cost savings, solving deployment problems, and improving\nDevOps and production operations by using containers. Microsoft has been releasing container\ninnovations for Windows and Linux by creating products like Azure Kubernetes Service and Azure\nService Fabric, and by partnering with industry leaders like Docker, Mesosphere, and Kubernetes.\nThese products deliver container solutions that help companies build and deploy applications at cloud\nspeed and scale, whatever their choice of platform or tools.', 'Docker is becoming the de facto standard in the container industry, supported by the most significant\nvendors in the Windows and Linux ecosystems. (Microsoft is one of the main cloud vendors\nsupporting Docker). In the future, Docker will probably be ubiquitous in any datacenter in the cloud or\non-premises.', '[In addition, the microservices architecture is emerging as an important approach for distributed](https://martinfowler.com/articles/microservices.html)\nmission-critical applications. In a microservice-based architecture, the application is built on a\ncollection of services that can be developed, tested, deployed, and versioned independently.', 'This guide is an introduction to developing microservices-based applications and managing them\nusing containers. It discusses architectural design and implementation approaches using .NET and\nDocker containers. To make it easier to get started with containers and microservices, the guide\nfocuses on a reference containerized and microservice-based application that you can explore. The\n[sample application is available at the eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers) GitHub repo.', 'This guide provides foundational development and architectural guidance primarily at a development\nenvironment level with a focus on two technologies: Docker and .NET. Our intention is that you read\nthis guide when thinking about your application design without focusing on the infrastructure (cloud\nor on-premises) of your production environment. You will make decisions about your infrastructure\nlater, when you create your production-ready applications. Therefore, this guide is intended to be\ninfrastructure agnostic and more development-environment-centric.', 'After you have studied this guide, your next step would be to learn about production-ready\nmicroservices on Microsoft Azure.', 'This guide has been revised to cover **.NET 7** version along with many additional updates related to\nthe same “wave” of technologies (that is, Azure and additional third-party technologies) coinciding in\ntime with the .NET 7 release. That’s why the book version has also been updated to version **7.0** .', 'This guide does not focus on the application lifecycle, DevOps, CI/CD pipelines, or team work. The\n[complementary guide Containerized Docker Application Lifecycle with Microsoft Platform and Tools](https://aka.ms/dockerlifecycleebook)\nfocuses on that subject. The current guide also does not provide implementation details on Azure\ninfrastructure, such as information on specific orchestrators.', 'We wrote this guide for developers and solution architects who are new to Docker-based application\ndevelopment and to microservices-based architecture. This guide is for you if you want to learn how\nto architect, design, and implement proof-of-concept applications with Microsoft development\ntechnologies (with special focus on .NET) and with Docker containers.', 'You will also find this guide useful if you are a technical decision maker, such as an enterprise\narchitect, who wants an architecture and technology overview before you decide on what approach to\nselect for new and modern distributed applications.', 'The first part of this guide introduces Docker containers, discusses how to choose between .NET 7 and\nthe .NET Framework as a development framework, and provides an overview of microservices. This\ncontent is for architects and technical decision makers who want an overview but don’t need to focus\non code implementation details.', 'The second part of the guide starts with the Development process for Docker based applications\nsection. It focuses on the development and microservice patterns for implementing applications using\n.NET and Docker. This section will be of most interest to developers and architects who want to focus\non code and on patterns and implementation details.', 'The eShopOnContainers application is an open-source reference app for .NET and microservices that\nis designed to be deployed using Docker containers. The application consists of multiple subsystems,\nincluding several e-store UI front-ends (a Web MVC app, a Web SPA, and a native mobile app). It also\nincludes the back-end microservices and containers for all required server-side operations.', 'The purpose of the application is to showcase architectural patterns. **IT IS NOT A PRODUCTION-**\n**READY TEMPLATE** to start real-world applications. In fact, the application is in a permanent beta\nstate, as it’s also used to test new potentially interesting technologies as they show up.', 'Co-Authors:', '**Cesar de la Torre**, Sr. PM, .NET product team, Microsoft Corp.', '**Bill Wagner**, Sr. Content Developer, C+E, Microsoft Corp.', '**Mike Rousos**, Principal Software Engineer, DevDiv CAT team, Microsoft', 'Editors:', '**Mike Pope**', '**Steve Hoag**', 'Participants and reviewers:', '**Jeffrey Richter**, Partner Software Eng, Azure team, Microsoft', '**Jimmy Bogard**, Chief Architect at Headspring', '**Udi Dahan**, Founder & CEO, Particular Software', '**Jimmy Nilsson**, Co-founder and CEO of Factor10', '**Glenn Condron**, Sr. Program Manager, ASP.NET team', '**Mark Fussell**, Principal PM Lead, Azure Service Fabric team, Microsoft', '**Diego Vega**, PM Lead, Entity Framework team, Microsoft', '**Barry Dorrans**, Sr. Security Program Manager', '**Rowan Miller**, Sr. Program Manager, Microsoft', '**Ankit Asthana**, Principal PM Manager, .NET team, Microsoft', '**Scott Hunter**, Partner Director PM, .NET team, Microsoft', '**Nish Anil**, Sr. Program Manager, .NET team, Microsoft', '**Dylan Reisenberger**, Architect and Dev Lead at Polly', '**Steve “ardalis” Smith** [- Software Architect and Trainer - Ardalis.com](https://ardalis.com/)', '**Ian Cooper**, Coding Architect at Brighter', '**Unai Zorrilla**, Architect and Dev Lead at Plain Concepts', '**Eduard Tomas**, Dev Lead at Plain Concepts', '**Ramon Tomas**, Developer at Plain Concepts', '**David Sanz**, Developer at Plain Concepts', '**Javier Valero**, Chief Operating Officer at Grupo Solutio', '**Pierre Millet**, Sr. Consultant, Microsoft', '**Michael Friis**, Product Manager, Docker Inc', '**Charles Lowell**, Software Engineer, VS CAT team, Microsoft', '**Miguel Veloso**, Software Development Engineer at Plain Concepts', '**Sumit Ghosh**, Principal Consultant at Neudesic', 'PUBLISHED BY', 'Microsoft Developer Division, .NET and Visual Studio product teams', 'A division of Microsoft Corporation', 'One Microsoft Way', 'Redmond, Washington 98052-6399', 'Copyright © 2023 by Microsoft Corporation', 'All rights reserved. No part of the contents of this book may be reproduced or transmitted in any\nform or by any means without the written permission of the publisher.', 'This book is provided “as-is” and expresses the author’s views and opinions. The views, opinions and\ninformation expressed in this book, including URL and other Internet website references, may change\nwithout notice.', 'Some examples depicted herein are provided for illustration only and are fictitious. No real association\nor connection is intended or should be inferred.', '[Microsoft and the trademarks listed at https://www.microsoft.com](https://www.microsoft.com/) on the “Trademarks” webpage are\ntrademarks of the Microsoft group of companies.', 'Mac and macOS are trademarks of Apple Inc.', 'The Docker whale logo is a registered trademark of Docker, Inc. Used by permission.', 'All other marks and logos are property of their respective owners.', '**Introduction to Containers and Docker ................................................................................ 1**', 'What is Docker? ........................................................................................................................................................................ 2', 'Comparing Docker containers with virtual machines ........................................................................................... 3', 'A simple analogy ................................................................................................................................................................. 4', 'Docker terminology ................................................................................................................................................................ 5', 'Docker containers, images, and registries ..................................................................................................................... 7', '**Choosing Between .NET and .NET Framework for Docker Containers .............................. 9**', 'General guidance ..................................................................................................................................................................... 9', 'When to choose .NET for Docker containers ............................................................................................................. 10', 'Developing and deploying cross platform ............................................................................................................ 10', 'Using containers for new (“green-field”) projects ............................................................................................... 11', 'Create and deploy microservices on containers .................................................................................................. 11', 'Deploying high density in scalable systems .......................................................................................................... 11', 'When to choose .NET Framework for Docker containers ..................................................................................... 12', 'Migrating existing applications directly to a Windows Server container .................................................. 12', 'Using third-party .NET libraries or NuGet packages not available for .NET 7 ......................................... 12', 'Using .NET technologies not available for .NET 7 ............................................................................................... 12', 'Using a platform or API that doesn’t support .NET 7 ........................................................................................ 13', 'Porting existing ASP.NET application to .NET 7 ................................................................................................... 13', 'Decision table: .NET implementations to use for Docker ..................................................................................... 13', 'What OS to target with .NET containers ...................................................................................................................... 14', 'Official .NET Docker images ............................................................................................................................................. 16', '.NET and Docker image optimizations for development versus production ........................................... 16', '**Architecting container and microservice-based applications .......................................... 18**', 'Container design principles .............................................................................................................................................. 18', 'Containerizing monolithic applications ....................................................................................................................... 19', 'Deploying a monolithic application as a container ............................................................................................ 21', 'Publishing a single-container-based application to Azure App Service .................................................... 21', 'i Contents', 'Manage state and data in Docker applications ........................................................................................................ 22', 'Service-oriented architecture ........................................................................................................................................... 25', 'Microservices architecture ................................................................................................................................................. 25', 'Additional resources ....................................................................................................................................................... 27', 'Data sovereignty per microservice ................................................................................................................................ 27', 'The relationship between microservices and the Bounded Context pattern ........................................... 29', 'Logical architecture versus physical architecture ..................................................................................................... 30', 'Challenges and solutions for distributed data management .............................................................................. 31', 'Challenge #1: How to define the boundaries of each microservice ............................................................ 31', 'Challenge #2: How to create queries that retrieve data from several microservices ............................ 32', 'Challenge #3: How to achieve consistency across multiple microservices ............................................... 33', 'Challenge #4: How to design communication across microservice boundaries .................................... 35', 'Additional resources ....................................................................................................................................................... 36', 'Identify domain-model boundaries for each microservice .................................................................................. 36', 'The API gateway pattern versus the Direct client-to-microservice communication .................................. 40', 'Direct client-to-microservice communication ...................................................................................................... 40', 'Why consider API Gateways instead of direct client-to-microservice communication ....................... 41', 'What is the API Gateway pattern? ............................................................................................................................. 42', 'Main features in the API Gateway pattern ............................................................................................................. 44', 'Using products with API Gateway features ............................................................................................................ 45', 'Drawbacks of the API Gateway pattern ................................................................................................................... 47', 'Additional resources ....................................................................................................................................................... 48', 'Communication in a microservice architecture ........................................................................................................ 48', 'Communication types .................................................................................................................................................... 49', 'Asynchronous microservice integration enforces microservice’s autonomy ........................................... 50', 'Communication styles .................................................................................................................................................... 52', 'Asynchronous message-based communication ....................................................................................................... 54', 'Single-receiver message-based communication ................................................................................................ 55', 'Multiple-receivers message-based communication .......................................................................................... 56', 'Asynchronous event-driven communication ........................................................................................................ 56', 'A note about messaging technologies for production systems ................................................................... 57', 'Resiliently publishing to the event bus ................................................................................................................... 58', 'ii Contents', 'Additional resources ....................................................................................................................................................... 58', 'Creating, evolving, and versioning microservice APIs and contracts ............................................................... 59', 'Additional resources ....................................................................................................................................................... 59', 'Microservices addressability and the service registry ............................................................................................ 60', 'Additional resources ....................................................................................................................................................... 60', 'Creating composite UI based on microservices ....................................................................................................... 60', 'Additional resources ....................................................................................................................................................... 62', 'Resiliency and high availability in microservices ...................................................................................................... 63', 'Health management and diagnostics in microservices .................................................................................... 63', 'Additional resources ....................................................................................................................................................... 65', 'Orchestrate microservices and multi-container applications for high scalability and availability ....... 66', 'Software platforms for container clustering, orchestration, and scheduling ........................................... 68', 'Using container-based orchestrators in Microsoft Azure ................................................................................ 68', 'Using Azure Kubernetes Service ................................................................................................................................ 69', 'Development environment for Kubernetes ........................................................................................................... 70', 'Getting started with Azure Kubernetes Service (AKS) ....................................................................................... 70', 'Deploy with Helm charts into Kubernetes clusters ............................................................................................. 71', 'Additional resources ....................................................................................................................................................... 71', '**Development process for Docker-based applications ....................................................... 72**', 'Development environment for Docker apps ............................................................................................................. 72', 'Development tool choices: IDE or editor ................................................................................................................ 72', 'Additional resources ....................................................................................................................................................... 73', '.NET languages and frameworks for Docker containers ....................................................................................... 73', 'Development workflow for Docker apps ..................................................................................................................... 73', 'Workflow for developing Docker container-based applications .................................................................. 73', 'Step 1. Start coding and create your initial application or service baseline ............................................. 75', 'Step 2. Create a Dockerfile related to an existing .NET base image ............................................................ 76', 'Step 3. Create your custom Docker images and embed your application or service in them .......... 83', 'Step 4. Define your services in docker-compose.yml when building a multi-container Docker\napplication .......................................................................................................................................................................... 84', 'Step 5. Build and run your Docker application .................................................................................................... 87', 'Step 6. Test your Docker application using your local Docker host ............................................................ 89', 'iii Contents', 'Simplified workflow when developing containers with Visual Studio ........................................................ 90', 'Using PowerShell commands in a Dockerfile to set up Windows Containers ......................................... 91', '**Designing and Developing Multi-Container and Microservice-Based .NET Applications**\n**................................................................................................................................................. 93**', 'Design a microservice-oriented application .............................................................................................................. 93', 'Application specifications ............................................................................................................................................. 93', 'Development team context ......................................................................................................................................... 94', 'Choosing an architecture .............................................................................................................................................. 94', 'Benefits of a microservice-based solution ............................................................................................................. 97', 'Downsides of a microservice-based solution ....................................................................................................... 98', 'External versus internal architecture and design patterns............................................................................... 99', 'The new world: multiple architectural patterns and polyglot microservices .......................................... 100', 'Creating a simple data-driven CRUD microservice ............................................................................................... 102', 'Designing a simple CRUD microservice ................................................................................................................ 102', 'Implementing a simple CRUD microservice with ASP.NET Core ................................................................. 103', 'The DB connection string and environment variables used by Docker containers ............................. 109', 'Generating Swagger description metadata from your ASP.NET Core Web API ................................... 111', 'Defining your multi-container application with docker-compose.yml ......................................................... 116', 'Use a database server running as a container ........................................................................................................ 127', 'SQL Server running as a container with a microservice-related database .............................................. 128', 'Seeding with test data on Web application startup ......................................................................................... 129', 'EF Core InMemory database versus SQL Server running as a container ................................................. 132', 'Using a Redis cache service running in a container ......................................................................................... 132', 'Implementing event-based communication between microservices (integration events) ................... 133', 'Using message brokers and service buses for production systems .......................................................... 134', 'Integration events .......................................................................................................................................................... 135', 'The event bus .................................................................................................................................................................. 136', 'Additional resources ..................................................................................................................................................... 138', 'Implementing an event bus with RabbitMQ for the development or test environment ....................... 138', 'Implementing a simple publish method with RabbitMQ ............................................................................... 139', 'Implementing the subscription code with the RabbitMQ API ..................................................................... 140', 'Additional resources ..................................................................................................................................................... 141', 'iv Contents', 'Subscribing to events ........................................................................................................................................................ 141', 'Publishing events through the event bus............................................................................................................. 142', 'Idempotency in update message events .............................................................................................................. 149', 'Deduplicating integration event messages ......................................................................................................... 150', 'Testing ASP.NET Core services and web apps ........................................................................................................ 152', 'Testing in eShopOnContainers ................................................................................................................................. 155', 'Implement background tasks in microservices with IHostedService and the BackgroundService class\n.................................................................................................................................................................................................... 157', 'Registering hosted services in your WebHost or Host ................................................................................... 159', 'The IHostedService interface ..................................................................................................................................... 159', 'Implementing IHostedService with a custom hosted service class deriving from the\nBackgroundService base class................................................................................................................................... 160', 'Additional resources ..................................................................................................................................................... 163', 'Implement API Gateways with Ocelot ........................................................................................................................ 163', 'Architect and design your API Gateways .............................................................................................................. 163', 'Implementing your API Gateways with Ocelot .................................................................................................. 168', 'Using Kubernetes Ingress plus Ocelot API Gateways ...................................................................................... 180', 'Additional cross-cutting features in an Ocelot API Gateway ....................................................................... 181', '**Tackle Business Complexity in a Microservice with DDD and CQRS Patterns .............. 182**', 'Apply simplified CQRS and DDD patterns in a microservice............................................................................. 184', 'Additional resources ..................................................................................................................................................... 186', 'Apply CQRS and CQS approaches in a DDD microservice in eShopOnContainers ................................. 186', 'CQRS and DDD patterns are not top-level architectures ............................................................................... 187', 'Implement reads/queries in a CQRS microservice ................................................................................................ 188', 'Use ViewModels specifically made for client apps, independent from domain model constraints\n............................................................................................................................................................................................... 189', 'Use Dapper as a micro ORM to perform queries .............................................................................................. 189', 'Dynamic versus static ViewModels ......................................................................................................................... 190', 'Additional resources ..................................................................................................................................................... 193', 'Design a DDD-oriented microservice ......................................................................................................................... 194', 'Keep the microservice context boundaries relatively small .......................................................................... 194', 'Layers in DDD microservices ..................................................................................................................................... 195', 'v Contents', 'Design a microservice domain model ........................................................................................................................ 199', 'The Domain Entity pattern ......................................................................................................................................... 199', 'Implement a microservice domain model with .NET ............................................................................................ 204', 'Domain model structure in a custom .NET Standard Library ....................................................................... 204', 'Structure aggregates in a custom .NET Standard library ............................................................................... 205', 'Implement domain entities as POCO classes ..................................................................................................... 206', 'Encapsulate data in the Domain Entities .............................................................................................................. 207', 'Seedwork (reusable base classes and interfaces for your domain model) .................................................. 210', 'The custom Entity base class ..................................................................................................................................... 211', 'Repository contracts (interfaces) in the domain model layer ...................................................................... 212', 'Additional resources ..................................................................................................................................................... 213', 'Implement value objects .................................................................................................................................................. 213', 'Important characteristics of value objects ........................................................................................................... 214', 'Value object implementation in C# ........................................................................................................................ 215', 'How to persist value objects in the database with EF Core 2.0 and later ................................................ 217', 'Persist value objects as owned entity types in EF Core 2.0 and later ........................................................ 218', 'Additional resources ..................................................................................................................................................... 221', 'Use enumeration classes instead of enum types ................................................................................................... 221', 'Implement an Enumeration base class .................................................................................................................. 222', 'Additional resources ..................................................................................................................................................... 223', 'Design validations in the domain model layer ....................................................................................................... 223', 'Implement validations in the domain model layer ........................................................................................... 224', 'Additional resources ..................................................................................................................................................... 225', 'Client-side validation (validation in the presentation layers) ............................................................................ 226', 'Additional resources ..................................................................................................................................................... 227', 'Domain events: Design and implementation .......................................................................................................... 227', 'What is a domain event? ............................................................................................................................................. 228', 'Domain events versus integration events ............................................................................................................ 228', 'Domain events as a preferred way to trigger side effects across multiple aggregates within the\nsame domain ................................................................................................................................................................... 229', 'Implement domain events .......................................................................................................................................... 231', 'Conclusions on domain events ................................................................................................................................. 237', 'vi Contents', 'Additional resources ..................................................................................................................................................... 238', 'Design the infrastructure persistence layer .............................................................................................................. 238', 'The Repository pattern ................................................................................................................................................ 238', 'Additional resources ..................................................................................................................................................... 243', 'Implement the infrastructure persistence layer with Entity Framework Core ............................................ 243', 'Introduction to Entity Framework Core ................................................................................................................. 244', 'Infrastructure in Entity Framework Core from a DDD perspective ............................................................. 244', 'Implement custom repositories with Entity Framework Core ...................................................................... 246', 'EF DbContext and IUnitOfWork instance lifetime in your IoC container ................................................. 248', 'The repository instance lifetime in your IoC container ................................................................................... 249', 'Table mapping ................................................................................................................................................................ 250', 'Implement the Query Specification pattern ........................................................................................................ 253', 'Use NoSQL databases as a persistence infrastructure ......................................................................................... 255', 'Introduction to Azure Cosmos DB and the native Cosmos DB API ........................................................... 256', 'Implement .NET code targeting MongoDB and Azure Cosmos DB .......................................................... 258', 'Design the microservice application layer and Web API .................................................................................... 266', 'Use SOLID principles and Dependency Injection .............................................................................................. 266', 'Implement the microservice application layer using the Web API ................................................................. 267', 'Use Dependency Injection to inject infrastructure objects into your application layer ..................... 267', 'Implement the Command and Command Handler patterns ....................................................................... 271', 'The Command process pipeline: how to trigger a command handler ..................................................... 278', 'Implement the command process pipeline with a mediator pattern (MediatR) .................................. 281', 'Apply cross-cutting concerns when processing commands with the Behaviors in MediatR .......... 287', '**Implement resilient applications ....................................................................................... 291**', 'Handle partial failure ......................................................................................................................................................... 292', 'Strategies to handle partial failure ............................................................................................................................... 294', 'Additional resources ..................................................................................................................................................... 295', 'Implement retries with exponential backoff ............................................................................................................ 295', 'Implement resilient Entity Framework Core SQL connections.......................................................................... 295', 'Execution strategies and explicit transactions using BeginTransaction and multiple DbContexts296', 'Additional resources ..................................................................................................................................................... 298', 'Use IHttpClientFactory to implement resilient HTTP requests ......................................................................... 298', 'vii Contents', 'Issues with the original HttpClient class available in .NET ............................................................................. 298', 'Benefits of using IHttpClientFactory ....................................................................................................................... 299', 'Multiple ways to use IHttpClientFactory ............................................................................................................... 300', 'How to use Typed Clients with IHttpClientFactory ........................................................................................... 300', 'Additional resources ..................................................................................................................................................... 304', 'Implement HTTP call retries with exponential backoff with IHttpClientFactory and Polly policies ... 304', 'Add a jitter strategy to the retry policy ................................................................................................................. 305', 'Additional resources ..................................................................................................................................................... 306', 'Implement the Circuit Breaker pattern ....................................................................................................................... 306', 'Implement Circuit Breaker pattern with IHttpClientFactory and Polly ..................................................... 307', 'Test Http retries and circuit breakers in eShopOnContainers ...................................................................... 308', 'Additional resources ..................................................................................................................................................... 310', 'Health monitoring .............................................................................................................................................................. 310', 'Implement health checks in ASP.NET Core services ........................................................................................ 311', 'Use watchdogs ................................................................................................................................................................ 315', 'Health checks when using orchestrators .............................................................................................................. 317', 'Advanced monitoring: visualization, analysis, and alerts ............................................................................... 317', 'Additional resources ..................................................................................................................................................... 318', '**Make secure .NET Microservices and Web Applications ................................................. 319**', 'Implement authentication in .NET microservices and web applications ...................................................... 319', 'Authenticate with ASP.NET Core Identity ............................................................................................................. 320', 'Authenticate with external providers ..................................................................................................................... 321', 'Authenticate with bearer tokens .............................................................................................................................. 323', 'Authenticate with an OpenID Connect or OAuth 2.0 Identity provider ................................................... 324', 'Issue security tokens from an ASP.NET Core service ....................................................................................... 325', 'Consume security tokens ............................................................................................................................................ 326', 'Additional resources .......................................................................................................................................................... 327', 'About authorization in .NET microservices and web applications .................................................................. 327', 'Implement role-based authorization ..................................................................................................................... 328', 'Implement policy-based authorization ................................................................................................................. 329', 'Authorization and minimal apis ............................................................................................................................... 330', 'Additional resources ..................................................................................................................................................... 330', 'viii Contents', 'Store application secrets safely during development .......................................................................................... 330', 'Store secrets in environment variables ................................................................................................................. 331', 'Store secrets with the ASP.NET Core Secret Manager .................................................................................... 331', 'Use Azure Key Vault to protect secrets at production time .............................................................................. 332', 'Additional resources ..................................................................................................................................................... 333', '**.NET Microservices Architecture key takeaways .............................................................. 334**', 'ix Contents', '**CHAPTER**', 'Containerization is an approach to software development in which an application or service, its\ndependencies, and its configuration (abstracted as deployment manifest files) are packaged together\nas a container image. The containerized application can be tested as a unit and deployed as a\ncontainer image instance to the host operating system (OS).', 'Just as shipping containers allow goods to be transported by ship, train, or truck regardless of the\ncargo inside, software containers act as a standard unit of software deployment that can contain\ndifferent code and dependencies. Containerizing software this way enables developers and IT\nprofessionals to deploy them across environments with little or no modification.', 'Containers also isolate applications from each other on a shared OS. Containerized applications run\non top of a container host that in turn runs on the OS (Linux or Windows). Containers therefore have a\nsignificantly smaller footprint than virtual machine (VM) images.', 'Each container can run a whole web application or a service, as shown in Figure 2-1. In this example,\nDocker host is a container host, and App1, App2, Svc 1, and Svc 2 are containerized applications or\nservices.', '_Figure 2-1. Multiple containers running on a container host_', '1 CHAPTER 1 | Introduction to Containers and Docker', 'Another benefit of containerization is scalability. You can scale out quickly by creating new containers\nfor short-term tasks. From an application point of view, instantiating an image (creating a container) is\nsimilar to instantiating a process like a service or a web app. For reliability, however, when you run\nmultiple instances of the same image across multiple host servers, you typically want each container\n(image instance) to run in a different host server or VM in different fault domains.', 'In short, containers offer the benefits of isolation, portability, agility, scalability, and control across the\nwhole application lifecycle workflow. The most important benefit is the environment’s isolation\nprovided between Dev and Ops.', '[Docker](https://www.docker.com/) [is an open-source project for automating the deployment of applications as portable, self-](https://github.com/docker/docker)\n[sufficient containers that can run on the cloud or on-premises. Docker is also a company that](https://www.docker.com/)\npromotes and evolves this technology, working in collaboration with cloud, Linux, and Windows\nvendors, including Microsoft.', '_Figure 2-2. Docker deploys containers at all layers of the hybrid cloud._', 'Docker containers can run anywhere, on-premises in the customer datacenter, in an external service\nprovider or in the cloud, on Azure. Docker image containers can run natively on Linux and Windows.\nHowever, Windows images can run only on Windows hosts and Linux images can run on Linux hosts\nand Windows hosts (using a Hyper-V Linux VM, so far), where host means a server or a VM.', 'Developers can use development environments on Windows, Linux, or macOS. On the development\ncomputer, the developer runs a Docker host where Docker images are deployed, including the app\nand its dependencies. Developers who work on Linux or on macOS use a Docker host that is Linux\nbased, and they can create images only for Linux containers. (Developers working on macOS can edit\ncode or run the Docker CLI from macOS, but as of the time of this writing, containers don’t run', '2 CHAPTER 1 | Introduction to Containers and Docker', 'directly on macOS.) Developers who work on Windows can create images for either Linux or Windows\nContainers.', 'To host containers in development environments and provide additional developer tools, Docker\n[ships Docker Desktop for Windows](https://hub.docker.com/editions/community/docker-ce-desktop-windows) [or for macOS. These products install the necessary VM (the Docker](https://hub.docker.com/editions/community/docker-ce-desktop-mac)\nhost) to host the containers.', '[To run Windows Containers, there are two types of runtimes:](https://docs.microsoft.com/virtualization/windowscontainers/about/)', 'Figure 2-3 shows a comparison between VMs and Docker containers.', '3 CHAPTER 1 | Introduction to Containers and Docker', '_Figure 2-3. Comparison of traditional virtual machines to Docker containers_', 'For VMs, there are three base layers in the host server, from the bottom-up: infrastructure, Host\nOperating System and a Hypervisor and on top of all that each VM has its own OS and all necessary\nlibraries. For Docker, the host server only has the infrastructure and the OS and on top of that, the\ncontainer engine, that keeps container isolated but sharing the base OS services.', 'Because containers require far fewer resources (for example, they don’t need a full OS), they’re easy to\ndeploy and they start fast. This allows you to have higher density, meaning that it allows you to run\nmore services on the same hardware unit, thereby reducing costs.', 'As a side effect of running on the same kernel, you get less isolation than VMs.', 'The main goal of an image is that it makes the environment (dependencies) the same across different\ndeployments. This means that you can debug it on your machine and then deploy it to another\nmachine with the same environment guaranteed.', 'A container image is a way to package an app or service and deploy it in a reliable and reproducible\nway. You could say that Docker isn’t only a technology but also a philosophy and a process.', 'When using Docker, you won’t hear developers say, “It works on my machine, why not in production?”\nThey can simply say, “It runs on Docker”, because the packaged Docker application can be executed\non any supported Docker environment, and it runs the way it was intended to on all deployment\ntargets (such as Dev, QA, staging, and production).', 'Perhaps a simple analogy can help getting the grasp of the core concept of Docker.', 'Let’s go back in time to the 1950s for a moment. There were no word processors, and the\nphotocopiers were used everywhere (kind of).', 'Imagine you’re responsible for quickly issuing batches of letters as required, to mail them to\ncustomers, using real paper and envelopes, to be delivered physically to each customer’s address\n(there was no email back then).', 'At some point, you realize the letters are just a composition of a large set of paragraphs, which are\npicked and arranged as needed, according to the purpose of the letter, so you devise a system to\nissue letters quickly, expecting to get a hefty raise.', 'The system is simple:', 'This section lists terms and definitions you should be familiar with before getting deeper into Docker.\n[For further definitions, see the extensive glossary](https://docs.docker.com/glossary/) provided by Docker.', '**Container image** : A package with all the dependencies and information needed to create a container.\nAn image includes all the dependencies (such as frameworks) plus deployment and execution\nconfiguration to be used by a container runtime. Usually, an image derives from multiple base images\nthat are layers stacked on top of each other to form the container’s filesystem. An image is immutable\nonce it has been created.', '**Dockerfile** : A text file that contains instructions for building a Docker image. It’s like a batch script,\nthe first line states the base image to begin with and then follow the instructions to install required\nprograms, copy files, and so on, until you get the working environment you need.', '**Build** : The action of building a container image based on the information and context provided by its\nDockerfile, plus additional files in the folder where the image is built. You can build images with the\nfollowing Docker command:', '**Container** : An instance of a Docker image. A container represents the execution of a single\napplication, process, or service. It consists of the contents of a Docker image, an execution\nenvironment, and a standard set of instructions. When scaling a service, you create multiple instances\nof a container from the same image. Or a batch job can create multiple containers from the same\nimage, passing different parameters to each instance.', '**Volumes** : Offer a writable filesystem that the container can use. Since images are read-only but most\nprograms need to write to the filesystem, volumes add a writable layer, on top of the container image,\nso the programs have access to a writable filesystem. The program doesn’t know it’s accessing a', '5 CHAPTER 1 | Introduction to Containers and Docker', 'layered filesystem, it’s just the filesystem as usual. Volumes live in the host system and are managed\nby Docker.', '**Tag** : A mark or label you can apply to images so that different images or versions of the same image\n(depending on the version number or the target environment) can be identified.', '**Multi-stage Build** : Is a feature, since Docker 17.05 or higher, that helps to reduce the size of the final\nimages. For example, a large base image, containing the SDK can be used for compiling and\npublishing and then a small runtime-only base image can be used to host the application.', '**Repository (repo)** : A collection of related Docker images, labeled with a tag that indicates the image\nversion. Some repos contain multiple variants of a specific image, such as an image containing SDKs\n(heavier), an image containing only runtimes (lighter), etc. Those variants can be marked with tags. A\nsingle repo can contain platform variants, such as a Linux image and a Windows image.', '**Registry** : A service that provides access to repositories. The default registry for most public images is\n[Docker Hub](https://hub.docker.com/) (owned by Docker as an organization). A registry usually contains repositories from\nmultiple teams. Companies often have private registries to store and manage images they’ve created.\nAzure Container Registry is another example.', '**Multi-arch image** [: For multi-architecture (or multi-platform), it’s a Docker feature that simplifies the](https://docs.docker.com/build/building/multi-platform/)\nselection of the appropriate image, according to the platform where Docker is running. For example,\nwhen a Dockerfile requests a base image **FROM mcr.microsoft.com/dotnet/sdk:7.0** from the\nregistry, it actually gets **7.0-nanoserver-ltsc2022**, **7.0-nanoserver-1809** or **7.0-bullseye-slim**,\ndepending on the operating system and version where Docker is running.', '**Docker Hub** : A public registry to upload images and work with them. Docker Hub provides Docker\nimage hosting, public or private registries, build triggers and web hooks, and integration with GitHub\nand Bitbucket.', '**Azure Container Registry** : A public resource for working with Docker images and its components in\nAzure. This provides a registry that’s close to your deployments in Azure and that gives you control\nover access, making it possible to use your Azure Active Directory groups and permissions.', '**Docker Trusted Registry (DTR)** : A Docker registry service (from Docker) that can be installed onpremises so it lives within the organization’s datacenter and network. It’s convenient for private\nimages that should be managed within the enterprise. Docker Trusted Registry is included as part of\nthe Docker Datacenter product.', '**Docker Desktop** : Development tools for Windows and macOS for building, running, and testing\ncontainers locally. Docker Desktop for Windows provides development environments for both Linux\n[and Windows Containers. The Linux Docker host on Windows is based on a Hyper-V virtual machine.](https://www.microsoft.com/cloud-platform/server-virtualization)\nThe host for Windows Containers is directly based on Windows. Docker Desktop for Mac is based on\n[the Apple Hypervisor framework and the xhyve hypervisor, which provides a Linux Docker host virtual](https://github.com/mist64/xhyve)\nmachine on macOS. Docker Desktop for Windows and for Mac replaces Docker Toolbox, which was\nbased on Oracle VirtualBox.', '**Compose** : A command-line tool and YAML file format with metadata for defining and running multicontainer applications. You define a single application based on multiple images with one or more\n.yml files that can override values depending on the environment. After you’ve created the definitions,', '6 CHAPTER 1 | Introduction to Containers and Docker', 'you can deploy the whole multi-container application with a single command (docker-compose up)\nthat creates a container per image on the Docker host.', '**Cluster** : A collection of Docker hosts exposed as if it were a single virtual Docker host, so that the\napplication can scale to multiple instances of the services spread across multiple hosts within the\ncluster. Docker clusters can be created with Kubernetes, Azure Service Fabric, Docker Swarm and\nMesosphere DC/OS.', '**Orchestrator** : A tool that simplifies the management of clusters and Docker hosts. Orchestrators\nenable you to manage their images, containers, and hosts through a command-line interface (CLI) or a\ngraphical UI. You can manage container networking, configurations, load balancing, service discovery,\nhigh availability, Docker host configuration, and more. An orchestrator is responsible for running,\ndistributing, scaling, and healing workloads across a collection of nodes. Typically, orchestrator\nproducts are the same products that provide cluster infrastructure, like Kubernetes and Azure Service\nFabric, among other offerings in the market.', 'When using Docker, a developer creates an app or service and packages it and its dependencies into\na container image. An image is a static representation of the app or service and its configuration and\ndependencies.', 'To run the app or service, the app’s image is instantiated to create a container, which will be running\non the Docker host. Containers are initially tested in a development environment or PC.', 'Developers should store images in a registry, which acts as a library of images and is needed when\n[deploying to production orchestrators. Docker maintains a public registry via Docker Hub; other](https://hub.docker.com/)\n[vendors provide registries for different collections of images, including Azure Container Registry.](https://azure.microsoft.com/services/container-registry/)\nAlternatively, enterprises can have a private registry on-premises for their own Docker images.', 'Figure 2-4 shows how images and registries in Docker relate to other components. It also shows the\nmultiple registry offerings from vendors.', '7 CHAPTER 1 | Introduction to Containers and Docker', '_Figure 2-4. Taxonomy of Docker terms and concepts_', 'The registry is like a bookshelf where images are stored and available to be pulled for building\ncontainers to run services or web apps. There are private Docker registries on-premises and on the\npublic cloud. Docker Hub is a public registry maintained by Docker, along the Docker Trusted Registry\nan enterprise-grade solution, Azure offers the Azure Container Registry. AWS, Google, and others also\nhave container registries.', 'Putting images in a registry lets you store static and immutable application bits, including all their\ndependencies at a framework level. Those images can then be versioned and deployed in multiple\nenvironments and therefore provide a consistent deployment unit.', 'Private image registries, either hosted on-premises or in the cloud, are recommended when:', 'There are two supported frameworks for building server-side containerized Docker applications with\n[.NET: .NET Framework and .NET 7. They share many .NET platform components, and you can share](https://dotnet.microsoft.com/download)\ncode across the two. However, there are fundamental differences between them, and which\nframework you use will depend on what you want to accomplish. This section provides guidance on\nwhen to choose each framework.', 'This section provides a summary of when to choose .NET 7 or .NET Framework. We provide more\ndetails about these choices in the sections that follow.', 'Use .NET 7, with Linux or Windows Containers, for your containerized Docker server application when:', 'The modularity and lightweight nature of .NET 7 makes it perfect for containers. When you deploy\nand start a container, its image is far smaller with .NET 7 than with .NET Framework. In contrast, to use\n.NET Framework for a container, you must base your image on the Windows Server Core image, which\nis a lot heavier than the Windows Nano Server or Linux images that you use for .NET 7.', 'Additionally, .NET 7 is cross-platform, so you can deploy server apps with Linux or Windows container\nimages. However, if you are using the traditional .NET Framework, you can only deploy images based\non Windows Server Core.', 'The following is a more detailed explanation of why to choose .NET 7.', 'Clearly, if your goal is to have an application (web app or service) that can run on multiple platforms\nsupported by Docker (Linux and Windows), the right choice is .NET 7, because .NET Framework only\nsupports Windows.', '.NET 7 also supports macOS as a development platform. However, when you deploy containers to a\nDocker host, that host must (currently) be based on Linux or Windows. For example, in a development\nenvironment, you could use a Linux VM running on a Mac.', '[Visual Studio](https://www.visualstudio.com/vs/) provides an integrated development environment (IDE) for Windows and supports\nDocker development.', '[Visual Studio for Mac](https://www.visualstudio.com/vs/visual-studio-mac/) is an IDE, evolution of Xamarin Studio, that runs on macOS and supports\nDocker-based application development. This tool should be the preferred choice for developers\nworking in Mac machines who also want to use a powerful IDE.', '[You can also use Visual Studio Code on macOS, Linux, and Windows. Visual Studio Code fully](https://code.visualstudio.com/)\nsupports .NET 7, including IntelliSense and debugging. Because VS Code is a lightweight editor, you', '10 CHAPTER 2 | Choosing Between .NET and .NET Framework for Docker Containers', 'can use it to develop containerized apps on the machine in conjunction with the Docker CLI and the\n[.NET CLI. You can also target .NET 7 with most third-party editors like Sublime, Emacs, vi, and the](https://docs.microsoft.com/dotnet/core/tools/)\nopen-source OmniSharp project, which also provides IntelliSense support.', '[In addition to the IDEs and editors, you can use the .NET CLI](https://docs.microsoft.com/dotnet/core/tools/) for all supported platforms.', 'Containers are commonly used in conjunction with a microservices architecture, although they can\nalso be used to containerize web apps or services that follow any architectural pattern. You can use\n.NET Framework on Windows Containers, but the modularity and lightweight nature of .NET 7 makes\nit perfect for containers and microservices architectures. When you create and deploy a container, its\nimage is far smaller with .NET 7 than with .NET Framework.', 'You could use the traditional .NET Framework for building microservices-based applications (without\ncontainers) by using plain processes. That way, because the .NET Framework is already installed and\nshared across processes, processes are light and fast to start. However, if you are using containers, the\nimage for the traditional .NET Framework is also based on Windows Server Core and that makes it too\nheavy for a microservices-on-containers approach. However, teams have been looking for\nopportunities to improve the experience for .NET Framework users as well. Recently, size of the\n[Windows Server Core container images have been reduced to >40% smaller.](https://devblogs.microsoft.com/dotnet/we-made-windows-server-core-container-images-40-smaller)', 'On the other hand, .NET 7 is the best candidate if you’re embracing a microservices-oriented system\nthat is based on containers because .NET 7 is lightweight. In addition, its related container images, for\neither Linux or Windows Nano Server, are lean and small, making containers light and fast to start.', 'A microservice is meant to be as small as possible: to be light when spinning up, to have a small\n[footprint, to have a small Bounded Context (check DDD, Domain-Driven Design), to represent a small](https://en.wikipedia.org/wiki/Domain-driven_design)\narea of concerns, and to be able to start and stop fast. For those requirements, you will want to use\nsmall and fast-to-instantiate container images like the .NET 7 container image.', 'A microservices architecture also allows you to mix technologies across a service boundary. This\napproach enables a gradual migration to .NET 7 for new microservices that work in conjunction with\nother microservices or with services developed with Node.js, Python, Java, GoLang, or other\ntechnologies.', 'When your container-based system needs the best possible density, granularity, and performance,\n.NET and ASP.NET Core are your best options. ASP.NET Core is up to 10 times faster than ASP.NET in\nthe traditional .NET Framework, and it leads to other popular industry technologies for microservices,\nsuch as Java servlets, Go, and Node.js.', 'This approach is especially relevant for microservices architectures, where you could have hundreds of\nmicroservices (containers) running. With ASP.NET Core images (based on the .NET runtime) on Linux\nor Windows Nano, you can run your system with a much lower number of servers or VMs, ultimately\nsaving costs in infrastructure and hosting.', '11 CHAPTER 2 | Choosing Between .NET and .NET Framework for Docker Containers', 'While .NET 7 offers significant benefits for new applications and application patterns, .NET Framework\nwill continue to be a good choice for many existing scenarios.', 'You might want to use Docker containers just to simplify deployment, even if you are not creating\nmicroservices. For example, perhaps you want to improve your DevOps workflow with Docker—\ncontainers can give you better isolated test environments and can also eliminate deployment issues\ncaused by missing dependencies when you move to a production environment. In cases like these,\neven if you are deploying a monolithic application, it makes sense to use Docker and Windows\nContainers for your current .NET Framework applications.', 'In most cases for this scenario, you will not need to migrate your existing applications to .NET 7; you\ncan use Docker containers that include the traditional .NET Framework. However, a recommended\napproach is to use .NET 7 as you extend an existing application, such as writing a new service in\nASP.NET Core.', '[Third-party libraries are quickly embracing .NET Standard, which enables code sharing across all .NET](https://docs.microsoft.com/dotnet/standard/net-standard)\nflavors, including .NET 7. With .NET Standard 2.0 and later, the API surface compatibility across\ndifferent frameworks has become significantly larger. Even more, .NET Core 2.x and newer applications\n[can also directly reference existing .NET Framework libraries (see .NET Framework 4.6.1 supporting](https://github.com/dotnet/standard/blob/v2.1.0/docs/planning/netstandard-2.0/README.md#net-framework-461-supporting-net-standard-20)\n[.NET Standard 2.0).](https://github.com/dotnet/standard/blob/v2.1.0/docs/planning/netstandard-2.0/README.md#net-framework-461-supporting-net-standard-20)', '[In addition, the Windows Compatibility Pack extends the API surface available for .NET Standard 2.0](https://docs.microsoft.com/dotnet/core/porting/windows-compat-pack)\non Windows. This pack allows recompiling most existing code to .NET Standard 2.x with little or no\nmodification, to run on Windows.', 'However, even with that exceptional progression since .NET Standard 2.0 and .NET Core 2.1 or later,\nthere might be cases where certain NuGet packages need Windows to run and might not support\n.NET Core or later. If those packages are critical for your application, then you will need to use .NET\nFramework on Windows Containers.', 'Some .NET Framework technologies aren’t available in .NET 7. Some of them might become available\nin later releases, but others don’t fit the new application patterns targeted by .NET Core and might\nnever be available.', 'The following list shows most of the technologies that aren’t available in .NET 7:', '12 CHAPTER 2 | Choosing Between .NET and .NET Framework for Docker Containers', 'Some Microsoft and third-party platforms don’t support .NET 7. For example, some Azure services\nprovide an SDK that isn’t yet available for consumption on .NET 7 yet. Most Azure SDK should\neventually be ported to .NET 7/.NET Standard, but some might not for several reasons. You can see\n[the available Azure SDKs in the Azure SDK Latest Releases](https://azure.github.io/azure-sdk/releases/latest/index.html) page.', 'In the meantime, if any platform or service in Azure still doesn’t support .NET 7 with its client API, you\ncan use the equivalent REST API from the Azure service or the client SDK on .NET Framework.', '.NET Core is a revolutionary step forward from .NET Framework. It offers a host of advantages over\n.NET Framework across the board from productivity to performance, and from cross-platform support\nto developer satisfaction. If you are using .NET Framework and planning to migrate your application\n[to .NET Core or .NET 5+, see Porting Existing ASP.NET Apps to .NET Core.](https://docs.microsoft.com/dotnet/architecture/porting-existing-aspnet-apps/)', '**Additional resources**', 'The following decision table summarizes whether to use .NET Framework or .NET 7. Remember that\nfor Linux containers, you need Linux-based Docker hosts (VMs or servers), and that for Windows\nContainers, you need Windows Server-based Docker hosts (VMs or servers).', '13 CHAPTER 2 | Choosing Between .NET and .NET Framework for Docker Containers', 'Given the diversity of operating systems supported by Docker and the differences between .NET\nFramework and .NET 7, you should target a specific OS and specific versions depending on the\nframework you are using.', 'For Windows, you can use Windows Server Core or Windows Nano Server. These Windows versions\nprovide different characteristics (IIS in Windows Server Core versus a self-hosted web server like\nKestrel in Nano Server) that might be needed by .NET Framework or .NET 7, respectively.', 'For Linux, multiple distros are available and supported in official .NET Docker images (like Debian).', '14 CHAPTER 2 | Choosing Between .NET and .NET Framework for Docker Containers', 'In Figure 3-1, you can see the possible OS version depending on the .NET framework used.', '_Figure 3-1. Operating systems to target depending on versions of the .NET framework_', 'When deploying legacy .NET Framework applications you have to target Windows Server Core,\ncompatible with legacy apps and IIS, but it has a larger image. When deploying .NET 7 applications,\nyou can target Windows Nano Server, which is cloud optimized, uses Kestrel and is smaller and starts\nfaster. You can also target Linux, supporting Debian, Alpine, and others.', 'You can also create your own Docker image in cases where you want to use a different Linux distro or\nwhere you want an image with versions not provided by Microsoft. For example, you might create an\nimage with ASP.NET Core running on the traditional .NET Framework and Windows Server Core, which\nis a not-so-common scenario for Docker.', 'When you add the image name to your Dockerfile file, you can select the operating system and\nversion depending on the tag you use, as in the following examples:', '15 CHAPTER 2 | Choosing Between .NET and .NET Framework for Docker Containers', 'The Official .NET Docker images are Docker images created and optimized by Microsoft. They’re\n[publicly available on Microsoft Artifact Registry. You can search over the catalog to find all .NET image](https://mcr.microsoft.com/)\n[repositories, for example .NET SDK](https://mcr.microsoft.com/product/dotnet/sdk/about) repository.', 'Each repository can contain multiple images, depending on .NET versions, and depending on the OS\nand versions (Linux Debian, Linux Alpine, Windows Nano Server, Windows Server Core, and so on).\nImage repositories provide extensive tagging to help you select not just a specific framework version,\nbut also to choose an OS (Linux distribution or Windows version).', 'When building Docker images for developers, Microsoft focused on the following main scenarios:', '_Microservices offer great benefits but also raise huge new challenges. Microservice architecture patterns_\n_are fundamental pillars when creating a microservice-based application._', 'Earlier in this guide, you learned basic concepts about containers and Docker. That information was\nthe minimum you needed to get started with containers. Even though containers are enablers of, and\na great fit for microservices, they aren’t mandatory for a microservice architecture. Many architectural\nconcepts in this architecture section could be applied without containers. However, this guide focuses\non the intersection of both due to the already introduced importance of containers.', 'Enterprise applications can be complex and are often composed of multiple services instead of a\nsingle service-based application. For those cases, you need to understand other architectural\napproaches, such as the microservices and certain Domain-Driven Design (DDD) patterns plus\ncontainer orchestration concepts. Note that this chapter describes not just microservices on\ncontainers, but any containerized application, as well.', 'In the container model, a container image instance represents a single process. By defining a\ncontainer image as a process boundary, you can create primitives that can be used to scale or batch\nthe process.', '[When you design a container image, you’ll see an ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint) definition in the Dockerfile. This\ndefinition defines the process whose lifetime controls the lifetime of the container. When the process\ncompletes, the container lifecycle ends. Containers might represent long-running processes like web\nservers, but can also represent short-lived processes like batch jobs, which formerly might have been\n[implemented as Azure WebJobs.](https://github.com/Azure/azure-webjobs-sdk/wiki)', 'If the process fails, the container ends, and the orchestrator takes over. If the orchestrator was\nconfigured to keep five instances running and one fails, the orchestrator will create another container\ninstance to replace the failed process. In a batch job, the process is started with parameters. When the\nprocess completes, the work is complete. This guidance drills-down on orchestrators, later on.', '18 CHAPTER 3 | Architecting container and microservice-based applications', 'You might find a scenario where you want multiple processes running in a single container. For that\nscenario, since there can be only one entry point per container, you could run a script within the\n[container that launches as many programs as needed. For example, you can use Supervisor or a](http://supervisord.org/)\nsimilar tool to take care of launching multiple processes inside a single container. However, even\nthough you can find architectures that hold multiple processes per container, that approach isn’t very\ncommon.', 'You might want to build a single, monolithically deployed web application or service and deploy it as\na container. The application itself might not be internally monolithic, but structured as several\nlibraries, components, or even layers (application layer, domain layer, data-access layer, etc.).\nExternally, however, it’s a single container—a single process, a single web application, or a single\nservice.', 'To manage this model, you deploy a single container to represent the application. To increase\ncapacity, you scale out, that is, just add more copies with a load balancer in front. The simplicity\ncomes from managing a single deployment in a single container or VM.', '_Figure 4-1. Example of the architecture of a containerized monolithic application_', 'You can include multiple components, libraries, or internal layers in each container, as illustrated in\nFigure 4-1. A monolithic containerized application has most of its functionality within a single\ncontainer, with internal layers or libraries, and scales out by cloning the container on multiple\nservers/VMs. However, this monolithic pattern might conflict with the container principle “a container\ndoes one thing, and does it in one process”, but might be ok for some cases.', 'The downside of this approach becomes evident if the application grows, requiring it to scale. If the\nentire application can scale, it isn’t really a problem. However, in most cases, just a few parts of the\napplication are the choke points that require scaling, while other components are used less.', 'For example, in a typical e-commerce application, you likely need to scale the product information\nsubsystem, because many more customers browse products than purchase them. More customers use', '19 CHAPTER 3 | Architecting container and microservice-based applications', 'their basket than use the payment pipeline. Fewer customers add comments or view their purchase\nhistory. And you might have only a handful of employees that need to manage the content and\nmarketing campaigns. If you scale the monolithic design, all the code for these different tasks is\ndeployed multiple times and scaled at the same grade.', 'There are multiple ways to scale an application-horizontal duplication, splitting different areas of the\napplication, and partitioning similar business concepts or data. But, in addition to the problem of\nscaling all components, changes to a single component require complete retesting of the entire\napplication, and a complete redeployment of all the instances.', 'However, the monolithic approach is common, because the development of the application is initially\neasier than for microservices approaches. Thus, many organizations develop using this architectural\napproach. While some organizations have had good enough results, others are hitting limits. Many\norganizations designed their applications using this model because tools and infrastructure made it\ntoo difficult to build service-oriented architectures (SOA) years ago, and they did not see the needuntil the application grew.', 'From an infrastructure perspective, each server can run many applications within the same host and\nhave an acceptable ratio of efficiency in resources usage, as shown in Figure 4-2.', '_Figure 4-2. Monolithic approach: Host running multiple apps, each app running as a container_', 'Monolithic applications in Microsoft Azure can be deployed using dedicated VMs for each instance.\n[Additionally, using Azure virtual machine scale sets, you can easily scale the VMs. Azure App Service](https://azure.microsoft.com/documentation/services/virtual-machine-scale-sets/)\ncan also run monolithic applications and easily scale instances without requiring you to manage the\nVMs. Since 2016, Azure App Services can run single instances of Docker containers as well, simplifying\ndeployment.', 'As a QA environment or a limited production environment, you can deploy multiple Docker host VMs\nand balance them using the Azure balancer, as shown in Figure 4-3. This lets you manage scaling with\na coarse-grain approach, because the whole application lives within a single container.', '20 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-3. Example of multiple hosts scaling up a single container application_', 'Deployment to the various hosts can be managed with traditional deployment techniques. Docker\nhosts can be managed with commands like docker run or docker-compose performed manually, or\nthrough automation such as continuous delivery (CD) pipelines.', 'There are benefits to using containers to manage monolithic application deployments. Scaling\ncontainer instances is far faster and easier than deploying additional VMs. Even if you use virtual\nmachine scale sets, VMs take time to start. When deployed as traditional application instances instead\nof containers, the configuration of the application is managed as part of the VM, which isn’t ideal.', 'Deploying updates as Docker images is far faster and network efficient. Docker images typically start\nin seconds, which speeds rollouts. Tearing down a Docker image instance is as easy as issuing a\ndocker stop command, and typically completes in less than a second.', 'Because containers are immutable by design, you never need to worry about corrupted VMs. In\ncontrast, update scripts for a VM might forget to account for some specific configuration or file left on\ndisk.', 'While monolithic applications can benefit from Docker, we’re touching only on the benefits.\nAdditional benefits of managing containers come from deploying with container orchestrators, which\nmanage the various instances and lifecycle of each container instance. Breaking up the monolithic\napplication into subsystems that can be scaled, developed, and deployed individually is your entry\npoint into the realm of microservices.', 'Whether you want to get validation of a container deployed to Azure or when an application is simply\na single-container application, Azure App Service provides a great way to provide scalable singlecontainer-based services. Using Azure App Service is simple. It provides great integration with Git to\nmake it easy to take your code, build it in Visual Studio, and deploy it directly to Azure.', '21 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-4. Publishing a single-container application to Azure App Service from Visual Studio 2022_', 'Without Docker, if you needed other capabilities, frameworks, or dependencies that aren’t supported\nin Azure App Service, you had to wait until the Azure team updated those dependencies in App\nService. Or you had to switch to other services like Azure Cloud Services or VMs, where you had\nfurther control and you could install a required component or framework for your application.', 'Container support in Visual Studio 2017 and later gives you the ability to include whatever you want\nin your application environment, as shown in Figure 4-4. Since you’re running it in a container, if you\nadd a dependency to your application, you can include the dependency in your Dockerfile or Docker\nimage.', 'As also shown in Figure 4-4, the publish flow pushes an image through a container registry. This can\nbe the Azure Container Registry (a registry close to your deployments in Azure and secured by Azure\nActive Directory groups and accounts), or any other Docker registry, like Docker Hub or an onpremises registry.', 'In most cases, you can think of a container as an instance of a process. A process doesn’t maintain\npersistent state. While a container can write to its local storage, assuming that an instance will be\naround indefinitely would be like assuming that a single location in memory will be durable. You', '22 CHAPTER 3 | Architecting container and microservice-based applications', 'should assume that container images, like processes, have multiple instances or will eventually be\nkilled. If they’re managed with a container orchestrator, you should assume that they might get\nmoved from one node or VM to another.', 'The following solutions are used to manage data in Docker applications:', '[From the Docker host, as Docker Volumes:](https://docs.docker.com/engine/admin/volumes/)', 'Service-oriented architecture (SOA) was an overused term and has meant different things to different\npeople. But as a common denominator, SOA means that you structure your application by\ndecomposing it into multiple services (most commonly as HTTP services) that can be classified as\ndifferent types like subsystems or tiers.', 'Those services can now be deployed as Docker containers, which solves deployment issues, because\nall the dependencies are included in the container image. However, when you need to scale up SOA\napplications, you might have scalability and availability challenges if you’re deploying based on single\nDocker hosts. This is where Docker clustering software or an orchestrator can help you, as explained in\nlater sections where deployment approaches for microservices are described.', 'Docker containers are useful (but not required) for both traditional service-oriented architectures and\nthe more advanced microservices architectures.', 'Microservices derive from SOA, but SOA is different from microservices architecture. Features like\n[large central brokers, central orchestrators at the organization level, and the Enterprise Service Bus](https://en.wikipedia.org/wiki/Enterprise_service_bus)\n[(ESB)](https://en.wikipedia.org/wiki/Enterprise_service_bus) are typical in SOA. But in most cases, these are anti-patterns in the microservice community. In\nfact, some people argue that “The microservice architecture is SOA done right.”', 'This guide focuses on microservices, because a SOA approach is less prescriptive than the\nrequirements and techniques used in a microservice architecture. If you know how to build a\nmicroservice-based application, you also know how to build a simpler service-oriented application.', 'As the name implies, a microservices architecture is an approach to building a server application as a\nset of small services. That means a microservices architecture is mainly oriented to the back-end,\nalthough the approach is also being used for the front end. Each service runs in its own process and\n[communicates with other processes using protocols such as HTTP/HTTPS, WebSockets, or AMQP.](https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol)\nEach microservice implements a specific end-to-end domain or business capability within a certain\ncontext boundary, and each must be developed autonomously and be deployable independently.\nFinally, each microservice should own its related domain data model and domain logic (sovereignty', '25 CHAPTER 3 | Architecting container and microservice-based applications', 'and decentralized data management) and could be based on different data storage technologies\n(SQL, NoSQL) and different programming languages.', 'What size should a microservice be? When developing a microservice, size shouldn’t be the important\npoint. Instead, the important point should be to create loosely coupled services so you have\nautonomy of development, deployment, and scale, for each service. Of course, when identifying and\ndesigning microservices, you should try to make them as small as possible as long as you don’t have\ntoo many direct dependencies with other microservices. More important than the size of the\nmicroservice is the internal cohesion it must have and its independence from other services.', 'Why a microservices architecture? In short, it provides long-term agility. Microservices enable better\nmaintainability in complex, large, and highly-scalable systems by letting you create applications based\non many independently deployable services that each have granular and autonomous lifecycles.', 'As an additional benefit, microservices can scale out independently. Instead of having a single\nmonolithic application that you must scale out as a unit, you can instead scale out specific\nmicroservices. That way, you can scale just the functional area that needs more processing power or\nnetwork bandwidth to support demand, rather than scaling out other areas of the application that\ndon’t need to be scaled. That means cost savings because you need less hardware.', '_Figure 4-6. Monolithic deployment versus the microservices approach_', 'As Figure 4-6 shows, in the traditional monolithic approach, the application scales by cloning the\nwhole app in several servers/VM. In the microservices approach, functionality is segregated in smaller\nservices, so each service can scale independently. The microservices approach allows agile changes\nand rapid iteration of each microservice, because you can change specific, small areas of complex,\nlarge, and scalable applications.', 'Architecting fine-grained microservices-based applications enables continuous integration and\ncontinuous delivery practices. It also accelerates delivery of new functions into the application. Finegrained composition of applications also allows you to run and test microservices in isolation, and to', '26 CHAPTER 3 | Architecting container and microservice-based applications', 'evolve them autonomously while maintaining clear contracts between them. As long as you don’t\nchange the interfaces or contracts, you can change the internal implementation of any microservice or\nadd new functionality without breaking other microservices.', 'The following are important aspects to enable success in going into production with a microservicesbased system:', 'An important rule for microservices architecture is that each microservice must own its domain data\nand logic. Just as a full application owns its logic and data, so must each microservice own its logic\nand data under an autonomous lifecycle, with independent deployment per microservice.', 'This means that the conceptual model of the domain will differ between subsystems or microservices.\nConsider enterprise applications, where customer relationship management (CRM) applications,', '27 CHAPTER 3 | Architecting container and microservice-based applications', 'transactional purchase subsystems, and customer support subsystems each call on unique customer\nentity attributes and data, and where each employs a different Bounded Context (BC).', '[This principle is similar in Domain-driven design (DDD), where each Bounded Context or autonomous](https://en.wikipedia.org/wiki/Domain-driven_design)\nsubsystem or service must own its domain model (data plus logic and behavior). Each DDD Bounded\nContext correlates to one business microservice (one or several services). This point about the\nBounded Context pattern is expanded in the next section.', 'On the other hand, the traditional (monolithic data) approach used in many applications is to have a\nsingle centralized database or just a few databases. This is often a normalized SQL database that’s\nused for the whole application and all its internal subsystems, as shown in Figure 4-7.', '_Figure 4-7. Data sovereignty comparison: monolithic database versus microservices_', 'In the traditional approach, there’s a single database shared across all services, typically in a tiered\narchitecture. In the microservices approach, each microservice owns its model/data. The centralized\ndatabase approach initially looks simpler and seems to enable reuse of entities in different subsystems\nto make everything consistent. But the reality is you end up with huge tables that serve many different\nsubsystems, and that include attributes and columns that aren’t needed in most cases. It’s like trying\nto use the same physical map for hiking a short trail, taking a day-long car trip, and learning\ngeography.', '[A monolithic application with typically a single relational database has two important benefits: ACID](https://en.wikipedia.org/wiki/ACID)\n[transactions](https://en.wikipedia.org/wiki/ACID) and the SQL language, both working across all the tables and data related to your\napplication. This approach provides a way to easily write a query that combines data from multiple\ntables.', 'However, data access becomes much more complicated when you move to a microservices\narchitecture. Even when using ACID transactions within a microservice or Bounded Context, it is crucial\nto consider that the data owned by each microservice is private to that microservice and should only', '28 CHAPTER 3 | Architecting container and microservice-based applications', 'be accessed either synchronously through its API endpoints(REST, gRPC, SOAP, etc) or asynchronously\nvia messaging(AMQP or similar).', 'Encapsulating the data ensures that the microservices are loosely coupled and can evolve\nindependently of one another. If multiple services were accessing the same data, schema updates\nwould require coordinated updates to all the services. This would break the microservice lifecycle\nautonomy. But distributed data structures mean that you can’t make a single ACID transaction across\nmicroservices. This in turn means you must use eventual consistency when a business process spans\nmultiple microservices. This is much harder to implement than simple SQL joins, because you can’t\ncreate integrity constraints or use distributed transactions between separate databases, as we’ll\nexplain later on. Similarly, many other relational database features aren’t available across multiple\nmicroservices.', 'Going even further, different microservices often use different _kinds_ of databases. Modern\napplications store and process diverse kinds of data, and a relational database isn’t always the best\nchoice. For some use cases, a NoSQL database such as Azure CosmosDB or MongoDB might have a\nmore convenient data model and offer better performance and scalability than a SQL database like\nSQL Server or Azure SQL Database. In other cases, a relational database is still the best approach.\nTherefore, microservices-based applications often use a mixture of SQL and NoSQL databases, which\n[is sometimes called the polyglot persistence approach.](https://martinfowler.com/bliki/PolyglotPersistence.html)', 'A partitioned, polyglot-persistent architecture for data storage has many benefits. These include\nloosely coupled services and better performance, scalability, costs, and manageability. However, it can\nintroduce some distributed data management challenges, as explained in “Identifying domain-model\nboundaries” later in this chapter.', '[The concept of microservice derives from the Bounded Context (BC) pattern](https://martinfowler.com/bliki/BoundedContext.html) [in domain-driven design](https://en.wikipedia.org/wiki/Domain-driven_design)\n[(DDD). DDD deals with large models by dividing them into multiple BCs and being explicit about their](https://en.wikipedia.org/wiki/Domain-driven_design)\nboundaries. Each BC must have its own model and database; likewise, each microservice owns its\n[related data. In addition, each BC usually has its own ubiquitous language](https://martinfowler.com/bliki/UbiquitousLanguage.html) to help communication\nbetween software developers and domain experts.', 'Those terms (mainly domain entities) in the ubiquitous language can have different names in different\nBounded Contexts, even when different domain entities share the same identity (that is, the unique ID\nthat’s used to read the entity from storage). For instance, in a user-profile Bounded Context, the User\ndomain entity might share identity with the Buyer domain entity in the ordering Bounded Context.', 'A microservice is therefore like a Bounded Context, but it also specifies that it’s a distributed service.\nIt’s built as a separate process for each Bounded Context, and it must use the distributed protocols\n[noted earlier, like HTTP/HTTPS, WebSockets, or AMQP. The Bounded Context pattern, however,](https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol)\ndoesn’t specify whether the Bounded Context is a distributed service or if it’s simply a logical\nboundary (such as a generic subsystem) within a monolithic-deployment application.', 'It’s important to highlight that defining a service for each Bounded Context is a good place to start.\nBut you don’t have to constrain your design to it. Sometimes you must design a Bounded Context or', '29 CHAPTER 3 | Architecting container and microservice-based applications', 'business microservice composed of several physical services. But ultimately, both patterns -Bounded\nContext and microservice- are closely related.', 'DDD benefits from microservices by getting real boundaries in the form of distributed microservices.\nBut ideas like not sharing the model between microservices are what you also want in a Bounded\nContext.', '**Additional resources**', 'It’s useful at this point to stop and discuss the distinction between logical architecture and physical\narchitecture, and how this applies to the design of microservice-based applications.', 'To begin, building microservices doesn’t require the use of any specific technology. For instance,\nDocker containers aren’t mandatory to create a microservice-based architecture. Those microservices\ncould also be run as plain processes. Microservices is a logical architecture.', 'Moreover, even when a microservice could be physically implemented as a single service, process, or\n[container (for simplicity’s sake, that’s the approach taken in the initial version of eShopOnContainers),](https://aka.ms/MicroservicesArchitecture)\nthis parity between business microservice and physical service or container isn’t necessarily required in\nall cases when you build a large and complex application composed of many dozens or even\nhundreds of services.', 'This is where there’s a difference between an application’s logical architecture and physical\narchitecture. The logical architecture and logical boundaries of a system do not necessarily map oneto-one to the physical or deployment architecture. It can happen, but it often doesn’t.', 'Although you might have identified certain business microservices or Bounded Contexts, it doesn’t\nmean that the best way to implement them is always by creating a single service (such as an ASP.NET\nWeb API) or single Docker container for each business microservice. Having a rule saying each\nbusiness microservice has to be implemented using a single service or container is too rigid.', 'Therefore, a business microservice or Bounded Context is a logical architecture that might coincide (or\nnot) with physical architecture. The important point is that a business microservice or Bounded\nContext must be autonomous by allowing code and state to be independently versioned, deployed,\nand scaled.', '30 CHAPTER 3 | Architecting container and microservice-based applications', 'As Figure 4-8 shows, the catalog business microservice could be composed of several services or\nprocesses. These could be multiple ASP.NET Web API services or any other kind of services using\nHTTP or any other protocol. More importantly, the services could share the same data, as long as\nthese services are cohesive with respect to the same business domain.', '_Figure 4-8. Business microservice with several physical services_', 'The services in the example share the same data model because the Web API service targets the same\ndata as the Search service. So, in the physical implementation of the business microservice, you’re\nsplitting that functionality so you can scale each of those internal services up or down as needed.\nMaybe the Web API service usually needs more instances than the Search service, or vice versa.', 'In short, the logical architecture of microservices doesn’t always have to coincide with the physical\ndeployment architecture. In this guide, whenever we mention a microservice, we mean a business or\nlogical microservice that could map to one or more (physical) services. In most cases, this will be a\nsingle service, but it might be more.', 'Defining microservice boundaries is probably the first challenge anyone encounters. Each microservice\nhas to be a piece of your application and each microservice should be autonomous with all the\nbenefits and challenges that it conveys. But how do you identify those boundaries?', 'First, you need to focus on the application’s logical domain models and related data. Try to identify\ndecoupled islands of data and different contexts within the same application. Each context could have\na different business language (different business terms). The contexts should be defined and managed\nindependently. The terms and entities that are used in those different contexts might sound similar,\nbut you might discover that in a particular context, a business concept with one is used for a different', '31 CHAPTER 3 | Architecting container and microservice-based applications', 'purpose in another context, and might even have a different name. For instance, a user can be\nreferred as a user in the identity or membership context, as a customer in a CRM context, as a buyer in\nan ordering context, and so forth.', 'The way you identify boundaries between multiple application contexts with a different domain for\neach context is exactly how you can identify the boundaries for each business microservice and its\nrelated domain model and data. You always attempt to minimize the coupling between those\nmicroservices. This guide goes into more detail about this identification and domain model design in\nthe section Identifying domain-model boundaries for each microservice later.', 'A second challenge is how to implement queries that retrieve data from several microservices, while\navoiding chatty communication to the microservices from remote client apps. An example could be a\nsingle screen from a mobile app that needs to show user information that’s owned by the basket,\ncatalog, and user identity microservices. Another example would be a complex report involving many\ntables located in multiple microservices. The right solution depends on the complexity of the queries.\nBut in any case, you’ll need a way to aggregate information if you want to improve the efficiency in\nthe communications of your system. The most popular solutions are the following.', '**API Gateway.** For simple data aggregation from multiple microservices that own different databases,\nthe recommended approach is an aggregation microservice referred to as an API Gateway. However,\nyou need to be careful about implementing this pattern, because it can be a choke point in your\nsystem, and it can violate the principle of microservice autonomy. To mitigate this possibility, you can\nhave multiple fined-grained API Gateways each one focusing on a vertical “slice” or business area of\nthe system. The API Gateway pattern is explained in more detail in the API Gateway section later.', '**GraphQL Federation** One option to consider if your microservices are already using GraphQL is\n[GraphQL Federation. Federation allows you to define “subgraphs” from other services and compose](https://www.apollographql.com/docs/federation/)\nthem into an aggregate “supergraph” that acts as a standalone schema.', '**CQRS with query/reads tables.** Another solution for aggregating data from multiple microservices is\n[the Materialized View pattern. In this approach, you generate, in advance (prepare denormalized data](https://docs.microsoft.com/azure/architecture/patterns/materialized-view)\nbefore the actual queries happen), a read-only table with the data that’s owned by multiple\nmicroservices. The table has a format suited to the client app’s needs.', 'Consider something like the screen for a mobile app. If you have a single database, you might pull\ntogether the data for that screen using a SQL query that performs a complex join involving multiple\ntables. However, when you have multiple databases, and each database is owned by a different\nmicroservice, you cannot query those databases and create a SQL join. Your complex query becomes\na challenge. You can address the requirement using a CQRS approach—you create a denormalized\ntable in a different database that’s used just for queries. The table can be designed specifically for the\ndata you need for the complex query, with a one-to-one relationship between fields needed by your\napplication’s screen and the columns in the query table. It could also serve for reporting purposes.', 'This approach not only solves the original problem (how to query and join across microservices), but it\nalso improves performance considerably when compared with a complex join, because you already', '32 CHAPTER 3 | Architecting container and microservice-based applications', 'have the data that the application needs in the query table. Of course, using Command and Query\nResponsibility Segregation (CQRS) with query/reads tables means additional development work, and\nyou’ll need to embrace eventual consistency. Nonetheless, requirements on performance and high\n[scalability in collaborative scenarios (or competitive scenarios, depending on the point of view) are](http://udidahan.com/2011/10/02/why-you-should-be-using-cqrs-almost-everywhere/)\nwhere you should apply CQRS with multiple databases.', '**“Cold data” in central databases.** For complex reports and queries that might not require real-time\ndata, a common approach is to export your “hot data” (transactional data from the microservices) as\n“cold data” into large databases that are used only for reporting. That central database system can be\na Big Data-based system, like Hadoop; a data warehouse like one based on Azure SQL Data\nWarehouse; or even a single SQL database that’s used just for reports (if size won’t be an issue).', 'Keep in mind that this centralized database would be used only for queries and reports that do not\nneed real-time data. The original updates and transactions, as your source of truth, have to be in your\nmicroservices data. The way you would synchronize data would be either by using event-driven\ncommunication (covered in the next sections) or by using other database infrastructure import/export\ntools. If you use event-driven communication, that integration process would be similar to the way\nyou propagate data as described earlier for CQRS query tables.', 'However, if your application design involves constantly aggregating information from multiple\nmicroservices for complex queries, it might be a symptom of a bad design -a microservice should be\nas isolated as possible from other microservices. (This excludes reports/analytics that always should\nuse cold-data central databases.) Having this problem often might be a reason to merge\nmicroservices. You need to balance the autonomy of evolution and deployment of each microservice\nwith strong dependencies, cohesion, and data aggregation.', 'As stated previously, the data owned by each microservice is private to that microservice and can only\nbe accessed using its microservice API. Therefore, a challenge presented is how to implement end-toend business processes while keeping consistency across multiple microservices.', '[To analyze this problem, let’s look at an example from the eShopOnContainers reference application.](https://aka.ms/eshoponcontainers)\nThe Catalog microservice maintains information about all the products, including the product price.\nThe Basket microservice manages temporal data about product items that users are adding to their\nshopping baskets, which includes the price of the items at the time they were added to the basket.\nWhen a product’s price is updated in the catalog, that price should also be updated in the active\nbaskets that hold that same product, plus the system should probably warn the user saying that a\nparticular item’s price has changed since they added it to their basket.', 'In a hypothetical monolithic version of this application, when the price changes in the products table,\nthe catalog subsystem could simply use an ACID transaction to update the current price in the Basket\ntable.', 'However, in a microservices-based application, the Product and Basket tables are owned by their\nrespective microservices. No microservice should ever include tables/storage owned by another\nmicroservice in its own transactions, not even in direct queries, as shown in Figure 4-9.', '33 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-9. A microservice can’t directly access a table in another microservice_', 'The Catalog microservice shouldn’t update the Basket table directly, because the Basket table is\nowned by the Basket microservice. To make an update to the Basket microservice, the Catalog\nmicroservice should use eventual consistency probably based on asynchronous communication such\n[as integration events (message and event-based communication). This is how the eShopOnContainers](https://aka.ms/eshoponcontainers)\nreference application performs this type of consistency across microservices.', '[As stated by the CAP theorem, you need to choose between availability and ACID strong consistency.](https://en.wikipedia.org/wiki/CAP_theorem)\nMost microservice-based scenarios demand availability and high scalability as opposed to strong\nconsistency. Mission-critical applications must remain up and running, and developers can work\naround strong consistency by using techniques for working with weak or eventual consistency. This is\nthe approach taken by most microservice-based architectures.', 'Moreover, ACID-style or two-phase commit transactions are not just against microservices principles;\nmost NoSQL databases (like Azure Cosmos DB, MongoDB, etc.) do not support two-phase commit\ntransactions, typical in distributed databases scenarios. However, maintaining data consistency across\nservices and databases is essential. This challenge is also related to the question of how to propagate\nchanges across multiple microservices when certain data needs to be redundant—for example, when\nyou need to have the product’s name or description in the Catalog microservice and the Basket\nmicroservice.', 'A good solution for this problem is to use eventual consistency between microservices articulated\nthrough event-driven communication and a publish-and-subscribe system. These topics are covered\nin the section Asynchronous event-driven communication later in this guide.', '34 CHAPTER 3 | Architecting container and microservice-based applications', 'Communicating across microservice boundaries is a real challenge. In this context, communication\ndoesn’t refer to what protocol you should use (HTTP and REST, AMQP, messaging, and so on). Instead,\nit addresses what communication style you should use, and especially how coupled your\nmicroservices should be. Depending on the level of coupling, when failure occurs, the impact of that\nfailure on your system will vary significantly.', 'In a distributed system like a microservices-based application, with so many artifacts moving around\nand with distributed services across many servers or hosts, components will eventually fail. Partial\nfailure and even larger outages will occur, so you need to design your microservices and the\ncommunication across them considering the common risks in this type of distributed system.', 'A popular approach is to implement HTTP (REST)-based microservices, due to their simplicity. An\nHTTP-based approach is perfectly acceptable; the issue here is related to how you use it. If you use\nHTTP requests and responses just to interact with your microservices from client applications or from\nAPI Gateways, that’s fine. But if you create long chains of synchronous HTTP calls across microservices,\ncommunicating across their boundaries as if the microservices were objects in a monolithic\napplication, your application will eventually run into problems.', 'For instance, imagine that your client application makes an HTTP API call to an individual microservice\nlike the Ordering microservice. If the Ordering microservice in turn calls additional microservices using\nHTTP within the same request/response cycle, you’re creating a chain of HTTP calls. It might sound\nreasonable initially. However, there are important points to consider when going down this path:', 'The goal when identifying model boundaries and size for each microservice isn’t to get to the most\ngranular separation possible, although you should tend toward small microservices if possible.\nInstead, your goal should be to get to the most meaningful separation guided by your domain\nknowledge. The emphasis isn’t on the size, but instead on business capabilities. In addition, if there’s\nclear cohesion needed for a certain area of the application based on a high number of dependencies,\nthat indicates the need for a single microservice, too. Cohesion is a way to identify how to break apart', '36 CHAPTER 3 | Architecting container and microservice-based applications', 'or group together microservices. Ultimately, while you gain more knowledge about the domain, you\nshould adapt the size of your microservice, iteratively. Finding the right size isn’t a one-shot process.', '[Sam Newman, a recognized promoter of microservices and author of the book Building Microservices,](https://samnewman.io/)\nhighlights that you should design your microservices based on the Bounded Context (BC) pattern\n(part of domain-driven design), as introduced earlier. Sometimes, a BC could be composed of several\nphysical services, but not vice versa.', 'A domain model with specific domain entities applies within a concrete BC or microservice. A BC\ndelimits the applicability of a domain model and gives developer team members a clear and shared\nunderstanding of what must be cohesive and what can be developed independently. These are the\nsame goals for microservices.', '[Another tool that informs your design choice is Conway’s law, which states that an application will](https://en.wikipedia.org/wiki/Conway%27s_law)\nreflect the social boundaries of the organization that produced it. But sometimes the opposite is true the company’s organization is formed by the software. You might need to reverse Conway’s law and\nbuild the boundaries the way you want the company to be organized, leaning toward business\nprocess consulting.', '[To identify bounded contexts, you can use a DDD pattern called the Context Mapping pattern. With](https://www.infoq.com/articles/ddd-contextmapping)\nContext Mapping, you identify the various contexts in the application and their boundaries. It’s\ncommon to have a different context and boundary for each small subsystem, for instance. The Context\nMap is a way to define and make explicit those boundaries between domains. A BC is autonomous\nand includes the details of a single domain -details like the domain entities- and defines integration\ncontracts with other BCs. This is similar to the definition of a microservice: it’s autonomous, it\nimplements certain domain capability, and it must provide interfaces. This is why Context Mapping\nand the Bounded Context pattern are good approaches for identifying the domain model boundaries\nof your microservices.', 'When designing a large application, you’ll see how its domain model can be fragmented - a domain\nexpert from the catalog domain will name entities differently in the catalog and inventory domains\nthan a shipping domain expert, for instance. Or the user domain entity might be different in size and\nnumber of attributes when dealing with a CRM expert who wants to store every detail about the\ncustomer than for an ordering domain expert who just needs partial data about the customer. It’s very\nhard to disambiguate all domain terms across all the domains related to a large application. But the\nmost important thing is that you shouldn’t try to unify the terms. Instead, accept the differences and\nrichness provided by each domain. If you try to have a unified database for the whole application,\nattempts at a unified vocabulary will be awkward and won’t sound right to any of the multiple domain\nexperts. Therefore, BCs (implemented as microservices) will help you to clarify where you can use\ncertain domain terms and where you’ll need to split the system and create additional BCs with\ndifferent domains.', 'You’ll know that you got the right boundaries and sizes of each BC and domain model if you have few\nstrong relationships between domain models, and you do not usually need to merge information\nfrom multiple domain models when performing typical application operations.', 'Perhaps the best answer to the question of how large a domain model for each microservice should\nbe is the following: it should have an autonomous BC, as isolated as possible, that enables you to\nwork without having to constantly switch to other contexts (other microservice’s models). In Figure 4', '37 CHAPTER 3 | Architecting container and microservice-based applications', '10, you can see how multiple microservices (multiple BCs) each has their own model and how their\nentities can be defined, depending on the specific requirements for each of the identified domains in\nyour application.', '_Figure 4-10. Identifying entities and microservice model boundaries_', 'Figure 4-10 illustrates a sample scenario related to an online conference management system. The\nsame entity appears as “Users”, “Buyers”, “Payers”, and “Customers” depending on the bounded\ncontext. You’ve identified several BCs that could be implemented as microservices, based on domains\nthat domain experts defined for you. As you can see, there are entities that are present just in a single\nmicroservice model, like Payments in the Payment microservice. Those will be easy to implement.', 'However, you might also have entities that have a different shape but share the same identity across\nthe multiple domain models from the multiple microservices. For example, the User entity is identified\nin the Conferences Management microservice. That same user, with the same identity, is the one\nnamed Buyers in the Ordering microservice, or the one named Payer in the Payment microservice, and\neven the one named Customer in the Customer Service microservice. This is because, depending on\n[the ubiquitous language](https://martinfowler.com/bliki/UbiquitousLanguage.html) that each domain expert is using, a user might have a different perspective\neven with different attributes. The user entity in the microservice model named Conferences\nManagement might have most of its personal data attributes. However, that same user in the shape of\nPayer in the microservice Payment or in the shape of Customer in the microservice Customer Service\nmight not need the same list of attributes.', 'A similar approach is illustrated in Figure 4-11.', '38 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-11. Decomposing traditional data models into multiple domain models_', 'When decomposing a traditional data model between bounded contexts, you can have different\nentities that share the same identity (a buyer is also a user) with different attributes in each bounded\ncontext. You can see how the user is present in the Conferences Management microservice model as\nthe User entity and is also present in the form of the Buyer entity in the Pricing microservice, with\nalternate attributes or details about the user when it’s actually a buyer. Each microservice or BC might\nnot need all the data related to a User entity, just part of it, depending on the problem to solve or the\ncontext. For instance, in the Pricing microservice model, you do not need the address or the name of\nthe user, just the ID (as identity) and Status, which will have an impact on discounts when pricing the\nseats per buyer.', 'The Seat entity has the same name but different attributes in each domain model. However, Seat\nshares identity based on the same ID, as happens with User and Buyer.', 'Basically, there’s a shared concept of a user that exists in multiple services (domains), which all share\nthe identity of that user. But in each domain model there might be additional or different details\nabout the user entity. Therefore, there needs to be a way to map a user entity from one domain\n(microservice) to another.', 'There are several benefits to not sharing the same user entity with the same number of attributes\nacross domains. One benefit is to reduce duplication, so that microservice models do not have any\ndata that they do not need. Another benefit is having a primary microservice that owns a certain type\nof data per entity so that updates and queries for that type of data are driven only by that\nmicroservice.', '39 CHAPTER 3 | Architecting container and microservice-based applications', 'In a microservices architecture, each microservice exposes a set of (typically) fine-grained endpoints.\nThis fact can impact the client-to-microservice communication, as explained in this section.', 'A possible approach is to use a direct client-to-microservice communication architecture. In this\napproach, a client app can make requests directly to some of the microservices, as shown in Figure 412.', '_Figure 4-12. Using a direct client-to-microservice communication architecture_', 'In this approach, each microservice has a public endpoint, sometimes with a different TCP port for\neach microservice. An example of a URL for a particular service could be the following URL in Azure:', 'http://eshoponcontainers.westus.cloudapp.azure.com:88/', 'In a production environment based on a cluster, that URL would map to the load balancer used in the\ncluster, which in turn distributes the requests across the microservices. In production environments,\n[you could have an Application Delivery Controller (ADC) like Azure Application Gateway between your](https://docs.microsoft.com/azure/application-gateway/application-gateway-introduction)\nmicroservices and the Internet. This layer acts as a transparent tier that not only performs load\nbalancing, but secures your services by offering SSL termination. This approach improves the load of\nyour hosts by offloading CPU-intensive SSL termination and other routing duties to the Azure\nApplication Gateway. In any case, a load balancer and ADC are transparent from a logical application\narchitecture point of view.', 'A direct client-to-microservice communication architecture could be good enough for a small\nmicroservice-based application, especially if the client app is a server-side web application like an\nASP.NET MVC app. However, when you build large and complex microservice-based applications (for\nexample, when handling dozens of microservice types), and especially when the client apps are\nremote mobile apps or SPA web applications, that approach faces a few issues.', '40 CHAPTER 3 | Architecting container and microservice-based applications', 'Consider the following questions when developing a large application based on microservices:', 'In a microservices architecture, the client apps usually need to consume functionality from more than\none microservice. If that consumption is performed directly, the client needs to handle multiple calls\nto microservice endpoints. What happens when the application evolves and new microservices are\nintroduced or existing microservices are updated? If your application has many microservices,\nhandling so many endpoints from the client apps can be a nightmare. Since the client app would be\ncoupled to those internal endpoints, evolving the microservices in the future can cause high impact\nfor the client apps.', 'Therefore, having an intermediate level or tier of indirection (Gateway) can be convenient for\nmicroservice-based applications. If you don’t have API Gateways, the client apps must send requests\ndirectly to the microservices and that raises problems, such as the following issues:', '41 CHAPTER 3 | Architecting container and microservice-based applications', 'When you design and build large or complex microservice-based applications with multiple client\n[apps, a good approach to consider can be an API Gateway. This pattern is a service that provides a](https://microservices.io/patterns/apigateway.html)\n[single-entry point for certain groups of microservices. It’s similar to the Facade pattern from object-](https://en.wikipedia.org/wiki/Facade_pattern)\noriented design, but in this case, it’s part of a distributed system. The API Gateway pattern is also\n[sometimes known as the “backend for frontend” (BFF) because you build it while thinking about the](https://samnewman.io/patterns/architectural/bff/)\nneeds of the client app.', 'Therefore, the API gateway sits between the client apps and the microservices. It acts as a reverse\nproxy, routing requests from clients to services. It can also provide other cross-cutting features such\nas authentication, SSL termination, and cache.', 'Figure 4-13 shows how a custom API Gateway can fit into a simplified microservice-based architecture\nwith just a few microservices.', '42 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-13. Using an API Gateway implemented as a custom service_', 'Apps connect to a single endpoint, the API Gateway, that’s configured to forward requests to\nindividual microservices. In this example, the API Gateway would be implemented as a custom\nASP.NET Core WebHost service running as a container.', 'It’s important to highlight that in that diagram, you would be using a single custom API Gateway\nservice facing multiple and different client apps. That fact can be an important risk because your API\nGateway service will be growing and evolving based on many different requirements from the client\napps. Eventually, it will be bloated because of those different needs and effectively it could be similar\nto a monolithic application or monolithic service. That’s why it’s very much recommended to split the\nAPI Gateway in multiple services or multiple smaller API Gateways, one per client app form-factor\ntype, for instance.', 'You need to be careful when implementing the API Gateway pattern. Usually it isn’t a good idea to\nhave a single API Gateway aggregating all the internal microservices of your application. If it does, it\nacts as a monolithic aggregator or orchestrator and violates microservice autonomy by coupling all\nthe microservices.', 'Therefore, the API Gateways should be segregated based on business boundaries and the client apps\nand not act as a single aggregator for all the internal microservices.', 'When splitting the API Gateway tier into multiple API Gateways, if your application has multiple client\napps, that can be a primary pivot when identifying the multiple API Gateways types, so that you can\nhave a different facade for the needs of each client app. This case is a pattern named “Backend for\n[Frontend” (BFF) where each API Gateway can provide a different API tailored for each client app type,](https://samnewman.io/patterns/architectural/bff/)\npossibly even based on the client form factor by implementing specific adapter code which\nunderneath calls multiple internal microservices, as shown in the following image:', '43 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-13.1. Using multiple custom API Gateways_', 'Figure 4-13.1 shows API Gateways that are segregated by client type; one for mobile clients and one\nfor web clients. A traditional web app connects to an MVC microservice that uses the web API\nGateway. The example depicts a simplified architecture with multiple fine-grained API Gateways. In\nthis case, the boundaries identified for each API Gateway are based purely on the “Backend for\n[Frontend” (BFF) pattern, hence based just on the API needed per client app. But in larger applications](https://samnewman.io/patterns/architectural/bff/)\nyou should also go further and create other API Gateways based on business boundaries as a second\ndesign pivot.', 'An API Gateway can offer multiple features. Depending on the product it might offer richer or simpler\nfeatures, however, the most important and foundational features for any API Gateway are the\nfollowing design patterns:', '**Reverse proxy or gateway routing.** The API Gateway offers a reverse proxy to redirect or route\nrequests (layer 7 routing, usually HTTP requests) to the endpoints of the internal microservices. The\ngateway provides a single endpoint or URL for the client apps and then internally maps the requests\nto a group of internal microservices. This routing feature helps to decouple the client apps from the\nmicroservices but it’s also convenient when modernizing a monolithic API by sitting the API Gateway\nin between the monolithic API and the client apps, then you can add new APIs as new microservices\nwhile still using the legacy monolithic API until it’s split into many microservices in the future. Because\nof the API Gateway, the client apps won’t notice if the APIs being used are implemented as internal\nmicroservices or a monolithic API and more importantly, when evolving and refactoring the\nmonolithic API into microservices, thanks to the API Gateway routing, client apps won’t be impacted\nwith any URI change.', '44 CHAPTER 3 | Architecting container and microservice-based applications', '[For more information, see Gateway routing pattern.](https://docs.microsoft.com/azure/architecture/patterns/gateway-routing)', '**Requests aggregation.** As part of the gateway pattern you can aggregate multiple client requests\n(usually HTTP requests) targeting multiple internal microservices into a single client request. This\npattern is especially convenient when a client page/screen needs information from several\nmicroservices. With this approach, the client app sends a single request to the API Gateway that\ndispatches several requests to the internal microservices and then aggregates the results and sends\neverything back to the client app. The main benefit and goal of this design pattern is to reduce\nchattiness between the client apps and the backend API, which is especially important for remote\napps out of the datacenter where the microservices live, like mobile apps or requests coming from\nSPA apps that come from JavaScript in client remote browsers. For regular web apps performing the\nrequests in the server environment (like an ASP.NET Core MVC web app), this pattern is not so\nimportant as the latency is very much smaller than for remote client apps.', 'Depending on the API Gateway product you use, it might be able to perform this aggregation.\nHowever, in many cases it’s more flexible to create aggregation microservices under the scope of the\nAPI Gateway, so you define the aggregation in code (that is, C# code):', '[For more information, see Gateway aggregation pattern.](https://docs.microsoft.com/azure/architecture/patterns/gateway-aggregation)', '**Cross-cutting concerns or gateway offloading.** Depending on the features offered by each API\nGateway product, you can offload functionality from individual microservices to the gateway, which\nsimplifies the implementation of each microservice by consolidating cross-cutting concerns into one\ntier. This approach is especially convenient for specialized features that can be complex to implement\nproperly in every internal microservice, such as the following functionality:', 'There can be many more cross-cutting concerns offered by the API Gateways products depending on\neach implementation. We’ll explore here:', 'In a monolithic application running on a single process, components invoke one another using\nlanguage-level method or function calls. These can be strongly coupled if you’re creating objects with\ncode (for example, new ClassName()), or can be invoked in a decoupled way if you’re using\nDependency Injection by referencing abstractions rather than concrete object instances. Either way,\nthe objects are running within the same process. The biggest challenge when changing from a\nmonolithic application to a microservices-based application lies in changing the communication\nmechanism. A direct conversion from in-process method calls into RPC calls to services will cause a\nchatty and not efficient communication that won’t perform well in distributed environments. The\nchallenges of designing distributed system properly are well enough known that there’s even a canon\n[known as the Fallacies of distributed computing that lists assumptions that developers often make](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)\nwhen moving from monolithic to distributed designs.', 'There isn’t one solution, but several. One solution involves isolating the business microservices as\nmuch as possible. You then use asynchronous communication between the internal microservices and\nreplace fine-grained communication that’s typical in intra-process communication between objects\nwith coarser-grained communication. You can do this by grouping calls, and by returning data that\naggregates the results of multiple internal calls, to the client.', '48 CHAPTER 3 | Architecting container and microservice-based applications', 'A microservices-based application is a distributed system running on multiple processes or services,\nusually even across multiple servers or hosts. Each service instance is typically a process. Therefore,\nservices must interact using an inter-process communication protocol such as HTTP, AMQP, or a\nbinary protocol like TCP, depending on the nature of each service.', '[The microservice community promotes the philosophy of “smart endpoints and dumb pipes”. This](https://simplicable.com/new/smart-endpoints-and-dumb-pipes)\nslogan encourages a design that’s as decoupled as possible between microservices, and as cohesive\nas possible within a single microservice. As explained earlier, each microservice owns its own data and\nits own domain logic. But the microservices composing an end-to-end application are usually simply\nchoreographed by using REST communications rather than complex protocols such as WS-* and\nflexible event-driven communications instead of centralized business-process-orchestrators.', 'The two commonly used protocols are HTTP request/response with resource APIs (when querying\nmost of all), and lightweight asynchronous messaging when communicating updates across multiple\nmicroservices. These are explained in more detail in the following sections.', 'Client and services can communicate through many different types of communication, each one\ntargeting a different scenario and goals. Initially, those types of communications can be classified in\ntwo axes.', 'The first axis defines if the protocol is synchronous or asynchronous:', 'As mentioned, the important point when building a microservices-based application is the way you\nintegrate your microservices. Ideally, you should try to minimize the communication between the\ninternal microservices. The fewer communications between microservices, the better. But in many\ncases, you’ll have to somehow integrate the microservices. When you need to do that, the critical rule\nhere is that the communication between the microservices should be asynchronous. That doesn’t\nmean that you have to use a specific protocol (for example, asynchronous messaging versus\nsynchronous HTTP). It just means that the communication between microservices should be done only\nby propagating data asynchronously, but try not to depend on other internal microservices as part of\nthe initial service’s HTTP request/response operation.', 'If possible, never depend on synchronous communication (request/response) between multiple\nmicroservices, not even for queries. The goal of each microservice is to be autonomous and available\nto the client consumer, even if the other services that are part of the end-to-end application are down\nor unhealthy. If you think you need to make a call from one microservice to other microservices (like\nperforming an HTTP request for a data query) to be able to provide a response to a client application,\nyou have an architecture that won’t be resilient when some microservices fail.', 'Moreover, having HTTP dependencies between microservices, like when creating long\nrequest/response cycles with HTTP request chains, as shown in the first part of the Figure 4-15, not\nonly makes your microservices not autonomous but also their performance is impacted as soon as\none of the services in that chain isn’t performing well.', 'The more you add synchronous dependencies between microservices, such as query requests, the\nworse the overall response time gets for the client apps.', '50 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-15. Anti-patterns and patterns in communication between microservices_', 'As shown in the above diagram, in synchronous communication a “chain” of requests is created\nbetween microservices while serving the client request. This is an anti-pattern. In asynchronous\ncommunication microservices use asynchronous messages or http polling to communicate with other\nmicroservices, but the client request is served right away.', 'If your microservice needs to raise an additional action in another microservice, if possible, do not\nperform that action synchronously and as part of the original microservice request and reply\noperation. Instead, do it asynchronously (using asynchronous messaging or integration events,\nqueues, etc.). But, as much as possible, do not invoke the action synchronously as part of the original\nsynchronous request and reply operation.', 'And finally (and this is where most of the issues arise when building microservices), if your initial\nmicroservice needs data that’s originally owned by other microservices, do not rely on making\nsynchronous requests for that data. Instead, replicate or propagate that data (only the attributes you\nneed) into the initial service’s database by using eventual consistency (typically by using integration\nevents, as explained in upcoming sections).', 'As noted earlier in the Identifying domain-model boundaries for each microservice section,\nduplicating some data across several microservices isn’t an incorrect design—on the contrary, when\ndoing that you can translate the data into the specific language or terms of that additional domain or\n[Bounded Context. For instance, in the eShopOnContainers application](https://github.com/dotnet-architecture/eShopOnContainers) you have a microservice named\nidentity-api that’s in charge of most of the user’s data with an entity named User. However, when you\nneed to store data about the user within the Ordering microservice, you store it as a different entity\nnamed Buyer. The Buyer entity shares the same identity with the original User entity, but it might have\nonly the few attributes needed by the Ordering domain, and not the whole user profile.', '51 CHAPTER 3 | Architecting container and microservice-based applications', 'You might use any protocol to communicate and propagate data asynchronously across microservices\nin order to have eventual consistency. As mentioned, you could use integration events using an event\nbus or message broker or you could even use HTTP by polling the other services instead. It doesn’t\nmatter. The important rule is to not create synchronous dependencies between your microservices.', 'The following sections explain the multiple communication styles you can consider using in a\nmicroservice-based application.', 'There are many protocols and choices you can use for communication, depending on the\ncommunication type you want to use. If you’re using a synchronous request/response-based\ncommunication mechanism, protocols such as HTTP and REST approaches are the most common,\nespecially if you’re publishing your services outside the Docker host or microservice cluster. If you’re\ncommunicating between services internally (within your Docker host or microservices cluster), you\nmight also want to use binary format communication mechanisms (like WCF using TCP and binary\nformat). Alternatively, you can use asynchronous, message-based communication mechanisms such as\nAMQP.', 'There are also multiple message formats like JSON or XML, or even binary formats, which can be more\nefficient. If your chosen binary format isn’t a standard, it’s probably not a good idea to publicly\npublish your services using that format. You could use a non-standard format for internal\ncommunication between your microservices. You might do this when communicating between\nmicroservices within your Docker host or microservice cluster (for example, Docker orchestrators), or\nfor proprietary client applications that talk to the microservices.', '**Request/response communication with HTTP and REST**', 'When a client uses request/response communication, it sends a request to a service, then the service\nprocesses the request and sends back a response. Request/response communication is especially well\nsuited for querying data for a real-time UI (a live user interface) from client apps. Therefore, in a\nmicroservice architecture you’ll probably use this communication mechanism for most queries, as\nshown in Figure 4-16.', '_Figure 4-16. Using HTTP request/response communication (synchronous or asynchronous)_', '52 CHAPTER 3 | Architecting container and microservice-based applications', 'When a client uses request/response communication, it assumes that the response will arrive in a\nshort time, typically less than a second, or a few seconds at most. For delayed responses, you need to\n[implement asynchronous communication based on messaging patterns](https://docs.microsoft.com/azure/architecture/patterns/category/messaging) [and messaging technologies,](https://en.wikipedia.org/wiki/Message-oriented_middleware)\nwhich is a different approach that we explain in the next section.', '[A popular architectural style for request/response communication is REST. This approach is based on,](https://en.wikipedia.org/wiki/Representational_state_transfer)\n[and tightly coupled to, the HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) protocol, embracing HTTP verbs like GET, POST, and PUT. REST is the\nmost commonly used architectural communication approach when creating services. You can\nimplement REST services when you develop ASP.NET Core Web API services.', 'There’s additional value when using HTTP REST services as your interface definition language. For\n[instance, if you use Swagger metadata to describe your service API, you can use tools that generate](https://swagger.io/)\nclient stubs that can directly discover and consume your services.', '**Additional resources**', 'Asynchronous messaging and event-driven communication are critical when propagating changes\nacross multiple microservices and their related domain models. As mentioned earlier in the discussion\nmicroservices and Bounded Contexts (BCs), models (User, Customer, Product, Account, etc.) can mean\ndifferent things to different microservices or BCs. That means that when changes occur, you need\nsome way to reconcile changes across the different models. A solution is eventual consistency and\nevent-driven communication based on asynchronous messaging.', 'When using messaging, processes communicate by exchanging messages asynchronously. A client\nmakes a command or a request to a service by sending it a message. If the service needs to reply, it\nsends a different message back to the client. Since it’s a message-based communication, the client\nassumes that the reply won’t be received immediately, and that there might be no response at all.', 'A message is composed by a header (metadata such as identification or security information) and a\nbody. Messages are usually sent through asynchronous protocols like AMQP.', 'The preferred infrastructure for this type of communication in the microservices community is a\nlightweight message broker, which is different than the large brokers and orchestrators used in SOA.\nIn a lightweight message broker, the infrastructure is typically “dumb,” acting only as a message\nbroker, with simple implementations such as RabbitMQ or a scalable service bus in the cloud like', '54 CHAPTER 3 | Architecting container and microservice-based applications', 'Azure Service Bus. In this scenario, most of the “smart” thinking still lives in the endpoints that are\nproducing and consuming messages-that is, in the microservices.', 'Another rule you should try to follow, as much as possible, is to use only asynchronous messaging\nbetween the internal services, and to use synchronous communication (such as HTTP) only from the\nclient apps to the front-end services (API Gateways plus the first level of microservices).', 'There are two kinds of asynchronous messaging communication: single receiver message-based\ncommunication, and multiple receivers message-based communication. The following sections\nprovide details about them.', 'Message-based asynchronous communication with a single receiver means there’s point-to-point\ncommunication that delivers a message to exactly one of the consumers that’s reading from the\nchannel, and that the message is processed just once. However, there are special situations. For\ninstance, in a cloud system that tries to automatically recover from failures, the same message could\nbe sent multiple times. Due to network or other failures, the client has to be able to retry sending\nmessages, and the server has to implement an operation to be idempotent in order to process a\nparticular message just once.', 'Single-receiver message-based communication is especially well suited for sending asynchronous\ncommands from one microservice to another as shown in Figure 4-18 that illustrates this approach.', 'Once you start sending message-based communication (either with commands or events), you should\navoid mixing message-based communication with synchronous HTTP communication.', '_Figure 4-18. A single microservice receiving an asynchronous message_', '55 CHAPTER 3 | Architecting container and microservice-based applications', 'When the commands come from client applications, they can be implemented as HTTP synchronous\ncommands. Use message-based commands when you need higher scalability or when you’re already\nin a message-based business process.', 'As a more flexible approach, you might also want to use a publish/subscribe mechanism so that your\ncommunication from the sender will be available to additional subscriber microservices or to external\n[applications. Thus, it helps you to follow the open/closed principle](https://en.wikipedia.org/wiki/Open/closed_principle) in the sending service. That way,\nadditional subscribers can be added in the future without the need to modify the sender service.', 'When you use a publish/subscribe communication, you might be using an event bus interface to\npublish events to any subscriber.', 'When using asynchronous event-driven communication, a microservice publishes an integration event\nwhen something happens within its domain and another microservice needs to be aware of it, like a\nprice change in a product catalog microservice. Additional microservices subscribe to the events so\nthey can receive them asynchronously. When that happens, the receivers might update their own\ndomain entities, which can cause more integration events to be published. This publish/subscribe\nsystem is performed by using an implementation of an event bus. The event bus can be designed as\nan abstraction or interface, with the API that’s needed to subscribe or unsubscribe to events and to\npublish events. The event bus can also have one or more implementations based on any inter-process\nand messaging broker, like a messaging queue or service bus that supports asynchronous\ncommunication and a publish/subscribe model.', 'If a system uses eventual consistency driven by integration events, it’s recommended that this\napproach is made clear to the end user. The system shouldn’t use an approach that mimics\nintegration events, like SignalR or polling systems from the client. The end user and the business\nowner have to explicitly embrace eventual consistency in the system and realize that in many cases\nthe business doesn’t have any problem with this approach, as long as it’s explicit. This approach is\nimportant because users might expect to see some results immediately and this aspect might not\nhappen with eventual consistency.', 'As noted earlier in the Challenges and solutions for distributed data management section, you can use\nintegration events to implement business tasks that span multiple microservices. Thus, you’ll have\neventual consistency between those services. An eventually consistent transaction is made up of a\ncollection of distributed actions. At each action, the related microservice updates a domain entity and\npublishes another integration event that raises the next action within the same end-to-end business\ntask.', 'An important point is that you might want to communicate to multiple microservices that are\nsubscribed to the same event. To do so, you can use publish/subscribe messaging based on eventdriven communication, as shown in Figure 4-19. This publish/subscribe mechanism isn’t exclusive to\n[the microservice architecture. It’s similar to the way Bounded Contexts](https://martinfowler.com/bliki/BoundedContext.html) in DDD should communicate,\n[or to the way you propagate updates from the write database to the read database in the Command](https://martinfowler.com/bliki/CQRS.html)', '56 CHAPTER 3 | Architecting container and microservice-based applications', '[and Query Responsibility Segregation (CQRS) architecture pattern. The goal is to have eventual](https://martinfowler.com/bliki/CQRS.html)\nconsistency between multiple data sources across your distributed system.', '_Figure 4-19. Asynchronous event-driven message communication_', 'In asynchronous event-driven communication, one microservice publishes events to an event bus and\nmany microservices can subscribe to it, to get notified and act on it. Your implementation will\n[determine what protocol to use for event-driven, message-based communications. AMQP can help](https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol)\nachieve reliable queued communication.', 'When you use an event bus, you might want to use an abstraction level (like an event bus interface)\nbased on a related implementation in classes with code using the API from a message broker like\n[RabbitMQ](https://www.rabbitmq.com/) [or a service bus like Azure Service Bus with Topics. Alternatively, you might want to use a](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-dotnet-how-to-use-topics-subscriptions)\n[higher-level service bus like NServiceBus,](https://particular.net/nservicebus) [MassTransit, or Brighter](https://masstransit.io/) to articulate your event bus and\npublish/subscribe system.', 'The messaging technologies available for implementing your abstract event bus are at different levels.\nFor instance, products like RabbitMQ (a messaging broker transport) and Azure Service Bus sit at a\n[lower level than other products like NServiceBus,](https://particular.net/nservicebus) [MassTransit, or Brighter, which can work on top of](https://masstransit.io/)\nRabbitMQ and Azure Service Bus. Your choice depends on how many rich features at the application\nlevel and out-of-the-box scalability you need for your application. For implementing just a proof-ofconcept event bus for your development environment, as it was done in the eShopOnContainers\nsample, a simple implementation on top of RabbitMQ running on a Docker container might be\nenough.', 'However, for mission-critical and production systems that need hyper-scalability, you might want to\nevaluate Azure Service Bus. For high-level abstractions and features that make the development of\ndistributed applications easier, we recommend that you evaluate other commercial and open-source\n[service buses, such as NServiceBus, MassTransit, and Brighter. Of course, you can build your own](https://particular.net/nservicebus)', '57 CHAPTER 3 | Architecting container and microservice-based applications', 'service-bus features on top of lower-level technologies like RabbitMQ and Docker. But that plumbing\nwork might cost too much for a custom enterprise application.', 'A challenge when implementing an event-driven architecture across multiple microservices is how to\natomically update state in the original microservice while resiliently publishing its related integration\nevent into the event bus, somehow based on transactions. The following are a few ways to accomplish\nthis functionality, although there could be additional approaches as well.', 'A microservice API is a contract between the service and its clients. You’ll be able to evolve a\nmicroservice independently only if you do not break its API contract, which is why the contract is so\nimportant. If you change the contract, it will impact your client applications or your API Gateway.', 'The nature of the API definition depends on which protocol you’re using. For instance, if you’re using\nmessaging, like AMQP, the API consists of the message types. If you’re using HTTP and RESTful\nservices, the API consists of the URLs and the request and response JSON formats.', 'However, even if you’re thoughtful about your initial contract, a service API will need to change over\ntime. When that happens—and especially if your API is a public API consumed by multiple client\napplications — you typically can’t force all clients to upgrade to your new API contract. You usually\nneed to incrementally deploy new versions of a service in a way that both old and new versions of a\nservice contract are running simultaneously. Therefore, it’s important to have a strategy for your\nservice versioning.', 'When the API changes are small, like if you add attributes or parameters to your API, clients that use\nan older API should switch and work with the new version of the service. You might be able to provide\ndefault values for any missing attributes that are required, and the clients might be able to ignore any\nextra response attributes.', 'However, sometimes you need to make major and incompatible changes to a service API. Because\nyou might not be able to force client applications or services to upgrade immediately to the new\nversion, a service must support older versions of the API for some period. If you’re using an HTTPbased mechanism such as REST, one approach is to embed the API version number in the URL or into\nan HTTP header. Then you can decide between implementing both versions of the service\nsimultaneously within the same service instance, or deploying different instances that each handle a\n[version of the API. A good approach for this functionality is the Mediator pattern (for example,](https://en.wikipedia.org/wiki/Mediator_pattern)\n[MediatR library) to decouple the different implementation versions into independent handlers.](https://github.com/jbogard/MediatR)', '[Finally, if you’re using a REST architecture, Hypermedia](https://www.infoq.com/articles/mark-baker-hypermedia) is the best solution for versioning your services\nand allowing evolvable APIs.', 'Each microservice has a unique name (URL) that’s used to resolve its location. Your microservice needs\nto be addressable wherever it’s running. If you have to think about which computer is running a\nparticular microservice, things can go bad quickly. In the same way that DNS resolves a URL to a\nparticular computer, your microservice needs to have a unique name so that its current location is\ndiscoverable. Microservices need addressable names that make them independent from the\ninfrastructure that they’re running on. This approach implies that there’s an interaction between how\n[your service is deployed and how it’s discovered, because there needs to be a service registry. In the](https://microservices.io/patterns/service-registry.html)\nsame vein, when a computer fails, the registry service must be able to indicate where the service is\nnow running.', '[The service registry pattern is a key part of service discovery. The registry is a database containing the](https://microservices.io/patterns/service-registry.html)\nnetwork locations of service instances. A service registry needs to be highly available and up-to-date.\nClients could cache network locations obtained from the service registry. However, that information\neventually goes out of date and clients can no longer discover service instances. So, a service registry\nconsists of a cluster of servers that use a replication protocol to maintain consistency.', 'In some microservice deployment environments (called clusters, to be covered in a later section),\nservice discovery is built in. For example, an Azure Kubernetes Service (AKS) environment can handle\nservice instance registration and deregistration. It also runs a proxy on each cluster host that plays the\nrole of server-side discovery router.', 'Microservices architecture often starts with the server-side handling data and logic, but, in many\ncases, the UI is still handled as a monolith. However, a more advanced approach, called micro\n[frontends, is to design your application UI based on microservices as well. That means having a](https://martinfowler.com/articles/micro-frontends.html)\ncomposite UI produced by the microservices, instead of having microservices on the server and just a\nmonolithic client app consuming the microservices. With this approach, the microservices you build\ncan be complete with both logic and visual representation.', 'Figure 4-20 shows the simpler approach of just consuming microservices from a monolithic client\napplication. Of course, you could have an ASP.NET MVC service in between producing the HTML and\nJavaScript. The figure is a simplification that highlights that you have a single (monolithic) client UI', '60 CHAPTER 3 | Architecting container and microservice-based applications', 'consuming the microservices, which just focus on logic and data and not on the UI shape (HTML and\nJavaScript).', '_Figure 4-20. A monolithic UI application consuming back-end microservices_', 'In contrast, a composite UI is precisely generated and composed by the microservices themselves.\nSome of the microservices drive the visual shape of specific areas of the UI. The key difference is that\nyou have client UI components (TypeScript classes, for example) based on templates, and the datashaping-UI ViewModel for those templates comes from each microservice.', 'At client application start-up time, each of the client UI components (TypeScript classes, for example)\nregisters itself with an infrastructure microservice capable of providing ViewModels for a given\nscenario. If the microservice changes the shape, the UI changes also.', 'Figure 4-21 shows a version of this composite UI approach. This approach is simplified because you\nmight have other microservices that are aggregating granular parts that are based on different\ntechniques. It depends on whether you’re building a traditional web approach (ASP.NET MVC) or an\nSPA (Single Page Application).', '61 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-21. Example of a composite UI application shaped by back-end microservices_', 'Each of those UI composition microservices would be similar to a small API Gateway. But in this case,\neach one is responsible for a small UI area.', 'A composite UI approach that’s driven by microservices can be more challenging or less so,\ndepending on what UI technologies you’re using. For instance, you won’t use the same techniques for\nbuilding a traditional web application that you use for building an SPA or for native mobile app (as\nwhen developing Xamarin apps, which can be more challenging for this approach).', '[The eShopOnContainers](https://aka.ms/MicroservicesArchitecture) sample application uses the monolithic UI approach for multiple reasons.\nFirst, it’s an introduction to microservices and containers. A composite UI is more advanced but also\nrequires further complexity when designing and developing the UI. Second, eShopOnContainers also\nprovides a native mobile app based on Xamarin, which would make it more complex on the client C#\nside.', 'However, we encourage you to use the following references to learn more about composite UI based\non microservices.', 'Dealing with unexpected failures is one of the hardest problems to solve, especially in a distributed\nsystem. Much of the code that developers write involves handling exceptions, and this is also where\nthe most time is spent in testing. The problem is more involved than writing code to handle failures.\nWhat happens when the machine where the microservice is running fails? Not only do you need to\ndetect this microservice failure (a hard problem on its own), but you also need something to restart\nyour microservice.', 'A microservice needs to be resilient to failures and to be able to restart often on another machine for\navailability. This resiliency also comes down to the state that was saved on behalf of the microservice,\nwhere the microservice can recover this state from, and whether the microservice can restart\nsuccessfully. In other words, there needs to be resiliency in the compute capability (the process can\nrestart at any time) as well as resilience in the state or data (no data loss, and the data remains\nconsistent).', 'The problems of resiliency are compounded during other scenarios, such as when failures occur\nduring an application upgrade. The microservice, working with the deployment system, needs to\ndetermine whether it can continue to move forward to the newer version or instead roll back to a\nprevious version to maintain a consistent state. Questions such as whether enough machines are\navailable to keep moving forward and how to recover previous versions of the microservice need to\nbe considered. This approach requires the microservice to emit health information so that the overall\napplication and orchestrator can make these decisions.', 'In addition, resiliency is related to how cloud-based systems must behave. As mentioned, a cloudbased system must embrace failures and must try to automatically recover from them. For instance, in\ncase of network or container failures, client apps or client services must have a strategy to retry\nsending messages or to retry requests, since in many cases failures in the cloud are partial. The\nImplementing Resilient Applications section in this guide addresses how to handle partial failure. It\ndescribes techniques like retries with exponential backoff or the Circuit Breaker pattern in .NET by\n[using libraries like Polly, which offers a large variety of policies to handle this subject.](https://github.com/App-vNext/Polly)', 'It may seem obvious, and it’s often overlooked, but a microservice must report its health and\ndiagnostics. Otherwise, there’s little insight from an operations perspective. Correlating diagnostic\nevents across a set of independent services and dealing with machine clock skews to make sense of', '63 CHAPTER 3 | Architecting container and microservice-based applications', 'the event order is challenging. In the same way that you interact with a microservice over agreedupon protocols and data formats, there’s a need for standardization in how to log health and\ndiagnostic events that ultimately end up in an event store for querying and viewing. In a microservices\napproach, it’s key that different teams agree on a single logging format. There needs to be a\nconsistent approach to viewing diagnostic events in the application.', '**Health checks**', 'Health is different from diagnostics. Health is about the microservice reporting its current state to take\nappropriate actions. A good example is working with upgrade and deployment mechanisms to\nmaintain availability. Although a service might currently be unhealthy due to a process crash or\nmachine reboot, the service might still be operational. The last thing you need is to make this worse\nby performing an upgrade. The best approach is to do an investigation first or allow time for the\nmicroservice to recover. Health events from a microservice help us make informed decisions and, in\neffect, help create self-healing services.', 'In the Implementing health checks in ASP.NET Core services section of this guide, we explain how to\nuse a new ASP.NET HealthChecks library in your microservices so they can report their state to a\nmonitoring service to take appropriate actions.', 'You also have the option of using an excellent open-source library called\n[AspNetCore.Diagnostics.HealthChecks, available on GitHub](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks) [and as a NuGet package. This library also](https://www.nuget.org/packages/Microsoft.AspNetCore.Diagnostics.HealthChecks/)\ndoes health checks, with a twist, it handles two types of checks:', 'Using orchestrators for production-ready applications is essential if your application is based on\nmicroservices or simply split across multiple containers. As introduced previously, in a microservicebased approach, each microservice owns its model and data so that it will be autonomous from a\ndevelopment and deployment point of view. But even if you have a more traditional application that’s\ncomposed of multiple services (like SOA), you’ll also have multiple containers or services comprising a\nsingle business application that need to be deployed as a distributed system. These kinds of systems\nare complex to scale out and manage; therefore, you absolutely need an orchestrator if you want to\nhave a production-ready and scalable multi-container application.', 'Figure 4-23 illustrates deployment into a cluster of an application composed of multiple microservices\n(containers).', '66 CHAPTER 3 | Architecting container and microservice-based applications', '_Figure 4-23. A cluster of containers_', 'You use one container for each service instance. Docker containers are “units of deployment” and a\ncontainer is an instance of a Docker. A host handles many containers. It looks like a logical approach.\nBut how are you handling load-balancing, routing, and orchestrating these composed applications?', 'The plain Docker Engine in single Docker hosts meets the needs of managing single image instances\non one host, but it falls short when it comes to managing multiple containers deployed on multiple\nhosts for more complex distributed applications. In most cases, you need a management platform\nthat will automatically start containers, scale out containers with multiple instances per image,\nsuspend them or shut them down when needed, and ideally also control how they access resources\nlike the network and data storage.', 'To go beyond the management of individual containers or simple composed apps and move toward\nlarger enterprise applications with microservices, you must turn to orchestration and clustering\nplatforms.', 'From an architecture and development point of view, if you’re building large enterprise composed of\nmicroservices-based applications, it’s important to understand the following platforms and products\nthat support advanced scenarios:', '**Clusters and orchestrators.** When you need to scale out applications across many Docker hosts, as\nwhen a large microservice-based application, it’s critical to be able to manage all those hosts as a\nsingle cluster by abstracting the complexity of the underlying platform. That’s what the container\nclusters and orchestrators provide. Kubernetes is an example of an orchestrator, and is available in\nAzure through Azure Kubernetes Service.', '**Schedulers.** _Scheduling_ means to have the capability for an administrator to launch containers in a\ncluster so they also provide a UI. A cluster scheduler has several responsibilities: to use the cluster’s', '67 CHAPTER 3 | Architecting container and microservice-based applications', 'resources efficiently, to set the constraints provided by the user, to efficiently load-balance containers\nacross nodes or hosts, and to be robust against errors while providing high availability.', 'The concepts of a cluster and a scheduler are closely related, so the products provided by different\nvendors often provide both sets of capabilities. The following list shows the most important platform\nand software choices you have for clusters and schedulers. These orchestrators are generally offered\nin public clouds like Azure.', 'Several cloud vendors offer Docker containers support plus Docker clusters and orchestration support,\nincluding Microsoft Azure, Amazon EC2 Container Service, and Google Container Engine. Microsoft\nAzure provides Docker cluster and orchestrator support through Azure Kubernetes Service (AKS).', '68 CHAPTER 3 | Architecting container and microservice-based applications', 'A Kubernetes cluster pools multiple Docker hosts and exposes them as a single virtual Docker host, so\nyou can deploy multiple containers into the cluster and scale-out with any number of container\ninstances. The cluster will handle all the complex management plumbing, like scalability, health, and\nso forth.', 'AKS provides a way to simplify the creation, configuration, and management of a cluster of virtual\nmachines in Azure that are preconfigured to run containerized applications. Using an optimized\nconfiguration of popular open-source scheduling and orchestration tools, AKS enables you to use\nyour existing skills or draw on a large and growing body of community expertise to deploy and\nmanage container-based applications on Microsoft Azure.', 'Azure Kubernetes Service optimizes the configuration of popular Docker clustering open-source tools\nand technologies specifically for Azure. You get an open solution that offers portability for both your\ncontainers and your application configuration. You select the size, the number of hosts, and the\norchestrator tools, and AKS handles everything else.', '_Figure 4-24. Kubernetes cluster’s simplified structure and topology_', '69 CHAPTER 3 | Architecting container and microservice-based applications', 'In figure 4-24, you can see the structure of a Kubernetes cluster where a master node (VM) controls\nmost of the coordination of the cluster and you can deploy containers to the rest of the nodes, which\nare managed as a single pool from an application point of view and allows you to scale to thousands\nor even tens of thousands of containers.', 'In the development environment, Docker announced in July 2018 that Kubernetes can also run in a\n[single development machine (Windows 10 or macOS) by installing Docker Desktop. You can later](https://docs.docker.com/install/)\ndeploy to the cloud (AKS) for further integration tests, as shown in figure 4-25.', '_Figure 4-25. Running Kubernetes in dev machine and the cloud_', 'To begin using AKS, you deploy an AKS cluster from the Azure portal or by using the CLI. For more\n[information on deploying a Kubernetes cluster in Azure, see Deploy an Azure Kubernetes Service](https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal)\n[(AKS) cluster.](https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal)', 'There are no fees for any of the software installed by default as part of AKS. All default options are\nimplemented with open-source software. AKS is available for multiple virtual machines in Azure.\nYou’re charged only for the compute instances you choose, and the other underlying infrastructure\nresources consumed, such as storage and networking. There are no incremental charges for AKS itself.', 'The default production deployment option for Kubernetes is to use Helm charts, which are introduced\nin the next section.', '70 CHAPTER 3 | Architecting container and microservice-based applications', 'When deploying an application to a Kubernetes cluster, you can use the original kubectl.exe CLI tool\nusing deployment files based on the native format (.yaml files), as already mentioned in the previous\nsection. However, for more complex Kubernetes applications such as when deploying complex\n[microservice-based applications, it’s recommended to use Helm.](https://helm.sh/)', 'Helm Charts helps you define, version, install, share, upgrade, or rollback even the most complex\nKubernetes application.', 'Going further, Helm usage is also recommended because other Kubernetes environments in Azure,\n[such as Azure Dev Spaces are also based on Helm charts.](https://docs.microsoft.com/azure/dev-spaces/azure-dev-spaces)', '[Helm is maintained by the Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/) - in collaboration with\nMicrosoft, Google, Bitnami, and the Helm contributor community.', '[For more implementation information on Helm charts and Kubernetes, see the Using Helm Charts to](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Deploy-to-Azure-Kubernetes-Service-(AKS))\n[deploy eShopOnContainers to AKS post.](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Deploy-to-Azure-Kubernetes-Service-(AKS))', '_Develop containerized .NET applications the way you like, either Integrated Development Environment_\n_(IDE) focused with Visual Studio and Visual Studio tools for Docker or CLI/Editor focused with Docker CLI_\n_and Visual Studio Code._', 'Whether you prefer a full and powerful IDE or a lightweight and agile editor, Microsoft has tools that\nyou can use for developing Docker applications.', '**Visual Studio (for Windows).** Docker-based .NET 7 application development with Visual Studio\nrequires Visual Studio 2022 version 17.0 or later. Visual Studio 2022 comes with tools for Docker\nalready built in. The tools for Docker let you develop, run, and validate your applications directly in the\ntarget Docker environment. You can press F5 to run and debug your application (single container or\nmultiple containers) directly into a Docker host, or press CTRL + F5 to edit and refresh your\napplication without having to rebuild the container. This IDE is the most powerful development choice\nfor Docker-based apps.', '**Visual Studio for Mac.** It’s an IDE, evolution of Xamarin Studio, running in macOS. This tool should\nbe the preferred choice for developers working in macOS machines who also want to use a powerful\nIDE.', '**Visual Studio Code and Docker CLI** . If you prefer a lightweight and cross-platform editor that\nsupports any development language, you can use Visual Studio Code and the Docker CLI. This IDE is a\ncross-platform development approach for macOS, Linux, and Windows. Additionally, Visual Studio\nCode supports extensions for Docker such as IntelliSense for Dockerfiles and shortcut tasks to run\nDocker commands from the editor.', '[By installing Docker Desktop, you can use a single Docker CLI to build apps for both Windows and](https://hub.docker.com/search/?type=edition&offering=community)\nLinux.', '72 CHAPTER 4 | Development process for Docker-based applications', 'As mentioned in earlier sections of this guide, you can use .NET Framework, .NET 7, or the opensource Mono project when developing Docker containerized .NET applications. You can develop in\nC#, F#, or Visual Basic when targeting Linux or Windows Containers, depending on which .NET\n[framework is in use. For more details about.NET languages, see the blog post The .NET Language](https://devblogs.microsoft.com/dotnet/the-net-language-strategy/)\n[Strategy.](https://devblogs.microsoft.com/dotnet/the-net-language-strategy/)', 'The application development life cycle starts at your computer, as a developer, where you code the\napplication using your preferred language and test it locally. With this workflow, no matter which\nlanguage, framework, and platform you choose, you’re always developing and testing Docker\ncontainers, but doing so locally.', 'Each container (an instance of a Docker image) includes the following components:', 'This section describes the _inner-loop_ development workflow for Docker container-based applications.\nThe inner-loop workflow means it’s not considering the broader DevOps workflow, which can include\nup to production deployment, and just focuses on the development work done on the developer’s\ncomputer. The initial steps to set up the environment aren’t included, since those steps are done only\nonce.', '73 CHAPTER 4 | Development process for Docker-based applications', 'An application is composed of your own services plus additional libraries (dependencies). The\nfollowing are the basic steps you usually take when building a Docker application, as illustrated in\nFigure 5-1.', '_Figure 5-1. Step-by-step workflow for developing Docker containerized apps_', 'In this section, this whole process is detailed and every major step is explained by focusing on a Visual\nStudio environment.', 'When you’re using an editor/CLI development approach (for example, Visual Studio Code plus Docker\nCLI on macOS or Windows), you need to know every step, generally in more detail than if you’re using\nVisual Studio. For more information about working in a CLI environment, see the e-book\n[Containerized Docker Application lifecycle with Microsoft Platforms and Tools.](https://aka.ms/dockerlifecycleebook/)', 'When you’re using Visual Studio 2022, many of those steps are handled for you, which dramatically\nimproves your productivity. This is especially true when you’re using Visual Studio 2022 and targeting\nmulti-container applications. For instance, with just one mouse click, Visual Studio adds the Dockerfile\nand docker-compose.yml file to your projects with the configuration for your application. When you\nrun the application in Visual Studio, it builds the Docker image and runs the multi-container\napplication directly in Docker; it even allows you to debug several containers at once. These features\nwill boost your development speed.', 'However, just because Visual Studio makes those steps automatic doesn’t mean that you don’t need\nto know what’s going on underneath with Docker. Therefore, the following guidance details every\nstep.', '74 CHAPTER 4 | Development process for Docker-based applications', 'Developing a Docker application is similar to the way you develop an application without Docker. The\ndifference is that while developing for Docker, you’re deploying and testing your application or\nservices running within Docker containers in your local environment (either a Linux VM setup by\nDocker or directly Windows if using Windows Containers).', '**Set up your local environment with Visual Studio**', '[To begin, make sure you have Docker Desktop for Windows for Windows installed, as explained in the](https://docs.docker.com/docker-for-windows/)\nfollowing instructions:', '[Get started with Docker Desktop for Windows](https://docs.docker.com/docker-for-windows/)', 'In addition, you need Visual Studio 2022 version 17.0, with the **.ASP.NET and web development**\nworkload installed, as shown in Figure 5-2.', '_Figure 5-2. Selecting the ASP.NET and web development workload during Visual Studio 2022 setup_', 'You can start coding your application in plain .NET (usually in .NET Core or later if you’re planning to\nuse containers) even before enabling Docker in your application and deploying and testing in Docker.\nHowever, it is recommended that you start working on Docker as soon as possible, because that will\nbe the real environment and any issues can be discovered as soon as possible. This is encouraged', '75 CHAPTER 4 | Development process for Docker-based applications', 'because Visual Studio makes it so easy to work with Docker that it almost feels transparent—the best\nexample when debugging multi-container applications from Visual Studio.', '**Additional resources**', 'You need a Dockerfile for each custom image you want to build; you also need a Dockerfile for each\ncontainer to be deployed, whether you deploy automatically from Visual Studio or manually using the\nDocker CLI (docker run and docker-compose commands). If your application contains a single custom\nservice, you need a single Dockerfile. If your application contains multiple services (as in a\nmicroservices architecture), you need one Dockerfile for each service.', 'The Dockerfile is placed in the root folder of your application or service. It contains the commands\nthat tell Docker how to set up and run your application or service in a container. You can manually\ncreate a Dockerfile in code and add it to your project along with your .NET dependencies.', 'With Visual Studio and its tools for Docker, this task requires only a few mouse clicks. When you\ncreate a new project in Visual Studio 2022, there’s an option named **Enable Docker Support**, as\nshown in Figure 5-3.', '76 CHAPTER 4 | Development process for Docker-based applications', '_Figure 5-3. Enabling Docker Support when creating a new ASP.NET Core project in Visual Studio 2022_', 'You can also enable Docker support on an existing ASP.NET Core web app project by right-clicking\nthe project in **Solution Explorer** and selecting **Add** - **Docker Support…**, as shown in Figure 5-4.', '_Figure 5-4. Enabling Docker support in an existing Visual Studio 2022 project_', '77 CHAPTER 4 | Development process for Docker-based applications', 'This action adds a _Dockerfile_ to the project with the required configuration, and is only available on\nASP.NET Core projects.', 'In a similar fashion, Visual Studio can also add a docker-compose.yml file for the whole solution with\nthe option **Add > Container Orchestrator Support…** . In step 4, we’ll explore this option in greater\ndetail.', '**Using an existing official .NET Docker image**', 'You usually build a custom image for your container on top of a base image you get from an official\n[repository like the Docker Hub registry. That is precisely what happens under the covers when you](https://hub.docker.com/)\nenable Docker support in Visual Studio. Your Dockerfile will use an existing dotnet/core/aspnet image.', 'Earlier we explained which Docker images and repos you can use, depending on the framework and\nOS you have chosen. For instance, if you want to use ASP.NET Core (Linux or Windows), the image to\nuse is mcr.microsoft.com/dotnet/aspnet:7.0. Therefore, you just need to specify what base Docker\nimage you will use for your container. You do that by adding FROM\nmcr.microsoft.com/dotnet/aspnet:7.0 to your Dockerfile. This will be automatically performed by\nVisual Studio, but if you were to update the version, you update this value.', 'Using an official .NET image repository from Docker Hub with a version number ensures that the same\nlanguage features are available on all machines (including development, testing, and production).', 'The following example shows a sample Dockerfile for an ASP.NET Core container.', 'In this case, the image is based on version 7.0 of the official ASP.NET Core Docker image (multi-arch\nfor Linux and Windows). This is the setting FROM mcr.microsoft.com/dotnet/aspnet:7.0. (For more\n[information about this base image, see the ASP.NET Core Docker Image](https://hub.docker.com/_/microsoft-dotnet-aspnet/) page.) In the Dockerfile, you\nalso need to instruct Docker to listen on the TCP port you will use at runtime (in this case, port 80, as\nconfigured with the EXPOSE setting).', 'You can specify additional configuration settings in the Dockerfile, depending on the language and\nframework you’re using. For instance, the ENTRYPOINT line with ["dotnet",\n"MySingleContainerWebApp.dll"] tells Docker to run a .NET application. If you’re using the SDK and\nthe .NET CLI (dotnet CLI) to build and run the .NET application, this setting would be different. The\nbottom line is that the ENTRYPOINT line and other settings will be different depending on the\nlanguage and platform you choose for your application.', '**Additional resources**', 'Then it would be just the same for every service, it would copy the whole solution and would create a\nlarger layer but:', 'It would restore the packages for the whole solution, but then again, it would do it just once, instead\nof the 15 times with the current strategy.', 'However, dotnet restore only runs if there’s a single project or solution file in the folder, so achieving\nthis is a bit more complicated and the way to solve it, without getting into too many details, is this:', 'For each service in your application, you need to create a related image. If your application is made up\nof a single service or web application, you just need a single image.', 'Note that the Docker images are built automatically for you in Visual Studio. The following steps are\nonly needed for the editor/CLI workflow and explained for clarity about what happens underneath.', 'You, as a developer, need to develop and test locally until you push a completed feature or change to\nyour source control system (for example, to GitHub). This means that you need to create the Docker\nimages and deploy containers to a local Docker host (Windows or Linux VM) and run, test, and debug\nagainst those local containers.', 'To create a custom image in your local environment by using Docker CLI and your Dockerfile, you can\nuse the docker build command, as in Figure 5-5.', '_Figure 5-5. Creating a custom Docker image_', 'Optionally, instead of directly running docker build from the project folder, you can first generate a\ndeployable folder with the required .NET libraries and binaries by running dotnet publish, and then\nuse the docker build command.', 'This will create a Docker image with the name cesardl/netcore-webapi-microservice-docker:first. In\nthis case, :first is a tag that represents a specific version. You can repeat this step for each custom\nimage you need to create for your composed Docker application.', 'When an application is made of multiple containers (that is, it is a multi-container application), you\ncan also use the docker-compose up --build command to build all the related images with a single\ncommand by using the metadata exposed in the related docker-compose.yml files.', 'You can find the existing images in your local repository by using the docker images command, as\nshown in Figure 5-6.', '83 CHAPTER 4 | Development process for Docker-based applications', '_Figure 5-6. Viewing existing images using the docker images command_', '**Creating Docker images with Visual Studio**', 'When you use Visual Studio to create a project with Docker support, you don’t explicitly create an\nimage. Instead, the image is created for you when you press F5 (or Ctrl+F5) to run the dockerized\napplication or service. This step is automatic in Visual Studio and you won’t see it happen, but it’s\nimportant that you know what’s going on underneath.', '[The docker-compose.yml file lets you define a set of related services to be deployed as a composed](https://docs.docker.com/compose/compose-file/)\napplication with deployment commands. It also configures its dependency relations and runtime\nconfiguration.', 'To use a docker-compose.yml file, you need to create the file in your main or root solution folder, with\ncontent similar to that in the following example:', '84 CHAPTER 4 | Development process for Docker-based applications', 'This docker-compose.yml file is a simplified and merged version. It contains static configuration data\nfor each container (like the name of the custom image), which is always required, and configuration\ninformation that might depend on the deployment environment, like the connection string. In later\nsections, you will learn how to split the docker-compose.yml configuration into multiple dockercompose files and override values depending on the environment and execution type (debug or\nrelease).', 'The docker-compose.yml file example defines four services: the webmvc service (a web application),\ntwo microservices (ordering-api and basket-api), and one data source container, sqldata, based on\nSQL Server for Linux running as a container. Each service will be deployed as a container, so a Docker\nimage is required for each.', 'The docker-compose.yml file specifies not only what containers are being used, but how they are\nindividually configured. For instance, the webmvc container definition in the .yml file:', 'If your application only has a single container, you can run it by deploying it to your Docker host (VM\nor physical server). However, if your application contains multiple services, you can deploy it as a\ncomposed application, either using a single CLI command (docker-compose up), or with Visual Studio,\nwhich will use that command under the covers. Let’s look at the different options.', '**Option A: Running a single-container application**', '**Using Docker CLI**', 'You can run a Docker container using the docker run command, as shown in Figure 5-9:', 'The above command will create a new container instance from the specified image, every time it’s run.\nYou can use the --name parameter to give a name to the container and then use docker start {name}\n(or use the container ID or automatic name) to run an existing container instance.', '_Figure 5-9. Running a Docker container using the docker run command_', 'In this case, the command binds the internal port 5000 of the container to port 80 of the host\nmachine. This means that the host is listening on port 80 and forwarding to port 5000 on the\ncontainer.', 'The hash shown is the container ID and it’s also assigned a random readable name if the --name\noption is not used.', '**Using Visual Studio**', 'If you haven’t added container orchestrator support, you can also run a single container app in Visual\nStudio by pressing Ctrl+F5 and you can also use F5 to debug the application within the container. The\ncontainer runs locally using docker run.', '**Option B: Running a multi-container application**', 'In most enterprise scenarios, a Docker application will be composed of multiple services, which means\nyou need to run a multi-container application, as shown in Figure 5-10.', '87 CHAPTER 4 | Development process for Docker-based applications', '_Figure 5-10. VM with Docker containers deployed_', '**Using Docker CLI**', 'To run a multi-container application with the Docker CLI, you use the docker-compose up command.\nThis command uses the **docker-compose.yml** file that you have at the solution level to deploy a\nmulti-container application. Figure 5-11 shows the results when running the command from your\nmain solution directory, which contains the docker-compose.yml file.', '_Figure 5-11. Example results when running the docker-compose up command_', 'After the docker-compose up command runs, the application and its related containers are deployed\ninto your Docker host, as depicted in Figure 5-10.', '**Using Visual Studio**', 'Running a multi-container application using Visual Studio 2019 can’t get any simpler. You just press\nCtrl+F5 to run or F5 to debug, as usual, setting up the **docker-compose** project as the startup project.\nVisual Studio handles all needed setup, so you can create breakpoints as usual and debug what finally\nbecome independent processes running in “remote servers”, with the debugger already attached, just\nlike that.', 'As mentioned before, each time you add Docker solution support to a project within a solution, that\nproject is configured in the global (solution-level) docker-compose.yml file, which lets you run or\ndebug the whole solution at once. Visual Studio will start one container for each project that has\nDocker solution support enabled, and perform all the internal steps for you (dotnet publish, docker\nbuild, etc.).', 'If you want to take a peek at all the drudgery, take a look at the file:', '{root solution folder}\\obj\\Docker\\docker-compose.vs.debug.g.yml', '88 CHAPTER 4 | Development process for Docker-based applications', 'The important point here is that, as shown in Figure 5-12, in Visual Studio 2019 there is an additional\n**Docker** command for the F5 key action. This option lets you run or debug a multi-container\napplication by running all the containers that are defined in the docker-compose.yml files at the\nsolution level. The ability to debug multiple-container solutions means that you can set several\nbreakpoints, each breakpoint in a different project (container), and while debugging from Visual\nStudio you will stop at breakpoints defined in different projects and running on different containers.', '_Figure 5-12. Running multi-container apps in Visual Studio 2022_', '**Additional resources**', 'This step will vary depending on what your application is doing. In a simple .NET Web application that\nis deployed as a single container or service, you can access the service by opening a browser on the\nDocker host and navigating to that site, as shown in Figure 5-13. (If the configuration in the Dockerfile\nmaps the container to a port on the host that is anything other than 80, include the host port in the\nURL.)', '_Figure 5-13. Example of testing your Docker application locally using localhost_', '89 CHAPTER 4 | Development process for Docker-based applications', 'If localhost is not pointing to the Docker host IP (by default, when using Docker CE, it should), to\nnavigate to your service, use the IP address of your machine’s network card.', 'This URL in the browser uses port 80 for the particular container example being discussed. However,\ninternally the requests are being redirected to port 5000, because that was how it was deployed with\nthe docker run command, as explained in a previous step.', 'You can also test the application using curl from the terminal, as shown in Figure 5-14. In a Docker\ninstallation on Windows, the default Docker Host IP is always 10.0.75.1 in addition to your machine’s\nactual IP address.', '_Figure 5-14. Example of testing your Docker application locally using curl_', '**Testing and debugging containers with Visual Studio 2022**', 'When running and debugging the containers with Visual Studio 2022, you can debug the .NET\napplication in much the same way as you would when running without containers.', '**Testing and debugging without Visual Studio**', 'If you’re developing using the editor/CLI approach, debugging containers is more difficult and you’ll\nprobably want to debug by generating traces.', '**Additional resources**', 'Effectively, the workflow when using Visual Studio is a lot simpler than if you use the editor/CLI\napproach. Most of the steps required by Docker related to the Dockerfile and docker-compose.yml\nfiles are hidden or simplified by Visual Studio, as shown in Figure 5-15.', '90 CHAPTER 4 | Development process for Docker-based applications', '_Figure 5-15. Simplified workflow when developing with Visual Studio_', 'In addition, you need to perform step 2 (adding Docker support to your projects) just once. Therefore,\nthe workflow is similar to your usual development tasks when using .NET for any other development.\nYou need to know what is going on under the covers (the image build process, what base images\nyou’re using, deployment of containers, etc.) and sometimes you will also need to edit the Dockerfile\nor docker-compose.yml file to customize behaviors. But most of the work is greatly simplified by using\nVisual Studio, making you a lot more productive.', '[Windows Containers](https://docs.microsoft.com/virtualization/windowscontainers/about/index) allow you to convert your existing Windows applications into Docker images and\ndeploy them with the same tools as the rest of the Docker ecosystem. To use Windows Containers,\nyou run PowerShell commands in the Dockerfile, as shown in the following example:', 'In this case, we are using a Windows Server Core base image (the FROM setting) and installing IIS with\na PowerShell command (the RUN setting). In a similar way, you could also use PowerShell commands\nto set up additional components like ASP.NET 4.x, .NET Framework 4.6, or any other Windows\nsoftware. For example, the following command in a Dockerfile sets up ASP.NET 4.5:', '**Additional resources**', '_Developing containerized microservice applications means you are building multi-container_\n_applications. However, a multi-container application could also be simpler—for example, a three-tier_\n_application—and might not be built using a microservice architecture._', 'Earlier we raised the question “Is Docker necessary when building a microservice architecture?” The\nanswer is a clear no. Docker is an enabler and can provide significant benefits, but containers and\nDocker are not a hard requirement for microservices. As an example, you could create a\nmicroservices-based application with or without Docker when using Azure Service Fabric, which\nsupports microservices running as simple processes or as Docker containers.', 'However, if you know how to design and develop a microservices-based application that is also based\non Docker containers, you will be able to design and develop any other, simpler application model.\nFor example, you might design a three-tier application that also requires a multi-container approach.\nBecause of that, and because microservice architectures are an important trend within the container\nworld, this section focuses on a microservice architecture implementation using Docker containers.', 'This section focuses on developing a hypothetical server-side enterprise application.', 'The hypothetical application handles requests by executing business logic, accessing databases, and\nthen returning HTML, JSON, or XML responses. We will say that the application must support various\nclients, including desktop browsers running Single Page Applications (SPAs), traditional web apps,\nmobile web apps, and native mobile apps. The application might also expose an API for third parties', '93 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'to consume. It should also be able to integrate its microservices or external applications\nasynchronously, so that approach will help resiliency of the microservices in the case of partial failures.', 'The application will consist of these types of components:', 'We also assume the following about the development process for the application:', 'What should the application deployment architecture be? The specifications for the application, along\nwith the development context, strongly suggest that you should architect the application by\ndecomposing it into autonomous subsystems in the form of collaborating microservices and\ncontainers, where a microservice is a container.', '94 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'In this approach, each service (container) implements a set of cohesive and narrowly related functions.\nFor example, an application might consist of services such as the catalog service, ordering service,\nbasket service, user profile service, etc.', 'Microservices communicate using protocols such as HTTP (REST), but also asynchronously (for\nexample, using AMQP) whenever possible, especially when propagating updates with integration\nevents.', 'Microservices are developed and deployed as containers independently of one another. This approach\nmeans that a development team can be developing and deploying a certain microservice without\nimpacting other subsystems.', 'Each microservice has its own database, allowing it to be fully decoupled from other microservices.\nWhen necessary, consistency between databases from different microservices is achieved using\napplication-level integration events (through a logical event bus), as handled in Command and Query\nResponsibility Segregation (CQRS). Because of that, the business constraints must embrace eventual\nconsistency between the multiple microservices and related databases.', '**eShopOnContainers: A reference application for .NET and microservices deployed**\n**using containers**', 'So that you can focus on the architecture and technologies instead of thinking about a hypothetical\nbusiness domain that you might not know, we have selected a well-known business domain—namely,\na simplified e-commerce (e-shop) application that presents a catalog of products, takes orders from\ncustomers, verifies inventory, and performs other business functions. This container-based application\n[source code is available in the eShopOnContainers](https://aka.ms/MicroservicesArchitecture) GitHub repo.', 'The application consists of multiple subsystems, including several store UI front ends (a Web\napplication and a native mobile app), along with the back-end microservices and containers for all the\nrequired server-side operations with several API Gateways as consolidated entry points to the internal\nmicroservices. Figure 6-1 shows the architecture of the reference application.', '95 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-1. The eShopOnContainers reference application architecture for development environment_', 'The above diagram shows that Mobile and SPA clients communicate to single API gateway endpoints,\nthat then communicate to microservices. Traditional web clients communicate to MVC microservice,\nthat communicates to microservices through the API gateway.', '**Hosting environment** . In Figure 6-1, you see several containers deployed within a single Docker host.\nThat would be the case when deploying to a single Docker host with the docker-compose up\ncommand. However, if you are using an orchestrator or container cluster, each container could be\nrunning in a different host (node), and any node could be running any number of containers, as we\nexplained earlier in the architecture section.', '**Communication architecture** . The eShopOnContainers application uses two communication types,\ndepending on the kind of the functional action (queries versus updates and transactions):', 'A microservice-based solution like this has many benefits:', '**Each microservice is relatively small—easy to manage and evolve** . Specifically:', 'A microservice-based solution like this also has some drawbacks:', '**Distributed application** . Distributing the application adds complexity for developers when they are\ndesigning and building the services. For example, developers must implement inter-service\ncommunication using protocols like HTTP or AMQP, which adds complexity for testing and exception\nhandling. It also adds latency to the system.', '**Deployment complexity** . An application that has dozens of microservices types and needs high\nscalability (it needs to be able to create many instances per service and balance those services across\nmany hosts) means a high degree of deployment complexity for IT operations and management. If\nyou are not using a microservice-oriented infrastructure (like an orchestrator and scheduler), that\nadditional complexity can require far more development efforts than the business application itself.', '**Atomic transactions** . Atomic transactions between multiple microservices usually are not possible.\nThe business requirements have to embrace eventual consistency between multiple microservices.', '**Increased global resource needs** (total memory, drives, and network resources for all the servers or\nhosts). In many cases, when you replace a monolithic application with a microservices approach, the\namount of initial global resources needed by the new microservice-based application will be larger\nthan the infrastructure needs of the original monolithic application. This approach is because the\nhigher degree of granularity and distributed services requires more global resources. However, given\nthe low cost of resources in general and the benefit of being able to scale out certain areas of the\napplication compared to long-term costs when evolving monolithic applications, the increased use of\nresources is usually a good tradeoff for large, long-term applications.', '**Issues with direct client-to-microservice communication** . When the application is large, with\ndozens of microservices, there are challenges and limitations if the application requires direct clientto-microservice communications. One problem is a potential mismatch between the needs of the\nclient and the APIs exposed by each of the microservices. In certain cases, the client application might\nneed to make many separate requests to compose the UI, which can be inefficient over the Internet\nand would be impractical over a mobile network. Therefore, requests from the client application to the\nback-end system should be minimized.', 'Another problem with direct client-to-microservice communications is that some microservices might\nbe using protocols that are not Web-friendly. One service might use a binary protocol, while another\nservice might use AMQP messaging. Those protocols are not firewall-friendly and are best used\ninternally. Usually, an application should use protocols such as HTTP and WebSockets for\ncommunication outside of the firewall.', '98 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Yet another drawback with this direct client-to-service approach is that it makes it difficult to refactor\nthe contracts for those microservices. Over time developers might want to change how the system is\npartitioned into services. For example, they might merge two services or split a service into two or\nmore services. However, if clients communicate directly with the services, performing this kind of\nrefactoring can break compatibility with client apps.', 'As mentioned in the architecture section, when designing and building a complex application based\non microservices, you might consider the use of multiple fine-grained API Gateways instead of the\nsimpler direct client-to-microservice communication approach.', '**Partitioning the microservices** . Finally, no matter, which approach you take for your microservice\narchitecture, another challenge is deciding how to partition an end-to-end application into multiple\nmicroservices. As noted in the architecture section of the guide, there are several techniques and\napproaches you can take. Basically, you need to identify areas of the application that are decoupled\nfrom the other areas and that have a low number of hard dependencies. In many cases, this approach\nis aligned to partitioning services by use case. For example, in our e-shop application, we have an\nordering service that is responsible for all the business logic related to the order process. We also\nhave the catalog service and the basket service that implement other capabilities. Ideally, each service\nshould have only a small set of responsibilities. This approach is similar to the single responsibility\nprinciple (SRP) applied to classes, which states that a class should only have one reason to change. But\nin this case, it is about microservices, so the scope will be larger than a single class. Most of all, a\nmicroservice has to be autonomous, end to end, including responsibility for its own data sources.', 'The external architecture is the microservice architecture composed by multiple services, following the\nprinciples described in the architecture section of this guide. However, depending on the nature of\neach microservice, and independently of high-level microservice architecture you choose, it is\ncommon and sometimes advisable to have different internal architectures, each based on different\npatterns, for different microservices. The microservices can even use different technologies and\nprogramming languages. Figure 6-2 illustrates this diversity.', '99 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-2. External versus internal architecture and design_', 'For instance, in our _eShopOnContainers_ sample, the catalog, basket, and user profile microservices are\nsimple (basically, CRUD subsystems). Therefore, their internal architecture and design is\nstraightforward. However, you might have other microservices, such as the ordering microservice,\nwhich is more complex and represents ever-changing business rules with a high degree of domain\ncomplexity. In cases like these, you might want to implement more advanced patterns within a\nparticular microservice, like the ones defined with domain-driven design (DDD) approaches, as we are\ndoing in the _eShopOnContainers_ ordering microservice. (We will review these DDD patterns in the\nsection later that explains the implementation of the _eShopOnContainers_ ordering microservice.)', 'Another reason for a different technology per microservice might be the nature of each microservice.\nFor example, it might be better to use a functional programming language like F#, or even a language\nlike R if you are targeting AI and machine learning domains, instead of a more object-oriented\nprogramming language like C#.', 'The bottom line is that each microservice can have a different internal architecture based on different\ndesign patterns. Not all microservices should be implemented using advanced DDD patterns, because\nthat would be over-engineering them. Similarly, complex microservices with ever-changing business\nlogic should not be implemented as CRUD components, or you can end up with low-quality code.', 'There are many architectural patterns used by software architects and developers. The following are a\nfew (mixing architecture styles and architecture patterns):', 'This section outlines how to create a simple microservice that performs create, read, update, and\ndelete (CRUD) operations on a data source.', 'From a design point of view, this type of containerized microservice is very simple. Perhaps the\nproblem to solve is simple, or perhaps the implementation is only a proof of concept.', '_Figure 6-4. Internal design for simple CRUD microservices_', 'An example of this kind of simple data-drive service is the catalog microservice from the\neShopOnContainers sample application. This type of service implements all its functionality in a single\nASP.NET Core Web API project that includes classes for its data model, its business logic, and its data\naccess code. It also stores its related data in a database running in SQL Server (as another container\nfor dev/test purposes), but could also be any regular SQL Server host, as shown in Figure 6-5.', '102 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-5. Simple data-driven/CRUD microservice design_', 'The previous diagram shows the logical Catalog microservice, that includes its Catalog database,\nwhich can be or not in the same Docker host. Having the database in the same Docker host might be\ngood for development, but not for production. When you are developing this kind of service, you only\n[need ASP.NET Core](https://docs.microsoft.com/aspnet/core/) [and a data-access API or ORM like Entity Framework Core. You could also](https://docs.microsoft.com/ef/core/index)\n[generate Swagger](https://swagger.io/) [metadata automatically through Swashbuckle](https://github.com/domaindrivendev/Swashbuckle.AspNetCore) to provide a description of what your\nservice offers, as explained in the next section.', 'Note that running a database server like SQL Server within a Docker container is great for\ndevelopment environments, because you can have all your dependencies up and running without\nneeding to provision a database in the cloud or on-premises. This approach is convenient when\nrunning integration tests. However, for production environments, running a database server in a\ncontainer is not recommended, because you usually do not get high availability with that approach.\nFor a production environment in Azure, it is recommended that you use Azure SQL DB or any other\ndatabase technology that can provide high availability and high scalability. For example, for a NoSQL\napproach, you might choose CosmosDB.', 'Finally, by editing the Dockerfile and docker-compose.yml metadata files, you can configure how the\nimage of this container will be created—what base image it will use, plus design settings such as\ninternal and external names and TCP ports.', 'To implement a simple CRUD microservice using .NET and Visual Studio, you start by creating a\nsimple ASP.NET Core Web API project (running on .NET so it can run on a Linux Docker host), as\nshown in Figure 6-6.', '103 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-6. Creating an ASP.NET Core Web API project in Visual Studio 2019_', 'To create an ASP.NET Core Web API Project, first select an ASP.NET Core Web Application and then\nselect the API type. After creating the project, you can implement your MVC controllers as you would\nin any other Web API project, using the Entity Framework API or other API. In a new Web API project,\nyou can see that the only dependency you have in that microservice is on ASP.NET Core itself.\nInternally, within the _Microsoft.AspNetCore.All_ dependency, it is referencing Entity Framework and\nmany other .NET NuGet packages, as shown in Figure 6-7.', '104 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-7. Dependencies in a simple CRUD Web API microservice_', 'The API project includes references to Microsoft.AspNetCore.App NuGet package, that includes\nreferences to all essential packages. It could include some other packages as well.', '**Implementing CRUD Web API services with Entity Framework Core**', 'Entity Framework (EF) Core is a lightweight, extensible, and cross-platform version of the popular\nEntity Framework data access technology. EF Core is an object-relational mapper (ORM) that enables\n.NET developers to work with a database using .NET objects.', 'The catalog microservice uses EF and the SQL Server provider because its database is running in a\ncontainer with the SQL Server for Linux Docker image. However, the database could be deployed into', '105 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'any SQL Server, such as Windows on-premises or Azure SQL DB. The only thing you would need to\nchange is the connection string in the ASP.NET Web API microservice.', '**The data model**', 'With EF Core, data access is performed by using a model. A model is made up of (domain model)\nentity classes and a derived context (DbContext) that represents a session with the database, allowing\nyou to query and save data. You can generate a model from an existing database, manually code a\nmodel to match your database, or use EF migrations technique to create a database from your model,\nusing the code-first approach (that makes it easy to evolve the database as your model changes over\ntime). For the catalog microservice, the last approach has been used. You can see an example of the\nCatalogItem entity class in the following code example, which is a simple Plain Old Class Object\n[(POCO) entity class.](https://docs.microsoft.com/dotnet/standard/glossary#poco)', 'You also need a DbContext that represents a session with the database. For the catalog microservice,\nthe CatalogContext class derives from the DbContext base class, as shown in the following example:', 'You can have additional DbContext implementations. For example, in the sample Catalog.API\nmicroservice, there’s a second DbContext named CatalogContextSeed where it automatically\npopulates the sample data the first time it tries to access the database. This method is useful for demo\ndata and for automated testing scenarios, as well.', '106 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Within the DbContext, you use the OnModelCreating method to customize object/database entity\n[mappings and other EF extensibility points.](https://devblogs.microsoft.com/dotnet/implementing-seeding-custom-conventions-and-interceptors-in-ef-core-1-0/)', 'Querying data from Web API controllers', 'Instances of your entity classes are typically retrieved from the database using Language-Integrated\nQuery (LINQ), as shown in the following example:', '107 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Saving data', 'Data is created, deleted, and modified in the database using instances of your entity classes. You\ncould add code like the following hard-coded example (mock data, in this case) to your Web API\ncontrollers.', 'Dependency Injection in ASP.NET Core and Web API controllers', 'In ASP.NET Core, you can use Dependency Injection (DI) out of the box. You do not need to set up a\nthird-party Inversion of Control (IoC) container, although you can plug your preferred IoC container\ninto the ASP.NET Core infrastructure if you want. In this case, it means that you can directly inject the\nrequired EF DBContext or additional repositories through the controller constructor.', 'In the CatalogController class mentioned earlier, CatalogContext (which inherits from DbContext) type\nis injected along with the other required objects in the CatalogController() constructor.', 'An important configuration to set up in the Web API project is the DbContext class registration into\nthe service’s IoC container. You typically do so in the _Program.cs_ file by calling the\nbuilder.Services.AddDbContext<CatalogContext>() method, as shown in the following **simplified**\nexample:', '108 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Additional resources**', 'You can use the ASP.NET Core settings and add a ConnectionString property to your settings.json file\nas shown in the following example:', 'The settings.json file can have default values for the ConnectionString property or for any other\nproperty. However, those properties will be overridden by the values of environment variables that\nyou specify in the docker-compose.override.yml file, when using Docker.', 'From your docker-compose.yml or docker-compose.override.yml files, you can initialize those\nenvironment variables so that Docker will set them up as OS environment variables for you, as shown\nin the following docker-compose.override.yml file (the connection string and other lines wrap in this\nexample, but it would not wrap in your own file).', '109 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'The docker-compose.yml files at the solution level are not only more flexible than configuration files\nat the project or microservice level, but also more secure if you override the environment variables\ndeclared at the docker-compose files with values set from your deployment tools, like from Azure\nDevOps Services Docker deployment tasks.', 'Finally, you can get that value from your code by using builder.Configuration\\["ConnectionString"\\], as\nshown in an earlier code example.', 'However, for production environments, you might want to explore additional ways on how to store\n[secrets like the connection strings. An excellent way to manage application secrets is using Azure Key](https://azure.microsoft.com/services/key-vault/)\n[Vault.](https://azure.microsoft.com/services/key-vault/)', 'Azure Key Vault helps to store and safeguard cryptographic keys and secrets used by your cloud\napplications and services. A secret is anything you want to keep strict control of, like API keys,\nconnection strings, passwords, etc. and strict control includes usage logging, setting expiration,\nmanaging access, _among others_ .', 'Azure Key Vault allows a detailed control level of the application secrets usage without the need to let\nanyone know them. The secrets can even be rotated for enhanced security without disrupting\ndevelopment or operations.', 'Applications have to be registered in the organization’s Active Directory, so they can use the Key\nVault.', 'You can check the _Key Vault Concepts documentation_ for more details.', '**Implementing versioning in ASP.NET Web APIs**', 'As business requirements change, new collections of resources may be added, the relationships\nbetween resources might change, and the structure of the data in resources might be amended.\nUpdating a Web API to handle new requirements is a relatively straightforward process, but you must\nconsider the effects that such changes will have on client applications consuming the Web API.\nAlthough the developer designing and implementing a Web API has full control over that API, the\ndeveloper does not have the same degree of control over client applications that might be built by\nthird-party organizations operating remotely.', 'Versioning enables a Web API to indicate the features and resources that it exposes. A client\napplication can then submit requests to a specific version of a feature or resource. There are several\napproaches to implement versioning:', '[Swagger](https://swagger.io/) is a commonly used open source framework backed by a large ecosystem of tools that helps\nyou design, build, document, and consume your RESTful APIs. It is becoming the standard for the APIs\ndescription metadata domain. You should include Swagger description metadata with any kind of\nmicroservice, either data-driven microservices or more advanced domain-driven microservices (as\nexplained in the following section).', 'The heart of Swagger is the Swagger specification, which is API description metadata in a JSON or\nYAML file. The specification creates the RESTful contract for your API, detailing all its resources and\noperations in both a human- and machine-readable format for easy development, discovery, and\nintegration.', 'The specification is the basis of the OpenAPI Specification (OAS) and is developed in an open,\ntransparent, and collaborative community to standardize the way RESTful interfaces are defined.', '111 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'The specification defines the structure for how a service can be discovered and how its capabilities\nunderstood. For more information, including a web editor and examples of Swagger specifications\n[from companies like Spotify, Uber, Slack, and Microsoft, see the Swagger site (https://swagger.io).](https://swagger.io/)', '**Why use Swagger?**', 'The main reasons to generate Swagger metadata for your APIs are the following.', '**Ability for other products to automatically consume and integrate your APIs** . Dozens of products\n[and commercial tools](https://swagger.io/commercial-tools/) [and many libraries and frameworks](https://swagger.io/open-source-integrations/) support Swagger. Microsoft has high-level\nproducts and tools that can automatically consume Swagger-based APIs, such as the following:', '[In this guide, the docker-compose.yml file was introduced in the section Step 4. Define your services](https://docs.docker.com/compose/compose-file/)\nin docker-compose.yml when building a multi-container Docker application. However, there are\nadditional ways to use the docker-compose files that are worth exploring in further detail.', 'For example, you can explicitly describe how you want to deploy your multi-container application in\nthe docker-compose.yml file. Optionally, you can also describe how you are going to build your\ncustom Docker images. (Custom Docker images can also be built with the Docker CLI.)', '116 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Basically, you define each of the containers you want to deploy plus certain characteristics for each\ncontainer deployment. Once you have a multi-container deployment description file, you can deploy\n[the whole solution in a single action orchestrated by the docker-compose up](https://docs.docker.com/compose/overview/) CLI command, or you\ncan deploy it transparently from Visual Studio. Otherwise, you would need to use the Docker CLI to\ndeploy container-by-container in multiple steps by using the docker run command from the\ncommand line. Therefore, each service defined in docker-compose.yml must specify exactly one\nimage or build. Other keys are optional, and are analogous to their docker run command-line\ncounterparts.', 'The following YAML code is the definition of a possible global but single docker-compose.yml file for\nthe eShopOnContainers sample. This code is not the actual docker-compose file from\neShopOnContainers. Instead, it is a simplified and consolidated version in a single file, which is not the\nbest way to work with docker-compose files, as will be explained later.', '117 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'The root key in this file is services. Under that key, you define the services you want to deploy and run\nwhen you execute the docker-compose up command or when you deploy from Visual Studio by using\nthis docker-compose.yml file. In this case, the docker-compose.yml file has multiple services defined,', '**A simple Web Service API container**', 'Focusing on a single container, the catalog-api container-microservice has a straightforward\ndefinition:', '118 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'This containerized service has the following basic configuration:', '**Production deployments**', 'You can also use Compose to deploy to a remote Docker Engine. A typical case is to deploy to a\n[single Docker host instance (like a production VM or server provisioned with Docker Machine).](https://docs.docker.com/machine/overview/)', 'If you are using any other orchestrator (Azure Service Fabric, Kubernetes, etc.), you might need to add\nsetup and metadata configuration settings like those in docker-compose.yml, but in the format\nrequired by the other orchestrator.', 'In any case, docker-compose is a convenient tool and metadata format for development, testing and\nproduction workflows, although the production workflow might vary on the orchestrator you are\nusing.', '120 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Using multiple docker-compose files to handle several environments**', 'When targeting different environments, you should use multiple compose files. This approach lets you\ncreate multiple configuration variants depending on the environment.', '**Overriding the base docker-compose file**', 'You could use a single docker-compose.yml file as in the simplified examples shown in previous\nsections. However, that is not recommended for most applications.', 'By default, Compose reads two files, a docker-compose.yml and an optional dockercompose.override.yml file. As shown in Figure 6-11, when you are using Visual Studio and enabling\nDocker support, Visual Studio also creates an additional docker-compose.vs.debug.g.yml file for\ndebugging the application, you can take a look at this file in folder obj\\Docker\\ in the main solution\nfolder.', '_Figure 6-11. docker-compose files in Visual Studio 2019_', '**docker-compose** project file structure:', '125 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Using environment variables in docker-compose files**', 'It is convenient, especially in production environments, to be able to get configuration information\nfrom environment variables, as we have shown in previous examples. You can reference an\nenvironment variable in your docker-compose files using the syntax ${MY_VAR}. The following line\nfrom a docker-compose.prod.yml file shows how to reference the value of an environment variable.', 'Environment variables are created and initialized in different ways, depending on your host\nenvironment (Linux, Windows, Cloud cluster, etc.). However, a convenient approach is to use an .env\nfile. The docker-compose files support declaring default environment variables in the .env file. These\nvalues for the environment variables are the default values. But they can be overridden by the values\nyou might have defined in each of your environments (host OS or environment variables from your\ncluster). You place this .env file in the folder where the docker-compose command is executed from.', '[The following example shows an .env file like the .env](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/.env) file for the eShopOnContainers application.', 'Docker-compose expects each line in an .env file to be in the format <variable>=<value>.', 'The values set in the run-time environment always override the values defined inside the .env file. In a\nsimilar way, values passed via command-line arguments also override the default values set in the .env\nfile.', '**Additional resources**', 'You can have your databases (SQL Server, PostgreSQL, MySQL, etc.) on regular standalone servers, in\non-premises clusters, or in PaaS services in the cloud like Azure SQL DB. However, for development\nand test environments, having your databases running as containers is convenient, because you don’t', '127 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'have any external dependency and simply running the docker-compose up command starts the whole\napplication. Having those databases as containers is also great for integration tests, because the\ndatabase is started in the container and is always populated with the same sample data, so tests can\nbe more predictable.', '[In eShopOnContainers, there’s a container named sqldata, as defined in the docker-compose.yml file,](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/docker-compose.yml)\nthat runs a SQL Server for Linux instance with the SQL databases for all microservices that need one.', 'A key point in microservices is that each microservice owns its related data, so it should have its own\ndatabase. However, the databases can be anywhere. In this case, they are all in the same container to\nkeep Docker memory requirements as low as possible. Keep in mind that this is a good-enough\nsolution for development and, perhaps, testing but not for production.', 'The SQL Server container in the sample application is configured with the following YAML code in the\ndocker-compose.yml file, which is executed when you run docker-compose up. Note that the YAML\ncode has consolidated configuration information from the generic docker-compose.yml file and the\ndocker-compose.override.yml file. (Usually you would separate the environment settings from the\nbase or static information related to the SQL Server image.)', 'In a similar way, instead of using docker-compose, the following docker run command can run that\ncontainer:', 'However, if you are deploying a multi-container application like eShopOnContainers, it is more\nconvenient to use the docker-compose up command so that it deploys all the required containers for\nthe application.', 'When you start this SQL Server container for the first time, the container initializes SQL Server with the\npassword that you provide. Once SQL Server is running as a container, you can update the database\nby connecting through any regular SQL connection, such as from SQL Server Management Studio,\nVisual Studio, or C# code.', 'The eShopOnContainers application initializes each microservice database with sample data by\nseeding it with data on startup, as explained in the following section.', 'Having SQL Server running as a container is not just useful for a demo where you might not have\naccess to an instance of SQL Server. As noted, it is also great for development and testing', '128 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'environments so that you can easily run integration tests starting from a clean SQL Server image and\nknown data by seeding new sample data.', '**Additional resources**', 'To add data to the database when the application starts up, you can add code like the following to\nthe Main method in the Program class of the Web API project:', '129 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'There’s an important caveat when applying migrations and seeding a database during container\nstartup. Since the database server might not be available for whatever reason, you must handle retries\nwhile waiting for the server to be available. This retry logic is handled by the MigrateDbContext()\nextension method, as shown in the following code:', '130 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'The following code in the custom CatalogContextSeed class populates the data.', 'When you run integration tests, having a way to generate data consistent with your integration tests is\nuseful. Being able to create everything from scratch, including an instance of SQL Server running on a\ncontainer, is great for test environments.', '131 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Another good choice when running tests is to use the Entity Framework InMemory database provider.\nYou can specify that configuration in the ConfigureServices method of the Startup class in your Web\nAPI project:', 'There is an important catch, though. The in-memory database does not support many constraints that\nare specific to a particular database. For instance, you might add a unique index on a column in your\nEF Core model and write a test against your in-memory database to check that it does not let you add\na duplicate value. But when you are using the in-memory database, you cannot handle unique indexes\non a column. Therefore, the in-memory database does not behave exactly the same as a real SQL\nServer database—it does not emulate database-specific constraints.', 'Even so, an in-memory database is still useful for testing and prototyping. But if you want to create\naccurate integration tests that take into account the behavior of a specific database implementation,\nyou need to use a real database like SQL Server. For that purpose, running SQL Server in a container is\na great choice and more accurate than the EF Core InMemory database provider.', 'You can run Redis on a container, especially for development and testing and for proof-of-concept\nscenarios. This scenario is convenient, because you can have all your dependencies running on\ncontainers—not just for your local development machines, but for your testing environments in your\nCI/CD pipelines.', 'However, when you run Redis in production, it is better to look for a high-availability solution like\nRedis Microsoft Azure, which runs as a PaaS (Platform as a Service). In your code, you just need to\nchange your connection strings.', 'Redis provides a Docker image with Redis. That image is available from Docker Hub at this URL:', '[https://hub.docker.com/_/redis/](https://hub.docker.com/_/redis/)', '132 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'You can directly run a Docker Redis container by executing the following Docker CLI command in your\ncommand prompt:', 'The Redis image includes expose:6379 (the port used by Redis), so standard container linking will\nmake it automatically available to the linked containers.', 'In eShopOnContainers, the basket-api microservice uses a Redis cache running as a container. That\nbasketdata container is defined as part of the multi-container _docker-compose.yml_ file, as shown in the\nfollowing example:', 'This code in the docker-compose.yml defines a container named basketdata based on the redis image\nand publishing the port 6379 internally. This configuration means that it will only be accessible from\nother containers running within the Docker host.', 'Finally, in the _docker-compose.override.yml_ file, the basket-api microservice for the\neShopOnContainers sample defines the connection string to use for that Redis container:', 'As mentioned before, the name of the microservice basketdata is resolved by Docker’s internal\nnetwork DNS.', 'As described earlier, when you use event-based communication, a microservice publishes an event\nwhen something notable happens, such as when it updates a business entity. Other microservices\nsubscribe to those events. When a microservice receives an event, it can update its own business\nentities, which might lead to more events being published. This is the essence of the eventual\nconsistency concept. This publish/subscribe system is usually performed by using an implementation\nof an event bus. The event bus can be designed as an interface with the API needed to subscribe and\nunsubscribe to events and to publish events. It can also have one or more implementations based on\nany inter-process or messaging communication, such as a messaging queue or a service bus that\nsupports asynchronous communication and a publish/subscribe model.', 'You can use events to implement business transactions that span multiple services, which give you\neventual consistency between those services. An eventually consistent transaction consists of a series', '133 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'of distributed actions. At each action, the microservice updates a business entity and publishes an\nevent that triggers the next action. Figure 6-18 below, shows a PriceUpdated event published through\nan event bus, so the price update is propagated to the Basket and other microservices.', '_Figure 6-18. Event-driven communication based on an event bus_', 'This section describes how you can implement this type of communication with .NET by using a\ngeneric event bus interface, as shown in Figure 6-18. There are multiple potential implementations,\neach using a different technology or infrastructure such as RabbitMQ, Azure Service Bus, or any other\nthird-party open-source or commercial service bus.', 'As noted in the architecture section, you can choose from multiple messaging technologies for\nimplementing your abstract event bus. But these technologies are at different levels. For instance,\nRabbitMQ, a messaging broker transport, is at a lower level than commercial products like Azure\nService Bus, NServiceBus, MassTransit, or Brighter. Most of these products can work on top of either\nRabbitMQ or Azure Service Bus. Your choice of product depends on how many features and how\nmuch out-of-the-box scalability you need for your application.', 'For implementing just an event bus proof-of-concept for your development environment, as in the\neShopOnContainers sample, a simple implementation on top of RabbitMQ running as a container\nmight be enough. But for mission-critical and production systems that need high scalability, you\nmight want to evaluate and use Azure Service Bus.', '[If you require high-level abstractions and richer features like Sagas for long-running processes that](https://docs.particular.net/nservicebus/sagas/)\nmake distributed development easier, other commercial and open-source service buses like\nNServiceBus, MassTransit, and Brighter are worth evaluating. In this case, the abstractions and API to\nuse would usually be directly the ones provided by those high-level service buses instead of your own\n[abstractions (like the simple event bus abstractions provided at eShopOnContainers). For that matter,](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/BuildingBlocks/EventBus/EventBus/Abstractions/IEventBus.cs)', '134 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '[you can research the forked eShopOnContainers using NServiceBus](https://go.particular.net/eShopOnContainers) (additional derived sample\nimplemented by Particular Software).', 'Of course, you could always build your own service bus features on top of lower-level technologies\nlike RabbitMQ and Docker, but the work needed to “reinvent the wheel” might be too costly for a\ncustom enterprise application.', 'To reiterate: the sample event bus abstractions and implementation showcased in the\neShopOnContainers sample are intended to be used only as a proof of concept. Once you have\ndecided that you want to have asynchronous and event-driven communication, as explained in the\ncurrent section, you should choose the service bus product that best fits your needs for production.', 'Integration events are used for bringing domain state in sync across multiple microservices or external\nsystems. This functionality is done by publishing integration events outside the microservice. When an\nevent is published to multiple receiver microservices (to as many microservices as are subscribed to\nthe integration event), the appropriate event handler in each receiver microservice handles the event.', 'An integration event is basically a data-holding class, as in the following example:', 'The integration events can be defined at the application level of each microservice, so they are\ndecoupled from other microservices, in a way comparable to how ViewModels are defined in the\nserver and client. What is not recommended is sharing a common integration events library across\nmultiple microservices; doing that would be coupling those microservices with a single event\ndefinition data library. You do not want to do that for the same reasons that you do not want to share\na common domain model across multiple microservices: microservices must be completely\n[autonomous. For more information, see this blog post on the amount of data to put in events. Be](https://particular.net/blog/putting-your-events-on-a-diet)\n[careful not to take this too far, as this other blog post describes the problem data deficient messages](https://ardalis.com/data-deficient-messages/)\n[can produce. Your design of your events should aim to be “just right” for the needs of their](https://ardalis.com/data-deficient-messages/)\nconsumers.', 'There are only a few kinds of libraries you should share across microservices. One is libraries that are\n[final application blocks, like the Event Bus client API, as in eShopOnContainers. Another is libraries](https://github.com/dotnet-architecture/eShopOnContainers/tree/main/src/BuildingBlocks/EventBus)\nthat constitute tools that could also be shared as NuGet components, like JSON serializers.', '135 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'An event bus allows publish/subscribe-style communication between microservices without requiring\nthe components to explicitly be aware of each other, as shown in Figure 6-19.', '_Figure 6-19. Publish/subscribe basics with an event bus_', 'The above diagram shows that microservice A publishes to Event Bus, which distributes to subscribing\nmicroservices B and C, without the publisher needing to know the subscribers. The event bus is\nrelated to the Observer pattern and the publish-subscribe pattern.', '**Observer pattern**', '[In the Observer pattern, your primary object (known as the Observable) notifies other interested](https://en.wikipedia.org/wiki/Observer_pattern)\nobjects (known as Observers) with relevant information (events).', '**Publish/Subscribe (Pub/Sub) pattern**', '[The purpose of the Publish/Subscribe pattern is the same as the Observer pattern: you want to notify](https://docs.microsoft.com/previous-versions/msp-n-p/ff649664(v=pandp.10))\nother services when certain events take place. But there is an important difference between the\nObserver and Pub/Sub patterns. In the observer pattern, the broadcast is performed directly from the\nobservable to the observers, so they “know” each other. But when using a Pub/Sub pattern, there is a\nthird component, called broker, or message broker or event bus, which is known by both the\npublisher and subscriber. Therefore, when using the Pub/Sub pattern the publisher and the\nsubscribers are precisely decoupled thanks to the mentioned event bus or message broker.', '**The middleman or event bus**', 'How do you achieve anonymity between publisher and subscriber? An easy way is let a middleman\ntake care of all the communication. An event bus is one such middleman.', 'An event bus is typically composed of two parts:', 'Some production-ready messaging solutions:', '[We should start by saying that if you create your custom event bus based on RabbitMQ running in a](https://www.rabbitmq.com/)\ncontainer, as the eShopOnContainers application does, it should be used only for your development\nand test environments. Don’t use it for your production environment, unless you are building it as a\npart of a production-ready service bus as described in the Additional resources section below. A\nsimple custom event bus might be missing many production-ready critical features that a commercial\nservice bus has.', 'One of the event bus custom implementations in eShopOnContainers is basically a library using the\nRabbitMQ API. (There’s another implementation based on Azure Service Bus.)', 'The event bus implementation with RabbitMQ lets microservices subscribe to events, publish events,\nand receive events, as shown in Figure 6-21.', '138 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-21. RabbitMQ implementation of an event bus_', 'RabbitMQ functions as an intermediary between message publisher and subscribers, to handle\ndistribution. In the code, the EventBusRabbitMQ class implements the generic IEventBus interface.\nThis implementation is based on Dependency Injection so that you can swap from this dev/test\nversion to a production version.', 'The RabbitMQ implementation of a sample dev/test event bus is boilerplate code. It has to handle the\nconnection to the RabbitMQ server and provide code for publishing a message event to the queues. It\nalso has to implement a dictionary of collections of integration event handlers for each event type;\nthese event types can have a different instantiation and different subscriptions for each receiver\nmicroservice, as shown in Figure 6-21.', 'The following code is a _**simplified**_ version of an event bus implementation for RabbitMQ, to\nshowcase the whole scenario. You don’t really handle the connection this way. To see the full\n[implementation, see the actual code in the dotnet-architecture/eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/BuildingBlocks/EventBus/EventBusRabbitMQ/EventBusRabbitMQ.cs) repository.', '139 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '[The actual code](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/BuildingBlocks/EventBus/EventBusRabbitMQ/EventBusRabbitMQ.cs) of the Publish method in the eShopOnContainers application is improved by using a\n[Polly](https://github.com/App-vNext/Polly) retry policy, which retries the task some times in case the RabbitMQ container is not ready. This\nscenario can occur when docker-compose is starting the containers; for example, the RabbitMQ\ncontainer might start more slowly than the other containers.', 'As mentioned earlier, there are many possible configurations in RabbitMQ, so this code should be\nused only for dev/test environments.', 'As with the publish code, the following code is a simplification of part of the event bus\nimplementation for RabbitMQ. Again, you usually do not need to change it unless you are improving\nit.', '140 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Each event type has a related channel to get events from RabbitMQ. You can then have as many event\nhandlers per channel and event type as needed.', 'The Subscribe method accepts an IIntegrationEventHandler object, which is like a callback method in\nthe current microservice, plus its related IntegrationEvent object. The code then adds that event\nhandler to the list of event handlers that each integration event type can have per client microservice.\nIf the client code has not already been subscribed to the event, the code creates a channel for the\nevent type so it can receive events in a push style from RabbitMQ when that event is published from\nany other service.', 'As mentioned above, the event bus implemented in eShopOnContainers has only an educational\npurpose, since it only handles the main scenarios, so it’s not ready for production.', 'For production scenarios check the additional resources below, specific for RabbitMQ, and the\nImplementing event-based communication between microservices section.', 'A production-ready solution with support for RabbitMQ.', 'The first step for using the event bus is to subscribe the microservices to the events they want to\nreceive. That functionality should be done in the receiver microservices.', 'The following simple code shows what each receiver microservice needs to implement when starting\nthe service (that is, in the Startup class) so it subscribes to the events it needs. In this case, the basketapi microservice needs to subscribe to ProductPriceChangedIntegrationEvent and the\nOrderStartedIntegrationEvent messages.', 'For instance, when subscribing to the ProductPriceChangedIntegrationEvent event, that makes the\nbasket microservice aware of any changes to the product price and lets it warn the user about the\nchange if that product is in the user’s basket.', '141 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'After this code runs, the subscriber microservice will be listening through RabbitMQ channels. When\nany message of type ProductPriceChangedIntegrationEvent arrives, the code invokes the event\nhandler that is passed to it and processes the event.', 'Finally, the message sender (origin microservice) publishes the integration events with code similar to\nthe following example. (This approach is a simplified example that does not take atomicity into\naccount.) You would implement similar code whenever an event must be propagated across multiple\nmicroservices, usually right after committing data or transactions from the origin microservice.', 'First, the event bus implementation object (based on RabbitMQ or based on a service bus) would be\ninjected at the controller constructor, as in the following code:', 'Then you use it from your controller’s methods, like in the UpdateProduct method:', '142 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'In this case, since the origin microservice is a simple CRUD microservice, that code is placed right into\na Web API controller.', 'In more advanced microservices, like when using CQRS approaches, it can be implemented in the\nCommandHandler class, within the Handle() method.', '**Designing atomicity and resiliency when publishing to the event bus**', 'When you publish integration events through a distributed messaging system like your event bus, you\nhave the problem of atomically updating the original database and publishing an event (that is, either\nboth operations complete or none of them). For instance, in the simplified example shown earlier, the\ncode commits data to the database when the product price is changed and then publishes a\nProductPriceChangedIntegrationEvent message. Initially, it might look essential that these two\noperations be performed atomically. However, if you are using a distributed transaction involving the\n[database and the message broker, as you do in older systems like Microsoft Message Queuing](https://docs.microsoft.com/previous-versions/windows/desktop/legacy/ms711472(v=vs.85))\n[(MSMQ), this approach is not recommended for the reasons described by the CAP theorem.](https://docs.microsoft.com/previous-versions/windows/desktop/legacy/ms711472(v=vs.85))', 'Basically, you use microservices to build scalable and highly available systems. Simplifying somewhat,\nthe CAP theorem says that you cannot build a (distributed) database (or a microservice that owns its\nmodel) that’s continually available, strongly consistent, _and_ tolerant to any partition. You must choose\ntwo of these three properties.', 'In microservices-based architectures, you should choose availability and tolerance, and you should\nde-emphasize strong consistency. Therefore, in most modern microservice-based applications, you\nusually do not want to use distributed transactions in messaging, as you do when you implement\n[distributed transactions](https://docs.microsoft.com/previous-versions/windows/desktop/ms681205(v=vs.85)) based on the Windows Distributed Transaction Coordinator (DTC) with\n[MSMQ.](https://docs.microsoft.com/previous-versions/windows/desktop/legacy/ms711472(v=vs.85))', 'Let’s go back to the initial issue and its example. If the service crashes after the database is updated\n(in this case, right after the line of code with _context.SaveChangesAsync()), but before the integration\nevent is published, the overall system could become inconsistent. This approach might be business\ncritical, depending on the specific business operation you are dealing with.', 'As mentioned earlier in the architecture section, you can have several approaches for dealing with this\nissue:', 'An important aspect of update message events is that a failure at any point in the communication\nshould cause the message to be retried. Otherwise a background task might try to publish an event\nthat has already been published, creating a race condition. Make sure that the updates are either\nidempotent or that they provide enough information to ensure that you can detect a duplicate,\ndiscard it, and send back only one response.', 'As noted earlier, idempotency means that an operation can be performed multiple times without\nchanging the result. In a messaging environment, as when communicating events, an event is\nidempotent if it can be delivered multiple times without changing the result for the receiver\nmicroservice. This may be necessary because of the nature of the event itself, or because of the way\nthe system handles the event. Message idempotency is important in any application that uses\nmessaging, not just in applications that implement the event bus pattern.', 'An example of an idempotent operation is a SQL statement that inserts data into a table only if that\ndata is not already in the table. It does not matter how many times you run that insert SQL statement;\nthe result will be the same—the table will contain that data. Idempotency like this can also be\nnecessary when dealing with messages if the messages could potentially be sent and therefore', '149 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'processed more than once. For instance, if retry logic causes a sender to send exactly the same\nmessage more than once, you need to make sure that it is idempotent.', 'It is possible to design idempotent messages. For example, you can create an event that says “set the\nproduct price to $25” instead of “add $5 to the product price.” You could safely process the first\nmessage any number of times and the result will be the same. That is not true for the second\nmessage. But even in the first case, you might not want to process the first event, because the system\ncould also have sent a newer price-change event and you would be overwriting the new price.', 'Another example might be an order-completed event that’s propagated to multiple subscribers. The\napp has to make sure that order information is updated in other systems only once, even if there are\nduplicated message events for the same order-completed event.', 'It is convenient to have some kind of identity per event so that you can create logic that enforces that\neach event is processed only once per receiver.', 'Some message processing is inherently idempotent. For example, if a system generates image\nthumbnails, it might not matter how many times the message about the generated thumbnail is\nprocessed; the outcome is that the thumbnails are generated and they are the same every time. On\nthe other hand, operations such as calling a payment gateway to charge a credit card may not be\nidempotent at all. In these cases, you need to ensure that processing a message multiple times has\nthe effect that you expect.', '**Additional resources**', 'You can make sure that message events are sent and processed only once per subscriber at different\nlevels. One way is to use a deduplication feature offered by the messaging infrastructure you are\nusing. Another is to implement custom logic in your destination microservice. Having validations at\nboth the transport level and the application level is your best bet.', '**Deduplicating message events at the EventHandler level**', 'One way to make sure that an event is processed only once by any receiver is by implementing certain\nlogic when processing the message events in event handlers. For example, that is the approach used\nin the eShopOnContainers application, as you can see in the source code of the\n[UserCheckoutAcceptedIntegrationEventHandler class when it receives a](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/Services/Ordering/Ordering.API/Application/IntegrationEvents/EventHandling/UserCheckoutAcceptedIntegrationEventHandler.cs)\nUserCheckoutAcceptedIntegrationEvent integration event. (In this case, the CreateOrderCommand is\nwrapped with an IdentifiedCommand, using the eventMsg.RequestId as an identifier, before sending it\nto the command handler).', '150 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Deduplicating messages when using RabbitMQ**', 'When intermittent network failures happen, messages can be duplicated, and the message receiver\nmust be ready to handle these duplicated messages. If possible, receivers should handle messages in\nan idempotent way, which is better than explicitly handling them with deduplication.', '[According to the RabbitMQ documentation, “If a message is delivered to a consumer and then](https://www.rabbitmq.com/reliability.html#consumer)\nrequeued (because it was not acknowledged before the consumer connection dropped, for example)\nthen RabbitMQ will set the redelivered flag on it when it is delivered again (whether to the same\nconsumer or a different one).', 'If the “redelivered” flag is set, the receiver must take that into account, because the message might\nalready have been processed. But that is not guaranteed; the message might never have reached the\nreceiver after it left the message broker, perhaps because of network issues. On the other hand, if the\n“redelivered” flag is not set, it is guaranteed that the message has not been sent more than once.\nTherefore, the receiver needs to deduplicate messages or process messages in an idempotent way\nonly if the “redelivered” flag is set in the message.', '**Additional resources**', 'Controllers are a central part of any ASP.NET Core API service and ASP.NET MVC Web application. As\nsuch, you should have confidence they behave as intended for your application. Automated tests can\nprovide you with this confidence and can detect errors before they reach production.', 'You need to test how the controller behaves based on valid or invalid inputs, and test controller\nresponses based on the result of the business operation it performs. However, you should have these\ntypes of tests for your microservices:', 'The reference application (eShopOnContainers) tests were recently restructured and now there are\nfour categories:', 'As you can see, these docker-compose files only start the Redis, RabbitMQ, SQL Server, and MongoDB\nmicroservices.', '**Additional resources**', 'Background tasks and scheduled jobs are something you might need to use in any application,\nwhether or not it follows the microservices architecture pattern. The difference when using a\nmicroservices architecture is that you can implement the background task in a separate\nprocess/container for hosting so you can scale it down/up based on your need.', '157 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'From a generic point of view, in .NET we called these type of tasks _Hosted Services_, because they are\nservices/logic that you host within your host/application/microservice. Note that in this case, the\nhosted service simply means a class with the background task logic.', '[Since .NET Core 2.0, the framework provides a new interface named IHostedService helping you to](https://docs.microsoft.com/dotnet/api/microsoft.extensions.hosting.ihostedservice)\neasily implement hosted services. The basic idea is that you can register multiple background tasks\n(hosted services) that run in the background while your web host or host is running, as shown in the\nimage 6-26.', '_Figure 6-26. Using IHostedService in a WebHost vs. a Host_', 'ASP.NET Core 1.x and 2.x support IWebHost for background processes in web apps. .NET Core 2.1 and\nlater versions support IHost for background processes with plain console apps. Note the difference\nmade between WebHost and Host.', 'A WebHost (base class implementing IWebHost) in ASP.NET Core 2.0 is the infrastructure artifact you\nuse to provide HTTP server features to your process, such as when you’re implementing an MVC web\napp or Web API service. It provides all the new infrastructure goodness in ASP.NET Core, enabling you\nto use dependency injection, insert middlewares in the request pipeline, and similar. The WebHost\nuses these very same IHostedServices for background tasks.', 'A Host (base class implementing IHost) was introduced in .NET Core 2.1. Basically, a Host allows you\nto have a similar infrastructure than what you have with WebHost (dependency injection, hosted\nservices, etc.), but in this case, you just want to have a simple and lighter process as the host, with\nnothing related to MVC, Web API or HTTP server features.', 'Therefore, you can choose and either create a specialized host-process with IHost to handle the\nhosted services and nothing else, such a microservice made just for hosting the IHostedServices, or\nyou can alternatively extend an existing ASP.NET Core WebHost, such as an existing ASP.NET Core\nWeb API or MVC app.', '158 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Each approach has pros and cons depending on your business and scalability needs. The bottom line\nis basically that if your background tasks have nothing to do with HTTP (IWebHost) you should use\nIHost.', 'Let’s drill down further on the IHostedService interface since its usage is pretty similar in a WebHost or\nin a Host.', 'SignalR is one example of an artifact using hosted services, but you can also use it for much simpler\nthings like:', 'When you register an IHostedService, .NET calls the StartAsync() and StopAsync() methods of your\nIHostedService type during application start and stop respectively. For more details, see\n[IHostedService interface.](https://docs.microsoft.com/aspnet/core/fundamentals/host/hosted-services#ihostedservice-interface)', '159 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'As you can imagine, you can create multiple implementations of IHostedService and register each of\nthem in _Program.cs_, as shown previously. All those hosted services will be started and stopped along\nwith the application/microservice.', 'As a developer, you are responsible for handling the stopping action of your services when\nStopAsync() method is triggered by the host.', 'You could go ahead and create your custom hosted service class from scratch and implement the\nIHostedService, as you need to do when using .NET Core 2.0 and later.', 'However, since most background tasks will have similar needs in regard to the cancellation tokens\nmanagement and other typical operations, there is a convenient abstract base class you can derive\nfrom, named BackgroundService (available since .NET Core 2.1).', 'That class provides the main work needed to set up the background task.', 'The next code is the abstract BackgroundService base class as implemented in .NET.', '160 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'When deriving from the previous abstract base class, thanks to that inherited implementation, you just\nneed to implement the ExecuteAsync() method in your own custom hosted service class, as in the\nfollowing simplified code from eShopOnContainers which is polling a database and publishing\nintegration events into the Event Bus when needed.', '161 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'In this specific case for eShopOnContainers, it’s executing an application method that’s querying a\ndatabase table looking for orders with a specific state and when applying changes, it is publishing\nintegration events through the event bus (underneath it can be using RabbitMQ or Azure Service Bus).', 'Of course, you could run any other business background task, instead.', 'By default, the cancellation token is set with a 5 seconds timeout, although you can change that value\nwhen building your WebHost using the UseShutdownTimeout extension of the IWebHostBuilder. This\nmeans that our service is expected to cancel within 5 seconds otherwise it will be more abruptly killed.', 'The following code would be changing that time to 10 seconds.', '**Summary class diagram**', 'The following image shows a visual summary of the classes and interfaces involved when\nimplementing IHostedServices.', '_Figure 6-27. Class diagram showing the multiple classes and interfaces related to IHostedService_', 'Class diagram: IWebHost and IHost can host many services, which inherit from BackgroundService,\nwhich implements IHostedService.', '162 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Deployment considerations and takeaways**', 'It is important to note that the way you deploy your ASP.NET Core WebHost or .NET Host might\nimpact the final solution. For instance, if you deploy your WebHost on IIS or a regular Azure App\nService, your host can be shut down because of app pool recycles. But if you are deploying your host\nas a container into an orchestrator like Kubernetes, you can control the assured number of live\ninstances of your host. In addition, you could consider other approaches in the cloud especially made\nfor these scenarios, like Azure Functions. Finally, if you need the service to be running all the time and\nare deploying on a Windows Server you could use a Windows Service.', 'But even for a WebHost deployed into an app pool, there are scenarios like repopulating or flushing\napplication’s in-memory cache that would be still applicable.', 'The IHostedService interface provides a convenient way to start background tasks in an ASP.NET Core\nweb application (in .NET Core 2.0 and later versions) or in any process/host (starting in .NET Core 2.1\nwith IHost). Its main benefit is the opportunity you get with the graceful cancellation to clean-up the\ncode of your background tasks when the host itself is shutting down.', 'The following architecture diagram shows how API Gateways were implemented with Ocelot in\neShopOnContainers.', '163 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-28. eShopOnContainers architecture with API Gateways_', 'That diagram shows how the whole application is deployed into a single Docker host or development\nPC with “Docker for Windows” or “Docker for Mac”. However, deploying into any orchestrator would\nbe similar, but any container in the diagram could be scaled out in the orchestrator.', 'In addition, the infrastructure assets such as databases, cache, and message brokers should be\noffloaded from the orchestrator and deployed into high available systems for infrastructure, like Azure\nSQL Database, Azure Cosmos DB, Azure Redis, Azure Service Bus, or any HA clustering solution onpremises.', 'As you can also notice in the diagram, having several API Gateways allows multiple development\nteams to be autonomous (in this case Marketing features vs. Shopping features) when developing and\ndeploying their microservices plus their own related API Gateways.', 'If you had a single monolithic API Gateway that would mean a single point to be updated by several\ndevelopment teams, which could couple all the microservices with a single part of the application.', 'Going much further in the design, sometimes a fine-grained API Gateway can also be limited to a\nsingle business microservice depending on the chosen architecture. Having the API Gateway’s\nboundaries dictated by the business or domain will help you to get a better design.', 'For instance, fine granularity in the API Gateway tier can be especially useful for more advanced\ncomposite UI applications that are based on microservices, because the concept of a fine-grained API\nGateway is similar to a UI composition service.', 'We delve into more details in the previous section Creating composite UI based on microservices.', 'As a key takeaway, for many medium- and large-size applications, using a custom-built API Gateway\nproduct is usually a good approach, but not as a single monolithic aggregator or unique central\ncustom API Gateway unless that API Gateway allows multiple independent configuration areas for the\nseveral development teams creating autonomous microservices.', '164 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Sample microservices/containers to reroute through the API Gateways**', 'As an example, eShopOnContainers has around six internal microservice-types that have to be\npublished through the API Gateways, as shown in the following image.', '_Figure 6-29. Microservice folders in eShopOnContainers solution in Visual Studio_', 'About the Identity service, in the design it’s left out of the API Gateway routing because it’s the only\ncross-cutting concern in the system, although with Ocelot it’s also possible to include it as part of the\nrerouting lists.', 'All those services are currently implemented as ASP.NET Core Web API services, as you can tell from\nthe code. Let’s focus on one of the microservices like the Catalog microservice code.', '_Figure 6-30. Sample Web API microservice (Catalog microservice)_', '165 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'You can see that the Catalog microservice is a typical ASP.NET Core Web API project with several\ncontrollers and methods like in the following code.', 'The HTTP request will end up running that kind of C# code accessing the microservice database and\nany additional required action.', 'Regarding the microservice URL, when the containers are deployed in your local development PC\n(local Docker host), each microservice’s container always has an internal port (usually port 80)\nspecified in its dockerfile, as in the following dockerfile:', 'The port 80 shown in the code is internal within the Docker host, so it can’t be reached by client apps.', 'Client apps can access only the external ports (if any) published when deploying with dockercompose.', 'Those external ports shouldn’t be published when deploying to a production environment. For this\nspecific reason, why you want to use the API Gateway, to avoid the direct communication between the\nclient apps and the microservices.', 'However, when developing, you want to access the microservice/container directly and run it through\nSwagger. That’s why in eShopOnContainers, the external ports are still specified even when they won’t\nbe used by the API Gateway or the client apps.', 'Here’s an example of the docker-compose.override.yml file for the Catalog microservice:', '166 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'You can see how in the docker-compose.override.yml configuration the internal port for the Catalog\ncontainer is port 80, but the port for external access is 5101. But this port shouldn’t be used by the\napplication when using an API Gateway, only to debug, run, and test just the Catalog microservice.', 'Normally, you won’t be deploying with docker-compose into a production environment because the\nright production deployment environment for microservices is an orchestrator like Kubernetes or\nService Fabric. When deploying to those environments you use different configuration files where you\nwon’t publish directly any external port for the microservices but, you’ll always use the reverse proxy\nfrom the API Gateway.', 'Run the catalog microservice in your local Docker host. Either run the full eShopOnContainers solution\nfrom Visual Studio (it runs all the services in the docker-compose files), or start the Catalog\nmicroservice with the following docker-compose command in CMD or PowerShell positioned at the\nfolder where the docker-compose.yml and docker-compose.override.yml are placed.', 'This command only runs the catalog-api service container plus dependencies that are specified in the\ndocker-compose.yml. In this case, the SQL Server container and RabbitMQ container.', 'Then, you can directly access the Catalog microservice and see its methods through the Swagger UI\naccessing directly through that “external” port, in this case http://host.docker.internal:5101/swagger:', '167 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-31. Testing the Catalog microservice with its Swagger UI_', 'At this point, you could set a breakpoint in C# code in Visual Studio, test the microservice with the\nmethods exposed in Swagger UI, and finally clean-up everything with the docker-compose down\ncommand.', 'However, direct-access communication to the microservice, in this case through the external port\n5101, is precisely what you want to avoid in your application. And you can avoid that by setting the\nadditional level of indirection of the API Gateway (Ocelot, in this case). That way, the client app won’t\ndirectly access the microservice.', 'Ocelot is basically a set of middleware that you can apply in a specific order.', 'Ocelot is designed to work with ASP.NET Core only. The latest version of the package is 18.0 which\ntargets .NET 6 and hence is not suitable for .NET Framework applications.', '168 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '[You install Ocelot and its dependencies in your ASP.NET Core project with Ocelot’s NuGet package,](https://www.nuget.org/packages/Ocelot/)\nfrom Visual Studio.', 'In eShopOnContainers, its API Gateway implementation is a simple ASP.NET Core WebHost project,\nand Ocelot’s middleware handles all the API Gateway features, as shown in the following image:', '_Figure 6-32. The OcelotApiGw base project in eShopOnContainers_', 'This ASP.NET Core WebHost project is built with two simple files: Program.cs and Startup.cs.', 'The Program.cs just needs to create and configure the typical ASP.NET Core BuildWebHost.', '169 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'The important point here for Ocelot is the configuration.json file that you must provide to the builder\nthrough the AddJsonFile() method. That configuration.json is where you specify all the API Gateway\nReRoutes, meaning the external endpoints with specific ports and the correlated internal endpoints,\nusually using different ports.', 'There are two sections to the configuration. An array of ReRoutes and a GlobalConfiguration. The\nReRoutes are the objects that tell Ocelot how to treat an upstream request. The Global configuration\nallows overrides of ReRoute specific settings. It’s useful if you don’t want to manage lots of ReRoute\nspecific settings.', '[Here’s a simplified example of ReRoute configuration file](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/ApiGateways/Mobile.Bff.Shopping/apigw/configuration.json) from one of the API Gateways from\neShopOnContainers.', '170 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'The main functionality of an Ocelot API Gateway is to take incoming HTTP requests and forward them\non to a downstream service, currently as another HTTP request. Ocelot’s describes the routing of one\nrequest to another as a ReRoute.', 'For instance, let’s focus on one of the ReRoutes in the configuration.json from above, the\nconfiguration for the Basket microservice.', 'The DownstreamPathTemplate, Scheme, and DownstreamHostAndPorts make the internal\nmicroservice URL that this request will be forwarded to.', 'The port is the internal port used by the service. When using containers, the port specified at its\ndockerfile.', 'The Host is a service name that depends on the service name resolution you are using. When using\ndocker-compose, the services names are provided by the Docker Host, which is using the service\nnames provided in the docker-compose files. If using an orchestrator like Kubernetes or Service\nFabric, that name should be resolved by the DNS or name resolution provided by each orchestrator.', 'DownstreamHostAndPorts is an array that contains the host and port of any downstream services that\nyou wish to forward requests to. Usually this configuration will just contain one entry but sometimes\nyou might want to load balance requests to your downstream services and Ocelot lets you add more\nthan one entry and then select a load balancer. But if using Azure and any orchestrator it is probably a\nbetter idea to load balance with the cloud and orchestrator infrastructure.', 'The UpstreamPathTemplate is the URL that Ocelot will use to identify which\nDownstreamPathTemplate to use for a given request from the client. Finally, the\nUpstreamHttpMethod is used so Ocelot can distinguish between different requests (GET, POST, PUT)\nto the same URL.', 'At this point, you could have a single Ocelot API Gateway (ASP.NET Core WebHost) using one or\n[multiple merged configuration.json files or you can also store the configuration in a Consul KV store.](https://ocelot.readthedocs.io/en/latest/features/configuration.html#merging-configuration-files)', 'But as introduced in the architecture and design sections, if you really want to have autonomous\nmicroservices, it might be better to split that single monolithic API Gateway into multiple API\nGateways and/or BFF (Backend for Frontend). For that purpose, let’s see how to implement that\napproach with Docker containers.', '171 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '**Using a single Docker container image to run multiple different API Gateway / BFF**\n**container types**', 'In eShopOnContainers, we’re using a single Docker container image with the Ocelot API Gateway but\nthen, at run time, we create different services/containers for each type of API-Gateway/BFF by\nproviding a different configuration.json file, using a docker volume to access a different PC folder for\neach service.', '_Figure 6-33. Reusing a single Ocelot Docker image across multiple API Gateway types_', 'In eShopOnContainers, the “Generic Ocelot API Gateway Docker Image” is created with the project\nnamed ‘OcelotApiGw’ and the image name “eshop/ocelotapigw” that is specified in the dockercompose.yml file. Then, when deploying to Docker, there will be four API-Gateway containers created\nfrom that same Docker image, as shown in the following extract from the docker-compose.yml file.', '172 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Additionally, as you can see in the following docker-compose.override.yml file, the only difference\nbetween those API Gateway containers is the Ocelot configuration file, which is different for each\nservice container and it’s specified at run time through a Docker volume.', '173 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Because of that previous code, and as shown in the Visual Studio Explorer below, the only file needed\nto define each specific business/BFF API Gateway is just a configuration.json file, because the four API\nGateways are based on the same Docker image.', '_Figure 6-34. The only file needed to define each API Gateway / BFF with Ocelot is a configuration file_', 'By splitting the API Gateway into multiple API Gateways, different development teams focusing on\ndifferent subsets of microservices can manage their own API Gateways by using independent Ocelot\nconfiguration files. Plus, at the same time they can reuse the same Ocelot Docker image.', 'Now, if you run eShopOnContainers with the API Gateways (included by default in VS when opening\neShopOnContainers-ServicesAndWebApps.sln solution or if running “docker-compose up”), the\nfollowing sample routes will be performed.', 'For instance, when visiting the upstream URL\nhttp://host.docker.internal:5202/api/v1/c/catalog/items/2/ served by the webshoppingapigw API\nGateway, you get the same result from the internal Downstream URL http://catalog-api/api/v1/2\nwithin the Docker host, as in the following browser.', '_Figure 6-35. Accessing a microservice through a URL provided by the API Gateway_', 'Because of testing or debugging reasons, if you wanted to directly access to the Catalog Docker\ncontainer (only at the development environment) without passing through the API Gateway, since\n‘catalog-api’ is a DNS resolution internal to the Docker host (service discovery handled by dockercompose service names), the only way to directly access the container is through the external port\npublished in the docker-compose.override.yml, which is provided only for development tests, such as\nhttp://host.docker.internal:5101/api/v1/Catalog/items/1 in the following browser.', '174 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-36. Direct access to a microservice for testing purposes_', 'But the application is configured so it accesses all the microservices through the API Gateways, not\nthrough the direct port “shortcuts”.', '**The Gateway aggregation pattern in eShopOnContainers**', 'As introduced previously, a flexible way to implement requests aggregation is with custom services, by\n[code. You could also implement request aggregation with the Request Aggregation feature in Ocelot,](https://ocelot.readthedocs.io/en/latest/features/requestaggregation.html#request-aggregation)\nbut it might not be as flexible as you need. Therefore, the selected way to implement aggregation in\neShopOnContainers is with an explicit ASP.NET Core Web API service for each aggregator.', 'According to that approach, the API Gateway composition diagram is in reality a bit more extended\nwhen considering the aggregator services that are not shown in the simplified global architecture\ndiagram shown previously.', 'In the following diagram, you can also see how the aggregator services work with their related API\nGateways.', '_Figure 6-37. eShopOnContainers architecture with aggregator services_', 'Zooming in further, on the “Shopping” business area in the following image, you can see that\nchattiness between the client apps and the microservices is reduced when using the aggregator\nservices in the API Gateways.', '175 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-38. Zoom in vision of the Aggregator services_', 'You can notice how when the diagram shows the possible requests coming from the API Gateways it\ncan get complex. On the other hand, when you use the aggregator pattern, you can see how the\narrows in blue would simplify the communication from a client app perspective. This pattern not only\nhelps to reduce the chattiness and latency in the communication, it also improves the user experience\nsignificantly for the remote apps (mobile and SPA apps).', 'In the case of the “Marketing” business area and microservices, it is a simple use case so there was no\nneed to use aggregators, but it could also be possible, if needed.', '**Authentication and authorization in Ocelot API Gateways**', 'In an Ocelot API Gateway, you can sit the authentication service, such as an ASP.NET Core Web API\n[service using IdentityServer providing the auth token, either out or inside the API Gateway.](https://docs.microsoft.com/dotnet/architecture/cloud-native/identity-server)', 'Since eShopOnContainers is using multiple API Gateways with boundaries based on BFF and business\nareas, the Identity/Auth service is left out of the API Gateways, as highlighted in yellow in the\nfollowing diagram.', '176 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', '_Figure 6-39. Position of the Identity service in eShopOnContainers_', 'However, Ocelot also supports sitting the Identity/Auth microservice within the API Gateway\nboundary, as in this other diagram.', '_Figure 6-40. Authentication in Ocelot_', 'As the previous diagram shows, when the Identity microservice is beneath the API gateway (AG): 1) AG\nrequests an auth token from identity microservice, 2) The identity microservice returns token to AG, 34) AG requests from microservices using the auth token. Because eShopOnContainers application has\nsplit the API Gateway into multiple BFF (Backend for Frontend) and business areas API Gateways,\nanother option would have been to create an additional API Gateway for cross-cutting concerns. That\nchoice would be fair in a more complex microservice based architecture with multiple cross-cutting\nconcerns microservices. Since there’s only one cross-cutting concern in eShopOnContainers, it was\ndecided to just handle the security service out of the API Gateway realm, for simplicity’s sake.', '177 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'In any case, if the app is secured at the API Gateway level, the authentication module of the Ocelot\nAPI Gateway is visited at first when trying to use any secured microservice. That redirects the HTTP\nrequest to visit the Identity or auth microservice to get the access token so you can visit the protected\nservices with the access_token.', 'The way you secure with authentication any service at the API Gateway level is by setting the\nAuthenticationProviderKey in its related settings at the configuration.json.', 'When Ocelot runs, it will look at the ReRoutes AuthenticationOptions.AuthenticationProviderKey and\ncheck that there is an Authentication Provider registered with the given key. If there isn’t, then Ocelot\nwill not start up. If there is, then the ReRoute will use that provider when it executes.', 'Because the Ocelot WebHost is configured with the authenticationProviderKey = "IdentityApiKey",\nthat will require authentication whenever that service has any requests without any auth token.', '178 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'Then, you also need to set authorization with the [Authorize] attribute on any resource to be accessed\nlike the microservices, such as in the following Basket microservice controller.', 'The ValidAudiences such as “basket” are correlated with the audience defined in each microservice\nwith AddJwtBearer() at the ConfigureServices() of the Startup class, such as in the code below.', 'If you try to access any secured microservice, like the Basket microservice with a ReRoute URL based\non the API Gateway like http://host.docker.internal:5202/api/v1/b/basket/1, then you’ll get a 401\nUnauthorized unless you provide a valid token. On the other hand, if a ReRoute URL is authenticated,\nOcelot will invoke whatever downstream scheme is associated with it (the internal microservice URL).', '**Authorization at Ocelot’s ReRoutes tier.** Ocelot supports claims-based authorization evaluated after\nthe authentication. You set the authorization at a route level by adding the following lines to the\nReRoute configuration.', 'In that example, when the authorization middleware is called, Ocelot will find if the user has the claim\ntype ‘UserType’ in the token and if the value of that claim is ‘employee’. If it isn’t, then the user will not\nbe authorized and the response will be 403 forbidden.', '179 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'When using Kubernetes (like in an Azure Kubernetes Service cluster), you usually unify all the HTTP\n[requests through the Kubernetes Ingress tier based on](https://kubernetes.io/docs/concepts/services-networking/ingress/) _Nginx_ .', 'In Kubernetes, if you don’t use any ingress approach, then your services and pods have IPs only\nroutable by the cluster network.', 'But if you use an ingress approach, you’ll have a middle tier between the Internet and your services\n(including your API Gateways), acting as a reverse proxy.', 'As a definition, an Ingress is a collection of rules that allow inbound connections to reach the cluster\nservices. An ingress is configured to provide services externally reachable URLs, load balance traffic,\nSSL termination and more. Users request ingress by POSTing the Ingress resource to the API server.', 'In eShopOnContainers, when developing locally and using just your development machine as the\nDocker host, you are not using any ingress but only the multiple API Gateways.', 'However, when targeting a “production” environment based on Kubernetes, eShopOnContainers is\nusing an ingress in front of the API gateways. That way, the clients still call the same base URL but the\nrequests are routed to multiple API Gateways or BFF.', 'API Gateways are front-ends or façades surfacing only the services but not the web applications that\nare usually out of their scope. In addition, the API Gateways might hide certain internal microservices.', 'The ingress, however, is just redirecting HTTP requests but not trying to hide any microservice or web\napp.', 'Having an ingress Nginx tier in Kubernetes in front of the web applications plus the several Ocelot API\nGateways / BFF is the ideal architecture, as shown in the following diagram.', '_Figure 6-41. The ingress tier in eShopOnContainers when deployed into Kubernetes_', 'A Kubernetes Ingress acts as a reverse proxy for all traffic to the app, including the web applications,\nthat are out of the Api gateway scope. When you deploy eShopOnContainers into Kubernetes, it', '180 CHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications', 'exposes just a few services or endpoints via _ingress_, basically the following list of postfixes on the\nURLs:', 'There are other important features to research and use, when using an Ocelot API Gateway, described\nin the following links.', '_Design a domain model for each microservice or Bounded Context that reflects understanding of the_\n_business domain._', 'This section focuses on more advanced microservices that you implement when you need to tackle\ncomplex subsystems, or microservices derived from the knowledge of domain experts with everchanging business rules. The architecture patterns used in this section are based on domain-driven\ndesign (DDD) and Command and Query Responsibility Segregation (CQRS) approaches, as illustrated\nin Figure 7-1.', '182 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-1. External microservice architecture versus internal architecture patterns for each microservice_', 'However, most of the techniques for data driven microservices, such as how to implement an ASP.NET\nCore Web API service or how to expose Swagger metadata with Swashbuckle or NSwag, are also\napplicable to the more advanced microservices implemented internally with DDD patterns. This\nsection is an extension of the previous sections, because most of the practices explained earlier also\napply here or for any kind of microservice.', 'This section first provides details on the simplified CQRS patterns used in the eShopOnContainers\nreference application. Later, you will get an overview of the DDD techniques that enable you to find\ncommon patterns that you can reuse in your applications.', '[DDD is a large topic with a rich set of resources for learning. You can start with books like Domain-](https://domainlanguage.com/ddd/)\n[Driven Design](https://domainlanguage.com/ddd/) by Eric Evans and additional materials from Vaughn Vernon, Jimmy Nilsson, Greg\nYoung, Udi Dahan, Jimmy Bogard, and many other DDD/CQRS experts. But most of all you need to try\nto learn how to apply DDD techniques from the conversations, whiteboarding, and domain modeling\nsessions with the experts in your concrete business domain.', '**Additional resources**', '**DDD (Domain-Driven Design)**', 'CQRS is an architectural pattern that separates the models for reading and writing data. The related\n[term Command Query Separation (CQS) was originally defined by Bertrand Meyer in his book](https://martinfowler.com/bliki/CommandQuerySeparation.html) _Object-_\n_Oriented Software Construction_ . The basic idea is that you can divide a system’s operations into two\nsharply separated categories:', 'The design of the ordering microservice at the eShopOnContainers reference application is based on\nCQRS principles. However, it uses the simplest approach, which is just separating the queries from the\ncommands and using the same database for both actions.', 'The essence of those patterns, and the important point here, is that queries are idempotent: no matter\nhow many times you query a system, the state of that system won’t change. In other words, queries\nare side-effect free.', '186 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Therefore, you could use a different “reads” data model than the transactional logic “writes” domain\nmodel, even though the ordering microservices are using the same database. Hence, this is a\nsimplified CQRS approach.', 'On the other hand, commands, which trigger transactions and data updates, change state in the\nsystem. With commands, you need to be careful when dealing with complexity and ever-changing\nbusiness rules. This is where you want to apply DDD techniques to have a better modeled system.', 'The DDD patterns presented in this guide should not be applied universally. They introduce\nconstraints on your design. Those constraints provide benefits such as higher quality over time,\nespecially in commands and other code that modifies system state. However, those constraints add\ncomplexity with fewer benefits for reading and querying data.', 'One such pattern is the Aggregate pattern, which we examine more in later sections. Briefly, in the\nAggregate pattern, you treat many domain objects as a single unit as a result of their relationship in\nthe domain. You might not always gain advantages from this pattern in queries; it can increase the\ncomplexity of query logic. For read-only queries, you do not get the advantages of treating multiple\nobjects as a single Aggregate. You only get the complexity.', 'As shown in Figure 7-2 in the previous section, this guide suggests using DDD patterns only in the\ntransactional/updates area of your microservice (that is, as triggered by commands). Queries can\nfollow a simpler approach and should be separated from commands, following a CQRS approach.', 'For implementing the “queries side”, you can choose between many approaches, from your full-blown\nORM like EF Core, AutoMapper projections, stored procedures, views, materialized views or a micro\nORM.', 'In this guide and in eShopOnContainers (specifically the ordering microservice) we chose to\n[implement straight queries using a micro ORM like Dapper. This guide lets you implement any query](https://github.com/StackExchange/dapper-dot-net)\nbased on SQL statements to get the best performance, thanks to a light framework with little\noverhead.', 'When you use this approach, any updates to your model that impact how entities are persisted to a\nSQL database also need separate updates to SQL queries used by Dapper or any other separate (nonEF) approaches to querying.', 'It’s important to understand that CQRS and most DDD patterns (like DDD layers or a domain model\nwith aggregates) are not architectural styles, but only architecture patterns. Microservices, SOA, and\nevent-driven architecture (EDA) are examples of architectural styles. They describe a system of many\ncomponents, such as many microservices. CQRS and DDD patterns describe something inside a single\nsystem or component; in this case, something inside a microservice.', 'Different Bounded Contexts (BCs) will employ different patterns. They have different responsibilities,\nand that leads to different solutions. It is worth emphasizing that forcing the same pattern everywhere\nleads to failure. Do not use CQRS and DDD patterns everywhere. Many subsystems, BCs, or\nmicroservices are simpler and can be implemented more easily using simple CRUD services or using\nanother approach.', '187 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'There is only one application architecture: the architecture of the system or end-to-end application\nyou are designing (for example, the microservices architecture). However, the design of each Bounded\nContext or microservice within that application reflects its own tradeoffs and internal design decisions\nat an architecture patterns level. Do not try to apply the same architectural patterns as CQRS or DDD\neverywhere.', '**Additional resources**', 'For reads/queries, the ordering microservice from the eShopOnContainers reference application\nimplements the queries independently from the DDD model and transactional area. This\nimplementation was done primarily because the demands for queries and for transactions are\ndrastically different. Writes execute transactions that must be compliant with the domain logic.\nQueries, on the other hand, are idempotent and can be segregated from the domain rules.', 'The approach is simple, as shown in Figure 7-3. The API interface is implemented by the Web API\ncontrollers using any infrastructure, such as a micro Object Relational Mapper (ORM) like Dapper, and\nreturning dynamic ViewModels depending on the needs of the UI applications.', '_Figure 7-3. The simplest approach for queries in a CQRS microservice_', 'The simplest approach for the queries-side in a simplified CQRS approach can be implemented by\nquerying the database with a Micro-ORM like Dapper, returning dynamic ViewModels. The query', '188 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'definitions query the database and return a dynamic ViewModel built on the fly for each query. Since\nthe queries are idempotent, they won’t change the data no matter how many times you run a query.\nTherefore, you don’t need to be restricted by any DDD pattern used in the transactional side, like\naggregates and other patterns, and that is why queries are separated from the transactional area. You\nquery the database for the data that the UI needs and return a dynamic ViewModel that does not\nneed to be statically defined anywhere (no classes for the ViewModels) except in the SQL statements\nthemselves.', 'Since this approach is simple, the code required for the queries side (such as code using a micro ORM\n[like Dapper) can be implemented within the same Web API project. Figure 7-4 shows this approach.](https://github.com/StackExchange/Dapper)\nThe queries are defined in the **Ordering.API** microservice project within the eShopOnContainers\nsolution.', '_Figure 7-4. Queries in the Ordering microservice in eShopOnContainers_', 'Since the queries are performed to obtain the data needed by the client applications, the returned\ntype can be specifically made for the clients, based on the data returned by the queries. These models,\nor Data Transfer Objects (DTOs), are called ViewModels.', 'The returned data (ViewModel) can be the result of joining data from multiple entities or tables in the\ndatabase, or even across multiple aggregates defined in the domain model for the transactional area.\nIn this case, because you are creating queries independent of the domain model, the aggregates\nboundaries and constraints are ignored and you’re free to query any table and column you might\nneed. This approach provides great flexibility and productivity for the developers creating or updating\nthe queries.', 'The ViewModels can be static types defined in classes (as is implemented in the ordering\nmicroservice). Or they can be created dynamically based on the queries performed, which is agile for\ndevelopers.', 'You can use any micro ORM, Entity Framework Core, or even plain ADO.NET for querying. In the\nsample application, Dapper was selected for the ordering microservice in eShopOnContainers as a\ngood example of a popular micro ORM. It can run plain SQL queries with great performance, because\nit’s a light framework. Using Dapper, you can write a SQL query that can access and join multiple\ntables.', '189 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Dapper is an open-source project (original created by Sam Saffron), and is part of the building blocks\n[used in Stack Overflow. To use Dapper, you just need to install it through the Dapper NuGet package,](https://stackoverflow.com/)\nas shown in the following figure:', 'You also need to add a using directive so your code has access to the Dapper extension methods.', '[When you use Dapper in your code, you directly use the SqlConnection](https://learn.microsoft.com/dotnet/api/microsoft.data.sqlclient.sqlconnection) class available in the\n[Microsoft.Data.SqlClient](https://learn.microsoft.com/dotnet/api/microsoft.data.sqlclient) namespace. Through the QueryAsync method and other extension methods\n[that extend the SqlConnection class, you can run queries in a straightforward and performant way.](https://learn.microsoft.com/dotnet/api/microsoft.data.sqlclient.sqlconnection)', 'When returning ViewModels from the server-side to client apps, you can think about those\nViewModels as DTOs (Data Transfer Objects) that can be different to the internal domain entities of\nyour entity model because the ViewModels hold the data the way the client app needs. Therefore, in\nmany cases, you can aggregate data coming from multiple domain entities and compose the\nViewModels precisely according to how the client app needs that data.', 'Those ViewModels or DTOs can be defined explicitly (as data holder classes), like the OrderSummary\nclass shown in a later code snippet. Or, you could just return dynamic ViewModels or dynamic DTOs\nbased on the attributes returned by your queries as a dynamic type.', '**ViewModel as dynamic type**', 'As shown in the following code, a ViewModel can be directly returned by the queries by just returning\na _dynamic_ type that internally is based on the attributes returned by a query. That means that the\nsubset of attributes to be returned is based on the query itself. Therefore, if you add a new column to\nthe query or join, that data is dynamically added to the returned ViewModel.', '190 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The important point is that by using a dynamic type, the returned collection of data is dynamically\nassembled as the ViewModel.', '**Pros:** This approach reduces the need to modify static ViewModel classes whenever you update the\nSQL sentence of a query, making this design approach agile when coding, straightforward, and quick\nto evolve in regard to future changes.', '**Cons:** In the long term, dynamic types can negatively impact the clarity and the compatibility of a\nservice with client apps. In addition, middleware software like Swashbuckle cannot provide the same\nlevel of documentation on returned types if using dynamic types.', '**ViewModel as predefined DTO classes**', '**Pros** : Having static, predefined ViewModel classes, like “contracts” based on explicit DTO classes, is\ndefinitely better for public APIs but also for long-term microservices, even if they are only used by the\nsame application.', 'If you want to specify response types for Swagger, you need to use explicit DTO classes as the return\ntype. Therefore, predefined DTO classes allow you to offer richer information from Swagger. That\nimproves the API documentation and compatibility when consuming an API.', '**Cons** : As mentioned earlier, when updating the code, it takes some more steps to update the DTO\nclasses.', '_Tip based on our experience_ : In the queries implemented at the Ordering microservice in\neShopOnContainers, we started developing by using dynamic ViewModels as it was straightforward\nand agile on the early development stages. But, once the development was stabilized, we chose to\nrefactor the APIs and use static or pre-defined DTOs for the ViewModels, because it is clearer for the\nmicroservice’s consumers to know explicit DTO types, used as “contracts”.', 'In the following example, you can see how the query is returning data by using an explicit ViewModel\nDTO class: the OrderSummary class.', '191 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Describe response types of Web APIs**', 'Developers consuming web APIs and microservices are most concerned with what is returned—\nspecifically response types and error codes (if not standard). The response types are handled in the\nXML comments and data annotations.', 'Without proper documentation in the Swagger UI, the consumer lacks knowledge of what types are\nbeing returned or what HTTP codes can be returned. That problem is fixed by adding the\n[Microsoft.AspNetCore.Mvc.ProducesResponseTypeAttribute, so Swashbuckle can generate richer](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.mvc.producesresponsetypeattribute)\ninformation about the API return model and values, as shown in the following code:', 'However, the ProducesResponseType attribute cannot use dynamic as a type but requires to use\nexplicit types, like the OrderSummary ViewModel DTO, shown in the following example:', '192 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'This is another reason why explicit returned types are better than dynamic types, in the long term.\nWhen using the ProducesResponseType attribute, you can also specify what is the expected outcome\nregarding possible HTTP errors/codes, like 200, 400, etc.', 'In the following image, you can see how Swagger UI shows the ResponseType information.', '_Figure 7-5. Swagger UI showing response types and possible HTTP status codes from a Web API_', 'The image shows some example values based on the ViewModel types and the possible HTTP status\ncodes that can be returned.', 'Domain-driven design (DDD) advocates modeling based on the reality of business as relevant to your\nuse cases. In the context of building applications, DDD talks about problems as domains. It describes\nindependent problem areas as Bounded Contexts (each Bounded Context correlates to a\nmicroservice), and emphasizes a common language to talk about these problems. It also suggests\n[many technical concepts and patterns, like domain entities with rich models (no anemic-domain](https://martinfowler.com/bliki/AnemicDomainModel.html)\n[model), value objects, aggregates, and aggregate root (or root entity) rules to support the internal](https://martinfowler.com/bliki/AnemicDomainModel.html)\nimplementation. This section introduces the design and implementation of those internal patterns.', 'Sometimes these DDD technical rules and patterns are perceived as obstacles that have a steep\nlearning curve for implementing DDD approaches. But the important part is not the patterns\nthemselves, but organizing the code so it is aligned to the business problems, and using the same\nbusiness terms (ubiquitous language). In addition, DDD approaches should be applied only if you are\nimplementing complex microservices with significant business rules. Simpler responsibilities, like a\nCRUD service, can be managed with simpler approaches.', 'Where to draw the boundaries is the key task when designing and defining a microservice. DDD\npatterns help you understand the complexity in the domain. For the domain model for each Bounded\nContext, you identify and define the entities, value objects, and aggregates that model your domain.\nYou build and refine a domain model that is contained within a boundary that defines your context.\nAnd that is explicit in the form of a microservice. The components within those boundaries end up\nbeing your microservices, although in some cases a BC or business microservices can be composed of\nseveral physical services. DDD is about boundaries and so are microservices.', 'Determining where to place boundaries between Bounded Contexts balances two competing goals.\nFirst, you want to initially create the smallest possible microservices, although that should not be the\nmain driver; you should create a boundary around things that need cohesion. Second, you want to\navoid chatty communications between microservices. These goals can contradict one another. You\nshould balance them by decomposing the system into as many small microservices as you can until\nyou see communication boundaries growing quickly with each additional attempt to separate a new\nBounded Context. Cohesion is key within a single bounded context.', '[It is similar to the Inappropriate Intimacy code smell when implementing classes. If two microservices](https://sourcemaking.com/refactoring/smells/inappropriate-intimacy)\nneed to collaborate a lot with each other, they should probably be the same microservice.', '194 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Another way to look at this aspect is autonomy. If a microservice must rely on another service to\ndirectly service a request, it is not truly autonomous.', 'Most enterprise applications with significant business and technical complexity are defined by\nmultiple layers. The layers are a logical artifact, and are not related to the deployment of the service.\nThey exist to help developers manage the complexity in the code. Different layers (like the domain\nmodel layer versus the presentation layer, etc.) might have different types, which mandate translations\nbetween those types.', 'For example, an entity could be loaded from the database. Then part of that information, or an\naggregation of information including additional data from other entities, can be sent to the client UI\nthrough a REST Web API. The point here is that the domain entity is contained within the domain\nmodel layer and should not be propagated to other areas that it does not belong to, like to the\npresentation layer.', 'Additionally, you need to have always-valid entities (see the Designing validations in the domain\nmodel layer section) controlled by aggregate roots (root entities). Therefore, entities should not be\nbound to client views, because at the UI level some data might still not be validated. This reason is\nwhat the ViewModel is for. The ViewModel is a data model exclusively for presentation layer needs.\nThe domain entities do not belong directly to the ViewModel. Instead, you need to translate between\nViewModels and domain entities and vice versa.', 'When tackling complexity, it is important to have a domain model controlled by aggregate roots that\nmake sure that all the invariants and rules related to that group of entities (aggregate) are performed\nthrough a single entry-point or gate, the aggregate root.', 'Figure 7-5 shows how a layered design is implemented in the eShopOnContainers application.', '195 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-5. DDD layers in the ordering microservice in eShopOnContainers_', 'The three layers in a DDD microservice like Ordering. Each layer is a VS project: Application layer is\nOrdering.API, Domain layer is Ordering.Domain and the Infrastructure layer is Ordering.Infrastructure.\nYou want to design the system so that each layer communicates only with certain other layers. That\napproach may be easier to enforce if layers are implemented as different class libraries, because you\ncan clearly identify what dependencies are set between libraries. For instance, the domain model layer\nshould not take a dependency on any other layer (the domain model classes should be Plain Old Class\n[Objects, or POCO, classes). As shown in Figure 7-6, the](https://docs.microsoft.com/dotnet/standard/glossary#poco) **Ordering.Domain** layer library has\ndependencies only on the .NET libraries or NuGet packages, but not on any other custom library, such\nas data library or persistence library.', '_Figure 7-6. Layers implemented as libraries allow better control of dependencies between layers_', '**The domain model layer**', '[Eric Evans’s excellent book Domain Driven Design](https://domainlanguage.com/ddd/) says the following about the domain model layer\nand the application layer.', '**Domain Model Layer** : Responsible for representing concepts of the business, information about the\nbusiness situation, and business rules. State that reflects the business situation is controlled and used\nhere, even though the technical details of storing it are delegated to the infrastructure. This layer is\nthe heart of business software.', '196 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The domain model layer is where the business is expressed. When you implement a microservice\ndomain model layer in .NET, that layer is coded as a class library with the domain entities that capture\ndata plus behavior (methods with logic).', '[Following the Persistence Ignorance and the Infrastructure Ignorance](https://deviq.com/persistence-ignorance/) principles, this layer must\ncompletely ignore data persistence details. These persistence tasks should be performed by the\ninfrastructure layer. Therefore, this layer should not take direct dependencies on the infrastructure,\nwhich means that an important rule is that your domain model entity classes should be POCOs.', 'Domain entities should not have any direct dependency (like deriving from a base class) on any data\naccess infrastructure framework like Entity Framework or NHibernate. Ideally, your domain entities\nshould not derive from or implement any type defined in any infrastructure framework.', 'Most modern ORM frameworks like Entity Framework Core allow this approach, so that your domain\nmodel classes are not coupled to the infrastructure. However, having POCO entities is not always\npossible when using certain NoSQL databases and frameworks, like Actors and Reliable Collections in\nAzure Service Fabric.', 'Even when it is important to follow the Persistence Ignorance principle for your Domain model, you\nshould not ignore persistence concerns. It is still important to understand the physical data model and\nhow it maps to your entity object model. Otherwise you can create impossible designs.', 'Also, this aspect does not mean you can take a model designed for a relational database and directly\nmove it to a NoSQL or document-oriented database. In some entity models, the model might fit, but\nusually it does not. There are still constraints that your entity model must adhere to, based both on\nthe storage technology and ORM technology.', '**The application layer**', '[Moving on to the application layer, we can again cite Eric Evans’s book Domain Driven Design:](https://domainlanguage.com/ddd/)', '**Application Layer:** Defines the jobs the software is supposed to do and directs the expressive domain\nobjects to work out problems. The tasks this layer is responsible for are meaningful to the business or\nnecessary for interaction with the application layers of other systems. This layer is kept thin. It does\nnot contain business rules or knowledge, but only coordinates tasks and delegates work to\ncollaborations of domain objects in the next layer down. It does not have state reflecting the business\nsituation, but it can have state that reflects the progress of a task for the user or the program.', 'A microservice’s application layer in .NET is commonly coded as an ASP.NET Core Web API project.\nThe project implements the microservice’s interaction, remote network access, and the external Web\nAPIs used from the UI or client apps. It includes queries if using a CQRS approach, commands\naccepted by the microservice, and even the event-driven communication between microservices\n(integration events). The ASP.NET Core Web API that represents the application layer must not contain\nbusiness rules or domain knowledge (especially domain rules for transactions or updates); these\nshould be owned by the domain model class library. The application layer must only coordinate tasks\nand must not hold or define any domain state (domain model). It delegates the execution of business\nrules to the domain model classes themselves (aggregate roots and domain entities), which will\nultimately update the data within those domain entities.', '197 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Basically, the application logic is where you implement all use cases that depend on a given front end.\nFor example, the implementation related to a Web API service.', 'The goal is that the domain logic in the domain model layer, its invariants, the data model, and\nrelated business rules must be completely independent from the presentation and application layers.\nMost of all, the domain model layer must not directly depend on any infrastructure framework.', '**The infrastructure layer**', 'The infrastructure layer is how the data that is initially held in domain entities (in memory) is persisted\nin databases or another persistent store. An example is using Entity Framework Core code to\nimplement the Repository pattern classes that use a DBContext to persist data in a relational\ndatabase.', '[In accordance with the previously mentioned Persistence Ignorance](https://deviq.com/persistence-ignorance/) [and Infrastructure Ignorance](https://ayende.com/blog/3137/infrastructure-ignorance)\nprinciples, the infrastructure layer must not “contaminate” the domain model layer. You must keep the\ndomain model entity classes agnostic from the infrastructure that you use to persist data (EF or any\nother framework) by not taking hard dependencies on frameworks. Your domain model layer class\nlibrary should have only your domain code, just POCO entity classes implementing the heart of your\nsoftware and completely decoupled from infrastructure technologies.', 'Thus, your layers or class libraries and projects should ultimately depend on your domain model layer\n(library), not vice versa, as shown in Figure 7-7.', '_Figure 7-7. Dependencies between layers in DDD_', 'Dependencies in a DDD Service, the Application layer depends on Domain and Infrastructure, and\nInfrastructure depends on Domain, but Domain doesn’t depend on any layer. This layer design should\nbe independent for each microservice. As noted earlier, you can implement the most complex\nmicroservices following DDD patterns, while implementing simpler data-driven microservices (simple\nCRUD in a single layer) in a simpler way.', '198 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Additional resources**', '_Define one rich domain model for each business microservice or Bounded Context._', 'Your goal is to create a single cohesive domain model for each business microservice or Bounded\nContext (BC). Keep in mind, however, that a BC or business microservice could sometimes be\ncomposed of several physical services that share a single domain model. The domain model must\ncapture the rules, behavior, business language, and constraints of the single Bounded Context or\nbusiness microservice that it represents.', 'Entities represent domain objects and are primarily defined by their identity, continuity, and\npersistence over time, and not only by the attributes that comprise them. As Eric Evans says, “an\nobject primarily defined by its identity is called an Entity.” Entities are very important in the domain\nmodel, since they are the base for a model. Therefore, you should identify and design them carefully.', '_An entity’s identity can cross multiple microservices or Bounded Contexts._', 'The same identity (that is, the same Id value, although perhaps not the same domain entity) can be\nmodeled across multiple Bounded Contexts or microservices. However, that does not imply that the\nsame entity, with the same attributes and logic would be implemented in multiple Bounded Contexts.\nInstead, entities in each Bounded Context limit their attributes and behaviors to those required in that\nBounded Context’s domain.', 'For instance, the buyer entity might have most of a person’s attributes that are defined in the user\nentity in the profile or identity microservice, including the identity. But the buyer entity in the ordering\nmicroservice might have fewer attributes, because only certain buyer data is related to the order\nprocess. The context of each microservice or Bounded Context impacts its domain model.', '_Domain entities must implement behavior in addition to implementing data attributes._', 'A domain entity in DDD must implement the domain logic or behavior related to the entity data (the\nobject accessed in memory). For example, as part of an order entity class you must have business logic\nand operations implemented as methods for tasks such as adding an order item, data validation, and\ntotal calculation. The entity’s methods take care of the invariants and rules of the entity instead of\nhaving those rules spread across the application layer.', '199 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Figure 7-8 shows a domain entity that implements not only data attributes but operations or methods\nwith related domain logic.', '_Figure 7-8. Example of a domain entity design implementing data plus behavior_', 'A domain model entity implements behaviors through methods, that is, it’s not an “anemic” model. Of\ncourse, sometimes you can have entities that do not implement any logic as part of the entity class.\nThis can happen in child entities within an aggregate if the child entity does not have any special logic\nbecause most of the logic is defined in the aggregate root. If you have a complex microservice that\nhas logic implemented in the service classes instead of in the domain entities, you could be falling\ninto the anemic domain model, explained in the following section.', '**Rich domain model versus anemic domain model**', '[In his post AnemicDomainModel, Martin Fowler describes an anemic domain model this way:](https://martinfowler.com/bliki/AnemicDomainModel.html)', 'The basic symptom of an Anemic Domain Model is that at first blush it looks like the real thing. There\nare objects, many named after the nouns in the domain space, and these objects are connected with\nthe rich relationships and structure that true domain models have. The catch comes when you look at\nthe behavior, and you realize that there is hardly any behavior on these objects, making them little\nmore than bags of getters and setters.', 'Of course, when you use an anemic domain model, those data models will be used from a set of\nservice objects (traditionally named the _business layer_ ) which capture all the domain or business logic.\nThe business layer sits on top of the data model and uses the data model just as data.', 'The anemic domain model is just a procedural style design. Anemic entity objects are not real objects\nbecause they lack behavior (methods). They only hold data properties and thus it is not objectoriented design. By putting all the behavior out into service objects (the business layer), you\n[essentially end up with spaghetti code or transaction scripts, and therefore you lose the advantages](https://en.wikipedia.org/wiki/Spaghetti_code)\nthat a domain model provides.', 'Regardless, if your microservice or Bounded Context is very simple (a CRUD service), the anemic\ndomain model in the form of entity objects with just data properties might be good enough, and it\nmight not be worth implementing more complex DDD patterns. In that case, it will be simply a\npersistence model, because you have intentionally created an entity with only data for CRUD\npurposes.', '200 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'That is why microservices architectures are perfect for a multi-architectural approach depending on\neach Bounded Context. For instance, in eShopOnContainers, the ordering microservice implements\nDDD patterns, but the catalog microservice, which is a simple CRUD service, does not.', 'Some people say that the anemic domain model is an anti-pattern. It really depends on what you are\nimplementing. If the microservice you are creating is simple enough (for example, a CRUD service),\nfollowing the anemic domain model it is not an anti-pattern. However, if you need to tackle the\ncomplexity of a microservice’s domain that has a lot of ever-changing business rules, the anemic\ndomain model might be an anti-pattern for that microservice or Bounded Context. In that case,\ndesigning it as a rich model with entities containing data plus behavior as well as implementing\nadditional DDD patterns (aggregates, value objects, etc.) might have huge benefits for the long-term\nsuccess of such a microservice.', '**Additional resources**', 'In the previous section, the fundamental design principles and patterns for designing a domain model\nwere explained. Now it’s time to explore possible ways to implement the domain model by using .NET\n(plain C# code) and EF Core. Your domain model will be composed simply of your code. It will have\njust the EF Core model requirements, but not real dependencies on EF. You shouldn’t have hard\ndependencies or references to EF Core or any other ORM in your domain model.', 'The folder organization used for the eShopOnContainers reference application demonstrates the DDD\nmodel for the application. You might find that a different folder organization more clearly\ncommunicates the design choices made for your application. As you can see in Figure 7-10, in the\nordering domain model there are two aggregates, the order aggregate and the buyer aggregate. Each\naggregate is a group of domain entities and value objects, although you could have an aggregate\ncomposed of a single domain entity (the aggregate root or root entity) as well.', '204 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-10. Domain model structure for the ordering microservice in eShopOnContainers_', 'Additionally, the domain model layer includes the repository contracts (interfaces) that are the\ninfrastructure requirements of your domain model. In other words, these interfaces express what\nrepositories and the methods the infrastructure layer must implement. It’s critical that the\nimplementation of the repositories be placed outside of the domain model layer, in the infrastructure\nlayer library, so the domain model layer isn’t “contaminated” by API or classes from infrastructure\ntechnologies, like Entity Framework.', '[You can also see a SeedWork folder that contains custom base classes that you can use as a base for](https://martinfowler.com/bliki/Seedwork.html)\nyour domain entities and value objects, so you don’t have redundant code in each domain’s object\nclass.', 'An aggregate refers to a cluster of domain objects grouped together to match transactional\nconsistency. Those objects could be instances of entities (one of which is the aggregate root or root\nentity) plus any additional value objects.', 'Transactional consistency means that an aggregate is guaranteed to be consistent and up to date at\nthe end of a business action. For example, the order aggregate from the eShopOnContainers ordering\nmicroservice domain model is composed as shown in Figure 7-11.', '205 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-11. The order aggregate in Visual Studio solution_', 'If you open any of the files in an aggregate folder, you can see how it’s marked as either a custom\n[base class or interface, like entity or value object, as implemented in the SeedWork folder.](https://github.com/dotnet-architecture/eShopOnContainers/tree/main/src/Services/Ordering/Ordering.Domain/SeedWork)', 'You implement a domain model in .NET by creating POCO classes that implement your domain\nentities. In the following example, the Order class is defined as an entity and also as an aggregate\nroot. Because the Order class derives from the Entity base class, it can reuse common code related to\nentities. Bear in mind that these base classes and interfaces are defined by you in the domain model\nproject, so it is your code, not infrastructure code from an ORM like EF.', '206 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'It’s important to note that this is a domain entity implemented as a POCO class. It doesn’t have any\ndirect dependency on Entity Framework Core or any other infrastructure framework. This\nimplementation is as it should be in DDD, just C# code implementing a domain model.', 'In addition, the class is decorated with an interface named IAggregateRoot. That interface is an empty\ninterface, sometimes called a _marker interface_, that’s used just to indicate that this entity class is also\nan aggregate root.', 'A marker interface is sometimes considered as an anti-pattern; however, it’s also a clean way to mark\na class, especially when that interface might be evolving. An attribute could be the other choice for\nthe marker, but it’s quicker to see the base class (Entity) next to the IAggregate interface instead of\nputting an Aggregate attribute marker above the class. It’s a matter of preferences, in any case.', 'Having an aggregate root means that most of the code related to consistency and business rules of\nthe aggregate’s entities should be implemented as methods in the Order aggregate root class (for\nexample, AddOrderItem when adding an OrderItem object to the aggregate). You should not create\nor update OrderItems objects independently or directly; the AggregateRoot class must keep control\nand consistency of any update operation against its child entities.', 'A common problem in entity models is that they expose collection navigation properties as publicly\naccessible list types. This allows any collaborator developer to manipulate the contents of these\ncollection types, which may bypass important business rules related to the collection, possibly leaving\nthe object in an invalid state. The solution to this is to expose read-only access to related collections\nand explicitly provide methods that define ways in which clients can manipulate them.', 'In the previous code, note that many attributes are read-only or private and are only updatable by the\nclass methods, so any update considers business domain invariants and logic specified within the class\nmethods.', '207 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'For example, following DDD patterns, **you should** _**not**_ **do the following** from any command handler\nmethod or application layer class (actually, it should be impossible for you to do so):', 'In this case, the Add method is purely an operation to add data, with direct access to the OrderItems\ncollection. Therefore, most of the domain logic, rules, or validations related to that operation with the\nchild entities will be spread across the application layer (command handlers and Web API controllers).', 'If you go around the aggregate root, the aggregate root cannot guarantee its invariants, its validity, or\nits consistency. Eventually you’ll have spaghetti code or transactional script code.', 'To follow DDD patterns, entities must not have public setters in any entity property. Changes in an\nentity should be driven by explicit methods with explicit ubiquitous language about the change\nthey’re performing in the entity.', 'Furthermore, collections within the entity (like the order items) should be read-only properties (the\nAsReadOnly method explained later). You should be able to update it only from within the aggregate\nroot class methods or the child entity methods.', 'As you can see in the code for the Order aggregate root, all setters should be private or at least readonly externally, so that any operation against the entity’s data or its child entities has to be performed\nthrough methods in the entity class. This maintains consistency in a controlled and object-oriented\nway instead of implementing transactional script code.', 'The following code snippet shows the proper way to code the task of adding an OrderItem object to\nthe Order aggregate.', 'In this snippet, most of the validations or logic related to the creation of an OrderItem object will be\nunder the control of the Order aggregate root—in the AddOrderItem method—especially validations\nand logic related to other elements in the aggregate. For instance, you might get the same product\nitem as the result of multiple calls to AddOrderItem. In that method, you could examine the product\nitems and consolidate the same product items into a single OrderItem object with several units.', '208 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Additionally, if there are different discount amounts but the product ID is the same, you would likely\napply the higher discount. This principle applies to any other domain logic for the OrderItem object.', 'In addition, the new OrderItem(params) operation will also be controlled and performed by the\nAddOrderItem method from the Order aggregate root. Therefore, most of the logic or validations\nrelated to that operation (especially anything that impacts the consistency between other child\nentities) will be in a single place within the aggregate root. That is the ultimate purpose of the\naggregate root pattern.', 'When you use Entity Framework Core 1.1 or later, a DDD entity can be better expressed because it\n[allows mapping to fields](https://docs.microsoft.com/ef/core/modeling/backing-field) in addition to properties. This is useful when protecting collections of child\nentities or value objects. With this enhancement, you can use simple private fields instead of\nproperties and you can implement any update to the field collection in public methods and provide\nread-only access through the AsReadOnly method.', 'In DDD, you want to update the entity only through methods in the entity (or the constructor) in order\nto control any invariant and the consistency of the data, so properties are defined only with a get\naccessor. The properties are backed by private fields. Private members can only be accessed from\nwithin the class. However, there is one exception: EF Core needs to set these fields as well (so it can\nreturn the object with the proper values).', '**Map properties with only get accessors to the fields in the database table**', 'Mapping properties to database table columns is not a domain responsibility but part of the\ninfrastructure and persistence layer. We mention this here just so you’re aware of the new capabilities\nin EF Core 1.1 or later related to how you can model entities. Additional details on this topic are\nexplained in the infrastructure and persistence section.', 'When you use EF Core 1.0 or later, within the DbContext you need to map the properties that are\ndefined only with getters to the actual fields in the database table. This is done with the HasField\nmethod of the PropertyBuilder class.', '**Map fields without properties**', 'With the feature in EF Core 1.1 or later to map columns to fields, it’s also possible to not use\nproperties. Instead, you can just map columns from a table to fields. A common use case for this is\nprivate fields for an internal state that doesn’t need to be accessed from outside the entity.', 'For example, in the preceding OrderAggregate code example, there are several private fields, like the\n_paymentMethodId field, that have no related property for either a setter or getter. That field could\nalso be calculated within the order’s business logic and used from the order’s methods, but it needs\nto be persisted in the database as well. So in EF Core (since v1.1), there’s a way to map a field without\na related property to a column in the database. This is also explained in the Infrastructure layer section\nof this guide.', '209 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Additional resources**', 'The solution folder contains a _SeedWork_ folder. This folder contains custom base classes that you can\nuse as a base for your domain entities and value objects. Use these base classes so you don’t have\nredundant code in each domain’s object class. The folder for these types of classes is called _SeedWork_\nand not something like _Framework_ . It’s called _SeedWork_ because the folder contains just a small\nsubset of reusable classes that cannot really be considered a framework. _Seedwork_ is a term\n[introduced by Michael Feathers and popularized by Martin Fowler](https://www.artima.com/forums/flat.jsp?forum=106&thread=8826) but you could also name that\nfolder Common, SharedKernel, or similar.', 'Figure 7-12 shows the classes that form the seedwork of the domain model in the ordering\nmicroservice. It has a few custom base classes like Entity, ValueObject, and Enumeration, plus a few\ninterfaces. These interfaces (IRepository and IUnitOfWork) inform the infrastructure layer about what\nneeds to be implemented. Those interfaces are also used through Dependency Injection from the\napplication layer.', '_Figure 7-12. A sample set of domain model “seedwork” base classes and interfaces_', 'This is the type of copy and paste reuse that many developers share between projects, not a formal\nframework. You can have seedworks in any layer or library. However, if the set of classes and\ninterfaces gets large enough, you might want to create a single class library.', '210 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The following code is an example of an Entity base class where you can place code that can be used\n[the same way by any domain entity, such as the entity ID, equality operators, a domain event list per](https://docs.microsoft.com/dotnet/csharp/language-reference/operators/equality-operators)\nentity, etc.', '211 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The previous code using a domain event list per entity will be explained in the next sections when\nfocusing on domain events.', 'Repository contracts are simply .NET interfaces that express the contract requirements of the\nrepositories to be used for each aggregate.', 'The repositories themselves, with EF Core code or any other infrastructure dependencies and code\n(Linq, SQL, etc.), must not be implemented within the domain model; the repositories should only\nimplement the interfaces you define in the domain model.', 'A pattern related to this practice (placing the repository interfaces in the domain model layer) is the\n[Separated Interface pattern. As explained by Martin Fowler, “Use Separated Interface to define an](https://www.martinfowler.com/eaaCatalog/separatedInterface.html)\ninterface in one package but implement it in another. This way a client that needs the dependency to\nthe interface can be completely unaware of the implementation.”', 'Following the Separated Interface pattern enables the application layer (in this case, the Web API\nproject for the microservice) to have a dependency on the requirements defined in the domain model,\nbut not a direct dependency to the infrastructure/persistence layer. In addition, you can use\nDependency Injection to isolate the implementation, which is implemented in the infrastructure/\npersistence layer using repositories.', 'For example, the following example with the IOrderRepository interface defines what operations the\nOrderRepository class will need to implement at the infrastructure layer. In the current\nimplementation of the application, the code just needs to add or update orders to the database, since\nqueries are split following the simplified CQRS approach.', '212 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'As discussed in earlier sections about entities and aggregates, identity is fundamental for entities.\nHowever, there are many objects and data items in a system that do not require an identity and\nidentity tracking, such as value objects.', 'A value object can reference other entities. For example, in an application that generates a route that\ndescribes how to get from one point to another, that route would be a value object. It would be a\nsnapshot of points on a specific route, but this suggested route would not have an identity, even\nthough internally it might refer to entities like City, Road, etc.', 'Figure 7-13 shows the Address value object within the Order aggregate.', '213 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-13. Address value object within the Order aggregate_', 'As shown in Figure 7-13, an entity is usually composed of multiple attributes. For example, the Order\nentity can be modeled as an entity with an identity and composed internally of a set of attributes such\nas OrderId, OrderDate, OrderItems, etc. But the address, which is simply a complex-value composed of\ncountry/region, street, city, etc., and has no identity in this domain, must be modeled and treated as a\nvalue object.', 'There are two main characteristics for value objects:', 'In terms of implementation, you can have a value object base class that has basic utility methods like\nequality based on the comparison between all the attributes (since a value object must not be based\non identity) and other fundamental characteristics. The following example shows a value object base\nclass used in the ordering microservice from eShopOnContainers.', '215 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The ValueObject is an abstract class type, but in this example, it doesn’t overload the == and !=\noperators. You could choose to do so, making comparisons delegate to the Equals override. For\nexample, consider the following operator overloads to the ValueObject type:', 'This value object implementation of Address has no identity, and therefore no ID field is defined for it,\neither in the Address class definition or the ValueObject class definition.', 'Having no ID field in a class to be used by Entity Framework (EF) was not possible until EF Core 2.0,\nwhich greatly helps to implement better value objects with no ID. That is precisely the explanation of\nthe next section.', 'It could be argued that value objects, being immutable, should be read-only (that is, have get-only\nproperties), and that’s indeed true. However, value objects are usually serialized and deserialized to go', '216 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'through message queues, and being read-only stops the deserializer from assigning values, so you\njust leave them as private set, which is read-only enough to be practical.', '**Value object comparison semantics**', 'Two instances of the Address type can be compared using all the following methods:', 'When all the values are the same, the comparisons are correctly evaluated as true. If you didn’t choose\nto overload the == and != operators, then the last comparison of one == two would evaluate as false.\nFor more information, see Overload ValueObject equality operators.', 'You just saw how to define a value object in your domain model. But how can you actually persist it\ninto the database using Entity Framework Core since it usually targets entities with identity?', '**Background and older approaches using EF Core 1.1**', '[As background, a limitation when using EF Core 1.0 and 1.1 was that you could not use complex types](https://docs.microsoft.com/dotnet/api/system.componentmodel.dataannotations.schema.complextypeattribute)\nas defined in EF 6.x in the traditional .NET Framework. Therefore, if using EF Core 1.0 or 1.1, you\nneeded to store your value object as an EF entity with an ID field. Then, so it looked more like a value\nobject with no identity, you could hide its ID so you make clear that the identity of a value object is\n[not important in the domain model. You could hide that ID by using the ID as a shadow property.](https://docs.microsoft.com/ef/core/modeling/shadow-properties)\nSince that configuration for hiding the ID in the model is set up in the EF infrastructure level, it would\nbe kind of transparent for your domain model.', 'In the initial version of eShopOnContainers (.NET Core 1.1), the hidden ID needed by EF Core\ninfrastructure was implemented in the following way in the DbContext level, using Fluent API at the\ninfrastructure project. Therefore, the ID was hidden from the domain model point of view, but still\npresent in the infrastructure.', 'However, the persistence of that value object into the database was performed like a regular entity in\na different table.', '217 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'With EF Core 2.0 and later, there are new and better ways to persist value objects.', 'Even with some gaps between the canonical value object pattern in DDD and the owned entity type in\nEF Core, it’s currently the best way to persist value objects with EF Core 2.0 and later. You can see\nlimitations at the end of this section.', 'The owned entity type feature was added to EF Core since version 2.0.', 'An owned entity type allows you to map types that do not have their own identity explicitly defined in\nthe domain model and are used as properties, such as a value object, within any of your entities. An\nowned entity type shares the same CLR type with another entity type (that is, it’s just a regular class).\nThe entity containing the defining navigation is the owner entity. When querying the owner, the\nowned types are included by default.', 'Just by looking at the domain model, an owned type looks like it doesn’t have any identity. However,\nunder the covers, owned types do have the identity, but the owner navigation property is part of this\nidentity.', 'The identity of instances of owned types is not completely their own. It consists of three components:', '[Enumerations](https://docs.microsoft.com/dotnet/csharp/language-reference/builtin-types/enum) (or _enum types_ for short) are a thin language wrapper around an integral type. You\nmight want to limit their use to when you are storing one value from a closed set of values.\nClassification based on sizes (small, medium, large) is a good example. Using enums for control flow\n[or more robust abstractions can be a code smell. This type of usage leads to fragile code with many](https://deviq.com/antipatterns/code-smells)\ncontrol flow statements checking values of the enum.', '221 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Instead, you can create Enumeration classes that enable all the rich features of an object-oriented\nlanguage.', '[However, this isn’t a critical topic and in many cases, for simplicity, you can still use regular enum](https://docs.microsoft.com/dotnet/csharp/language-reference/builtin-types/enum)\n[types](https://docs.microsoft.com/dotnet/csharp/language-reference/builtin-types/enum) if that’s your preference. The use of enumeration classes is more related to business-related\nconcepts.', 'The ordering microservice in eShopOnContainers provides a sample Enumeration base class\nimplementation, as shown in the following example:', '222 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'In DDD, validation rules can be thought as invariants. The main responsibility of an aggregate is to\nenforce invariants across state changes for all the entities within that aggregate.', 'Domain entities should always be valid entities. There are a certain number of invariants for an object\nthat should always be true. For example, an order item object always has to have a quantity that must\nbe a positive integer, plus an article name and price. Therefore, invariants enforcement is the\nresponsibility of the domain entities (especially of the aggregate root) and an entity object should not\nbe able to exist without being valid. Invariant rules are simply expressed as contracts, and exceptions\nor notifications are raised when they are violated.', 'The reasoning behind this is that many bugs occur because objects are in a state they should never\nhave been in.', 'Let’s propose we now have a SendUserCreationEmailService that takes a UserProfile … how can we\nrationalize in that service that Name is not null? Do we check it again? Or more likely … you just don’t\nbother to check and “hope for the best”—you hope that someone bothered to validate it before\nsending it to you. Of course, using TDD one of the first tests we should be writing is that if I send a\ncustomer with a null name that it should raise an error. But once we start writing these kinds of tests\nover and over again we realize … “what if we never allowed name to become null? we wouldn’t have\nall of these tests!”.', '223 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Validations are usually implemented in domain entity constructors or in methods that can update the\nentity. There are multiple ways to implement validations, such as verifying data and raising exceptions\nif the validation fails. There are also more advanced patterns such as using the Specification pattern\nfor validations, and the Notification pattern to return a collection of errors instead of returning an\nexception for each validation as it occurs.', '**Validate conditions and throw exceptions**', 'The following code example shows the simplest approach to validation in a domain entity by raising\nan exception. In the references table at the end of this section you can see links to more advanced\nimplementations based on the patterns we have discussed previously.', 'If the value of the state is invalid, the first address line and the city have already been changed. That\nmight make the address invalid.', 'A similar approach can be used in the entity’s constructor, raising an exception to make sure that the\nentity is valid once it is created.', '**Use validation attributes in the model based on data annotations**', 'Data annotations, like the Required or MaxLength attributes, can be used to configure EF Core\ndatabase field properties, as explained in detail in the Table mapping [section, but they no longer work](https://github.com/dotnet/efcore/issues/3680)\n[for entity validation in EF Core (neither does the IValidatableObject.Validate](https://github.com/dotnet/efcore/issues/3680) method), as they have\ndone since EF 4.x in .NET Framework.', '[Data annotations and the IValidatableObject](https://docs.microsoft.com/dotnet/api/system.componentmodel.dataannotations.ivalidatableobject) interface can still be used for model validation during\nmodel binding, prior to the controller’s actions invocation as usual, but that model is meant to be a\nViewModel or DTO and that’s an MVC or API concern not a domain model concern.', 'Having made the conceptual difference clear, you can still use data annotations and\nIValidatableObject in the entity class for validation, if your actions receive an entity class object\nparameter, which is not recommended. In that case, validation will occur upon model binding, just\nbefore invoking the action and you can check the controller’s ModelState.IsValid property to check', '224 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'the result, but then again, it happens in the controller, not before persisting the entity object in the\nDbContext, as it had done since EF 4.x.', 'You can still implement custom validation in the entity class using data annotations and the\nIValidatableObject.Validate method, by overriding the DbContext’s SaveChanges method.', '[You can see a sample implementation for validating IValidatableObject entities in this comment on](https://github.com/dotnet/efcore/issues/3680#issuecomment-155502539)\n[GitHub. That sample doesn’t do attribute-based validations, but they should be easy to implement](https://github.com/dotnet/efcore/issues/3680#issuecomment-155502539)\nusing reflection in the same override.', 'However, from a DDD point of view, the domain model is best kept lean with the use of exceptions in\nyour entity’s behavior methods, or by implementing the Specification and Notification patterns to\nenforce validation rules.', 'It can make sense to use data annotations at the application layer in ViewModel classes (instead of\ndomain entities) that will accept input, to allow for model validation within the UI layer. However, this\nshould not be done at the exclusion of validation within the domain model.', '**Validate entities by implementing the Specification pattern and the Notification**\n**pattern**', 'Finally, a more elaborate approach to implementing validations in the domain model is by\nimplementing the Specification pattern in conjunction with the Notification pattern, as explained in\nsome of the additional resources listed later.', 'It is worth mentioning that you can also use just one of those patterns—for example, validating\nmanually with control statements, but using the Notification pattern to stack and return a list of\nvalidation errors.', '**Use deferred validation in the domain**', 'There are various approaches to deal with deferred validations in the domain. In his book\n[Implementing Domain-Driven Design, Vaughn Vernon discusses these in the section on validation.](https://www.amazon.com/Implementing-Domain-Driven-Design-Vaughn-Vernon/dp/0321834577)', '**Two-step validation**', 'Also consider two-step validation. Use field-level validation on your command Data Transfer Objects\n(DTOs) and domain-level validation inside your entities. You can do this by returning a result object\ninstead of exceptions in order to make it easier to deal with the validation errors.', 'Using field validation with data annotations, for example, you do not duplicate the validation\ndefinition. The execution, though, can be both server-side and client-side in the case of DTOs\n(commands and ViewModels, for instance).', 'Even when the source of truth is the domain model and ultimately you must have validation at the\ndomain model level, validation can still be handled at both the domain model level (server side) and\nthe UI (client side).', 'Client-side validation is a great convenience for users. It saves time they would otherwise spend\nwaiting for a round trip to the server that might return validation errors. In business terms, even a few\nfractions of seconds multiplied hundreds of times each day adds up to a lot of time, expense, and\nfrustration. Straightforward and immediate validation enables users to work more efficiently and\nproduce better quality input and output.', 'Just as the view model and the domain model are different, view model validation and domain model\nvalidation might be similar but serve a different purpose. If you are concerned about DRY (the Don’t\nRepeat Yourself principle), consider that in this case code reuse might also mean coupling, and in\nenterprise applications it is more important not to couple the server side to the client side than to\nfollow the DRY principle.', 'Even when using client-side validation, you should always validate your commands or input DTOs in\nserver code, because the server APIs are a possible attack vector. Usually, doing both is your best bet\nbecause if you have a client application, from a UX perspective, it is best to be proactive and not allow\nthe user to enter invalid information.', 'Therefore, in client-side code you typically validate the ViewModels. You could also validate the client\noutput DTOs or commands before you send them to the services.', 'The implementation of client-side validation depends on what kind of client application you are\nbuilding. It will be different if you are validating data in a web MVC web application with most of the\ncode in .NET, a SPA web application with that validation being coded in JavaScript or TypeScript, or a\nmobile app coded with Xamarin and C#.', '226 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Validation in Xamarin mobile apps**', 'Use domain events to explicitly implement side effects of changes within your domain. In other words,\nand using DDD terminology, use domain events to explicitly implement side effects across multiple\naggregates. Optionally, for better scalability and less impact in database locks, use eventual\nconsistency between aggregates within the same domain.', '227 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'An event is something that has happened in the past. A domain event is, something that happened in\nthe domain that you want other parts of the same domain (in-process) to be aware of. The notified\nparts usually react somehow to the events.', 'An important benefit of domain events is that side effects can be expressed explicitly.', 'For example, if you’re just using Entity Framework and there has to be a reaction to some event, you\nwould probably code whatever you need close to what triggers the event. So the rule gets coupled,\nimplicitly, to the code, and you have to look into the code to, hopefully, realize the rule is\nimplemented there.', 'On the other hand, using domain events makes the concept explicit, because there’s a DomainEvent\nand at least one DomainEventHandler involved.', 'For example, in the eShopOnContainers application, when an order is created, the user becomes a\nbuyer, so an OrderStartedDomainEvent is raised and handled in the\nValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler, so the underlying concept is\nevident.', 'In short, domain events help you to express, explicitly, the domain rules, based in the ubiquitous\nlanguage provided by the domain experts. Domain events also enable a better separation of concerns\namong classes within the same domain.', 'It’s important to ensure that, just like a database transaction, either all the operations related to a\ndomain event finish successfully or none of them do.', 'Domain events are similar to messaging-style events, with one important difference. With real\nmessaging, message queuing, message brokers, or a service bus using AMQP, a message is always\nsent asynchronously and communicated across processes and machines. This is useful for integrating\nmultiple Bounded Contexts, microservices, or even different applications. However, with domain\nevents, you want to raise an event from the domain operation you’re currently running, but you want\nany side effects to occur within the same domain.', 'The domain events and their side effects (the actions triggered afterwards that are managed by event\nhandlers) should occur almost immediately, usually in-process, and within the same domain. Thus,\ndomain events could be synchronous or asynchronous. Integration events, however, should always be\nasynchronous.', 'Semantically, domain and integration events are the same thing: notifications about something that\njust happened. However, their implementation must be different. Domain events are just messages\npushed to a domain event dispatcher, which could be implemented as an in-memory mediator based\non an IoC container or any other method.', 'On the other hand, the purpose of integration events is to propagate committed transactions and\nupdates to additional subsystems, whether they are other microservices, Bounded Contexts or even', '228 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'external applications. Hence, they should occur only if the entity is successfully persisted, otherwise it’s\nas if the entire operation never happened.', 'As mentioned before, integration events must be based on asynchronous communication between\nmultiple microservices (other Bounded Contexts) or even external systems/applications.', 'Thus, the event bus interface needs some infrastructure that allows inter-process and distributed\ncommunication between potentially remote services. It can be based on a commercial service bus,\nqueues, a shared database used as a mailbox, or any other distributed and ideally push based\nmessaging system.', 'If executing a command related to one aggregate instance requires additional domain rules to be run\non one or more additional aggregates, you should design and implement those side effects to be\ntriggered by domain events. As shown in Figure 7-14, and as one of the most important use cases, a\ndomain event should be used to propagate state changes across multiple aggregates within the same\ndomain model.', '_Figure 7-14. Domain events to enforce consistency between multiple aggregates within the same domain_', 'Figure 7-14 shows how consistency between aggregates is achieved by domain events. When the user\ninitiates an order, the Order Aggregate sends an OrderStarted domain event. The OrderStarted\ndomain event is handled by the Buyer Aggregate to create a Buyer object in the ordering\nmicroservice, based on the original user info from the identity microservice (with information provided\nin the CreateOrder command).', 'Alternately, you can have the aggregate root subscribed for events raised by members of its\naggregates (child entities). For instance, each OrderItem child entity can raise an event when the item\nprice is higher than a specific amount, or when the product item amount is too high. The aggregate\nroot can then receive those events and perform a global calculation or aggregation.', '229 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'It’s important to understand that this event-based communication is not implemented directly within\nthe aggregates; you need to implement domain event handlers.', 'Handling the domain events is an application concern. The domain model layer should only focus on\nthe domain logic—things that a domain expert would understand, not application infrastructure like\nhandlers and side-effect persistence actions using repositories. Therefore, the application layer level is\nwhere you should have domain event handlers triggering actions when a domain event is raised.', 'Domain events can also be used to trigger any number of application actions, and what is more\nimportant, must be open to increase that number in the future in a decoupled way. For instance, when\nthe order is started, you might want to publish a domain event to propagate that info to other\naggregates or even to raise application actions like notifications.', 'The key point is the open number of actions to be executed when a domain event occurs. Eventually,\nthe actions and rules in the domain and application will grow. The complexity or number of sideeffect actions when something happens will grow, but if your code were coupled with “glue” (that is,\ncreating specific objects with new), then every time you needed to add a new action you would also\nneed to change working and tested code.', '[This change could result in new bugs and this approach also goes against the Open/Closed principle](https://en.wikipedia.org/wiki/Open/closed_principle)\n[from SOLID. Not only that, the original class that was orchestrating the operations would grow and](https://en.wikipedia.org/wiki/SOLID)\n[grow, which goes against the Single Responsibility Principle (SRP).](https://en.wikipedia.org/wiki/Single_responsibility_principle)', 'On the other hand, if you use domain events, you can create a fine-grained and decoupled\nimplementation by segregating responsibilities using this approach:', 'In C#, a domain event is simply a data-holding structure or class, like a DTO, with all the information\nrelated to what just happened in the domain, as shown in the following example:', '231 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'This is essentially a class that holds all the data related to the OrderStarted event.', 'In terms of the ubiquitous language of the domain, since an event is something that happened in the\npast, the class name of the event should be represented as a past-tense verb, like\nOrderStartedDomainEvent or OrderShippedDomainEvent. That’s how the domain event is\nimplemented in the ordering microservice in eShopOnContainers.', 'As noted earlier, an important characteristic of events is that since an event is something that\nhappened in the past, it shouldn’t change. Therefore, it must be an immutable class. You can see in\nthe previous code that the properties are read-only. There’s no way to update the object, you can only\nset values when you create it.', 'It’s important to highlight here that if domain events were to be handled asynchronously, using a\nqueue that required serializing and deserializing the event objects, the properties would have to be\n“private set” instead of read-only, so the deserializer would be able to assign the values upon\ndequeuing. This is not an issue in the Ordering microservice, as the domain event pub/sub is\nimplemented synchronously using MediatR.', '**Raise domain events**', 'The next question is how to raise a domain event so it reaches its related event handlers. You can use\nmultiple approaches.', '[Udi Dahan originally proposed (for example, in several related posts, such as Domain Events – Take 2)](https://udidahan.com/2008/08/25/domain-events-take-2/)\nusing a static class for managing and raising the events. This might include a static class named\nDomainEvents that would raise domain events immediately when it’s called, using syntax like\n[DomainEvents.Raise(Event myEvent). Jimmy Bogard wrote a blog post (Strengthening your domain:](https://lostechies.com/jimmybogard/2010/04/08/strengthening-your-domain-domain-events/)\n[Domain Events) that recommends a similar approach.](https://lostechies.com/jimmybogard/2010/04/08/strengthening-your-domain-domain-events/)', 'However, when the domain events class is static, it also dispatches to handlers immediately. This\nmakes testing and debugging more difficult, because the event handlers with side-effects logic are\nexecuted immediately after the event is raised. When you’re testing and debugging, you just want to\nfocus on what is happening in the current aggregate classes; you don’t want to suddenly be', '232 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'redirected to other event handlers for side effects related to other aggregates or application logic.\nThis is why other approaches have evolved, as explained in the next section.', '**The deferred approach to raise and dispatch events**', 'Instead of dispatching to a domain event handler immediately, a better approach is to add the\ndomain events to a collection and then to dispatch those domain events _right before_ or _right_ _after_\ncommitting the transaction (as with SaveChanges in EF). (This approach was described by Jimmy\n[Bogard in this post A better domain events pattern.)](https://lostechies.com/jimmybogard/2014/05/13/a-better-domain-events-pattern/)', 'Deciding if you send the domain events right before or right after committing the transaction is\nimportant, since it determines whether you will include the side effects as part of the same transaction\nor in different transactions. In the latter case, you need to deal with eventual consistency across\nmultiple aggregates. This topic is discussed in the next section.', 'The deferred approach is what eShopOnContainers uses. First, you add the events happening in your\nentities into a collection or list of events per entity. That list should be part of the entity object, or\neven better, part of your base entity class, as shown in the following example of the Entity base class:', 'When you want to raise an event, you just add it to the event collection from code at any method of\nthe aggregate-root entity.', '[The following code, part of the Order aggregate-root at eShopOnContainers, shows an example:](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/AggregatesModel/OrderAggregate/Order.cs)', 'Notice that the only thing that the AddDomainEvent method is doing is adding an event to the list.\nNo event is dispatched yet, and no event handler is invoked yet.', '233 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'You actually want to dispatch the events later on, when you commit the transaction to the database. If\nyou are using Entity Framework Core, that means in the SaveChanges method of your EF DbContext,\nas in the following code:', 'With this code, you dispatch the entity events to their respective event handlers.', 'The overall result is that you’ve decoupled the raising of a domain event (a simple add into a list in\nmemory) from dispatching it to an event handler. In addition, depending on what kind of dispatcher\nyou are using, you could dispatch the events synchronously or asynchronously.', 'Be aware that transactional boundaries come into significant play here. If your unit of work and\ntransaction can span more than one aggregate (as when using EF Core and a relational database), this\ncan work well. But if the transaction cannot span aggregates, you have to implement additional steps\nto achieve consistency. This is another reason why persistence ignorance is not universal; it depends\non the storage system you use.', '**Single transaction across aggregates versus eventual consistency across**\n**aggregates**', 'The question of whether to perform a single transaction across aggregates versus relying on eventual\nconsistency across those aggregates is a controversial one. Many DDD authors like Eric Evans and\nVaughn Vernon advocate the rule that one transaction = one aggregate and therefore argue for\neventual consistency across aggregates. For example, in his book _Domain-Driven Design_, Eric Evans\nsays this:', 'Any rule that spans Aggregates will not be expected to be up-to-date at all times. Through event\nprocessing, batch processing, or other update mechanisms, other dependencies can be resolved\nwithin some specific time. (page 128)', '[Vaughn Vernon says the following in Effective Aggregate Design. Part II: Making Aggregates Work](https://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_2.pdf)\n[Together:](https://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_2.pdf)', '234 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Thus, if executing a command on one aggregate instance requires that additional business rules\nexecute on one or more aggregates, use eventual consistency […] There is a practical way to support\neventual consistency in a DDD model. An aggregate method publishes a domain event that is in time\ndelivered to one or more asynchronous subscribers.', 'This rationale is based on embracing fine-grained transactions instead of transactions spanning many\naggregates or entities. The idea is that in the second case, the number of database locks will be\nsubstantial in large-scale applications with high scalability needs. Embracing the fact that highly\nscalable applications need not have instant transactional consistency between multiple aggregates\nhelps with accepting the concept of eventual consistency. Atomic changes are often not needed by\nthe business, and it is in any case the responsibility of the domain experts to say whether particular\noperations need atomic transactions or not. If an operation always needs an atomic transaction\nbetween multiple aggregates, you might ask whether your aggregate should be larger or wasn’t\ncorrectly designed.', 'However, other developers and architects like Jimmy Bogard are okay with spanning a single\ntransaction across several aggregates—but only when those additional aggregates are related to side\n[effects for the same original command. For instance, in A better domain events pattern, Bogard says](https://lostechies.com/jimmybogard/2014/05/13/a-better-domain-events-pattern/)\nthis:', 'Typically, I want the side effects of a domain event to occur within the same logical transaction, but\nnot necessarily in the same scope of raising the domain event […] Just before we commit our\ntransaction, we dispatch our events to their respective handlers.', 'If you dispatch the domain events right _before_ committing the original transaction, it is because you\nwant the side effects of those events to be included in the same transaction. For example, if the EF\nDbContext SaveChanges method fails, the transaction will roll back all changes, including the result of\nany side effect operations implemented by the related domain event handlers. This is because the\nDbContext life scope is by default defined as “scoped.” Therefore, the DbContext object is shared\nacross multiple repository objects being instantiated within the same scope or object graph. This\ncoincides with the HttpRequest scope when developing Web API or MVC apps.', 'Actually, both approaches (single atomic transaction and eventual consistency) can be right. It really\ndepends on your domain or business requirements and what the domain experts tell you. It also\ndepends on how scalable you need the service to be (more granular transactions have less impact\nwith regard to database locks). And it depends on how much investment you’re willing to make in\nyour code, since eventual consistency requires more complex code in order to detect possible\ninconsistencies across aggregates and the need to implement compensatory actions. Consider that if\nyou commit changes to the original aggregate and afterwards, when the events are being dispatched,\nif there’s an issue and the event handlers cannot commit their side effects, you’ll have inconsistencies\nbetween aggregates.', 'A way to allow compensatory actions would be to store the domain events in additional database\ntables so they can be part of the original transaction. Afterwards, you could have a batch process that\ndetects inconsistencies and runs compensatory actions by comparing the list of events with the\ncurrent state of the aggregates. The compensatory actions are part of a complex topic that will require\ndeep analysis from your side, which includes discussing it with the business user and domain experts.', '235 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'In any case, you can choose the approach you need. But the initial deferred approach—raising the\nevents before committing, so you use a single transaction—is the simplest approach when using EF\nCore and a relational database. It’s easier to implement and valid in many business cases. It’s also the\napproach used in the ordering microservice in eShopOnContainers.', 'But how do you actually dispatch those events to their respective event handlers? What’s the\n_mediator object you see in the previous example? It has to do with the techniques and artifacts you\nuse to map between events and their event handlers.', '**The domain event dispatcher: mapping from events to event handlers**', 'Once you’re able to dispatch or publish the events, you need some kind of artifact that will publish the\nevent, so that every related handler can get it and process side effects based on that event.', 'One approach is a real messaging system or even an event bus, possibly based on a service bus as\nopposed to in-memory events. However, for the first case, real messaging would be overkill for\nprocessing domain events, since you just need to process those events within the same process (that\nis, within the same domain and application layer).', '**How to subscribe to domain events**', 'When you use MediatR, each event handler must use an event type that is provided on the generic\nparameter of the INotificationHandler interface, as you can see in the following code:', 'Based on the relationship between event and event handler, which can be considered the\nsubscription, the MediatR artifact can discover all the event handlers for each event and trigger each\none of those event handlers.', '**How to handle domain events**', 'Finally, the event handler usually implements application layer code that uses infrastructure\nrepositories to obtain the required additional aggregates and to execute side-effect domain logic. The\n[following domain event handler code at eShopOnContainers, shows an implementation example.](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/DomainEventHandlers/ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler.cs)', '236 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The previous domain event handler code is considered application layer code because it uses\ninfrastructure repositories, as explained in the next section on the infrastructure-persistence layer.\nEvent handlers could also use other infrastructure components.', '**Domain events can generate integration events to be published outside of the**\n**microservice boundaries**', 'Finally, it’s important to mention that you might sometimes want to propagate events across multiple\nmicroservices. That propagation is an integration event, and it could be published through an event\nbus from any specific domain event handler.', 'As stated, use domain events to explicitly implement side effects of changes within your domain. To\nuse DDD terminology, use domain events to explicitly implement side effects across one or multiple\naggregates. Additionally, and for better scalability and less impact on database locks, use eventual\nconsistency between aggregates within the same domain.', '237 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '[The reference app uses MediatR to propagate domain events synchronously across aggregates, within](https://github.com/jbogard/MediatR)\n[a single transaction. However, you could also use some AMQP implementation like RabbitMQ or](https://www.rabbitmq.com/)\n[Azure Service Bus](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview) to propagate domain events asynchronously, using eventual consistency but, as\nmentioned above, you have to consider the need for compensatory actions in case of failures.', 'Data persistence components provide access to the data hosted within the boundaries of a\nmicroservice (that is, a microservice’s database). They contain the actual implementation of\n[components such as repositories and Unit of Work](https://martinfowler.com/eaaCatalog/unitOfWork.html) classes, like custom Entity Framework (EF)\n[DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext) objects. EF DbContext implements both the Repository and the Unit of Work patterns.', 'The Repository pattern is a Domain-Driven Design pattern intended to keep persistence concerns\noutside of the system’s domain model. One or more persistence abstractions - interfaces - are defined', '238 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'in the domain model, and these abstractions have implementations in the form of persistence-specific\nadapters defined elsewhere in the application.', 'Repository implementations are classes that encapsulate the logic required to access data sources.\nThey centralize common data access functionality, providing better maintainability and decoupling\nthe infrastructure or technology used to access databases from the domain model. If you use an\nObject-Relational Mapper (ORM) like Entity Framework, the code that must be implemented is\nsimplified, thanks to LINQ and strong typing. This lets you focus on the data persistence logic rather\nthan on data access plumbing.', '[The Repository pattern is a well-documented way of working with a data source. In the book Patterns](https://www.amazon.com/Patterns-Enterprise-Application-Architecture-Martin/dp/0321127420/)\n[of Enterprise Application Architecture, Martin Fowler describes a repository as follows:](https://www.amazon.com/Patterns-Enterprise-Application-Architecture-Martin/dp/0321127420/)', 'A repository performs the tasks of an intermediary between the domain model layers and data\nmapping, acting in a similar way to a set of domain objects in memory. Client objects declaratively\nbuild queries and send them to the repositories for answers. Conceptually, a repository encapsulates a\nset of objects stored in the database and operations that can be performed on them, providing a way\nthat is closer to the persistence layer. Repositories, also, support the purpose of separating, clearly\nand in one direction, the dependency between the work domain and the data allocation or mapping.', '**Define one repository per aggregate**', 'For each aggregate or aggregate root, you should create one repository class. You may be able to\nleverage C# Generics to reduce the total number concrete classes you need to maintain (as\ndemonstrated later in this chapter). In a microservice based on Domain-Driven Design (DDD) patterns,\nthe only channel you should use to update the database should be the repositories. This is because\nthey have a one-to-one relationship with the aggregate root, which controls the aggregate’s\ninvariants and transactional consistency. It’s okay to query the database through other channels (as\nyou can do following a CQRS approach), because queries don’t change the state of the database.\nHowever, the transactional area (that is, the updates) must always be controlled by the repositories\nand the aggregate roots.', 'Basically, a repository allows you to populate data in memory that comes from the database in the\nform of the domain entities. Once the entities are in memory, they can be changed and then persisted\nback to the database through transactions.', 'As noted earlier, if you’re using the CQS/CQRS architectural pattern, the initial queries are performed\nby side queries out of the domain model, performed by simple SQL statements using Dapper. This\napproach is much more flexible than repositories because you can query and join any tables you\nneed, and these queries aren’t restricted by rules from the aggregates. That data goes to the\npresentation layer or client app.', 'If the user makes changes, the data to be updated comes from the client app or presentation layer to\nthe application layer (such as a Web API service). When you receive a command in a command\nhandler, you use repositories to get the data you want to update from the database. You update it in\nmemory with the data passed with the commands, and you then add or update the data (domain\nentities) in the database through a transaction.', '239 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'It’s important to emphasize again that you should only define one repository for each aggregate root,\nas shown in Figure 7-17. To achieve the goal of the aggregate root to maintain transactional\nconsistency between all the objects within the aggregate, you should never create a repository for\neach table in the database.', '_Figure 7-17. The relationship between repositories, aggregates, and database tables_', 'The above diagram shows the relationships between Domain and Infrastructure layers: Buyer\nAggregate depends on the IBuyerRepository and Order Aggregate depends on the IOrderRepository\ninterfaces, these interfaces are implemented in the Infrastructure layer by the corresponding\nrepositories that depend on UnitOfWork, also implemented there, that accesses the tables in the Data\ntier.', '**Enforce one aggregate root per repository**', 'It can be valuable to implement your repository design in such a way that it enforces the rule that only\naggregate roots should have repositories. You can create a generic or base repository type that\nconstrains the type of entities it works with to ensure they have the IAggregateRoot marker interface.', 'Thus, each repository class implemented at the infrastructure layer implements its own contract or\ninterface, as shown in the following code:', '240 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Each specific repository interface implements the generic IRepository interface:', 'However, a better way to have the code enforce the convention that each repository is related to a\nsingle aggregate is to implement a generic repository type. That way, it’s explicit that you’re using a\nrepository to target a specific aggregate. That can be easily done by implementing a generic\nIRepository base interface, as in the following code:', '**The Repository pattern makes it easier to test your application logic**', 'The Repository pattern allows you to easily test your application with unit tests. Remember that unit\ntests only test your code, not infrastructure, so the repository abstractions make it easier to achieve\nthat goal.', 'As noted in an earlier section, it’s recommended that you define and place the repository interfaces in\nthe domain model layer so the application layer, such as your Web API microservice, doesn’t depend\ndirectly on the infrastructure layer where you’ve implemented the actual repository classes. By doing\nthis and using Dependency Injection in the controllers of your Web API, you can implement mock\nrepositories that return fake data instead of data from the database. This decoupled approach allows\nyou to create and run unit tests that focus the logic of your application without requiring connectivity\nto the database.', 'Connections to databases can fail and, more importantly, running hundreds of tests against a\ndatabase is bad for two reasons. First, it can take a long time because of the large number of tests.\nSecond, the database records might change and impact the results of your tests, especially if your\ntests are running in parallel, so that they might not be consistent. Unit tests typically can run in\nparallel; integration tests may not support parallel execution depending on their implementation.\nTesting against the database isn’t a unit test but an integration test. You should have many unit tests\nrunning fast, but fewer integration tests against the databases.', 'In terms of separation of concerns for unit tests, your logic operates on domain entities in memory. It\nassumes the repository class has delivered those. Once your logic modifies the domain entities, it\nassumes the repository class will store them correctly. The important point here is to create unit tests\nagainst your domain model and its domain logic. Aggregate roots are the main consistency\nboundaries in DDD.', 'The repositories implemented in eShopOnContainers rely on EF Core’s DbContext implementation of\nthe Repository and Unit of Work patterns using its change tracker, so they don’t duplicate this\nfunctionality.', '241 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**The difference between the Repository pattern and the legacy Data Access class**\n**(DAL class) pattern**', 'A typical DAL object directly performs data access and persistence operations against storage, often\nat the level of a single table and row. Simple CRUD operations implemented with a set of DAL classes\nfrequently do not support transactions (though this is not always the case). Most DAL class\napproaches make minimal use of abstractions, resulting in tight coupling between application or\nBusiness Logic Layer (BLL) classes that call the DAL objects.', 'When using repository, the implementation details of persistence are encapsulated away from the\ndomain model. The use of an abstraction provides ease of extending behavior through patterns like\n[Decorators or Proxies. For instance, cross-cutting concerns like caching, logging, and error handling](https://ardalis.com/building-a-cachedrepository-in-aspnet-core/)\ncan all be applied using these patterns rather than hard-coded in the data access code itself. It’s also\ntrivial to support multiple repository adapters which may be used in different environments, from\nlocal development to shared staging environments to production.', '**Implementing Unit of Work**', '[A unit of work](https://martinfowler.com/eaaCatalog/unitOfWork.html) refers to a single transaction that involves multiple insert, update, or delete operations.\nIn simple terms, it means that for a specific user action, such as a registration on a website, all the\ninsert, update, and delete operations are handled in a single transaction. This is more efficient than\nhandling multiple database operations in a chattier way.', 'These multiple persistence operations are performed later in a single action when your code from the\napplication layer commands it. The decision about applying the in-memory changes to the actual\ndatabase storage is typically based on the Unit of Work pattern. In EF, the Unit of Work pattern is\n[implemented by a DbContext and is executed when a call is made to SaveChanges.](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext)', 'In many cases, this pattern or way of applying operations against the storage can increase application\nperformance and reduce the possibility of inconsistencies. It also reduces transaction blocking in the\ndatabase tables, because all the intended operations are committed as part of one transaction. This is\nmore efficient in comparison to executing many isolated operations against the database. Therefore,\nthe selected ORM can optimize the execution against the database by grouping several update\nactions within the same transaction, as opposed to many small and separate transaction executions.', 'The Unit of Work pattern can be implemented with or without using the Repository pattern.', '**Repositories shouldn’t be mandatory**', 'Custom repositories are useful for the reasons cited earlier, and that is the approach for the ordering\nmicroservice in eShopOnContainers. However, it isn’t an essential pattern to implement in a DDD\ndesign or even in general .NET development.', 'For instance, Jimmy Bogard, when providing direct feedback for this guide, said the following:', 'This’ll probably be my biggest feedback. I’m really not a fan of repositories, mainly because they hide\nthe important details of the underlying persistence mechanism. It’s why I go for MediatR for\ncommands, too. I can use the full power of the persistence layer, and push all that domain behavior\ninto my aggregate roots. I don’t usually want to mock my repositories – I still need to have that', '242 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'integration test with the real thing. Going CQRS meant that we didn’t really have a need for\nrepositories any more.', 'Repositories might be useful, but they are not critical for your DDD design in the way that the\nAggregate pattern and a rich domain model are. Therefore, use the Repository pattern or not, as you\nsee fit.', '**Repository pattern**', 'When you use relational databases such as SQL Server, Oracle, or PostgreSQL, a recommended\napproach is to implement the persistence layer based on Entity Framework (EF). EF supports LINQ and\nprovides strongly typed objects for your model, as well as simplified persistence into your database.', 'Entity Framework has a long history as part of the .NET Framework. When you use .NET, you should\nalso use Entity Framework Core, which runs on Windows or Linux in the same way as .NET. EF Core is a\ncomplete rewrite of Entity Framework that’s implemented with a much smaller footprint and\nimportant improvements in performance.', '243 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Entity Framework (EF) Core is a lightweight, extensible, and cross-platform version of the popular\nEntity Framework data access technology. It was introduced with .NET Core in mid-2016.', 'Since an introduction to EF Core is already available in Microsoft documentation, here we simply\nprovide links to that information.', '**Additional resources**', 'From a DDD point of view, an important capability of EF is the ability to use POCO domain entities,\nalso known in EF terminology as POCO _code-first entities_ . If you use POCO domain entities, your\n[domain model classes are persistence-ignorant, following the Persistence Ignorance and the](https://deviq.com/persistence-ignorance/)\n[Infrastructure Ignorance](https://ayende.com/blog/3137/infrastructure-ignorance) principles.', 'Per DDD patterns, you should encapsulate domain behavior and rules within the entity class itself, so\nit can control invariants, validations, and rules when accessing any collection. Therefore, it is not a\ngood practice in DDD to allow public access to collections of child entities or value objects. Instead,\nyou want to expose methods that control how and when your fields and property collections can be\nupdated, and what behavior and actions should occur when that happens.', 'Since EF Core 1.1, to satisfy those DDD requirements, you can have plain fields in your entities instead\nof public properties. If you do not want an entity field to be externally accessible, you can just create\nthe attribute or field instead of a property. You can also use private property setters.', 'In a similar way, you can now have read-only access to collections by using a public property typed as\nIReadOnlyCollection<T>, which is backed by a private field member for the collection (like a List<T>)\nin your entity that relies on EF for persistence. Previous versions of Entity Framework required\ncollection properties to support ICollection<T>, which meant that any developer using the parent\nentity class could add or remove items through its property collections. That possibility would be\nagainst the recommended patterns in DDD.', 'You can use a private collection while exposing a read-only IReadOnlyCollection<T> object, as shown\nin the following code example:', '244 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The OrderItems property can only be accessed as read-only using IReadOnlyCollection<OrderItem>.\nThis type is read-only so it is protected against regular external updates.', 'EF Core provides a way to map the domain model to the physical database without “contaminating”\nthe domain model. It is pure .NET POCO code, because the mapping action is implemented in the\npersistence layer. In that mapping action, you need to configure the fields-to-database mapping. In\nthe following example of the OnModelCreating method from OrderingContext and the\nOrderEntityTypeConfiguration class, the call to SetPropertyAccessMode tells EF Core to access the\nOrderItems property through its field.', '245 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'When you use fields instead of properties, the OrderItem entity is persisted as if it had a\nList<OrderItem> property. However, it exposes a single accessor, the AddOrderItem method, for\nadding new items to the order. As a result, behavior and data are tied together and will be consistent\nthroughout any application code that uses the domain model.', 'At the implementation level, a repository is simply a class with data persistence code coordinated by a\nunit of work (DBContext in EF Core) when performing updates, as shown in the following class:', 'The IBuyerRepository interface comes from the domain model layer as a contract. However, the\nrepository implementation is done at the persistence and infrastructure layer.', 'The EF DbContext comes through the constructor through Dependency Injection. It is shared between\nmultiple repositories within the same HTTP request scope, thanks to its default lifetime\n(ServiceLifetime.Scoped) in the IoC container (which can also be explicitly set with\nservices.AddDbContext<>).', '246 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Methods to implement in a repository (updates or transactions versus queries)**', 'Within each repository class, you should put the persistence methods that update the state of entities\ncontained by its related aggregate. Remember there is one-to-one relationship between an aggregate\nand its related repository. Consider that an aggregate root entity object might have embedded child\nentities within its EF graph. For example, a buyer might have multiple payment methods as related\nchild entities.', 'Since the approach for the ordering microservice in eShopOnContainers is also based on CQS/CQRS,\nmost of the queries are not implemented in custom repositories. Developers have the freedom to\ncreate the queries and joins they need for the presentation layer without the restrictions imposed by\naggregates, custom repositories per aggregate, and DDD in general. Most of the custom repositories\nsuggested by this guide have several update or transactional methods but just the query methods\nneeded to get data to be updated. For example, the BuyerRepository repository implements a\nFindAsync method, because the application needs to know whether a particular buyer exists before\ncreating a new buyer related to the order.', 'However, the real query methods to get data to send to the presentation layer or client apps are\nimplemented, as mentioned, in the CQRS queries based on flexible queries using Dapper.', '**Using a custom repository versus using EF DbContext directly**', 'The Entity Framework DbContext class is based on the Unit of Work and Repository patterns and can\nbe used directly from your code, such as from an ASP.NET Core MVC controller. The Unit of Work and\nRepository patterns result in the simplest code, as in the CRUD catalog microservice in\neShopOnContainers. In cases where you want the simplest code possible, you might want to directly\nuse the DbContext class, as many developers do.', 'However, implementing custom repositories provides several benefits when implementing more\ncomplex microservices or applications. The Unit of Work and Repository patterns are intended to\nencapsulate the infrastructure persistence layer so it is decoupled from the application and domainmodel layers. Implementing these patterns can facilitate the use of mock repositories simulating\naccess to the database.', 'In Figure 7-18, you can see the differences between not using repositories (directly using the EF\nDbContext) versus using repositories, which makes it easier to mock those repositories.', '247 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-18. Using custom repositories versus a plain DbContext_', 'Figure 7-18 shows that using a custom repository adds an abstraction layer that can be used to ease\ntesting by mocking the repository. There are multiple alternatives when mocking. You could mock just\nrepositories or you could mock a whole unit of work. Usually mocking just the repositories is enough,\nand the complexity to abstract and mock a whole unit of work is usually not needed.', 'Later, when we focus on the application layer, you will see how Dependency Injection works in\nASP.NET Core and how it is implemented when using repositories.', 'In short, custom repositories allow you to test code more easily with unit tests that are not impacted\nby the data tier state. If you run tests that also access the actual database through the Entity\nFramework, they are not unit tests but integration tests, which are a lot slower.', 'If you were using DbContext directly, you would have to mock it or to run unit tests by using an inmemory SQL Server with predictable data for unit tests. But mocking the DbContext or controlling\nfake data requires more work than mocking at the repository level. Of course, you could always test\nthe MVC controllers.', 'The DbContext object (exposed as an IUnitOfWork object) should be shared among multiple\nrepositories within the same HTTP request scope. For example, this is true when the operation being\nexecuted must deal with multiple aggregates, or simply because you are using multiple repository\ninstances. It is also important to mention that the IUnitOfWork interface is part of your domain layer,\nnot an EF Core type.', 'In order to do that, the instance of the DbContext object has to have its service lifetime set to\nServiceLifetime.Scoped. This is the default lifetime when registering a DbContext with\nbuilder.Services.AddDbContext in your IoC container from the _Program.cs_ file in your ASP.NET Core\nWeb API project. The following code illustrates this.', '248 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The DbContext instantiation mode should not be configured as ServiceLifetime.Transient or\nServiceLifetime.Singleton.', 'In a similar way, repository’s lifetime should usually be set as scoped (InstancePerLifetimeScope in\nAutofac). It could also be transient (InstancePerDependency in Autofac), but your service will be more\nefficient in regards to memory when using the scoped lifetime.', 'Using the singleton lifetime for the repository could cause you serious concurrency problems when\nyour DbContext is set to scoped (InstancePerLifetimeScope) lifetime (the default lifetimes for a\nDBContext). As long as your service lifetimes for your repositories and your DbContext are both\nScoped, you’ll avoid these issues.', '**Additional resources**', 'Table mapping identifies the table data to be queried from and saved to the database. Previously you\nsaw how domain entities (for example, a product or order domain) can be used to generate a related\ndatabase schema. EF is strongly designed around the concept of _conventions_ . Conventions address\nquestions like “What will the name of a table be?” or “What property is the primary key?” Conventions\nare typically based on conventional names. For example, it is typical for the primary key to be a\nproperty that ends with Id.', 'By convention, each entity will be set up to map to a table with the same name as the DbSet<TEntity>\nproperty that exposes the entity on the derived context. If no DbSet<TEntity> value is provided for\nthe given entity, the class name is used.', '**Data Annotations versus Fluent API**', 'There are many additional EF Core conventions, and most of them can be changed by using either\ndata annotations or Fluent API, implemented within the OnModelCreating method.', 'Data annotations must be used on the entity model classes themselves, which is a more intrusive way\nfrom a DDD point of view. This is because you are contaminating your model with data annotations\nrelated to the infrastructure database. On the other hand, Fluent API is a convenient way to change\nmost conventions and mappings within your data persistence infrastructure layer, so the entity model\nwill be clean and decoupled from the persistence infrastructure.', '**Fluent API and the OnModelCreating method**', 'As mentioned, in order to change conventions and mappings, you can use the OnModelCreating\nmethod in the DbContext class.', 'The ordering microservice in eShopOnContainers implements explicit mapping and configuration,\nwhen needed, as shown in the following code.', '250 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'You could set all the Fluent API mappings within the same OnModelCreating method, but it’s\nadvisable to partition that code and have multiple configuration classes, one per entity, as shown in', '251 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'the example. Especially for large models, it is advisable to have separate configuration classes for\nconfiguring different entity types.', 'The code in the example shows a few explicit declarations and mapping. However, EF Core\nconventions do many of those mappings automatically, so the actual code you would need in your\ncase might be smaller.', '**The Hi/Lo algorithm in EF Core**', '[An interesting aspect of code in the preceding example is that it uses the Hi/Lo algorithm as the key](https://vladmihalcea.com/the-hilo-algorithm/)\ngeneration strategy.', 'The Hi/Lo algorithm is useful when you need unique keys before committing changes. As a summary,\nthe Hi-Lo algorithm assigns unique identifiers to table rows while not depending on storing the row in\nthe database immediately. This lets you start using the identifiers right away, as happens with regular\nsequential database IDs.', 'The Hi/Lo algorithm describes a mechanism for getting a batch of unique IDs from a related database\nsequence. These IDs are safe to use because the database guarantees the uniqueness, so there will be\nno collisions between users. This algorithm is interesting for these reasons:', 'As introduced earlier in the design section, the Query Specification pattern is a Domain-Driven Design\npattern designed as the place where you can put the definition of a query with optional sorting and\npaging logic.', 'The Query Specification pattern defines a query in an object. For example, in order to encapsulate a\npaged query that searches for some products you can create a PagedProduct specification that takes\nthe necessary input parameters (pageNumber, pageSize, filter, etc.). Then, within any Repository\nmethod (usually a List() overload) it would accept an IQuerySpecification and run the expected query\nbased on that specification.', 'An example of a generic Specification interface is the following code, which is similar to code used in\n[the eShopOnWeb](https://github.com/dotnet-architecture/eShopOnWeb) reference application.', 'Then, the implementation of a generic specification base class is the following.', '253 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The following specification loads a single basket entity given either the basket’s ID or the ID of the\n[buyer to whom the basket belongs. It will eagerly load](https://docs.microsoft.com/ef/core/querying/related-data) the basket’s Items collection.', 'And finally, you can see below how a generic EF Repository can use such a specification to filter and\neager-load data related to a given entity type T.', 'In addition to encapsulating filtering logic, the specification can specify the shape of the data to be\nreturned, including which properties to populate.', 'Although we don’t recommend returning IQueryable from a repository, it’s perfectly fine to use them\nwithin the repository to build up a set of results. You can see this approach used in the List method\nabove, which uses intermediate IQueryable expressions to build up the query’s list of includes before\nexecuting the query with the specification’s criteria on the last line.', '[Learn how the specification pattern is applied in the eShopOnWeb sample.](https://github.com/dotnet-architecture/eShopOnWeb/wiki/Patterns#specification)', '254 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Additional resources**', 'When you use NoSQL databases for your infrastructure data tier, you typically do not use an ORM like\nEntity Framework Core. Instead you use the API provided by the NoSQL engine, such as Azure Cosmos\nDB, MongoDB, Cassandra, RavenDB, CouchDB, or Azure Storage Tables.', 'However, when you use a NoSQL database, especially a document-oriented database like Azure\nCosmos DB, CouchDB, or RavenDB, the way you design your model with DDD aggregates is partially\nsimilar to how you can do it in EF Core, in regards to the identification of aggregate roots, child entity\nclasses, and value object classes. But, ultimately, the database selection will impact in your design.', 'When you use a document-oriented database, you implement an aggregate as a single document,\nserialized in JSON or another format. However, the use of the database is transparent from a domain\nmodel code point of view. When using a NoSQL database, you still are using entity classes and\naggregate root classes, but with more flexibility than when using EF Core because the persistence is\nnot relational.', 'The difference is in how you persist that model. If you implemented your domain model based on\nPOCO entity classes, agnostic to the infrastructure persistence, it might look like you could move to a\ndifferent persistence infrastructure, even from relational to NoSQL. However, that should not be your\ngoal. There are always constraints and trade-offs in the different database technologies, so you will\nnot be able to have the same model for relational or NoSQL databases. Changing persistence models\nis not a trivial task, because transactions and persistence operations will be very different.', 'For example, in a document-oriented database, it is okay for an aggregate root to have multiple child\ncollection properties. In a relational database, querying multiple child collection properties is not', '255 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'easily optimized, because you get a UNION ALL SQL statement back from EF. Having the same\ndomain model for relational databases or NoSQL databases is not simple, and you should not try to\ndo it. You really have to design your model with an understanding of how the data is going to be\nused in each particular database.', 'A benefit when using NoSQL databases is that the entities are more denormalized, so you do not set a\ntable mapping. Your domain model can be more flexible than when using a relational database.', 'When you design your domain model based on aggregates, moving to NoSQL and documentoriented databases might be even easier than using a relational database, because the aggregates\nyou design are similar to serialized documents in a document-oriented database. Then you can\ninclude in those “bags” all the information you might need for that aggregate.', 'For instance, the following JSON code is a sample implementation of an order aggregate when using\na document-oriented database. It is similar to the order aggregate we implemented in the\neShopOnContainers sample, but without using EF Core underneath.', '[Azure Cosmos DB](https://docs.microsoft.com/azure/cosmos-db/introduction) is Microsoft’s globally distributed database service for mission-critical applications.\n[Azure Cosmos DB provides turn-key global distribution,](https://docs.microsoft.com/azure/cosmos-db/distribute-data-globally) [elastic scaling of throughput and storage](https://docs.microsoft.com/azure/cosmos-db/partition-data)\n[worldwide, single-digit millisecond latencies at the 99th percentile, five well-defined consistency](https://docs.microsoft.com/azure/cosmos-db/consistency-levels)\n[levels, and guaranteed high availability, all backed by industry-leading SLAs. Azure Cosmos DB](https://docs.microsoft.com/azure/cosmos-db/consistency-levels)\n[automatically indexes data without requiring you to deal with schema and index management. It is](https://www.vldb.org/pvldb/vol8/p1668-shukla.pdf)\nmulti-model and supports document, key-value, graph, and columnar data models.', '256 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-19. Azure Cosmos DB global distribution_', 'When you use a C# model to implement the aggregate to be used by the Azure Cosmos DB API, the\naggregate can be similar to the C# POCO classes used with EF Core. The difference is in the way to\nuse them from the application and infrastructure layers, as in the following code:', '257 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'You can see that the way you work with your domain model can be similar to the way you use it in\nyour domain model layer when the infrastructure is EF. You still use the same aggregate root methods\nto ensure consistency, invariants, and validations within the aggregate.', 'However, when you persist your model into the NoSQL database, the code and API change\ndramatically compared to EF Core code or any other code related to relational databases.', '**Use Azure Cosmos DB from .NET containers**', 'You can access Azure Cosmos DB databases from .NET code running in containers, like from any other\n.NET application. For instance, the Locations.API and Marketing.API microservices in\neShopOnContainers are implemented so they can consume Azure Cosmos DB databases.', 'However, there’s a limitation in Azure Cosmos DB from a Docker development environment point of\n[view. Even though there’s an on-premises Azure Cosmos DB Emulator](https://docs.microsoft.com/azure/cosmos-db/local-emulator) that can run in a local\ndevelopment machine, it only supports Windows. Linux and macOS aren’t supported.', 'There’s also the possibility to run this emulator on Docker, but just on Windows Containers, not with\nLinux Containers. That’s an initial handicap for the development environment if your application is\ndeployed as Linux containers, since, currently, you can’t deploy Linux and Windows Containers on\nDocker for Windows at the same time. Either all containers being deployed have to be for Linux or for\nWindows.', 'The ideal and more straightforward deployment for a dev/test solution is to be able to deploy your\ndatabase systems as containers along with your custom containers so your dev/test environments are\nalways consistent.', '258 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Use MongoDB API for local dev/test Linux/Windows containers plus Azure Cosmos**\n**DB**', 'Cosmos DB databases support MongoDB API for .NET as well as the native MongoDB wire protocol.\nThis means that by using existing drivers, your application written for MongoDB can now\ncommunicate with Cosmos DB and use Cosmos DB databases instead of MongoDB databases, as\nshown in Figure 7-20.', '_Figure 7-20. Using MongoDB API and protocol to access Azure Cosmos DB_', 'This is a very convenient approach for proof of concepts in Docker environments with Linux\n[containers because the MongoDB Docker image is a multi-arch image that supports Docker Linux](https://hub.docker.com/r/_/mongo/)\ncontainers and Docker Windows containers.', 'As shown in the following image, by using the MongoDB API, eShopOnContainers supports MongoDB\nLinux and Windows containers for the local development environment but then, you can move to a\n[scalable, PaaS cloud solution as Azure Cosmos DB by simply changing the MongoDB connection](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)\n[string to point to Azure Cosmos DB.](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)', '259 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-21. eShopOnContainers using MongoDB containers for dev-env or Azure Cosmos DB for production_', 'The production Azure Cosmos DB would be running in Azure’s cloud as a PaaS and scalable service.', 'Your custom .NET containers can run on a local development Docker host (that is using Docker for\nWindows in a Windows 10 machine) or be deployed into a production environment, like Kubernetes in\nAzure AKS or Azure Service Fabric. In this second environment, you would deploy only the .NET\ncustom containers but not the MongoDB container since you’d be using Azure Cosmos DB in the\ncloud for handling the data in production.', 'A clear benefit of using the MongoDB API is that your solution could run in both database engines,\nMongoDB or Azure Cosmos DB, so migrations to different environments should be easy. However,\nsometimes it is worthwhile to use a native API (that is the native Cosmos DB API) in order to take full\nadvantage of the capabilities of a specific database engine.', 'For further comparison between simply using MongoDB versus Cosmos DB in the cloud, see the\n[Benefits of using Azure Cosmos DB in this page.](https://docs.microsoft.com/azure/cosmos-db/mongodb-introduction)', '**Analyze your approach for production applications: MongoDB API vs. Cosmos DB**\n**API**', 'In eShopOnContainers, we’re using MongoDB API because our priority was fundamentally to have a\nconsistent dev/test environment using a NoSQL database that could also work with Azure Cosmos DB.', '260 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'However, if you are planning to use MongoDB API to access Azure Cosmos DB in Azure for\nproduction applications, you should analyze the differences in capabilities and performance when\nusing MongoDB API to access Azure Cosmos DB databases compared to using the native Azure\nCosmos DB API. If it is similar you can use MongoDB API and you get the benefit of supporting two\nNoSQL database engines at the same time.', 'You could also use MongoDB clusters as the production database in Azure’s cloud, too, with\n[MongoDB Azure Service. But that is not a PaaS service provided by Microsoft. In this case, Azure is just](https://www.mongodb.com/scale/mongodb-azure-service)\nhosting that solution coming from MongoDB.', 'Basically, this is just a disclaimer stating that you shouldn’t always use MongoDB API against Azure\nCosmos DB, as we did in eShopOnContainers because it was a convenient choice for Linux containers.\nThe decision should be based on the specific needs and tests you need to do for your production\napplication.', '**The code: Use MongoDB API in .NET applications**', 'MongoDB API for .NET is based on NuGet packages that you need to add to your projects, like in the\nLocations.API project shown in the following figure.', '261 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-22. MongoDB API NuGet packages references in a .NET project_', 'Let’s investigate the code in the following sections.', '**A Model used by MongoDB API**', 'First, you need to define a model that will hold the data coming from the database in your\napplication’s memory space. Here’s an example of the model used for Locations at\neShopOnContainers.', '262 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'You can see there are a few attributes and types coming from the MongoDB NuGet packages.', 'NoSQL databases are usually very well suited for working with non-relational hierarchical data. In this\nexample, we are using MongoDB types especially made for geo-locations, like\nGeoJson2DGeographicCoordinates.', '**Retrieve the database and the collection**', 'In eShopOnContainers, we have created a custom database context where we implement the code to\nretrieve the database and the MongoCollections, as in the following code.', '263 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Retrieve the data**', 'In C# code, like Web API controllers or custom Repositories implementation, you can write similar\ncode to the following when querying through the MongoDB API. Note that the _context object is an\ninstance of the previous LocationsContext class.', '**Use an env-var in the docker-compose.override.yml file for the MongoDB connection**\n**string**', 'When creating a MongoClient object, it needs a fundamental parameter which is precisely the\nConnectionString parameter pointing to the right database. In the case of eShopOnContainers, the\nconnection string can point to a local MongoDB Docker container or to a “production” Azure Cosmos\nDB database. That connection string comes from the environment variables defined in the dockercompose.override.yml files used when deploying with docker-compose or Visual Studio, as in the\nfollowing yml code.', 'The ConnectionString environment variable is resolved this way: If the ESHOP_AZURE_COSMOSDB\nglobal variable is defined in the .env file with the Azure Cosmos DB connection string, it will use it to\naccess the Azure Cosmos DB database in the cloud. If it’s not defined, it will take the\nmongodb://nosqldata value and use the development MongoDB container.', 'The following code shows the .env file with the Azure Cosmos DB connection string global\nenvironment variable, as implemented in eShopOnContainers:', '264 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Uncomment the ESHOP_AZURE_COSMOSDB line and update it with your Azure Cosmos DB\n[connection string obtained from the Azure portal as explained in Connect a MongoDB application to](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)\n[Azure Cosmos DB.](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)', 'If the ESHOP_AZURE_COSMOSDB global variable is empty, meaning it’s commented out in the .env\nfile, then the container uses a default MongoDB connection string. This connection string points to\nthe local MongoDB container deployed in eShopOnContainers that is named nosqldata and was\ndefined at the docker-compose file, as shown in the following .yml code:', '**Additional resources**', 'SOLID principles are critical techniques to be used in any modern and mission-critical application,\nsuch as developing a microservice with DDD patterns. SOLID is an acronym that groups five\nfundamental principles:', 'As mentioned previously, the application layer can be implemented as part of the artifact (assembly)\nyou are building, such as within a Web API project or an MVC web app project. In the case of a\nmicroservice built with ASP.NET Core, the application layer will usually be your Web API library. If you\nwant to separate what is coming from ASP.NET Core (its infrastructure plus your controllers) from your\ncustom application layer code, you could also place your application layer in a separate class library,\nbut that is optional.', 'For instance, the application layer code of the ordering microservice is directly implemented as part of\nthe **Ordering.API** project (an ASP.NET Core Web API project), as shown in Figure 7-23.', '_Figure 7-23. The application layer in the Ordering.API ASP.NET Core Web API project_', '[ASP.NET Core includes a simple built-in IoC container](https://docs.microsoft.com/aspnet/core/fundamentals/dependency-injection) (represented by the IServiceProvider interface)\nthat supports constructor injection by default, and ASP.NET makes certain services available through\nDI. ASP.NET Core uses the term _service_ for any of the types you register that will be injected through\nDI. You configure the built-in container’s services in your application’s _Program.cs_ file. Your', '267 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'dependencies are implemented in the services that a type needs and that you register in the IoC\ncontainer.', 'Typically, you want to inject dependencies that implement infrastructure objects. A typical\ndependency to inject is a repository. But you could inject any other infrastructure dependency that\nyou may have. For simpler implementations, you could directly inject your Unit of Work pattern object\n(the EF DbContext object), because the DBContext is also the implementation of your infrastructure\npersistence objects.', 'In the following example, you can see how .NET is injecting the required repository objects through\nthe constructor. The class is a command handler, which will get covered in the next section.', '268 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The class uses the injected repositories to execute the transaction and persist the state changes. It\ndoes not matter whether that class is a command handler, an ASP.NET Core Web API controller\n[method, or a DDD Application Service. It is ultimately a simple class that uses repositories, domain](https://lostechies.com/jimmybogard/2008/08/21/services-in-domain-driven-design/)\nentities, and other application coordination in a fashion similar to a command handler. Dependency\nInjection works the same way for all the mentioned classes, as in the example using DI based on the\nconstructor.', '**Register the dependency implementation types and interfaces or abstractions**', 'Before you use the objects injected through constructors, you need to know where to register the\ninterfaces and classes that produce the objects injected into your application classes through DI. (Like\nDI based on the constructor, as shown previously.)', '**Use the built-in IoC container provided by ASP.NET Core**', 'When you use the built-in IoC container provided by ASP.NET Core, you register the types you want\nto inject in the _Program.cs_ file, as in the following code:', 'The most common pattern when registering types in an IoC container is to register a pair of types—an\ninterface and its related implementation class. Then when you request an object from the IoC\ncontainer through any constructor, you request an object of a certain type of interface. For instance, in\nthe previous example, the last line states that when any of your constructors have a dependency on\nIMyCustomRepository (interface or abstraction), the IoC container will inject an instance of the\nMyCustomSQLServerRepository implementation class.', '**Use the Scrutor library for automatic types registration**', 'When using DI in .NET, you might want to be able to scan an assembly and automatically register its\ntypes by convention. This feature is not currently available in ASP.NET Core. However, you can use the', '269 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '[Scrutor](https://github.com/khellang/Scrutor) library for that. This approach is convenient when you have dozens of types that need to be\nregistered in your IoC container.', '**Additional resources**', 'In the DI-through-constructor example shown in the previous section, the IoC container was injecting\nrepositories through a constructor in a class. But exactly where were they injected? In a simple Web\nAPI (for example, the catalog microservice in eShopOnContainers), you inject them at the MVC\ncontrollers’ level, in a controller constructor, as part of the request pipeline of ASP.NET Core. However,\n[in the initial code of this section (the CreateOrderCommandHandler](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/Services/Ordering/Ordering.API/Application/Commands/CreateOrderCommandHandler.cs) class from the Ordering.API\nservice in eShopOnContainers), the injection of dependencies is done through the constructor of a\nparticular command handler. Let us explain what a command handler is and why you would want to\nuse it.', 'The Command pattern is intrinsically related to the CQRS pattern that was introduced earlier in this\n[guide. CQRS has two sides. The first area is queries, using simplified queries with the Dapper](https://github.com/StackExchange/dapper-dot-net) micro\nORM, which was explained previously. The second area is commands, which are the starting point for\ntransactions, and the input channel from outside the service.', 'As shown in Figure 7-24, the pattern is based on accepting commands from the client-side,\nprocessing them based on the domain model rules, and finally persisting the states with transactions.', '271 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '_Figure 7-24. High-level view of the commands or “transactional side” in a CQRS pattern_', 'Figure 7-24 shows that the UI app sends a command through the API that gets to a\nCommandHandler, that depends on the Domain model and the Infrastructure, to update the\ndatabase.', '**The command class**', 'A command is a request for the system to perform an action that changes the state of the system.\nCommands are imperative, and should be processed just once.', 'Since commands are imperatives, they are typically named with a verb in the imperative mood (for\nexample, “create” or “update”), and they might include the aggregate type, such as\nCreateOrderCommand. Unlike an event, a command is not a fact from the past; it is only a request,\nand thus may be refused.', 'Commands can originate from the UI as a result of a user initiating a request, or from a process\nmanager when the process manager is directing an aggregate to perform an action.', 'An important characteristic of a command is that it should be processed just once by a single receiver.\nThis is because a command is a single action or transaction you want to perform in the application.\nFor example, the same order creation command should not be processed more than once. This is an\nimportant difference between commands and events. Events may be processed multiple times,\nbecause many systems or microservices might be interested in the event.', 'In addition, it is important that a command be processed only once in case the command is not\nidempotent. A command is idempotent if it can be executed multiple times without changing the\nresult, either because of the nature of the command, or because of the way the system handles the\ncommand.', 'It is a good practice to make your commands and updates idempotent when it makes sense under\nyour domain’s business rules and invariants. For instance, to use the same example, if for any reason\n(retry logic, hacking, etc.) the same CreateOrder command reaches your system multiple times, you\nshould be able to identify it and ensure that you do not create multiple orders. To do so, you need to', '272 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'attach some kind of identity in the operations and identify whether the command or update was\nalready processed.', 'You send a command to a single receiver; you do not publish a command. Publishing is for events\nthat state a fact—that something has happened and might be interesting for event receivers. In the\ncase of events, the publisher has no concerns about which receivers get the event or what they do it.\nBut domain or integration events are a different story already introduced in previous sections.', 'A command is implemented with a class that contains data fields or collections with all the\ninformation that is needed in order to execute that command. A command is a special kind of Data\nTransfer Object (DTO), one that is specifically used to request changes or transactions. The command\nitself is based on exactly the information that is needed for processing the command, and nothing\nmore.', 'The following example shows the simplified CreateOrderCommand class. This is an immutable\ncommand that is used in the ordering microservice in eShopOnContainers.', '273 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '274 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Basically, the command class contains all the data you need for performing a business transaction by\nusing the domain model objects. Thus, commands are simply data structures that contain read-only\ndata, and no behavior. The command’s name indicates its purpose. In many languages like C#,\ncommands are represented as classes, but they are not true classes in the real object-oriented sense.', 'As an additional characteristic, commands are immutable, because the expected usage is that they are\nprocessed directly by the domain model. They do not need to change during their projected lifetime.\nIn a C# class, immutability can be achieved by not having any setters or other methods that change\nthe internal state.', 'Keep in mind that if you intend or expect commands to go through a serializing/deserializing process,\nthe properties must have a private setter, and the [DataMember] (or [JsonProperty]) attribute.\nOtherwise, the deserializer won’t be able to reconstruct the object at the destination with the required\nvalues. You can also use truly read-only properties if the class has a constructor with parameters for all\nproperties, with the usual camelCase naming convention, and annotate the constructor as', '[JsonConstructor]. However, this option requires more code.', 'For example, the command class for creating an order is probably similar in terms of data to the order\nyou want to create, but you probably do not need the same attributes. For instance,\nCreateOrderCommand does not have an order ID, because the order has not been created yet.', 'Many command classes can be simple, requiring only a few fields about some state that needs to be\nchanged. That would be the case if you are just changing the status of an order from “in process” to\n“paid” or “shipped” by using a command similar to the following:', 'Some developers make their UI request objects separate from their command DTOs, but that is just a\nmatter of preference. It is a tedious separation with not much additional value, and the objects are\nalmost exactly the same shape. For instance, in eShopOnContainers, some commands come directly\nfrom the client-side.', '**The Command handler class**', 'You should implement a specific command handler class for each command. That is how the pattern\nworks, and it’s where you’ll use the command object, the domain objects, and the infrastructure\nrepository objects. The command handler is in fact the heart of the application layer in terms of CQRS\nand DDD. However, all the domain logic should be contained in the domain classes—within the\n[aggregate roots (root entities), child entities, or domain services, but not within the command handler,](https://lostechies.com/jimmybogard/2008/08/21/services-in-domain-driven-design/)\nwhich is a class from the application layer.', '275 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'The command handler class offers a strong stepping stone in the way to achieve the Single\nResponsibility Principle (SRP) mentioned in a previous section.', 'A command handler receives a command and obtains a result from the aggregate that is used. The\nresult should be either successful execution of the command, or an exception. In the case of an\nexception, the system state should be unchanged.', 'The command handler usually takes the following steps:', 'The next question is how to invoke a command handler. You could manually call it from each related\nASP.NET Core controller. However, that approach would be too coupled and is not ideal.', 'The other two main options, which are the recommended options, are:', 'As a sample implementation, this guide proposes using the in-process pipeline based on the Mediator\npattern to drive command ingestion and route commands, in memory, to the right command\n[handlers. The guide also proposes applying behaviors](https://github.com/jbogard/MediatR/wiki/Behaviors) in order to separate cross-cutting concerns.', 'For implementation in .NET, there are multiple open-source libraries available that implement the\n[Mediator pattern. The library used in this guide is the MediatR](https://github.com/jbogard/MediatR) open-source library (created by Jimmy\nBogard), but you could use another approach. MediatR is a small and simple library that allows you to\nprocess in-memory messages like a command, while applying decorators or behaviors.', 'Using the Mediator pattern helps you to reduce coupling and to isolate the concerns of the requested\nwork, while automatically connecting to the handler that performs that work—in this case, to\ncommand handlers.', 'Another good reason to use the Mediator pattern was explained by Jimmy Bogard when reviewing\nthis guide:', '281 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'I think it might be worth mentioning testing here – it provides a nice consistent window into the\nbehavior of your system. Request-in, response-out. We’ve found that aspect quite valuable in building\nconsistently behaving tests.', 'First, let’s look at a sample WebAPI controller where you actually would use the mediator object. If\nyou weren’t using the mediator object, you’d need to inject all the dependencies for that controller,\nthings like a logger object and others. Therefore, the constructor would be complicated. On the other\nhand, if you use the mediator object, the constructor of your controller can be a lot simpler, with just a\nfew dependencies instead of many dependencies if you had one per cross-cutting operation, as in the\nfollowing example:', 'You can see that the mediator provides a clean and lean Web API controller constructor. In addition,\nwithin the controller methods, the code to send a command to the mediator object is almost one line:', '**Implement idempotent Commands**', 'In **eShopOnContainers**, a more advanced example than the above is submitting a\nCreateOrderCommand object from the Ordering microservice. But since the Ordering business\nprocess is a bit more complex and, in our case, it actually starts in the Basket microservice, this action\nof submitting the CreateOrderCommand object is performed from an integration-event handler\n[named UserCheckoutAcceptedIntegrationEventHandler](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/IntegrationEvents/EventHandling/UserCheckoutAcceptedIntegrationEventHandler.cs) instead of a simple WebAPI controller called\nfrom the client App as in the previous simpler example.', 'Nevertheless, the action of submitting the Command to MediatR is pretty similar, as shown in the\nfollowing code.', '282 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'However, this case is also slightly more advanced because we’re also implementing idempotent\ncommands. The CreateOrderCommand process should be idempotent, so if the same message comes\nduplicated through the network, because of any reason, like retries, the same business order will be\nprocessed just once.', 'This is implemented by wrapping the business command (in this case CreateOrderCommand) and\nembedding it into a generic IdentifiedCommand, which is tracked by an ID of every message coming\nthrough the network that has to be idempotent.', 'In the code below, you can see that the IdentifiedCommand is nothing more than a DTO with and ID\nplus the wrapped business command object.', '[Then the CommandHandler for the IdentifiedCommand named IdentifiedCommandHandler.cs will](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Commands/IdentifiedCommandHandler.cs)\nbasically check if the ID coming as part of the message already exists in a table. If it already exists, that\ncommand won’t be processed again, so it behaves as an idempotent command. That infrastructure\ncode is performed by the _requestManager.ExistAsync method call below.', '283 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '284 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Since the IdentifiedCommand acts like a business command’s envelope, when the business command\nneeds to be processed because it is not a repeated ID, then it takes that inner business command and\nresubmits it to Mediator, as in the last part of the code shown above when running\n[_mediator.Send(message.Command), from the IdentifiedCommandHandler.cs.](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Commands/IdentifiedCommandHandler.cs)', 'When doing that, it will link and run the business command handler, in this case, the\n[CreateOrderCommandHandler, which is running transactions against the Ordering database, as shown](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Commands/CreateOrderCommandHandler.cs)\nin the following code.', '285 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', '**Register the types used by MediatR**', 'In order for MediatR to be aware of your command handler classes, you need to register the mediator\nclasses and the command handler classes in your IoC container. By default, MediatR uses Autofac as\nthe IoC container, but you can also use the built-in ASP.NET Core IoC container or any other container\nsupported by MediatR.', 'The following code shows how to register Mediator’s types and commands when using Autofac\nmodules.', '286 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'This is where “the magic happens” with MediatR.', 'As each command handler implements the generic IRequestHandler<T> interface, when you register\nthe assemblies using RegisteredAssemblyTypes method all the types marked as IRequestHandler also\ngets registered with their Commands. For example:', 'That is the code that correlates commands with command handlers. The handler is just a simple class,\nbut it inherits from RequestHandler<T>, where T is the command type, and MediatR makes sure it is\ninvoked with the correct payload (the command).', 'There is one more thing: being able to apply cross-cutting concerns to the mediator pipeline. You can\nalso see at the end of the Autofac registration module code how it registers a behavior type,\nspecifically, a custom LoggingBehavior class and a ValidatorBehavior class. But you could add other\ncustom behaviors, too.', '[That LoggingBehavior](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Behaviors/LoggingBehavior.cs) class can be implemented as the following code, which logs information about\nthe command handler being executed and whether it was successful or not.', '287 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'Just by implementing this behavior class and by registering it in the pipeline (in the MediatorModule\nabove), all the commands processed through MediatR will be logging information about the\nexecution.', 'The eShopOnContainers ordering microservice also applies a second behavior for basic validations,\n[the ValidatorBehavior](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Behaviors/ValidatorBehavior.cs) [class that relies on the FluentValidation](https://github.com/JeremySkinner/FluentValidation) library, as shown in the following code:', 'Here the behavior is raising an exception if validation fails, but you could also return a result object,\ncontaining the command result if it succeeded or the validation messages in case it didn’t. This would\nprobably make it easier to display validation results to the user.', '[Then, based on the FluentValidation library, you would create validation for the data passed with](https://github.com/JeremySkinner/FluentValidation)\nCreateOrderCommand, as in the following code:', '288 CHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns', 'You could create additional validations. This is a very clean and elegant way to implement your\ncommand validations.', 'In a similar way, you could implement other behaviors for additional aspects or cross-cutting concerns\nthat you want to apply to commands when handling them.', '**Additional resources**', 'The mediator pattern', '_Your microservice and cloud-based applications must embrace the partial failures that will certainly_\n_occur eventually. You must design your application to be resilient to those partial failures._', 'Resiliency is the ability to recover from failures and continue to function. It isn’t about avoiding\nfailures but accepting the fact that failures will happen and responding to them in a way that avoids\ndowntime or data loss. The goal of resiliency is to return the application to a fully functioning state\nafter a failure.', 'It’s challenging enough to design and deploy a microservices-based application. But you also need to\nkeep your application running in an environment where some sort of failure is certain. Therefore, your\napplication should be resilient. It should be designed to cope with partial failures, like network\noutages or nodes or VMs crashing in the cloud. Even microservices (containers) being moved to a\ndifferent node within a cluster can cause intermittent short failures within the application.', 'The many individual components of your application should also incorporate health monitoring\nfeatures. By following the guidelines in this chapter, you can create an application that can work\nsmoothly in spite of transient downtime or the normal hiccups that occur in complex and cloud-based\ndeployments.', '291 CHAPTER 7 | Implement resilient applications', 'In distributed systems like microservices-based applications, there’s an ever-present risk of partial\nfailure. For instance, a single microservice/container can fail or might not be available to respond for a\nshort time, or a single VM or server can crash. Since clients and services are separate processes, a\nservice might not be able to respond in a timely way to a client’s request. The service might be\noverloaded and responding very slowly to requests or might simply not be accessible for a short time\nbecause of network issues.', 'For example, consider the Order details page from the eShopOnContainers sample application. If the\nordering microservice is unresponsive when the user tries to submit an order, a bad implementation\nof the client process (the MVC web application)—for example, if the client code were to use\nsynchronous RPCs with no timeout—would block threads indefinitely waiting for a response. Besides\ncreating a bad user experience, every unresponsive wait consumes or blocks a thread, and threads are\nextremely valuable in highly scalable applications. If there are many blocked threads, eventually the\napplication’s runtime can run out of threads. In that case, the application can become globally\nunresponsive instead of just partially unresponsive, as shown in Figure 8-1.', '_Figure 8-1. Partial failures because of dependencies that impact service thread availability_', 'In a large microservices-based application, any partial failure can be amplified, especially if most of\nthe internal microservices interaction is based on synchronous HTTP calls (which is considered an antipattern). Think about a system that receives millions of incoming calls per day. If your system has a\nbad design that’s based on long chains of synchronous HTTP calls, these incoming calls might result in\nmany more millions of outgoing calls (let’s suppose a ratio of 1:4) to dozens of internal microservices\nas synchronous dependencies. This situation is shown in Figure 8-2, especially dependency #3, that\nstarts a chain, calling dependency #4, which then calls #5.', '292 CHAPTER 7 | Implement resilient applications', '_Figure 8-2. The impact of having an incorrect design featuring long chains of HTTP requests_', 'Intermittent failure is guaranteed in a distributed and cloud-based system, even if every dependency\nitself has excellent availability. It’s a fact you need to consider.', 'If you do not design and implement techniques to ensure fault tolerance, even small downtimes can\nbe amplified. As an example, 50 dependencies each with 99.99% of availability would result in several\nhours of downtime each month because of this ripple effect. When a microservice dependency fails\nwhile handling a high volume of requests, that failure can quickly saturate all available request threads\nin each service and crash the whole application.', '_Figure 8-3. Partial failure amplified by microservices with long chains of synchronous HTTP calls_', '293 CHAPTER 7 | Implement resilient applications', 'To minimize this problem, in the section Asynchronous microservice integration enforce microservice’s\nautonomy, this guide encourages you to use asynchronous communication across the internal\nmicroservices.', 'In addition, it’s essential that you design your microservices and client applications to handle partial\nfailures—that is, to build resilient microservices and client applications.', 'To deal with partial failures, use one of the strategies described here.', '**Use asynchronous communication (for example, message-based communication) across**\n**internal microservices** . It’s highly advisable not to create long chains of synchronous HTTP calls\nacross the internal microservices because that incorrect design will eventually become the main cause\nof bad outages. On the contrary, except for the front-end communications between the client\napplications and the first level of microservices or fine-grained API Gateways, it’s recommended to use\nonly asynchronous (message-based) communication once past the initial request/response cycle,\nacross the internal microservices. Eventual consistency and event-driven architectures will help to\nminimize ripple effects. These approaches enforce a higher level of microservice autonomy and\ntherefore prevent against the problem noted here.', '**Use retries with exponential backoff** . This technique helps to avoid short and intermittent failures\nby performing call retries a certain number of times, in case the service was not available only for a\nshort time. This might occur due to intermittent network issues or when a microservice/container is\nmoved to a different node in a cluster. However, if these retries are not designed properly with circuit\n[breakers, it can aggravate the ripple effects, ultimately even causing a Denial of Service (DoS).](https://en.wikipedia.org/wiki/Denial-of-service_attack)', '**Work around network timeouts** . In general, clients should be designed not to block indefinitely and\nto always use timeouts when waiting for a response. Using timeouts ensures that resources are never\ntied up indefinitely.', '**Use the Circuit Breaker pattern** . In this approach, the client process tracks the number of failed\nrequests. If the error rate exceeds a configured limit, a “circuit breaker” trips so that further attempts\nfail immediately. (If a large number of requests are failing, that suggests the service is unavailable and\nthat sending requests is pointless.) After a timeout period, the client should try again and, if the new\nrequests are successful, close the circuit breaker.', '**Provide fallbacks** . In this approach, the client process performs fallback logic when a request fails,\nsuch as returning cached data or a default value. This is an approach suitable for queries, and is more\ncomplex for updates or commands.', '**Limit the number of queued requests** . Clients should also impose an upper bound on the number\nof outstanding requests that a client microservice can send to a particular service. If the limit has been\nreached, it’s probably pointless to make additional requests, and those attempts should fail\n[immediately. In terms of implementation, the Polly Bulkhead Isolation](https://github.com/App-vNext/Polly/wiki/Bulkhead) policy can be used to fulfill this\n[requirement. This approach is essentially a parallelization throttle with SemaphoreSlim as the](https://docs.microsoft.com/dotnet/api/system.threading.semaphoreslim)\nimplementation. It also permits a “queue” outside the bulkhead. You can proactively shed excess load\neven before execution (for example, because capacity is deemed full). This makes its response to', '294 CHAPTER 7 | Implement resilient applications', 'certain failure scenarios faster than a circuit breaker would be, since the circuit breaker waits for the\n[failures. The BulkheadPolicy object in Polly](https://thepollyproject.azurewebsites.net/) exposes how full the bulkhead and queue are, and offers\nevents on overflow so can also be used to drive automated horizontal scaling.', '_[Retries with exponential backoff](https://docs.microsoft.com/azure/architecture/patterns/retry)_ is a technique that retries an operation, with an exponentially\n[increasing wait time, up to a maximum retry count has been reached (the exponential backoff). This](https://en.wikipedia.org/wiki/Exponential_backoff)\ntechnique embraces the fact that cloud resources might intermittently be unavailable for more than a\nfew seconds for any reason. For example, an orchestrator might be moving a container to another\nnode in a cluster for load balancing. During that time, some requests might fail. Another example\ncould be a database like SQL Azure, where a database can be moved to another server for load\nbalancing, causing the database to be unavailable for a few seconds.', 'There are many approaches to implement retries logic with exponential backoff.', 'For Azure SQL DB, Entity Framework (EF) Core already provides internal database connection resiliency\n[and retry logic. But you need to enable the Entity Framework execution strategy for each DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext)\n[connection if you want to have resilient EF Core connections.](https://docs.microsoft.com/ef/core/miscellaneous/connection-resiliency)', 'For instance, the following code at the EF Core connection level enables resilient SQL connections that\nare retried if the connection fails.', '295 CHAPTER 7 | Implement resilient applications', 'When retries are enabled in EF Core connections, each operation you perform using EF Core becomes\nits own retryable operation. Each query and each call to SaveChanges will be retried as a unit if a\ntransient failure occurs.', 'However, if your code initiates a transaction using BeginTransaction, you’re defining your own group\nof operations that need to be treated as a unit. Everything inside the transaction has to be rolled back\nif a failure occurs.', 'If you try to execute that transaction when using an EF execution strategy (retry policy) and you call\nSaveChanges from multiple DbContexts, you’ll get an exception like this one:', 'System.InvalidOperationException: The configured execution strategy\n‘SqlServerRetryingExecutionStrategy’ does not support user initiated transactions. Use the execution\nstrategy returned by ‘DbContext.Database.CreateExecutionStrategy()’ to execute all the operations in\nthe transaction as a retriable unit.', 'The solution is to manually invoke the EF execution strategy with a delegate representing everything\nthat needs to be executed. If a transient failure occurs, the execution strategy will invoke the delegate\nagain. For example, the following code shows how it’s implemented in eShopOnContainers with two\nmultiple DbContexts (_catalogContext and the IntegrationEventLogContext) when updating a product\nand then saving the ProductPriceChangedIntegrationEvent object, which needs to use a different\nDbContext.', '296 CHAPTER 7 | Implement resilient applications', '[The first DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext) is _catalogContext and the second DbContext is within the\n_catalogIntegrationEventService object. The Commit action is performed across all DbContext objects\nusing an EF execution strategy.', 'To achieve this multiple DbContext commit, the SaveEventAndCatalogContextChangesAsync uses a\nResilientTransaction class, as shown in the following code:', 'The ResilientTransaction.ExecuteAsync method basically begins a transaction from the passed\nDbContext (_catalogContext) and then makes the EventLogService use that transaction to save\nchanges from the IntegrationEventLogContext and then commits the whole transaction.', '297 CHAPTER 7 | Implement resilient applications', '[IHttpClientFactory](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclientfactory) is a contract implemented by DefaultHttpClientFactory, an opinionated factory,\n[available since .NET Core 2.1, for creating HttpClient](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient) instances to be used in your applications.', '[The original and well-known HttpClient](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient) class can be easily used, but in some cases, it isn’t being\nproperly used by many developers.', 'Though this class implements IDisposable, declaring and instantiating it within a using statement is\nnot preferred because when the HttpClient object gets disposed of, the underlying socket is not\nimmediately released, which can lead to a _socket exhaustion_ problem. For more information about this\n[issue, see the blog post You’re using HttpClient wrong and it’s destabilizing your software.](https://aspnetmonsters.com/2016/08/2016-08-27-httpclientwrong/)', 'Therefore, HttpClient is intended to be instantiated once and reused throughout the life of an\napplication. Instantiating an HttpClient class for every request will exhaust the number of sockets\navailable under heavy loads. That issue will result in SocketException errors. Possible approaches to\nsolve that problem are based on the creation of the HttpClient object as singleton or static, as\n[explained in this Microsoft article on HttpClient usage. This can be a good solution for short-lived](https://docs.microsoft.com/dotnet/csharp/tutorials/console-webapiclient)\nconsole apps or similar, that run a few times a day.', '298 CHAPTER 7 | Implement resilient applications', 'Another issue that developers run into is when using a shared instance of HttpClient in long-running\nprocesses. In a situation where the HttpClient is instantiated as a singleton or a static object, it fails to\n[handle the DNS changes as described in this issue](https://github.com/dotnet/runtime/issues/18348) of the dotnet/runtime GitHub repository.', '[However, the issue isn’t really with HttpClient per se, but with the default constructor for HttpClient,](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient.-ctor#system-net-http-httpclient-ctor)\n[because it creates a new concrete instance of HttpMessageHandler, which is the one that has](https://docs.microsoft.com/dotnet/api/system.net.http.httpmessagehandler) _sockets_\n_exhaustion_ and DNS changes issues mentioned above.', 'To address the issues mentioned above and to make HttpClient instances manageable, .NET Core 2.1\n[introduced two approaches, one of them being IHttpClientFactory. It’s an interface that’s used to](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclientfactory)\nconfigure and create HttpClient instances in an app through Dependency Injection (DI). It also\nprovides extensions for Polly-based middleware to take advantage of delegating handlers in\nHttpClient.', 'The alternative is to use SocketsHttpHandler with configured PooledConnectionLifetime. This\napproach is applied to long-lived, static or singleton HttpClient instances. To learn more about\n[different strategies, see HttpClient guidelines for .NET.](https://docs.microsoft.com/dotnet/fundamentals/networking/http/httpclient-guidelines)', '[Polly](https://thepollyproject.azurewebsites.net/) is a transient-fault-handling library that helps developers add resiliency to their applications, by\nusing some pre-defined policies in a fluent and thread-safe manner.', '[The current implementation of IHttpClientFactory, that also implements IHttpMessageHandlerFactory,](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclientfactory)\noffers the following benefits:', 'There are several ways that you can use IHttpClientFactory in your application:', 'So, what’s a “Typed Client”? It’s just an HttpClient that’s pre-configured for some specific use. This\nconfiguration can include specific values such as the base server, HTTP headers or time outs.', 'The following diagram shows how Typed Clients are used with IHttpClientFactory:', '300 CHAPTER 7 | Implement resilient applications', '_Figure 8-4. Using IHttpClientFactory with Typed Client classes._', 'In the above image, a ClientService (used by a controller or client code) uses an HttpClient created by\nthe registered IHttpClientFactory. This factory assigns an HttpMessageHandler from a pool to the\nHttpClient. The HttpClient can be configured with Polly’s policies when registering the\n[IHttpClientFactory in the DI container with the extension method AddHttpClient.](https://docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.httpclientfactoryservicecollectionextensions.addhttpclient)', '[To configure the above structure, add IHttpClientFactory](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclientfactory) in your application by installing the\n[Microsoft.Extensions.Http NuGet package that includes the AddHttpClient](https://docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.httpclientfactoryservicecollectionextensions.addhttpclient) extension method for\n[IServiceCollection. This extension method registers the internal DefaultHttpClientFactory class to be](https://docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.iservicecollection)\nused as a singleton for the interface IHttpClientFactory. It defines a transient configuration for the\n[HttpMessageHandlerBuilder. This message handler (HttpMessageHandler](https://docs.microsoft.com/dotnet/api/microsoft.extensions.http.httpmessagehandlerbuilder) object), taken from a pool,\nis used by the HttpClient returned from the factory.', 'In the next snippet, you can see how AddHttpClient() can be used to register Typed Clients (Service\nAgents) that need to use HttpClient.', '301 CHAPTER 7 | Implement resilient applications', 'Registering the client services as shown in the previous snippet, makes the DefaultClientFactory create\na standard HttpClient for each service. The typed client is registered as transient with DI container. In\nthe preceding code, AddHttpClient() registers _CatalogService_, _BasketService_, _OrderingService_ as\ntransient services so they can be injected and consumed directly without any need for additional\nregistrations.', 'You could also add instance-specific configuration in the registration to, for example, configure the\nbase address, and add some resiliency policies, as shown in the following:', 'In this next example, you can see the configuration of one of the above policies:', 'You can find more details about using Polly in the Next article.', '**HttpClient lifetimes**', 'Each time you get an HttpClient object from the IHttpClientFactory, a new instance is returned. But\neach HttpClient uses an HttpMessageHandler that’s pooled and reused by the IHttpClientFactory to\nreduce resource consumption, as long as the HttpMessageHandler’s lifetime hasn’t expired.', 'Pooling of handlers is desirable as each handler typically manages its own underlying HTTP\nconnections; creating more handlers than necessary can result in connection delays. Some handlers\nalso keep connections open indefinitely, which can prevent the handler from reacting to DNS\nchanges.', 'The HttpMessageHandler objects in the pool have a lifetime that’s the length of time that an\nHttpMessageHandler instance in the pool can be reused. The default value is two minutes, but it can\n[be overridden per Typed Client. To override it, call SetHandlerLifetime() on the IHttpClientBuilder](https://docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.ihttpclientbuilder)\nthat’s returned when creating the client, as shown in the following code:', 'Each Typed Client can have its own configured handler lifetime value. Set the lifetime to\nInfiniteTimeSpan to disable handler expiry.', '302 CHAPTER 7 | Implement resilient applications', '**Implement your Typed Client classes that use the injected and configured**\n**HttpClient**', 'As a previous step, you need to have your Typed Client classes defined, such as the classes in the\nsample code, like ‘BasketService’, ‘CatalogService’, ‘OrderingService’, etc. – A Typed Client is a class\nthat accepts an HttpClient object (injected through its constructor) and uses it to call some remote\nHTTP service. For example:', 'The Typed Client (CatalogService in the example) is activated by DI (Dependency Injection), which\nmeans it can accept any registered service in its constructor, in addition to HttpClient.', 'A Typed Client is effectively a transient object, that means a new instance is created each time one is\nneeded. It receives a new HttpClient instance each time it’s constructed. However, the\nHttpMessageHandler objects in the pool are the objects that are reused by multiple HttpClient\ninstances.', '**Use your Typed Client classes**', 'Finally, once you have your typed classes implemented, you can have them registered and configured\nwith AddHttpClient(). After that you can use them wherever services are injected by DI, such as in\nRazor page code or an MVC web app controller, shown in the below code from eShopOnContainers:', '303 CHAPTER 7 | Implement resilient applications', 'Up to this point, the above code snippet only shows the example of performing regular HTTP\nrequests. But the ‘magic’ comes in the following sections where it shows how all the HTTP requests\nmade by HttpClient can have resilient policies such as retries with exponential backoff, circuit\nbreakers, security features using auth tokens, or even any other custom feature. And all of these can\nbe done just by adding policies and delegating handlers to your registered Typed Clients.', 'The recommended approach for retries with exponential backoff is to take advantage of more\n[advanced .NET libraries like the open-source Polly library.](https://github.com/App-vNext/Polly)', 'Polly is a .NET library that provides resilience and transient-fault handling capabilities. You can\nimplement those capabilities by applying Polly policies such as Retry, Circuit Breaker, Bulkhead\nIsolation, Timeout, and Fallback. Polly targets .NET Framework 4.x and .NET Standard 1.0, 1.1, and 2.0\n(which supports .NET Core and later).', '304 CHAPTER 7 | Implement resilient applications', 'The following steps show how you can use Http retries with Polly integrated into IHttpClientFactory,\nwhich is explained in the previous section.', '**Install .NET packages**', 'First, you will need to install the Microsoft.Extensions.Http.Polly package.', 'A regular Retry policy can affect your system in cases of high concurrency and scalability and under\nhigh contention. To overcome peaks of similar retries coming from many clients in partial outages, a\ngood workaround is to add a jitter strategy to the retry algorithm/policy. This strategy can improve\n[the overall performance of the end-to-end system. As recommended in Polly: Retry with Jitter, a good](https://github.com/App-vNext/Polly/wiki/Retry-with-jitter)', '305 CHAPTER 7 | Implement resilient applications', 'jitter strategy can be implemented by smooth and evenly distributed retry intervals applied with a\nwell-controlled median initial retry delay on an exponential backoff. This approach helps to spread out\nthe spikes when the issue arises. The principle is illustrated by the following example:', 'As noted earlier, you should handle faults that might take a variable amount of time to recover from,\nas might happen when you try to connect to a remote service or resource. Handling this type of fault\ncan improve the stability and resiliency of an application.', 'In a distributed environment, calls to remote resources and services can fail due to transient faults,\nsuch as slow network connections and timeouts, or if resources are responding slowly or are\ntemporarily unavailable. These faults typically correct themselves after a short time, and a robust cloud\napplication should be prepared to handle them by using a strategy like the “Retry pattern”.', 'However, there can also be situations where faults are due to unanticipated events that might take\nmuch longer to fix. These faults can range in severity from a partial loss of connectivity to the\ncomplete failure of a service. In these situations, it might be pointless for an application to continually\nretry an operation that’s unlikely to succeed.', 'Instead, the application should be coded to accept that the operation has failed and handle the failure\naccordingly.', '[Using Http retries carelessly could result in creating a Denial of Service (DoS) attack within your own](https://en.wikipedia.org/wiki/Denial-of-service_attack)\nsoftware. As a microservice fails or performs slowly, multiple clients might repeatedly retry failed\nrequests. That creates a dangerous risk of exponentially increasing traffic targeted at the failing\nservice.', '306 CHAPTER 7 | Implement resilient applications', 'Therefore, you need some kind of defense barrier so that excessive requests stop when it isn’t worth\nto keep trying. That defense barrier is precisely the circuit breaker.', 'The Circuit Breaker pattern has a different purpose than the “Retry pattern”. The “Retry pattern”\nenables an application to retry an operation in the expectation that the operation will eventually\nsucceed. The Circuit Breaker pattern prevents an application from performing an operation that’s\nlikely to fail. An application can combine these two patterns. However, the retry logic should be\nsensitive to any exception returned by the circuit breaker, and it should abandon retry attempts if the\ncircuit breaker indicates that a fault is not transient.', 'As when implementing retries, the recommended approach for circuit breakers is to take advantage of\nproven .NET libraries like Polly and its native integration with IHttpClientFactory.', 'Adding a circuit breaker policy into your IHttpClientFactory outgoing middleware pipeline is as simple\nas adding a single incremental piece of code to what you already have when using IHttpClientFactory.', 'The only addition here to the code used for HTTP call retries is the code where you add the Circuit\nBreaker policy to the list of policies to use, as shown in the following incremental code.', 'The AddPolicyHandler() method is what adds policies to the HttpClient objects you’ll use. In this case,\nit’s adding a Polly policy for a circuit breaker.', 'To have a more modular approach, the Circuit Breaker Policy is defined in a separate method called\nGetCircuitBreakerPolicy(), as shown in the following code:', 'In the code example above, the circuit breaker policy is configured so it breaks or opens the circuit\nwhen there have been five consecutive faults when retrying the Http requests. When that happens,\nthe circuit will break for 30 seconds: in that period, calls will be failed immediately by the circuit[breaker rather than actually be placed. The policy automatically interprets relevant exceptions and](https://docs.microsoft.com/aspnet/core/fundamentals/http-requests#handle-transient-faults)\n[HTTP status codes](https://docs.microsoft.com/aspnet/core/fundamentals/http-requests#handle-transient-faults) as faults.', 'Circuit breakers should also be used to redirect requests to a fallback infrastructure if you had issues\nin a particular resource that’s deployed in a different environment than the client application or', '307 CHAPTER 7 | Implement resilient applications', 'service that’s performing the HTTP call. That way, if there’s an outage in the datacenter that impacts\nonly your backend microservices but not your client applications, the client applications can redirect\n[to the fallback services. Polly is planning a new policy to automate this failover policy scenario.](https://github.com/App-vNext/Polly/wiki/Polly-Roadmap#failover-policy)', 'All those features are for cases where you’re managing the failover from within the .NET code, as\nopposed to having it managed automatically for you by Azure, with location transparency.', 'From a usage point of view, when using HttpClient, there’s no need to add anything new here\nbecause the code is the same than when using HttpClient with IHttpClientFactory, as shown in\nprevious sections.', 'Whenever you start the eShopOnContainers solution in a Docker host, it needs to start multiple\ncontainers. Some of the containers are slower to start and initialize, like the SQL Server container. This\nis especially true the first time you deploy the eShopOnContainers application into Docker because it\nneeds to set up the images and the database. The fact that some containers start slower than others\ncan cause the rest of the services to initially throw HTTP exceptions, even if you set dependencies\nbetween containers at the docker-compose level, as explained in previous sections. Those dockercompose dependencies between containers are just at the process level. The container’s entry point\nprocess might be started, but SQL Server might not be ready for queries. The result can be a cascade\nof errors, and the application can get an exception when trying to consume that particular container.', 'You might also see this type of error on startup when the application is deploying to the cloud. In that\ncase, orchestrators might be moving containers from one node or VM to another (that is, starting new\ninstances) when balancing the number of containers across the cluster’s nodes.', 'The way ‘eShopOnContainers’ solves those issues when starting all the containers is by using the Retry\npattern illustrated earlier.', '**Test the circuit breaker in eShopOnContainers**', 'There are a few ways you can break/open the circuit and test it with eShopOnContainers.', 'One option is to lower the allowed number of retries to 1 in the circuit breaker policy and redeploy\nthe whole solution into Docker. With a single retry, there’s a good chance that an HTTP request will\nfail during deployment, the circuit breaker will open, and you get an error.', 'Another option is to use custom middleware that’s implemented in the **Basket** microservice. When\nthis middleware is enabled, it catches all HTTP requests and returns status code 500. You can enable\nthe middleware by making a GET request to the failing URI, like the following:', 'Health monitoring can allow near-real-time information about the state of your containers and\nmicroservices. Health monitoring is critical to multiple aspects of operating microservices and is\nespecially important when orchestrators perform partial application upgrades in phases, as explained\nlater.', 'Microservices-based applications often use heartbeats or health checks to enable their performance\nmonitors, schedulers, and orchestrators to keep track of the multitude of services. If services cannot\nsend some sort of “I’m alive” signal, either on demand or on a schedule, your application might face\nrisks when you deploy updates, or it might just detect failures too late and not be able to stop\ncascading failures that can end up in major outages.', 'In the typical model, services send reports about their status, and that information is aggregated to\nprovide an overall view of the state of health of your application. If you’re using an orchestrator, you\ncan provide health information to your orchestrator’s cluster, so that the cluster can act accordingly. If\nyou invest in high-quality health reporting that’s customized for your application, you can detect and\nfix issues for your running application much more easily.', '310 CHAPTER 7 | Implement resilient applications', 'When developing an ASP.NET Core microservice or web application, you can use the built-in health\nchecks feature that was released in ASP .NET Core 2.2\n[(Microsoft.Extensions.Diagnostics.HealthChecks). Like many ASP.NET Core features, health checks](https://www.nuget.org/packages/Microsoft.Extensions.Diagnostics.HealthChecks)\ncome with a set of services and a middleware.', 'Health check services and middleware are easy to use and provide capabilities that let you validate if\nany external resource needed for your application (like a SQL Server database or a remote API) is\nworking properly. When you use this feature, you can also decide what it means that the resource is\nhealthy, as we explain later.', 'To use this feature effectively, you need to first configure services in your microservices. Second, you\nneed a front-end application that queries for the health reports. That front-end application could be a\ncustom reporting application, or it could be an orchestrator itself that can react accordingly to the\nhealth states.', '**Use the HealthChecks feature in your back-end ASP.NET microservices**', 'In this section, you’ll learn how to implement the HealthChecks feature in a sample ASP.NET Core 7.0\n[Web API application when using the Microsoft.Extensions.Diagnostics.HealthChecks](https://www.nuget.org/packages/Microsoft.Extensions.Diagnostics.HealthChecks) package. The\nImplementation of this feature in a large-scale microservices like the eShopOnContainers is explained\nin the next section.', 'To begin, you need to define what constitutes a healthy status for each microservice. In the sample\napplication, we define the microservice is healthy if its API is accessible via HTTP and its related SQL\nServer database is also available.', 'In .NET 7, with the built-in APIs, you can configure the services, add a Health Check for the\nmicroservice and its dependent SQL Server database in this way:', 'In the previous code, the services.AddHealthChecks() method configures a basic HTTP check that\nreturns a status code **200** with “Healthy”. Further, the AddCheck() extension method configures a\ncustom SqlConnectionHealthCheck that checks the related SQL Database’s health.', 'The AddCheck() method adds a new health check with a specified name and the implementation of\ntype IHealthCheck. You can add multiple Health Checks using AddCheck method, so a microservice\nwon’t provide a “healthy” status until all its checks are healthy.', '311 CHAPTER 7 | Implement resilient applications', 'SqlConnectionHealthCheck is a custom class that implements IHealthCheck, which takes a connection\nstring as a constructor parameter and executes a simple query to check if the connection to the SQL\ndatabase is successful. It returns HealthCheckResult.Healthy() if the query was executed successfully\nand a FailureStatus with the actual exception when it fails.', 'Note that in the previous code, Select 1 is the query used to check the Health of the database. To\nmonitor the availability of your microservices, orchestrators like Kubernetes periodically perform\nhealth checks by sending requests to test the microservices. It’s important to keep your database\nqueries efficient so that these operations are quick and don’t result in a higher utilization of resources.', '312 CHAPTER 7 | Implement resilient applications', 'Finally, add a middleware that responds to the url path /hc:', 'When the endpoint <yourmicroservice>/hc is invoked, it runs all the health checks that are configured\nin the AddHealthChecks() method in the Startup class and shows the result.', '**HealthChecks implementation in eShopOnContainers**', 'Microservices in eShopOnContainers rely on multiple services to perform its task. For example, the\nCatalog.API microservice from eShopOnContainers depends on many services, such as Azure Blob\nStorage, SQL Server, and RabbitMQ. Therefore, it has several health checks added using the\nAddCheck() method. For every dependent service, a custom IHealthCheck implementation that\ndefines its respective health status would need to be added.', '[The open-source project AspNetCore.Diagnostics.HealthChecks](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks) solves this problem by providing\ncustom health check implementations for each of these enterprise services, that are built on top of\n.NET 7. Each health check is available as an individual NuGet package that can be easily added to the\nproject. eShopOnContainers uses them extensively in all its microservices.', 'For instance, in the Catalog.API microservice, the following NuGet packages were added:', '_Figure 8-7. Custom Health Checks implemented in Catalog.API using AspNetCore.Diagnostics.HealthChecks_', 'In the following code, the health check implementations are added for each dependent service and\nthen the middleware is configured:', '313 CHAPTER 7 | Implement resilient applications', 'Finally, add the HealthCheck middleware to listen to “/hc” endpoint:', '**Query your microservices to report about their health status**', 'When you’ve configured health checks as described in this article and you have the microservice\nrunning in Docker, you can directly check from a browser if it’s healthy. You have to publish the\ncontainer port in the Docker host, so you can access the container through the external Docker host IP\nor through host.docker.internal, as shown in figure 8-8.', '314 CHAPTER 7 | Implement resilient applications', '_Figure 8-8. Checking health status of a single service from a browser_', 'In that test, you can see that the Catalog.API microservice (running on port 5101) is healthy, returning\nHTTP status 200 and status information in JSON. The service also checked the health of its SQL Server\ndatabase dependency and RabbitMQ, so the health check reported itself as healthy.', 'A watchdog is a separate service that can watch health and load across services, and report health\nabout the microservices by querying with the HealthChecks library introduced earlier. This can help\nprevent errors that would not be detected based on the view of a single service. Watchdogs also are a\ngood place to host code that can perform remediation actions for known conditions without user\ninteraction.', 'The eShopOnContainers sample contains a web page that displays sample health check reports, as\nshown in Figure 8-9. This is the simplest watchdog you could have since it only shows the state of the\nmicroservices and web applications in eShopOnContainers. Usually a watchdog also takes actions\nwhen it detects unhealthy states.', '[Fortunately, AspNetCore.Diagnostics.HealthChecks also provides AspNetCore.HealthChecks.UI NuGet](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks)\npackage that can be used to display the health check results from the configured URIs.', '315 CHAPTER 7 | Implement resilient applications', '_Figure 8-9. Sample health check report in eShopOnContainers_', 'In summary, this watchdog service queries each microservice’s “/hc” endpoint. This will execute all the\nhealth checks defined within it and return an overall health state depending on all those checks. The\nHealthChecksUI is easy to consume with a few configuration entries and two lines of code that needs\nto be added into the _Startup.cs_ of the watchdog service.', 'Sample configuration file for health check UI:', '_Program.cs_ file that adds HealthChecksUI:', '316 CHAPTER 7 | Implement resilient applications', 'To monitor the availability of your microservices, orchestrators like Kubernetes and Service Fabric\nperiodically perform health checks by sending requests to test the microservices. When an\norchestrator determines that a service/container is unhealthy, it stops routing requests to that\ninstance. It also usually creates a new instance of that container.', 'For instance, most orchestrators can use health checks to manage zero-downtime deployments. Only\nwhen the status of a service/container changes to healthy will the orchestrator start routing traffic to\nservice/container instances.', 'Health monitoring is especially important when an orchestrator performs an application upgrade.\nSome orchestrators (like Azure Service Fabric) update services in phases—for example, they might\nupdate one-fifth of the cluster surface for each application upgrade. The set of nodes that’s upgraded\nat the same time is referred to as an _upgrade domain_ . After each upgrade domain has been upgraded\nand is available to users, that upgrade domain must pass health checks before the deployment moves\nto the next upgrade domain.', 'Another aspect of service health is reporting metrics from the service. This is an advanced capability of\nthe health model of some orchestrators, like Service Fabric. Metrics are important when using an\norchestrator because they are used to balance resource usage. Metrics also can be an indicator of\nsystem health. For example, you might have an application that has many microservices, and each\ninstance reports a requests-per-second (RPS) metric. If one service is using more resources (memory,\nprocessor, etc.) than another service, the orchestrator could move service instances around in the\ncluster to try to maintain even resource utilization.', '[Note that Azure Service Fabric provides its own Health Monitoring model, which is more advanced](https://docs.microsoft.com/azure/service-fabric/service-fabric-health-introduction)\nthan simple health checks.', 'The final part of monitoring is visualizing the event stream, reporting on service performance, and\nalerting when an issue is detected. You can use different solutions for this aspect of monitoring.', 'You can use simple custom applications showing the state of your services, like the custom page\n[shown when explaining the AspNetCore.Diagnostics.HealthChecks. Or you could use more advanced](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks)\n[tools like Azure Monitor](https://azure.microsoft.com/services/monitor/) to raise alerts based on the stream of events.', 'Finally, if you’re storing all the event streams, you can use Microsoft Power BI or other solutions like\nKibana or Splunk to visualize the data.', '317 CHAPTER 7 | Implement resilient applications', 'There are so many aspects about security in microservices and web applications that the topic could\neasily take several books like this one. So, in this section, we’ll focus on authentication, authorization,\nand application secrets.', 'It’s often necessary for resources and APIs published by a service to be limited to certain trusted users\nor clients. The first step to making these sorts of API-level trust decisions is authentication.\nAuthentication is the process of reliably verifying a user’s identity.', 'In microservice scenarios, authentication is typically handled centrally. If you’re using an API Gateway,\nthe gateway is a good place to authenticate, as shown in Figure 9-1. If you use this approach, make\nsure that the individual microservices cannot be reached directly (without the API Gateway) unless\nadditional security is in place to authenticate messages whether they come from the gateway or not.', '_Figure 9-1. Centralized authentication with an API Gateway_', 'When the API Gateway centralizes authentication, it adds user information when forwarding requests\nto the microservices. If services can be accessed directly, an authentication service like Azure Active', '319 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'Directory or a dedicated authentication microservice acting as a security token service (STS) can be\nused to authenticate users. Trust decisions are shared between services with security tokens or\ncookies. (These tokens can be shared between ASP.NET Core applications, if needed, by implementing\n[cookie sharing.) This pattern is illustrated in Figure 9-2.](https://docs.microsoft.com/aspnet/core/security/cookie-sharing)', '_Figure 9-2. Authentication by identity microservice; trust is shared using an authorization token_', 'When microservices are accessed directly, trust, that includes authentication and authorization, is\nhandled by a security token issued by a dedicated microservice, shared between microservices.', '[The primary mechanism in ASP.NET Core for identifying an application’s users is the ASP.NET Core](https://docs.microsoft.com/aspnet/core/security/authentication/identity)\n[Identity](https://docs.microsoft.com/aspnet/core/security/authentication/identity) membership system. ASP.NET Core Identity stores user information (including sign-in\ninformation, roles, and claims) in a data store configured by the developer. Typically, the ASP.NET\nCore Identity data store is an Entity Framework store provided in the\nMicrosoft.AspNetCore.Identity.EntityFrameworkCore package. However, custom stores or other thirdparty packages can be used to store identity information in Azure Table Storage, CosmosDB, or other\nlocations.', 'The following code is taken from the ASP.NET Core Web Application MVC 3.1 project template with\nindividual user account authentication selected. It shows how to configure ASP.NET Core Identity\nusing Entity Framework Core in the _Program.cs_ file.', '320 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'Using ASP.NET Core Identity enables several scenarios:', '[ASP.NET Core also supports using external authentication providers](https://docs.microsoft.com/aspnet/core/security/authentication/social/) [to let users sign in via OAuth 2.0](https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2)\nflows. This means that users can sign in using existing authentication processes from providers like\nMicrosoft, Google, Facebook, or Twitter and associate those identities with an ASP.NET Core identity\nin your application.', 'To use external authentication, besides including the authentication middleware as mentioned before,\nusing the app.UseAuthentication() method, you also have to register the external provider in\n_Program.cs_ as shown in the following example:', '321 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'Popular external authentication providers and their associated NuGet packages are shown in the', 'In all cases, you must complete an application registration procedure that is vendor dependent and\nthat usually involves:', 'Authenticating with ASP.NET Core Identity (or Identity plus external authentication providers) works\nwell for many web application scenarios in which storing user information in a cookie is appropriate.\nIn other scenarios, though, cookies are not a natural means of persisting and transmitting data.', 'For example, in an ASP.NET Core Web API that exposes RESTful endpoints that might be accessed by\nSingle Page Applications (SPAs), by native clients, or even by other Web APIs, you typically want to\nuse bearer token authentication instead. These types of applications do not work with cookies, but\ncan easily retrieve a bearer token and include it in the authorization header of subsequent requests.\n[To enable token authentication, ASP.NET Core supports several options for using OAuth 2.0 and](https://oauth.net/2/)\n[OpenID Connect.](https://openid.net/connect/)', '323 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'If user information is stored in Azure Active Directory or another identity solution that supports\nOpenID Connect or OAuth 2.0, you can use the\n**Microsoft.AspNetCore.Authentication.OpenIdConnect** package to authenticate using the OpenID\nConnect workflow. For example, to authenticate to the Identity.Api microservice in\neShopOnContainers, an ASP.NET Core web application can use middleware from that package as\nshown in the following simplified example in _Program.cs_ :', 'When you use this workflow, the ASP.NET Core Identity middleware is not needed, because all user\ninformation storage and authentication is handled by the Identity service.', '324 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'If you prefer to issue security tokens for local ASP.NET Core Identity users rather than using an\nexternal identity provider, you can take advantage of some good third-party libraries.', '[IdentityServer4](https://github.com/IdentityServer/IdentityServer4) [and OpenIddict are OpenID Connect providers that integrate easily with ASP.NET Core](https://github.com/openiddict/openiddict-core)\n[Identity to let you issue security tokens from an ASP.NET Core service. The IdentityServer4](https://identityserver4.readthedocs.io/en/latest/)\n[documentation](https://identityserver4.readthedocs.io/en/latest/) has in-depth instructions for using the library. However, the basic steps to using\nIdentityServer4 to issue tokens are as follows.', 'Authenticating against an OpenID Connect endpoint or issuing your own security tokens covers some\nscenarios. But what about a service that simply needs to limit access to those users who have valid\nsecurity tokens that were provided by a different service?', 'For that scenario, authentication middleware that handles JWT tokens is available in the\n**Microsoft.AspNetCore.Authentication.JwtBearer** [package. JWT stands for “JSON Web Token” and](https://tools.ietf.org/html/rfc7519)\nis a common security token format (defined by RFC 7519) for communicating security claims. A\nsimplified example of how to use middleware to consume such tokens might look like this code\nfragment, taken from the Ordering.Api microservice of eShopOnContainers.', 'The parameters in this usage are:', 'After authentication, ASP.NET Core Web APIs need to authorize access. This process allows a service\n[to make APIs available to some authenticated users, but not to all. Authorization](https://docs.microsoft.com/aspnet/core/security/authorization/introduction) can be done based\non users’ roles or based on custom policy, which might include inspecting claims or other heuristics.', 'Restricting access to an ASP.NET Core MVC route is as easy as applying an Authorize attribute to the\naction method (or to the controller’s class if all the controller’s actions require authorization), as\nshown in following example:', '327 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'By default, adding an Authorize attribute without parameters will limit access to authenticated users\nfor that controller or action. To further restrict an API to be available for only specific users, the\nattribute can be expanded to specify required roles or policies that users must satisfy.', 'ASP.NET Core Identity has a built-in concept of roles. In addition to users, ASP.NET Core Identity\nstores information about different roles used by the application and keeps track of which users are\nassigned to which roles. These assignments can be changed programmatically with the RoleManager\ntype that updates roles in persisted storage, and the UserManager type that can grant or revoke roles\nfrom users.', 'If you’re authenticating with JWT bearer tokens, the ASP.NET Core JWT bearer authentication\nmiddleware will populate a user’s roles based on role claims found in the token. To limit access to an\nMVC action or controller to users in specific roles, you can include a Roles parameter in the Authorize\nannotation (attribute), as shown in the following code fragment:', 'In this example, only users in the Administrator or PowerUser roles can access APIs in the\nControlPanel controller (such as executing the SetTime action). The ShutDown API is further restricted\nto allow access only to users in the Administrator role.', 'To require a user be in multiple roles, you use multiple Authorize attributes, as shown in the following\nexample:', 'In this example, to call API1, a user must:', '[Custom authorization rules can also be written using authorization policies. This section provides an](https://docs.asp.net/en/latest/security/authorization/policies.html)\n[overview. For more information, see the ASP.NET Authorization Workshop.](https://github.com/blowdart/AspNetAuthorizationWorkshop)', 'Custom authorization policies are registered in the Startup.ConfigureServices method using the\nservice.AddAuthorization method. This method takes a delegate that configures an\nAuthorizationOptions argument.', 'As shown in the example, policies can be associated with different types of requirements. After the\npolicies are registered, they can be applied to an action or controller by passing the policy’s name as\nthe Policy argument of the Authorize attribute (for example, [Authorize(Policy="EmployeesOnly")])\nPolicies can have multiple requirements, not just one (as shown in these examples).', 'In the previous example, the first AddPolicy call is just an alternative way of authorizing by role. If', '[Authorize(Policy="AdministratorsOnly")] is applied to an API, only users in the Administrator role will\nbe able to access it.', '[The second AddPolicy](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.authorization.authorizationoptions.addpolicy) call demonstrates an easy way to require that a particular claim should be\n[present for the user. The RequireClaim](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.authorization.authorizationpolicybuilder.requireclaim) method also optionally takes expected values for the claim. If\nvalues are specified, the requirement is met only if the user has both a claim of the correct type and\none of the specified values. If you’re using the JWT bearer authentication middleware, all JWT\nproperties will be available as user claims.', 'The most interesting policy shown here is in the third AddPolicy method, because it uses a custom\nauthorization requirement. By using custom authorization requirements, you can have a great deal of\ncontrol over how authorization is performed. For this to work, you must implement these types:', 'ASP.NET supports minimal APIs as an alternative to controller-based APIs. Authorization policies are\nthe recommended way to configure authorization for minimal APIs, as this example demonstrates:', 'To connect with protected resources and other services, ASP.NET Core applications typically need to\nuse connection strings, passwords, or other credentials that contain sensitive information. These\nsensitive pieces of information are called _secrets_ . It’s a best practice to not include secrets in source', '330 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'code and making sure not to store secrets in source control. Instead, you should use the ASP.NET\nCore configuration model to read the secrets from more secure locations.', 'You must separate the secrets for accessing development and staging resources from the ones used\nfor accessing production resources, because different individuals will need access to those different\nsets of secrets. To store secrets used during development, common approaches are to either store\nsecrets in environment variables or by using the ASP.NET Core Secret Manager tool. For more secure\nstorage in production environments, microservices can store secrets in an Azure Key Vault.', 'One way to keep secrets out of source code is for developers to set string-based secrets as\n[environment variables](https://docs.microsoft.com/aspnet/core/security/app-secrets#environment-variables) on their development machines. When you use environment variables to store\nsecrets with hierarchical names, such as the ones nested in configuration sections, you must name the\nvariables to include the complete hierarchy of its sections, delimited with colons (:).', 'For example, setting an environment variable Logging:LogLevel:Default to Debug value would be\nequivalent to a configuration value from the following JSON file:', 'To access these values from environment variables, the application just needs to call\nAddEnvironmentVariables on its ConfigurationBuilder when constructing an IConfigurationRoot\nobject.', '[The ASP.NET Core Secret Manager tool provides another method of keeping secrets out of source](https://docs.microsoft.com/aspnet/core/security/app-secrets#secret-manager)\ncode **during development** . To use the Secret Manager tool, install the package\n**Microsoft.Extensions.Configuration.SecretManager** in your project file. Once that dependency is\npresent and has been restored, the dotnet user-secrets command can be used to set the value of\nsecrets from the command line. These secrets will be stored in a JSON file in the user’s profile\ndirectory (details vary by OS), away from source code.', 'Secrets set by the Secret Manager tool are organized by the UserSecretsId property of the project\nthat’s using the secrets. Therefore, you must be sure to set the UserSecretsId property in your project\nfile, as shown in the snippet below. The default value is a GUID assigned by Visual Studio, but the\nactual string is not important as long as it’s unique in your computer.', '331 CHAPTER 8 | Make secure .NET Microservices and Web Applications', 'Using secrets stored with Secret Manager in an application is accomplished by calling\nAddUserSecrets<T> on the ConfigurationBuilder instance to include secrets for the application in its\nconfiguration. The generic parameter T should be a type from the assembly that the UserSecretId was\napplied to. Usually, using AddUserSecrets<Startup> is fine.', 'The AddUserSecrets<Startup>() is included in the default options for the Development environment\nwhen using the CreateDefaultBuilder method in _Program.cs_ .', 'Secrets stored as environment variables or stored by the Secret Manager tool are still stored locally\n[and unencrypted on the machine. A more secure option for storing secrets is Azure Key Vault, which](https://azure.microsoft.com/services/key-vault/)\nprovides a secure, central location for storing keys and secrets.', 'The **Azure.Extensions.AspNetCore.Configuration.Secrets** package allows an ASP.NET Core\napplication to read configuration information from Azure Key Vault. To start using secrets from an\nAzure Key Vault, you follow these steps:', 'As a summary and key takeaways, the following are the most important conclusions from this guide.', '**Benefits of using containers.** Container-based solutions provide important cost savings because\nthey help reduce deployment problems caused by failing dependencies in production environments.\nContainers significantly improve DevOps and production operations.', '**Containers will be ubiquitous.** Docker-based containers are becoming the de facto standard in the\nindustry, supported by key vendors in the Windows and Linux ecosystems, such as Microsoft, Amazon\nAWS, Google, and IBM. Docker will probably soon be ubiquitous in both the cloud and on-premises\ndatacenters.', '**Containers as a unit of deployment.** A Docker container is becoming the standard unit of\ndeployment for any server-based application or service.', '**Microservices.** The microservices architecture is becoming the preferred approach for distributed and\nlarge or complex mission-critical applications based on many independent subsystems in the form of\nautonomous services. In a microservice-based architecture, the application is built as a collection of\nservices that are developed, tested, versioned, deployed, and scaled independently. Each service can\ninclude any related autonomous database.', '**Domain-driven design and SOA.** The microservices architecture patterns derive from serviceoriented architecture (SOA) and domain-driven design (DDD). When you design and develop\nmicroservices for environments with evolving business needs and rules, it’s important to consider DDD\napproaches and patterns.', '**Microservices challenges.** Microservices offer many powerful capabilities, like independent\ndeployment, strong subsystem boundaries, and technology diversity. However, they also raise many\nnew challenges related to distributed application development, such as fragmented and independent\ndata models, resilient communication between microservices, eventual consistency, and operational\ncomplexity that results from aggregating logging and monitoring information from multiple\nmicroservices. These aspects introduce a much higher complexity level than a traditional monolithic\napplication. As a result, only specific scenarios are suitable for microservice-based applications. These\ninclude large and complex applications with multiple evolving subsystems. In these cases, it’s worth\ninvesting in a more complex software architecture, because it will provide better long-term agility and\napplication maintenance.', '334 CHAPTER 9 | .NET Microservices Architecture key takeaways', '**Containers for any application.** Containers are convenient for microservices, but can also be useful\nfor monolithic applications based on the traditional .NET Framework, when using Windows\nContainers. The benefits of using Docker, such as solving many deployment-to-production issues and\nproviding state-of-the-art Dev and Test environments, apply to many different types of applications.', '**CLI versus IDE.** With Microsoft tools, you can develop containerized .NET applications using your\npreferred approach. You can develop with a CLI and an editor-based environment by using the Docker\nCLI and Visual Studio Code. Or you can use an IDE-focused approach with Visual Studio and its unique\nfeatures for Docker, such as multi-container debugging.', '**Resilient cloud applications.** In cloud-based systems and distributed systems in general, there is\nalways the risk of partial failure. Since clients and services are separate processes (containers), a\nservice might not be able to respond in a timely way to a client’s request. For example, a service might\nbe down because of a partial failure or for maintenance; the service might be overloaded and\nresponding slowly to requests; or it might not be accessible for a short time because of network\nissues. Therefore, a cloud-based application must embrace those failures and have a strategy in place\nto respond to those failures. These strategies can include retry policies (resending messages or\nretrying requests) and implementing circuit-breaker patterns to avoid exponential load of repeated\nrequests. Basically, cloud-based applications must have resilient mechanisms—either based on cloud\ninfrastructure or custom, as the high-level ones provided by orchestrators or service buses.', '**Security.** Our modern world of containers and microservices can expose new vulnerabilities. There are\nseveral ways to implement basic application security, based on authentication and authorization.\nHowever, container security must consider additional key components that result in inherently safer\napplications. A critical element of building safer apps is having a secure way of communicating with\nother apps and systems, something that often requires credentials, tokens, passwords, and the like,\ncommonly referred to as application secrets. Any secure solution must follow security best practices,\nsuch as encrypting secrets while in transit and at rest, and preventing secrets from leaking when\nconsumed by the final application. Those secrets need to be stored and kept safely, as when using\nAzure Key Vault.', '**Orchestrators.** Container-based orchestrators, such as Azure Kubernetes Service and Azure Service\nFabric are key part of any significant microservice and container-based application. These applications\ncarry with them high complexity, scalability needs, and go through constant evolution. This guide has\nintroduced orchestrators and their role in microservice-based and container-based solutions. If your\napplication needs are moving you toward complex containerized apps, you will find it useful to seek\nout additional resources for learning more about orchestrators.', '335 CHAPTER 9 | .NET Microservices Architecture key takeaways']}