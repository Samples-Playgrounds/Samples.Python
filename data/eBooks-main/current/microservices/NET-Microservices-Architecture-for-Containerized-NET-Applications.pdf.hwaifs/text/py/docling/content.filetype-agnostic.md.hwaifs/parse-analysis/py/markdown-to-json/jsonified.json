{"R E": {"The Gateway aggregation pattern in eShopOnContainers SQL Server": "Web-Marketing\n\nMongoDB,\n\nAs introduced previously, a flexible way to implement requests aggregation is with custom services, by code. You could also implement request aggregation with the Request Aggregation feature in Ocelot , but it might not be as flexible as you need. Therefore, the selected way to implement aggregation in eShopOnContainers is with an explicit ASP.NET Core Web API service for each aggregator.\n\nAccording to that approach, the API Gateway composition diagram is in reality a bit more extended when considering the aggregator services that are not shown in the simplified global architecture diagram shown previously.\n\nIn the following diagram, you can also see how the aggregator services work with their related API Gateways.\n\nFigure 6 -37. eShopOnContainers architecture with aggregator services\n\n<!-- image -->\n\nZooming in further, on the \"Shopping\" business area in the following image, you can see that chattiness between the client apps and the microservices is reduced when using the aggregator services in the API Gateways.\n\neShopOnContainers\n\n(API Gateways / BFF and Aggregator-services zoom-in)\n\nAPI Gateways / BFF\n\nMobile-Shopping\n\nAggregator\n\nMobile-Marketing\n\nWeb-Shopping\n\nAggregator\n\nWeb-Marketing\n\nFigure 6 -38. Zoom in vision of the Aggregator services\n\n<!-- image -->\n\nYou can notice how when the diagram shows the possible requests coming from the API Gateways it can get complex. On the other hand, when you use the aggregator pattern, you can see how the arrows in blue would simplify the communication from a client app perspective. This pattern not only helps to reduce the chattiness and latency in the communication, it also improves the user experience significantly for the remote apps (mobile and SPA apps).\n\nIn the case of the \"Marketing\" business area and microservices, it is a simple use case so there was no need to use aggregators, but it could also be possible, if needed.", "Authentication and authorization in Ocelot API Gateways": "In an Ocelot API Gateway, you can sit the authentication service, such as an ASP.NET Core Web API service using IdentityServer providing the auth token, either out or inside the API Gateway.\n\nSince eShopOnContainers is using multiple API Gateways with boundaries based on BFF and business areas, the Identity/Auth service is left out of the API Gateways, as highlighted in yellow in the following diagram.", "\"Shopping\"": "microservices\n\nClient apps eShop mobile app\n\nAuthentication in Ocelot API Gateway\n\nXamarin.Forms\n\nC#\n\nxPlat. OS:\n\niOS\n\nAndroid\n\nWindows eShop traditional Web app\n\nClient apps\n\neShop SPA Web app\n\nTypeScript/Angular\n\nIdentity microservice (STS+users)\n\nSQL Server database\n\nCatalog microservice\n\n\u2022 0-\n\nSQL Server database\n\nFigure 6 -39. Position of the Identity service in eShopOnContainers\n\n<!-- image -->\n\nHowever, Ocelot also supports sitting the Identity/Auth microservice within the API Gateway boundary, as in this other diagram.\n\nFigure 6 -40. Authentication in Ocelot\n\n<!-- image -->\n\nAs the previous diagram shows, when the Identity microservice is beneath the API gateway (AG): 1) AG requests an auth token from identity microservice, 2) The identity microservice returns token to AG, 34) AG requests from microservices using the auth token. Because eShopOnContainers application has split the API Gateway into multiple BFF (Backend for Frontend) and business areas API Gateways, another option would have been to create an additional API Gateway for cross-cutting concerns. That choice would be fair in a more complex microservice based architecture with multiple cross-cutting concerns microservices. Since there's only one cross-cutting concern in eShopOnContainers, it was decided to just handle the security service out of the API Gateway realm, for simplicity's sake.\n\nAPI Gateways / BFF\n\n\u2022 0\n\nInternal microservices\n\nDocker Host\n\nRabbitMO\n\nIn any case, if the app is secured at the API Gateway level, the authentication module of the Ocelot API Gateway is visited at first when trying to use any secured microservice. That redirects the HTTP request to visit the Identity or auth microservice to get the access token so you can visit the protected services with the access\\_token.\n\nThe way you secure with authentication any service at the API Gateway level is by setting the AuthenticationProviderKey in its related settings at the configuration.json.\n\n```\n{ \"DownstreamPathTemplate\": \"/api/{version}/{everything}\" , \"DownstreamScheme\": \"http\" , \"DownstreamHostAndPorts\": [ { \"Host\": \"basket -api\" , \"Port\": 80 } ] , \"UpstreamPathTemplate\": \"/api/{version}/b/{everything}\" , \"UpstreamHttpMethod\": [] , \"AuthenticationOptions\": { \"AuthenticationProviderKey\": \"IdentityApiKey\" , \"AllowedScopes\": [] } }\n```\n\nWhen Ocelot runs, it will look at the ReRoutes AuthenticationOptions.AuthenticationProviderKey and check that there is an Authentication Provider registered with the given key. If there isn't, then Ocelot will not start up. If there is, then the ReRoute will use that provider when it executes.\n\nBecause the Ocelot WebHost is configured with the authenticationProviderKey = \"IdentityApiKey\", that will require authentication whenever that service has any requests without any auth token.\n\n```\nnamespace OcelotApiGw { public class Startup { private readonly IConfiguration _cfg; public Startup(IConfiguration configuration) => _cfg = configuration; public void ConfigureServices(IServiceCollection services) { var identityUrl = _cfg . GetValue<string>(\"IdentityUrl\"); var authenticationProviderKey = \"IdentityApiKey\"; //\u2026 services . AddAuthentication() . AddJwtBearer(authenticationProviderKey , x => { x . Authority = identityUrl; x . RequireHttpsMetadata = false; x . TokenValidationParameters = new Microsoft . IdentityModel . Tokens . TokenValidationParameters() { ValidAudiences = new[] { \"orders\" , \"basket\" , \"locations\" , \"marketing\" , \"mobileshoppingagg\" , \"webshoppingagg\" } }; }); //...\n```\n\n```\n} } }\n```\n\nThen, you also need to set authorization with the [Authorize] attribute on any resource to be accessed like the microservices, such as in the following Basket microservice controller.\n\n```\nnamespace Microsoft . eShopOnContainers . Services . Basket . API . Controllers { [Route(\"api/v1/[controller]\")] [Authorize] public class BasketController : Controller { //... } }\n```\n\nThe ValidAudiences such as \"basket\" are correlated with the audience defined in each microservice with AddJwtBearer() at the ConfigureServices() of the Startup class, such as in the code below.\n\n```\n// prevent from mapping \"sub\" claim to nameidentifier. JwtSecurityTokenHandler . DefaultInboundClaimTypeMap . Clear(); var identityUrl = Configuration . GetValue<string>(\"IdentityUrl\"); services . AddAuthentication(options => { options . DefaultAuthenticateScheme = JwtBearerDefaults . AuthenticationScheme; options . DefaultChallengeScheme = JwtBearerDefaults . AuthenticationScheme; }).AddJwtBearer(options => { options . Authority = identityUrl; options . RequireHttpsMetadata = false; options . Audience = \"basket\"; });\n```\n\nIf you try to access any secured microservice, like the Basket microservice with a ReRoute URL based on the API Gateway like http://host.docker.internal:5202/api/v1/b/basket/1, then you'll get a 401 Unauthorized unless you provide a valid token. On the other hand, if a ReRoute URL is authenticated, Ocelot will invoke whatever downstream scheme is associated with it (the internal microservice URL).\n\nAuthorization at Ocelot's ReRoutes tier. Ocelot supports claims-based authorization evaluated after the authentication. You set the authorization at a route level by adding the following lines to the ReRoute configuration.\n\n```\n\"RouteClaimsRequirement\": { \"UserType\": \"employee\" }\n```\n\nIn that example, when the authorization middleware is called, Ocelot will find if the user has the claim type 'UserType' in the token and if the value of that claim is 'employee'. If it isn't, then the user will not be authorized and the response will be 403 forbidden.\n\nClient apps eShopOnContainers\n\n(Deployment into Kubernetes environment)\n\nAzure Kubernetes Service (AKS)\n\nIdentity microservice (STS+ users)\n\nSQL Server database", "Using Kubernetes Ingress plus Ocelot API Gateways": "RabbitMQl\n\nWhen using Kubernetes (like in an Azure Kubernetes Service cluster), you usually unify all the HTTP requests through the Kubernetes Ingress tier based on Nginx . Ordering API\n\nBasket microservice\n\nIn Kubernetes, if you don't use any ingress approach, then your services and pods have IPs only routable by the cluster network. Web-Shopping Aggregator Marketing microservice Azure\n\n\u2022 CosmosDB\n\nBut if you use an ingress approach, you'll have a middle tier between the Internet and your services (including your API Gateways), acting as a reverse proxy. Locations microservice MongoDB CosmosDB\n\nAs a definition, an Ingress is a collection of rules that allow inbound connections to reach the cluster services. An ingress is configured to provide services externally reachable URLs, load balance traffic, SSL termination and more. Users request ingress by POSTing the Ingress resource to the API server.\n\nIn eShopOnContainers, when developing locally and using just your development machine as the Docker host, you are not using any ingress but only the multiple API Gateways.\n\nHowever, when targeting a \"production\" environment based on Kubernetes, eShopOnContainers is using an ingress in front of the API gateways. That way, the clients still call the same base URL but the requests are routed to multiple API Gateways or BFF.\n\nAPI Gateways are front-ends or fa\u00e7ades surfacing only the services but not the web applications that are usually out of their scope. In addition, the API Gateways might hide certain internal microservices.\n\nThe ingress, however, is just redirecting HTTP requests but not trying to hide any microservice or web app.\n\nHaving an ingress Nginx tier in Kubernetes in front of the web applications plus the several Ocelot API Gateways / BFF is the ideal architecture, as shown in the following diagram.\n\nFigure 6 -41. The ingress tier in eShopOnContainers when deployed into Kubernetes\n\n<!-- image -->\n\nA Kubernetes Ingress acts as a reverse proxy for all traffic to the app, including the web applications, that are out of the Api gateway scope. When you deploy eShopOnContainers into Kubernetes, it\n\nexposes just a few services or endpoints via ingress, basically the following list of postfixes on the URLs:\n\n['/ for the client SPA web application', '/webmvc for the client MVC web application', '/webstatus for the client web app showing the status/healthchecks', '/webshoppingapigw for the web BFF and shopping business processes', '/webmarketingapigw for the web BFF and marketing business processes', '/mobileshoppingapigw for the mobile BFF and shopping business processes', '/mobilemarketingapigw for the mobile BFF and marketing business processes']\n\nWhen deploying to Kubernetes, each Ocelot API Gateway is using a different \"configuration.json\" file for each pod running the API Gateways. Those \"configuration.json\" files are provided by mounting (originally with the deploy.ps1 script) a volume created based on a Kubernetes config map named 'ocelot'. Each container mounts its related configuration file in the container's folder named /app/configuration.\n\nIn the source code files of eShopOnContainers, the original \"configuration.json\" files can be found within the k8s/ocelot/ folder. There's one file for each BFF/APIGateway.", "Additional cross -cutting features in an Ocelot API Gateway": "There are other important features to research and use, when using an Ocelot API Gateway, described in the following links.\n\n['Service discovery in the client side integrating Ocelot with Consul or Eureka https://ocelot.readthedocs.io/en/latest/features/servicediscovery.html', 'Caching at the API Gateway tier https://ocelot.readthedocs.io/en/latest/features/caching.html', 'Logging at the API Gateway tier https://ocelot.readthedocs.io/en/latest/features/logging.html', 'Quality of Service (Retries and Circuit breakers) at the API Gateway tier https://ocelot.readthedocs.io/en/latest/features/qualityofservice.html', 'Rate limiting https://ocelot.readthedocs.io/en/latest/features/ratelimiting.html', 'Swagger for Ocelot https://github.com/Burgyn/MMLib.SwaggerForOcelot']", "Tackle Business Complexity in a Microservice with DDD and CQRS Patterns": "Design a domain model for each microservice or Bounded Context that reflects understanding of the business domain.\n\nThis section focuses on more advanced microservices that you implement when you need to tackle complex subsystems, or microservices derived from the knowledge of domain experts with everchanging business rules. The architecture patterns used in this section are based on domain-driven design (DDD) and Command and Query Responsibility Segregation (CQRS) approaches, as illustrated in Figure 7-1.\n\nExternal architecture per application\n\nBack end", "Microservice 1": "Microservice 2]\n\nAPI Gateway\n\nInternal architecture per microservice\n\nDDD\n\nCQRS and\n\nDomain Events\n\nFigure 7 -1. External microservice architecture versus internal architecture patterns for each microservice\n\n<!-- image -->\n\nHowever, most of the techniques for data driven microservices, such as how to implement an ASP.NET Core Web API service or how to expose Swagger metadata with Swashbuckle or NSwag, are also applicable to the more advanced microservices implemented internally with DDD patterns. This section is an extension of the previous sections, because most of the practices explained earlier also apply here or for any kind of microservice.\n\nThis section first provides details on the simplified CQRS patterns used in the eShopOnContainers reference application. Later, you will get an overview of the DDD techniques that enable you to find common patterns that you can reuse in your applications.\n\nDDD is a large topic with a rich set of resources for learning. You can start with books like DomainDriven Design by Eric Evans and additional materials from Vaughn Vernon, Jimmy Nilsson, Greg Young, Udi Dahan, Jimmy Bogard, and many other DDD/CQRS experts. But most of all you need to try to learn how to apply DDD techniques from the conversations, whiteboarding, and domain modeling sessions with the experts in your concrete business domain.", "Additional resources": ["Using Azure Key Vault to protect application secrets https://learn.microsoft.com/azure/architecture/multitenant-identity", "Safe storage of app secrets during development https://learn.microsoft.com/aspnet/core/security/app-secrets", "Configuring data protection https://learn.microsoft.com/aspnet/core/security/data-protection/configuration/overview", "Data Protection key management and lifetime in ASP.NET Core https://learn.microsoft.com/aspnet/core/security/data-protection/configuration/defaultsettings"], "DDD (Domain-Driven Design)": ["Eric Evans. Domain Language https://domainlanguage.com/", "Martin Fowler. Domain -Driven Design https://martinfowler.com/tags/domain%20driven%20design.html", "Jimmy Bogard. Strengthening your domain: a primer https://lostechies.com/jimmybogard/2010/02/04/strengthening-your-domain-a-primer/"], "DDD books": ["Eric Evans. Domain -Driven Design: Tackling Complexity in the Heart of Software https://www.amazon.com/Domain-Driven-Design-Tackling-ComplexitySoftware/dp/0321125215/", "Eric Evans. Domain -Driven Design Reference: Definitions and Pattern Summaries https://www.amazon.com/Domain-Driven-Design-Reference-Definitions-2014-0922/dp/B01N8YB4ZO/", "Vaughn Vernon. Implementing Domain-Driven Design https://www.amazon.com/Implementing-Domain-Driven-Design-VaughnVernon/dp/0321834577/", "Vaughn Vernon. Domain-Driven Design Distilled https://www.amazon.com/Domain-Driven-Design-Distilled-Vaughn-Vernon/dp/0134434420/", "Jimmy Nilsson. Applying Domain-Driven Design and Patterns https://www.amazon.com/Applying-Domain-Driven-Design-PatternsExamples/dp/0321268202/", "Cesar de la Torre. N -Layered Domain-Oriented Architecture Guide with .NET https://www.amazon.com/N-Layered-Domain-Oriented-Architecture-GuideNET/dp/8493903612/", "Abel Avram and Floyd Marinescu. Domain-Driven Design Quickly https://www.amazon.com/Domain-Driven-Design-Quickly-Abel-Avram/dp/1411609255/", "Scott Millett, Nick Tune - Patterns, Principles, and Practices of Domain-Driven Design https://www.wiley.com/Patterns%2C+Principles%2C+and+Practices+of+Domain+Driven+Des ign-p-9781118714706"], "DDD training": ["Julie Lerman and Steve Smith. Domain -Driven Design Fundamentals https://www.pluralsight.com/courses/fundamentals-domain-driven-design"], "Apply simplified CQRS and DDD patterns in a microservice": "CQRS is an architectural pattern that separates the models for reading and writing data. The related term Command Query Separation (CQS) was originally defined by Bertrand Meyer in his book ObjectOriented Software Construction. The basic idea is that you can divide a system's operations into two sharply separated categories:\n\n[\"Queries. These queries return a result and don't change the state of the system, and they're free of side effects.\", 'Commands. These commands change the state of a system.']\n\nCQS is a simple concept: it is about methods within the same object being either queries or commands. Each method either returns state or mutates state, but not both. Even a single repository pattern object can comply with CQS. CQS can be considered a foundational principle for CQRS.\n\nCommand and Query Responsibility Segregation (CQRS) was introduced by Greg Young and strongly promoted by Udi Dahan and others. It's based on the CQS principle, although it's more detailed. It can be considered a pattern based on commands and events plus optionally on asynchronous messages. In many cases, CQRS is related to more advanced scenarios, like having a different physical database for reads (queries) than for writes (updates). Moreover, a more evolved CQRS system might implement Event-Sourcing (ES) for your updates database, so you would only store events in the domain model instead of storing the current-state data. However, this approach is not used in this guide. This guide uses the simplest CQRS approach, which consists of just separating the queries from the commands.\n\nThe separation aspect of CQRS is achieved by grouping query operations in one layer and commands in another layer. Each layer has its own data model (note that we say model, not necessarily a different database) and is built using its own combination of patterns and technologies. More importantly, the two layers can be within the same tier or microservice, as in the example (ordering microservice) used for this guide. Or they could be implemented on different microservices or processes so they can be optimized and scaled out separately without affecting one another.\n\nCQRS means having two objects for a read/write operation where in other contexts there's one. There are reasons to have a denormalized reads database, which you can learn about in more advanced CQRS literature. But we aren't using that approach here, where the goal is to have more flexibility in the queries instead of limiting the queries with constraints from DDD patterns like aggregates.\n\nAn example of this kind of service is the ordering microservice from the eShopOnContainers reference application. This service implements a microservice based on a simplified CQRS approach. It uses a single data source or database, but two logical models plus DDD patterns for the transactional domain, as shown in Figure 7-2.\n\n\u0413\n\nExternal IP\n\nand Port |\n\nSimplified CQRS and DDD microservice\n\nHigh level design\n\n[]\n\n[]\n\nDocker Host\n\nLogical \"Ordering\" Microservice\n\n<!-- image -->\n\n[]\n\nFigure 7 -2. Simplified CQRS- and DDD-based microservice\n\nThe Logical \"Ordering\" Microservice includes its Ordering database, which can be, but doesn't have to be, the same Docker host. Having the database in the same Docker host is good for development, but not for production.\n\nThe application layer can be the Web API itself. The important design aspect here is that the microservice has split the queries and ViewModels (data models especially created for the client applications) from the commands, domain model, and transactions following the CQRS pattern. This approach keeps the queries independent from restrictions and constraints coming from DDD patterns that only make sense for transactions and updates, as explained in later sections.", "Apply CQRS and CQS approaches in a DDD microservice in eShopOnContainers": "The design of the ordering microservice at the eShopOnContainers reference application is based on CQRS principles. However, it uses the simplest approach, which is just separating the queries from the commands and using the same database for both actions.\n\nThe essence of those patterns, and the important point here, is that queries are idempotent: no matter how many times you query a system, the state of that system won't change. In other words, queries are side -effect free.\n\nTherefore, you could use a different \"reads\" data model than the transactional logic \"writes\" domain model, even though the ordering microservices are using the same database. Hence, this is a simplified CQRS approach.\n\nOn the other hand, commands, which trigger transactions and data updates, change state in the system. With commands, you need to be careful when dealing with complexity and ever-changing business rules. This is where you want to apply DDD techniques to have a better modeled system.\n\nThe DDD patterns presented in this guide should not be applied universally. They introduce constraints on your design. Those constraints provide benefits such as higher quality over time, especially in commands and other code that modifies system state. However, those constraints add complexity with fewer benefits for reading and querying data.\n\nOne such pattern is the Aggregate pattern, which we examine more in later sections. Briefly, in the Aggregate pattern, you treat many domain objects as a single unit as a result of their relationship in the domain. You might not always gain advantages from this pattern in queries; it can increase the complexity of query logic. For read-only queries, you do not get the advantages of treating multiple objects as a single Aggregate. You only get the complexity.\n\nAs shown in Figure 7-2 in the previous section, this guide suggests using DDD patterns only in the transactional/updates area of your microservice (that is, as triggered by commands). Queries can follow a simpler approach and should be separated from commands, following a CQRS approach.\n\nFor implementing the \"queries side\", you can choose between many approaches, from your full-blown ORM like EF Core, AutoMapper projections, stored procedures, views, materialized views or a micro ORM.\n\nIn this guide and in eShopOnContainers (specifically the ordering microservice) we chose to implement straight queries using a micro ORM like Dapper. This guide lets you implement any query based on SQL statements to get the best performance, thanks to a light framework with little overhead.\n\nWhen you use this approach, any updates to your model that impact how entities are persisted to a SQL database also need separate updates to SQL queries used by Dapper or any other separate (nonEF) approaches to querying.", "CQRS and DDD patterns are not top-level architectures": "It's important to understand that CQRS and most DDD patterns (like DDD layers or a domain model with aggregates) are not architectural styles, but only architecture patterns. Microservices, SOA, and event -driven architecture (EDA) are examples of architectural styles. They describe a system of many components, such as many microservices. CQRS and DDD patterns describe something inside a single system or component; in this case, something inside a microservice.\n\nDifferent Bounded Contexts (BCs) will employ different patterns. They have different responsibilities, and that leads to different solutions. It is worth emphasizing that forcing the same pattern everywhere leads to failure. Do not use CQRS and DDD patterns everywhere. Many subsystems, BCs, or microservices are simpler and can be implemented more easily using simple CRUD services or using another approach.\n\nHigh level \"Queries-side\" in a simplified CQRS\n\nUl app\n\nThere is only one application architecture: the architecture of the system or end-to-end application you are designing (for example, the microservices architecture). However, the design of each Bounded Context or microservice within that application reflects its own tradeoffs and internal design decisions at an architecture patterns level. Do not try to apply the same architectural patterns as CQRS or DDD everywhere.", "Implement reads/queries in a CQRS microservice": "For reads/queries, the ordering microservice from the eShopOnContainers reference application implements the queries independently from the DDD model and transactional area. This implementation was done primarily because the demands for queries and for transactions are drastically different. Writes execute transactions that must be compliant with the domain logic. Queries, on the other hand, are idempotent and can be segregated from the domain rules.\n\nThe approach is simple, as shown in Figure 7-3. The API interface is implemented by the Web API controllers using any infrastructure, such as a micro Object Relational Mapper (ORM) like Dapper, and returning dynamic ViewModels depending on the needs of the UI applications.\n\nFigure 7 -3. The simplest approach for queries in a CQRS microservice\n\n<!-- image -->\n\nThe simplest approach for the queries-side in a simplified CQRS approach can be implemented by querying the database with a Micro-ORM like Dapper, returning dynamic ViewModels. The query\n\nDatabase\n\ndefinitions query the database and return a dynamic ViewModel built on the fly for each query. Since the queries are idempotent, they won't change the data no matter how many times you run a query. Therefore, you don't need to be restricted by any DDD pattern used in the transactional side, like aggregates and other patterns, and that is why queries are separated from the transactional area. You query the database for the data that the UI needs and return a dynamic ViewModel that does not need to be statically defined anywhere (no classes for the ViewModels) except in the SQL statements themselves.\n\nSince this approach is simple, the code required for the queries side (such as code using a micro ORM like Dapper) can be implemented within the same Web API project. Figure 7-4 shows this approach. The queries are defined in the Ordering.API microservice project within the eShopOnContainers solution.\n\nFigure 7 -4. Queries in the Ordering microservice in eShopOnContainers\n\n<!-- image -->", "Use ViewModels specifically made for client apps, independent from domain model constraints": "Since the queries are performed to obtain the data needed by the client applications, the returned type can be specifically made for the clients, based on the data returned by the queries. These models, or Data Transfer Objects (DTOs), are called ViewModels.\n\nThe returned data (ViewModel) can be the result of joining data from multiple entities or tables in the database, or even across multiple aggregates defined in the domain model for the transactional area. In this case, because you are creating queries independent of the domain model, the aggregates boundaries and constraints are ignored and you're free to query any table and column you might need. This approach provides great flexibility and productivity for the developers creating or updating the queries.\n\nThe ViewModels can be static types defined in classes (as is implemented in the ordering microservice). Or they can be created dynamically based on the queries performed, which is agile for developers.", "Use Dapper as a micro ORM to perform queries": "You can use any micro ORM, Entity Framework Core, or even plain ADO.NET for querying. In the sample application, Dapper was selected for the ordering microservice in eShopOnContainers as a good example of a popular micro ORM. It can run plain SQL queries with great performance, because it's a light framework. Using Dapper, you can write a SQL query that can access and join multiple tables.\n\n\u2022\n\nDapper by Sam Saffron, Marc Gravell, Nick Craver\n\nA high performance Micro-ORM supporting SQL Server, MySQL, Sqlite, SqICE, Firebird etc...\n\nDapper is an open-source project (original created by Sam Saffron), and is part of the building blocks used in Stack Overflow. To use Dapper, you just need to install it through the Dapper NuGet package , as shown in the following figure:\n\n<!-- image -->\n\nYou also need to add a using directive so your code has access to the Dapper extension methods.\n\nWhen you use Dapper in your code, you directly use the SqlConnection class available in the Microsoft.Data.SqlClient namespace. Through the QueryAsync method and other extension methods that extend the SqlConnection class, you can run queries in a straightforward and performant way.", "Dynamic versus static ViewModels": "When returning ViewModels from the server-side to client apps, you can think about those ViewModels as DTOs (Data Transfer Objects) that can be different to the internal domain entities of your entity model because the ViewModels hold the data the way the client app needs. Therefore, in many cases, you can aggregate data coming from multiple domain entities and compose the ViewModels precisely according to how the client app needs that data.\n\nThose ViewModels or DTOs can be defined explicitly (as data holder classes), like the OrderSummary class shown in a later code snippet. Or, you could just return dynamic ViewModels or dynamic DTOs based on the attributes returned by your queries as a dynamic type.", "ViewModel as dynamic type": "As shown in the following code, a ViewModel can be directly returned by the queries by just returning a dynamic type that internally is based on the attributes returned by a query. That means that the subset of attributes to be returned is based on the query itself. Therefore, if you add a new column to the query or join, that data is dynamically added to the returned ViewModel.\n\n```\nusing Dapper; using Microsoft . Extensions . Configuration; using System . Data . SqlClient; using System . Threading . Tasks; using System . Dynamic; using System . Collections . Generic; public class OrderQueries : IOrderQueries { public async Task<IEnumerable<dynamic>> GetOrdersAsync() { using (var connection = new SqlConnection(_connectionString)) { connection . Open(); return await connection . QueryAsync<dynamic>( @\"SELECT o.[Id] as ordernumber, o.[OrderDate] as [date],os.[Name] as [status], SUM(oi . units*oi . unitprice) as total FROM [ordering].[Orders] o LEFT JOIN[ordering].[orderitems] oi ON o . Id = oi . orderid LEFT JOIN[ordering].[orderstatus] os on o . OrderStatusId = os . Id\n```\n\n\u2022 v1.50.5\n\n```\nGROUP BY o.[Id], o.[OrderDate], os.[Name]\"); } } }\n```\n\nThe important point is that by using a dynamic type, the returned collection of data is dynamically assembled as the ViewModel.\n\nPros: This approach reduces the need to modify static ViewModel classes whenever you update the SQL sentence of a query, making this design approach agile when coding, straightforward, and quick to evolve in regard to future changes.\n\nCons: In the long term, dynamic types can negatively impact the clarity and the compatibility of a service with client apps. In addition, middleware software like Swashbuckle cannot provide the same level of documentation on returned types if using dynamic types.", "ViewModel as predefined DTO classes": "Pros: Having static, predefined ViewModel classes, like \"contracts\" based on explicit DTO classes, is definitely better for public APIs but also for long-term microservices, even if they are only used by the same application.\n\nIf you want to specify response types for Swagger, you need to use explicit DTO classes as the return type. Therefore, predefined DTO classes allow you to offer richer information from Swagger. That improves the API documentation and compatibility when consuming an API.\n\nCons: As mentioned earlier, when updating the code, it takes some more steps to update the DTO classes.\n\nTip based on our experience: In the queries implemented at the Ordering microservice in eShopOnContainers, we started developing by using dynamic ViewModels as it was straightforward and agile on the early development stages. But, once the development was stabilized, we chose to refactor the APIs and use static or pre-defined DTOs for the ViewModels, because it is clearer for the microservice's consumers to know explicit DTO types, used as \"contracts\".\n\nIn the following example, you can see how the query is returning data by using an explicit ViewModel DTO class: the OrderSummary class.\n\n```\nusing Dapper; using Microsoft . Extensions . Configuration; using System . Data . SqlClient; using System . Threading . Tasks; using System . Dynamic; using System . Collections . Generic; public class OrderQueries : IOrderQueries { public async Task<IEnumerable<OrderSummary>> GetOrdersAsync() { using (var connection = new SqlConnection(_connectionString)) { connection . Open(); return await connection . QueryAsync<OrderSummary>( @\"SELECT o.[Id] as ordernumber, o.[OrderDate] as [date],os.[Name] as [status],\n```\n\n```\nSUM(oi . units*oi . unitprice) as total FROM [ordering].[Orders] o LEFT JOIN[ordering].[orderitems] oi ON o . Id = oi . orderid LEFT JOIN[ordering].[orderstatus] os on o . OrderStatusId = os . Id GROUP BY o.[Id], o.[OrderDate], os.[Name] ORDER BY o.[Id]\"); } } }\n```", "Describe response types of Web APIs": "Developers consuming web APIs and microservices are most concerned with what is returned\u2014 specifically response types and error codes (if not standard). The response types are handled in the XML comments and data annotations.\n\nWithout proper documentation in the Swagger UI, the consumer lacks knowledge of what types are being returned or what HTTP codes can be returned. That problem is fixed by adding the Microsoft.AspNetCore.Mvc.ProducesResponseTypeAttribute, so Swashbuckle can generate richer information about the API return model and values, as shown in the following code:\n\n```\nnamespace Microsoft . eShopOnContainers . Services . Ordering . API . Controllers { [Route(\"api/v1/[controller]\")] [Authorize] public class OrdersController : Controller { //Additional code... [Route( \"\" )] [HttpGet] [ProducesResponseType(typeof(IEnumerable<OrderSummary>), (int)HttpStatusCode . OK)] public async Task<IActionResult> GetOrders() { var userid = _identityService . GetUserIdentity(); var orders = await _orderQueries . GetOrdersFromUserAsync(Guid . Parse(userid)); return Ok(orders); } } }\n```\n\nHowever, the ProducesResponseType attribute cannot use dynamic as a type but requires to use explicit types, like the OrderSummary ViewModel DTO, shown in the following example:\n\n```\npublic class OrderSummary { public int ordernumber { get; set; } public DateTime date { get; set; } public string status { get; set; } public double total { get; set; } } // or using C# 8 record types: public record OrderSummary(int ordernumber , DateTime date , string status , double total);\n```\n\n1- swagger\n\nOrdering HTTP API\u00ae\n\nswasger/v1/swagser.ison\n\nThe Ordering Service HTTP API\n\nTerms of service\n\nThis is another reason why explicit returned types are better than dynamic types, in the long term. When using the ProducesResponseType attribute, you can also specify what is the expected outcome regarding possible HTTP errors/codes, like 200, 400, etc. Authorize |\n\nIn the following image, you can see how Swagger UI shows the ResponseType information.\n\nV\n\nFigure 7 -5. Swagger UI showing response types and possible HTTP status codes from a Web API\n\n<!-- image -->\n\nThe image shows some example values based on the ViewModel types and the possible HTTP status codes that can be returned.", "Design a DDD -oriented microservice": "Domain -driven design (DDD) advocates modeling based on the reality of business as relevant to your use cases. In the context of building applications, DDD talks about problems as domains. It describes independent problem areas as Bounded Contexts (each Bounded Context correlates to a microservice), and emphasizes a common language to talk about these problems. It also suggests many technical concepts and patterns, like domain entities with rich models (no anemic-domain model), value objects, aggregates, and aggregate root (or root entity) rules to support the internal implementation. This section introduces the design and implementation of those internal patterns.\n\nSometimes these DDD technical rules and patterns are perceived as obstacles that have a steep learning curve for implementing DDD approaches. But the important part is not the patterns themselves, but organizing the code so it is aligned to the business problems, and using the same business terms (ubiquitous language). In addition, DDD approaches should be applied only if you are implementing complex microservices with significant business rules. Simpler responsibilities, like a CRUD service, can be managed with simpler approaches.\n\nWhere to draw the boundaries is the key task when designing and defining a microservice. DDD patterns help you understand the complexity in the domain. For the domain model for each Bounded Context, you identify and define the entities, value objects, and aggregates that model your domain. You build and refine a domain model that is contained within a boundary that defines your context. And that is explicit in the form of a microservice. The components within those boundaries end up being your microservices, although in some cases a BC or business microservices can be composed of several physical services. DDD is about boundaries and so are microservices.", "Keep the microservice context boundaries relatively small": "Determining where to place boundaries between Bounded Contexts balances two competing goals. First, you want to initially create the smallest possible microservices, although that should not be the main driver; you should create a boundary around things that need cohesion. Second, you want to avoid chatty communications between microservices. These goals can contradict one another. You should balance them by decomposing the system into as many small microservices as you can until you see communication boundaries growing quickly with each additional attempt to separate a new Bounded Context. Cohesion is key within a single bounded context.\n\nIt is similar to the Inappropriate Intimacy code smell when implementing classes. If two microservices need to collaborate a lot with each other, they should probably be the same microservice.\n\nAnother way to look at this aspect is autonomy. If a microservice must rely on another service to directly service a request, it is not truly autonomous.", "Layers in DDD microservices": "Most enterprise applications with significant business and technical complexity are defined by multiple layers. The layers are a logical artifact, and are not related to the deployment of the service. They exist to help developers manage the complexity in the code. Different layers (like the domain model layer versus the presentation layer, etc.) might have different types, which mandate translations between those types.\n\nFor example, an entity could be loaded from the database. Then part of that information, or an aggregation of information including additional data from other entities, can be sent to the client UI through a REST Web API. The point here is that the domain entity is contained within the domain model layer and should not be propagated to other areas that it does not belong to, like to the presentation layer.\n\nAdditionally, you need to have always-valid entities (see the Designing validations in the domain model layer section) controlled by aggregate roots (root entities). Therefore, entities should not be bound to client views, because at the UI level some data might still not be validated. This reason is what the ViewModel is for. The ViewModel is a data model exclusively for presentation layer needs. The domain entities do not belong directly to the ViewModel. Instead, you need to translate between ViewModels and domain entities and vice versa.\n\nWhen tackling complexity, it is important to have a domain model controlled by aggregate roots that make sure that all the invariants and rules related to that group of entities (aggregate) are performed through a single entry-point or gate, the aggregate root.\n\nFigure 7-5 shows how a layered design is implemented in the eShopOnContainers application.\n\nOrdering.Domain\n\n\u2022 i Dependencies\n\nLayers in a Domain-Driven Design Microservice\n\n\u2022 '\u00ae NuGet\n\n\u2022 SDK\n\nNETStandard.Library\n\nOrdering microservice\n\nOrdering\n\n\u2022 aQ] Ordering.API\n\n\u2022 a|c# Ordering.Domain +\n\nD all Ordering Infrastructure\n\nASP.NET Web API\n\nNetwork access to microservice", "Application layer API contracts/implementation Commands and command handlers Queries (when using an CQS approach)": "Micro ORMs like Dapper\n\nFigure 7 -5. DDD layers in the ordering microservice in eShopOnContainers\n\n<!-- image -->\n\nThe three layers in a DDD microservice like Ordering. Each layer is a VS project: Application layer is Ordering.API, Domain layer is Ordering.Domain and the Infrastructure layer is Ordering.Infrastructure. You want to design the system so that each layer communicates only with certain other layers. That approach may be easier to enforce if layers are implemented as different class libraries, because you can clearly identify what dependencies are set between libraries. For instance, the domain model layer should not take a dependency on any other layer (the domain model classes should be Plain Old Class Objects, or POCO, classes). As shown in Figure 7-6, the Ordering.Domain layer library has dependencies only on the .NET libraries or NuGet packages, but not on any other custom library, such as data library or persistence library.\n\nFigure 7 -6. Layers implemented as libraries allow better control of dependencies between layers\n\n<!-- image -->", "The domain model layer": "Eric Evans's excellent book Domain Driven Design says the following about the domain model layer and the application layer.\n\nDomain Model Layer: Responsible for representing concepts of the business, information about the business situation, and business rules. State that reflects the business situation is controlled and used here, even though the technical details of storing it are delegated to the infrastructure. This layer is the heart of business software.\n\n\u2022\n\nThe domain model layer is where the business is expressed. When you implement a microservice domain model layer in .NET, that layer is coded as a class library with the domain entities that capture data plus behavior (methods with logic).\n\nFollowing the Persistence Ignorance and the Infrastructure Ignorance principles, this layer must completely ignore data persistence details. These persistence tasks should be performed by the infrastructure layer. Therefore, this layer should not take direct dependencies on the infrastructure, which means that an important rule is that your domain model entity classes should be POCOs.\n\nDomain entities should not have any direct dependency (like deriving from a base class) on any data access infrastructure framework like Entity Framework or NHibernate. Ideally, your domain entities should not derive from or implement any type defined in any infrastructure framework.\n\nMost modern ORM frameworks like Entity Framework Core allow this approach, so that your domain model classes are not coupled to the infrastructure. However, having POCO entities is not always possible when using certain NoSQL databases and frameworks, like Actors and Reliable Collections in Azure Service Fabric.\n\nEven when it is important to follow the Persistence Ignorance principle for your Domain model, you should not ignore persistence concerns. It is still important to understand the physical data model and how it maps to your entity object model. Otherwise you can create impossible designs.\n\nAlso, this aspect does not mean you can take a model designed for a relational database and directly move it to a NoSQL or document -oriented database. In some entity models, the model might fit, but usually it does not. There are still constraints that your entity model must adhere to, based both on the storage technology and ORM technology.", "The application layer": "Moving on to the application layer, we can again cite Eric Evans\u2019s book Domain Driven Design:\n\nApplication Layer: Defines the jobs the software is supposed to do and directs the expressive domain objects to work out problems. The tasks this layer is responsible for are meaningful to the business or necessary for interaction with the application layers of other systems. This layer is kept thin. It does not contain business rules or knowledge, but only coordinates tasks and delegates work to collaborations of domain objects in the next layer down. It does not have state reflecting the business situation, but it can have state that reflects the progress of a task for the user or the program.\n\nA microservice's application layer in .NET is commonly coded as an ASP.NET Core Web API project. The project implements the microservice's interaction, remote network access, and the external Web APIs used from the UI or client apps. It includes queries if using a CQRS approach, commands accepted by the microservice, and even the event-driven communication between microservices (integration events). The ASP.NET Core Web API that represents the application layer must not contain business rules or domain knowledge (especially domain rules for transactions or updates); these should be owned by the domain model class library. The application layer must only coordinate tasks and must not hold or define any domain state (domain model). It delegates the execution of business rules to the domain model classes themselves (aggregate roots and domain entities), which will ultimately update the data within those domain entities.\n\nDependencies between Layers in a Domain-Driven Design service\n\nMicroservice\n\nApplication\n\nLayer\n\nDepends on the Domain-Model Layer so it can:\n\n\u2022 Use entity objects\n\n\u2022 Use Repository Interfaces/Contracts\n\nBasically, the application logic is where you implement all use cases that depend on a given front end. For example, the implementation related to a Web API service. Depends on the Infrastructure Layer (thru DI) so it can:\n\nIdeally, it must NOT take dependency on any other layer\n\nThe goal is that the domain logic in the domain model layer, its invariants, the data model, and related business rules must be completely independent from the presentation and application layers. Most of all, the domain model layer must not directly depend on any infrastructure framework.\n\nInfrastructure\n\nLayer", "The infrastructure layer \u00b7 Use entity objects.": "The infrastructure layer is how the data that is initially held in domain entities (in memory) is persisted in databases or another persistent store. An example is using Entity Framework Core code to implement the Repository pattern classes that use a DBContext to persist data in a relational database.\n\nIn accordance with the previously mentioned Persistence Ignorance and Infrastructure Ignorance principles, the infrastructure layer must not \"contaminate\" the domain model layer. You must keep the domain model entity classes agnostic from the infrastructure that you use to persist data (EF or any other framework) by not taking hard dependencies on frameworks. Your domain model layer class library should have only your domain code, just POCO entity classes implementing the heart of your software and completely decoupled from infrastructure technologies.\n\nThus, your layers or class libraries and projects should ultimately depend on your domain model layer (library), not vice versa, as shown in Figure 7-7.\n\nFigure 7 -7. Dependencies between layers in DDD\n\n<!-- image -->\n\nDependencies in a DDD Service, the Application layer depends on Domain and Infrastructure, and Infrastructure depends on Domain, but Domain doesn't depend on any layer. This layer design should be independent for each microservice. As noted earlier, you can implement the most complex microservices following DDD patterns, while implementing simpler data-driven microservices (simple CRUD in a single layer) in a simpler way.\n\n\u2022 Depends on the Domain-Model Layer so it can:\n\n\u2022", "Design a microservice domain model": "Define one rich domain model for each business microservice or Bounded Context.\n\nYour goal is to create a single cohesive domain model for each business microservice or Bounded Context (BC). Keep in mind, however, that a BC or business microservice could sometimes be composed of several physical services that share a single domain model. The domain model must capture the rules, behavior, business language, and constraints of the single Bounded Context or business microservice that it represents.", "The Domain Entity pattern": "Entities represent domain objects and are primarily defined by their identity, continuity, and persistence over time, and not only by the attributes that comprise them. As Eric Evans says, \"an object primarily defined by its identity is called an Entity.\" Entities are very important in the domain model, since they are the base for a model. Therefore, you should identify and design them carefully.\n\nAn entity\u2019s identity can cross multiple microservices or Bounded Contexts.\n\nThe same identity (that is, the same Id value, although perhaps not the same domain entity) can be modeled across multiple Bounded Contexts or microservices. However, that does not imply that the same entity, with the same attributes and logic would be implemented in multiple Bounded Contexts. Instead, entities in each Bounded Context limit their attributes and behaviors to those required in that Bounded Context's domain.\n\nFor instance, the buyer entity might have most of a person's attributes that are defined in the user entity in the profile or identity microservice, including the identity. But the buyer entity in the ordering microservice might have fewer attributes, because only certain buyer data is related to the order process. The context of each microservice or Bounded Context impacts its domain model.\n\nDomain entities must implement behavior in addition to implementing data attributes.\n\nA domain entity in DDD must implement the domain logic or behavior related to the entity data (the object accessed in memory). For example, as part of an order entity class you must have business logic and operations implemented as methods for tasks such as adding an order item, data validation, and total calculation. The entity's methods take care of the invariants and rules of the entity instead of having those rules spread across the application layer.\n\nDomain Entity pattern\n\nOrder entity class\n\nAttributes\n\nID\n\nFirstName\n\nLastName\n\nAddress\n\nOrderltems (List)\n\nFigure 7-8 shows a domain entity that implements not only data attributes but operations or methods with related domain logic.\n\nMethods\n\nOrder constructor\n\nAddOrderltem(item)\n\nSetAddress(address)", "Entity's": "Behavior\n\nFigure 7 -8. Example of a domain entity design implementing data plus behavior\n\n<!-- image -->\n\nA domain model entity implements behaviors through methods, that is, it's not an \"anemic\" model. Of course, sometimes you can have entities that do not implement any logic as part of the entity class. This can happen in child entities within an aggregate if the child entity does not have any special logic because most of the logic is defined in the aggregate root. If you have a complex microservice that has logic implemented in the service classes instead of in the domain entities, you could be falling into the anemic domain model, explained in the following section.", "Rich domain model versus anemic domain model": "In his post AnemicDomainModel, Martin Fowler describes an anemic domain model this way:\n\nThe basic symptom of an Anemic Domain Model is that at first blush it looks like the real thing. There are objects, many named after the nouns in the domain space, and these objects are connected with the rich relationships and structure that true domain models have. The catch comes when you look at the behavior, and you realize that there is hardly any behavior on these objects, making them little more than bags of getters and setters.\n\nOf course, when you use an anemic domain model, those data models will be used from a set of service objects (traditionally named the business layer) which capture all the domain or business logic. The business layer sits on top of the data model and uses the data model just as data.\n\nThe anemic domain model is just a procedural style design. Anemic entity objects are not real objects because they lack behavior (methods). They only hold data properties and thus it is not objectoriented design. By putting all the behavior out into service objects (the business layer), you essentially end up with spaghetti code or transaction scripts, and therefore you lose the advantages that a domain model provides.\n\nRegardless, if your microservice or Bounded Context is very simple (a CRUD service), the anemic domain model in the form of entity objects with just data properties might be good enough, and it might not be worth implementing more complex DDD patterns. In that case, it will be simply a persistence model, because you have intentionally created an entity with only data for CRUD purposes.\n\nThat is why microservices architectures are perfect for a multi-architectural approach depending on each Bounded Context. For instance, in eShopOnContainers, the ordering microservice implements DDD patterns, but the catalog microservice, which is a simple CRUD service, does not.\n\nSome people say that the anemic domain model is an anti-pattern. It really depends on what you are implementing. If the microservice you are creating is simple enough (for example, a CRUD service), following the anemic domain model it is not an anti-pattern. However, if you need to tackle the complexity of a microservice's domain that has a lot of ever-changing business rules, the anemic domain model might be an anti-pattern for that microservice or Bounded Context. In that case, designing it as a rich model with entities containing data plus behavior as well as implementing additional DDD patterns (aggregates, value objects, etc.) might have huge benefits for the long-term success of such a microservice.", "The Value Object pattern": "As Eric Evans has noted, \"Many objects do not have conceptual identity. These objects describe certain characteristics of a thing.\"\n\nAn entity requires an identity, but there are many objects in a system that do not, like the Value Object pattern. A value object is an object with no conceptual identity that describes a domain aspect. These are objects that you instantiate to represent design elements that only concern you temporarily. You care about what they are, not who they are. Examples include numbers and strings, but can also be higher-level concepts like groups of attributes.\n\nSomething that is an entity in a microservice might not be an entity in another microservice, because in the second case, the Bounded Context might have a different meaning. For example, an address in an e-commerce application might not have an identity at all, since it might only represent a group of attributes of the customer's profile for a person or company. In this case, the address should be classified as a value object. However, in an application for an electric power utility company, the customer address could be important for the business domain. Therefore, the address must have an identity so the billing system can be directly linked to the address. In that case, an address should be classified as a domain entity.\n\nA person with a name and surname is usually an entity because a person has identity, even if the name and surname coincide with another set of values, such as if those names also refer to a different person.\n\nValue objects are hard to manage in relational databases and ORMs like Entity Framework (EF), whereas in document -oriented databases they are easier to implement and use.\n\nEF Core 2.0 and later versions include the Owned Entities feature that makes it easier to handle value objects, as we'll see in detail later on.", "The Aggregate pattern": "A domain model contains clusters of different data entities and processes that can control a significant area of functionality, such as order fulfillment or inventory. A more fine-grained DDD unit is the aggregate, which describes a cluster or group of entities and behaviors that can be treated as a cohesive unit.\n\nYou usually define an aggregate based on the transactions that you need. A classic example is an order that also contains a list of order items. An order item will usually be an entity. But it will be a child entity within the order aggregate, which will also contain the order entity as its root entity, typically called an aggregate root.\n\nIdentifying aggregates can be hard. An aggregate is a group of objects that must be consistent together, but you cannot just pick a group of objects and label them an aggregate. You must start with a domain concept and think about the entities that are used in the most common transactions related to that concept. Those entities that need to be transactionally consistent are what forms an aggregate. Thinking about transaction operations is probably the best way to identify aggregates.", "The Aggregate Root or Root Entity pattern": "An aggregate is composed of at least one entity: the aggregate root, also called root entity or primary entity. Additionally, it can have multiple child entities and value objects, with all entities and objects working together to implement required behavior and transactions.\n\nThe purpose of an aggregate root is to ensure the consistency of the aggregate; it should be the only entry point for updates to the aggregate through methods or operations in the aggregate root class. You should make changes to entities within the aggregate only via the aggregate root. It is the aggregate's consistency guardian, considering all the invariants and consistency rules you might need to comply with in your aggregate. If you change a child entity or value object independently, the\n\nAggregate pattern\n\nBuyer Aggregate (One entity)\n\nOrder Aggregate (Multiple entities and Value-Object)\n\naggregate root cannot ensure that the aggregate is in a valid state. It would be like a table with a loose leg. Maintaining consistency is the main purpose of the aggregate root.\n\nAttributes\n\nID\n\nIn Figure 7-9, you can see sample aggregates like the buyer aggregate, which contains a single entity (the aggregate root Buyer). The order aggregate contains multiple entities and a value object.\n\nFullName\n\n[PaymentMethods]\n\nMethods\n\nBuyer (params) constructor", "[Orderitems] 1 Orderltem (Child Entity)": "Methods\n\nAttributes\n\nID\n\nFigure 7 -9. Example of aggregates with multiple or single entities\n\n<!-- image -->\n\nA DDD domain model is composed from aggregates, an aggregate can have just one entity or more, and can include value objects as well. Note that the Buyer aggregate could have additional child entities, depending on your domain, as it does in the ordering microservice in the eShopOnContainers reference application. Figure 7-9 just illustrates a case in which the buyer has a single entity, as an example of an aggregate that contains only an aggregate root.\n\nIn order to maintain separation of aggregates and keep clear boundaries between them, it is a good practice in a DDD domain model to disallow direct navigation between aggregates and only having the foreign key (FK) field, as implemented in the Ordering microservice domain model in eShopOnContainers. The Order entity only has a foreign key field for the buyer, but not an EF Core navigation property, as shown in the following code:\n\n```\npublic class Order : Entity , IAggregateRoot { private DateTime _orderDate; public Address Address { get; private set; } private int? _buyerId; // FK pointing to a different aggregate root public OrderStatus OrderStatus { get; private set; } private readonly List<OrderItem> _orderItems; public IReadOnlyCollection<OrderItem> OrderItems => _orderItems; // ... Additional code }\n```\n\n1\n\n&lt;-1\n\nIdentifying and working with aggregates requires research and experience. For more information, see the following Additional resources list.", "Implement a microservice domain model with .NET": "In the previous section, the fundamental design principles and patterns for designing a domain model were explained. Now it's time to explore possible ways to implement the domain model by using .NET (plain C# code) and EF Core. Your domain model will be composed simply of your code. It will have just the EF Core model requirements, but not real dependencies on EF. You shouldn't have hard dependencies or references to EF Core or any other ORM in your domain model.", "Domain model structure in a custom .NET Standard Library": "The folder organization used for the eShopOnContainers reference application demonstrates the DDD model for the application. You might find that a different folder organization more clearly communicates the design choices made for your application. As you can see in Figure 7-10, in the ordering domain model there are two aggregates, the order aggregate and the buyer aggregate. Each aggregate is a group of domain entities and value objects, although you could have an aggregate composed of a single domain entity (the aggregate root or root entity) as well.\n\nWeb API\n\napplication layer project/library\n\nInfrastructure layer project/library\n\nrepos &amp; EF code\n\nOrdering Microservice/Container\n\nOrdering\n\n\u2192 \u2022 6\u0424] Ordering.API\n\n4 a C# Ordering. Domain\n\n<!-- image -->\n\n-=----\n\nOrdering microservice\n\nFigure 7 -10. Domain model structure for the ordering microservice in eShopOnContainers\n\nAdditionally, the domain model layer includes the repository contracts (interfaces) that are the infrastructure requirements of your domain model. In other words, these interfaces express what repositories and the methods the infrastructure layer must implement. It's critical that the implementation of the repositories be placed outside of the domain model layer, in the infrastructure layer library, so the domain model layer isn't \"contaminated\" by API or classes from infrastructure technologies, like Entity Framework.\n\nYou can also see a SeedWork folder that contains custom base classes that you can use as a base for your domain entities and value objects, so you don't have redundant code in each domain's object class.", "Structure aggregates in a custom .NET Standard library": "An aggregate refers to a cluster of domain objects grouped together to match transactional consistency. Those objects could be instances of entities (one of which is the aggregate root or root entity) plus any additional value objects.\n\nTransactional consistency means that an aggregate is guaranteed to be consistent and up to date at the end of a business action. For example, the order aggregate from the eShopOnContainers ordering microservice domain model is composed as shown in Figure 7-11.\n\nOrder aggregate\n\nA a OrderAggregate\n\n\u2022 a C# Address.cs\n\n\u2022 a C# Order.cs\n\n\u2022 a C# Orderltem.cs\n\n\u2022 a C# OrderStatus.cs\n\nFigure 7 -11. The order aggregate in Visual Studio solution\n\n<!-- image -->\n\nIf you open any of the files in an aggregate folder, you can see how it's marked as either a custom base class or interface, like entity or value object, as implemented in the SeedWork folder.", "Implement domain entities as POCO classes": "You implement a domain model in .NET by creating POCO classes that implement your domain entities. In the following example, the Order class is defined as an entity and also as an aggregate root. Because the Order class derives from the Entity base class, it can reuse common code related to entities. Bear in mind that these base classes and interfaces are defined by you in the domain model project, so it is your code, not infrastructure code from an ORM like EF.\n\n```\n// COMPATIBLE WITH ENTITY FRAMEWORK CORE 5.0 // Entity is a custom base class with the ID public class Order : Entity , IAggregateRoot { private DateTime _orderDate; public Address Address { get; private set; } private int? _buyerId; public OrderStatus OrderStatus { get; private set; } private int _orderStatusId; private string _description; private int? _paymentMethodId; private readonly List<OrderItem> _orderItems; public IReadOnlyCollection<OrderItem> OrderItems => _orderItems; public Order(string userId , Address address , int cardTypeId , string cardNumber , string cardSecurityNumber , string cardHolderName , DateTime cardExpiration , int? buyerId = null , int? paymentMethodId = null) { _orderItems = new List<OrderItem>(); _buyerId = buyerId; _paymentMethodId = paymentMethodId; _orderStatusId = OrderStatus . Submitted . Id; _orderDate = DateTime . UtcNow;\n```\n\n```\nAddress = address; // ...Additional code ... } public void AddOrderItem(int productId , string productName , decimal unitPrice , decimal discount , string pictureUrl , int units = 1) { //... // Domain rules/logic for adding the OrderItem to the order // ... var orderItem = new OrderItem(productId , productName , unitPrice , discount , pictureUrl , units); _orderItems . Add(orderItem); } // ... // Additional methods with domain rules/logic related to the Order aggregate // ... }\n```\n\nIt's important to note that this is a domain entity implemented as a POCO class. It doesn't have any direct dependency on Entity Framework Core or any other infrastructure framework. This implementation is as it should be in DDD, just C# code implementing a domain model.\n\nIn addition, the class is decorated with an interface named IAggregateRoot. That interface is an empty interface, sometimes called a marker interface, that's used just to indicate that this entity class is also an aggregate root.\n\nA marker interface is sometimes considered as an anti -pattern; however, it's also a clean way to mark a class, especially when that interface might be evolving. An attribute could be the other choice for the marker, but it's quicker to see the base class (Entity) next to the IAggregate interface instead of putting an Aggregate attribute marker above the class. It's a matter of preferences, in any case.\n\nHaving an aggregate root means that most of the code related to consistency and business rules of the aggregate's entities should be implemented as methods in the Order aggregate root class (for example, AddOrderItem when adding an OrderItem object to the aggregate). You should not create or update OrderItems objects independently or directly; the AggregateRoot class must keep control and consistency of any update operation against its child entities.", "Encapsulate data in the Domain Entities": "A common problem in entity models is that they expose collection navigation properties as publicly accessible list types. This allows any collaborator developer to manipulate the contents of these collection types, which may bypass important business rules related to the collection, possibly leaving the object in an invalid state. The solution to this is to expose read-only access to related collections and explicitly provide methods that define ways in which clients can manipulate them.\n\nIn the previous code, note that many attributes are read-only or private and are only updatable by the class methods, so any update considers business domain invariants and logic specified within the class methods.\n\nFor example, following DDD patterns, you should not do the following from any command handler method or application layer class (actually, it should be impossible for you to do so):\n\n```\n// WRONG ACCORDING TO DDD PATTERNS \u2013 CODE AT THE APPLICATION LAYER OR // COMMAND HANDLERS // Code in command handler methods or Web API controllers //... (WRONG) Some code with business logic out of the domain classes ... OrderItem myNewOrderItem = new OrderItem(orderId , productId , productName , pictureUrl , unitPrice , discount , units); //... (WRONG) Accessing the OrderItems collection directly from the application layer // or command handlers myOrder . OrderItems . Add(myNewOrderItem); //...\n```\n\nIn this case, the Add method is purely an operation to add data, with direct access to the OrderItems collection. Therefore, most of the domain logic, rules, or validations related to that operation with the child entities will be spread across the application layer (command handlers and Web API controllers).\n\nIf you go around the aggregate root, the aggregate root cannot guarantee its invariants, its validity, or its consistency. Eventually you'll have spaghetti code or transactional script code.\n\nTo follow DDD patterns, entities must not have public setters in any entity property. Changes in an entity should be driven by explicit methods with explicit ubiquitous language about the change they're performing in the entity.\n\nFurthermore, collections within the entity (like the order items) should be read-only properties (the AsReadOnly method explained later). You should be able to update it only from within the aggregate root class methods or the child entity methods.\n\nAs you can see in the code for the Order aggregate root, all setters should be private or at least readonly externally, so that any operation against the entity's data or its child entities has to be performed through methods in the entity class. This maintains consistency in a controlled and object-oriented way instead of implementing transactional script code.\n\nThe following code snippet shows the proper way to code the task of adding an OrderItem object to the Order aggregate.\n\n```\n// RIGHT ACCORDING TO DDD--CODE AT THE APPLICATION LAYER OR COMMAND HANDLERS // The code in command handlers or WebAPI controllers, related only to application stuff // There is NO code here related to OrderItem object's business logic myOrder . AddOrderItem(productId , productName , pictureUrl , unitPrice , discount , units); // The code related to OrderItem params validations or domain rules should // be WITHIN the AddOrderItem method. //...\n```\n\nIn this snippet, most of the validations or logic related to the creation of an OrderItem object will be under the control of the Order aggregate root\u2014in the AddOrderItem method\u2014especially validations and logic related to other elements in the aggregate. For instance, you might get the same product item as the result of multiple calls to AddOrderItem. In that method, you could examine the product items and consolidate the same product items into a single OrderItem object with several units.\n\nAdditionally, if there are different discount amounts but the product ID is the same, you would likely apply the higher discount. This principle applies to any other domain logic for the OrderItem object.\n\nIn addition, the new OrderItem(params) operation will also be controlled and performed by the AddOrderItem method from the Order aggregate root. Therefore, most of the logic or validations related to that operation (especially anything that impacts the consistency between other child entities) will be in a single place within the aggregate root. That is the ultimate purpose of the aggregate root pattern.\n\nWhen you use Entity Framework Core 1.1 or later, a DDD entity can be better expressed because it allows mapping to fields in addition to properties. This is useful when protecting collections of child entities or value objects. With this enhancement, you can use simple private fields instead of properties and you can implement any update to the field collection in public methods and provide read -only access through the AsReadOnly method.\n\nIn DDD, you want to update the entity only through methods in the entity (or the constructor) in order to control any invariant and the consistency of the data, so properties are defined only with a get accessor. The properties are backed by private fields. Private members can only be accessed from within the class. However, there is one exception: EF Core needs to set these fields as well (so it can return the object with the proper values).", "Map properties with only get accessors to the fields in the database table": "Mapping properties to database table columns is not a domain responsibility but part of the infrastructure and persistence layer. We mention this here just so you're aware of the new capabilities in EF Core 1.1 or later related to how you can model entities. Additional details on this topic are explained in the infrastructure and persistence section.\n\nWhen you use EF Core 1.0 or later, within the DbContext you need to map the properties that are defined only with getters to the actual fields in the database table. This is done with the HasField method of the PropertyBuilder class.", "Map fields without properties": "With the feature in EF Core 1.1 or later to map columns to fields, it's also possible to not use properties. Instead, you can just map columns from a table to fields. A common use case for this is private fields for an internal state that doesn't need to be accessed from outside the entity.\n\nFor example, in the preceding OrderAggregate code example, there are several private fields, like the \\_paymentMethodId field, that have no related property for either a setter or getter. That field could also be calculated within the order's business logic and used from the order's methods, but it needs to be persisted in the database as well. So in EF Core (since v1.1), there's a way to map a field without a related property to a column in the database. This is also explained in the Infrastructure layer section of this guide.", "Seedwork (reusable base classes and interfaces for your domain model)": "The solution folder contains a SeedWork folder. This folder contains custom base classes that you can use as a base for your domain entities and value objects. Use these base classes so you don't have redundant code in each domain's object class. The folder for these types of classes is called SeedWork and not something like Framework. It's called SeedWork because the folder contains just a small subset of reusable classes that cannot really be considered a framework. Seedwork is a term introduced by Michael Feathers and popularized by Martin Fowler but you could also name that folder Common, SharedKernel, or similar.\n\nFigure 7-12 shows the classes that form the seedwork of the domain model in the ordering microservice. It has a few custom base classes like Entity, ValueObject, and Enumeration, plus a few interfaces. These interfaces (IRepository and IUnitOfWork) inform the infrastructure layer about what needs to be implemented. Those interfaces are also used through Dependency Injection from the application layer.\n\nFigure 7 -12. A sample set of domain model \"seedwork\" base classes and interfaces\n\n<!-- image -->\n\nThis is the type of copy and paste reuse that many developers share between projects, not a formal framework. You can have seedworks in any layer or library. However, if the set of classes and interfaces gets large enough, you might want to create a single class library.", "The custom Entity base class": "The following code is an example of an Entity base class where you can place code that can be used the same way by any domain entity, such as the entity ID, equality operators, a domain event list per entity, etc.\n\n```\n// COMPATIBLE WITH ENTITY FRAMEWORK CORE (1.1 and later) public abstract class Entity { int? _requestedHashCode; int _Id; private List<INotification> _domainEvents; public virtual int Id { get { return _Id; } protected set { _Id = value; } } public List<INotification> DomainEvents => _domainEvents; public void AddDomainEvent(INotification eventItem) { _domainEvents = _domainEvents ?? new List<INotification>(); _domainEvents . Add(eventItem); } public void RemoveDomainEvent(INotification eventItem) { if (_domainEvents is null) return; _domainEvents . Remove(eventItem); } public bool IsTransient() { return this . Id == default(Int32); } public override bool Equals(object obj) { if (obj == null || !(obj is Entity)) return false; if (Object . ReferenceEquals(this , obj)) return true; if (this . GetType() != obj . GetType()) return false; Entity item = (Entity)obj; if (item . IsTransient() || this . IsTransient()) return false; else return item . Id == this . Id; } public override int GetHashCode() { if (!IsTransient())\n```\n\n```\n{ if (!_requestedHashCode . HasValue) _requestedHashCode = this . Id . GetHashCode() ^ 31; // XOR for random distribution. See: // https://learn.microsoft.com/archive/blogs/ericlippert/guidelines-and-rulesfor -gethashcode return _requestedHashCode . Value; } else return base . GetHashCode(); } public static bool operator ==(Entity left , Entity right) { if (Object . Equals(left , null)) return (Object . Equals(right , null)); else return left . Equals(right); } public static bool operator !=(Entity left , Entity right) { return !(left == right); } }\n```\n\nThe previous code using a domain event list per entity will be explained in the next sections when focusing on domain events.", "Repository contracts (interfaces) in the domain model layer": "Repository contracts are simply .NET interfaces that express the contract requirements of the repositories to be used for each aggregate.\n\nThe repositories themselves, with EF Core code or any other infrastructure dependencies and code (Linq, SQL, etc.), must not be implemented within the domain model; the repositories should only implement the interfaces you define in the domain model.\n\nA pattern related to this practice (placing the repository interfaces in the domain model layer) is the Separated Interface pattern. As explained by Martin Fowler, \"Use Separated Interface to define an interface in one package but implement it in another. This way a client that needs the dependency to the interface can be completely unaware of the implementation.\"\n\nFollowing the Separated Interface pattern enables the application layer (in this case, the Web API project for the microservice) to have a dependency on the requirements defined in the domain model, but not a direct dependency to the infrastructure/persistence layer. In addition, you can use Dependency Injection to isolate the implementation, which is implemented in the infrastructure/ persistence layer using repositories.\n\nFor example, the following example with the IOrderRepository interface defines what operations the OrderRepository class will need to implement at the infrastructure layer. In the current implementation of the application, the code just needs to add or update orders to the database, since queries are split following the simplified CQRS approach.\n\n```\n// Defined at IOrderRepository.cs public interface IOrderRepository : IRepository<Order> {\n```\n\n```\nOrder Add(Order order); void Update(Order order); Task<Order> GetAsync(int orderId); } // Defined at IRepository.cs (Part of the Domain Seedwork) public interface IRepository<T> where T : IAggregateRoot { IUnitOfWork UnitOfWork { get; } }\n```", "Implement value objects": "As discussed in earlier sections about entities and aggregates, identity is fundamental for entities. However, there are many objects and data items in a system that do not require an identity and identity tracking, such as value objects.\n\nA value object can reference other entities. For example, in an application that generates a route that describes how to get from one point to another, that route would be a value object. It would be a snapshot of points on a specific route, but this suggested route would not have an identity, even though internally it might refer to entities like City, Road, etc.\n\nFigure 7-13 shows the Address value object within the Order aggregate.\n\nValue Object within Aggregate", "Order Aggregate (Multiple entities and Value-Object)": "Order (Aggregate Root)\n\nAttributes\n\nID\n\nOrderDate\n\n[BuyerID]\n\n[Address]\n\n[Orderitems]\n\n\u2022.\u2022\n\nMethods\n\nOrder (params) constructor\n\nAddOrderltem(item)\n\nSetAddress(address)\n\nCalculate TotalO\n\nAddress (Value-Object)", "1 Attributes": "Figure 7 -13. Address value object within the Order aggregate\n\n<!-- image -->\n\nAs shown in Figure 7-13, an entity is usually composed of multiple attributes. For example, the Order entity can be modeled as an entity with an identity and composed internally of a set of attributes such as OrderId, OrderDate, OrderItems, etc. But the address, which is simply a complex-value composed of country/region, street, city, etc., and has no identity in this domain, must be modeled and treated as a value object.", "Important characteristics of value objects": "There are two main characteristics for value objects:\n\n['They have no identity.', 'They are immutable.']\n\nThe first characteristic was already discussed. Immutability is an important requirement. The values of a value object must be immutable once the object is created. Therefore, when the object is\n\n1\n\nconstructed, you must provide the required values, but you must not allow them to change during the object's lifetime.\n\nValue objects allow you to perform certain tricks for performance, thanks to their immutable nature. This is especially true in systems where there may be thousands of value object instances, many of which have the same values. Their immutable nature allows them to be reused; they can be interchangeable objects, since their values are the same and they have no identity. This type of optimization can sometimes make a difference between software that runs slowly and software with good performance. Of course, all these cases depend on the application environment and deployment context.", "Value object implementation in C": "In terms of implementation, you can have a value object base class that has basic utility methods like equality based on the comparison between all the attributes (since a value object must not be based on identity) and other fundamental characteristics. The following example shows a value object base class used in the ordering microservice from eShopOnContainers.\n\n```\npublic abstract class ValueObject { protected static bool EqualOperator(ValueObject left , ValueObject right) { if (ReferenceEquals(left , null) ^ ReferenceEquals(right , null)) { return false; } return ReferenceEquals(left , right) || left . Equals(right); } protected static bool NotEqualOperator(ValueObject left , ValueObject right) { return !(EqualOperator(left , right)); } protected abstract IEnumerable<object> GetEqualityComponents(); public override bool Equals(object obj) { if (obj == null || obj . GetType() != GetType()) { return false; } var other = (ValueObject)obj; return this . GetEqualityComponents().SequenceEqual(other . GetEqualityComponents()); } public override int GetHashCode() { return GetEqualityComponents() . Select(x => x != null ? x . GetHashCode() : 0) . Aggregate((x , y) => x ^ y); } // Other utility methods }\n```\n\nThe ValueObject is an abstract class type, but in this example, it doesn't overload the == and != operators. You could choose to do so, making comparisons delegate to the Equals override. For example, consider the following operator overloads to the ValueObject type:\n\n```\npublic static bool operator ==(ValueObject one , ValueObject two) { return EqualOperator(one , two); } public static bool operator !=(ValueObject one , ValueObject two) { return NotEqualOperator(one , two); } You can use this class when implementing your actual value object, as with the Address value object shown in the following example: public class Address : ValueObject { public String Street { get; private set; } public String City { get; private set; } public String State { get; private set; } public String Country { get; private set; } public String ZipCode { get; private set; } public Address() { } public Address(string street , string city , string state , string country , string zipcode) { Street = street; City = city; State = state; Country = country; ZipCode = zipcode; } protected override IEnumerable<object> GetEqualityComponents() { // Using a yield return statement to return each element one at a time yield return Street; yield return City; yield return State; yield return Country; yield return ZipCode; } }\n```\n\nThis value object implementation of Address has no identity, and therefore no ID field is defined for it, either in the Address class definition or the ValueObject class definition.\n\nHaving no ID field in a class to be used by Entity Framework (EF) was not possible until EF Core 2.0, which greatly helps to implement better value objects with no ID. That is precisely the explanation of the next section.\n\nIt could be argued that value objects, being immutable, should be read-only (that is, have get-only properties), and that's indeed true. However, value objects are usually serialized and deserialized to go\n\nthrough message queues, and being read-only stops the deserializer from assigning values, so you just leave them as private set, which is read-only enough to be practical.", "Value object comparison semantics": "Two instances of the Address type can be compared using all the following methods:\n\n```\nvar one = new Address(\"1 Microsoft Way\" , \"Redmond\" , \"WA\" , \"US\" , \"98052\"); var two = new Address(\"1 Microsoft Way\" , \"Redmond\" , \"WA\" , \"US\" , \"98052\"); Console . WriteLine(EqualityComparer<Address>.Default . Equals(one , two)); // True Console . WriteLine(object . Equals(one , two)); // True Console . WriteLine(one . Equals(two)); // True Console . WriteLine(one == two); // True\n```\n\nWhen all the values are the same, the comparisons are correctly evaluated as true. If you didn't choose to overload the == and != operators, then the last comparison of one == two would evaluate as false. For more information, see Overload ValueObject equality operators .", "How to persist value objects in the database with EF Core 2.0 and later": "You just saw how to define a value object in your domain model. But how can you actually persist it into the database using Entity Framework Core since it usually targets entities with identity?", "Background and older approaches using EF Core 1.1": "As background, a limitation when using EF Core 1.0 and 1.1 was that you could not use complex types as defined in EF 6.x in the traditional .NET Framework. Therefore, if using EF Core 1.0 or 1.1, you needed to store your value object as an EF entity with an ID field. Then, so it looked more like a value object with no identity, you could hide its ID so you make clear that the identity of a value object is not important in the domain model. You could hide that ID by using the ID as a shadow property . Since that configuration for hiding the ID in the model is set up in the EF infrastructure level, it would be kind of transparent for your domain model.\n\nIn the initial version of eShopOnContainers (.NET Core 1.1), the hidden ID needed by EF Core infrastructure was implemented in the following way in the DbContext level, using Fluent API at the infrastructure project. Therefore, the ID was hidden from the domain model point of view, but still present in the infrastructure.\n\n```\n// Old approach with EF Core 1.1 // Fluent API within the OrderingContext:DbContext in the Infrastructure project void ConfigureAddress(EntityTypeBuilder<Address> addressConfiguration) { addressConfiguration . ToTable(\"address\" , DEFAULT_SCHEMA); addressConfiguration . Property<int>(\"Id\") // Id is a shadow property . IsRequired(); addressConfiguration . HasKey(\"Id\"); // Id is a shadow property }\n```\n\nHowever, the persistence of that value object into the database was performed like a regular entity in a different table.\n\nWith EF Core 2.0 and later, there are new and better ways to persist value objects.", "Persist value objects as owned entity types in EF Core 2.0 and later": "Even with some gaps between the canonical value object pattern in DDD and the owned entity type in EF Core, it's currently the best way to persist value objects with EF Core 2.0 and later. You can see limitations at the end of this section.\n\nThe owned entity type feature was added to EF Core since version 2.0.\n\nAn owned entity type allows you to map types that do not have their own identity explicitly defined in the domain model and are used as properties, such as a value object, within any of your entities. An owned entity type shares the same CLR type with another entity type (that is, it's just a regular class). The entity containing the defining navigation is the owner entity. When querying the owner, the owned types are included by default.\n\nJust by looking at the domain model, an owned type looks like it doesn't have any identity. However, under the covers, owned types do have the identity, but the owner navigation property is part of this identity.\n\nThe identity of instances of owned types is not completely their own. It consists of three components:\n\n['The identity of the owner', 'The navigation property pointing to them', 'In the case of collections of owned types, an independent component (supported in EF Core 2.2 and later).']\n\nFor example, in the Ordering domain model at eShopOnContainers, as part of the Order entity, the Address value object is implemented as an owned entity type within the owner entity, which is the Order entity. Address is a type with no identity property defined in the domain model. It is used as a property of the Order type to specify the shipping address for a particular order.\n\nBy convention, a shadow primary key is created for the owned type and it will be mapped to the same table as the owner by using table splitting. This allows to use owned types similarly to how complex types are used in EF6 in the traditional .NET Framework .\n\nIt is important to note that owned types are never discovered by convention in EF Core, so you have to declare them explicitly.\n\nIn eShopOnContainers, in the OrderingContext.cs file, within the OnModelCreating() method, multiple infrastructure configurations are applied. One of them is related to the Order entity.\n\n```\n// Part of the OrderingContext.cs class at the Ordering.Infrastructure project // protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder . ApplyConfiguration(new ClientRequestEntityTypeConfiguration()); modelBuilder . ApplyConfiguration(new PaymentMethodEntityTypeConfiguration()); modelBuilder . ApplyConfiguration(new OrderEntityTypeConfiguration()); modelBuilder . ApplyConfiguration(new OrderItemEntityTypeConfiguration()); //...Additional type configurations }\n```\n\nIn the following code, the persistence infrastructure is defined for the Order entity:\n\n```\n// Part of the OrderEntityTypeConfiguration.cs class // public void Configure(EntityTypeBuilder<Order> orderConfiguration) { orderConfiguration . ToTable(\"orders\" , OrderingContext . DEFAULT_SCHEMA); orderConfiguration . HasKey(o => o . Id); orderConfiguration . Ignore(b => b . DomainEvents); orderConfiguration . Property(o => o . Id) . ForSqlServerUseSequenceHiLo(\"orderseq\" , OrderingContext . DEFAULT_SCHEMA); //Address value object persisted as owned entity in EF Core 2.0 orderConfiguration . OwnsOne(o => o . Address); orderConfiguration . Property<DateTime>(\"OrderDate\").IsRequired(); //...Additional validations, constraints and code... //... }\n```\n\nIn the previous code, the orderConfiguration.OwnsOne(o =&gt; o.Address) method specifies that the Address property is an owned entity of the Order type.\n\nBy default, EF Core conventions name the database columns for the properties of the owned entity type as EntityProperty\\_OwnedEntityProperty. Therefore, the internal properties of Address will appear in the Orders table with the names Address\\_Street, Address\\_City (and so on for State, Country, and ZipCode).\n\nYou can append the Property().HasColumnName() fluent method to rename those columns. In the case where Address is a public property, the mappings would be like the following:\n\n```\norderConfiguration . OwnsOne(p => p . Address) . Property(p=>p . Street).HasColumnName(\"ShippingStreet\"); orderConfiguration . OwnsOne(p => p . Address) . Property(p=>p . City).HasColumnName(\"ShippingCity\");\n```\n\nIt's possible to chain the OwnsOne method in a fluent mapping. In the following hypothetical example, OrderDetails owns BillingAddress and ShippingAddress, which are both Address types. Then OrderDetails is owned by the Order type.\n\n```\norderConfiguration . OwnsOne(p => p . OrderDetails , cb => { cb . OwnsOne(c => c . BillingAddress); cb . OwnsOne(c => c . ShippingAddress); }); //... //... public class Order { public int Id { get; set; } public OrderDetails OrderDetails { get; set; } } public class OrderDetails { public Address BillingAddress { get; set; }\n```\n\n```\npublic Address ShippingAddress { get; set; } } public class Address { public string Street { get; set; } public string City { get; set; } }\n```", "Additional details on owned entity types": ["Owned types are defined when you configure a navigation property to a particular type using the OwnsOne fluent API.", "The definition of an owned type in our metadata model is a composite of: the owner type, the navigation property, and the CLR type of the owned type.", "The identity (key) of an owned type instance in our stack is a composite of the identity of the owner type and the definition of the owned type."], "Owned entities capabilities": ["Owned types can reference other entities, either owned (nested owned types) or non-owned (regular reference navigation properties to other entities).", "You can map the same CLR type as different owned types in the same owner entity through separate navigation properties.", "Table splitting is set up by convention, but you can opt out by mapping the owned type to a different table using ToTable.", "Eager loading is performed automatically on owned types, that is, there's no need to call .Include() on the query.", "Can be configured with attribute [Owned], using EF Core 2.1 and later.", "Can handle collections of owned types (using version 2.2 and later)."], "Owned entities limitations": ["You can't create a DbSet&lt;T&gt; of an owned type (by design).", "You can't call ModelBuilder.Entity&lt;T&gt;() on owned types (currently by design).", "No support for optional (that is, nullable) owned types that are mapped with the owner in the same table (that is, using table splitting). This is because mapping is done for each property, there is no separate sentinel for the null complex value as a whole.", "No inheritance -mapping support for owned types, but you should be able to map two leaf types of the same inheritance hierarchies as different owned types. EF Core will not reason about the fact that they are part of the same hierarchy."], "Main differences with EF6\u2019s complex types": ["Table splitting is optional, that is, they can optionally be mapped to a separate table and still be owned types."], "Implement an Enumeration base class": "The ordering microservice in eShopOnContainers provides a sample Enumeration base class implementation, as shown in the following example:\n\n```\npublic abstract class Enumeration : IComparable { public string Name { get; private set; } public int Id { get; private set; } protected Enumeration(int id , string name) => (Id , Name) = (id , name); public override string ToString() => Name; public static IEnumerable<T> GetAll<T>() where T : Enumeration => typeof(T).GetFields(BindingFlags . Public | BindingFlags . Static | BindingFlags . DeclaredOnly) . Select(f => f . GetValue(null)) . Cast<T>(); public override bool Equals(object obj) { if (obj is not Enumeration otherValue) { return false; } var typeMatches = GetType().Equals(obj . GetType()); var valueMatches = Id . Equals(otherValue . Id); return typeMatches && valueMatches; } public int CompareTo(object other) => Id . CompareTo(((Enumeration)other).Id); // Other utility methods ... } You can use this class as a type in any entity or value object, as for the following CardType : Enumeration class: public class CardType : Enumeration { public static CardType Amex = new(1 , nameof(Amex)); public static CardType Visa = new(2 , nameof(Visa)); public static CardType MasterCard = new(3 , nameof(MasterCard)); public CardType(int id , string name) : base(id , name) {\n```\n\n}", "Design validations in the domain model layer": "In DDD, validation rules can be thought as invariants. The main responsibility of an aggregate is to enforce invariants across state changes for all the entities within that aggregate.\n\nDomain entities should always be valid entities. There are a certain number of invariants for an object that should always be true. For example, an order item object always has to have a quantity that must be a positive integer, plus an article name and price. Therefore, invariants enforcement is the responsibility of the domain entities (especially of the aggregate root) and an entity object should not be able to exist without being valid. Invariant rules are simply expressed as contracts, and exceptions or notifications are raised when they are violated.\n\nThe reasoning behind this is that many bugs occur because objects are in a state they should never have been in.\n\nLet's propose we now have a SendUserCreationEmailService that takes a UserProfile \u2026 how can we rationalize in that service that Name is not null? Do we check it again? Or more likely \u2026 you just don't bother to check and \"hope for the best\"\u2014you hope that someone bothered to validate it before sending it to you. Of course, using TDD one of the first tests we should be writing is that if I send a customer with a null name that it should raise an error. But once we start writing these kinds of tests over and over again we realize \u2026 \"what if we never allowed name to become null? we wouldn't have all of these tests!\".\n\n}", "Implement validations in the domain model layer": "Validations are usually implemented in domain entity constructors or in methods that can update the entity. There are multiple ways to implement validations, such as verifying data and raising exceptions if the validation fails. There are also more advanced patterns such as using the Specification pattern for validations, and the Notification pattern to return a collection of errors instead of returning an exception for each validation as it occurs.", "Validate conditions and throw exceptions": "The following code example shows the simplest approach to validation in a domain entity by raising an exception. In the references table at the end of this section you can see links to more advanced implementations based on the patterns we have discussed previously.\n\n```\npublic void SetAddress(Address address) { _shippingAddress = address?? throw new ArgumentNullException(nameof(address)); } A better example would demonstrate the need to ensure that either the internal state did not change, or that all the mutations for a method occurred. For example, the following implementation would leave the object in an invalid state: public void SetAddress(string line1 , string line2 , string city , string state , int zip) { _shippingAddress . line1 = line1 ?? throw new ... _shippingAddress . line2 = line2; _shippingAddress . city = city ?? throw new ... _shippingAddress . state = (IsValid(state) ? state : throw new \u2026 ); }\n```\n\nIf the value of the state is invalid, the first address line and the city have already been changed. That might make the address invalid.\n\nA similar approach can be used in the entity's constructor, raising an exception to make sure that the entity is valid once it is created.", "Use validation attributes in the model based on data annotations": "Data annotations, like the Required or MaxLength attributes, can be used to configure EF Core database field properties, as explained in detail in the Table mapping section, but they no longer work for entity validation in EF Core (neither does the IValidatableObject.Validate method), as they have done since EF 4.x in .NET Framework.\n\nData annotations and the IValidatableObject interface can still be used for model validation during model binding, prior to the controller's actions invocation as usual, but that model is meant to be a ViewModel or DTO and that's an MVC or API concern not a domain model concern.\n\nHaving made the conceptual difference clear, you can still use data annotations and IValidatableObject in the entity class for validation, if your actions receive an entity class object parameter, which is not recommended. In that case, validation will occur upon model binding, just before invoking the action and you can check the controller's ModelState.IsValid property to check\n\nthe result, but then again, it happens in the controller, not before persisting the entity object in the DbContext, as it had done since EF 4.x.\n\nYou can still implement custom validation in the entity class using data annotations and the IValidatableObject.Validate method, by overriding the DbContext's SaveChanges method.\n\nYou can see a sample implementation for validating IValidatableObject entities in this comment on GitHub. That sample doesn't do attribute-based validations, but they should be easy to implement using reflection in the same override.\n\nHowever, from a DDD point of view, the domain model is best kept lean with the use of exceptions in your entity's behavior methods, or by implementing the Specification and Notification patterns to enforce validation rules.\n\nIt can make sense to use data annotations at the application layer in ViewModel classes (instead of domain entities) that will accept input, to allow for model validation within the UI layer. However, this should not be done at the exclusion of validation within the domain model.", "Validate entities by implementing the Specification pattern and the Notification pattern": "Finally, a more elaborate approach to implementing validations in the domain model is by implementing the Specification pattern in conjunction with the Notification pattern, as explained in some of the additional resources listed later.\n\nIt is worth mentioning that you can also use just one of those patterns\u2014for example, validating manually with control statements, but using the Notification pattern to stack and return a list of validation errors.", "Use deferred validation in the domain": "There are various approaches to deal with deferred validations in the domain. In his book Implementing Domain-Driven Design, Vaughn Vernon discusses these in the section on validation.", "Two -step validation": "Also consider two -step validation. Use field-level validation on your command Data Transfer Objects (DTOs) and domain-level validation inside your entities. You can do this by returning a result object instead of exceptions in order to make it easier to deal with the validation errors.\n\nUsing field validation with data annotations, for example, you do not duplicate the validation definition. The execution, though, can be both server-side and client-side in the case of DTOs (commands and ViewModels, for instance).", "Client -side validation (validation in the presentation layers)": "Even when the source of truth is the domain model and ultimately you must have validation at the domain model level, validation can still be handled at both the domain model level (server side) and the UI (client side).\n\nClient -side validation is a great convenience for users. It saves time they would otherwise spend waiting for a round trip to the server that might return validation errors. In business terms, even a few fractions of seconds multiplied hundreds of times each day adds up to a lot of time, expense, and frustration. Straightforward and immediate validation enables users to work more efficiently and produce better quality input and output.\n\nJust as the view model and the domain model are different, view model validation and domain model validation might be similar but serve a different purpose. If you are concerned about DRY (the Don't Repeat Yourself principle), consider that in this case code reuse might also mean coupling, and in enterprise applications it is more important not to couple the server side to the client side than to follow the DRY principle.\n\nEven when using client-side validation, you should always validate your commands or input DTOs in server code, because the server APIs are a possible attack vector. Usually, doing both is your best bet because if you have a client application, from a UX perspective, it is best to be proactive and not allow the user to enter invalid information.\n\nTherefore, in client -side code you typically validate the ViewModels. You could also validate the client output DTOs or commands before you send them to the services.\n\nThe implementation of client-side validation depends on what kind of client application you are building. It will be different if you are validating data in a web MVC web application with most of the code in .NET, a SPA web application with that validation being coded in JavaScript or TypeScript, or a mobile app coded with Xamarin and C#.", "Validation in Xamarin mobile apps": ["Validate Text Input and Show Errors"], "Validation in ASP.NET Core apps": ["Rick Anderson. Adding validation https://learn.microsoft.com/aspnet/core/tutorials/first-mvc-app/validation"], "Validation in SPA Web apps (Angular 2, TypeScript, JavaScript, Blazor WebAssembly)": ["Form Validation https://angular.io/guide/form-validation", "Validation. Breeze documentation. https://breeze.github.io/doc-js/validation.html", "ASP.NET Core Blazor forms and input components"], "Domain events: Design and implementation": "Use domain events to explicitly implement side effects of changes within your domain. In other words, and using DDD terminology, use domain events to explicitly implement side effects across multiple aggregates. Optionally, for better scalability and less impact in database locks, use eventual consistency between aggregates within the same domain.", "What is a domain event?": "An event is something that has happened in the past. A domain event is, something that happened in the domain that you want other parts of the same domain (in-process) to be aware of. The notified parts usually react somehow to the events.\n\nAn important benefit of domain events is that side effects can be expressed explicitly.\n\nFor example, if you're just using Entity Framework and there has to be a reaction to some event, you would probably code whatever you need close to what triggers the event. So the rule gets coupled, implicitly, to the code, and you have to look into the code to, hopefully, realize the rule is implemented there.\n\nOn the other hand, using domain events makes the concept explicit, because there's a DomainEvent and at least one DomainEventHandler involved.\n\nFor example, in the eShopOnContainers application, when an order is created, the user becomes a buyer, so an OrderStartedDomainEvent is raised and handled in the\n\nValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler, so the underlying concept is evident.\n\nIn short, domain events help you to express, explicitly, the domain rules, based in the ubiquitous language provided by the domain experts. Domain events also enable a better separation of concerns among classes within the same domain.\n\nIt's important to ensure that, just like a database transaction, either all the operations related to a domain event finish successfully or none of them do.\n\nDomain events are similar to messaging-style events, with one important difference. With real messaging, message queuing, message brokers, or a service bus using AMQP, a message is always sent asynchronously and communicated across processes and machines. This is useful for integrating multiple Bounded Contexts, microservices, or even different applications. However, with domain events, you want to raise an event from the domain operation you're currently running, but you want any side effects to occur within the same domain.\n\nThe domain events and their side effects (the actions triggered afterwards that are managed by event handlers) should occur almost immediately, usually in-process, and within the same domain. Thus, domain events could be synchronous or asynchronous. Integration events, however, should always be asynchronous.", "Domain events versus integration events": "Semantically, domain and integration events are the same thing: notifications about something that just happened. However, their implementation must be different. Domain events are just messages pushed to a domain event dispatcher, which could be implemented as an in-memory mediator based on an IoC container or any other method.\n\nOn the other hand, the purpose of integration events is to propagate committed transactions and updates to additional subsystems, whether they are other microservices, Bounded Contexts or even\n\nData\n\nBehavior\n\nOrder Aggregate external applications. Hence, they should occur only if the entity is successfully persisted, otherwise it's as if the entire operation never happened. Buyer Aggregate --\n\nAs mentioned before, integration events must be based on asynchronous communication between multiple microservices (other Bounded Contexts) or even external systems/applications.\n\nThus, the event bus interface needs some infrastructure that allows inter -process and distributed communication between potentially remote services. It can be based on a commercial service bus, queues, a shared database used as a mailbox, or any other distributed and ideally push based messaging system.", "Domain events as a preferred way to trigger side effects across multiple aggregates within the same domain": "If executing a command related to one aggregate instance requires additional domain rules to be run on one or more additional aggregates, you should design and implement those side effects to be triggered by domain events. As shown in Figure 7-14, and as one of the most important use cases, a domain event should be used to propagate state changes across multiple aggregates within the same domain model.\n\nFigure 7 -14. Domain events to enforce consistency between multiple aggregates within the same domain\n\n<!-- image -->\n\nFigure 7-14 shows how consistency between aggregates is achieved by domain events. When the user initiates an order, the Order Aggregate sends an OrderStarted domain event. The OrderStarted domain event is handled by the Buyer Aggregate to create a Buyer object in the ordering microservice, based on the original user info from the identity microservice (with information provided in the CreateOrder command).\n\nAlternately, you can have the aggregate root subscribed for events raised by members of its aggregates (child entities). For instance, each OrderItem child entity can raise an event when the item price is higher than a specific amount, or when the product item amount is too high. The aggregate root can then receive those events and perform a global calculation or aggregation.\n\nDomain Model\n\n(Ordering microservice)\n\nIt's important to understand that this event-based communication is not implemented directly within the aggregates; you need to implement domain event handlers.\n\nHandling the domain events is an application concern. The domain model layer should only focus on the domain logic\u2014things that a domain expert would understand, not application infrastructure like handlers and side -effect persistence actions using repositories. Therefore, the application layer level is where you should have domain event handlers triggering actions when a domain event is raised.\n\nDomain events can also be used to trigger any number of application actions, and what is more important, must be open to increase that number in the future in a decoupled way. For instance, when the order is started, you might want to publish a domain event to propagate that info to other aggregates or even to raise application actions like notifications.\n\nThe key point is the open number of actions to be executed when a domain event occurs. Eventually, the actions and rules in the domain and application will grow. The complexity or number of sideeffect actions when something happens will grow, but if your code were coupled with \"glue\" (that is, creating specific objects with new), then every time you needed to add a new action you would also need to change working and tested code.\n\nThis change could result in new bugs and this approach also goes against the Open/Closed principle from SOLID. Not only that, the original class that was orchestrating the operations would grow and grow, which goes against the Single Responsibility Principle (SRP) .\n\nOn the other hand, if you use domain events, you can create a fine-grained and decoupled implementation by segregating responsibilities using this approach:\n\n['Send a command (for example, CreateOrder).', 'Receive the command in a command handler.', \"\u2013 Execute a single aggregate's transaction.\", '\u2013 (Optional) Raise domain events for side effects (for example, OrderStartedDomainEvent).', 'Handle domain events (within the current process) that will execute an open number of side effects in multiple aggregates or application actions. For example:', '\u2013 Verify or create buyer and payment method.', '\u2013 Create and send a related integration event to the event bus to propagate states across microservices or trigger external actions like sending an email to the buyer.', '\u2013 Handle other side effects.']\n\nAs shown in Figure 7-15, starting from the same domain event, you can handle multiple actions related to other aggregates in the domain or additional application actions you need to perform across microservices connecting with integration events and the event bus.\n\nDomain\n\nAggregate (Order)\n\nOrder (Aggregate Root)\n\nData\n\nBehavior\n\nApplication\n\nEvent Handler 1\n\nFigure 7 -15. Handling multiple actions per domain\n\n<!-- image -->\n\nThere can be several handlers for the same domain event in the Application Layer, one handler can solve consistency between aggregates and another handler can publish an integration event, so other microservices can do something with it. The event handlers are typically in the application layer, because you'll use infrastructure objects like repositories or an application API for the microservice's behavior. In that sense, event handlers are similar to command handlers, so both are part of the application layer. The important difference is that a command should be processed only once. A domain event could be processed zero or n times, because it can be received by multiple receivers or event handlers with a different purpose for each handler.\n\nHaving an open number of handlers per domain event allows you to add as many domain rules as needed, without affecting current code. For instance, implementing the following business rule might be as easy as adding a few event handlers (or even just one):\n\nWhen the total amount purchased by a customer in the store, across any number of orders, exceeds $6,000, apply a 10% off discount to every new order and notify the customer with an email about that discount for future orders.", "Implement domain events": "In C#, a domain event is simply a data-holding structure or class, like a DTO, with all the information related to what just happened in the domain, as shown in the following example:\n\n```\npublic class OrderStartedDomainEvent : INotification { public string UserId { get; } public string UserName { get; } public int CardTypeId { get; } public string CardNumber { get; } public string CardSecurityNumber { get; }\n```\n\n```\npublic string CardHolderName { get; } public DateTime CardExpiration { get; } public Order Order { get; } public OrderStartedDomainEvent(Order order , string userId , string userName , int cardTypeId , string cardNumber , string cardSecurityNumber , string cardHolderName , DateTime cardExpiration) { Order = order; UserId = userId; UserName = userName; CardTypeId = cardTypeId; CardNumber = cardNumber; CardSecurityNumber = cardSecurityNumber; CardHolderName = cardHolderName; CardExpiration = cardExpiration; } }\n```\n\nThis is essentially a class that holds all the data related to the OrderStarted event.\n\nIn terms of the ubiquitous language of the domain, since an event is something that happened in the past, the class name of the event should be represented as a past-tense verb, like OrderStartedDomainEvent or OrderShippedDomainEvent. That's how the domain event is implemented in the ordering microservice in eShopOnContainers.\n\nAs noted earlier, an important characteristic of events is that since an event is something that happened in the past, it shouldn't change. Therefore, it must be an immutable class. You can see in the previous code that the properties are read-only. There's no way to update the object, you can only set values when you create it.\n\nIt's important to highlight here that if domain events were to be handled asynchronously, using a queue that required serializing and deserializing the event objects, the properties would have to be \"private set\" instead of read-only, so the deserializer would be able to assign the values upon dequeuing. This is not an issue in the Ordering microservice, as the domain event pub/sub is implemented synchronously using MediatR.", "Raise domain events": "The next question is how to raise a domain event so it reaches its related event handlers. You can use multiple approaches.\n\nUdi Dahan originally proposed (for example, in several related posts, such as Domain Events \u2013 Take 2) using a static class for managing and raising the events. This might include a static class named DomainEvents that would raise domain events immediately when it's called, using syntax like DomainEvents.Raise(Event myEvent). Jimmy Bogard wrote a blog post (Strengthening your domain: Domain Events) that recommends a similar approach.\n\nHowever, when the domain events class is static, it also dispatches to handlers immediately. This makes testing and debugging more difficult, because the event handlers with side-effects logic are executed immediately after the event is raised. When you're testing and debugging, you just want to focus on what is happening in the current aggregate classes; you don't want to suddenly be\n\nredirected to other event handlers for side effects related to other aggregates or application logic. This is why other approaches have evolved, as explained in the next section.", "The deferred approach to raise and dispatch events": "Instead of dispatching to a domain event handler immediately, a better approach is to add the domain events to a collection and then to dispatch those domain events right before or right after committing the transaction (as with SaveChanges in EF). (This approach was described by Jimmy Bogard in this post A better domain events pattern.)\n\nDeciding if you send the domain events right before or right after committing the transaction is important, since it determines whether you will include the side effects as part of the same transaction or in different transactions. In the latter case, you need to deal with eventual consistency across multiple aggregates. This topic is discussed in the next section.\n\nThe deferred approach is what eShopOnContainers uses. First, you add the events happening in your entities into a collection or list of events per entity. That list should be part of the entity object, or even better, part of your base entity class, as shown in the following example of the Entity base class:\n\n```\npublic abstract class Entity { //... private List<INotification> _domainEvents; public List<INotification> DomainEvents => _domainEvents; public void AddDomainEvent(INotification eventItem) { _domainEvents = _domainEvents ?? new List<INotification>(); _domainEvents . Add(eventItem); } public void RemoveDomainEvent(INotification eventItem) { _domainEvents?.Remove(eventItem); } //... Additional code }\n```\n\nWhen you want to raise an event, you just add it to the event collection from code at any method of the aggregate-root entity.\n\nThe following code, part of the Order aggregate-root at eShopOnContainers, shows an example:\n\n```\nvar orderStartedDomainEvent = new OrderStartedDomainEvent(this , //Order object cardTypeId , cardNumber , cardSecurityNumber , cardHolderName , cardExpiration); this . AddDomainEvent(orderStartedDomainEvent);\n```\n\nNotice that the only thing that the AddDomainEvent method is doing is adding an event to the list. No event is dispatched yet, and no event handler is invoked yet.\n\nYou actually want to dispatch the events later on, when you commit the transaction to the database. If you are using Entity Framework Core, that means in the SaveChanges method of your EF DbContext, as in the following code:\n\n```\n// EF Core DbContext public class OrderingContext : DbContext , IUnitOfWork { // ... public async Task<bool> SaveEntitiesAsync(CancellationToken cancellationToken = default(CancellationToken)) { // Dispatch Domain Events collection. // Choices: // A) Right BEFORE committing data (EF SaveChanges) into the DB. This makes // a single transaction including side effects from the domain event // handlers that are using the same DbContext with Scope lifetime // B) Right AFTER committing data (EF SaveChanges) into the DB. This makes // multiple transactions. You will need to handle eventual consistency and // compensatory actions in case of failures. await _mediator . DispatchDomainEventsAsync(this); // After this line runs, all the changes (from the Command Handler and Domain // event handlers) performed through the DbContext will be committed var result = await base . SaveChangesAsync(); } }\n```\n\nWith this code, you dispatch the entity events to their respective event handlers.\n\nThe overall result is that you've decoupled the raising of a domain event (a simple add into a list in memory) from dispatching it to an event handler. In addition, depending on what kind of dispatcher you are using, you could dispatch the events synchronously or asynchronously.\n\nBe aware that transactional boundaries come into significant play here. If your unit of work and transaction can span more than one aggregate (as when using EF Core and a relational database), this can work well. But if the transaction cannot span aggregates, you have to implement additional steps to achieve consistency. This is another reason why persistence ignorance is not universal; it depends on the storage system you use.", "Single transaction across aggregates versus eventual consistency across aggregates": "The question of whether to perform a single transaction across aggregates versus relying on eventual consistency across those aggregates is a controversial one. Many DDD authors like Eric Evans and Vaughn Vernon advocate the rule that one transaction = one aggregate and therefore argue for eventual consistency across aggregates. For example, in his book Domain-Driven Design, Eric Evans says this:\n\nAny rule that spans Aggregates will not be expected to be up-to-date at all times. Through event processing, batch processing, or other update mechanisms, other dependencies can be resolved within some specific time. (page 128)\n\nVaughn Vernon says the following in Effective Aggregate Design. Part II: Making Aggregates Work Together:\n\nThus, if executing a command on one aggregate instance requires that additional business rules execute on one or more aggregates, use eventual consistency [\u2026] There is a practical way to support eventual consistency in a DDD model. An aggregate method publishes a domain event that is in time delivered to one or more asynchronous subscribers.\n\nThis rationale is based on embracing fine-grained transactions instead of transactions spanning many aggregates or entities. The idea is that in the second case, the number of database locks will be substantial in large-scale applications with high scalability needs. Embracing the fact that highly scalable applications need not have instant transactional consistency between multiple aggregates helps with accepting the concept of eventual consistency. Atomic changes are often not needed by the business, and it is in any case the responsibility of the domain experts to say whether particular operations need atomic transactions or not. If an operation always needs an atomic transaction between multiple aggregates, you might ask whether your aggregate should be larger or wasn't correctly designed.\n\nHowever, other developers and architects like Jimmy Bogard are okay with spanning a single transaction across several aggregates\u2014but only when those additional aggregates are related to side effects for the same original command. For instance, in A better domain events pattern, Bogard says this:\n\nTypically, I want the side effects of a domain event to occur within the same logical transaction, but not necessarily in the same scope of raising the domain event [\u2026] Just before we commit our transaction, we dispatch our events to their respective handlers.\n\nIf you dispatch the domain events right before committing the original transaction, it is because you want the side effects of those events to be included in the same transaction. For example, if the EF DbContext SaveChanges method fails, the transaction will roll back all changes, including the result of any side effect operations implemented by the related domain event handlers. This is because the DbContext life scope is by default defined as \"scoped.\" Therefore, the DbContext object is shared across multiple repository objects being instantiated within the same scope or object graph. This coincides with the HttpRequest scope when developing Web API or MVC apps.\n\nActually, both approaches (single atomic transaction and eventual consistency) can be right. It really depends on your domain or business requirements and what the domain experts tell you. It also depends on how scalable you need the service to be (more granular transactions have less impact with regard to database locks). And it depends on how much investment you're willing to make in your code, since eventual consistency requires more complex code in order to detect possible inconsistencies across aggregates and the need to implement compensatory actions. Consider that if you commit changes to the original aggregate and afterwards, when the events are being dispatched, if there's an issue and the event handlers cannot commit their side effects, you'll have inconsistencies between aggregates.\n\nA way to allow compensatory actions would be to store the domain events in additional database tables so they can be part of the original transaction. Afterwards, you could have a batch process that detects inconsistencies and runs compensatory actions by comparing the list of events with the current state of the aggregates. The compensatory actions are part of a complex topic that will require deep analysis from your side, which includes discussing it with the business user and domain experts.\n\nIn any case, you can choose the approach you need. But the initial deferred approach\u2014raising the events before committing, so you use a single transaction\u2014is the simplest approach when using EF Core and a relational database. It's easier to implement and valid in many business cases. It's also the approach used in the ordering microservice in eShopOnContainers.\n\nBut how do you actually dispatch those events to their respective event handlers? What's the \\_mediator object you see in the previous example? It has to do with the techniques and artifacts you use to map between events and their event handlers.", "The domain event dispatcher: mapping from events to event handlers": "Once you're able to dispatch or publish the events, you need some kind of artifact that will publish the event, so that every related handler can get it and process side effects based on that event.\n\nOne approach is a real messaging system or even an event bus, possibly based on a service bus as opposed to in-memory events. However, for the first case, real messaging would be overkill for processing domain events, since you just need to process those events within the same process (that is, within the same domain and application layer).", "How to subscribe to domain events": "When you use MediatR, each event handler must use an event type that is provided on the generic parameter of the INotificationHandler interface, as you can see in the following code:\n\n```\npublic class ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler : INotificationHandler<OrderStartedDomainEvent>\n```\n\nBased on the relationship between event and event handler, which can be considered the subscription, the MediatR artifact can discover all the event handlers for each event and trigger each one of those event handlers.", "How to handle domain events": "Finally, the event handler usually implements application layer code that uses infrastructure repositories to obtain the required additional aggregates and to execute side-effect domain logic. The following domain event handler code at eShopOnContainers, shows an implementation example.\n\n```\npublic class ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler : INotificationHandler<OrderStartedDomainEvent> { private readonly ILogger _logger; private readonly IBuyerRepository _buyerRepository; private readonly IOrderingIntegrationEventService _orderingIntegrationEventService; public ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler( ILogger<ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler> logger , IBuyerRepository buyerRepository , IOrderingIntegrationEventService orderingIntegrationEventService) { _buyerRepository = buyerRepository ?? throw new ArgumentNullException(nameof(buyerRepository)); _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new ArgumentNullException(nameof(orderingIntegrationEventService));\n```\n\n```\n_logger = logger ?? throw new ArgumentNullException(nameof(logger)); } public async Task Handle( OrderStartedDomainEvent domainEvent , CancellationToken cancellationToken) { var cardTypeId = domainEvent . CardTypeId != 0 ? domainEvent . CardTypeId : 1; var buyer = await _buyerRepository . FindAsync(domainEvent . UserId); var buyerExisted = buyer is not null; if (!buyerExisted) { buyer = new Buyer(domainEvent . UserId , domainEvent . UserName); } buyer . VerifyOrAddPaymentMethod( cardTypeId , $\"Payment Method on {DateTime.UtcNow}\" , domainEvent . CardNumber , domainEvent . CardSecurityNumber , domainEvent . CardHolderName , domainEvent . CardExpiration , domainEvent . Order . Id); var buyerUpdated = buyerExisted ? _buyerRepository . Update(buyer) : _buyerRepository . Add(buyer); await _buyerRepository . UnitOfWork . SaveEntitiesAsync(cancellationToken); var integrationEvent = new OrderStatusChangedToSubmittedIntegrationEvent( domainEvent . Order . Id , domainEvent . Order . OrderStatus . Name , buyer . Name); await _orderingIntegrationEventService . AddAndSaveEventAsync(integrationEvent); OrderingApiTrace . LogOrderBuyerAndPaymentValidatedOrUpdated( _logger , buyerUpdated . Id , domainEvent . Order . Id); } }\n```\n\nThe previous domain event handler code is considered application layer code because it uses infrastructure repositories, as explained in the next section on the infrastructure-persistence layer. Event handlers could also use other infrastructure components.", "Domain events can generate integration events to be published outside of the microservice boundaries": "Finally, it's important to mention that you might sometimes want to propagate events across multiple microservices. That propagation is an integration event, and it could be published through an event bus from any specific domain event handler.", "Conclusions on domain events": "As stated, use domain events to explicitly implement side effects of changes within your domain. To use DDD terminology, use domain events to explicitly implement side effects across one or multiple aggregates. Additionally, and for better scalability and less impact on database locks, use eventual consistency between aggregates within the same domain.\n\nThe reference app uses MediatR to propagate domain events synchronously across aggregates, within a single transaction. However, you could also use some AMQP implementation like RabbitMQ or Azure Service Bus to propagate domain events asynchronously, using eventual consistency but, as mentioned above, you have to consider the need for compensatory actions in case of failures.", "Design the infrastructure persistence layer": "Data persistence components provide access to the data hosted within the boundaries of a microservice (that is, a microservice's database). They contain the actual implementation of components such as repositories and Unit of Work classes, like custom Entity Framework (EF) DbContext objects. EF DbContext implements both the Repository and the Unit of Work patterns.", "The Repository pattern": "The Repository pattern is a Domain-Driven Design pattern intended to keep persistence concerns outside of the system's domain model. One or more persistence abstractions - interfaces - are defined\n\nin the domain model, and these abstractions have implementations in the form of persistence-specific adapters defined elsewhere in the application.\n\nRepository implementations are classes that encapsulate the logic required to access data sources. They centralize common data access functionality, providing better maintainability and decoupling the infrastructure or technology used to access databases from the domain model. If you use an Object-Relational Mapper (ORM) like Entity Framework, the code that must be implemented is simplified, thanks to LINQ and strong typing. This lets you focus on the data persistence logic rather than on data access plumbing.\n\nThe Repository pattern is a well-documented way of working with a data source. In the book Patterns of Enterprise Application Architecture, Martin Fowler describes a repository as follows:\n\nA repository performs the tasks of an intermediary between the domain model layers and data mapping, acting in a similar way to a set of domain objects in memory. Client objects declaratively build queries and send them to the repositories for answers. Conceptually, a repository encapsulates a set of objects stored in the database and operations that can be performed on them, providing a way that is closer to the persistence layer. Repositories, also, support the purpose of separating, clearly and in one direction, the dependency between the work domain and the data allocation or mapping.", "Define one repository per aggregate": "For each aggregate or aggregate root, you should create one repository class. You may be able to leverage C# Generics to reduce the total number concrete classes you need to maintain (as demonstrated later in this chapter). In a microservice based on Domain-Driven Design (DDD) patterns, the only channel you should use to update the database should be the repositories. This is because they have a one-to-one relationship with the aggregate root, which controls the aggregate's invariants and transactional consistency. It's okay to query the database through other channels (as you can do following a CQRS approach), because queries don't change the state of the database. However, the transactional area (that is, the updates) must always be controlled by the repositories and the aggregate roots.\n\nBasically, a repository allows you to populate data in memory that comes from the database in the form of the domain entities. Once the entities are in memory, they can be changed and then persisted back to the database through transactions.\n\nAs noted earlier, if you're using the CQS/CQRS architectural pattern, the initial queries are performed by side queries out of the domain model, performed by simple SQL statements using Dapper. This approach is much more flexible than repositories because you can query and join any tables you need, and these queries aren't restricted by rules from the aggregates. That data goes to the presentation layer or client app.\n\nIf the user makes changes, the data to be updated comes from the client app or presentation layer to the application layer (such as a Web API service). When you receive a command in a command handler, you use repositories to get the data you want to update from the database. You update it in memory with the data passed with the commands, and you then add or update the data (domain entities) in the database through a transaction.\n\nLayer\n\nOrder (Aggregate Root)\n\nData\n\nIt's important to emphasize again that you should only define one repository for each aggregate root, as shown in Figure 7-17. To achieve the goal of the aggregate root to maintain transactional consistency between all the objects within the aggregate, you should never create a repository for each table in the database.\n\nInfrastructure-\n\nPersistence\n\nLayer\n\nData\n\nTier\n\nFigure 7 -17. The relationship between repositories, aggregates, and database tables\n\n<!-- image -->\n\nThe above diagram shows the relationships between Domain and Infrastructure layers: Buyer Aggregate depends on the IBuyerRepository and Order Aggregate depends on the IOrderRepository interfaces, these interfaces are implemented in the Infrastructure layer by the corresponding repositories that depend on UnitOfWork, also implemented there, that accesses the tables in the Data tier.", "Enforce one aggregate root per repository": "It can be valuable to implement your repository design in such a way that it enforces the rule that only aggregate roots should have repositories. You can create a generic or base repository type that constrains the type of entities it works with to ensure they have the IAggregateRoot marker interface.\n\nThus, each repository class implemented at the infrastructure layer implements its own contract or interface, as shown in the following code:\n\n```\nnamespace Microsoft . eShopOnContainers . Services . Ordering . Infrastructure . Repositories { public class OrderRepository : IOrderRepository { // ...\n```\n\nBuyer Aggregate\n\nBuyer (Aggregate Root)\n\nOrder Aggregate\n\nAddress (Value-Object)\n\nData\n\nBehavior\n\n```\n} }\n```\n\nEach specific repository interface implements the generic IRepository interface:\n\n```\npublic interface IOrderRepository : IRepository<Order> { Order Add(Order order); // ... }\n```\n\nHowever, a better way to have the code enforce the convention that each repository is related to a single aggregate is to implement a generic repository type. That way, it's explicit that you're using a repository to target a specific aggregate. That can be easily done by implementing a generic IRepository base interface, as in the following code:\n\n```\npublic interface IRepository<T> where T : IAggregateRoot { //.... }\n```", "The Repository pattern makes it easier to test your application logic": "The Repository pattern allows you to easily test your application with unit tests. Remember that unit tests only test your code, not infrastructure, so the repository abstractions make it easier to achieve that goal.\n\nAs noted in an earlier section, it's recommended that you define and place the repository interfaces in the domain model layer so the application layer, such as your Web API microservice, doesn't depend directly on the infrastructure layer where you've implemented the actual repository classes. By doing this and using Dependency Injection in the controllers of your Web API, you can implement mock repositories that return fake data instead of data from the database. This decoupled approach allows you to create and run unit tests that focus the logic of your application without requiring connectivity to the database.\n\nConnections to databases can fail and, more importantly, running hundreds of tests against a database is bad for two reasons. First, it can take a long time because of the large number of tests. Second, the database records might change and impact the results of your tests, especially if your tests are running in parallel, so that they might not be consistent. Unit tests typically can run in parallel; integration tests may not support parallel execution depending on their implementation. Testing against the database isn't a unit test but an integration test. You should have many unit tests running fast, but fewer integration tests against the databases.\n\nIn terms of separation of concerns for unit tests, your logic operates on domain entities in memory. It assumes the repository class has delivered those. Once your logic modifies the domain entities, it assumes the repository class will store them correctly. The important point here is to create unit tests against your domain model and its domain logic. Aggregate roots are the main consistency boundaries in DDD.\n\nThe repositories implemented in eShopOnContainers rely on EF Core's DbContext implementation of the Repository and Unit of Work patterns using its change tracker, so they don't duplicate this functionality.", "The difference between the Repository pattern and the legacy Data Access class (DAL class) pattern": "A typical DAL object directly performs data access and persistence operations against storage, often at the level of a single table and row. Simple CRUD operations implemented with a set of DAL classes frequently do not support transactions (though this is not always the case). Most DAL class approaches make minimal use of abstractions, resulting in tight coupling between application or Business Logic Layer (BLL) classes that call the DAL objects.\n\nWhen using repository, the implementation details of persistence are encapsulated away from the domain model. The use of an abstraction provides ease of extending behavior through patterns like Decorators or Proxies. For instance, cross-cutting concerns like caching, logging, and error handling can all be applied using these patterns rather than hard-coded in the data access code itself. It's also trivial to support multiple repository adapters which may be used in different environments, from local development to shared staging environments to production.", "Implementing Unit of Work": "A unit of work refers to a single transaction that involves multiple insert, update, or delete operations. In simple terms, it means that for a specific user action, such as a registration on a website, all the insert, update, and delete operations are handled in a single transaction. This is more efficient than handling multiple database operations in a chattier way.\n\nThese multiple persistence operations are performed later in a single action when your code from the application layer commands it. The decision about applying the in-memory changes to the actual database storage is typically based on the Unit of Work pattern. In EF, the Unit of Work pattern is implemented by a DbContext and is executed when a call is made to SaveChanges.\n\nIn many cases, this pattern or way of applying operations against the storage can increase application performance and reduce the possibility of inconsistencies. It also reduces transaction blocking in the database tables, because all the intended operations are committed as part of one transaction. This is more efficient in comparison to executing many isolated operations against the database. Therefore, the selected ORM can optimize the execution against the database by grouping several update actions within the same transaction, as opposed to many small and separate transaction executions.\n\nThe Unit of Work pattern can be implemented with or without using the Repository pattern.", "Repositories shouldn\u2019t be mandatory": "Custom repositories are useful for the reasons cited earlier, and that is the approach for the ordering microservice in eShopOnContainers. However, it isn't an essential pattern to implement in a DDD design or even in general .NET development.\n\nFor instance, Jimmy Bogard, when providing direct feedback for this guide, said the following:\n\nThis'll probably be my biggest feedback. I'm really not a fan of repositories, mainly because they hide the important details of the underlying persistence mechanism. It's why I go for MediatR for commands, too. I can use the full power of the persistence layer, and push all that domain behavior into my aggregate roots. I don't usually want to mock my repositories \u2013 I still need to have that\n\nintegration test with the real thing. Going CQRS meant that we didn't really have a need for repositories any more.\n\nRepositories might be useful, but they are not critical for your DDD design in the way that the Aggregate pattern and a rich domain model are. Therefore, use the Repository pattern or not, as you see fit.", "Repository pattern": ["Edward Hieatt and Rob Mee. Repository pattern. https://martinfowler.com/eaaCatalog/repository.html", "The Repository pattern"], "Unit of Work pattern": ["Martin Fowler. Unit of Work pattern. https://martinfowler.com/eaaCatalog/unitOfWork.html", "Implementing the Repository and Unit of Work Patterns in an ASP.NET MVC Application"], "Implement the infrastructure persistence layer with Entity Framework Core": "When you use relational databases such as SQL Server, Oracle, or PostgreSQL, a recommended approach is to implement the persistence layer based on Entity Framework (EF). EF supports LINQ and provides strongly typed objects for your model, as well as simplified persistence into your database.\n\nEntity Framework has a long history as part of the .NET Framework. When you use .NET, you should also use Entity Framework Core, which runs on Windows or Linux in the same way as .NET. EF Core is a complete rewrite of Entity Framework that's implemented with a much smaller footprint and important improvements in performance.", "Introduction to Entity Framework Core": "Entity Framework (EF) Core is a lightweight, extensible, and cross-platform version of the popular Entity Framework data access technology. It was introduced with .NET Core in mid-2016.\n\nSince an introduction to EF Core is already available in Microsoft documentation, here we simply provide links to that information.", "Infrastructure in Entity Framework Core from a DDD perspective": "From a DDD point of view, an important capability of EF is the ability to use POCO domain entities, also known in EF terminology as POCO code-first entities. If you use POCO domain entities, your domain model classes are persistence-ignorant, following the Persistence Ignorance and the Infrastructure Ignorance principles.\n\nPer DDD patterns, you should encapsulate domain behavior and rules within the entity class itself, so it can control invariants, validations, and rules when accessing any collection. Therefore, it is not a good practice in DDD to allow public access to collections of child entities or value objects. Instead, you want to expose methods that control how and when your fields and property collections can be updated, and what behavior and actions should occur when that happens.\n\nSince EF Core 1.1, to satisfy those DDD requirements, you can have plain fields in your entities instead of public properties. If you do not want an entity field to be externally accessible, you can just create the attribute or field instead of a property. You can also use private property setters.\n\nIn a similar way, you can now have read-only access to collections by using a public property typed as IReadOnlyCollection&lt;T&gt;, which is backed by a private field member for the collection (like a List&lt;T&gt;) in your entity that relies on EF for persistence. Previous versions of Entity Framework required collection properties to support ICollection&lt;T&gt;, which meant that any developer using the parent entity class could add or remove items through its property collections. That possibility would be against the recommended patterns in DDD.\n\nYou can use a private collection while exposing a read-only IReadOnlyCollection&lt;T&gt; object, as shown in the following code example:\n\n```\npublic class Order : Entity {\n```\n\n```\n// Using private fields, allowed since EF Core 1.1 private DateTime _orderDate; // Other fields ... private readonly List<OrderItem> _orderItems; public IReadOnlyCollection<OrderItem> OrderItems => _orderItems; protected Order() { } public Order(int buyerId , int paymentMethodId , Address address) { // Initializations ... } public void AddOrderItem(int productId , string productName , decimal unitPrice , decimal discount , string pictureUrl , int units = 1) { // Validation logic... var orderItem = new OrderItem(productId , productName , unitPrice , discount , pictureUrl , units); _orderItems . Add(orderItem); } }\n```\n\nThe OrderItems property can only be accessed as read-only using IReadOnlyCollection&lt;OrderItem&gt;. This type is read-only so it is protected against regular external updates.\n\nEF Core provides a way to map the domain model to the physical database without \"contaminating\" the domain model. It is pure .NET POCO code, because the mapping action is implemented in the persistence layer. In that mapping action, you need to configure the fields-to-database mapping. In the following example of the OnModelCreating method from OrderingContext and the OrderEntityTypeConfiguration class, the call to SetPropertyAccessMode tells EF Core to access the OrderItems property through its field.\n\n```\n// At OrderingContext.cs from eShopOnContainers protected override void OnModelCreating(ModelBuilder modelBuilder) { // ... modelBuilder . ApplyConfiguration(new OrderEntityTypeConfiguration()); // Other entities' configuration ... } // At OrderEntityTypeConfiguration.cs from eShopOnContainers class OrderEntityTypeConfiguration : IEntityTypeConfiguration<Order> { public void Configure(EntityTypeBuilder<Order> orderConfiguration) { orderConfiguration . ToTable(\"orders\" , OrderingContext . DEFAULT_SCHEMA); // Other configuration var navigation = orderConfiguration . Metadata . FindNavigation(nameof(Order . OrderItems)); //EF access the OrderItem collection property through its backing field navigation . SetPropertyAccessMode(PropertyAccessMode . Field);\n```\n\n```\n// Other configuration } }\n```\n\nWhen you use fields instead of properties, the OrderItem entity is persisted as if it had a List&lt;OrderItem&gt; property. However, it exposes a single accessor, the AddOrderItem method, for adding new items to the order. As a result, behavior and data are tied together and will be consistent throughout any application code that uses the domain model.", "Implement custom repositories with Entity Framework Core": "At the implementation level, a repository is simply a class with data persistence code coordinated by a unit of work (DBContext in EF Core) when performing updates, as shown in the following class:\n\n```\n// using directives... namespace Microsoft . eShopOnContainers . Services . Ordering . Infrastructure . Repositories { public class BuyerRepository : IBuyerRepository { private readonly OrderingContext _context; public IUnitOfWork UnitOfWork { get { return _context; } } public BuyerRepository(OrderingContext context) { _context = context ?? throw new ArgumentNullException(nameof(context)); } public Buyer Add(Buyer buyer) { return _context . Buyers . Add(buyer).Entity; } public async Task<Buyer> FindAsync(string buyerIdentityGuid) { var buyer = await _context . Buyers . Include(b => b . Payments) . Where(b => b . FullName == buyerIdentityGuid) . SingleOrDefaultAsync(); return buyer; } } }\n```\n\nThe IBuyerRepository interface comes from the domain model layer as a contract. However, the repository implementation is done at the persistence and infrastructure layer.\n\nThe EF DbContext comes through the constructor through Dependency Injection. It is shared between multiple repositories within the same HTTP request scope, thanks to its default lifetime (ServiceLifetime.Scoped) in the IoC container (which can also be explicitly set with services.AddDbContext&lt;&gt;).", "Methods to implement in a repository (updates or transactions versus queries)": "Within each repository class, you should put the persistence methods that update the state of entities contained by its related aggregate. Remember there is one-to-one relationship between an aggregate and its related repository. Consider that an aggregate root entity object might have embedded child entities within its EF graph. For example, a buyer might have multiple payment methods as related child entities.\n\nSince the approach for the ordering microservice in eShopOnContainers is also based on CQS/CQRS, most of the queries are not implemented in custom repositories. Developers have the freedom to create the queries and joins they need for the presentation layer without the restrictions imposed by aggregates, custom repositories per aggregate, and DDD in general. Most of the custom repositories suggested by this guide have several update or transactional methods but just the query methods needed to get data to be updated. For example, the BuyerRepository repository implements a FindAsync method, because the application needs to know whether a particular buyer exists before creating a new buyer related to the order.\n\nHowever, the real query methods to get data to send to the presentation layer or client apps are implemented, as mentioned, in the CQRS queries based on flexible queries using Dapper.", "Using a custom repository versus using EF DbContext directly": "The Entity Framework DbContext class is based on the Unit of Work and Repository patterns and can be used directly from your code, such as from an ASP.NET Core MVC controller. The Unit of Work and Repository patterns result in the simplest code, as in the CRUD catalog microservice in eShopOnContainers. In cases where you want the simplest code possible, you might want to directly use the DbContext class, as many developers do.\n\nHowever, implementing custom repositories provides several benefits when implementing more complex microservices or applications. The Unit of Work and Repository patterns are intended to encapsulate the infrastructure persistence layer so it is decoupled from the application and domainmodel layers. Implementing these patterns can facilitate the use of mock repositories simulating access to the database.\n\nIn Figure 7-18, you can see the differences between not using repositories (directly using the EF DbContext) versus using repositories, which makes it easier to mock those repositories.\n\nNo Repository\n\nDirect access to database from controller\n\nWeb Server\n\n(Kestrel, IIS, etc.)\n\nController or\n\nApplication Layer\n\nDbContext\n\nEntity Framework\n\nDatabase\n\nAbstraction layer between controller and database context.\n\nWith Repository\n\nUnit tests can mock data to facilitate testing\n\nWeb Server\n\n(Kestrel, IIS, etc.)\n\nWeb Server\n\n(Kestrel, IIS, etc.)\n\nFigure 7 -18. Using custom repositories versus a plain DbContext\n\n<!-- image -->\n\nFigure 7-18 shows that using a custom repository adds an abstraction layer that can be used to ease testing by mocking the repository. There are multiple alternatives when mocking. You could mock just repositories or you could mock a whole unit of work. Usually mocking just the repositories is enough, and the complexity to abstract and mock a whole unit of work is usually not needed.\n\nLater, when we focus on the application layer, you will see how Dependency Injection works in ASP.NET Core and how it is implemented when using repositories.\n\nIn short, custom repositories allow you to test code more easily with unit tests that are not impacted by the data tier state. If you run tests that also access the actual database through the Entity Framework, they are not unit tests but integration tests, which are a lot slower.\n\nIf you were using DbContext directly, you would have to mock it or to run unit tests by using an inmemory SQL Server with predictable data for unit tests. But mocking the DbContext or controlling fake data requires more work than mocking at the repository level. Of course, you could always test the MVC controllers.", "EF DbContext and IUnitOfWork instance lifetime in your IoC container": "The DbContext object (exposed as an IUnitOfWork object) should be shared among multiple repositories within the same HTTP request scope. For example, this is true when the operation being executed must deal with multiple aggregates, or simply because you are using multiple repository instances. It is also important to mention that the IUnitOfWork interface is part of your domain layer, not an EF Core type.\n\nIn order to do that, the instance of the DbContext object has to have its service lifetime set to ServiceLifetime.Scoped. This is the default lifetime when registering a DbContext with builder.Services.AddDbContext in your IoC container from the Program.cs file in your ASP.NET Core Web API project. The following code illustrates this.\n\n```\n// Add framework services. builder . Services . AddMvc(options => { options . Filters . Add(typeof(HttpGlobalExceptionFilter)); }).AddControllersAsServices(); builder . Services . AddEntityFrameworkSqlServer() . AddDbContext<OrderingContext>(options => { options . UseSqlServer(Configuration[\"ConnectionString\"], sqlOptions => sqlOptions . MigrationsAssembly(typeof(Startup).GetTypeInfo(). Assembly . GetName().Name)); }, ServiceLifetime . Scoped // Note that Scoped is the default choice // in AddDbContext. It is shown here only for // pedagogic purposes. );\n```\n\nThe DbContext instantiation mode should not be configured as ServiceLifetime.Transient or ServiceLifetime.Singleton.", "The repository instance lifetime in your IoC container": "In a similar way, repository's lifetime should usually be set as scoped (InstancePerLifetimeScope in Autofac). It could also be transient (InstancePerDependency in Autofac), but your service will be more efficient in regards to memory when using the scoped lifetime.\n\n```\n// Registering a Repository in Autofac IoC container builder . RegisterType<OrderRepository>() . As<IOrderRepository>() . InstancePerLifetimeScope();\n```\n\nUsing the singleton lifetime for the repository could cause you serious concurrency problems when your DbContext is set to scoped (InstancePerLifetimeScope) lifetime (the default lifetimes for a DBContext). As long as your service lifetimes for your repositories and your DbContext are both Scoped, you'll avoid these issues.", "Table mapping": "Table mapping identifies the table data to be queried from and saved to the database. Previously you saw how domain entities (for example, a product or order domain) can be used to generate a related database schema. EF is strongly designed around the concept of conventions. Conventions address questions like \"What will the name of a table be?\" or \"What property is the primary key?\" Conventions are typically based on conventional names. For example, it is typical for the primary key to be a property that ends with Id.\n\nBy convention, each entity will be set up to map to a table with the same name as the DbSet&lt;TEntity&gt; property that exposes the entity on the derived context. If no DbSet&lt;TEntity&gt; value is provided for the given entity, the class name is used.", "Data Annotations versus Fluent API": "There are many additional EF Core conventions, and most of them can be changed by using either data annotations or Fluent API, implemented within the OnModelCreating method.\n\nData annotations must be used on the entity model classes themselves, which is a more intrusive way from a DDD point of view. This is because you are contaminating your model with data annotations related to the infrastructure database. On the other hand, Fluent API is a convenient way to change most conventions and mappings within your data persistence infrastructure layer, so the entity model will be clean and decoupled from the persistence infrastructure.", "Fluent API and the OnModelCreating method": "As mentioned, in order to change conventions and mappings, you can use the OnModelCreating method in the DbContext class.\n\nThe ordering microservice in eShopOnContainers implements explicit mapping and configuration, when needed, as shown in the following code.\n\n```\n// At OrderingContext.cs from eShopOnContainers protected override void OnModelCreating(ModelBuilder modelBuilder) { // ... modelBuilder . ApplyConfiguration(new OrderEntityTypeConfiguration()); // Other entities' configuration ... } // At OrderEntityTypeConfiguration.cs from eShopOnContainers class OrderEntityTypeConfiguration : IEntityTypeConfiguration<Order> { public void Configure(EntityTypeBuilder<Order> orderConfiguration) { orderConfiguration . ToTable(\"orders\" , OrderingContext . DEFAULT_SCHEMA); orderConfiguration . HasKey(o => o . Id); orderConfiguration . Ignore(b => b . DomainEvents); orderConfiguration . Property(o => o . Id) . UseHiLo(\"orderseq\" , OrderingContext . DEFAULT_SCHEMA);\n```\n\n```\n//Address value object persisted as owned entity type supported since EF Core 2.0 orderConfiguration . OwnsOne(o => o . Address , a => { a . WithOwner(); }); orderConfiguration . Property<int?>(\"_buyerId\") . UsePropertyAccessMode(PropertyAccessMode . Field) . HasColumnName(\"BuyerId\") . IsRequired(false); orderConfiguration . Property<DateTime>(\"_orderDate\") . UsePropertyAccessMode(PropertyAccessMode . Field) . HasColumnName(\"OrderDate\") . IsRequired(); orderConfiguration . Property<int>(\"_orderStatusId\") . UsePropertyAccessMode(PropertyAccessMode . Field) . HasColumnName(\"OrderStatusId\") . IsRequired(); orderConfiguration . Property<int?>(\"_paymentMethodId\") . UsePropertyAccessMode(PropertyAccessMode . Field) . HasColumnName(\"PaymentMethodId\") . IsRequired(false); orderConfiguration . Property<string>(\"Description\").IsRequired(false); var navigation = orderConfiguration . Metadata . FindNavigation(nameof(Order . OrderItems)); // DDD Patterns comment: //Set as field (New since EF 1.1) to access the OrderItem collection property through its field navigation . SetPropertyAccessMode(PropertyAccessMode . Field); orderConfiguration . HasOne<PaymentMethod>() . WithMany() . HasForeignKey(\"_paymentMethodId\") . IsRequired(false) . OnDelete(DeleteBehavior . Restrict); orderConfiguration . HasOne<Buyer>() . WithMany() . IsRequired(false) . HasForeignKey(\"_buyerId\"); orderConfiguration . HasOne(o => o . OrderStatus) . WithMany() . HasForeignKey(\"_orderStatusId\"); } }\n```\n\nYou could set all the Fluent API mappings within the same OnModelCreating method, but it's advisable to partition that code and have multiple configuration classes, one per entity, as shown in\n\nthe example. Especially for large models, it is advisable to have separate configuration classes for configuring different entity types.\n\nThe code in the example shows a few explicit declarations and mapping. However, EF Core conventions do many of those mappings automatically, so the actual code you would need in your case might be smaller.", "The Hi/Lo algorithm in EF Core": "An interesting aspect of code in the preceding example is that it uses the Hi/Lo algorithm as the key generation strategy.\n\nThe Hi/Lo algorithm is useful when you need unique keys before committing changes. As a summary, the Hi -Lo algorithm assigns unique identifiers to table rows while not depending on storing the row in the database immediately. This lets you start using the identifiers right away, as happens with regular sequential database IDs.\n\nThe Hi/Lo algorithm describes a mechanism for getting a batch of unique IDs from a related database sequence. These IDs are safe to use because the database guarantees the uniqueness, so there will be no collisions between users. This algorithm is interesting for these reasons:\n\n['It does not break the Unit of Work pattern.', 'It gets sequence IDs in batches, to minimize round trips to the database.', 'It generates a human readable identifier, unlike techniques that use GUIDs.']\n\nEF Core supports HiLo with the UseHiLo method, as shown in the preceding example.", "Map fields instead of properties": "With this feature, available since EF Core 1.1, you can directly map columns to fields. It is possible to not use properties in the entity class, and just to map columns from a table to fields. A common use for that would be private fields for any internal state that do not need to be accessed from outside the entity.\n\nYou can do this with single fields or also with collections, like a List&lt;&gt; field. This point was mentioned earlier when we discussed modeling the domain model classes, but here you can see how that mapping is performed with the PropertyAccessMode.Field configuration highlighted in the previous code.", "Use shadow properties in EF Core, hidden at the infrastructure level": "Shadow properties in EF Core are properties that do not exist in your entity class model. The values and states of these properties are maintained purely in the ChangeTracker class at the infrastructure level.", "Implement the Query Specification pattern": "As introduced earlier in the design section, the Query Specification pattern is a Domain-Driven Design pattern designed as the place where you can put the definition of a query with optional sorting and paging logic.\n\nThe Query Specification pattern defines a query in an object. For example, in order to encapsulate a paged query that searches for some products you can create a PagedProduct specification that takes the necessary input parameters (pageNumber, pageSize, filter, etc.). Then, within any Repository method (usually a List() overload) it would accept an IQuerySpecification and run the expected query based on that specification.\n\nAn example of a generic Specification interface is the following code, which is similar to code used in the eShopOnWeb reference application.\n\n```\n// GENERIC SPECIFICATION INTERFACE // https://github.com/dotnet-architecture/eShopOnWeb public interface ISpecification<T> { Expression<Func<T , bool>> Criteria { get; } List<Expression<Func<T , object>>> Includes { get; } List<string> IncludeStrings { get; } }\n```\n\nThen, the implementation of a generic specification base class is the following.\n\n```\n// GENERIC SPECIFICATION IMPLEMENTATION (BASE CLASS) // https://github.com/dotnet-architecture/eShopOnWeb public abstract class BaseSpecification<T> : ISpecification<T> { public BaseSpecification(Expression<Func<T , bool>> criteria) { Criteria = criteria; } public Expression<Func<T , bool>> Criteria { get; } public List<Expression<Func<T , object>>> Includes { get; } = new List<Expression<Func<T , object>>>(); public List<string> IncludeStrings { get; } = new List<string>(); protected virtual void AddInclude(Expression<Func<T , object>> includeExpression) { Includes . Add(includeExpression); } // string-based includes allow for including children of children // e.g. Basket.Items.Product protected virtual void AddInclude(string includeString) { IncludeStrings . Add(includeString); } }\n```\n\nThe following specification loads a single basket entity given either the basket's ID or the ID of the buyer to whom the basket belongs. It will eagerly load the basket's Items collection.\n\n```\n// SAMPLE QUERY SPECIFICATION IMPLEMENTATION public class BasketWithItemsSpecification : BaseSpecification<Basket> { public BasketWithItemsSpecification(int basketId) : base(b => b . Id == basketId) { AddInclude(b => b . Items); } public BasketWithItemsSpecification(string buyerId) : base(b => b . BuyerId == buyerId) { AddInclude(b => b . Items); } }\n```\n\nAnd finally, you can see below how a generic EF Repository can use such a specification to filter and eager -load data related to a given entity type T.\n\n```\n// GENERIC EF REPOSITORY WITH SPECIFICATION // https://github.com/dotnet-architecture/eShopOnWeb public IEnumerable<T> List(ISpecification<T> spec) { // fetch a Queryable that includes all expression-based includes var queryableResultWithIncludes = spec . Includes . Aggregate(_dbContext . Set<T>().AsQueryable(), (current , include) => current . Include(include)); // modify the IQueryable to include any string-based include statements var secondaryResult = spec . IncludeStrings . Aggregate(queryableResultWithIncludes , (current , include) => current . Include(include)); // return the result of the query using the specification's criteria expression return secondaryResult . Where(spec . Criteria) . AsEnumerable(); }\n```\n\nIn addition to encapsulating filtering logic, the specification can specify the shape of the data to be returned, including which properties to populate.\n\nAlthough we don't recommend returning IQueryable from a repository, it's perfectly fine to use them within the repository to build up a set of results. You can see this approach used in the List method above, which uses intermediate IQueryable expressions to build up the query's list of includes before executing the query with the specification's criteria on the last line.\n\nLearn how the specification pattern is applied in the eShopOnWeb sample .", "Use NoSQL databases as a persistence infrastructure": "When you use NoSQL databases for your infrastructure data tier, you typically do not use an ORM like Entity Framework Core. Instead you use the API provided by the NoSQL engine, such as Azure Cosmos DB, MongoDB, Cassandra, RavenDB, CouchDB, or Azure Storage Tables.\n\nHowever, when you use a NoSQL database, especially a document-oriented database like Azure Cosmos DB, CouchDB, or RavenDB, the way you design your model with DDD aggregates is partially similar to how you can do it in EF Core, in regards to the identification of aggregate roots, child entity classes, and value object classes. But, ultimately, the database selection will impact in your design.\n\nWhen you use a document-oriented database, you implement an aggregate as a single document, serialized in JSON or another format. However, the use of the database is transparent from a domain model code point of view. When using a NoSQL database, you still are using entity classes and aggregate root classes, but with more flexibility than when using EF Core because the persistence is not relational.\n\nThe difference is in how you persist that model. If you implemented your domain model based on POCO entity classes, agnostic to the infrastructure persistence, it might look like you could move to a different persistence infrastructure, even from relational to NoSQL. However, that should not be your goal. There are always constraints and trade-offs in the different database technologies, so you will not be able to have the same model for relational or NoSQL databases. Changing persistence models is not a trivial task, because transactions and persistence operations will be very different.\n\nFor example, in a document-oriented database, it is okay for an aggregate root to have multiple child collection properties. In a relational database, querying multiple child collection properties is not\n\neasily optimized, because you get a UNION ALL SQL statement back from EF. Having the same domain model for relational databases or NoSQL databases is not simple, and you should not try to do it. You really have to design your model with an understanding of how the data is going to be used in each particular database.\n\nA benefit when using NoSQL databases is that the entities are more denormalized, so you do not set a table mapping. Your domain model can be more flexible than when using a relational database.\n\nWhen you design your domain model based on aggregates, moving to NoSQL and documentoriented databases might be even easier than using a relational database, because the aggregates you design are similar to serialized documents in a document-oriented database. Then you can include in those \"bags\" all the information you might need for that aggregate.\n\nFor instance, the following JSON code is a sample implementation of an order aggregate when using a document -oriented database. It is similar to the order aggregate we implemented in the eShopOnContainers sample, but without using EF Core underneath.\n\n```\n{ \"id\": \"2024001\" , \"orderDate\": \"2/25/2024\" , \"buyerId\": \"1234567\" , \"address\": [ { \"street\": \"100 One Microsoft Way\" , \"city\": \"Redmond\" , \"state\": \"WA\" , \"zip\": \"98052\" , \"country\": \"U.S.\" } ] , \"orderItems\": [ {\"id\": 20240011 , \"productId\": \"123456\" , \"productName\": \".NET T-Shirt\" , \"unitPrice\": 25 , \"units\": 2 , \"discount\": 0} , {\"id\": 20240012 , \"productId\": \"123457\" , \"productName\": \".NET Mug\" , \"unitPrice\": 15 , \"units\": 1 , \"discount\": 0} ] }\n```", "Introduction to Azure Cosmos DB and the native Cosmos DB API": "Azure Cosmos DB is Microsoft's globally distributed database service for mission-critical applications. Azure Cosmos DB provides turn-key global distribution , elastic scaling of throughput and storage worldwide, single-digit millisecond latencies at the 99th percentile, five well-defined consistency levels, and guaranteed high availability, all backed by industry-leading SLAs. Azure Cosmos DB automatically indexes data without requiring you to deal with schema and index management. It is multi -model and supports document, key-value, graph, and columnar data models.\n\nGlobal distribution\n\nAzure Cosmos DB\n\nKey-Value\n\n:\n\nElastic scale out\n\n<!-- image -->\n\nComprehensive SLAs\n\nFigure 7 -19. Azure Cosmos DB global distribution\n\n<!-- image -->\n\nWhen you use a C# model to implement the aggregate to be used by the Azure Cosmos DB API, the aggregate can be similar to the C# POCO classes used with EF Core. The difference is in the way to use them from the application and infrastructure layers, as in the following code:\n\n```\n// C# EXAMPLE OF AN ORDER AGGREGATE BEING PERSISTED WITH AZURE COSMOS DB API // *** Domain Model Code *** // Aggregate: Create an Order object with its child entities and/or value objects. // Then, use AggregateRoot's methods to add the nested objects so invariants and // logic is consistent across the nested properties (value objects and entities). Order orderAggregate = new Order { Id = \"2024001\" , OrderDate = new DateTime(2005 , 7 , 1), BuyerId = \"1234567\" , PurchaseOrderNumber = \"PO18009186470\" } Address address = new Address { Street = \"100 One Microsoft Way\" , City = \"Redmond\" , State = \"WA\" , Zip = \"98052\" , Country = \"U.S.\" } orderAggregate . UpdateAddress(address); OrderItem orderItem1 = new OrderItem {\n```\n\n```\nId = 20240011 , ProductId = \"123456\" , ProductName = \".NET T -Shirt\" , UnitPrice = 25 , Units = 2 , Discount = 0; }; //Using methods with domain logic within the entity. No anemic-domain model orderAggregate . AddOrderItem(orderItem1); // *** End of Domain Model Code *** // *** Infrastructure Code using Cosmos DB Client API *** Uri collectionUri = UriFactory . CreateDocumentCollectionUri(databaseName , collectionName); await client . CreateDocumentAsync(collectionUri , orderAggregate); // As your app evolves, let's say your object has a new schema. You can insert // OrderV2 objects without any changes to the database tier. Order2 newOrder = GetOrderV2Sample(\"IdForSalesOrder2\"); await client . CreateDocumentAsync(collectionUri , newOrder);\n```\n\nYou can see that the way you work with your domain model can be similar to the way you use it in your domain model layer when the infrastructure is EF. You still use the same aggregate root methods to ensure consistency, invariants, and validations within the aggregate.\n\nHowever, when you persist your model into the NoSQL database, the code and API change dramatically compared to EF Core code or any other code related to relational databases.", "Implement .NET code targeting MongoDB and Azure Cosmos DB": "", "Use Azure Cosmos DB from .NET containers": "You can access Azure Cosmos DB databases from .NET code running in containers, like from any other .NET application. For instance, the Locations.API and Marketing.API microservices in eShopOnContainers are implemented so they can consume Azure Cosmos DB databases.\n\nHowever, there's a limitation in Azure Cosmos DB from a Docker development environment point of view. Even though there's an on-premises Azure Cosmos DB Emulator that can run in a local development machine, it only supports Windows. Linux and macOS aren't supported.\n\nThere's also the possibility to run this emulator on Docker, but just on Windows Containers, not with Linux Containers. That's an initial handicap for the development environment if your application is deployed as Linux containers, since, currently, you can't deploy Linux and Windows Containers on Docker for Windows at the same time. Either all containers being deployed have to be for Linux or for Windows.\n\nThe ideal and more straightforward deployment for a dev/test solution is to be able to deploy your database systems as containers along with your custom containers so your dev/test environments are always consistent.\n\nnodeo\n\nMicrosoft\n\n~ python\"", "Use MongoDB API for local dev/test Linux/Windows containers plus Azure Cosmos DB Azure Cosmos DB:": "NET\n\nRuby\n\nCosmos DB databases support MongoDB API for .NET as well as the native MongoDB wire protocol. This means that by using existing drivers, your application written for MongoDB can now communicate with Cosmos DB and use Cosmos DB databases instead of MongoDB databases, as shown in Figure 7-20.\n\nFigure 7 -20. Using MongoDB API and protocol to access Azure Cosmos DB\n\n<!-- image -->\n\nThis is a very convenient approach for proof of concepts in Docker environments with Linux containers because the MongoDB Docker image is a multi-arch image that supports Docker Linux containers and Docker Windows containers.\n\nAs shown in the following image, by using the MongoDB API, eShopOnContainers supports MongoDB Linux and Windows containers for the local development environment but then, you can move to a scalable, PaaS cloud solution as Azure Cosmos DB by simply changing the MongoDB connection string to point to Azure Cosmos DB .\n\n| Docker Host\n\nMicrosott\n\nAzure\n\nFigure 7 -21. eShopOnContainers using MongoDB containers for dev -env or Azure Cosmos DB for production\n\n<!-- image -->\n\nThe production Azure Cosmos DB would be running in Azure\u2019s cloud as a PaaS and scalable service.\n\nYour custom .NET containers can run on a local development Docker host (that is using Docker for Windows in a Windows 10 machine) or be deployed into a production environment, like Kubernetes in Azure AKS or Azure Service Fabric. In this second environment, you would deploy only the .NET custom containers but not the MongoDB container since you'd be using Azure Cosmos DB in the cloud for handling the data in production.\n\nA clear benefit of using the MongoDB API is that your solution could run in both database engines, MongoDB or Azure Cosmos DB, so migrations to different environments should be easy. However, sometimes it is worthwhile to use a native API (that is the native Cosmos DB API) in order to take full advantage of the capabilities of a specific database engine.\n\nFor further comparison between simply using MongoDB versus Cosmos DB in the cloud, see the Benefits of using Azure Cosmos DB in this page .", "Analyze your approach for production applications: MongoDB API vs. Cosmos DB API": "In eShopOnContainers, we're using MongoDB API because our priority was fundamentally to have a consistent dev/test environment using a NoSQL database that could also work with Azure Cosmos DB.\n\nHowever, if you are planning to use MongoDB API to access Azure Cosmos DB in Azure for production applications, you should analyze the differences in capabilities and performance when using MongoDB API to access Azure Cosmos DB databases compared to using the native Azure Cosmos DB API. If it is similar you can use MongoDB API and you get the benefit of supporting two NoSQL database engines at the same time.\n\nYou could also use MongoDB clusters as the production database in Azure's cloud, too, with MongoDB Azure Service. But that is not a PaaS service provided by Microsoft. In this case, Azure is just hosting that solution coming from MongoDB.\n\nBasically, this is just a disclaimer stating that you shouldn't always use MongoDB API against Azure Cosmos DB, as we did in eShopOnContainers because it was a convenient choice for Linux containers. The decision should be based on the specific needs and tests you need to do for your production application.", "The code: Use MongoDB API in .NET applications": "MongoDB API for .NET is based on NuGet packages that you need to add to your projects, like in the Locations.API project shown in the following figure.\n\nLocations.API\n\nConnected Services\n\n#\u2022 Dependencies\n\nF Analyzers\n\nFrameworks\n\n*.\n\n*.\n\n<!-- image -->\n\nP\n\nP\n\nP\n\nP\n\nP\n\nP\n\nP\n\nP\n\n\u2022.\n\n\u2022 Projects\n\nProperties\n\nFigure 7 -22. MongoDB API NuGet packages references in a .NET project\n\nLet\u2019s investigate the code in the following sections.", "A Model used by MongoDB API": "First, you need to define a model that will hold the data coming from the database in your application's memory space. Here's an example of the model used for Locations at eShopOnContainers.\n\n```\nusing MongoDB . Bson; using MongoDB . Bson . Serialization . Attributes; using MongoDB . Driver . GeoJsonObjectModel; using System . Collections . Generic; public class Locations {\n```\n\n```\n[BsonId] [BsonRepresentation(BsonType . ObjectId)] public string Id { get; set; } public int LocationId { get; set; } public string Code { get; set; } [BsonRepresentation(BsonType . ObjectId)] public string Parent_Id { get; set; } public string Description { get; set; } public double Latitude { get; set; } public double Longitude { get; set; } public GeoJsonPoint<GeoJson2DGeographicCoordinates> Location { get; private set; } public GeoJsonPolygon<GeoJson2DGeographicCoordinates> Polygon { get; private set; } public void SetLocation(double lon , double lat) => SetPosition(lon , lat); public void SetArea(List<GeoJson2DGeographicCoordinates> coordinatesList) => SetPolygon(coordinatesList); private void SetPosition(double lon , double lat) { Latitude = lat; Longitude = lon; Location = new GeoJsonPoint<GeoJson2DGeographicCoordinates>( new GeoJson2DGeographicCoordinates(lon , lat)); } private void SetPolygon(List<GeoJson2DGeographicCoordinates> coordinatesList) { Polygon = new GeoJsonPolygon<GeoJson2DGeographicCoordinates>( new GeoJsonPolygonCoordinates<GeoJson2DGeographicCoordinates>( new GeoJsonLinearRingCoordinates<GeoJson2DGeographicCoordinates>( coordinatesList))); } }\n```\n\nYou can see there are a few attributes and types coming from the MongoDB NuGet packages.\n\nNoSQL databases are usually very well suited for working with non-relational hierarchical data. In this example, we are using MongoDB types especially made for geo-locations, like GeoJson2DGeographicCoordinates.", "Retrieve the database and the collection": "In eShopOnContainers, we have created a custom database context where we implement the code to retrieve the database and the MongoCollections, as in the following code.\n\n```\npublic class LocationsContext { private readonly IMongoDatabase _database = null; public LocationsContext(IOptions<LocationSettings> settings) { var client = new MongoClient(settings . Value . ConnectionString); if (client != null) _database = client . GetDatabase(settings . Value . Database); } public IMongoCollection<Locations> Locations {\n```\n\n```\nget { return _database . GetCollection<Locations>(\"Locations\"); } } }\n```", "Retrieve the data": "In C# code, like Web API controllers or custom Repositories implementation, you can write similar code to the following when querying through the MongoDB API. Note that the \\_context object is an instance of the previous LocationsContext class.\n\n```\npublic async Task<Locations> GetAsync(int locationId) { var filter = Builders<Locations>.Filter . Eq(\"LocationId\" , locationId); return await _context . Locations . Find(filter) . FirstOrDefaultAsync(); }\n```", "Use an env -var in the docker -compose.override.yml file for the MongoDB connection string": "When creating a MongoClient object, it needs a fundamental parameter which is precisely the ConnectionString parameter pointing to the right database. In the case of eShopOnContainers, the connection string can point to a local MongoDB Docker container or to a \"production\" Azure Cosmos DB database. That connection string comes from the environment variables defined in the dockercompose.override.yml files used when deploying with docker-compose or Visual Studio, as in the following yml code.\n\n```\n# docker -compose.override.yml version: '3.4' services: # Other services locations -api: environment: # Other settings -ConnectionString=${ESHOP_AZURE_COSMOSDB:-mongodb://nosqldata}\n```\n\nThe ConnectionString environment variable is resolved this way: If the ESHOP\\_AZURE\\_COSMOSDB global variable is defined in the .env file with the Azure Cosmos DB connection string, it will use it to access the Azure Cosmos DB database in the cloud. If it's not defined, it will take the mongodb://nosqldata value and use the development MongoDB container.\n\nThe following code shows the .env file with the Azure Cosmos DB connection string global environment variable, as implemented in eShopOnContainers:\n\n```\n# .env file, in eShopOnContainers root folder # Other Docker environment variables ESHOP_EXTERNAL_DNS_NAME_OR_IP=host.docker.internal ESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP=<YourDockerHostIP> #ESHOP_AZURE_COSMOSDB=<YourAzureCosmosDBConnData>\n```\n\n```\n#Other environment variables for additional Azure infrastructure assets #ESHOP_AZURE_REDIS_BASKET_DB=<YourAzureRedisBasketInfo> #ESHOP_AZURE_STORAGE_CATALOG_URL=<YourAzureStorage_Catalog_BLOB_URL> #ESHOP_AZURE_SERVICE_BUS=<YourAzureServiceBusInfo>\n```\n\nUncomment the ESHOP\\_AZURE\\_COSMOSDB line and update it with your Azure Cosmos DB connection string obtained from the Azure portal as explained in Connect a MongoDB application to Azure Cosmos DB .\n\nIf the ESHOP\\_AZURE\\_COSMOSDB global variable is empty, meaning it's commented out in the .env file, then the container uses a default MongoDB connection string. This connection string points to the local MongoDB container deployed in eShopOnContainers that is named nosqldata and was defined at the docker -compose file, as shown in the following .yml code:\n\n```\n# docker -compose.yml version: '3.4' services: # ...Other services... nosqldata: image: mongo\n```", "Design the microservice application layer and Web API": "", "Use SOLID principles and Dependency Injection": "SOLID principles are critical techniques to be used in any modern and mission-critical application, such as developing a microservice with DDD patterns. SOLID is an acronym that groups five fundamental principles:\n\n['Single Responsibility principle', 'Open/closed principle', 'Liskov substitution principle', 'Interface Segregation principle', 'Dependency Inversion principle']\n\nSOLID is more about how you design your application or microservice internal layers and about decoupling dependencies between them. It is not related to the domain, but to the application's technical design. The final principle, the Dependency Inversion principle, allows you to decouple the infrastructure layer from the rest of the layers, which allows a better decoupled implementation of the DDD layers.\n\nDependency Injection (DI) is one way to implement the Dependency Inversion principle. It is a technique for achieving loose coupling between objects and their dependencies. Rather than directly instantiating collaborators, or using static references (that is, using new\u2026), the objects that a class needs in order to perform its actions are provided to (or \"injected into\") the class. Most often, classes will declare their dependencies via their constructor, allowing them to follow the Explicit Dependencies principle. Dependency Injection is usually based on specific Inversion of Control (IoC) containers. ASP.NET Core provides a simple built-in IoC container, but you can also use your favorite IoC container, like Autofac or Ninject.\n\nBy following the SOLID principles, your classes will tend naturally to be small, well-factored, and easily tested. But how can you know if too many dependencies are being injected into your classes? If you use DI through the constructor, it will be easy to detect that by just looking at the number of parameters for your constructor. If there are too many dependencies, this is generally a sign (a code smell) that your class is trying to do too much, and is probably violating the Single Responsibility principle.\n\nIt would take another guide to cover SOLID in detail. Therefore, this guide requires you to have only a minimum knowledge of these topics.", "Implement the microservice application layer using the Web API Infrastructure": "\u2022 a", "Use Dependency Injection to inject infrastructure objects into your application layer": "As mentioned previously, the application layer can be implemented as part of the artifact (assembly) you are building, such as within a Web API project or an MVC web app project. In the case of a microservice built with ASP.NET Core, the application layer will usually be your Web API library. If you want to separate what is coming from ASP.NET Core (its infrastructure plus your controllers) from your custom application layer code, you could also place your application layer in a separate class library, but that is optional.\n\nFor instance, the application layer code of the ordering microservice is directly implemented as part of the Ordering.API project (an ASP.NET Core Web API project), as shown in Figure 7-23.\n\nFigure 7 -23. The application layer in the Ordering.API ASP.NET Core Web API project\n\n<!-- image -->\n\nASP.NET Core includes a simple built-in IoC container (represented by the IServiceProvider interface) that supports constructor injection by default, and ASP.NET makes certain services available through DI. ASP.NET Core uses the term service for any of the types you register that will be injected through DI. You configure the built-in container's services in your application's Program.cs file. Your\n\ndependencies are implemented in the services that a type needs and that you register in the IoC container.\n\nTypically, you want to inject dependencies that implement infrastructure objects. A typical dependency to inject is a repository. But you could inject any other infrastructure dependency that you may have. For simpler implementations, you could directly inject your Unit of Work pattern object (the EF DbContext object), because the DBContext is also the implementation of your infrastructure persistence objects.\n\nIn the following example, you can see how .NET is injecting the required repository objects through the constructor. The class is a command handler, which will get covered in the next section.\n\n```\npublic class CreateOrderCommandHandler : IRequestHandler<CreateOrderCommand , bool> { private readonly IOrderRepository _orderRepository; private readonly IIdentityService _identityService; private readonly IMediator _mediator; private readonly IOrderingIntegrationEventService _orderingIntegrationEventService; private readonly ILogger<CreateOrderCommandHandler> _logger; // Using DI to inject infrastructure persistence Repositories public CreateOrderCommandHandler(IMediator mediator , IOrderingIntegrationEventService orderingIntegrationEventService , IOrderRepository orderRepository , IIdentityService identityService , ILogger<CreateOrderCommandHandler> logger) { _orderRepository = orderRepository ?? throw new ArgumentNullException(nameof(orderRepository)); _identityService = identityService ?? throw new ArgumentNullException(nameof(identityService)); _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator)); _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new ArgumentNullException(nameof(orderingIntegrationEventService)); _logger = logger ?? throw new ArgumentNullException(nameof(logger)); } public async Task<bool> Handle(CreateOrderCommand message , CancellationToken cancellationToken) { // Add Integration event to clean the basket var orderStartedIntegrationEvent = new OrderStartedIntegrationEvent(message . UserId); await _orderingIntegrationEventService . AddAndSaveEventAsync(orderStartedIntegrationEvent); // Add/Update the Buyer AggregateRoot // DDD patterns comment: Add child entities and value-objects through the Order Aggregate -Root // methods and constructor so validations, invariants and business logic // make sure that consistency is preserved across the whole aggregate var address = new Address(message . Street , message.City , message.State , message.Country , message.ZipCode); var order = new Order(message . UserId , message.UserName , address , message.CardTypeId , message.CardNumber , message.CardSecurityNumber , message.CardHolderName , message.CardExpiration); foreach (var item in message . OrderItems)\n```\n\n```\n{ order . AddOrderItem(item . ProductId , item . ProductName , item . UnitPrice , item . Discount , item . PictureUrl , item . Units); } _logger . LogInformation( \" -----Creating Order -Order: {@Order}\" , order); _orderRepository . Add(order); return await _orderRepository . UnitOfWork . SaveEntitiesAsync(cancellationToken); } }\n```\n\nThe class uses the injected repositories to execute the transaction and persist the state changes. It does not matter whether that class is a command handler, an ASP.NET Core Web API controller method, or a DDD Application Service. It is ultimately a simple class that uses repositories, domain entities, and other application coordination in a fashion similar to a command handler. Dependency Injection works the same way for all the mentioned classes, as in the example using DI based on the constructor.", "Register the dependency implementation types and interfaces or abstractions": "Before you use the objects injected through constructors, you need to know where to register the interfaces and classes that produce the objects injected into your application classes through DI. (Like DI based on the constructor, as shown previously.)", "Use the built -in IoC container provided by ASP.NET Core": "When you use the built-in IoC container provided by ASP.NET Core, you register the types you want to inject in the Program.cs file, as in the following code:\n\n```\n// Register out-of-the-box framework services. builder . Services . AddDbContext<CatalogContext>(c => c . UseSqlServer(Configuration[\"ConnectionString\"]), ServiceLifetime . Scoped); builder . Services . AddMvc(); // Register custom application dependencies. builder . Services . AddScoped<IMyCustomRepository , MyCustomSQLRepository>();\n```\n\nThe most common pattern when registering types in an IoC container is to register a pair of types\u2014an interface and its related implementation class. Then when you request an object from the IoC container through any constructor, you request an object of a certain type of interface. For instance, in the previous example, the last line states that when any of your constructors have a dependency on IMyCustomRepository (interface or abstraction), the IoC container will inject an instance of the MyCustomSQLServerRepository implementation class.", "Use the Scrutor library for automatic types registration": "When using DI in .NET, you might want to be able to scan an assembly and automatically register its types by convention. This feature is not currently available in ASP.NET Core. However, you can use the\n\nScrutor library for that. This approach is convenient when you have dozens of types that need to be registered in your IoC container.", "Use Autofac as an IoC container": "You can also use additional IoC containers and plug them into the ASP.NET Core pipeline, as in the ordering microservice in eShopOnContainers, which uses Autofac. When using Autofac you typically register the types via modules, which allow you to split the registration types between multiple files depending on where your types are, just as you could have the application types distributed across multiple class libraries.\n\nFor example, the following is the Autofac application module for the Ordering.API Web API project with the types you will want to inject.\n\n```\npublic class ApplicationModule : Autofac . Module { public string QueriesConnectionString { get; } public ApplicationModule(string qconstr) { QueriesConnectionString = qconstr; } protected override void Load(ContainerBuilder builder) { builder . Register(c => new OrderQueries(QueriesConnectionString)) . As<IOrderQueries>() . InstancePerLifetimeScope(); builder . RegisterType<BuyerRepository>() . As<IBuyerRepository>() . InstancePerLifetimeScope(); builder . RegisterType<OrderRepository>() . As<IOrderRepository>() . InstancePerLifetimeScope(); builder . RegisterType<RequestManager>() . As<IRequestManager>() . InstancePerLifetimeScope(); } }\n```\n\nAutofac also has a feature to scan assemblies and register types by name conventions .\n\nThe registration process and concepts are very similar to the way you can register types with the builtin ASP.NET Core IoC container, but the syntax when using Autofac is a bit different.\n\nIn the example code, the abstraction IOrderRepository is registered along with the implementation class OrderRepository. This means that whenever a constructor is declaring a dependency through the\n\nIOrderRepository abstraction or interface, the IoC container will inject an instance of the OrderRepository class.\n\nThe instance scope type determines how an instance is shared between requests for the same service or dependency. When a request is made for a dependency, the IoC container can return the following:\n\n['A single instance per lifetime scope (referred to in the ASP.NET Core IoC container as scoped).', 'A new instance per dependency (referred to in the ASP.NET Core IoC container as transient).', 'A single instance shared across all objects using the IoC container (referred to in the ASP.NET Core IoC container as singleton).']", "Implement the Command and Command Handler patterns": "In the DI -through-constructor example shown in the previous section, the IoC container was injecting repositories through a constructor in a class. But exactly where were they injected? In a simple Web API (for example, the catalog microservice in eShopOnContainers), you inject them at the MVC controllers' level, in a controller constructor, as part of the request pipeline of ASP.NET Core. However, in the initial code of this section (the CreateOrderCommandHandler class from the Ordering.API service in eShopOnContainers), the injection of dependencies is done through the constructor of a particular command handler. Let us explain what a command handler is and why you would want to use it.\n\nThe Command pattern is intrinsically related to the CQRS pattern that was introduced earlier in this guide. CQRS has two sides. The first area is queries, using simplified queries with the Dapper micro ORM, which was explained previously. The second area is commands, which are the starting point for transactions, and the input channel from outside the service.\n\nAs shown in Figure 7-24, the pattern is based on accepting commands from the client-side, processing them based on the domain model rules, and finally persisting the states with transactions.\n\nUl app\n\nCommand\n\nHigh level \"Writes-side\" in CQRS\n\nDomain", "Command Model": "API\n\ninterface\n\nCommand\n\nHandler\n\nFigure 7 -24. High -level view of the commands or \"transactional side\" in a CQRS pattern\n\n<!-- image -->\n\nFigure 7-24 shows that the UI app sends a command through the API that gets to a CommandHandler, that depends on the Domain model and the Infrastructure, to update the database.", "The command class": "A command is a request for the system to perform an action that changes the state of the system. Commands are imperative, and should be processed just once.\n\nSince commands are imperatives, they are typically named with a verb in the imperative mood (for example, \"create\" or \"update\"), and they might include the aggregate type, such as CreateOrderCommand. Unlike an event, a command is not a fact from the past; it is only a request, and thus may be refused.\n\nCommands can originate from the UI as a result of a user initiating a request, or from a process manager when the process manager is directing an aggregate to perform an action.\n\nAn important characteristic of a command is that it should be processed just once by a single receiver. This is because a command is a single action or transaction you want to perform in the application. For example, the same order creation command should not be processed more than once. This is an important difference between commands and events. Events may be processed multiple times, because many systems or microservices might be interested in the event.\n\nIn addition, it is important that a command be processed only once in case the command is not idempotent. A command is idempotent if it can be executed multiple times without changing the result, either because of the nature of the command, or because of the way the system handles the command.\n\nIt is a good practice to make your commands and updates idempotent when it makes sense under your domain's business rules and invariants. For instance, to use the same example, if for any reason (retry logic, hacking, etc.) the same CreateOrder command reaches your system multiple times, you should be able to identify it and ensure that you do not create multiple orders. To do so, you need to\n\nattach some kind of identity in the operations and identify whether the command or update was already processed.\n\nYou send a command to a single receiver; you do not publish a command. Publishing is for events that state a fact \u2014 that something has happened and might be interesting for event receivers. In the case of events, the publisher has no concerns about which receivers get the event or what they do it. But domain or integration events are a different story already introduced in previous sections.\n\nA command is implemented with a class that contains data fields or collections with all the information that is needed in order to execute that command. A command is a special kind of Data Transfer Object (DTO), one that is specifically used to request changes or transactions. The command itself is based on exactly the information that is needed for processing the command, and nothing more.\n\nThe following example shows the simplified CreateOrderCommand class. This is an immutable command that is used in the ordering microservice in eShopOnContainers.\n\n```\n// DDD and CQRS patterns comment: Note that it is recommended to implement immutable Commands // In this case, its immutability is achieved by having all the setters as private // plus only being able to update the data just once, when creating the object through its constructor. // References on Immutable Commands: // http://cqrs.nu/Faq // https://docs.spine3.org/motivation/immutability.html // http://blog.gauffin.org/2012/06/griffin-container-introducing-command-support/ // https://learn.microsoft.com/dotnet/csharp/programming-guide/classes-and-structs/how-toimplement-a-lightweight-class-with-auto-implemented-properties [DataContract] public class CreateOrderCommand : IRequest<bool> { [DataMember] private readonly List<OrderItemDTO> _orderItems; [DataMember] public string UserId { get; private set; } [DataMember] public string UserName { get; private set; } [DataMember] public string City { get; private set; } [DataMember] public string Street { get; private set; } [DataMember] public string State { get; private set; } [DataMember] public string Country { get; private set; } [DataMember] public string ZipCode { get; private set; } [DataMember]\n```\n\n```\npublic string CardNumber { get; private set; } [DataMember] public string CardHolderName { get; private set; } [DataMember] public DateTime CardExpiration { get; private set; } [DataMember] public string CardSecurityNumber { get; private set; } [DataMember] public int CardTypeId { get; private set; } [DataMember] public IEnumerable<OrderItemDTO> OrderItems => _orderItems; public CreateOrderCommand() { _orderItems = new List<OrderItemDTO>(); } public CreateOrderCommand(List<BasketItem> basketItems , string userId , string userName , string city , string street , string state , string country , string zipcode , string cardNumber , string cardHolderName , DateTime cardExpiration , string cardSecurityNumber , int cardTypeId) : this() { _orderItems = basketItems . ToOrderItemsDTO().ToList(); UserId = userId; UserName = userName; City = city; Street = street; State = state; Country = country; ZipCode = zipcode; CardNumber = cardNumber; CardHolderName = cardHolderName; CardExpiration = cardExpiration; CardSecurityNumber = cardSecurityNumber; CardTypeId = cardTypeId; CardExpiration = cardExpiration; } public class OrderItemDTO { public int ProductId { get; set; } public string ProductName { get; set; } public decimal UnitPrice { get; set; } public decimal Discount { get; set; } public int Units { get; set; } public string PictureUrl { get; set; } } }\n```\n\nBasically, the command class contains all the data you need for performing a business transaction by using the domain model objects. Thus, commands are simply data structures that contain read-only data, and no behavior. The command's name indicates its purpose. In many languages like C#, commands are represented as classes, but they are not true classes in the real object-oriented sense.\n\nAs an additional characteristic, commands are immutable, because the expected usage is that they are processed directly by the domain model. They do not need to change during their projected lifetime. In a C# class, immutability can be achieved by not having any setters or other methods that change the internal state.\n\nKeep in mind that if you intend or expect commands to go through a serializing/deserializing process, the properties must have a private setter, and the [DataMember] (or [JsonProperty]) attribute. Otherwise, the deserializer won't be able to reconstruct the object at the destination with the required values. You can also use truly read-only properties if the class has a constructor with parameters for all properties, with the usual camelCase naming convention, and annotate the constructor as [JsonConstructor]. However, this option requires more code.\n\nFor example, the command class for creating an order is probably similar in terms of data to the order you want to create, but you probably do not need the same attributes. For instance, CreateOrderCommand does not have an order ID, because the order has not been created yet.\n\nMany command classes can be simple, requiring only a few fields about some state that needs to be changed. That would be the case if you are just changing the status of an order from \"in process\" to \"paid\" or \"shipped\" by using a command similar to the following:\n\n```\n[DataContract] public class UpdateOrderStatusCommand :IRequest<bool> { [DataMember] public string Status { get; private set; } [DataMember] public string OrderId { get; private set; } [DataMember] public string BuyerIdentityGuid { get; private set; } }\n```\n\nSome developers make their UI request objects separate from their command DTOs, but that is just a matter of preference. It is a tedious separation with not much additional value, and the objects are almost exactly the same shape. For instance, in eShopOnContainers, some commands come directly from the client -side.", "The Command handler class": "You should implement a specific command handler class for each command. That is how the pattern works, and it's where you'll use the command object, the domain objects, and the infrastructure repository objects. The command handler is in fact the heart of the application layer in terms of CQRS and DDD. However, all the domain logic should be contained in the domain classes\u2014within the aggregate roots (root entities), child entities, or domain services, but not within the command handler, which is a class from the application layer.\n\nThe command handler class offers a strong stepping stone in the way to achieve the Single Responsibility Principle (SRP) mentioned in a previous section.\n\nA command handler receives a command and obtains a result from the aggregate that is used. The result should be either successful execution of the command, or an exception. In the case of an exception, the system state should be unchanged.\n\nThe command handler usually takes the following steps:\n\n['It receives the command object, like a DTO (from the mediator or other infrastructure object).', 'It validates that the command is valid (if not validated by the mediator).', 'It instantiates the aggregate root instance that is the target of the current command.', 'It executes the method on the aggregate root instance, getting the required data from the command.', 'It persists the new state of the aggregate to its related database. This last operation is the actual transaction.']\n\nTypically, a command handler deals with a single aggregate driven by its aggregate root (root entity). If multiple aggregates should be impacted by the reception of a single command, you could use domain events to propagate states or actions across multiple aggregates.\n\nThe important point here is that when a command is being processed, all the domain logic should be inside the domain model (the aggregates), fully encapsulated and ready for unit testing. The command handler just acts as a way to get the domain model from the database, and as the final step, to tell the infrastructure layer (repositories) to persist the changes when the model is changed. The advantage of this approach is that you can refactor the domain logic in an isolated, fully encapsulated, rich, behavioral domain model without changing code in the application or infrastructure layers, which are the plumbing level (command handlers, Web API, repositories, etc.).\n\nWhen command handlers get complex, with too much logic, that can be a code smell. Review them, and if you find domain logic, refactor the code to move that domain behavior to the methods of the domain objects (the aggregate root and child entity).\n\nAs an example of a command handler class, the following code shows the same CreateOrderCommandHandler class that you saw at the beginning of this chapter. In this case, it also highlights the Handle method and the operations with the domain model objects/aggregates.\n\n```\npublic class CreateOrderCommandHandler : IRequestHandler<CreateOrderCommand , bool> { private readonly IOrderRepository _orderRepository; private readonly IIdentityService _identityService; private readonly IMediator _mediator; private readonly IOrderingIntegrationEventService _orderingIntegrationEventService; private readonly ILogger<CreateOrderCommandHandler> _logger; // Using DI to inject infrastructure persistence Repositories public CreateOrderCommandHandler(IMediator mediator , IOrderingIntegrationEventService orderingIntegrationEventService , IOrderRepository orderRepository ,\n```\n\n```\nIIdentityService identityService , ILogger<CreateOrderCommandHandler> logger) { _orderRepository = orderRepository ?? throw new ArgumentNullException(nameof(orderRepository)); _identityService = identityService ?? throw new ArgumentNullException(nameof(identityService)); _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator)); _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new ArgumentNullException(nameof(orderingIntegrationEventService)); _logger = logger ?? throw new ArgumentNullException(nameof(logger)); } public async Task<bool> Handle(CreateOrderCommand message , CancellationToken cancellationToken) { // Add Integration event to clean the basket var orderStartedIntegrationEvent = new OrderStartedIntegrationEvent(message . UserId); await _orderingIntegrationEventService . AddAndSaveEventAsync(orderStartedIntegrationEvent); // Add/Update the Buyer AggregateRoot // DDD patterns comment: Add child entities and value-objects through the Order Aggregate -Root // methods and constructor so validations, invariants and business logic // make sure that consistency is preserved across the whole aggregate var address = new Address(message . Street , message.City , message.State , message.Country , message.ZipCode); var order = new Order(message . UserId , message.UserName , address , message.CardTypeId , message.CardNumber , message.CardSecurityNumber , message.CardHolderName , message.CardExpiration); foreach (var item in message . OrderItems) { order . AddOrderItem(item . ProductId , item . ProductName , item . UnitPrice , item . Discount , item . PictureUrl , item . Units); } _logger . LogInformation( \" -----Creating Order -Order: {@Order}\" , order); _orderRepository . Add(order); return await _orderRepository . UnitOfWork . SaveEntitiesAsync(cancellationToken); } }\n```\n\nThese are additional steps a command handler should take:\n\n[\"Use the command's data to operate with the aggregate root's methods and behavior.\", 'Internally within the domain objects, raise domain events while the transaction is executed, but that is transparent from a command handler point of view.', \"If the aggregate's operation result is successful and after the transaction is finished, raise integration events. (These might also be raised by infrastructure classes like repositories.)\"]", "The Command process pipeline: how to trigger a command handler": "The next question is how to invoke a command handler. You could manually call it from each related ASP.NET Core controller. However, that approach would be too coupled and is not ideal.\n\nThe other two main options, which are the recommended options, are:\n\n['Through an in-memory Mediator pattern artifact.', 'With an asynchronous message queue, in between controllers and handlers.']", "Use the Mediator pattern (in-memory) in the command pipeline": "As shown in Figure 7-25, in a CQRS approach you use an intelligent mediator, similar to an in-memory bus, which is smart enough to redirect to the right command handler based on the type of the command or DTO being received. The single black arrows between components represent the dependencies between objects (in many cases, injected through DI) with their related interactions.\n\nRunning as a container\n\nASP.NET Core\n\nController\n\n(Application\n\nLayer)\n\nAggregates\n\nFigure 7 -25. Using the Mediator pattern in process in a single CQRS microservice\n\n<!-- image -->\n\nThe above diagram shows a zoom-in from image 7-24: the ASP.NET Core controller sends the command to MediatR's command pipeline, so they get to the appropriate handler.\n\nThe reason that using the Mediator pattern makes sense is that in enterprise applications, the processing requests can get complicated. You want to be able to add an open number of crosscutting concerns like logging, validations, audit, and security. In these cases, you can rely on a mediator pipeline (see Mediator pattern) to provide a means for these extra behaviors or crosscutting concerns.\n\nA mediator is an object that encapsulates the \"how\" of this process: it coordinates execution based on state, the way a command handler is invoked, or the payload you provide to the handler. With a mediator component, you can apply cross-cutting concerns in a centralized and transparent way by applying decorators (or pipeline behaviors since MediatR 3). For more information, see the Decorator pattern .\n\nDecorators and behaviors are similar to Aspect Oriented Programming (AOP), only applied to a specific process pipeline managed by the mediator component. Aspects in AOP that implement crosscutting concerns are applied based on aspect weavers injected at compilation time or based on object call interception. Both typical AOP approaches are sometimes said to work \"like magic,\" because it is not easy to see how AOP does its work. When dealing with serious issues or bugs, AOP can be difficult to debug. On the other hand, these decorators/behaviors are explicit and applied only in the context of the mediator, so debugging is much more predictable and easy.\n\nFor example, in the eShopOnContainers ordering microservice, has an implementation of two sample behaviors, a LogBehavior class and a ValidatorBehavior class. The implementation of the behaviors is explained in the next section by showing how eShopOnContainers uses MediatR behaviors .\n\nMicroservice\n\n(Writes-side of a simplified CQRS Architecture pattern)\n\nCommand\n\nCommand\n\n(Domain Model\n\nRunning as a container I\n\nWrites-side of a CQRS Architecture pattern using messaging\n\nWeb API\n\nmicroservice\n\nCommand-Handlers backend microservice\n\nRunning as a container", "Use message queues (out-of-proc) in the command\u2019s pipeline": "Another choice is to use asynchronous messages based on brokers or message queues, as shown in Figure 7-26. That option could also be combined with the mediator component right before the command handler. Handler\n\n(Application\n\nLayer)\n\n[]\n\n(Application", "Layer) HA Message Queue (External to the Repository (Infrastructure": "microservices)\n\nLayer)\n\nDatabase\n\nFigure 7 -26. Using message queues (out of the process and inter -process communication) with CQRS commands\n\n<!-- image -->\n\nCommand's pipeline can also be handled by a high availability message queue to deliver the commands to the appropriate handler. Using message queues to accept the commands can further complicate your command's pipeline, because you will probably need to split the pipeline into two processes connected through the external message queue. Still, it should be used if you need to have improved scalability and performance based on asynchronous messaging. Consider that in the case of Figure 7-26, the controller just posts the command message into the queue and returns. Then the command handlers process the messages at their own pace. That is a great benefit of queues: the message queue can act as a buffer in cases when hyper scalability is needed, such as for stocks or any other scenario with a high volume of ingress data.\n\nHowever, because of the asynchronous nature of message queues, you need to figure out how to communicate with the client application about the success or failure of the command's process. As a rule, you should never use \"fire and forget\" commands. Every business application needs to know if a command was processed successfully, or at least validated and accepted.\n\nThus, being able to respond to the client after validating a command message that was submitted to an asynchronous queue adds complexity to your system, as compared to an in-process command process that returns the operation's result after running the transaction. Using queues, you might need to return the result of the command process through other operation result messages, which will require additional components and custom communication in your system.\n\nAdditionally, async commands are one-way commands, which in many cases might not be needed, as is explained in the following interesting exchange between Burtsev Alexey and Greg Young in an online conversation:\n\n[Burtsev Alexey] I find lots of code where people use async command handling or one-way command messaging without any reason to do so (they are not doing some long operation, they are not executing external async code, they do not even cross-application boundary to be using message bus). Why do they introduce this unnecessary complexity? And actually, I haven't seen a CQRS code example with blocking command handlers so far, though it will work just fine in most cases.\n\n[Greg Young] [\u2026] an asynchronous command doesn't exist; it's actually another event. If I must accept what you send me and raise an event if I disagree, it's no longer you telling me to do something [that is, it's not a command]. It's you telling me something has been done. This seems like a slight difference at first, but it has many implications.\n\nAsynchronous commands greatly increase the complexity of a system, because there is no simple way to indicate failures. Therefore, asynchronous commands are not recommended other than when scaling requirements are needed or in special cases when communicating the internal microservices through messaging. In those cases, you must design a separate reporting and recovery system for failures.\n\nIn the initial version of eShopOnContainers, it was decided to use synchronous command processing, started from HTTP requests and driven by the Mediator pattern. That easily allows you to return the success or failure of the process, as in the CreateOrderCommandHandler implementation.\n\nIn any case, this should be a decision based on your application's or microservice's business requirements.", "Implement the command process pipeline with a mediator pattern (MediatR)": "As a sample implementation, this guide proposes using the in-process pipeline based on the Mediator pattern to drive command ingestion and route commands, in memory, to the right command handlers. The guide also proposes applying behaviors in order to separate cross-cutting concerns.\n\nFor implementation in .NET, there are multiple open-source libraries available that implement the Mediator pattern. The library used in this guide is the MediatR open-source library (created by Jimmy Bogard), but you could use another approach. MediatR is a small and simple library that allows you to process in-memory messages like a command, while applying decorators or behaviors.\n\nUsing the Mediator pattern helps you to reduce coupling and to isolate the concerns of the requested work, while automatically connecting to the handler that performs that work\u2014in this case, to command handlers.\n\nAnother good reason to use the Mediator pattern was explained by Jimmy Bogard when reviewing this guide:\n\nI think it might be worth mentioning testing here \u2013 it provides a nice consistent window into the behavior of your system. Request-in, response-out. We've found that aspect quite valuable in building consistently behaving tests.\n\nFirst, let's look at a sample WebAPI controller where you actually would use the mediator object. If you weren't using the mediator object, you'd need to inject all the dependencies for that controller, things like a logger object and others. Therefore, the constructor would be complicated. On the other hand, if you use the mediator object, the constructor of your controller can be a lot simpler, with just a few dependencies instead of many dependencies if you had one per cross-cutting operation, as in the following example:\n\n```\npublic class MyMicroserviceController : Controller { public MyMicroserviceController(IMediator mediator , IMyMicroserviceQueries microserviceQueries) { // ... } }\n```\n\nYou can see that the mediator provides a clean and lean Web API controller constructor. In addition, within the controller methods, the code to send a command to the mediator object is almost one line:\n\n```\n[Route(\"new\")] [HttpPost] public async Task<IActionResult> ExecuteBusinessOperation([FromBody]RunOpCommand runOperationCommand) { var commandResult = await _mediator . SendAsync(runOperationCommand); return commandResult ? (IActionResult)Ok() : (IActionResult)BadRequest(); }\n```", "Implement idempotent Commands": "In eShopOnContainers, a more advanced example than the above is submitting a CreateOrderCommand object from the Ordering microservice. But since the Ordering business process is a bit more complex and, in our case, it actually starts in the Basket microservice, this action of submitting the CreateOrderCommand object is performed from an integration-event handler named UserCheckoutAcceptedIntegrationEventHandler instead of a simple WebAPI controller called from the client App as in the previous simpler example.\n\nNevertheless, the action of submitting the Command to MediatR is pretty similar, as shown in the following code.\n\n```\nvar createOrderCommand = new CreateOrderCommand(eventMsg . Basket . Items , eventMsg . UserId , eventMsg . City , eventMsg . Street , eventMsg . State , eventMsg . Country , eventMsg . ZipCode , eventMsg . CardNumber , eventMsg . CardHolderName , eventMsg . CardExpiration , eventMsg . CardSecurityNumber , eventMsg . CardTypeId);\n```\n\n```\nvar requestCreateOrder = new IdentifiedCommand<CreateOrderCommand , bool>(createOrderCommand , eventMsg . RequestId); result = await _mediator . Send(requestCreateOrder);\n```\n\nHowever, this case is also slightly more advanced because we're also implementing idempotent commands. The CreateOrderCommand process should be idempotent, so if the same message comes duplicated through the network, because of any reason, like retries, the same business order will be processed just once.\n\nThis is implemented by wrapping the business command (in this case CreateOrderCommand) and embedding it into a generic IdentifiedCommand, which is tracked by an ID of every message coming through the network that has to be idempotent.\n\nIn the code below, you can see that the IdentifiedCommand is nothing more than a DTO with and ID plus the wrapped business command object.\n\n```\npublic class IdentifiedCommand<T , R> : IRequest<R> where T : IRequest<R> { public T Command { get; } public Guid Id { get; } public IdentifiedCommand(T command , Guid id) { Command = command; Id = id; } }\n```\n\nThen the CommandHandler for the IdentifiedCommand named IdentifiedCommandHandler.cs will basically check if the ID coming as part of the message already exists in a table. If it already exists, that command won't be processed again, so it behaves as an idempotent command. That infrastructure code is performed by the \\_requestManager.ExistAsync method call below.\n\n```\n// IdentifiedCommandHandler.cs public class IdentifiedCommandHandler<T , R> : IRequestHandler<IdentifiedCommand<T , R>, R> where T : IRequest<R> { private readonly IMediator _mediator; private readonly IRequestManager _requestManager; private readonly ILogger<IdentifiedCommandHandler<T , R>> _logger; public IdentifiedCommandHandler( IMediator mediator , IRequestManager requestManager , ILogger<IdentifiedCommandHandler<T , R>> logger) { _mediator = mediator; _requestManager = requestManager; _logger = logger ?? throw new System . ArgumentNullException(nameof(logger)); } /// <summary> /// Creates the result value to return if a previous request was found /// </summary> /// <returns></returns>\n```\n\n```\nprotected virtual R CreateResultForDuplicateRequest() { return default(R); } /// <summary> /// This method handles the command. It just ensures that no other request exists with the same ID, and if this is the case /// just enqueues the original inner command. /// </summary> /// <param name=\"message\">IdentifiedCommand which contains both original command & request ID</param> /// <returns>Return value of inner command or default value if request same ID was found</returns> public async Task<R> Handle(IdentifiedCommand<T , R> message , CancellationToken cancellationToken) { var alreadyExists = await _requestManager . ExistAsync(message . Id); if (alreadyExists) { return CreateResultForDuplicateRequest(); } else { await _requestManager . CreateRequestForCommandAsync<T>(message . Id); try { var command = message . Command; var commandName = command . GetGenericTypeName(); var idProperty = string . Empty; var commandId = string . Empty; switch (command) { case CreateOrderCommand createOrderCommand: idProperty = nameof(createOrderCommand . UserId); commandId = createOrderCommand . UserId; break; case CancelOrderCommand cancelOrderCommand: idProperty = nameof(cancelOrderCommand . OrderNumber); commandId = $\"{cancelOrderCommand.OrderNumber}\"; break; case ShipOrderCommand shipOrderCommand: idProperty = nameof(shipOrderCommand . OrderNumber); commandId = $\"{shipOrderCommand.OrderNumber}\"; break; default: idProperty = \"Id?\"; commandId = \"n/a\"; break; } _logger . LogInformation( \" -----Sending command: {CommandName} -{IdProperty}: {CommandId} ({@Command})\" , commandName , idProperty , commandId ,\n```\n\n```\ncommand); // Send the embedded business command to mediator so it runs its related CommandHandler var result = await _mediator . Send(command , cancellationToken); _logger . LogInformation( \" -----Command result: {@Result} -{CommandName} -{IdProperty}: {CommandId} ({@Command})\" , result , commandName , idProperty , commandId , command); return result; } catch { return default(R); } } } }\n```\n\nSince the IdentifiedCommand acts like a business command's envelope, when the business command needs to be processed because it is not a repeated ID, then it takes that inner business command and resubmits it to Mediator, as in the last part of the code shown above when running \\_mediator.Send(message.Command), from the IdentifiedCommandHandler.cs .\n\nWhen doing that, it will link and run the business command handler, in this case, the CreateOrderCommandHandler, which is running transactions against the Ordering database, as shown in the following code.\n\n```\n// CreateOrderCommandHandler.cs public class CreateOrderCommandHandler : IRequestHandler<CreateOrderCommand , bool> { private readonly IOrderRepository _orderRepository; private readonly IIdentityService _identityService; private readonly IMediator _mediator; private readonly IOrderingIntegrationEventService _orderingIntegrationEventService; private readonly ILogger<CreateOrderCommandHandler> _logger; // Using DI to inject infrastructure persistence Repositories public CreateOrderCommandHandler(IMediator mediator , IOrderingIntegrationEventService orderingIntegrationEventService , IOrderRepository orderRepository , IIdentityService identityService , ILogger<CreateOrderCommandHandler> logger) { _orderRepository = orderRepository ?? throw new ArgumentNullException(nameof(orderRepository)); _identityService = identityService ?? throw new ArgumentNullException(nameof(identityService)); _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator)); _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new ArgumentNullException(nameof(orderingIntegrationEventService)); _logger = logger ?? throw new ArgumentNullException(nameof(logger));\n```\n\n```\n} public async Task<bool> Handle(CreateOrderCommand message , CancellationToken cancellationToken) { // Add Integration event to clean the basket var orderStartedIntegrationEvent = new OrderStartedIntegrationEvent(message . UserId); await _orderingIntegrationEventService . AddAndSaveEventAsync(orderStartedIntegrationEvent); // Add/Update the Buyer AggregateRoot // DDD patterns comment: Add child entities and value-objects through the Order Aggregate -Root // methods and constructor so validations, invariants and business logic // make sure that consistency is preserved across the whole aggregate var address = new Address(message . Street , message.City , message.State , message.Country , message.ZipCode); var order = new Order(message . UserId , message.UserName , address , message.CardTypeId , message.CardNumber , message.CardSecurityNumber , message.CardHolderName , message.CardExpiration); foreach (var item in message . OrderItems) { order . AddOrderItem(item . ProductId , item . ProductName , item . UnitPrice , item . Discount , item . PictureUrl , item . Units); } _logger . LogInformation( \" -----Creating Order -Order: {@Order}\" , order); _orderRepository . Add(order); return await _orderRepository . UnitOfWork . SaveEntitiesAsync(cancellationToken); } }\n```", "Register the types used by MediatR": "In order for MediatR to be aware of your command handler classes, you need to register the mediator classes and the command handler classes in your IoC container. By default, MediatR uses Autofac as the IoC container, but you can also use the built-in ASP.NET Core IoC container or any other container supported by MediatR.\n\nThe following code shows how to register Mediator's types and commands when using Autofac modules.\n\n```\npublic class MediatorModule : Autofac . Module { protected override void Load(ContainerBuilder builder) { builder . RegisterAssemblyTypes(typeof(IMediator).GetTypeInfo().Assembly) . AsImplementedInterfaces(); // Register all the Command classes (they implement IRequestHandler) // in assembly holding the Commands builder . RegisterAssemblyTypes(typeof(CreateOrderCommand).GetTypeInfo().Assembly) . AsClosedTypesOf(typeof(IRequestHandler<,>)); // Other types registration\n```\n\n```\n//... } }\n```\n\nThis is where \u201cthe magic happens\u201d with MediatR.\n\nAs each command handler implements the generic IRequestHandler&lt;T&gt; interface, when you register the assemblies using RegisteredAssemblyTypes method all the types marked as IRequestHandler also gets registered with their Commands. For example:\n\n```\npublic class CreateOrderCommandHandler : IRequestHandler<CreateOrderCommand , bool> {\n```\n\nThat is the code that correlates commands with command handlers. The handler is just a simple class, but it inherits from RequestHandler&lt;T&gt;, where T is the command type, and MediatR makes sure it is invoked with the correct payload (the command).", "Apply cross-cutting concerns when processing commands with the Behaviors in MediatR": "There is one more thing: being able to apply cross-cutting concerns to the mediator pipeline. You can also see at the end of the Autofac registration module code how it registers a behavior type, specifically, a custom LoggingBehavior class and a ValidatorBehavior class. But you could add other custom behaviors, too.\n\n```\npublic class MediatorModule : Autofac . Module { protected override void Load(ContainerBuilder builder) { builder . RegisterAssemblyTypes(typeof(IMediator).GetTypeInfo().Assembly) . AsImplementedInterfaces(); // Register all the Command classes (they implement IRequestHandler) // in assembly holding the Commands builder . RegisterAssemblyTypes( typeof(CreateOrderCommand).GetTypeInfo().Assembly). AsClosedTypesOf(typeof(IRequestHandler<,>)); // Other types registration //... builder . RegisterGeneric(typeof(LoggingBehavior<,>)). As(typeof(IPipelineBehavior<,>)); builder . RegisterGeneric(typeof(ValidatorBehavior<,>)). As(typeof(IPipelineBehavior<,>)); } }\n```\n\nThat LoggingBehavior class can be implemented as the following code, which logs information about the command handler being executed and whether it was successful or not.\n\n```\npublic class LoggingBehavior<TRequest , TResponse> : IPipelineBehavior<TRequest , TResponse> { private readonly ILogger<LoggingBehavior<TRequest , TResponse>> _logger; public LoggingBehavior(ILogger<LoggingBehavior<TRequest , TResponse>> logger) => _logger = logger;\n```\n\n```\npublic async Task<TResponse> Handle(TRequest request , RequestHandlerDelegate<TResponse> next) { _logger . LogInformation($\"Handling {typeof(TRequest).Name}\"); var response = await next(); _logger . LogInformation($\"Handled {typeof(TResponse).Name}\"); return response; } }\n```\n\nJust by implementing this behavior class and by registering it in the pipeline (in the MediatorModule above), all the commands processed through MediatR will be logging information about the execution.\n\nThe eShopOnContainers ordering microservice also applies a second behavior for basic validations, the ValidatorBehavior class that relies on the FluentValidation library, as shown in the following code:\n\n```\npublic class ValidatorBehavior<TRequest , TResponse> : IPipelineBehavior<TRequest , TResponse> { private readonly IValidator<TRequest>[] _validators; public ValidatorBehavior(IValidator<TRequest>[] validators) => _validators = validators; public async Task<TResponse> Handle(TRequest request , RequestHandlerDelegate<TResponse> next) { var failures = _validators . Select(v => v . Validate(request)) . SelectMany(result => result . Errors) . Where(error => error != null) . ToList(); if (failures . Any()) { throw new OrderingDomainException( $\"Command Validation Errors for type {typeof(TRequest).Name}\" , new ValidationException(\"Validation exception\" , failures)); } var response = await next(); return response; } }\n```\n\nHere the behavior is raising an exception if validation fails, but you could also return a result object, containing the command result if it succeeded or the validation messages in case it didn't. This would probably make it easier to display validation results to the user.\n\nThen, based on the FluentValidation library, you would create validation for the data passed with CreateOrderCommand, as in the following code:\n\n```\npublic class CreateOrderCommandValidator : AbstractValidator<CreateOrderCommand> { public CreateOrderCommandValidator() { RuleFor(command => command . City).NotEmpty(); RuleFor(command => command . Street).NotEmpty();\n```\n\n```\nRuleFor(command => command . State).NotEmpty(); RuleFor(command => command . Country).NotEmpty(); RuleFor(command => command . ZipCode).NotEmpty(); RuleFor(command => command . CardNumber).NotEmpty().Length(12 , 19); RuleFor(command => command . CardHolderName).NotEmpty(); RuleFor(command => command . CardExpiration).NotEmpty().Must(BeValidExpirationDate).WithMessage(\"Please specify a valid card expiration date\"); RuleFor(command => command . CardSecurityNumber).NotEmpty().Length(3); RuleFor(command => command . CardTypeId).NotEmpty(); RuleFor(command => command . OrderItems).Must(ContainOrderItems).WithMessage(\"No order items found\"); } private bool BeValidExpirationDate(DateTime dateTime) { return dateTime >= DateTime . UtcNow; } private bool ContainOrderItems(IEnumerable<OrderItemDTO> orderItems) { return orderItems . Any(); } }\n```\n\nYou could create additional validations. This is a very clean and elegant way to implement your command validations.\n\nIn a similar way, you could implement other behaviors for additional aspects or cross-cutting concerns that you want to apply to commands when handling them.", "The mediator pattern": ["\u2022", "Mediator pattern"], "The decorator pattern": ["\u2022", "Decorator pattern https://en.wikipedia.org/wiki/Decorator\\_pattern"], "MediatR (Jimmy Bogard)": ["\u2022", "\u2022", "MediatR. GitHub repo."], "Fluent validation": ["Jeremy Skinner. FluentValidation. GitHub repo. https://github.com/JeremySkinner/FluentValidation"], "Implement resilient applications": "Your microservice and cloud -based applications must embrace the partial failures that will certainly occur eventually. You must design your application to be resilient to those partial failures.\n\nResiliency is the ability to recover from failures and continue to function. It isn't about avoiding failures but accepting the fact that failures will happen and responding to them in a way that avoids downtime or data loss. The goal of resiliency is to return the application to a fully functioning state after a failure.\n\nIt's challenging enough to design and deploy a microservices-based application. But you also need to keep your application running in an environment where some sort of failure is certain. Therefore, your application should be resilient. It should be designed to cope with partial failures, like network outages or nodes or VMs crashing in the cloud. Even microservices (containers) being moved to a different node within a cluster can cause intermittent short failures within the application.\n\nThe many individual components of your application should also incorporate health monitoring features. By following the guidelines in this chapter, you can create an application that can work smoothly in spite of transient downtime or the normal hiccups that occur in complex and cloud-based deployments.", "Important": "We recommend that you register Azure Key Vault as the last configuration provider, so it can override configuration values from previous providers.", "Warning": "All code samples and images in this section were valid before using Linkerd and are not updated to reflect the current actual code. So they make sense in the context of this section.\n\nMultiple\n\nCustomers\n\nBrowser\n\nSubmit Order page r - -\n\n| Backend", "Handle partial failure": "ASP.NET Core\n\nRunning as\n\nIn distributed systems like microservices-based applications, there's an ever-present risk of partial failure. For instance, a single microservice/container can fail or might not be available to respond for a short time, or a single VM or server can crash. Since clients and services are separate processes, a service might not be able to respond in a timely way to a client's request. The service might be overloaded and responding very slowly to requests or might simply not be accessible for a short time because of network issues. ad n Container\n\nThread pool\n\nFor example, consider the Order details page from the eShopOnContainers sample application. If the ordering microservice is unresponsive when the user tries to submit an order, a bad implementation of the client process (the MVC web application)\u2014for example, if the client code were to use synchronous RPCs with no timeout\u2014would block threads indefinitely waiting for a response. Besides creating a bad user experience, every unresponsive wait consumes or blocks a thread, and threads are extremely valuable in highly scalable applications. If there are many blocked threads, eventually the application's runtime can run out of threads. In that case, the application can become globally unresponsive instead of just partially unresponsive, as shown in Figure 8-1.\n\nFigure 8 -1. Partial failures because of dependencies that impact service thread availability\n\n<!-- image -->\n\nIn a large microservices-based application, any partial failure can be amplified, especially if most of the internal microservices interaction is based on synchronous HTTP calls (which is considered an antipattern). Think about a system that receives millions of incoming calls per day. If your system has a bad design that's based on long chains of synchronous HTTP calls, these incoming calls might result in many more millions of outgoing calls (let's suppose a ratio of 1:4) to dozens of internal microservices as synchronous dependencies. This situation is shown in Figure 8-2, especially dependency #3, that starts a chain, calling dependency #4, which then calls #5.\n\nPartial failures\n\nRunning as\n\nContainer\n\nMultiple\n\nCustomers\n\nMultiple\n\nCustomers\n\nBrowser\n\nBrowser\n\nSubmit Order page\n\nSubmit Order page\n\nMultiple distributed dependencies\n\nPartial Failure Amplified in Microservices\n\n[[]]\n\n| Backend / Orchestrator\n\n| Backend / Orchestrator\n\nWeb App\n\n<!-- image -->\n\nDependency\n\nDependency\n\nFigure 8 -2. The impact of having an incorrect design featuring long chains of HTTP requests\n\nIntermittent failure is guaranteed in a distributed and cloud-based system, even if every dependency itself has excellent availability. It's a fact you need to consider.\n\nIf you do not design and implement techniques to ensure fault tolerance, even small downtimes can be amplified. As an example, 50 dependencies each with 99.99% of availability would result in several hours of downtime each month because of this ripple effect. When a microservice dependency fails while handling a high volume of requests, that failure can quickly saturate all available request threads in each service and crash the whole application.\n\nFigure 8 -3. Partial failure amplified by microservices with long chains of synchronous HTTP calls\n\n<!-- image -->\n\nTo minimize this problem, in the section Asynchronous microservice integration enforce microservice's autonomy, this guide encourages you to use asynchronous communication across the internal microservices.\n\nIn addition, it's essential that you design your microservices and client applications to handle partial failures \u2014 that is, to build resilient microservices and client applications.", "Strategies to handle partial failure": "To deal with partial failures, use one of the strategies described here.\n\nUse asynchronous communication (for example, message-based communication) across internal microservices. It's highly advisable not to create long chains of synchronous HTTP calls across the internal microservices because that incorrect design will eventually become the main cause of bad outages. On the contrary, except for the front-end communications between the client applications and the first level of microservices or fine-grained API Gateways, it's recommended to use only asynchronous (message-based) communication once past the initial request/response cycle, across the internal microservices. Eventual consistency and event-driven architectures will help to minimize ripple effects. These approaches enforce a higher level of microservice autonomy and therefore prevent against the problem noted here.\n\nUse retries with exponential backoff. This technique helps to avoid short and intermittent failures by performing call retries a certain number of times, in case the service was not available only for a short time. This might occur due to intermittent network issues or when a microservice/container is moved to a different node in a cluster. However, if these retries are not designed properly with circuit breakers, it can aggravate the ripple effects, ultimately even causing a Denial of Service (DoS) .\n\nWork around network timeouts. In general, clients should be designed not to block indefinitely and to always use timeouts when waiting for a response. Using timeouts ensures that resources are never tied up indefinitely.\n\nUse the Circuit Breaker pattern. In this approach, the client process tracks the number of failed requests. If the error rate exceeds a configured limit, a \"circuit breaker\" trips so that further attempts fail immediately. (If a large number of requests are failing, that suggests the service is unavailable and that sending requests is pointless.) After a timeout period, the client should try again and, if the new requests are successful, close the circuit breaker.\n\nProvide fallbacks. In this approach, the client process performs fallback logic when a request fails, such as returning cached data or a default value. This is an approach suitable for queries, and is more complex for updates or commands.\n\nLimit the number of queued requests. Clients should also impose an upper bound on the number of outstanding requests that a client microservice can send to a particular service. If the limit has been reached, it's probably pointless to make additional requests, and those attempts should fail immediately. In terms of implementation, the Polly Bulkhead Isolation policy can be used to fulfill this requirement. This approach is essentially a parallelization throttle with SemaphoreSlim as the implementation. It also permits a \"queue\" outside the bulkhead. You can proactively shed excess load even before execution (for example, because capacity is deemed full). This makes its response to\n\ncertain failure scenarios faster than a circuit breaker would be, since the circuit breaker waits for the failures. The BulkheadPolicy object in Polly exposes how full the bulkhead and queue are, and offers events on overflow so can also be used to drive automated horizontal scaling.", "Implement retries with exponential backoff": "Retries with exponential backoff is a technique that retries an operation, with an exponentially increasing wait time, up to a maximum retry count has been reached (the exponential backoff). This technique embraces the fact that cloud resources might intermittently be unavailable for more than a few seconds for any reason. For example, an orchestrator might be moving a container to another node in a cluster for load balancing. During that time, some requests might fail. Another example could be a database like SQL Azure, where a database can be moved to another server for load balancing, causing the database to be unavailable for a few seconds.\n\nThere are many approaches to implement retries logic with exponential backoff.", "Implement resilient Entity Framework Core SQL connections": "For Azure SQL DB, Entity Framework (EF) Core already provides internal database connection resiliency and retry logic. But you need to enable the Entity Framework execution strategy for each DbContext connection if you want to have resilient EF Core connections .\n\nFor instance, the following code at the EF Core connection level enables resilient SQL connections that are retried if the connection fails.\n\n```\n// Program.cs from any ASP.NET Core Web API // Other code ... builder . Services . AddDbContext<CatalogContext>(options => {\n```\n\n```\noptions . UseSqlServer(builder . Configuration[\"ConnectionString\"], sqlServerOptionsAction: sqlOptions => { sqlOptions . EnableRetryOnFailure( maxRetryCount: 10 , maxRetryDelay: TimeSpan . FromSeconds(30), errorNumbersToAdd: null); }); });\n```", "Execution strategies and explicit transactions using BeginTransaction and multiple DbContexts": "When retries are enabled in EF Core connections, each operation you perform using EF Core becomes its own retryable operation. Each query and each call to SaveChanges will be retried as a unit if a transient failure occurs.\n\nHowever, if your code initiates a transaction using BeginTransaction, you're defining your own group of operations that need to be treated as a unit. Everything inside the transaction has to be rolled back if a failure occurs.\n\nIf you try to execute that transaction when using an EF execution strategy (retry policy) and you call SaveChanges from multiple DbContexts, you'll get an exception like this one:\n\nSystem.InvalidOperationException: The configured execution strategy\n\n'SqlServerRetryingExecutionStrategy' does not support user initiated transactions. Use the execution strategy returned by 'DbContext.Database.CreateExecutionStrategy()' to execute all the operations in the transaction as a retriable unit.\n\nThe solution is to manually invoke the EF execution strategy with a delegate representing everything that needs to be executed. If a transient failure occurs, the execution strategy will invoke the delegate again. For example, the following code shows how it's implemented in eShopOnContainers with two multiple DbContexts (\\_catalogContext and the IntegrationEventLogContext) when updating a product and then saving the ProductPriceChangedIntegrationEvent object, which needs to use a different DbContext.\n\n```\npublic async Task<IActionResult> UpdateProduct( [FromBody]CatalogItem productToUpdate) { // Other code ... var oldPrice = catalogItem . Price; var raiseProductPriceChangedEvent = oldPrice != productToUpdate . Price; // Update current product catalogItem = productToUpdate; // Save product's data and publish integration event through the Event Bus // if price has changed if (raiseProductPriceChangedEvent) { //Create Integration Event to be published through the Event Bus var priceChangedEvent = new ProductPriceChangedIntegrationEvent( catalogItem . Id , productToUpdate . Price , oldPrice);\n```\n\n```\n// Achieving atomicity between original Catalog database operation and the // IntegrationEventLog thanks to a local transaction await _catalogIntegrationEventService . SaveEventAndCatalogContextChangesAsync( priceChangedEvent); // Publish through the Event Bus and mark the saved event as published await _catalogIntegrationEventService . PublishThroughEventBusAsync( priceChangedEvent); } // Just save the updated product because the Product's Price hasn't changed. else { await _catalogContext . SaveChangesAsync(); } }\n```\n\nThe first DbContext is \\_catalogContext and the second DbContext is within the \\_catalogIntegrationEventService object. The Commit action is performed across all DbContext objects using an EF execution strategy.\n\nTo achieve this multiple DbContext commit, the SaveEventAndCatalogContextChangesAsync uses a ResilientTransaction class, as shown in the following code:\n\n```\npublic class CatalogIntegrationEventService : ICatalogIntegrationEventService { //\u2026 public async Task SaveEventAndCatalogContextChangesAsync( IntegrationEvent evt) { // Use of an EF Core resiliency strategy when using multiple DbContexts // within an explicit BeginTransaction(): // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency await ResilientTransaction . New(_catalogContext).ExecuteAsync(async () => { // Achieving atomicity between original catalog database // operation and the IntegrationEventLog thanks to a local transaction await _catalogContext . SaveChangesAsync(); await _eventLogService . SaveEventAsync(evt , _catalogContext . Database . CurrentTransaction . GetDbTransaction()); }); } }\n```\n\nThe ResilientTransaction.ExecuteAsync method basically begins a transaction from the passed DbContext (\\_catalogContext) and then makes the EventLogService use that transaction to save changes from the IntegrationEventLogContext and then commits the whole transaction.\n\n```\npublic class ResilientTransaction { private DbContext _context; private ResilientTransaction(DbContext context) => _context = context ?? throw new ArgumentNullException(nameof(context)); public static ResilientTransaction New (DbContext context) => new ResilientTransaction(context); public async Task ExecuteAsync(Func<Task> action) {\n```\n\n```\n// Use of an EF Core resiliency strategy when using multiple DbContexts // within an explicit BeginTransaction(): // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency var strategy = _context . Database . CreateExecutionStrategy(); await strategy . ExecuteAsync(async () => { await using var transaction = await _context . Database . BeginTransactionAsync(); await action(); await transaction . CommitAsync(); }); } }\n```", "Use IHttpClientFactory to implement resilient HTTP requests": "IHttpClientFactory is a contract implemented by DefaultHttpClientFactory, an opinionated factory, available since .NET Core 2.1, for creating HttpClient instances to be used in your applications.", "Issues with the original HttpClient class available in .NET": "The original and well-known HttpClient class can be easily used, but in some cases, it isn't being properly used by many developers.\n\nThough this class implements IDisposable, declaring and instantiating it within a using statement is not preferred because when the HttpClient object gets disposed of, the underlying socket is not immediately released, which can lead to a socket exhaustion problem. For more information about this issue, see the blog post You're using HttpClient wrong and it's destabilizing your software .\n\nTherefore, HttpClient is intended to be instantiated once and reused throughout the life of an application. Instantiating an HttpClient class for every request will exhaust the number of sockets available under heavy loads. That issue will result in SocketException errors. Possible approaches to solve that problem are based on the creation of the HttpClient object as singleton or static, as explained in this Microsoft article on HttpClient usage. This can be a good solution for short-lived console apps or similar, that run a few times a day.\n\nAnother issue that developers run into is when using a shared instance of HttpClient in long-running processes. In a situation where the HttpClient is instantiated as a singleton or a static object, it fails to handle the DNS changes as described in this issue of the dotnet/runtime GitHub repository.\n\nHowever, the issue isn't really with HttpClient per se, but with the default constructor for HttpClient , because it creates a new concrete instance of HttpMessageHandler, which is the one that has sockets exhaustion and DNS changes issues mentioned above.\n\nTo address the issues mentioned above and to make HttpClient instances manageable, .NET Core 2.1 introduced two approaches, one of them being IHttpClientFactory. It's an interface that's used to configure and create HttpClient instances in an app through Dependency Injection (DI). It also provides extensions for Polly-based middleware to take advantage of delegating handlers in HttpClient.\n\nThe alternative is to use SocketsHttpHandler with configured PooledConnectionLifetime. This approach is applied to long-lived, static or singleton HttpClient instances. To learn more about different strategies, see HttpClient guidelines for .NET .\n\nPolly is a transient-fault-handling library that helps developers add resiliency to their applications, by using some pre-defined policies in a fluent and thread-safe manner.", "Benefits of using IHttpClientFactory": "The current implementation of IHttpClientFactory, that also implements IHttpMessageHandlerFactory , offers the following benefits:\n\n[\"Provides a central location for naming and configuring logical HttpClient objects. For example, you may configure a client (Service Agent) that's pre-configured to access a specific microservice.\", \"Codify the concept of outgoing middleware via delegating handlers in HttpClient and implementing Polly-based middleware to take advantage of Polly's policies for resiliency.\", 'HttpClient already has the concept of delegating handlers that could be linked together for outgoing HTTP requests. You can register HTTP clients into the factory and you can use a Polly handler to use Polly policies for Retry, CircuitBreakers, and so on.', 'Manage the lifetime of HttpMessageHandler to avoid the mentioned problems/issues that can occur when managing HttpClient lifetimes yourself.']", "Tip": "All details are handled by the authorization middleware and services previously mentioned. So, you just have to choose the Individual User Account authentication option when you create the ASP.NET Core web application project in Visual Studio, as shown in Figure 9-3, besides registering the authentication providers previously mentioned.\n\nCreate a new ASP.NET Core web application\n\n.NET Core\n\nASP.NET Core Empty it.\n\nASP.NET Core Web API\n\nASP.NET Core Web App\n\nASP.NET Core with Angular\n\nD\n\nASP.NET Core with React.js\n\nGet additional project templates\n\nFigure 9 -3. Selecting the Individual User Accounts option, for using external authentication, when creating a web application project in Visual Studio 2019.\n\n<!-- image -->\n\nIn addition to the external authentication providers listed previously, third-party packages are available that provide middleware for using many more external authentication providers. For a list, see the AspNet.Security.OAuth.Providers repository on GitHub.\n\nYou can also create your own external authentication middleware to solve some special need.", "Note": "Environment variables are commonly stored as plain text, so if the machine or process with the environment variables is compromised, the environment variable values will be visible.", "Multiple ways to use IHttpClientFactory": "There are several ways that you can use IHttpClientFactory in your application:\n\n['Basic usage', 'Use Named Clients', 'Use Typed Clients', 'Use Generated Clients']\n\nFor the sake of brevity, this guidance shows the most structured way to use IHttpClientFactory, which is to use Typed Clients (Service Agent pattern). However, all options are documented and are currently listed in this article covering the IHttpClientFactory usage .", "How to use Typed Clients with IHttpClientFactory": "So, what's a \"Typed Client\"? It's just an HttpClient that's pre-configured for some specific use. This configuration can include specific values such as the base server, HTTP headers or time outs.\n\nThe following diagram shows how Typed Clients are used with IHttpClientFactory:\n\nClient application/code\n\nDependency Injection\n\nU\n\nController or client cod\n\nFigure 8 -4. Using IHttpClientFactory with Typed Client classes.\n\n<!-- image -->\n\nIn the above image, a ClientService (used by a controller or client code) uses an HttpClient created by the registered IHttpClientFactory. This factory assigns an HttpMessageHandler from a pool to the HttpClient. The HttpClient can be configured with Polly's policies when registering the IHttpClientFactory in the DI container with the extension method AddHttpClient .\n\nTo configure the above structure, add IHttpClientFactory in your application by installing the Microsoft.Extensions.Http NuGet package that includes the AddHttpClient extension method for IServiceCollection. This extension method registers the internal DefaultHttpClientFactory class to be used as a singleton for the interface IHttpClientFactory. It defines a transient configuration for the HttpMessageHandlerBuilder. This message handler (HttpMessageHandler object), taken from a pool, is used by the HttpClient returned from the factory.\n\nIn the next snippet, you can see how AddHttpClient() can be used to register Typed Clients (Service Agents) that need to use HttpClient.\n\n```\n// Program.cs //Add http client services at ConfigureServices(IServiceCollection services) builder . Services . AddHttpClient<ICatalogService , CatalogService>(); builder . Services . AddHttpClient<IBasketService , BasketService>(); builder . Services . AddHttpClient<IOrderingService , OrderingService>();\n```\n\nRegistering the client services as shown in the previous snippet, makes the DefaultClientFactory create a standard HttpClient for each service. The typed client is registered as transient with DI container. In the preceding code, AddHttpClient() registers CatalogService , BasketService , OrderingService as transient services so they can be injected and consumed directly without any need for additional registrations.\n\nYou could also add instance -specific configuration in the registration to, for example, configure the base address, and add some resiliency policies, as shown in the following:\n\n```\nbuilder . Services . AddHttpClient<ICatalogService , CatalogService>(client => { client . BaseAddress = new Uri(builder . Configuration[\"BaseUrl\"]); }) . AddPolicyHandler(GetRetryPolicy()) . AddPolicyHandler(GetCircuitBreakerPolicy());\n```\n\nIn this next example, you can see the configuration of one of the above policies:\n\n```\nstatic IAsyncPolicy<HttpResponseMessage> GetRetryPolicy() { return HttpPolicyExtensions . HandleTransientHttpError() . OrResult(msg => msg . StatusCode == System . Net . HttpStatusCode . NotFound) . WaitAndRetryAsync(6 , retryAttempt => TimeSpan . FromSeconds(Math . Pow(2 , retryAttempt))); }\n```\n\nYou can find more details about using Polly in the Next article .", "HttpClient lifetimes": "Each time you get an HttpClient object from the IHttpClientFactory, a new instance is returned. But each HttpClient uses an HttpMessageHandler that's pooled and reused by the IHttpClientFactory to reduce resource consumption, as long as the HttpMessageHandler's lifetime hasn't expired.\n\nPooling of handlers is desirable as each handler typically manages its own underlying HTTP connections; creating more handlers than necessary can result in connection delays. Some handlers also keep connections open indefinitely, which can prevent the handler from reacting to DNS changes.\n\nThe HttpMessageHandler objects in the pool have a lifetime that's the length of time that an HttpMessageHandler instance in the pool can be reused. The default value is two minutes, but it can be overridden per Typed Client. To override it, call SetHandlerLifetime() on the IHttpClientBuilder that's returned when creating the client, as shown in the following code:\n\n```\n//Set 5 min as the lifetime for the HttpMessageHandler objects in the pool used for the Catalog Typed Client builder . Services . AddHttpClient<ICatalogService , CatalogService>() . SetHandlerLifetime(TimeSpan . FromMinutes(5));\n```\n\nEach Typed Client can have its own configured handler lifetime value. Set the lifetime to InfiniteTimeSpan to disable handler expiry.", "Implement your Typed Client classes that use the injected and configured HttpClient": "As a previous step, you need to have your Typed Client classes defined, such as the classes in the sample code, like 'BasketService', 'CatalogService', 'OrderingService', etc. \u2013 A Typed Client is a class that accepts an HttpClient object (injected through its constructor) and uses it to call some remote HTTP service. For example:\n\n```\npublic class CatalogService : ICatalogService { private readonly HttpClient _httpClient; private readonly string _remoteServiceBaseUrl; public CatalogService(HttpClient httpClient) { _httpClient = httpClient; } public async Task<Catalog> GetCatalogItems(int page , int take , int? brand , int? type) { var uri = API . Catalog . GetAllCatalogItems(_remoteServiceBaseUrl , page, take , brand , type); var responseString = await _httpClient . GetStringAsync(uri); var catalog = JsonConvert . DeserializeObject<Catalog>(responseString); return catalog; } }\n```\n\nThe Typed Client (CatalogService in the example) is activated by DI (Dependency Injection), which means it can accept any registered service in its constructor, in addition to HttpClient.\n\nA Typed Client is effectively a transient object, that means a new instance is created each time one is needed. It receives a new HttpClient instance each time it's constructed. However, the HttpMessageHandler objects in the pool are the objects that are reused by multiple HttpClient instances.", "Use your Typed Client classes": "Finally, once you have your typed classes implemented, you can have them registered and configured with AddHttpClient(). After that you can use them wherever services are injected by DI, such as in Razor page code or an MVC web app controller, shown in the below code from eShopOnContainers:\n\n```\nnamespace Microsoft . eShopOnContainers . WebMVC . Controllers { public class CatalogController : Controller { private ICatalogService _catalogSvc; public CatalogController(ICatalogService catalogSvc) => _catalogSvc = catalogSvc; public async Task<IActionResult> Index(int? BrandFilterApplied , int? TypesFilterApplied ,\n```\n\n```\nint? page , [FromQuery]string errorMsg) { var itemsPage = 10; var catalog = await _catalogSvc . GetCatalogItems(page ?? 0 , itemsPage , BrandFilterApplied , TypesFilterApplied); //\u2026 Additional code } } }\n```\n\nUp to this point, the above code snippet only shows the example of performing regular HTTP requests. But the 'magic' comes in the following sections where it shows how all the HTTP requests made by HttpClient can have resilient policies such as retries with exponential backoff, circuit breakers, security features using auth tokens, or even any other custom feature. And all of these can be done just by adding policies and delegating handlers to your registered Typed Clients.", "Implement HTTP call retries with exponential backoff with IHttpClientFactory and Polly policies": "The recommended approach for retries with exponential backoff is to take advantage of more advanced .NET libraries like the open-source Polly library .\n\nPolly is a .NET library that provides resilience and transient-fault handling capabilities. You can implement those capabilities by applying Polly policies such as Retry, Circuit Breaker, Bulkhead Isolation, Timeout, and Fallback. Polly targets .NET Framework 4.x and .NET Standard 1.0, 1.1, and 2.0 (which supports .NET Core and later).\n\nThe following steps show how you can use Http retries with Polly integrated into IHttpClientFactory, which is explained in the previous section.", "Install .NET packages": "First, you will need to install the Microsoft.Extensions.Http.Polly package.\n\n['Install with Visual Studio', 'Install with dotnet CLI', 'Install with nuget.exe CLI', 'Install with Package Manager Console (PowerShell)']", "Reference the .NET 7 packages": "IHttpClientFactory is available since .NET Core 2.1, however, we recommend you use the latest .NET 7 packages from NuGet in your project. You typically also need to reference the extension package Microsoft.Extensions.Http.Polly.", "Configure a client with Polly\u2019s Retry policy, in app startup": "The AddPolicyHandler() method is what adds policies to the HttpClient objects you'll use. In this case, it's adding a Polly's policy for Http Retries with exponential backoff.\n\nTo have a more modular approach, the Http Retry Policy can be defined in a separate method within the Program.cs file, as shown in the following code:\n\n```\nstatic IAsyncPolicy<HttpResponseMessage> GetRetryPolicy() { return HttpPolicyExtensions . HandleTransientHttpError() . OrResult(msg => msg . StatusCode == System . Net . HttpStatusCode . NotFound) . WaitAndRetryAsync(6 , retryAttempt => TimeSpan . FromSeconds(Math . Pow(2 , retryAttempt))); }\n```\n\nAs shown in previous sections, you need to define a named or typed client HttpClient configuration in your standard Program.cs app configuration. Now you add incremental code specifying the policy for the Http retries with exponential backoff, as follows:\n\n```\n// Program.cs builder . Services . AddHttpClient<IBasketService , BasketService>() . SetHandlerLifetime(TimeSpan . FromMinutes(5)) //Set lifetime to five minutes . AddPolicyHandler(GetRetryPolicy());\n```\n\nWith Polly, you can define a Retry policy with the number of retries, the exponential backoff configuration, and the actions to take when there's an HTTP exception, such as logging the error. In this case, the policy is configured to try six times with an exponential retry, starting at two seconds.", "Add a jitter strategy to the retry policy": "A regular Retry policy can affect your system in cases of high concurrency and scalability and under high contention. To overcome peaks of similar retries coming from many clients in partial outages, a good workaround is to add a jitter strategy to the retry algorithm/policy. This strategy can improve the overall performance of the end-to-end system. As recommended in Polly: Retry with Jitter, a good\n\njitter strategy can be implemented by smooth and evenly distributed retry intervals applied with a well -controlled median initial retry delay on an exponential backoff. This approach helps to spread out the spikes when the issue arises. The principle is illustrated by the following example:\n\n```\nvar delay = Backoff . DecorrelatedJitterBackoffV2(medianFirstRetryDelay: TimeSpan . FromSeconds(1), retryCount: 5); var retryPolicy = Policy . Handle<FooException>() . WaitAndRetryAsync(delay);\n```", "Implement the Circuit Breaker pattern": "As noted earlier, you should handle faults that might take a variable amount of time to recover from, as might happen when you try to connect to a remote service or resource. Handling this type of fault can improve the stability and resiliency of an application.\n\nIn a distributed environment, calls to remote resources and services can fail due to transient faults, such as slow network connections and timeouts, or if resources are responding slowly or are temporarily unavailable. These faults typically correct themselves after a short time, and a robust cloud application should be prepared to handle them by using a strategy like the \"Retry pattern\".\n\nHowever, there can also be situations where faults are due to unanticipated events that might take much longer to fix. These faults can range in severity from a partial loss of connectivity to the complete failure of a service. In these situations, it might be pointless for an application to continually retry an operation that's unlikely to succeed.\n\nInstead, the application should be coded to accept that the operation has failed and handle the failure accordingly.\n\nUsing Http retries carelessly could result in creating a Denial of Service (DoS) attack within your own software. As a microservice fails or performs slowly, multiple clients might repeatedly retry failed requests. That creates a dangerous risk of exponentially increasing traffic targeted at the failing service.\n\nTherefore, you need some kind of defense barrier so that excessive requests stop when it isn't worth to keep trying. That defense barrier is precisely the circuit breaker.\n\nThe Circuit Breaker pattern has a different purpose than the \"Retry pattern\". The \"Retry pattern\" enables an application to retry an operation in the expectation that the operation will eventually succeed. The Circuit Breaker pattern prevents an application from performing an operation that's likely to fail. An application can combine these two patterns. However, the retry logic should be sensitive to any exception returned by the circuit breaker, and it should abandon retry attempts if the circuit breaker indicates that a fault is not transient.", "Implement Circuit Breaker pattern with IHttpClientFactory and Polly": "As when implementing retries, the recommended approach for circuit breakers is to take advantage of proven .NET libraries like Polly and its native integration with IHttpClientFactory.\n\nAdding a circuit breaker policy into your IHttpClientFactory outgoing middleware pipeline is as simple as adding a single incremental piece of code to what you already have when using IHttpClientFactory.\n\nThe only addition here to the code used for HTTP call retries is the code where you add the Circuit Breaker policy to the list of policies to use, as shown in the following incremental code.\n\n```\n// Program.cs var retryPolicy = GetRetryPolicy(); var circuitBreakerPolicy = GetCircuitBreakerPolicy(); builder . Services . AddHttpClient<IBasketService , BasketService>() . SetHandlerLifetime(TimeSpan . FromMinutes(5)) // Sample: default lifetime is 2 minutes . AddHttpMessageHandler<HttpClientAuthorizationDelegatingHandler>() . AddPolicyHandler(retryPolicy) . AddPolicyHandler(circuitBreakerPolicy);\n```\n\nThe AddPolicyHandler() method is what adds policies to the HttpClient objects you'll use. In this case, it's adding a Polly policy for a circuit breaker.\n\nTo have a more modular approach, the Circuit Breaker Policy is defined in a separate method called GetCircuitBreakerPolicy(), as shown in the following code:\n\n```\n// also in Program.cs static IAsyncPolicy<HttpResponseMessage> GetCircuitBreakerPolicy() { return HttpPolicyExtensions . HandleTransientHttpError() . CircuitBreakerAsync(5 , TimeSpan . FromSeconds(30)); }\n```\n\nIn the code example above, the circuit breaker policy is configured so it breaks or opens the circuit when there have been five consecutive faults when retrying the Http requests. When that happens, the circuit will break for 30 seconds: in that period, calls will be failed immediately by the circuitbreaker rather than actually be placed. The policy automatically interprets relevant exceptions and HTTP status codes as faults.\n\nCircuit breakers should also be used to redirect requests to a fallback infrastructure if you had issues in a particular resource that's deployed in a different environment than the client application or\n\nservice that's performing the HTTP call. That way, if there's an outage in the datacenter that impacts only your backend microservices but not your client applications, the client applications can redirect to the fallback services. Polly is planning a new policy to automate this failover policy scenario.\n\nAll those features are for cases where you're managing the failover from within the .NET code, as opposed to having it managed automatically for you by Azure, with location transparency.\n\nFrom a usage point of view, when using HttpClient, there's no need to add anything new here because the code is the same than when using HttpClient with IHttpClientFactory, as shown in previous sections.", "Test Http retries and circuit breakers in eShopOnContainers": "Whenever you start the eShopOnContainers solution in a Docker host, it needs to start multiple containers. Some of the containers are slower to start and initialize, like the SQL Server container. This is especially true the first time you deploy the eShopOnContainers application into Docker because it needs to set up the images and the database. The fact that some containers start slower than others can cause the rest of the services to initially throw HTTP exceptions, even if you set dependencies between containers at the docker -compose level, as explained in previous sections. Those dockercompose dependencies between containers are just at the process level. The container's entry point process might be started, but SQL Server might not be ready for queries. The result can be a cascade of errors, and the application can get an exception when trying to consume that particular container.\n\nYou might also see this type of error on startup when the application is deploying to the cloud. In that case, orchestrators might be moving containers from one node or VM to another (that is, starting new instances) when balancing the number of containers across the cluster's nodes.\n\nThe way 'eShopOnContainers' solves those issues when starting all the containers is by using the Retry pattern illustrated earlier.", "Test the circuit breaker in eShopOnContainers": "There are a few ways you can break/open the circuit and test it with eShopOnContainers.\n\nOne option is to lower the allowed number of retries to 1 in the circuit breaker policy and redeploy the whole solution into Docker. With a single retry, there's a good chance that an HTTP request will fail during deployment, the circuit breaker will open, and you get an error.\n\nAnother option is to use custom middleware that's implemented in the Basket microservice. When this middleware is enabled, it catches all HTTP requests and returns status code 500. You can enable the middleware by making a GET request to the failing URI, like the following:\n\n[\"GET http://localhost:5103/failing This request returns the current state of the middleware. If the middleware is enabled, the request return status code 500. If the middleware is disabled, there's no response.\", 'GET http://localhost:5103/failing?enable This request enables the middleware.', 'GET http://localhost:5103/failing?disable This request disables the middleware.']\n\nFor instance, once the application is running, you can enable the middleware by making a request using the following URI in any browser. Note that the ordering microservice uses port 5103.\n\nhttp://localhost:5103/failing?enable\n\nYou can then check the status using the URI http://localhost:5103/failing, as shown in Figure 8-5.\n\nFigure 8 -5. Checking the state of the \"Failing\" ASP.NET middleware \u2013 In this case, disabled.\n\n<!-- image -->\n\nAt this point, the Basket microservice responds with status code 500 whenever you call invoke it.\n\nOnce the middleware is running, you can try making an order from the MVC web application. Because the requests fail, the circuit will open.\n\nIn the following example, you can see that the MVC web application has a catch block in the logic for placing an order. If the code catches an open-circuit exception, it shows the user a friendly message telling them to wait.\n\n```\npublic class CartController : Controller { //\u2026 public async Task<IActionResult> Index() { try { var user = _appUserParser . Parse(HttpContext . User); //Http requests using the Typed Client (Service Agent) var vm = await _basketSvc . GetBasket(user); return View(vm); } catch (BrokenCircuitException) { // Catches error when Basket.api is in circuit-opened mode HandleBrokenCircuitException(); } return View(); } private void HandleBrokenCircuitException() { TempData[\"BasketInoperativeMsg\"] = \"Basket Service is inoperative, please try later on. (Business message due to Circuit-Breaker)\"; } }\n```\n\nHere's a summary. The Retry policy tries several times to make the HTTP request and gets HTTP errors. When the number of retries reaches the maximum number set for the Circuit Breaker policy (in this case, 5), the application throws a BrokenCircuitException. The result is a friendly message, as shown in Figure 8-6.\n\nMy Cart - Microsoft.eSh X\n\n['v']\n\n\u2022 localhost:5100/Cart\n\ne\n\neSHOP\n\nonCONTAINERS\n\nBACK TO CATALOG\n\nBasket Service is inoperative, please try later on. (Business Msg Due to Circuit-Breaker)\n\n\u2022 *\n\ndemouser@microsoft.com sa\n\nFigure 8 -6. Circuit breaker returning an error to the UI\n\n<!-- image -->\n\nYou can implement different logic for when to open/break the circuit. Or you can try an HTTP request against a different back-end microservice if there's a fallback datacenter or redundant back-end system.\n\nFinally, another possibility for the CircuitBreakerPolicy is to use Isolate (which forces open and holds open the circuit) and Reset (which closes it again). These could be used to build a utility HTTP endpoint that invokes Isolate and Reset directly on the policy. Such an HTTP endpoint could also be used, suitably secured, in production for temporarily isolating a downstream system, such as when you want to upgrade it. Or it could trip the circuit manually to protect a downstream system you suspect to be faulting.", "Health monitoring": "Health monitoring can allow near-real-time information about the state of your containers and microservices. Health monitoring is critical to multiple aspects of operating microservices and is especially important when orchestrators perform partial application upgrades in phases, as explained later.\n\nMicroservices -based applications often use heartbeats or health checks to enable their performance monitors, schedulers, and orchestrators to keep track of the multitude of services. If services cannot send some sort of \"I'm alive\" signal, either on demand or on a schedule, your application might face risks when you deploy updates, or it might just detect failures too late and not be able to stop cascading failures that can end up in major outages.\n\nIn the typical model, services send reports about their status, and that information is aggregated to provide an overall view of the state of health of your application. If you're using an orchestrator, you can provide health information to your orchestrator's cluster, so that the cluster can act accordingly. If you invest in high-quality health reporting that's customized for your application, you can detect and fix issues for your running application much more easily.\n\n[]", "Implement health checks in ASP.NET Core services": "When developing an ASP.NET Core microservice or web application, you can use the built-in health checks feature that was released in ASP .NET Core 2.2\n\n(Microsoft.Extensions.Diagnostics.HealthChecks). Like many ASP.NET Core features, health checks come with a set of services and a middleware.\n\nHealth check services and middleware are easy to use and provide capabilities that let you validate if any external resource needed for your application (like a SQL Server database or a remote API) is working properly. When you use this feature, you can also decide what it means that the resource is healthy, as we explain later.\n\nTo use this feature effectively, you need to first configure services in your microservices. Second, you need a front -end application that queries for the health reports. That front-end application could be a custom reporting application, or it could be an orchestrator itself that can react accordingly to the health states.", "Use the HealthChecks feature in your back-end ASP.NET microservices": "In this section, you'll learn how to implement the HealthChecks feature in a sample ASP.NET Core 7.0 Web API application when using the Microsoft.Extensions.Diagnostics.HealthChecks package. The Implementation of this feature in a large-scale microservices like the eShopOnContainers is explained in the next section.\n\nTo begin, you need to define what constitutes a healthy status for each microservice. In the sample application, we define the microservice is healthy if its API is accessible via HTTP and its related SQL Server database is also available.\n\nIn .NET 7, with the built-in APIs, you can configure the services, add a Health Check for the microservice and its dependent SQL Server database in this way:\n\n```\n// Program.cs from .NET 7 Web API sample //... // Registers required services for health checks builder . Services . AddHealthChecks() // Add a health check for a SQL Server database . AddCheck( \"OrderingDB-check\" , new SqlConnectionHealthCheck(builder . Configuration[\"ConnectionString\"]), HealthStatus . Unhealthy , new string[] { \"orderingdb\" });\n```\n\nIn the previous code, the services.AddHealthChecks() method configures a basic HTTP check that returns a status code 200 with \"Healthy\". Further, the AddCheck() extension method configures a custom SqlConnectionHealthCheck that checks the related SQL Database's health.\n\nThe AddCheck() method adds a new health check with a specified name and the implementation of type IHealthCheck. You can add multiple Health Checks using AddCheck method, so a microservice won't provide a \"healthy\" status until all its checks are healthy.\n\nSqlConnectionHealthCheck is a custom class that implements IHealthCheck, which takes a connection string as a constructor parameter and executes a simple query to check if the connection to the SQL database is successful. It returns HealthCheckResult.Healthy() if the query was executed successfully and a FailureStatus with the actual exception when it fails.\n\n```\n// Sample SQL Connection Health Check public class SqlConnectionHealthCheck : IHealthCheck { private const string DefaultTestQuery = \"Select 1\"; public string ConnectionString { get; } public string TestQuery { get; } public SqlConnectionHealthCheck(string connectionString) : this(connectionString , testQuery: DefaultTestQuery) { } public SqlConnectionHealthCheck(string connectionString , string testQuery) { ConnectionString = connectionString ?? throw new ArgumentNullException(nameof(connectionString)); TestQuery = testQuery; } public async Task<HealthCheckResult> CheckHealthAsync(HealthCheckContext context , CancellationToken cancellationToken = default(CancellationToken)) { using (var connection = new SqlConnection(ConnectionString)) { try { await connection . OpenAsync(cancellationToken); if (TestQuery != null) { var command = connection . CreateCommand(); command . CommandText = TestQuery; await command . ExecuteNonQueryAsync(cancellationToken); } } catch (DbException ex) { return new HealthCheckResult(status: context . Registration . FailureStatus , exception: ex); } } return HealthCheckResult . Healthy(); } }\n```\n\nNote that in the previous code, Select 1 is the query used to check the Health of the database. To monitor the availability of your microservices, orchestrators like Kubernetes periodically perform health checks by sending requests to test the microservices. It's important to keep your database queries efficient so that these operations are quick and don't result in a higher utilization of resources.\n\nCatalog.API\n\n\u2022 Connected Services\n\n** Dependencies\n\ni\n\nAnalyzers\n\nFinally, add a middleware that responds to the url path /hc:\n\n```\n// Program.cs from .NET 7 Web Api sample app.MapHealthChecks(\"/hc\");\n```\n\nWhen the endpoint &lt;yourmicroservice&gt;/hc is invoked, it runs all the health checks that are configured in the AddHealthChecks() method in the Startup class and shows the result.\n\nAutofac.Extensions.Dependencylnjection (5.0.1)", "HealthChecks implementation in eShopOnContainers": "Microservices in eShopOnContainers rely on multiple services to perform its task. For example, the Catalog.API microservice from eShopOnContainers depends on many services, such as Azure Blob Storage, SQL Server, and RabbitMQ. Therefore, it has several health checks added using the AddCheck() method. For every dependent service, a custom IHealthCheck implementation that defines its respective health status would need to be added.\n\nThe open-source project AspNetCore.Diagnostics.HealthChecks solves this problem by providing custom health check implementations for each of these enterprise services, that are built on top of .NET 7. Each health check is available as an individual NuGet package that can be easily added to the project. eShopOnContainers uses them extensively in all its microservices.\n\nFor instance, in the Catalog.API microservice, the following NuGet packages were added:\n\nFigure 8 -7. Custom Health Checks implemented in Catalog.API using AspNetCore.Diagnostics.HealthChecks\n\n<!-- image -->\n\nIn the following code, the health check implementations are added for each dependent service and then the middleware is configured:\n\n```\n// Extension method from Catalog.api microservice // public static IServiceCollection AddCustomHealthCheck(this IServiceCollection services , IConfiguration configuration) { var accountName = configuration . GetValue<string>(\"AzureStorageAccountName\"); var accountKey = configuration . GetValue<string>(\"AzureStorageAccountKey\"); var hcBuilder = services . AddHealthChecks();\n```\n\nP\n\n```\nhcBuilder . AddSqlServer( configuration[\"ConnectionString\"], name: \"CatalogDB-check\" , tags: new string[] { \"catalogdb\" }); if (!string . IsNullOrEmpty(accountName) && !string . IsNullOrEmpty(accountKey)) { hcBuilder . AddAzureBlobStorage( $\"DefaultEndpointsProtocol=https;AccountName={accountName};AccountKey={accountKey};Endpoint Suffix=core.windows.net\" , name: \"catalog-storage-check\" , tags: new string[] { \"catalogstorage\" }); } if (configuration . GetValue<bool>(\"AzureServiceBusEnabled\")) { hcBuilder . AddAzureServiceBusTopic( configuration[\"EventBusConnection\"], topicName: \"eshop_event_bus\" , name: \"catalog-servicebus-check\" , tags: new string[] { \"servicebus\" }); } else { hcBuilder . AddRabbitMQ( $\"amqp://{configuration[\"EventBusConnection\"]}\" , name: \"catalog-rabbitmqbus-check\" , tags: new string[] { \"rabbitmqbus\" }); } return services; }\n```\n\nFinally, add the HealthCheck middleware to listen to \u201c/hc\u201d endpoint:\n\n```\n// HealthCheck middleware app.UseHealthChecks(\"/hc\" , new HealthCheckOptions() { Predicate = _ => true , ResponseWriter = UIResponseWriter . WriteHealthCheckUIResponse });\n```", "Query your microservices to report about their health status": "When you've configured health checks as described in this article and you have the microservice running in Docker, you can directly check from a browser if it's healthy. You have to publish the container port in the Docker host, so you can access the container through the external Docker host IP or through host.docker.internal, as shown in figure 8-8.\n\n&lt; \u2192\n\nL host.docker.internal:5101/hc\n\nA Notsecure | host.docker.internal:5101/hc\n\n\u2022 (\"status\": \"Healthy\", \"totalDuration\": \"00:00:00.0035390\", \"entries\": (\"self\": (\"data\":\n\n{), \"duration\": \"00:00:00.0000017\"\n\n0), \"duration\": \"00:00:00.0034585\".\n\n\"duration\" 0: 09:90. 0024642 status Healthy tags\": *load-Cacklegrabbiembus-sheck\": (\"data\":\n\nFigure 8 -8. Checking health status of a single service from a browser\n\n<!-- image -->\n\nIn that test, you can see that the Catalog.API microservice (running on port 5101) is healthy, returning HTTP status 200 and status information in JSON. The service also checked the health of its SQL Server database dependency and RabbitMQ, so the health check reported itself as healthy.", "Use watchdogs": "A watchdog is a separate service that can watch health and load across services, and report health about the microservices by querying with the HealthChecks library introduced earlier. This can help prevent errors that would not be detected based on the view of a single service. Watchdogs also are a good place to host code that can perform remediation actions for known conditions without user interaction.\n\nThe eShopOnContainers sample contains a web page that displays sample health check reports, as shown in Figure 8-9. This is the simplest watchdog you could have since it only shows the state of the microservices and web applications in eShopOnContainers. Usually a watchdog also takes actions when it detects unhealthy states.\n\nFortunately, AspNetCore.Diagnostics.HealthChecks also provides AspNetCore.HealthChecks.UI NuGet package that can be used to display the health check results from the configured URIs.\n\n|- Health Checks UI\n\n[]\n\n&lt; lcalhost:5107/hc-ui#/healthchecks\n\nE\n\n&amp; Health Checks\n\n\u2022 Webhooks\n\nHealth Checks status\n\n(=)\n\nRefresh every\n\nm\n\n10\n\n[]\n\na\n\nseconds\n\nChange\n\nFigure 8 -9. Sample health check report in eShopOnContainers\n\n<!-- image -->\n\nIn summary, this watchdog service queries each microservice's \"/hc\" endpoint. This will execute all the health checks defined within it and return an overall health state depending on all those checks. The HealthChecksUI is easy to consume with a few configuration entries and two lines of code that needs to be added into the Startup.cs of the watchdog service.\n\nSample configuration file for health check UI:\n\n```\n// Configuration { \"HealthChecksUI\": { \"HealthChecks\": [ { \"Name\": \"Ordering HTTP Check\" , \"Uri\": \"http://host.docker.internal:5102/hc\" } , { \"Name\": \"Ordering HTTP Background Check\" , \"Uri\": \"http://host.docker.internal:5111/hc\" } , //... ]} }\n```\n\nProgram.cs file that adds HealthChecksUI:\n\n```\n// Program.cs from WebStatus(Watch Dog) service // // Registers required services for health checks builder . Services . AddHealthChecksUI(); // build the app, register other middleware app.UseHealthChecksUI(config => config . UIPath = \"/hc-ui\");\n```", "Health checks when using orchestrators": "To monitor the availability of your microservices, orchestrators like Kubernetes and Service Fabric periodically perform health checks by sending requests to test the microservices. When an orchestrator determines that a service/container is unhealthy, it stops routing requests to that instance. It also usually creates a new instance of that container.\n\nFor instance, most orchestrators can use health checks to manage zero-downtime deployments. Only when the status of a service/container changes to healthy will the orchestrator start routing traffic to service/container instances.\n\nHealth monitoring is especially important when an orchestrator performs an application upgrade. Some orchestrators (like Azure Service Fabric) update services in phases\u2014for example, they might update one-fifth of the cluster surface for each application upgrade. The set of nodes that's upgraded at the same time is referred to as an upgrade domain. After each upgrade domain has been upgraded and is available to users, that upgrade domain must pass health checks before the deployment moves to the next upgrade domain.\n\nAnother aspect of service health is reporting metrics from the service. This is an advanced capability of the health model of some orchestrators, like Service Fabric. Metrics are important when using an orchestrator because they are used to balance resource usage. Metrics also can be an indicator of system health. For example, you might have an application that has many microservices, and each instance reports a requests-per-second (RPS) metric. If one service is using more resources (memory, processor, etc.) than another service, the orchestrator could move service instances around in the cluster to try to maintain even resource utilization.\n\nNote that Azure Service Fabric provides its own Health Monitoring model, which is more advanced than simple health checks.", "Advanced monitoring: visualization, analysis, and alerts": "The final part of monitoring is visualizing the event stream, reporting on service performance, and alerting when an issue is detected. You can use different solutions for this aspect of monitoring.\n\nYou can use simple custom applications showing the state of your services, like the custom page shown when explaining the AspNetCore.Diagnostics.HealthChecks. Or you could use more advanced tools like Azure Monitor to raise alerts based on the stream of events.\n\nFinally, if you're storing all the event streams, you can use Microsoft Power BI or other solutions like Kibana or Splunk to visualize the data.", "Make secure .NET Microservices and Web Applications": "There are so many aspects about security in microservices and web applications that the topic could easily take several books like this one. So, in this section, we'll focus on authentication, authorization, and application secrets.", "Implement authentication in .NET microservices and web applications": "It's often necessary for resources and APIs published by a service to be limited to certain trusted users or clients. The first step to making these sorts of API-level trust decisions is authentication. Authentication is the process of reliably verifying a user's identity.\n\nIn microservice scenarios, authentication is typically handled centrally. If you're using an API Gateway, the gateway is a good place to authenticate, as shown in Figure 9-1. If you use this approach, make sure that the individual microservices cannot be reached directly (without the API Gateway) unless additional security is in place to authenticate messages whether they come from the gateway or not.\n\nFigure 9 -1. Centralized authentication with an API Gateway\n\n<!-- image -->\n\nWhen the API Gateway centralizes authentication, it adds user information when forwarding requests to the microservices. If services can be accessed directly, an authentication service like Azure Active\n\nCatalog Microservice\n\nClient Apps\n\nMobile\n\nApp\n\nWeb\n\nApp\n\nBackend Microservices\n\nSign-in\n\nSecurity token\n\nIdentity Microservice (STS + Users)\n\nSQL Server database\n\nDirectory or a dedicated authentication microservice acting as a security token service (STS) can be used to authenticate users. Trust decisions are shared between services with security tokens or cookies. (These tokens can be shared between ASP.NET Core applications, if needed, by implementing cookie sharing.) This pattern is illustrated in Figure 9-2. database\n\nFigure 9 -2. Authentication by identity microservice; trust is shared using an authorization token\n\n<!-- image -->\n\nWhen microservices are accessed directly, trust, that includes authentication and authorization, is handled by a security token issued by a dedicated microservice, shared between microservices.", "Authenticate with ASP.NET Core Identity": "The primary mechanism in ASP.NET Core for identifying an application's users is the ASP.NET Core Identity membership system. ASP.NET Core Identity stores user information (including sign-in information, roles, and claims) in a data store configured by the developer. Typically, the ASP.NET Core Identity data store is an Entity Framework store provided in the\n\nMicrosoft.AspNetCore.Identity.EntityFrameworkCore package. However, custom stores or other thirdparty packages can be used to store identity information in Azure Table Storage, CosmosDB, or other locations.", "Authenticate with external providers": "ASP.NET Core also supports using external authentication providers to let users sign in via OAuth 2.0 flows. This means that users can sign in using existing authentication processes from providers like Microsoft, Google, Facebook, or Twitter and associate those identities with an ASP.NET Core identity in your application.\n\nTo use external authentication, besides including the authentication middleware as mentioned before, using the app.UseAuthentication() method, you also have to register the external provider in Program.cs as shown in the following example:\n\n```\n//... services . AddDefaultIdentity<IdentityUser>(options => options . SignIn . RequireConfirmedAccount = true) . AddEntityFrameworkStores<ApplicationDbContext>(); services . AddAuthentication() . AddMicrosoftAccount(microsoftOptions => { microsoftOptions . ClientId = builder . Configuration[\"Authentication:Microsoft:ClientId\"]; microsoftOptions . ClientSecret = builder . Configuration[\"Authentication:Microsoft:ClientSecret\"]; }) . AddGoogle(googleOptions => { ... }) . AddTwitter(twitterOptions => { ... }) . AddFacebook(facebookOptions => { ... }); //...\n```\n\nPopular external authentication providers and their associated NuGet packages are shown in the following table:\n\n| Provider   | Package                                              |\n|------------|------------------------------------------------------|\n| Microsoft  | Microsoft.AspNetCore.Authentication.MicrosoftAccount |\n| Google     | Microsoft.AspNetCore.Authentication.Google           |\n| Facebook   | Microsoft.AspNetCore.Authentication.Facebook         |\n| Twitter    | Microsoft.AspNetCore.Authentication.Twitter          |\n\nIn all cases, you must complete an application registration procedure that is vendor dependent and that usually involves:\n\n['Getting a Client Application ID.', 'Getting a Client Application Secret.', \"Configuring a redirection URL, that's handled by the authorization middleware and the registered provider\", 'Optionally, configuring a sign-out URL to properly handle sign out in a Single Sign On (SSO) scenario.']\n\nFor details on configuring your app for an external provider, see the External provider authentication in the ASP.NET Core documentation).", "Authenticate with bearer tokens": "Authenticating with ASP.NET Core Identity (or Identity plus external authentication providers) works well for many web application scenarios in which storing user information in a cookie is appropriate. In other scenarios, though, cookies are not a natural means of persisting and transmitting data.\n\nFor example, in an ASP.NET Core Web API that exposes RESTful endpoints that might be accessed by Single Page Applications (SPAs), by native clients, or even by other Web APIs, you typically want to use bearer token authentication instead. These types of applications do not work with cookies, but can easily retrieve a bearer token and include it in the authorization header of subsequent requests. To enable token authentication, ASP.NET Core supports several options for using OAuth 2.0 and OpenID Connect .", "Authenticate with an OpenID Connect or OAuth 2.0 Identity provider": "If user information is stored in Azure Active Directory or another identity solution that supports OpenID Connect or OAuth 2.0, you can use the\n\nMicrosoft.AspNetCore.Authentication.OpenIdConnect package to authenticate using the OpenID Connect workflow. For example, to authenticate to the Identity.Api microservice in eShopOnContainers, an ASP.NET Core web application can use middleware from that package as shown in the following simplified example in Program.cs:\n\n```\n// Program.cs var identityUrl = builder . Configuration . GetValue<string>(\"IdentityUrl\"); var callBackUrl = builder . Configuration . GetValue<string>(\"CallBackUrl\"); var sessionCookieLifetime = builder . Configuration . GetValue(\"SessionCookieLifetimeMinutes\" , 60); // Add Authentication services services . AddAuthentication(options => { options . DefaultScheme = CookieAuthenticationDefaults . AuthenticationScheme; options . DefaultChallengeScheme = JwtBearerDefaults . AuthenticationScheme; }) . AddCookie(setup => setup . ExpireTimeSpan = TimeSpan . FromMinutes(sessionCookieLifetime)) . AddOpenIdConnect(options => { options . SignInScheme = CookieAuthenticationDefaults . AuthenticationScheme; options . Authority = identityUrl . ToString(); options . SignedOutRedirectUri = callBackUrl . ToString(); options . ClientId = useLoadTest ? \"mvctest\" : \"mvc\"; options . ClientSecret = \"secret\"; options . ResponseType = useLoadTest ? \"code id_token token\" : \"code id_token\"; options . SaveTokens = true; options . GetClaimsFromUserInfoEndpoint = true; options . RequireHttpsMetadata = false; options . Scope . Add(\"openid\"); options . Scope . Add(\"profile\"); options . Scope . Add(\"orders\"); options . Scope . Add(\"basket\"); options . Scope . Add(\"marketing\"); options . Scope . Add(\"locations\"); options . Scope . Add(\"webshoppingagg\"); options . Scope . Add(\"orders.signalrhub\"); }); // Build the app //\u2026 app.UseAuthentication(); //\u2026 app.UseEndpoints(endpoints => { //... });\n```\n\nWhen you use this workflow, the ASP.NET Core Identity middleware is not needed, because all user information storage and authentication is handled by the Identity service.", "Issue security tokens from an ASP.NET Core service": "If you prefer to issue security tokens for local ASP.NET Core Identity users rather than using an external identity provider, you can take advantage of some good third-party libraries.\n\nIdentityServer4 and OpenIddict are OpenID Connect providers that integrate easily with ASP.NET Core Identity to let you issue security tokens from an ASP.NET Core service. The IdentityServer4 documentation has in -depth instructions for using the library. However, the basic steps to using IdentityServer4 to issue tokens are as follows.\n\n['You configure IdentityServer4 in Program.cs by making a call to builder.Services.AddIdentityServer.', \"You call app.UseIdentityServer in Program.cs to add IdentityServer4 to the application's HTTP request processing pipeline. This lets the library serve requests to OpenID Connect and OAuth2 endpoints like /connect/token.\", 'You configure identity server by setting the following data:', '\u2013 The credentials to use for signing.', '\u2013 The Identity and API resources that users might request access to:']\n\n['API resources represent protected data or functionality that a user can access with an access token. An example of an API resource would be a web API (or set of APIs) that requires authorization.', 'Identity resources represent information (claims) that are given to a client to identify a user. The claims might include the user name, email address, and so on.']\n\n['\u2013 The clients that will be connecting in order to request tokens.', '\u2013 The storage mechanism for user information, such as ASP.NET Core Identity or an alternative.']\n\nWhen you specify clients and resources for IdentityServer4 to use, you can pass an IEnumerable collection of the appropriate type to methods that take in-memory client or resource stores. Or for more complex scenarios, you can provide client or resource provider types via Dependency Injection.\n\nA sample configuration for IdentityServer4 to use in-memory resources and clients provided by a custom IClientStore type might look like the following example:\n\n```\n// Program.cs builder . Services . AddSingleton<IClientStore , CustomClientStore>(); builder . Services . AddIdentityServer() . AddSigningCredential(\"CN=sts\") . AddInMemoryApiResources(MyApiResourceProvider . GetAllResources()) . AddAspNetIdentity<ApplicationUser>(); //...\n```", "Consume security tokens": "Authenticating against an OpenID Connect endpoint or issuing your own security tokens covers some scenarios. But what about a service that simply needs to limit access to those users who have valid security tokens that were provided by a different service?\n\nFor that scenario, authentication middleware that handles JWT tokens is available in the Microsoft.AspNetCore.Authentication.JwtBearer package. JWT stands for \"JSON Web Token\" and is a common security token format (defined by RFC 7519) for communicating security claims. A simplified example of how to use middleware to consume such tokens might look like this code fragment, taken from the Ordering.Api microservice of eShopOnContainers.\n\n```\n// Program.cs var identityUrl = builder . Configuration . GetValue<string>(\"IdentityUrl\"); // Add Authentication services builder . Services . AddAuthentication(options => { options . DefaultAuthenticateScheme = AspNetCore . Authentication . JwtBearer . JwtBearerDefaults . AuthenticationScheme; options . DefaultChallengeScheme = AspNetCore . Authentication . JwtBearer . JwtBearerDefaults . AuthenticationScheme; }).AddJwtBearer(options => { options . Authority = identityUrl; options . RequireHttpsMetadata = false; options . Audience = \"orders\"; }); // Build the app app.UseAuthentication(); //\u2026 app.UseEndpoints(endpoints => { //... });\n```\n\nThe parameters in this usage are:\n\n['Audience represents the receiver of the incoming token or the resource that the token grants access to. If the value specified in this parameter does not match the parameter in the token, the token will be rejected.', \"Authority is the address of the token-issuing authentication server. The JWT bearer authentication middleware uses this URI to get the public key that can be used to validate the token's signature. The middleware also confirms that the iss parameter in the token matches this URI.\"]\n\nAnother parameter, RequireHttpsMetadata, is useful for testing purposes; you set this parameter to false so you can test in environments where you don't have certificates. In real-world deployments, JWT bearer tokens should always be passed only over HTTPS .\n\nWith this middleware in place, JWT tokens are automatically extracted from authorization headers. They are then deserialized, validated (using the values in the Audience and Authority parameters), and stored as user information to be referenced later by MVC actions or authorization filters.\n\nThe JWT bearer authentication middleware can also support more advanced scenarios, such as using a local certificate to validate a token if the authority is not available. For this scenario, you can specify a TokenValidationParameters object in the JwtBearerOptions object.", "About authorization in .NET microservices and web applications": "After authentication, ASP.NET Core Web APIs need to authorize access. This process allows a service to make APIs available to some authenticated users, but not to all. Authorization can be done based on users' roles or based on custom policy, which might include inspecting claims or other heuristics.\n\nRestricting access to an ASP.NET Core MVC route is as easy as applying an Authorize attribute to the action method (or to the controller's class if all the controller's actions require authorization), as shown in following example:\n\n```\npublic class AccountController : Controller { public ActionResult Login() { } [Authorize]\n```\n\n```\npublic ActionResult Logout() { } }\n```\n\nBy default, adding an Authorize attribute without parameters will limit access to authenticated users for that controller or action. To further restrict an API to be available for only specific users, the attribute can be expanded to specify required roles or policies that users must satisfy.", "Implement role-based authorization": "ASP.NET Core Identity has a built-in concept of roles. In addition to users, ASP.NET Core Identity stores information about different roles used by the application and keeps track of which users are assigned to which roles. These assignments can be changed programmatically with the RoleManager type that updates roles in persisted storage, and the UserManager type that can grant or revoke roles from users.\n\nIf you're authenticating with JWT bearer tokens, the ASP.NET Core JWT bearer authentication middleware will populate a user's roles based on role claims found in the token. To limit access to an MVC action or controller to users in specific roles, you can include a Roles parameter in the Authorize annotation (attribute), as shown in the following code fragment:\n\n```\n[Authorize(Roles = \"Administrator, PowerUser\")] public class ControlPanelController : Controller { public ActionResult SetTime() { } [Authorize(Roles = \"Administrator\")] public ActionResult ShutDown() { } }\n```\n\nIn this example, only users in the Administrator or PowerUser roles can access APIs in the ControlPanel controller (such as executing the SetTime action). The ShutDown API is further restricted to allow access only to users in the Administrator role.\n\nTo require a user be in multiple roles, you use multiple Authorize attributes, as shown in the following example:\n\n```\n[Authorize(Roles = \"Administrator, PowerUser\")] [Authorize(Roles = \"RemoteEmployee \")] [Authorize(Policy = \"CustomPolicy\")] public ActionResult API1 () { }\n```\n\nIn this example, to call API1, a user must:\n\n['Be in the Administrator or PowerUser role, and', 'Be in the RemoteEmployee role, and', 'Satisfy a custom handler for CustomPolicy authorization.']", "Implement policy-based authorization": "Custom authorization rules can also be written using authorization policies. This section provides an overview. For more information, see the ASP.NET Authorization Workshop .\n\nCustom authorization policies are registered in the Startup.ConfigureServices method using the service.AddAuthorization method. This method takes a delegate that configures an AuthorizationOptions argument.\n\n```\nservices . AddAuthorization(options => { options . AddPolicy(\"AdministratorsOnly\" , policy => policy . RequireRole(\"Administrator\")); options . AddPolicy(\"EmployeesOnly\" , policy => policy . RequireClaim(\"EmployeeNumber\")); options . AddPolicy(\"Over21\" , policy => policy . Requirements . Add(new MinimumAgeRequirement(21))); });\n```\n\nAs shown in the example, policies can be associated with different types of requirements. After the policies are registered, they can be applied to an action or controller by passing the policy's name as the Policy argument of the Authorize attribute (for example, [Authorize(Policy=\"EmployeesOnly\")]) Policies can have multiple requirements, not just one (as shown in these examples).\n\nIn the previous example, the first AddPolicy call is just an alternative way of authorizing by role. If [Authorize(Policy=\"AdministratorsOnly\")] is applied to an API, only users in the Administrator role will be able to access it.\n\nThe second AddPolicy call demonstrates an easy way to require that a particular claim should be present for the user. The RequireClaim method also optionally takes expected values for the claim. If values are specified, the requirement is met only if the user has both a claim of the correct type and one of the specified values. If you're using the JWT bearer authentication middleware, all JWT properties will be available as user claims.\n\nThe most interesting policy shown here is in the third AddPolicy method, because it uses a custom authorization requirement. By using custom authorization requirements, you can have a great deal of control over how authorization is performed. For this to work, you must implement these types:\n\n['A Requirements type that derives from IAuthorizationRequirement and that contains fields specifying the details of the requirement. In the example, this is an age field for the sample MinimumAgeRequirement type.', 'A handler that implements AuthorizationHandler, where T is the type of IAuthorizationRequirement that the handler can satisfy. The handler must implement the HandleRequirementAsync method, which checks whether a specified context that contains information about the user satisfies the requirement.']\n\nIf the user meets the requirement, a call to context.Succeed will indicate that the user is authorized. If there are multiple ways that a user might satisfy an authorization requirement, multiple handlers can be created.\n\nIn addition to registering custom policy requirements with AddPolicy calls, you also need to register custom requirement handlers via Dependency Injection (services.AddTransient&lt;IAuthorizationHandler, MinimumAgeHandler&gt;()).\n\nAn example of a custom authorization requirement and handler for checking a user's age (based on a DateOfBirth claim) is available in the ASP.NET Core authorization documentation .", "Authorization and minimal apis": "ASP.NET supports minimal APIs as an alternative to controller-based APIs. Authorization policies are the recommended way to configure authorization for minimal APIs, as this example demonstrates:\n\n```\n// Program.cs builder . Services . AddAuthorizationBuilder() . AddPolicy(\"admin_greetings\" , policy => policy . RequireRole(\"admin\") . RequireScope(\"greetings_api\")); // build the app app.MapGet(\"/hello\" , () => \"Hello world!\") . RequireAuthorization(\"admin_greetings\");\n```", "Store application secrets safely during development": "To connect with protected resources and other services, ASP.NET Core applications typically need to use connection strings, passwords, or other credentials that contain sensitive information. These sensitive pieces of information are called secrets. It's a best practice to not include secrets in source\n\ncode and making sure not to store secrets in source control. Instead, you should use the ASP.NET Core configuration model to read the secrets from more secure locations.\n\nYou must separate the secrets for accessing development and staging resources from the ones used for accessing production resources, because different individuals will need access to those different sets of secrets. To store secrets used during development, common approaches are to either store secrets in environment variables or by using the ASP.NET Core Secret Manager tool. For more secure storage in production environments, microservices can store secrets in an Azure Key Vault.", "Store secrets in environment variables": "One way to keep secrets out of source code is for developers to set string-based secrets as environment variables on their development machines. When you use environment variables to store secrets with hierarchical names, such as the ones nested in configuration sections, you must name the variables to include the complete hierarchy of its sections, delimited with colons (:).\n\nFor example, setting an environment variable Logging:LogLevel:Default to Debug value would be equivalent to a configuration value from the following JSON file:\n\n```\n{ \"Logging\": { \"LogLevel\": { \"Default\": \"Debug\" } } }\n```\n\nTo access these values from environment variables, the application just needs to call AddEnvironmentVariables on its ConfigurationBuilder when constructing an IConfigurationRoot object.", "Store secrets with the ASP.NET Core Secret Manager": "The ASP.NET Core Secret Manager tool provides another method of keeping secrets out of source code during development. To use the Secret Manager tool, install the package Microsoft.Extensions.Configuration.SecretManager in your project file. Once that dependency is present and has been restored, the dotnet user-secrets command can be used to set the value of secrets from the command line. These secrets will be stored in a JSON file in the user's profile directory (details vary by OS), away from source code.\n\nSecrets set by the Secret Manager tool are organized by the UserSecretsId property of the project that's using the secrets. Therefore, you must be sure to set the UserSecretsId property in your project file, as shown in the snippet below. The default value is a GUID assigned by Visual Studio, but the actual string is not important as long as it's unique in your computer.\n\n&lt;PropertyGroup&gt; &lt;UserSecretsId&gt;UniqueIdentifyingString&lt;/UserSecretsId&gt; &lt;/PropertyGroup&gt;\n\nUsing secrets stored with Secret Manager in an application is accomplished by calling AddUserSecrets&lt;T&gt; on the ConfigurationBuilder instance to include secrets for the application in its configuration. The generic parameter T should be a type from the assembly that the UserSecretId was applied to. Usually, using AddUserSecrets&lt;Startup&gt; is fine.\n\nThe AddUserSecrets&lt;Startup&gt;() is included in the default options for the Development environment when using the CreateDefaultBuilder method in Program.cs .", "Use Azure Key Vault to protect secrets at production time": "Secrets stored as environment variables or stored by the Secret Manager tool are still stored locally and unencrypted on the machine. A more secure option for storing secrets is Azure Key Vault, which provides a secure, central location for storing keys and secrets.\n\nThe Azure.Extensions.AspNetCore.Configuration.Secrets package allows an ASP.NET Core application to read configuration information from Azure Key Vault. To start using secrets from an Azure Key Vault, you follow these steps:\n\n['Register your application as an Azure AD application. (Access to key vaults is managed by Azure AD.) This can be done through the Azure management portal.']\n\nAlternatively, if you want your application to authenticate using a certificate instead of a password or client secret, you can use the New-AzADApplication PowerShell cmdlet. The certificate that you register with Azure Key Vault needs only your public key. Your application will use the private key.\n\n['Give the registered application access to the key vault by creating a new service principal. You can do this using the following PowerShell commands:']\n\n$sp = New-AzADServicePrincipal -ApplicationId \"&lt;Application ID guid&gt;\" Set -AzKeyVaultAccessPolicy -VaultName \"&lt;VaultName&gt;\" -ServicePrincipalName $sp . ServicePrincipalNames[0] -PermissionsToSecrets all -ResourceGroupName \"&lt;KeyVault Resource Group&gt;\"\n\n['Include the key vault as a configuration source in your application by calling the AzureKeyVaultConfigurationExtensions.AddAzureKeyVault extension method when you create an IConfigurationRoot instance.']\n\nNote that calling AddAzureKeyVault requires the application ID that was registered and given access to the key vault in the previous steps. Or you can firstly running the Azure CLI command: az login, then using an overload of AddAzureKeyVault that takes a DefaultAzureCredential in place of the client.", ".NET Microservices Architecture key takeaways": "As a summary and key takeaways, the following are the most important conclusions from this guide.\n\nBenefits of using containers. Container-based solutions provide important cost savings because they help reduce deployment problems caused by failing dependencies in production environments. Containers significantly improve DevOps and production operations.\n\nContainers will be ubiquitous. Docker-based containers are becoming the de facto standard in the industry, supported by key vendors in the Windows and Linux ecosystems, such as Microsoft, Amazon AWS, Google, and IBM. Docker will probably soon be ubiquitous in both the cloud and on-premises datacenters.\n\nContainers as a unit of deployment. A Docker container is becoming the standard unit of deployment for any server-based application or service.\n\nMicroservices. The microservices architecture is becoming the preferred approach for distributed and large or complex mission-critical applications based on many independent subsystems in the form of autonomous services. In a microservice -based architecture, the application is built as a collection of services that are developed, tested, versioned, deployed, and scaled independently. Each service can include any related autonomous database.\n\nDomain -driven design and SOA. The microservices architecture patterns derive from serviceoriented architecture (SOA) and domain-driven design (DDD). When you design and develop microservices for environments with evolving business needs and rules, it's important to consider DDD approaches and patterns.\n\nMicroservices challenges. Microservices offer many powerful capabilities, like independent deployment, strong subsystem boundaries, and technology diversity. However, they also raise many new challenges related to distributed application development, such as fragmented and independent data models, resilient communication between microservices, eventual consistency, and operational complexity that results from aggregating logging and monitoring information from multiple microservices. These aspects introduce a much higher complexity level than a traditional monolithic application. As a result, only specific scenarios are suitable for microservice-based applications. These include large and complex applications with multiple evolving subsystems. In these cases, it's worth investing in a more complex software architecture, because it will provide better long-term agility and application maintenance.\n\nContainers for any application. Containers are convenient for microservices, but can also be useful for monolithic applications based on the traditional .NET Framework, when using Windows Containers. The benefits of using Docker, such as solving many deployment-to-production issues and providing state-of-the-art Dev and Test environments, apply to many different types of applications.\n\nCLI versus IDE. With Microsoft tools, you can develop containerized .NET applications using your preferred approach. You can develop with a CLI and an editor-based environment by using the Docker CLI and Visual Studio Code. Or you can use an IDE-focused approach with Visual Studio and its unique features for Docker, such as multi -container debugging.\n\nResilient cloud applications. In cloud-based systems and distributed systems in general, there is always the risk of partial failure. Since clients and services are separate processes (containers), a service might not be able to respond in a timely way to a client's request. For example, a service might be down because of a partial failure or for maintenance; the service might be overloaded and responding slowly to requests; or it might not be accessible for a short time because of network issues. Therefore, a cloud -based application must embrace those failures and have a strategy in place to respond to those failures. These strategies can include retry policies (resending messages or retrying requests) and implementing circuit-breaker patterns to avoid exponential load of repeated requests. Basically, cloud-based applications must have resilient mechanisms\u2014either based on cloud infrastructure or custom, as the high-level ones provided by orchestrators or service buses.\n\nSecurity. Our modern world of containers and microservices can expose new vulnerabilities. There are several ways to implement basic application security, based on authentication and authorization. However, container security must consider additional key components that result in inherently safer applications. A critical element of building safer apps is having a secure way of communicating with other apps and systems, something that often requires credentials, tokens, passwords, and the like, commonly referred to as application secrets. Any secure solution must follow security best practices, such as encrypting secrets while in transit and at rest, and preventing secrets from leaking when consumed by the final application. Those secrets need to be stored and kept safely, as when using Azure Key Vault.\n\nOrchestrators. Container -based orchestrators, such as Azure Kubernetes Service and Azure Service Fabric are key part of any significant microservice and container-based application. These applications carry with them high complexity, scalability needs, and go through constant evolution. This guide has introduced orchestrators and their role in microservice -based and container -based solutions. If your application needs are moving you toward complex containerized apps, you will find it useful to seek out additional resources for learning more about orchestrators."}}