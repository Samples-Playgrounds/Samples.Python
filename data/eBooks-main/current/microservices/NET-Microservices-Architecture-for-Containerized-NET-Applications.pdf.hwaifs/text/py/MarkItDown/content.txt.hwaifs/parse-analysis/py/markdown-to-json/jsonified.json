{"docker-compose.override.yml": "", "": "catalog-api:\nenvironment:\n-\nConnectionString=Server=sqldata;Database=Microsoft.eShopOnContainers.Services.CatalogDb;Use\nr Id=sa;Password=[PLACEHOLDER]\n# Additional environment variables for this service\nports:\n- \"5101:80\"\n\n109\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThe docker-compose.yml files at the solution level are not only more flexible than configuration files\nat the project or microservice level, but also more secure if you override the environment variables\ndeclared at the docker-compose files with values set from your deployment tools, like from Azure\nDevOps Services Docker deployment tasks.\n\nFinally, you can get that value from your code by using builder.Configuration\\[\"ConnectionString\"\\], as\nshown in an earlier code example.\n\nHowever, for production environments, you might want to explore additional ways on how to store\nsecrets like the connection strings. An excellent way to manage application secrets is using Azure Key\nVault.\n\nAzure Key Vault helps to store and safeguard cryptographic keys and secrets used by your cloud\napplications and services. A secret is anything you want to keep strict control of, like API keys,\nconnection strings, passwords, etc. and strict control includes usage logging, setting expiration,\nmanaging access, among others.\n\nAzure Key Vault allows a detailed control level of the application secrets usage without the need to let\nanyone know them. The secrets can even be rotated for enhanced security without disrupting\ndevelopment or operations.\n\nApplications have to be registered in the organization\u2019s Active Directory, so they can use the Key\nVault.\n\nYou can check the Key Vault Concepts documentation for more details.\n\nImplementing versioning in ASP.NET Web APIs\n\nAs business requirements change, new collections of resources may be added, the relationships\nbetween resources might change, and the structure of the data in resources might be amended.\nUpdating a Web API to handle new requirements is a relatively straightforward process, but you must\nconsider the effects that such changes will have on client applications consuming the Web API.\nAlthough the developer designing and implementing a Web API has full control over that API, the\ndeveloper does not have the same degree of control over client applications that might be built by\nthird-party organizations operating remotely.\n\nVersioning enables a Web API to indicate the features and resources that it exposes. A client\napplication can then submit requests to a specific version of a feature or resource. There are several\napproaches to implement versioning:\n\n\u2022\n\n\u2022\n\n\u2022\n\nURI versioning\n\nQuery string versioning\n\nHeader versioning\n\nQuery string and URI versioning are the simplest to implement. Header versioning is a good\napproach. However, header versioning is not as explicit and straightforward as URI versioning.\nBecause URL versioning is the simplest and most explicit, the eShopOnContainers sample application\nuses URI versioning.\n\n110\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fWith URI versioning, as in the eShopOnContainers sample application, each time you modify the Web\nAPI or change the schema of resources, you add a version number to the URI for each resource.\nExisting URIs should continue to operate as before, returning resources that conform to the schema\nthat matches the requested version.\n\nAs shown in the following code example, the version can be set by using the Route attribute in the\nWeb API controller, which makes the version explicit in the URI (v1 in this case).\n\n[Route(\"api/v1/[controller]\")]\npublic class CatalogController : ControllerBase\n{\n// Implementation ...\n\nThis versioning mechanism is simple and depends on the server routing the request to the\nappropriate endpoint. However, for a more sophisticated versioning and the best method when using\nREST, you should use hypermedia and implement HATEOAS (Hypertext as the Engine of Application\nState).\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nASP.NET API Versioning  https://github.com/dotnet/aspnet-api-versioning\n\nScott Hanselman. ASP.NET Core RESTful Web API versioning made easy\nhttps://www.hanselman.com/blog/ASPNETCoreRESTfulWebAPIVersioningMadeEasy.aspx\n\nVersioning a RESTful web API\nhttps://learn.microsoft.com/azure/architecture/best-practices/api-design#versioning-a-\nrestful-web-api\n\nRoy Fielding. Versioning, Hypermedia, and REST\nhttps://www.infoq.com/articles/roy-fielding-on-versioning\n\nGenerating Swagger description metadata from your ASP.NET Core\nWeb API\n\nSwagger is a commonly used open source framework backed by a large ecosystem of tools that helps\nyou design, build, document, and consume your RESTful APIs. It is becoming the standard for the APIs\ndescription metadata domain. You should include Swagger description metadata with any kind of\nmicroservice, either data-driven microservices or more advanced domain-driven microservices (as\nexplained in the following section).\n\nThe heart of Swagger is the Swagger specification, which is API description metadata in a JSON or\nYAML file. The specification creates the RESTful contract for your API, detailing all its resources and\noperations in both a human- and machine-readable format for easy development, discovery, and\nintegration.\n\nThe specification is the basis of the OpenAPI Specification (OAS) and is developed in an open,\ntransparent, and collaborative community to standardize the way RESTful interfaces are defined.\n\n111\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThe specification defines the structure for how a service can be discovered and how its capabilities\nunderstood. For more information, including a web editor and examples of Swagger specifications\nfrom companies like Spotify, Uber, Slack, and Microsoft, see the Swagger site (https://swagger.io).\n\nWhy use Swagger?\n\nThe main reasons to generate Swagger metadata for your APIs are the following.\n\nAbility for other products to automatically consume and integrate your APIs. Dozens of products\nand commercial tools and many libraries and frameworks support Swagger. Microsoft has high-level\nproducts and tools that can automatically consume Swagger-based APIs, such as the following:\n\n\u2022\n\nAutoRest. You can automatically generate .NET client classes for calling Swagger. This tool can\nbe used from the CLI and it also integrates with Visual Studio for easy use through the GUI.\n\n\u2022  Microsoft Flow. You can automatically use and integrate your API into a high-level Microsoft\n\nFlow workflow, with no programming skills required.\n\n\u2022  Microsoft PowerApps. You can automatically consume your API from PowerApps mobile apps\n\nbuilt with PowerApps Studio, with no programming skills required.\n\n\u2022\n\nAzure App Service Logic Apps. You can automatically use and integrate your API into an Azure\nApp Service Logic App, with no programming skills required.\n\nAbility to automatically generate API documentation. When you create large-scale RESTful APIs,\nsuch as complex microservice-based applications, you need to handle many endpoints with different\ndata models used in the request and response payloads. Having proper documentation and having a\nsolid API explorer, as you get with Swagger, is key for the success of your API and adoption by\ndevelopers.\n\nSwagger\u2019s metadata is what Microsoft Flow, PowerApps, and Azure Logic Apps use to understand how\nto use APIs and connect to them.\n\nThere are several options to automate Swagger metadata generation for ASP.NET Core REST API\napplications, in the form of functional API help pages, based on swagger-ui.\n\nProbably the best know is Swashbuckle, which is currently used in eShopOnContainers and we\u2019ll cover\nin some detail in this guide but there\u2019s also the option to use NSwag, which can generate Typescript\nand C# API clients, as well as C# controllers, from a Swagger or OpenAPI specification and even by\nscanning the .dll that contains the controllers, using NSwagStudio.\n\nHow to automate API Swagger metadata generation with the Swashbuckle NuGet\npackage\n\nGenerating Swagger metadata manually (in a JSON or YAML file) can be tedious work. However, you\ncan automate API discovery of ASP.NET Web API services by using the Swashbuckle NuGet package to\ndynamically generate Swagger API metadata.\n\nSwashbuckle automatically generates Swagger metadata for your ASP.NET Web API projects. It\nsupports ASP.NET Core Web API projects and the traditional ASP.NET Web API and any other flavor,\n\n112\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fsuch as Azure API App, Azure Mobile App, Azure Service Fabric microservices based on ASP.NET. It\nalso supports plain Web API deployed on containers, as in for the reference application.\n\nSwashbuckle combines API Explorer and Swagger or swagger-ui to provide a rich discovery and\ndocumentation experience for your API consumers. In addition to its Swagger metadata generator\nengine, Swashbuckle also contains an embedded version of swagger-ui, which it will automatically\nserve up once Swashbuckle is installed.\n\nThis means you can complement your API with a nice discovery UI to help developers to use your API.\nIt requires a small amount of code and maintenance because it is automatically generated, allowing\nyou to focus on building your API. The result for the API Explorer looks like Figure 6-8.\n\nFigure 6-8. Swashbuckle API Explorer based on Swagger metadata\u2014eShopOnContainers catalog microservice\n\nThe Swashbuckle generated Swagger UI API documentation includes all published actions. The API\nexplorer is not the most important thing here. Once you have a Web API that can describe itself in\nSwagger metadata, your API can be used seamlessly from Swagger-based tools, including client\nproxy-class code generators that can target many platforms. For example, as mentioned, AutoRest\nautomatically generates .NET client classes. But additional tools like swagger-codegen are also\navailable, which allow code generation of API client libraries, server stubs, and documentation\nautomatically.\n\nCurrently, Swashbuckle consists of five internal NuGet packages under the high-level metapackage\nSwashbuckle.AspNetCore for ASP.NET Core applications.\n\nAfter you have installed these NuGet packages in your Web API project, you need to configure\nSwagger in the Program.cs class, as in the following simplified code:\n\n// Add framework services.\n\nbuilder.Services.AddSwaggerGen(options =>\n{\n\n113\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f    options.DescribeAllEnumsAsStrings();\noptions.SwaggerDoc(\"v1\", new OpenApiInfo\n{\nTitle = \"eShopOnContainers - Catalog HTTP API\",\nVersion = \"v1\",\nDescription = \"The Catalog Microservice HTTP API. This is a Data-Driven/CRUD\nmicroservice sample\"\n});\n});\n\n// Other startup code...\n\napp.UseSwagger()\n.UseSwaggerUI(c =>\n{\nc.SwaggerEndpoint(\"/swagger/v1/swagger.json\", \"My API V1\");\n});\n```\n:::\n\nOnce this is done, you can start your application and browse the following Swagger JSON and\nUI endpoints using URLs like these:\n\n:::{custom-style=CodeBox}\n\n```\n  http://<your-root-url>/swagger/v1/swagger.json\n\n  http://<your-root-url>/swagger/\n\nYou previously saw the generated UI created by Swashbuckle for a URL like http://<your-root-\nurl>/swagger. In Figure 6-9, you can also see how you can test any API method.\n\n114\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-9. Swashbuckle UI testing the Catalog/Items API method\n\nThe Swagger UI API detail shows a sample of the response and can be used to execute the real API,\nwhich is great for developer discovery. Figure 6-10 shows the Swagger JSON metadata generated\nfrom the eShopOnContainers microservice (which is what the tools use underneath) when you request\nhttp://<your-root-url>/swagger/v1/swagger.json using Postman.\n\n115\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-10. Swagger JSON metadata\n\nIt is that simple. And because it is automatically generated, the Swagger metadata will grow when you\nadd more functionality to your API.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nASP.NET Web API Help Pages using Swagger\nhttps://learn.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger\n\nGet started with Swashbuckle and ASP.NET Core\nhttps://learn.microsoft.com/aspnet/core/tutorials/getting-started-with-swashbuckle\n\nGet started with NSwag and ASP.NET Core\nhttps://learn.microsoft.com/aspnet/core/tutorials/getting-started-with-nswag\n\nDefining your multi-container application with\ndocker-compose.yml\n\nIn this guide, the docker-compose.yml file was introduced in the section Step 4. Define your services\nin docker-compose.yml when building a multi-container Docker application. However, there are\nadditional ways to use the docker-compose files that are worth exploring in further detail.\n\nFor example, you can explicitly describe how you want to deploy your multi-container application in\nthe docker-compose.yml file. Optionally, you can also describe how you are going to build your\ncustom Docker images. (Custom Docker images can also be built with the Docker CLI.)\n\n116\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fBasically, you define each of the containers you want to deploy plus certain characteristics for each\ncontainer deployment. Once you have a multi-container deployment description file, you can deploy\nthe whole solution in a single action orchestrated by the docker-compose up CLI command, or you\ncan deploy it transparently from Visual Studio. Otherwise, you would need to use the Docker CLI to\ndeploy container-by-container in multiple steps by using the docker run command from the\ncommand line. Therefore, each service defined in docker-compose.yml must specify exactly one\nimage or build. Other keys are optional, and are analogous to their docker run command-line\ncounterparts.\n\nThe following YAML code is the definition of a possible global but single docker-compose.yml file for\nthe eShopOnContainers sample. This code is not the actual docker-compose file from\neShopOnContainers. Instead, it is a simplified and consolidated version in a single file, which is not the\nbest way to work with docker-compose files, as will be explained later.\n\nversion: '3.4'\n\nservices:\n  webmvc:\n    image: eshop/webmvc\n    environment:\n      - CatalogUrl=http://catalog-api\n      - OrderingUrl=http://ordering-api\n      - BasketUrl=http://basket-api\n    ports:\n      - \"5100:80\"\n    depends_on:\n      - catalog-api\n      - ordering-api\n      - basket-api\n\n  catalog-api:\n    image: eshop/catalog-api\n    environment:\n      - ConnectionString=Server=sqldata;Initial Catalog=CatalogData;User\nId=sa;Password=[PLACEHOLDER]\n    expose:\n      - \"80\"\n    ports:\n      - \"5101:80\"\n    #extra hosts can be used for standalone SQL Server or services at the dev PC\n    extra_hosts:\n      - \"CESARDLSURFBOOK:10.0.75.1\"\n    depends_on:\n      - sqldata\n\n  ordering-api:\n    image: eshop/ordering-api\n    environment:\n      - ConnectionString=Server=sqldata;Database=Services.OrderingDb;User\nId=sa;Password=[PLACEHOLDER]\n    ports:\n      - \"5102:80\"\n    #extra hosts can be used for standalone SQL Server or services at the dev PC\n    extra_hosts:\n      - \"CESARDLSURFBOOK:10.0.75.1\"\n    depends_on:\n      - sqldata\n\n117\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f  basket-api:\n    image: eshop/basket-api\n    environment:\n      - ConnectionString=sqldata\n    ports:\n      - \"5103:80\"\n    depends_on:\n      - sqldata\n\n  sqldata:\n    environment:\n      - SA_PASSWORD=[PLACEHOLDER]\n      - ACCEPT_EULA=Y\n    ports:\n      - \"5434:1433\"\n\n  basketdata:\n    image: redis\n\nThe root key in this file is services. Under that key, you define the services you want to deploy and run\nwhen you execute the docker-compose up command or when you deploy from Visual Studio by using\nthis docker-compose.yml file. In this case, the docker-compose.yml file has multiple services defined,\nas described in the following table.\n\nService name\n\nDescription\n\nwebmvc\n\ncatalog-api\n\nordering-api\n\nsqldata\n\nbasket-api\n\nbasketdata\n\nContainer including the ASP.NET Core MVC\napplication consuming the microservices from\nserver-side C#\n\nContainer including the Catalog ASP.NET Core\nWeb API microservice\n\nContainer including the Ordering ASP.NET\nCore Web API microservice\n\nContainer running SQL Server for Linux,\nholding the microservices databases\n\nContainer with the Basket ASP.NET Core Web\nAPI microservice\n\nContainer running the REDIS cache service,\nwith the basket database as a REDIS cache\n\nA simple Web Service API container\n\nFocusing on a single container, the catalog-api container-microservice has a straightforward\ndefinition:\n\n  catalog-api:\n    image: eshop/catalog-api\n    environment:\n      - ConnectionString=Server=sqldata;Initial Catalog=CatalogData;User\nId=sa;Password=[PLACEHOLDER]\n    expose:\n\n118\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f      - \"80\"\n    ports:\n      - \"5101:80\"\n    #extra hosts can be used for standalone SQL Server or services at the dev PC\n    extra_hosts:\n      - \"CESARDLSURFBOOK:10.0.75.1\"\n    depends_on:\n      - sqldata\n\nThis containerized service has the following basic configuration:\n\n\u2022\n\n\u2022\n\n\u2022\n\nIt is based on the custom eshop/catalog-api image. For simplicity\u2019s sake, there is no\nbuild: key setting in the file. This means that the image must have been previously built (with\ndocker build) or have been downloaded (with the docker pull command) from any Docker\nregistry.\n\nIt defines an environment variable named ConnectionString with the connection string to be\nused by Entity Framework to access the SQL Server instance that contains the catalog data\nmodel. In this case, the same SQL Server container is holding multiple databases. Therefore,\nyou need less memory in your development machine for Docker. However, you could also\ndeploy one SQL Server container for each microservice database.\n\nThe SQL Server name is sqldata, which is the same name used for the container that is\nrunning the SQL Server instance for Linux. This is convenient; being able to use this name\nresolution (internal to the Docker host) will resolve the network address so you don\u2019t need to\nknow the internal IP for the containers you are accessing from other containers.\n\nBecause the connection string is defined by an environment variable, you could set that variable\nthrough a different mechanism and at a different time. For example, you could set a different\nconnection string when deploying to production in the final hosts, or by doing it from your CI/CD\npipelines in Azure DevOps Services or your preferred DevOps system.\n\n\u2022\n\n\u2022\n\n\u2022\n\nIt exposes port 80 for internal access to the catalog-api service within the Docker host. The\nhost is currently a Linux VM because it is based on a Docker image for Linux, but you could\nconfigure the container to run on a Windows image instead.\n\nIt forwards the exposed port 80 on the container to port 5101 on the Docker host machine\n(the Linux VM).\n\nIt links the web service to the sqldata service (the SQL Server instance for Linux database\nrunning in a container). When you specify this dependency, the catalog-api container will not\nstart until the sqldata container has already started; this aspect is important because catalog-\napi needs to have the SQL Server database up and running first. However, this kind of\ncontainer dependency is not enough in many cases, because Docker checks only at the\ncontainer level. Sometimes the service (in this case SQL Server) might still not be ready, so it is\nadvisable to implement retry logic with exponential backoff in your client microservices. That\nway, if a dependency container is not ready for a short time, the application will still be\nresilient.\n\n\u2022\n\nIt is configured to allow access to external servers: the extra_hosts setting allows you to access\nexternal servers or machines outside of the Docker host (that is, outside the default Linux VM,\n\n119\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fwhich is a development Docker host), such as a local SQL Server instance on your\ndevelopment PC.\n\nThere are also other, more advanced docker-compose.yml settings that we\u2019ll discuss in the following\nsections.\n\nUsing docker-compose files to target multiple environments\n\nThe docker-compose.*.yml files are definition files and can be used by multiple infrastructures that\nunderstand that format. The most straightforward tool is the docker-compose command.\n\nTherefore, by using the docker-compose command you can target the following main scenarios.\n\nDevelopment environments\n\nWhen you develop applications, it is important to be able to run an application in an isolated\ndevelopment environment. You can use the docker-compose CLI command to create that\nenvironment or Visual Studio, which uses docker-compose under the covers.\n\nThe docker-compose.yml file allows you to configure and document all your application\u2019s service\ndependencies (other services, cache, databases, queues, etc.). Using the docker-compose CLI\ncommand, you can create and start one or more containers for each dependency with a single\ncommand (docker-compose up).\n\nThe docker-compose.yml files are configuration files interpreted by Docker engine but also serve as\nconvenient documentation files about the composition of your multi-container application.\n\nTesting environments\n\nAn important part of any continuous deployment (CD) or continuous integration (CI) process are the\nunit tests and integration tests. These automated tests require an isolated environment so they are\nnot impacted by the users or any other change in the application\u2019s data.\n\nWith Docker Compose, you can create and destroy that isolated environment very easily in a few\ncommands from your command prompt or scripts, like the following commands:\n\ndocker-compose -f docker-compose.yml -f docker-compose-test.override.yml up -d\n./run_unit_tests\ndocker-compose -f docker-compose.yml -f docker-compose-test.override.yml down\n\nProduction deployments\n\nYou can also use Compose to deploy to a remote Docker Engine. A typical case is to deploy to a\nsingle Docker host instance (like a production VM or server provisioned with Docker Machine).\n\nIf you are using any other orchestrator (Azure Service Fabric, Kubernetes, etc.), you might need to add\nsetup and metadata configuration settings like those in docker-compose.yml, but in the format\nrequired by the other orchestrator.\n\nIn any case, docker-compose is a convenient tool and metadata format for development, testing and\nproduction workflows, although the production workflow might vary on the orchestrator you are\nusing.\n\n120\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fUsing multiple docker-compose files to handle several environments\n\nWhen targeting different environments, you should use multiple compose files. This approach lets you\ncreate multiple configuration variants depending on the environment.\n\nOverriding the base docker-compose file\n\nYou could use a single docker-compose.yml file as in the simplified examples shown in previous\nsections. However, that is not recommended for most applications.\n\nBy default, Compose reads two files, a docker-compose.yml and an optional docker-\ncompose.override.yml file. As shown in Figure 6-11, when you are using Visual Studio and enabling\nDocker support, Visual Studio also creates an additional docker-compose.vs.debug.g.yml file for\ndebugging the application, you can take a look at this file in folder obj\\Docker\\ in the main solution\nfolder.\n\nFigure 6-11. docker-compose files in Visual Studio 2019\n\ndocker-compose project file structure:\n\n\u2022\n\n\u2022\n\n\u2022\n\n.dockerignore - used to ignore files\n\ndocker-compose.yml - used to compose microservices\n\ndocker-compose.override.yml - used to configure microservices environment\n\nYou can edit the docker-compose files with any editor, like Visual Studio Code or Sublime, and run the\napplication with the docker-compose up command.\n\nBy convention, the docker-compose.yml file contains your base configuration and other static\nsettings. That means that the service configuration should not change depending on the deployment\nenvironment you are targeting.\n\nThe docker-compose.override.yml file, as its name suggests, contains configuration settings that\noverride the base configuration, such as configuration that depends on the deployment environment.\nYou can have multiple override files with different names also. The override files usually contain\nadditional information needed by the application but specific to an environment or to a deployment.\n\nTargeting multiple environments\n\nA typical use case is when you define multiple compose files so you can target multiple environments,\nlike production, staging, CI, or development. To support these differences, you can split your\nCompose configuration into multiple files, as shown in Figure 6-12.\n\n121\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-12. Multiple docker-compose files overriding values in the base docker-compose.yml file\n\nYou can combine multiple docker-compose*.yml files to handle different environments. You start with\nthe base docker-compose.yml file. This base file contains the base or static configuration settings that\ndo not change depending on the environment. For example, the eShopOnContainers app has the\nfollowing docker-compose.yml file (simplified with fewer services) as the base file.\n\n#docker-compose.yml (Base)\nversion: '3.4'\nservices:\n  basket-api:\n    image: eshop/basket-api:${TAG:-latest}\n    build:\n      context: .\n      dockerfile: src/Services/Basket/Basket.API/Dockerfile\n    depends_on:\n      - basketdata\n      - identity-api\n      - rabbitmq\n\n  catalog-api:\n    image: eshop/catalog-api:${TAG:-latest}\n    build:\n      context: .\n      dockerfile: src/Services/Catalog/Catalog.API/Dockerfile\n    depends_on:\n      - sqldata\n      - rabbitmq\n\n  marketing-api:\n    image: eshop/marketing-api:${TAG:-latest}\n    build:\n      context: .\n      dockerfile: src/Services/Marketing/Marketing.API/Dockerfile\n    depends_on:\n      - sqldata\n      - nosqldata\n      - identity-api\n      - rabbitmq\n\n  webmvc:\n    image: eshop/webmvc:${TAG:-latest}\n\n122\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f    build:\n      context: .\n      dockerfile: src/Web/WebMVC/Dockerfile\n    depends_on:\n      - catalog-api\n      - ordering-api\n      - identity-api\n      - basket-api\n      - marketing-api\n\n  sqldata:\n    image: mcr.microsoft.com/mssql/server:2019-latest\n\n  nosqldata:\n    image: mongo\n\n  basketdata:\n    image: redis\n\n  rabbitmq:\n    image: rabbitmq:3-management\n\nThe values in the base docker-compose.yml file should not change because of different target\ndeployment environments.\n\nIf you focus on the webmvc service definition, for instance, you can see how that information is much\nthe same no matter what environment you might be targeting. You have the following information:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nThe service name: webmvc.\n\nThe container\u2019s custom image: eshop/webmvc.\n\nThe command to build the custom Docker image, indicating which Dockerfile to use.\n\nDependencies on other services, so this container does not start until the other dependency\ncontainers have started.\n\nYou can have additional configuration, but the important point is that in the base docker-\ncompose.yml file, you just want to set the information that is common across environments. Then in\nthe docker-compose.override.yml or similar files for production or staging, you should place\nconfiguration that is specific for each environment.\n\nUsually, the docker-compose.override.yml is used for your development environment, as in the\nfollowing example from eShopOnContainers:\n\n#docker-compose.override.yml (Extended config for DEVELOPMENT env.)\nversion: '3.4'\n\nservices:\n# Simplified number of services here:\n\n  basket-api:\n    environment:\n      - ASPNETCORE_ENVIRONMENT=Development\n      - ASPNETCORE_URLS=http://0.0.0.0:80\n      - ConnectionString=${ESHOP_AZURE_REDIS_BASKET_DB:-basketdata}\n      - identityUrl=http://identity-api\n      - IdentityUrlExternal=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5105\n\n123\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f      - EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq}\n      - EventBusUserName=${ESHOP_SERVICE_BUS_USERNAME}\n      - EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD}\n      - AzureServiceBusEnabled=False\n      - ApplicationInsights__InstrumentationKey=${INSTRUMENTATION_KEY}\n      - OrchestratorType=${ORCHESTRATOR_TYPE}\n      - UseLoadTest=${USE_LOADTEST:-False}\n\n    ports:\n      - \"5103:80\"\n\n  catalog-api:\n    environment:\n      - ASPNETCORE_ENVIRONMENT=Development\n      - ASPNETCORE_URLS=http://0.0.0.0:80\n      - ConnectionString=${ESHOP_AZURE_CATALOG_DB:-\nServer=sqldata;Database=Microsoft.eShopOnContainers.Services.CatalogDb;User\nId=sa;Password=[PLACEHOLDER]}\n      - PicBaseUrl=${ESHOP_AZURE_STORAGE_CATALOG_URL:-\nhttp://host.docker.internal:5202/api/v1/catalog/items/[0]/pic/}\n      - EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq}\n      - EventBusUserName=${ESHOP_SERVICE_BUS_USERNAME}\n      - EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD}\n      - AzureStorageAccountName=${ESHOP_AZURE_STORAGE_CATALOG_NAME}\n      - AzureStorageAccountKey=${ESHOP_AZURE_STORAGE_CATALOG_KEY}\n      - UseCustomizationData=True\n      - AzureServiceBusEnabled=False\n      - AzureStorageEnabled=False\n      - ApplicationInsights__InstrumentationKey=${INSTRUMENTATION_KEY}\n      - OrchestratorType=${ORCHESTRATOR_TYPE}\n    ports:\n      - \"5101:80\"\n\n  marketing-api:\n    environment:\n      - ASPNETCORE_ENVIRONMENT=Development\n      - ASPNETCORE_URLS=http://0.0.0.0:80\n      - ConnectionString=${ESHOP_AZURE_MARKETING_DB:-\nServer=sqldata;Database=Microsoft.eShopOnContainers.Services.MarketingDb;User\nId=sa;Password=[PLACEHOLDER]}\n      - MongoConnectionString=${ESHOP_AZURE_COSMOSDB:-mongodb://nosqldata}\n      - MongoDatabase=MarketingDb\n      - EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq}\n      - EventBusUserName=${ESHOP_SERVICE_BUS_USERNAME}\n      - EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD}\n      - identityUrl=http://identity-api\n      - IdentityUrlExternal=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5105\n      - CampaignDetailFunctionUri=${ESHOP_AZUREFUNC_CAMPAIGN_DETAILS_URI}\n      - PicBaseUrl=${ESHOP_AZURE_STORAGE_MARKETING_URL:-\nhttp://host.docker.internal:5110/api/v1/campaigns/[0]/pic/}\n      - AzureStorageAccountName=${ESHOP_AZURE_STORAGE_MARKETING_NAME}\n      - AzureStorageAccountKey=${ESHOP_AZURE_STORAGE_MARKETING_KEY}\n      - AzureServiceBusEnabled=False\n      - AzureStorageEnabled=False\n      - ApplicationInsights__InstrumentationKey=${INSTRUMENTATION_KEY}\n      - OrchestratorType=${ORCHESTRATOR_TYPE}\n      - UseLoadTest=${USE_LOADTEST:-False}\n    ports:\n      - \"5110:80\"\n\n  webmvc:\n\n124\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f    environment:\n      - ASPNETCORE_ENVIRONMENT=Development\n      - ASPNETCORE_URLS=http://0.0.0.0:80\n      - PurchaseUrl=http://webshoppingapigw\n      - IdentityUrl=http://10.0.75.1:5105\n      - MarketingUrl=http://webmarketingapigw\n      - CatalogUrlHC=http://catalog-api/hc\n      - OrderingUrlHC=http://ordering-api/hc\n      - IdentityUrlHC=http://identity-api/hc\n      - BasketUrlHC=http://basket-api/hc\n      - MarketingUrlHC=http://marketing-api/hc\n      - PaymentUrlHC=http://payment-api/hc\n      - SignalrHubUrl=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5202\n      - UseCustomizationData=True\n      - ApplicationInsights__InstrumentationKey=${INSTRUMENTATION_KEY}\n      - OrchestratorType=${ORCHESTRATOR_TYPE}\n      - UseLoadTest=${USE_LOADTEST:-False}\n    ports:\n      - \"5100:80\"\n  sqldata:\n    environment:\n      - SA_PASSWORD=[PLACEHOLDER]\n      - ACCEPT_EULA=Y\n    ports:\n      - \"5433:1433\"\n  nosqldata:\n    ports:\n      - \"27017:27017\"\n  basketdata:\n    ports:\n      - \"6379:6379\"\n  rabbitmq:\n    ports:\n      - \"15672:15672\"\n      - \"5672:5672\"\n\nIn this example, the development override configuration exposes some ports to the host, defines\nenvironment variables with redirect URLs, and specifies connection strings for the development\nenvironment. These settings are all just for the development environment.\n\nWhen you run docker-compose up (or launch it from Visual Studio), the command reads the overrides\nautomatically as if it were merging both files.\n\nSuppose that you want another Compose file for the production environment, with different\nconfiguration values, ports, or connection strings. You can create another override file, like file named\ndocker-compose.prod.yml with different settings and environment variables. That file might be stored\nin a different Git repo or managed and secured by a different team.\n\nHow to deploy with a specific override file\n\nTo use multiple override files, or an override file with a different name, you can use the -f option with\nthe docker-compose command and specify the files. Compose merges files in the order they are\nspecified on the command line. The following example shows how to deploy with override files.\n\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n\n125\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fUsing environment variables in docker-compose files\n\nIt is convenient, especially in production environments, to be able to get configuration information\nfrom environment variables, as we have shown in previous examples. You can reference an\nenvironment variable in your docker-compose files using the syntax ${MY_VAR}. The following line\nfrom a docker-compose.prod.yml file shows how to reference the value of an environment variable.\n\nIdentityUrl=http://${ESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP}:5105\n\nEnvironment variables are created and initialized in different ways, depending on your host\nenvironment (Linux, Windows, Cloud cluster, etc.). However, a convenient approach is to use an .env\nfile. The docker-compose files support declaring default environment variables in the .env file. These\nvalues for the environment variables are the default values. But they can be overridden by the values\nyou might have defined in each of your environments (host OS or environment variables from your\ncluster). You place this .env file in the folder where the docker-compose command is executed from.\n\nThe following example shows an .env file like the .env file for the eShopOnContainers application.\n\n# .env file\n\nESHOP_EXTERNAL_DNS_NAME_OR_IP=host.docker.internal\n\nESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP=10.121.122.92\n\nDocker-compose expects each line in an .env file to be in the format <variable>=<value>.\n\nThe values set in the run-time environment always override the values defined inside the .env file. In a\nsimilar way, values passed via command-line arguments also override the default values set in the .env\nfile.\n\nAdditional resources\n\n\u2022\n\nOverview of Docker Compose\nhttps://docs.docker.com/compose/overview/\n\n\u2022  Multiple Compose files\n\nhttps://docs.docker.com/compose/multiple-compose-files/\n\nBuilding optimized ASP.NET Core Docker images\n\nIf you are exploring Docker and .NET on sources on the Internet, you will find Dockerfiles that\ndemonstrate the simplicity of building a Docker image by copying your source into a container. These\nexamples suggest that by using a simple configuration, you can have a Docker image with the\nenvironment packaged with your application. The following example shows a simple Dockerfile in this\nvein.\n\nFROM mcr.microsoft.com/dotnet/sdk:7.0\nWORKDIR /app\nENV ASPNETCORE_URLS http://+:80\nEXPOSE 80\nCOPY . .\nRUN dotnet restore\nENTRYPOINT [\"dotnet\", \"run\"]\n\n126\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fA Dockerfile like this will work. However, you can substantially optimize your images, especially your\nproduction images.\n\nIn the container and microservices model, you are constantly starting containers. The typical way of\nusing containers does not restart a sleeping container, because the container is disposable.\nOrchestrators (like Kubernetes and Azure Service Fabric) create new instances of images. What this\nmeans is that you would need to optimize by precompiling the application when it is built so the\ninstantiation process will be faster. When the container is started, it should be ready to run. Don\u2019t\nrestore and compile at run time using the dotnet restore and dotnet build CLI commands as you may\nsee in blog posts about .NET and Docker.\n\nThe .NET team has been doing important work to make .NET and ASP.NET Core a container-optimized\nframework. Not only is .NET a lightweight framework with a small memory footprint; the team has\nfocused on optimized Docker images for three main scenarios and published them in the Docker Hub\nregistry at dotnet/, beginning with version 2.1:\n\n1.  Development: The priority is the ability to quickly iterate and debug changes, and where size\n\nis secondary.\n\n2.\n\n3.\n\nBuild: The priority is compiling the application, and the image includes binaries and other\ndependencies to optimize binaries.\n\nProduction: The focus is fast deploying and starting of containers, so these images are\nlimited to the binaries and content needed to run the application.\n\nThe .NET team provides four basic variants in dotnet/ (at Docker Hub):\n\n1.\n\n2.\n\n3.\n\n4.\n\nsdk: for development and build scenarios\n\naspnet: for ASP.NET production scenarios\n\nruntime: for .NET production scenarios\n\nruntime-deps: for production scenarios of self-contained applications\n\nFor faster startup, runtime images also automatically set aspnetcore_urls to port 80 and use Ngen to\ncreate a native image cache of assemblies.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\nBuilding Optimized Docker Images with ASP.NET Core\nhttps://learn.microsoft.com/archive/blogs/stevelasker/building-optimized-docker-images-\nwith-asp-net-core\n\nBuilding Docker Images for .NET Applications\nhttps://learn.microsoft.com/dotnet/core/docker/building-net-docker-images\n\nUse a database server running as a container\n\nYou can have your databases (SQL Server, PostgreSQL, MySQL, etc.) on regular standalone servers, in\non-premises clusters, or in PaaS services in the cloud like Azure SQL DB. However, for development\nand test environments, having your databases running as containers is convenient, because you don\u2019t\n\n127\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fhave any external dependency and simply running the docker-compose up command starts the whole\napplication. Having those databases as containers is also great for integration tests, because the\ndatabase is started in the container and is always populated with the same sample data, so tests can\nbe more predictable.\n\nSQL Server running as a container with a microservice-related\ndatabase\n\nIn eShopOnContainers, there\u2019s a container named sqldata, as defined in the docker-compose.yml file,\nthat runs a SQL Server for Linux instance with the SQL databases for all microservices that need one.\n\nA key point in microservices is that each microservice owns its related data, so it should have its own\ndatabase. However, the databases can be anywhere. In this case, they are all in the same container to\nkeep Docker memory requirements as low as possible. Keep in mind that this is a good-enough\nsolution for development and, perhaps, testing but not for production.\n\nThe SQL Server container in the sample application is configured with the following YAML code in the\ndocker-compose.yml file, which is executed when you run docker-compose up. Note that the YAML\ncode has consolidated configuration information from the generic docker-compose.yml file and the\ndocker-compose.override.yml file. (Usually you would separate the environment settings from the\nbase or static information related to the SQL Server image.)\n\n  sqldata:\n    image: mcr.microsoft.com/mssql/server:2017-latest\n    environment:\n      - SA_PASSWORD=Pass@word\n      - ACCEPT_EULA=Y\n    ports:\n      - \"5434:1433\"\n\nIn a similar way, instead of using docker-compose, the following docker run command can run that\ncontainer:\n\ndocker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=Pass@word' -p 5433:1433 -d\nmcr.microsoft.com/mssql/server:2017-latest\n\nHowever, if you are deploying a multi-container application like eShopOnContainers, it is more\nconvenient to use the docker-compose up command so that it deploys all the required containers for\nthe application.\n\nWhen you start this SQL Server container for the first time, the container initializes SQL Server with the\npassword that you provide. Once SQL Server is running as a container, you can update the database\nby connecting through any regular SQL connection, such as from SQL Server Management Studio,\nVisual Studio, or C# code.\n\nThe eShopOnContainers application initializes each microservice database with sample data by\nseeding it with data on startup, as explained in the following section.\n\nHaving SQL Server running as a container is not just useful for a demo where you might not have\naccess to an instance of SQL Server. As noted, it is also great for development and testing\n\n128\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fenvironments so that you can easily run integration tests starting from a clean SQL Server image and\nknown data by seeding new sample data.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\nRun the SQL Server Docker image on Linux, Mac, or Windows\nhttps://learn.microsoft.com/sql/linux/sql-server-linux-setup-docker\n\nConnect and query SQL Server on Linux with sqlcmd\nhttps://learn.microsoft.com/sql/linux/sql-server-linux-connect-and-query-sqlcmd\n\nSeeding with test data on Web application startup\n\nTo add data to the database when the application starts up, you can add code like the following to\nthe Main method in the Program class of the Web API project:\n\npublic static int Main(string[] args)\n{\n    var configuration = GetConfiguration();\n\n    Log.Logger = CreateSerilogLogger(configuration);\n\n    try\n    {\n        Log.Information(\"Configuring web host ({ApplicationContext})...\", AppName);\n        var host = CreateHostBuilder(configuration, args);\n\n        Log.Information(\"Applying migrations ({ApplicationContext})...\", AppName);\n        host.MigrateDbContext<CatalogContext>((context, services) =>\n        {\n            var env = services.GetService<IWebHostEnvironment>();\n            var settings = services.GetService<IOptions<CatalogSettings>>();\n            var logger = services.GetService<ILogger<CatalogContextSeed>>();\n\n            new CatalogContextSeed()\n                .SeedAsync(context, env, settings, logger)\n                .Wait();\n        })\n        .MigrateDbContext<IntegrationEventLogContext>((_, __) => { });\n\n        Log.Information(\"Starting web host ({ApplicationContext})...\", AppName);\n        host.Run();\n\n        return 0;\n    }\n    catch (Exception ex)\n    {\n        Log.Fatal(ex, \"Program terminated unexpectedly ({ApplicationContext})!\", AppName);\n        return 1;\n    }\n    finally\n    {\n        Log.CloseAndFlush();\n    }\n}\n\n129\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThere\u2019s an important caveat when applying migrations and seeding a database during container\nstartup. Since the database server might not be available for whatever reason, you must handle retries\nwhile waiting for the server to be available. This retry logic is handled by the MigrateDbContext()\nextension method, as shown in the following code:\n\npublic static IWebHost MigrateDbContext<TContext>(\n    this IWebHost host,\n    Action<TContext,\n    IServiceProvider> seeder)\n      where TContext : DbContext\n{\n    var underK8s = host.IsInKubernetes();\n\n    using (var scope = host.Services.CreateScope())\n    {\n        var services = scope.ServiceProvider;\n\n        var logger = services.GetRequiredService<ILogger<TContext>>();\n\n        var context = services.GetService<TContext>();\n\n        try\n        {\n            logger.LogInformation(\"Migrating database associated with context\n{DbContextName}\", typeof(TContext).Name);\n\n            if (underK8s)\n            {\n                InvokeSeeder(seeder, context, services);\n            }\n            else\n            {\n                var retry = Policy.Handle<SqlException>()\n                    .WaitAndRetry(new TimeSpan[]\n                    {\n                    TimeSpan.FromSeconds(3),\n                    TimeSpan.FromSeconds(5),\n                    TimeSpan.FromSeconds(8),\n                    });\n\n                //if the sql server container is not created on run docker compose this\n                //migration can't fail for network related exception. The retry options for\nDbContext only\n                //apply to transient exceptions\n                // Note that this is NOT applied when running some orchestrators (let the\norchestrator to recreate the failing service)\n                retry.Execute(() => InvokeSeeder(seeder, context, services));\n            }\n\n            logger.LogInformation(\"Migrated database associated with context\n{DbContextName}\", typeof(TContext).Name);\n        }\n        catch (Exception ex)\n        {\n            logger.LogError(ex, \"An error occurred while migrating the database used on\ncontext {DbContextName}\", typeof(TContext).Name);\n            if (underK8s)\n            {\n                throw;          // Rethrow under k8s because we rely on k8s to re-run the\n\n130\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fpod\n            }\n        }\n    }\n\n    return host;\n}\n\nThe following code in the custom CatalogContextSeed class populates the data.\n\npublic class CatalogContextSeed\n{\n    public static async Task SeedAsync(IApplicationBuilder applicationBuilder)\n    {\n        var context = (CatalogContext)applicationBuilder\n            .ApplicationServices.GetService(typeof(CatalogContext));\n        using (context)\n        {\n            context.Database.Migrate();\n            if (!context.CatalogBrands.Any())\n            {\n                context.CatalogBrands.AddRange(\n                    GetPreconfiguredCatalogBrands());\n                await context.SaveChangesAsync();\n            }\n            if (!context.CatalogTypes.Any())\n            {\n                context.CatalogTypes.AddRange(\n                    GetPreconfiguredCatalogTypes());\n                await context.SaveChangesAsync();\n            }\n        }\n    }\n\n    static IEnumerable<CatalogBrand> GetPreconfiguredCatalogBrands()\n    {\n        return new List<CatalogBrand>()\n       {\n           new CatalogBrand() { Brand = \"Azure\"},\n           new CatalogBrand() { Brand = \".NET\" },\n           new CatalogBrand() { Brand = \"Visual Studio\" },\n           new CatalogBrand() { Brand = \"SQL Server\" }\n       };\n    }\n\n    static IEnumerable<CatalogType> GetPreconfiguredCatalogTypes()\n    {\n        return new List<CatalogType>()\n        {\n            new CatalogType() { Type = \"Mug\"},\n            new CatalogType() { Type = \"T-Shirt\" },\n            new CatalogType() { Type = \"Backpack\" },\n            new CatalogType() { Type = \"USB Memory Stick\" }\n        };\n    }\n}\n\nWhen you run integration tests, having a way to generate data consistent with your integration tests is\nuseful. Being able to create everything from scratch, including an instance of SQL Server running on a\ncontainer, is great for test environments.\n\n131\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fEF Core InMemory database versus SQL Server running as a container\n\nAnother good choice when running tests is to use the Entity Framework InMemory database provider.\nYou can specify that configuration in the ConfigureServices method of the Startup class in your Web\nAPI project:\n\npublic class Startup\n{\n    // Other Startup code ...\n    public void ConfigureServices(IServiceCollection services)\n    {\n        services.AddSingleton<IConfiguration>(Configuration);\n        // DbContext using an InMemory database provider\n        services.AddDbContext<CatalogContext>(opt => opt.UseInMemoryDatabase());\n        //(Alternative: DbContext using a SQL Server provider\n        //services.AddDbContext<CatalogContext>(c =>\n        //{\n            // c.UseSqlServer(Configuration[\"ConnectionString\"]);\n            //\n        //});\n    }\n\n    // Other Startup code ...\n}\n\nThere is an important catch, though. The in-memory database does not support many constraints that\nare specific to a particular database. For instance, you might add a unique index on a column in your\nEF Core model and write a test against your in-memory database to check that it does not let you add\na duplicate value. But when you are using the in-memory database, you cannot handle unique indexes\non a column. Therefore, the in-memory database does not behave exactly the same as a real SQL\nServer database\u2014it does not emulate database-specific constraints.\n\nEven so, an in-memory database is still useful for testing and prototyping. But if you want to create\naccurate integration tests that take into account the behavior of a specific database implementation,\nyou need to use a real database like SQL Server. For that purpose, running SQL Server in a container is\na great choice and more accurate than the EF Core InMemory database provider.\n\nUsing a Redis cache service running in a container\n\nYou can run Redis on a container, especially for development and testing and for proof-of-concept\nscenarios. This scenario is convenient, because you can have all your dependencies running on\ncontainers\u2014not just for your local development machines, but for your testing environments in your\nCI/CD pipelines.\n\nHowever, when you run Redis in production, it is better to look for a high-availability solution like\nRedis Microsoft Azure, which runs as a PaaS (Platform as a Service). In your code, you just need to\nchange your connection strings.\n\nRedis provides a Docker image with Redis. That image is available from Docker Hub at this URL:\n\nhttps://hub.docker.com/_/redis/\n\n132\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fYou can directly run a Docker Redis container by executing the following Docker CLI command in your\ncommand prompt:\n\ndocker run --name some-redis -d redis\n\nThe Redis image includes expose:6379 (the port used by Redis), so standard container linking will\nmake it automatically available to the linked containers.\n\nIn eShopOnContainers, the basket-api microservice uses a Redis cache running as a container. That\nbasketdata container is defined as part of the multi-container docker-compose.yml file, as shown in the\nfollowing example:\n\n#docker-compose.yml file\n#...\n  basketdata:\n    image: redis\n    expose:\n      - \"6379\"\n\nThis code in the docker-compose.yml defines a container named basketdata based on the redis image\nand publishing the port 6379 internally. This configuration means that it will only be accessible from\nother containers running within the Docker host.\n\nFinally, in the docker-compose.override.yml file, the basket-api microservice for the\neShopOnContainers sample defines the connection string to use for that Redis container:\n\n  basket-api:\n    environment:\n      # Other data ...\n      - ConnectionString=basketdata\n      - EventBusConnection=rabbitmq\n\nAs mentioned before, the name of the microservice basketdata is resolved by Docker\u2019s internal\nnetwork DNS.\n\nImplementing event-based communication between\nmicroservices (integration events)\n\nAs described earlier, when you use event-based communication, a microservice publishes an event\nwhen something notable happens, such as when it updates a business entity. Other microservices\nsubscribe to those events. When a microservice receives an event, it can update its own business\nentities, which might lead to more events being published. This is the essence of the eventual\nconsistency concept. This publish/subscribe system is usually performed by using an implementation\nof an event bus. The event bus can be designed as an interface with the API needed to subscribe and\nunsubscribe to events and to publish events. It can also have one or more implementations based on\nany inter-process or messaging communication, such as a messaging queue or a service bus that\nsupports asynchronous communication and a publish/subscribe model.\n\nYou can use events to implement business transactions that span multiple services, which give you\neventual consistency between those services. An eventually consistent transaction consists of a series\n\n133\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fof distributed actions. At each action, the microservice updates a business entity and publishes an\nevent that triggers the next action. Figure 6-18 below, shows a PriceUpdated event published through\nan event bus, so the price update is propagated to the Basket and other microservices.\n\nFigure 6-18. Event-driven communication based on an event bus\n\nThis section describes how you can implement this type of communication with .NET by using a\ngeneric event bus interface, as shown in Figure 6-18. There are multiple potential implementations,\neach using a different technology or infrastructure such as RabbitMQ, Azure Service Bus, or any other\nthird-party open-source or commercial service bus.\n\nUsing message brokers and service buses for production systems\n\nAs noted in the architecture section, you can choose from multiple messaging technologies for\nimplementing your abstract event bus. But these technologies are at different levels. For instance,\nRabbitMQ, a messaging broker transport, is at a lower level than commercial products like Azure\nService Bus, NServiceBus, MassTransit, or Brighter. Most of these products can work on top of either\nRabbitMQ or Azure Service Bus. Your choice of product depends on how many features and how\nmuch out-of-the-box scalability you need for your application.\n\nFor implementing just an event bus proof-of-concept for your development environment, as in the\neShopOnContainers sample, a simple implementation on top of RabbitMQ running as a container\nmight be enough. But for mission-critical and production systems that need high scalability, you\nmight want to evaluate and use Azure Service Bus.\n\nIf you require high-level abstractions and richer features like Sagas for long-running processes that\nmake distributed development easier, other commercial and open-source service buses like\nNServiceBus, MassTransit, and Brighter are worth evaluating. In this case, the abstractions and API to\nuse would usually be directly the ones provided by those high-level service buses instead of your own\nabstractions (like the simple event bus abstractions provided at eShopOnContainers). For that matter,\n\n134\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fyou can research the forked eShopOnContainers using NServiceBus (additional derived sample\nimplemented by Particular Software).\n\nOf course, you could always build your own service bus features on top of lower-level technologies\nlike RabbitMQ and Docker, but the work needed to \u201creinvent the wheel\u201d might be too costly for a\ncustom enterprise application.\n\nTo reiterate: the sample event bus abstractions and implementation showcased in the\neShopOnContainers sample are intended to be used only as a proof of concept. Once you have\ndecided that you want to have asynchronous and event-driven communication, as explained in the\ncurrent section, you should choose the service bus product that best fits your needs for production.\n\nIntegration events\n\nIntegration events are used for bringing domain state in sync across multiple microservices or external\nsystems. This functionality is done by publishing integration events outside the microservice. When an\nevent is published to multiple receiver microservices (to as many microservices as are subscribed to\nthe integration event), the appropriate event handler in each receiver microservice handles the event.\n\nAn integration event is basically a data-holding class, as in the following example:\n\npublic class ProductPriceChangedIntegrationEvent : IntegrationEvent\n{\n    public int ProductId { get; private set; }\n    public decimal NewPrice { get; private set; }\n    public decimal OldPrice { get; private set; }\n\n    public ProductPriceChangedIntegrationEvent(int productId, decimal newPrice,\n        decimal oldPrice)\n    {\n        ProductId = productId;\n        NewPrice = newPrice;\n        OldPrice = oldPrice;\n    }\n}\n\nThe integration events can be defined at the application level of each microservice, so they are\ndecoupled from other microservices, in a way comparable to how ViewModels are defined in the\nserver and client. What is not recommended is sharing a common integration events library across\nmultiple microservices; doing that would be coupling those microservices with a single event\ndefinition data library. You do not want to do that for the same reasons that you do not want to share\na common domain model across multiple microservices: microservices must be completely\nautonomous. For more information, see this blog post on the amount of data to put in events. Be\ncareful not to take this too far, as this other blog post describes the problem data deficient messages\ncan produce. Your design of your events should aim to be \u201cjust right\u201d for the needs of their\nconsumers.\n\nThere are only a few kinds of libraries you should share across microservices. One is libraries that are\nfinal application blocks, like the Event Bus client API, as in eShopOnContainers. Another is libraries\nthat constitute tools that could also be shared as NuGet components, like JSON serializers.\n\n135\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThe event bus\n\nAn event bus allows publish/subscribe-style communication between microservices without requiring\nthe components to explicitly be aware of each other, as shown in Figure 6-19.\n\nFigure 6-19. Publish/subscribe basics with an event bus\n\nThe above diagram shows that microservice A publishes to Event Bus, which distributes to subscribing\nmicroservices B and C, without the publisher needing to know the subscribers. The event bus is\nrelated to the Observer pattern and the publish-subscribe pattern.\n\nObserver pattern\n\nIn the Observer pattern, your primary object (known as the Observable) notifies other interested\nobjects (known as Observers) with relevant information (events).\n\nPublish/Subscribe (Pub/Sub) pattern\n\nThe purpose of the Publish/Subscribe pattern is the same as the Observer pattern: you want to notify\nother services when certain events take place. But there is an important difference between the\nObserver and Pub/Sub patterns. In the observer pattern, the broadcast is performed directly from the\nobservable to the observers, so they \u201cknow\u201d each other. But when using a Pub/Sub pattern, there is a\nthird component, called broker, or message broker or event bus, which is known by both the\npublisher and subscriber. Therefore, when using the Pub/Sub pattern the publisher and the\nsubscribers are precisely decoupled thanks to the mentioned event bus or message broker.\n\nThe middleman or event bus\n\nHow do you achieve anonymity between publisher and subscriber? An easy way is let a middleman\ntake care of all the communication. An event bus is one such middleman.\n\nAn event bus is typically composed of two parts:\n\n\u2022\n\n\u2022\n\nThe abstraction or interface.\n\nOne or more implementations.\n\n136\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fIn Figure 6-19 you can see how, from an application point of view, the event bus is nothing more than\na Pub/Sub channel. The way you implement this asynchronous communication can vary. It can have\nmultiple implementations so that you can swap between them, depending on the environment\nrequirements (for example, production versus development environments).\n\nIn Figure 6-20, you can see an abstraction of an event bus with multiple implementations based on\ninfrastructure messaging technologies like RabbitMQ, Azure Service Bus, or another event/message\nbroker.\n\nFigure 6- 20. Multiple implementations of an event bus\n\nIt\u2019s good to have the event bus defined through an interface so it can be implemented with several\ntechnologies, like RabbitMQ, Azure Service bus or others. However, and as mentioned previously,\nusing your own abstractions (the event bus interface) is good only if you need basic event bus\nfeatures supported by your abstractions. If you need richer service bus features, you should probably\nuse the API and abstractions provided by your preferred commercial service bus instead of your own\nabstractions.\n\nDefining an event bus interface\n\nLet\u2019s start with some implementation code for the event bus interface and possible implementations\nfor exploration purposes. The interface should be generic and straightforward, as in the following\ninterface.\n\npublic interface IEventBus\n{\n    void Publish(IntegrationEvent @event);\n\n    void Subscribe<T, TH>()\n        where T : IntegrationEvent\n        where TH : IIntegrationEventHandler<T>;\n\n    void SubscribeDynamic<TH>(string eventName)\n        where TH : IDynamicIntegrationEventHandler;\n\n137\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f    void UnsubscribeDynamic<TH>(string eventName)\n        where TH : IDynamicIntegrationEventHandler;\n\n    void Unsubscribe<T, TH>()\n        where TH : IIntegrationEventHandler<T>\n        where T : IntegrationEvent;\n}\n\nThe Publish method is straightforward. The event bus will broadcast the integration event passed to it\nto any microservice, or even an external application, subscribed to that event. This method is used by\nthe microservice that is publishing the event.\n\nThe Subscribe methods (you can have several implementations depending on the arguments) are\nused by the microservices that want to receive events. This method has two arguments. The first is the\nintegration event to subscribe to (IntegrationEvent). The second argument is the integration event\nhandler (or callback method), named IIntegrationEventHandler<T>, to be executed when the receiver\nmicroservice gets that integration event message.\n\nAdditional resources\n\nSome production-ready messaging solutions:\n\n\u2022\n\n\u2022\n\nAzure Service Bus\nhttps://learn.microsoft.com/azure/service-bus-messaging/\n\nNServiceBus\nhttps://particular.net/nservicebus\n\n\u2022  MassTransit\n\nhttps://masstransit-project.com/\n\nImplementing an event bus with RabbitMQ for the\ndevelopment or test environment\n\nWe should start by saying that if you create your custom event bus based on RabbitMQ running in a\ncontainer, as the eShopOnContainers application does, it should be used only for your development\nand test environments. Don\u2019t use it for your production environment, unless you are building it as a\npart of a production-ready service bus as described in the Additional resources section below. A\nsimple custom event bus might be missing many production-ready critical features that a commercial\nservice bus has.\n\nOne of the event bus custom implementations in eShopOnContainers is basically a library using the\nRabbitMQ API. (There\u2019s another implementation based on Azure Service Bus.)\n\nThe event bus implementation with RabbitMQ lets microservices subscribe to events, publish events,\nand receive events, as shown in Figure 6-21.\n\n138\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-21. RabbitMQ implementation of an event bus\n\nRabbitMQ functions as an intermediary between message publisher and subscribers, to handle\ndistribution. In the code, the EventBusRabbitMQ class implements the generic IEventBus interface.\nThis implementation is based on Dependency Injection so that you can swap from this dev/test\nversion to a production version.\n\npublic class EventBusRabbitMQ : IEventBus, IDisposable\n{\n    // Implementation using RabbitMQ API\n    //...\n}\n\nThe RabbitMQ implementation of a sample dev/test event bus is boilerplate code. It has to handle the\nconnection to the RabbitMQ server and provide code for publishing a message event to the queues. It\nalso has to implement a dictionary of collections of integration event handlers for each event type;\nthese event types can have a different instantiation and different subscriptions for each receiver\nmicroservice, as shown in Figure 6-21.\n\nImplementing a simple publish method with RabbitMQ\n\nThe following code is a simplified version of an event bus implementation for RabbitMQ, to\nshowcase the whole scenario. You don\u2019t really handle the connection this way. To see the full\nimplementation, see the actual code in the dotnet-architecture/eShopOnContainers repository.\n\npublic class EventBusRabbitMQ : IEventBus, IDisposable\n{\n    // Member objects and other methods ...\n    // ...\n\n    public void Publish(IntegrationEvent @event)\n    {\n        var eventName = @event.GetType().Name;\n        var factory = new ConnectionFactory() { HostName = _connectionString };\n        using (var connection = factory.CreateConnection())\n        using (var channel = connection.CreateModel())\n\n139\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f        {\n            channel.ExchangeDeclare(exchange: _brokerName,\n                type: \"direct\");\n            string message = JsonConvert.SerializeObject(@event);\n            var body = Encoding.UTF8.GetBytes(message);\n            channel.BasicPublish(exchange: _brokerName,\n                routingKey: eventName,\n                basicProperties: null,\n                body: body);\n       }\n    }\n}\n\nThe actual code of the Publish method in the eShopOnContainers application is improved by using a\nPolly retry policy, which retries the task some times in case the RabbitMQ container is not ready. This\nscenario can occur when docker-compose is starting the containers; for example, the RabbitMQ\ncontainer might start more slowly than the other containers.\n\nAs mentioned earlier, there are many possible configurations in RabbitMQ, so this code should be\nused only for dev/test environments.\n\nImplementing the subscription code with the RabbitMQ API\n\nAs with the publish code, the following code is a simplification of part of the event bus\nimplementation for RabbitMQ. Again, you usually do not need to change it unless you are improving\nit.\n\npublic class EventBusRabbitMQ : IEventBus, IDisposable\n{\n    // Member objects and other methods ...\n    // ...\n\n    public void Subscribe<T, TH>()\n        where T : IntegrationEvent\n        where TH : IIntegrationEventHandler<T>\n    {\n        var eventName = _subsManager.GetEventKey<T>();\n\n        var containsKey = _subsManager.HasSubscriptionsForEvent(eventName);\n        if (!containsKey)\n        {\n            if (!_persistentConnection.IsConnected)\n            {\n                _persistentConnection.TryConnect();\n            }\n\n            using (var channel = _persistentConnection.CreateModel())\n            {\n                channel.QueueBind(queue: _queueName,\n                                    exchange: BROKER_NAME,\n                                    routingKey: eventName);\n            }\n        }\n\n        _subsManager.AddSubscription<T, TH>();\n    }\n}\n\n140\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fEach event type has a related channel to get events from RabbitMQ. You can then have as many event\nhandlers per channel and event type as needed.\n\nThe Subscribe method accepts an IIntegrationEventHandler object, which is like a callback method in\nthe current microservice, plus its related IntegrationEvent object. The code then adds that event\nhandler to the list of event handlers that each integration event type can have per client microservice.\nIf the client code has not already been subscribed to the event, the code creates a channel for the\nevent type so it can receive events in a push style from RabbitMQ when that event is published from\nany other service.\n\nAs mentioned above, the event bus implemented in eShopOnContainers has only an educational\npurpose, since it only handles the main scenarios, so it\u2019s not ready for production.\n\nFor production scenarios check the additional resources below, specific for RabbitMQ, and the\nImplementing event-based communication between microservices section.\n\nAdditional resources\n\nA production-ready solution with support for RabbitMQ.\n\n\u2022\n\n\u2022\n\nNServiceBus - Fully-supported commercial service bus with advanced management and\nmonitoring tooling for .NET\nhttps://particular.net/\n\nEasyNetQ - Open Source .NET API client for RabbitMQ\nhttps://easynetq.com/\n\n\u2022  MassTransit - Free, open-source distributed application framework for .NET\n\nhttps://masstransit-project.com/\n\n\u2022\n\nRebus - Open source .NET Service Bus\nhttps://github.com/rebus-org/Rebus\n\nSubscribing to events\n\nThe first step for using the event bus is to subscribe the microservices to the events they want to\nreceive. That functionality should be done in the receiver microservices.\n\nThe following simple code shows what each receiver microservice needs to implement when starting\nthe service (that is, in the Startup class) so it subscribes to the events it needs. In this case, the basket-\napi microservice needs to subscribe to ProductPriceChangedIntegrationEvent and the\nOrderStartedIntegrationEvent messages.\n\nFor instance, when subscribing to the ProductPriceChangedIntegrationEvent event, that makes the\nbasket microservice aware of any changes to the product price and lets it warn the user about the\nchange if that product is in the user\u2019s basket.\n\nvar eventBus = app.ApplicationServices.GetRequiredService<IEventBus>();\n\neventBus.Subscribe<ProductPriceChangedIntegrationEvent,\n\n141\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f                   ProductPriceChangedIntegrationEventHandler>();\n\neventBus.Subscribe<OrderStartedIntegrationEvent,\n                   OrderStartedIntegrationEventHandler>();\n\nAfter this code runs, the subscriber microservice will be listening through RabbitMQ channels. When\nany message of type ProductPriceChangedIntegrationEvent arrives, the code invokes the event\nhandler that is passed to it and processes the event.\n\nPublishing events through the event bus\n\nFinally, the message sender (origin microservice) publishes the integration events with code similar to\nthe following example. (This approach is a simplified example that does not take atomicity into\naccount.) You would implement similar code whenever an event must be propagated across multiple\nmicroservices, usually right after committing data or transactions from the origin microservice.\n\nFirst, the event bus implementation object (based on RabbitMQ or based on a service bus) would be\ninjected at the controller constructor, as in the following code:\n\n[Route(\"api/v1/[controller]\")]\npublic class CatalogController : ControllerBase\n{\n    private readonly CatalogContext _context;\n    private readonly IOptionsSnapshot<Settings> _settings;\n    private readonly IEventBus _eventBus;\n\n    public CatalogController(CatalogContext context,\n        IOptionsSnapshot<Settings> settings,\n        IEventBus eventBus)\n    {\n        _context = context;\n        _settings = settings;\n        _eventBus = eventBus;\n    }\n    // ...\n}\n\nThen you use it from your controller\u2019s methods, like in the UpdateProduct method:\n\n[Route(\"items\")]\n[HttpPost]\npublic async Task<IActionResult> UpdateProduct([FromBody]CatalogItem product)\n{\n    var item = await _context.CatalogItems.SingleOrDefaultAsync(\n        i => i.Id == product.Id);\n    // ...\n    if (item.Price != product.Price)\n    {\n        var oldPrice = item.Price;\n        item.Price = product.Price;\n        _context.CatalogItems.Update(item);\n        var @event = new ProductPriceChangedIntegrationEvent(item.Id,\n            item.Price,\n            oldPrice);\n        // Commit changes in original transaction\n        await _context.SaveChangesAsync();\n        // Publish integration event to the event bus\n        // (RabbitMQ or a service bus underneath)\n\n142\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f        _eventBus.Publish(@event);\n        // ...\n    }\n    // ...\n}\n\nIn this case, since the origin microservice is a simple CRUD microservice, that code is placed right into\na Web API controller.\n\nIn more advanced microservices, like when using CQRS approaches, it can be implemented in the\nCommandHandler class, within the Handle() method.\n\nDesigning atomicity and resiliency when publishing to the event bus\n\nWhen you publish integration events through a distributed messaging system like your event bus, you\nhave the problem of atomically updating the original database and publishing an event (that is, either\nboth operations complete or none of them). For instance, in the simplified example shown earlier, the\ncode commits data to the database when the product price is changed and then publishes a\nProductPriceChangedIntegrationEvent message. Initially, it might look essential that these two\noperations be performed atomically. However, if you are using a distributed transaction involving the\ndatabase and the message broker, as you do in older systems like Microsoft Message Queuing\n(MSMQ), this approach is not recommended for the reasons described by the CAP theorem.\n\nBasically, you use microservices to build scalable and highly available systems. Simplifying somewhat,\nthe CAP theorem says that you cannot build a (distributed) database (or a microservice that owns its\nmodel) that\u2019s continually available, strongly consistent, and tolerant to any partition. You must choose\ntwo of these three properties.\n\nIn microservices-based architectures, you should choose availability and tolerance, and you should\nde-emphasize strong consistency. Therefore, in most modern microservice-based applications, you\nusually do not want to use distributed transactions in messaging, as you do when you implement\ndistributed transactions based on the Windows Distributed Transaction Coordinator (DTC) with\nMSMQ.\n\nLet\u2019s go back to the initial issue and its example. If the service crashes after the database is updated\n(in this case, right after the line of code with _context.SaveChangesAsync()), but before the integration\nevent is published, the overall system could become inconsistent. This approach might be business\ncritical, depending on the specific business operation you are dealing with.\n\nAs mentioned earlier in the architecture section, you can have several approaches for dealing with this\nissue:\n\n\u2022\n\n\u2022\n\n\u2022\n\nUsing the full Event Sourcing pattern.\n\nUsing transaction log mining.\n\nUsing the Outbox pattern. This is a transactional table to store the integration events\n(extending the local transaction).\n\nFor this scenario, using the full Event Sourcing (ES) pattern is one of the best approaches, if not the\nbest. However, in many application scenarios, you might not be able to implement a full ES system. ES\nmeans storing only domain events in your transactional database, instead of storing current state\n\n143\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fdata. Storing only domain events can have great benefits, such as having the history of your system\navailable and being able to determine the state of your system at any moment in the past. However,\nimplementing a full ES system requires you to rearchitect most of your system and introduces many\nother complexities and requirements. For example, you would want to use a database specifically\nmade for event sourcing, such as Event Store, or a document-oriented database such as Azure\nCosmos DB, MongoDB, Cassandra, CouchDB, or RavenDB. ES is a great approach for this problem, but\nnot the easiest solution unless you are already familiar with event sourcing.\n\nThe option to use transaction log mining initially looks transparent. However, to use this approach,\nthe microservice has to be coupled to your RDBMS transaction log, such as the SQL Server transaction\nlog. This approach is probably not desirable. Another drawback is that the low-level updates recorded\nin the transaction log might not be at the same level as your high-level integration events. If so, the\nprocess of reverse-engineering those transaction log operations can be difficult.\n\nA balanced approach is a mix of a transactional database table and a simplified ES pattern. You can\nuse a state such as \u201cready to publish the event,\u201d which you set in the original event when you commit\nit to the integration events table. You then try to publish the event to the event bus. If the publish-\nevent action succeeds, you start another transaction in the origin service and move the state from\n\u201cready to publish the event\u201d to \u201cevent already published.\u201d\n\nIf the publish-event action in the event bus fails, the data still will not be inconsistent within the origin\nmicroservice\u2014it is still marked as \u201cready to publish the event,\u201d and with respect to the rest of the\nservices, it will eventually be consistent. You can always have background jobs checking the state of\nthe transactions or integration events. If the job finds an event in the \u201cready to publish the event\u201d\nstate, it can try to republish that event to the event bus.\n\nNotice that with this approach, you are persisting only the integration events for each origin\nmicroservice, and only the events that you want to communicate to other microservices or external\nsystems. In contrast, in a full ES system, you store all domain events as well.\n\nTherefore, this balanced approach is a simplified ES system. You need a list of integration events with\ntheir current state (\u201cready to publish\u201d versus \u201cpublished\u201d). But you only need to implement these\nstates for the integration events. And in this approach, you do not need to store all your domain data\nas events in the transactional database, as you would in a full ES system.\n\nIf you are already using a relational database, you can use a transactional table to store integration\nevents. To achieve atomicity in your application, you use a two-step process based on local\ntransactions. Basically, you have an IntegrationEvent table in the same database where you have your\ndomain entities. That table works as an insurance for achieving atomicity so that you include persisted\nintegration events into the same transactions that are committing your domain data.\n\nStep by step, the process goes like this:\n\n1.\n\n2.\n\nThe application begins a local database transaction.\n\nIt then updates the state of your domain entities and inserts an event into the integration\nevent table.\n\n3.\n\nFinally, it commits the transaction, so you get the desired atomicity and then\n\n144\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f4.\n\nYou publish the event somehow (next).\n\nWhen implementing the steps of publishing the events, you have these choices:\n\n\u2022\n\n\u2022\n\nPublish the integration event right after committing the transaction and use another local\ntransaction to mark the events in the table as being published. Then, use the table just as an\nartifact to track the integration events in case of issues in the remote microservices, and\nperform compensatory actions based on the stored integration events.\n\nUse the table as a kind of queue. A separate application thread or process queries the\nintegration event table, publishes the events to the event bus, and then uses a local\ntransaction to mark the events as published.\n\nFigure 6-22 shows the architecture for the first of these approaches.\n\nFigure 6-22. Atomicity when publishing events to the event bus\n\nThe approach illustrated in Figure 6-22 is missing an additional worker microservice that is in charge\nof checking and confirming the success of the published integration events. In case of failure, that\nadditional checker worker microservice can read events from the table and republish them, that is,\nrepeat step number 2.\n\nAbout the second approach: you use the EventLog table as a queue and always use a worker\nmicroservice to publish the messages. In that case, the process is like that shown in Figure 6-23. This\nshows an additional microservice, and the table is the single source when publishing events.\n\n145\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-23. Atomicity when publishing events to the event bus with a worker microservice\n\nFor simplicity, the eShopOnContainers sample uses the first approach (with no additional processes or\nchecker microservices) plus the event bus. However, the eShopOnContainers sample is not handling\nall possible failure cases. In a real application deployed to the cloud, you must embrace the fact that\nissues will arise eventually, and you must implement that check and resend logic. Using the table as a\nqueue can be more effective than the first approach if you have that table as a single source of events\nwhen publishing them (with the worker) through the event bus.\n\nImplementing atomicity when publishing integration events through the event\nbus\n\nThe following code shows how you can create a single transaction involving multiple DbContext\nobjects\u2014one context related to the original data being updated, and the second context related to\nthe IntegrationEventLog table.\n\nThe transaction in the example code below will not be resilient if connections to the database have\nany issue at the time when the code is running. This can happen in cloud-based systems like Azure\nSQL DB, which might move databases across servers. For implementing resilient transactions across\nmultiple contexts, see the Implementing resilient Entity Framework Core SQL connections section later\nin this guide.\n\nFor clarity, the following example shows the whole process in a single piece of code. However, the\neShopOnContainers implementation is refactored and splits this logic into multiple classes so it\u2019s\neasier to maintain.\n\n// Update Product from the Catalog microservice\n//\npublic async Task<IActionResult> UpdateProduct([FromBody]CatalogItem productToUpdate)\n{\n  var catalogItem =\n       await _catalogContext.CatalogItems.SingleOrDefaultAsync(i => i.Id ==\n                                                               productToUpdate.Id);\n\n146\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f  if (catalogItem == null) return NotFound();\n\n  bool raiseProductPriceChangedEvent = false;\n  IntegrationEvent priceChangedEvent = null;\n\n  if (catalogItem.Price != productToUpdate.Price)\n          raiseProductPriceChangedEvent = true;\n\n  if (raiseProductPriceChangedEvent) // Create event if price has changed\n  {\n      var oldPrice = catalogItem.Price;\n      priceChangedEvent = new ProductPriceChangedIntegrationEvent(catalogItem.Id,\n                                                                  productToUpdate.Price,\n                                                                  oldPrice);\n  }\n  // Update current product\n  catalogItem = productToUpdate;\n\n  // Just save the updated product if the Product's Price hasn't changed.\n  if (!raiseProductPriceChangedEvent)\n  {\n      await _catalogContext.SaveChangesAsync();\n  }\n  else  // Publish to event bus only if product price changed\n  {\n        // Achieving atomicity between original DB and the IntegrationEventLog\n        // with a local transaction\n        using (var transaction = _catalogContext.Database.BeginTransaction())\n        {\n           _catalogContext.CatalogItems.Update(catalogItem);\n           await _catalogContext.SaveChangesAsync();\n\n           await _integrationEventLogService.SaveEventAsync(priceChangedEvent);\n\n           transaction.Commit();\n        }\n\n      // Publish the integration event through the event bus\n      _eventBus.Publish(priceChangedEvent);\n\n      _integrationEventLogService.MarkEventAsPublishedAsync(\n                                                priceChangedEvent);\n  }\n\n  return Ok();\n}\n\nAfter the ProductPriceChangedIntegrationEvent integration event is created, the transaction that\nstores the original domain operation (update the catalog item) also includes the persistence of the\nevent in the EventLog table. This makes it a single transaction, and you will always be able to check\nwhether event messages were sent.\n\nThe event log table is updated atomically with the original database operation, using a local\ntransaction against the same database. If any of the operations fail, an exception is thrown and the\ntransaction rolls back any completed operation, thus maintaining consistency between the domain\noperations and the event messages saved to the table.\n\n147\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fReceiving messages from subscriptions: event handlers in receiver microservices\n\nIn addition to the event subscription logic, you need to implement the internal code for the\nintegration event handlers (like a callback method). The event handler is where you specify where the\nevent messages of a certain type will be received and processed.\n\nAn event handler first receives an event instance from the event bus. Then it locates the component to\nbe processed related to that integration event, propagating and persisting the event as a change in\nstate in the receiver microservice. For example, if a ProductPriceChanged event originates in the\ncatalog microservice, it is handled in the basket microservice and changes the state in this receiver\nbasket microservice as well, as shown in the following code.\n\nnamespace Microsoft.eShopOnContainers.Services.Basket.API.IntegrationEvents.EventHandling\n{\n    public class ProductPriceChangedIntegrationEventHandler :\n        IIntegrationEventHandler<ProductPriceChangedIntegrationEvent>\n    {\n        private readonly IBasketRepository _repository;\n\n        public ProductPriceChangedIntegrationEventHandler(\n            IBasketRepository repository)\n        {\n            _repository = repository;\n        }\n\n        public async Task Handle(ProductPriceChangedIntegrationEvent @event)\n        {\n            var userIds = await _repository.GetUsers();\n            foreach (var id in userIds)\n            {\n                var basket = await _repository.GetBasket(id);\n                await UpdatePriceInBasketItems(@event.ProductId, @event.NewPrice, basket);\n            }\n        }\n\n        private async Task UpdatePriceInBasketItems(int productId, decimal newPrice,\n            CustomerBasket basket)\n        {\n            var itemsToUpdate = basket?.Items?.Where(x => int.Parse(x.ProductId) ==\n                productId).ToList();\n            if (itemsToUpdate != null)\n            {\n                foreach (var item in itemsToUpdate)\n                {\n                    if(item.UnitPrice != newPrice)\n                    {\n                        var originalPrice = item.UnitPrice;\n                        item.UnitPrice = newPrice;\n                        item.OldUnitPrice = originalPrice;\n                    }\n                }\n                await _repository.UpdateBasket(basket);\n            }\n        }\n    }\n}\n\n148\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThe event handler needs to verify whether the product exists in any of the basket instances. It also\nupdates the item price for each related basket line item. Finally, it creates an alert to be displayed to\nthe user about the price change, as shown in Figure 6-24.\n\nFigure 6-24. Displaying an item price change in a basket, as communicated by integration events\n\nIdempotency in update message events\n\nAn important aspect of update message events is that a failure at any point in the communication\nshould cause the message to be retried. Otherwise a background task might try to publish an event\nthat has already been published, creating a race condition. Make sure that the updates are either\nidempotent or that they provide enough information to ensure that you can detect a duplicate,\ndiscard it, and send back only one response.\n\nAs noted earlier, idempotency means that an operation can be performed multiple times without\nchanging the result. In a messaging environment, as when communicating events, an event is\nidempotent if it can be delivered multiple times without changing the result for the receiver\nmicroservice. This may be necessary because of the nature of the event itself, or because of the way\nthe system handles the event. Message idempotency is important in any application that uses\nmessaging, not just in applications that implement the event bus pattern.\n\nAn example of an idempotent operation is a SQL statement that inserts data into a table only if that\ndata is not already in the table. It does not matter how many times you run that insert SQL statement;\nthe result will be the same\u2014the table will contain that data. Idempotency like this can also be\nnecessary when dealing with messages if the messages could potentially be sent and therefore\n\n149\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fprocessed more than once. For instance, if retry logic causes a sender to send exactly the same\nmessage more than once, you need to make sure that it is idempotent.\n\nIt is possible to design idempotent messages. For example, you can create an event that says \u201cset the\nproduct price to $25\u201d instead of \u201cadd $5 to the product price.\u201d You could safely process the first\nmessage any number of times and the result will be the same. That is not true for the second\nmessage. But even in the first case, you might not want to process the first event, because the system\ncould also have sent a newer price-change event and you would be overwriting the new price.\n\nAnother example might be an order-completed event that\u2019s propagated to multiple subscribers. The\napp has to make sure that order information is updated in other systems only once, even if there are\nduplicated message events for the same order-completed event.\n\nIt is convenient to have some kind of identity per event so that you can create logic that enforces that\neach event is processed only once per receiver.\n\nSome message processing is inherently idempotent. For example, if a system generates image\nthumbnails, it might not matter how many times the message about the generated thumbnail is\nprocessed; the outcome is that the thumbnails are generated and they are the same every time. On\nthe other hand, operations such as calling a payment gateway to charge a credit card may not be\nidempotent at all. In these cases, you need to ensure that processing a message multiple times has\nthe effect that you expect.\n\nAdditional resources\n\n\u2022\n\nHonoring message idempotency\nhttps://learn.microsoft.com/previous-versions/msp-n-p/jj591565(v=pandp.10)#honoring-\nmessage-idempotency\n\nDeduplicating integration event messages\n\nYou can make sure that message events are sent and processed only once per subscriber at different\nlevels. One way is to use a deduplication feature offered by the messaging infrastructure you are\nusing. Another is to implement custom logic in your destination microservice. Having validations at\nboth the transport level and the application level is your best bet.\n\nDeduplicating message events at the EventHandler level\n\nOne way to make sure that an event is processed only once by any receiver is by implementing certain\nlogic when processing the message events in event handlers. For example, that is the approach used\nin the eShopOnContainers application, as you can see in the source code of the\nUserCheckoutAcceptedIntegrationEventHandler class when it receives a\nUserCheckoutAcceptedIntegrationEvent integration event. (In this case, the CreateOrderCommand is\nwrapped with an IdentifiedCommand, using the eventMsg.RequestId as an identifier, before sending it\nto the command handler).\n\n150\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fDeduplicating messages when using RabbitMQ\n\nWhen intermittent network failures happen, messages can be duplicated, and the message receiver\nmust be ready to handle these duplicated messages. If possible, receivers should handle messages in\nan idempotent way, which is better than explicitly handling them with deduplication.\n\nAccording to the RabbitMQ documentation, \u201cIf a message is delivered to a consumer and then\nrequeued (because it was not acknowledged before the consumer connection dropped, for example)\nthen RabbitMQ will set the redelivered flag on it when it is delivered again (whether to the same\nconsumer or a different one).\n\nIf the \u201credelivered\u201d flag is set, the receiver must take that into account, because the message might\nalready have been processed. But that is not guaranteed; the message might never have reached the\nreceiver after it left the message broker, perhaps because of network issues. On the other hand, if the\n\u201credelivered\u201d flag is not set, it is guaranteed that the message has not been sent more than once.\nTherefore, the receiver needs to deduplicate messages or process messages in an idempotent way\nonly if the \u201credelivered\u201d flag is set in the message.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nForked eShopOnContainers using NServiceBus (Particular Software)\nhttps://go.particular.net/eShopOnContainers\n\nEvent Driven Messaging\nhttps://patterns.arcitura.com/soa-patterns/design_patterns/event_driven_messaging\n\nJimmy Bogard. Refactoring Towards Resilience: Evaluating Coupling\nhttps://jimmybogard.com/refactoring-towards-resilience-evaluating-coupling/\n\nPublish-Subscribe channel\nhttps://www.enterpriseintegrationpatterns.com/patterns/messaging/PublishSubscribeChannel.\nhtml\n\nCommunicating Between Bounded Contexts\nhttps://learn.microsoft.com/previous-versions/msp-n-p/jj591572(v=pandp.10)\n\nEventual Consistency\nhttps://en.wikipedia.org/wiki/Eventual_consistency\n\nPhilip Brown. Strategies for Integrating Bounded Contexts\nhttps://www.culttt.com/2014/11/26/strategies-integrating-bounded-contexts/\n\nChris Richardson. Developing Transactional Microservices Using Aggregates, Event\nSourcing and CQRS - Part 2\nhttps://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-2-richardson\n\nChris Richardson. Event Sourcing pattern\nhttps://microservices.io/patterns/data/event-sourcing.html\n\nIntroducing Event Sourcing\nhttps://learn.microsoft.com/previous-versions/msp-n-p/jj591559(v=pandp.10)\n\n151\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f\u2022\n\n\u2022\n\n\u2022\n\nEvent Store database. Official site.\nhttps://geteventstore.com/\n\nPatrick Nommensen. Event-Driven Data Management for Microservices\nhttps://dzone.com/articles/event-driven-data-management-for-microservices-1\n\nThe CAP Theorem\nhttps://en.wikipedia.org/wiki/CAP_theorem\n\n\u2022  What is CAP Theorem?\n\nhttps://www.quora.com/What-Is-CAP-Theorem-1\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nData Consistency Primer\nhttps://learn.microsoft.com/previous-versions/msp-n-p/dn589800(v=pandp.10)\n\nRick Saling. The CAP Theorem: Why \u201cEverything is Different\u201d with the Cloud and\nInternet\nhttps://learn.microsoft.com/archive/blogs/rickatmicrosoft/the-cap-theorem-why-everything-\nis-different-with-the-cloud-and-internet/\n\nEric Brewer. CAP Twelve Years Later: How the \u201cRules\u201d Have Changed\nhttps://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed\n\nCAP, PACELC, and Microservices\nhttps://ardalis.com/cap-pacelc-and-microservices/\n\nAzure Service Bus. Brokered Messaging: Duplicate Detection\nhttps://github.com/microsoftarchive/msdn-code-gallery-\nmicrosoft/tree/master/Windows%20Azure%20Product%20Team/Brokered%20Messaging%20\nDuplicate%20Detection\n\nReliability Guide (RabbitMQ documentation)\nhttps://www.rabbitmq.com/reliability.html#consumer\n\nTesting ASP.NET Core services and web apps\n\nControllers are a central part of any ASP.NET Core API service and ASP.NET MVC Web application. As\nsuch, you should have confidence they behave as intended for your application. Automated tests can\nprovide you with this confidence and can detect errors before they reach production.\n\nYou need to test how the controller behaves based on valid or invalid inputs, and test controller\nresponses based on the result of the business operation it performs. However, you should have these\ntypes of tests for your microservices:\n\n\u2022\n\n\u2022\n\nUnit tests. These tests ensure that individual components of the application work as expected.\nAssertions test the component API.\n\nIntegration tests. These tests ensure that component interactions work as expected against\nexternal artifacts like databases. Assertions can test component API, UI, or the side effects of\nactions like database I/O, logging, etc.\n\n152\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f\u2022\n\n\u2022\n\nFunctional tests for each microservice. These tests ensure that the application works as\nexpected from the user\u2019s perspective.\n\nService tests. These tests ensure that end-to-end service use cases, including testing multiple\nservices at the same time, are tested. For this type of testing, you need to prepare the\nenvironment first. In this case, it means starting the services (for example, by using docker-\ncompose up).\n\nImplementing unit tests for ASP.NET Core Web APIs\n\nUnit testing involves testing a part of an application in isolation from its infrastructure and\ndependencies. When you unit test controller logic, only the content of a single action or method is\ntested, not the behavior of its dependencies or of the framework itself. Unit tests do not detect issues\nin the interaction between components\u2014that is the purpose of integration testing.\n\nAs you unit test your controller actions, make sure you focus only on their behavior. A controller unit\ntest avoids things like filters, routing, or model binding (the mapping of request data to a ViewModel\nor DTO). Because they focus on testing just one thing, unit tests are generally simple to write and\nquick to run. A well-written set of unit tests can be run frequently without much overhead.\n\nUnit tests are implemented based on test frameworks like xUnit.net, MSTest, Moq, or NUnit. For the\neShopOnContainers sample application, we are using xUnit.\n\nWhen you write a unit test for a Web API controller, you instantiate the controller class directly using\nthe new keyword in C#, so that the test will run as fast as possible. The following example shows how\nto do this when using xUnit as the Test framework.\n\n[Fact]\npublic async Task Get_order_detail_success()\n{\n    //Arrange\n    var fakeOrderId = \"12\";\n    var fakeOrder = GetFakeOrder();\n\n    //...\n\n    //Act\n    var orderController = new OrderController(\n        _orderServiceMock.Object,\n        _basketServiceMock.Object,\n        _identityParserMock.Object);\n\n    orderController.ControllerContext.HttpContext = _contextMock.Object;\n    var actionResult = await orderController.Detail(fakeOrderId);\n\n    //Assert\n    var viewResult = Assert.IsType<ViewResult>(actionResult);\n    Assert.IsAssignableFrom<Order>(viewResult.ViewData.Model);\n}\n\n153\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fImplementing integration and functional tests for each microservice\n\nAs noted, integration tests and functional tests have different purposes and goals. However, the way\nyou implement both when testing ASP.NET Core controllers is similar, so in this section we\nconcentrate on integration tests.\n\nIntegration testing ensures that an application\u2019s components function correctly when assembled.\nASP.NET Core supports integration testing using unit test frameworks and a built-in test web host that\ncan be used to handle requests without network overhead.\n\nUnlike unit testing, integration tests frequently involve application infrastructure concerns, such as a\ndatabase, file system, network resources, or web requests and responses. Unit tests use fakes or mock\nobjects in place of these concerns. But the purpose of integration tests is to confirm that the system\nworks as expected with these systems, so for integration testing you do not use fakes or mock objects.\nInstead, you include the infrastructure, like database access or service invocation from other services.\n\nBecause integration tests exercise larger segments of code than unit tests, and because integration\ntests rely on infrastructure elements, they tend to be orders of magnitude slower than unit tests. Thus,\nit is a good idea to limit how many integration tests you write and run.\n\nASP.NET Core includes a built-in test web host that can be used to handle HTTP requests without\nnetwork overhead, meaning that you can run those tests faster than when using a real web host. The\ntest web host (TestServer) is available in a NuGet component as Microsoft.AspNetCore.TestHost. It can\nbe added to integration test projects and used to host ASP.NET Core applications.\n\nAs you can see in the following code, when you create integration tests for ASP.NET Core controllers,\nyou instantiate the controllers through the test host. This functionality is comparable to an HTTP\nrequest, but it runs faster.\n\npublic class PrimeWebDefaultRequestShould\n{\n    private readonly TestServer _server;\n    private readonly HttpClient _client;\n\n    public PrimeWebDefaultRequestShould()\n    {\n        // Arrange\n        _server = new TestServer(new WebHostBuilder()\n           .UseStartup<Startup>());\n        _client = _server.CreateClient();\n    }\n\n    [Fact]\n    public async Task ReturnHelloWorld()\n    {\n        // Act\n        var response = await _client.GetAsync(\"/\");\n        response.EnsureSuccessStatusCode();\n        var responseString = await response.Content.ReadAsStringAsync();\n        // Assert\n        Assert.Equal(\"Hello World!\", responseString);\n    }\n}\n\n154\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nSteve Smith. Testing controllers (ASP.NET Core)\nhttps://learn.microsoft.com/aspnet/core/mvc/controllers/testing\n\nSteve Smith. Integration testing (ASP.NET Core)\nhttps://learn.microsoft.com/aspnet/core/test/integration-tests\n\nUnit testing in .NET using dotnet test\nhttps://learn.microsoft.com/dotnet/core/testing/unit-testing-with-dotnet-test\n\nxUnit.net. Official site.\nhttps://xunit.net/\n\nUnit Test Basics.\nhttps://learn.microsoft.com/visualstudio/test/unit-test-basics\n\n\u2022  Moq. GitHub repo.\n\nhttps://github.com/moq/moq\n\n\u2022\n\nNUnit. Official site.\nhttps://nunit.org/\n\nImplementing service tests on a multi-container application\n\nAs noted earlier, when you test multi-container applications, all the microservices need to be running\nwithin the Docker host or container cluster. End-to-end service tests that include multiple operations\ninvolving several microservices require you to deploy and start the whole application in the Docker\nhost by running docker-compose up (or a comparable mechanism if you are using an orchestrator).\nOnce the whole application and all its services is running, you can execute end-to-end integration and\nfunctional tests.\n\nThere are a few approaches you can use. In the docker-compose.yml file that you use to deploy the\napplication at the solution level you can expand the entry point to use dotnet test. You can also use\nanother compose file that would run your tests in the image you are targeting. By using another\ncompose file for integration tests that includes your microservices and databases on containers, you\ncan make sure that the related data is always reset to its original state before running the tests.\n\nOnce the compose application is up and running, you can take advantage of breakpoints and\nexceptions if you are running Visual Studio. Or you can run the integration tests automatically in your\nCI pipeline in Azure DevOps Services or any other CI/CD system that supports Docker containers.\n\nTesting in eShopOnContainers\n\nThe reference application (eShopOnContainers) tests were recently restructured and now there are\nfour categories:\n\n1.  Unit tests, just plain old regular unit tests, contained in the {MicroserviceName}.UnitTests\n\nprojects\n\n155\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f2.  Microservice functional/integration tests, with test cases involving the infrastructure for\n\neach microservice but isolated from the others and are contained in the\n{MicroserviceName}.FunctionalTests projects.\n\n3.  Application functional/integration tests, which focus on microservices integration, with test\n\ncases that exert several microservices. These tests are located in project\nApplication.FunctionalTests.\n\nWhile unit and integration tests are organized in a test folder within the microservice project,\napplication and load tests are managed separately under the root folder, as shown in Figure 6-25.\n\nFigure 6-25. Test folder structure in eShopOnContainers\n\nMicroservice and Application functional/integration tests are run from Visual Studio, using the regular\ntests runner, but first you need to start the required infrastructure services, with a set of docker-\ncompose files contained in the solution test folder:\n\ndocker-compose-test.yml\n\nversion: '3.4'\n\nservices:\n  redis.data:\n    image: redis:alpine\n  rabbitmq:\n    image: rabbitmq:3-management-alpine\n  sqldata:\n\n156\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f    image: mcr.microsoft.com/mssql/server:2017-latest\n  nosqldata:\n    image: mongo\n\ndocker-compose-test.override.yml\n\nversion: '3.4'\n\nservices:\n  redis.data:\n    ports:\n      - \"6379:6379\"\n  rabbitmq:\n    ports:\n      - \"15672:15672\"\n      - \"5672:5672\"\n  sqldata:\n    environment:\n      - SA_PASSWORD=Pass@word\n      - ACCEPT_EULA=Y\n    ports:\n      - \"5433:1433\"\n  nosqldata:\n    ports:\n      - \"27017:27017\"\n\nSo, to run the functional/integration tests you must first run this command, from the solution test\nfolder:\n\ndocker-compose -f docker-compose-test.yml -f docker-compose-test.override.yml up\n\nAs you can see, these docker-compose files only start the Redis, RabbitMQ, SQL Server, and MongoDB\nmicroservices.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\nUnit & Integration testing on the eShopOnContainers\nhttps://github.com/dotnet-architecture/eShopOnContainers/wiki/Unit-and-integration-\ntesting\n\nLoad testing on the eShopOnContainers\nhttps://github.com/dotnet-architecture/eShopOnContainers/wiki/Load-testing\n\nImplement background tasks in microservices with\nIHostedService and the BackgroundService class\n\nBackground tasks and scheduled jobs are something you might need to use in any application,\nwhether or not it follows the microservices architecture pattern. The difference when using a\nmicroservices architecture is that you can implement the background task in a separate\nprocess/container for hosting so you can scale it down/up based on your need.\n\n157\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFrom a generic point of view, in .NET we called these type of tasks Hosted Services, because they are\nservices/logic that you host within your host/application/microservice. Note that in this case, the\nhosted service simply means a class with the background task logic.\n\nSince .NET Core 2.0, the framework provides a new interface named IHostedService helping you to\neasily implement hosted services. The basic idea is that you can register multiple background tasks\n(hosted services) that run in the background while your web host or host is running, as shown in the\nimage 6-26.\n\nFigure 6-26. Using IHostedService in a WebHost vs. a Host\n\nASP.NET Core 1.x and 2.x support IWebHost for background processes in web apps. .NET Core 2.1 and\nlater versions support IHost for background processes with plain console apps. Note the difference\nmade between WebHost and Host.\n\nA WebHost (base class implementing IWebHost) in ASP.NET Core 2.0 is the infrastructure artifact you\nuse to provide HTTP server features to your process, such as when you\u2019re implementing an MVC web\napp or Web API service. It provides all the new infrastructure goodness in ASP.NET Core, enabling you\nto use dependency injection, insert middlewares in the request pipeline, and similar. The WebHost\nuses these very same IHostedServices for background tasks.\n\nA Host (base class implementing IHost) was introduced in .NET Core 2.1. Basically, a Host allows you\nto have a similar infrastructure than what you have with WebHost (dependency injection, hosted\nservices, etc.), but in this case, you just want to have a simple and lighter process as the host, with\nnothing related to MVC, Web API or HTTP server features.\n\nTherefore, you can choose and either create a specialized host-process with IHost to handle the\nhosted services and nothing else, such a microservice made just for hosting the IHostedServices, or\nyou can alternatively extend an existing ASP.NET Core WebHost, such as an existing ASP.NET Core\nWeb API or MVC app.\n\n158\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fEach approach has pros and cons depending on your business and scalability needs. The bottom line\nis basically that if your background tasks have nothing to do with HTTP (IWebHost) you should use\nIHost.\n\nRegistering hosted services in your WebHost or Host\n\nLet\u2019s drill down further on the IHostedService interface since its usage is pretty similar in a WebHost or\nin a Host.\n\nSignalR is one example of an artifact using hosted services, but you can also use it for much simpler\nthings like:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nA background task polling a database looking for changes.\n\nA scheduled task updating some cache periodically.\n\nAn implementation of QueueBackgroundWorkItem that allows a task to be executed on a\nbackground thread.\n\nProcessing messages from a message queue in the background of a web app while sharing\ncommon services such as ILogger.\n\nA background task started with Task.Run().\n\nYou can basically offload any of those actions to a background task that implements IHostedService.\n\nThe way you add one or multiple IHostedServices into your WebHost or Host is by registering them\nup through the AddHostedService extension method in an ASP.NET Core WebHost (or in a Host in\n.NET Core 2.1 and above). Basically, you have to register the hosted services within application startup\nin Program.cs.\n\n//Other DI registrations;\n\n// Register Hosted Services\nbuilder.Services.AddHostedService<GracePeriodManagerService>();\nbuilder.Services.AddHostedService<MyHostedServiceB>();\nbuilder.Services.AddHostedService<MyHostedServiceC>();\n//...\n\nIn that code, the GracePeriodManagerService hosted service is real code from the Ordering business\nmicroservice in eShopOnContainers, while the other two are just two additional samples.\n\nThe IHostedService background task execution is coordinated with the lifetime of the application (host\nor microservice, for that matter). You register tasks when the application starts and you have the\nopportunity to do some graceful action or clean-up when the application is shutting down.\n\nWithout using IHostedService, you could always start a background thread to run any task. The\ndifference is precisely at the app\u2019s shutdown time when that thread would simply be killed without\nhaving the opportunity to run graceful clean-up actions.\n\nThe IHostedService interface\n\nWhen you register an IHostedService, .NET calls the StartAsync() and StopAsync() methods of your\nIHostedService type during application start and stop respectively. For more details, see\nIHostedService interface.\n\n159\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fAs you can imagine, you can create multiple implementations of IHostedService and register each of\nthem in Program.cs, as shown previously. All those hosted services will be started and stopped along\nwith the application/microservice.\n\nAs a developer, you are responsible for handling the stopping action of your services when\nStopAsync() method is triggered by the host.\n\nImplementing IHostedService with a custom hosted service class\nderiving from the BackgroundService base class\n\nYou could go ahead and create your custom hosted service class from scratch and implement the\nIHostedService, as you need to do when using .NET Core 2.0 and later.\n\nHowever, since most background tasks will have similar needs in regard to the cancellation tokens\nmanagement and other typical operations, there is a convenient abstract base class you can derive\nfrom, named BackgroundService (available since .NET Core 2.1).\n\nThat class provides the main work needed to set up the background task.\n\nThe next code is the abstract BackgroundService base class as implemented in .NET.\n\n// Copyright (c) .NET Foundation. Licensed under the Apache License, Version 2.0.\n/// <summary>\n/// Base class for implementing a long running <see cref=\"IHostedService\"/>.\n/// </summary>\npublic abstract class BackgroundService : IHostedService, IDisposable\n{\n    private Task _executingTask;\n    private readonly CancellationTokenSource _stoppingCts =\n                                                   new CancellationTokenSource();\n\n    protected abstract Task ExecuteAsync(CancellationToken stoppingToken);\n\n    public virtual Task StartAsync(CancellationToken cancellationToken)\n    {\n        // Store the task we're executing\n        _executingTask = ExecuteAsync(_stoppingCts.Token);\n\n        // If the task is completed then return it,\n        // this will bubble cancellation and failure to the caller\n        if (_executingTask.IsCompleted)\n        {\n            return _executingTask;\n        }\n\n        // Otherwise it's running\n        return Task.CompletedTask;\n    }\n\n    public virtual async Task StopAsync(CancellationToken cancellationToken)\n    {\n        // Stop called without start\n        if (_executingTask == null)\n        {\n            return;\n        }\n\n160\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f        try\n        {\n            // Signal cancellation to the executing method\n            _stoppingCts.Cancel();\n        }\n        finally\n        {\n            // Wait until the task completes or the stop token triggers\n            await Task.WhenAny(_executingTask, Task.Delay(Timeout.Infinite,\n                                                          cancellationToken));\n        }\n\n    }\n\n    public virtual void Dispose()\n    {\n        _stoppingCts.Cancel();\n    }\n}\n\nWhen deriving from the previous abstract base class, thanks to that inherited implementation, you just\nneed to implement the ExecuteAsync() method in your own custom hosted service class, as in the\nfollowing simplified code from eShopOnContainers which is polling a database and publishing\nintegration events into the Event Bus when needed.\n\npublic class GracePeriodManagerService : BackgroundService\n{\n    private readonly ILogger<GracePeriodManagerService> _logger;\n    private readonly OrderingBackgroundSettings _settings;\n\n    private readonly IEventBus _eventBus;\n\n    public GracePeriodManagerService(IOptions<OrderingBackgroundSettings> settings,\n                                     IEventBus eventBus,\n                                     ILogger<GracePeriodManagerService> logger)\n    {\n        // Constructor's parameters validations...\n    }\n\n    protected override async Task ExecuteAsync(CancellationToken stoppingToken)\n    {\n        _logger.LogDebug($\"GracePeriodManagerService is starting.\");\n\n        stoppingToken.Register(() =>\n            _logger.LogDebug($\" GracePeriod background task is stopping.\"));\n\n        while (!stoppingToken.IsCancellationRequested)\n        {\n            _logger.LogDebug($\"GracePeriod task doing background work.\");\n\n            // This eShopOnContainers method is querying a database table\n            // and publishing events into the Event Bus (RabbitMQ / ServiceBus)\n            CheckConfirmedGracePeriodOrders();\n\n            try {\n                    await Task.Delay(_settings.CheckUpdateTime, stoppingToken);\n                }\n            catch (TaskCanceledException exception) {\n                    _logger.LogCritical(exception, \"TaskCanceledException Error\",\nexception.Message);\n\n161\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f                }\n        }\n\n        _logger.LogDebug($\"GracePeriod background task is stopping.\");\n    }\n\n    .../...\n}\n\nIn this specific case for eShopOnContainers, it\u2019s executing an application method that\u2019s querying a\ndatabase table looking for orders with a specific state and when applying changes, it is publishing\nintegration events through the event bus (underneath it can be using RabbitMQ or Azure Service Bus).\n\nOf course, you could run any other business background task, instead.\n\nBy default, the cancellation token is set with a 5 seconds timeout, although you can change that value\nwhen building your WebHost using the UseShutdownTimeout extension of the IWebHostBuilder. This\nmeans that our service is expected to cancel within 5 seconds otherwise it will be more abruptly killed.\n\nThe following code would be changing that time to 10 seconds.\n\nWebHost.CreateDefaultBuilder(args)\n    .UseShutdownTimeout(TimeSpan.FromSeconds(10))\n    ...\n\nSummary class diagram\n\nThe following image shows a visual summary of the classes and interfaces involved when\nimplementing IHostedServices.\n\nFigure 6-27. Class diagram showing the multiple classes and interfaces related to IHostedService\n\nClass diagram: IWebHost and IHost can host many services, which inherit from BackgroundService,\nwhich implements IHostedService.\n\n162\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fDeployment considerations and takeaways\n\nIt is important to note that the way you deploy your ASP.NET Core WebHost or .NET Host might\nimpact the final solution. For instance, if you deploy your WebHost on IIS or a regular Azure App\nService, your host can be shut down because of app pool recycles. But if you are deploying your host\nas a container into an orchestrator like Kubernetes, you can control the assured number of live\ninstances of your host. In addition, you could consider other approaches in the cloud especially made\nfor these scenarios, like Azure Functions. Finally, if you need the service to be running all the time and\nare deploying on a Windows Server you could use a Windows Service.\n\nBut even for a WebHost deployed into an app pool, there are scenarios like repopulating or flushing\napplication\u2019s in-memory cache that would be still applicable.\n\nThe IHostedService interface provides a convenient way to start background tasks in an ASP.NET Core\nweb application (in .NET Core 2.0 and later versions) or in any process/host (starting in .NET Core 2.1\nwith IHost). Its main benefit is the opportunity you get with the graceful cancellation to clean-up the\ncode of your background tasks when the host itself is shutting down.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nBuilding a scheduled task in ASP.NET Core/Standard 2.0\nhttps://blog.maartenballiauw.be/post/2017/08/01/building-a-scheduled-cache-updater-in-\naspnet-core-2.html\n\nImplementing IHostedService in ASP.NET Core 2.0\nhttps://www.stevejgordon.co.uk/asp-net-core-2-ihostedservice\n\nGenericHost Sample using ASP.NET Core 2.1\nhttps://github.com/aspnet/Hosting/tree/release/2.1/samples/GenericHostSample\n\nImplement API Gateways with Ocelot\n\nImportant\n\nThe reference microservice application eShopOnContainers is currently using features provided by\nEnvoy to implement the API Gateway instead of the earlier referenced Ocelot. We made this design\nchoice because of Envoy\u2019s built-in support for the WebSocket protocol, required by the new gRPC\ninter-service communications implemented in eShopOnContainers. However, we\u2019ve retained this\nsection in the guide so you can consider Ocelot as a simple, capable, and lightweight API Gateway\nsuitable for production-grade scenarios. Also, latest Ocelot version contains a breaking change on its\njson schema. Consider using Ocelot < v16.0.0, or use the key Routes instead of ReRoutes.\n\nArchitect and design your API Gateways\n\nThe following architecture diagram shows how API Gateways were implemented with Ocelot in\neShopOnContainers.\n\n163\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-28. eShopOnContainers architecture with API Gateways\n\nThat diagram shows how the whole application is deployed into a single Docker host or development\nPC with \u201cDocker for Windows\u201d or \u201cDocker for Mac\u201d. However, deploying into any orchestrator would\nbe similar, but any container in the diagram could be scaled out in the orchestrator.\n\nIn addition, the infrastructure assets such as databases, cache, and message brokers should be\noffloaded from the orchestrator and deployed into high available systems for infrastructure, like Azure\nSQL Database, Azure Cosmos DB, Azure Redis, Azure Service Bus, or any HA clustering solution on-\npremises.\n\nAs you can also notice in the diagram, having several API Gateways allows multiple development\nteams to be autonomous (in this case Marketing features vs. Shopping features) when developing and\ndeploying their microservices plus their own related API Gateways.\n\nIf you had a single monolithic API Gateway that would mean a single point to be updated by several\ndevelopment teams, which could couple all the microservices with a single part of the application.\n\nGoing much further in the design, sometimes a fine-grained API Gateway can also be limited to a\nsingle business microservice depending on the chosen architecture. Having the API Gateway\u2019s\nboundaries dictated by the business or domain will help you to get a better design.\n\nFor instance, fine granularity in the API Gateway tier can be especially useful for more advanced\ncomposite UI applications that are based on microservices, because the concept of a fine-grained API\nGateway is similar to a UI composition service.\n\nWe delve into more details in the previous section Creating composite UI based on microservices.\n\nAs a key takeaway, for many medium- and large-size applications, using a custom-built API Gateway\nproduct is usually a good approach, but not as a single monolithic aggregator or unique central\ncustom API Gateway unless that API Gateway allows multiple independent configuration areas for the\nseveral development teams creating autonomous microservices.\n\n164\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fSample microservices/containers to reroute through the API Gateways\n\nAs an example, eShopOnContainers has around six internal microservice-types that have to be\npublished through the API Gateways, as shown in the following image.\n\nFigure 6-29. Microservice folders in eShopOnContainers solution in Visual Studio\n\nAbout the Identity service, in the design it\u2019s left out of the API Gateway routing because it\u2019s the only\ncross-cutting concern in the system, although with Ocelot it\u2019s also possible to include it as part of the\nrerouting lists.\n\nAll those services are currently implemented as ASP.NET Core Web API services, as you can tell from\nthe code. Let\u2019s focus on one of the microservices like the Catalog microservice code.\n\nFigure 6-30. Sample Web API microservice (Catalog microservice)\n\n165\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fYou can see that the Catalog microservice is a typical ASP.NET Core Web API project with several\ncontrollers and methods like in the following code.\n\n[HttpGet]\n[Route(\"items/{id:int}\")]\n[ProducesResponseType((int)HttpStatusCode.BadRequest)]\n[ProducesResponseType((int)HttpStatusCode.NotFound)]\n[ProducesResponseType(typeof(CatalogItem),(int)HttpStatusCode.OK)]\npublic async Task<IActionResult> GetItemById(int id)\n{\n    if (id <= 0)\n    {\n        return BadRequest();\n    }\n    var item = await _catalogContext.CatalogItems.\n                                          SingleOrDefaultAsync(ci => ci.Id == id);\n    //\u2026\n\n    if (item != null)\n    {\n        return Ok(item);\n    }\n    return NotFound();\n}\n\nThe HTTP request will end up running that kind of C# code accessing the microservice database and\nany additional required action.\n\nRegarding the microservice URL, when the containers are deployed in your local development PC\n(local Docker host), each microservice\u2019s container always has an internal port (usually port 80)\nspecified in its dockerfile, as in the following dockerfile:\n\nFROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base\nWORKDIR /app\nEXPOSE 80\n\nThe port 80 shown in the code is internal within the Docker host, so it can\u2019t be reached by client apps.\n\nClient apps can access only the external ports (if any) published when deploying with docker-\ncompose.\n\nThose external ports shouldn\u2019t be published when deploying to a production environment. For this\nspecific reason, why you want to use the API Gateway, to avoid the direct communication between the\nclient apps and the microservices.\n\nHowever, when developing, you want to access the microservice/container directly and run it through\nSwagger. That\u2019s why in eShopOnContainers, the external ports are still specified even when they won\u2019t\nbe used by the API Gateway or the client apps.\n\nHere\u2019s an example of the docker-compose.override.yml file for the Catalog microservice:\n\ncatalog-api:\n  environment:\n    - ASPNETCORE_ENVIRONMENT=Development\n    - ASPNETCORE_URLS=http://0.0.0.0:80\n    - ConnectionString=YOUR_VALUE\n    - ... Other Environment Variables\n\n166\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f  ports:\n    - \"5101:80\"   # Important: In a production environment you should remove the external\nport (5101) kept here for microservice debugging purposes.\n                  # The API Gateway redirects and access through the internal port (80).\n\nYou can see how in the docker-compose.override.yml configuration the internal port for the Catalog\ncontainer is port 80, but the port for external access is 5101. But this port shouldn\u2019t be used by the\napplication when using an API Gateway, only to debug, run, and test just the Catalog microservice.\n\nNormally, you won\u2019t be deploying with docker-compose into a production environment because the\nright production deployment environment for microservices is an orchestrator like Kubernetes or\nService Fabric. When deploying to those environments you use different configuration files where you\nwon\u2019t publish directly any external port for the microservices but, you\u2019ll always use the reverse proxy\nfrom the API Gateway.\n\nRun the catalog microservice in your local Docker host. Either run the full eShopOnContainers solution\nfrom Visual Studio (it runs all the services in the docker-compose files), or start the Catalog\nmicroservice with the following docker-compose command in CMD or PowerShell positioned at the\nfolder where the docker-compose.yml and docker-compose.override.yml are placed.\n\ndocker-compose run --service-ports catalog-api\n\nThis command only runs the catalog-api service container plus dependencies that are specified in the\ndocker-compose.yml. In this case, the SQL Server container and RabbitMQ container.\n\nThen, you can directly access the Catalog microservice and see its methods through the Swagger UI\naccessing directly through that \u201cexternal\u201d port, in this case http://host.docker.internal:5101/swagger:\n\n167\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-31. Testing the Catalog microservice with its Swagger UI\n\nAt this point, you could set a breakpoint in C# code in Visual Studio, test the microservice with the\nmethods exposed in Swagger UI, and finally clean-up everything with the docker-compose down\ncommand.\n\nHowever, direct-access communication to the microservice, in this case through the external port\n5101, is precisely what you want to avoid in your application. And you can avoid that by setting the\nadditional level of indirection of the API Gateway (Ocelot, in this case). That way, the client app won\u2019t\ndirectly access the microservice.\n\nImplementing your API Gateways with Ocelot\n\nOcelot is basically a set of middleware that you can apply in a specific order.\n\nOcelot is designed to work with ASP.NET Core only. The latest version of the package is 18.0 which\ntargets .NET 6 and hence is not suitable for .NET Framework applications.\n\n168\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fYou install Ocelot and its dependencies in your ASP.NET Core project with Ocelot\u2019s NuGet package,\nfrom Visual Studio.\n\nInstall-Package Ocelot\n\nIn eShopOnContainers, its API Gateway implementation is a simple ASP.NET Core WebHost project,\nand Ocelot\u2019s middleware handles all the API Gateway features, as shown in the following image:\n\nFigure 6-32. The OcelotApiGw base project in eShopOnContainers\n\nThis ASP.NET Core WebHost project is built with two simple files: Program.cs and Startup.cs.\n\nThe Program.cs just needs to create and configure the typical ASP.NET Core BuildWebHost.\n\nnamespace OcelotApiGw\n{\n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            BuildWebHost(args).Run();\n        }\n\n        public static IWebHost BuildWebHost(string[] args)\n        {\n            var builder = WebHost.CreateDefaultBuilder(args);\n\n            builder.ConfigureServices(s => s.AddSingleton(builder))\n                    .ConfigureAppConfiguration(\n                          ic => ic.AddJsonFile(Path.Combine(\"configuration\",\n                                                            \"configuration.json\")))\n                    .UseStartup<Startup>();\n            var host = builder.Build();\n            return host;\n        }\n    }\n}\n\n169\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThe important point here for Ocelot is the configuration.json file that you must provide to the builder\nthrough the AddJsonFile() method. That configuration.json is where you specify all the API Gateway\nReRoutes, meaning the external endpoints with specific ports and the correlated internal endpoints,\nusually using different ports.\n\n{\n    \"ReRoutes\": [],\n    \"GlobalConfiguration\": {}\n}\n\nThere are two sections to the configuration. An array of ReRoutes and a GlobalConfiguration. The\nReRoutes are the objects that tell Ocelot how to treat an upstream request. The Global configuration\nallows overrides of ReRoute specific settings. It\u2019s useful if you don\u2019t want to manage lots of ReRoute\nspecific settings.\n\nHere\u2019s a simplified example of ReRoute configuration file from one of the API Gateways from\neShopOnContainers.\n\n{\n  \"ReRoutes\": [\n    {\n      \"DownstreamPathTemplate\": \"/api/{version}/{everything}\",\n      \"DownstreamScheme\": \"http\",\n      \"DownstreamHostAndPorts\": [\n        {\n          \"Host\": \"catalog-api\",\n          \"Port\": 80\n        }\n      ],\n      \"UpstreamPathTemplate\": \"/api/{version}/c/{everything}\",\n      \"UpstreamHttpMethod\": [ \"POST\", \"PUT\", \"GET\" ]\n    },\n    {\n      \"DownstreamPathTemplate\": \"/api/{version}/{everything}\",\n      \"DownstreamScheme\": \"http\",\n      \"DownstreamHostAndPorts\": [\n        {\n          \"Host\": \"basket-api\",\n          \"Port\": 80\n        }\n      ],\n      \"UpstreamPathTemplate\": \"/api/{version}/b/{everything}\",\n      \"UpstreamHttpMethod\": [ \"POST\", \"PUT\", \"GET\" ],\n      \"AuthenticationOptions\": {\n        \"AuthenticationProviderKey\": \"IdentityApiKey\",\n        \"AllowedScopes\": []\n      }\n    }\n\n  ],\n    \"GlobalConfiguration\": {\n      \"RequestIdKey\": \"OcRequestId\",\n      \"AdministrationPath\": \"/administration\"\n    }\n  }\n\n170\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fThe main functionality of an Ocelot API Gateway is to take incoming HTTP requests and forward them\non to a downstream service, currently as another HTTP request. Ocelot\u2019s describes the routing of one\nrequest to another as a ReRoute.\n\nFor instance, let\u2019s focus on one of the ReRoutes in the configuration.json from above, the\nconfiguration for the Basket microservice.\n\n{\n      \"DownstreamPathTemplate\": \"/api/{version}/{everything}\",\n      \"DownstreamScheme\": \"http\",\n      \"DownstreamHostAndPorts\": [\n        {\n          \"Host\": \"basket-api\",\n          \"Port\": 80\n        }\n      ],\n      \"UpstreamPathTemplate\": \"/api/{version}/b/{everything}\",\n      \"UpstreamHttpMethod\": [ \"POST\", \"PUT\", \"GET\" ],\n      \"AuthenticationOptions\": {\n        \"AuthenticationProviderKey\": \"IdentityApiKey\",\n        \"AllowedScopes\": []\n      }\n}\n\nThe DownstreamPathTemplate, Scheme, and DownstreamHostAndPorts make the internal\nmicroservice URL that this request will be forwarded to.\n\nThe port is the internal port used by the service. When using containers, the port specified at its\ndockerfile.\n\nThe Host is a service name that depends on the service name resolution you are using. When using\ndocker-compose, the services names are provided by the Docker Host, which is using the service\nnames provided in the docker-compose files. If using an orchestrator like Kubernetes or Service\nFabric, that name should be resolved by the DNS or name resolution provided by each orchestrator.\n\nDownstreamHostAndPorts is an array that contains the host and port of any downstream services that\nyou wish to forward requests to. Usually this configuration will just contain one entry but sometimes\nyou might want to load balance requests to your downstream services and Ocelot lets you add more\nthan one entry and then select a load balancer. But if using Azure and any orchestrator it is probably a\nbetter idea to load balance with the cloud and orchestrator infrastructure.\n\nThe UpstreamPathTemplate is the URL that Ocelot will use to identify which\nDownstreamPathTemplate to use for a given request from the client. Finally, the\nUpstreamHttpMethod is used so Ocelot can distinguish between different requests (GET, POST, PUT)\nto the same URL.\n\nAt this point, you could have a single Ocelot API Gateway (ASP.NET Core WebHost) using one or\nmultiple merged configuration.json files or you can also store the configuration in a Consul KV store.\n\nBut as introduced in the architecture and design sections, if you really want to have autonomous\nmicroservices, it might be better to split that single monolithic API Gateway into multiple API\nGateways and/or BFF (Backend for Frontend). For that purpose, let\u2019s see how to implement that\napproach with Docker containers.\n\n171\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fUsing a single Docker container image to run multiple different API Gateway / BFF\ncontainer types\n\nIn eShopOnContainers, we\u2019re using a single Docker container image with the Ocelot API Gateway but\nthen, at run time, we create different services/containers for each type of API-Gateway/BFF by\nproviding a different configuration.json file, using a docker volume to access a different PC folder for\neach service.\n\nFigure 6-33. Reusing a single Ocelot Docker image across multiple API Gateway types\n\nIn eShopOnContainers, the \u201cGeneric Ocelot API Gateway Docker Image\u201d is created with the project\nnamed \u2018OcelotApiGw\u2019 and the image name \u201ceshop/ocelotapigw\u201d that is specified in the docker-\ncompose.yml file. Then, when deploying to Docker, there will be four API-Gateway containers created\nfrom that same Docker image, as shown in the following extract from the docker-compose.yml file.\n\n  mobileshoppingapigw:\n    image: eshop/ocelotapigw:${TAG:-latest}\n\n172\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f    build:\n      context: .\n      dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\n\n  mobilemarketingapigw:\n    image: eshop/ocelotapigw:${TAG:-latest}\n    build:\n      context: .\n      dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\n\n  webshoppingapigw:\n    image: eshop/ocelotapigw:${TAG:-latest}\n    build:\n      context: .\n      dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\n\n  webmarketingapigw:\n    image: eshop/ocelotapigw:${TAG:-latest}\n    build:\n      context: .\n      dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\n\nAdditionally, as you can see in the following docker-compose.override.yml file, the only difference\nbetween those API Gateway containers is the Ocelot configuration file, which is different for each\nservice container and it\u2019s specified at run time through a Docker volume.\n\nmobileshoppingapigw:\n  environment:\n    - ASPNETCORE_ENVIRONMENT=Development\n    - IdentityUrl=http://identity-api\n  ports:\n    - \"5200:80\"\n  volumes:\n    - ./src/ApiGateways/Mobile.Bff.Shopping/apigw:/app/configuration\n\nmobilemarketingapigw:\n  environment:\n    - ASPNETCORE_ENVIRONMENT=Development\n    - IdentityUrl=http://identity-api\n  ports:\n    - \"5201:80\"\n  volumes:\n    - ./src/ApiGateways/Mobile.Bff.Marketing/apigw:/app/configuration\n\nwebshoppingapigw:\n  environment:\n    - ASPNETCORE_ENVIRONMENT=Development\n    - IdentityUrl=http://identity-api\n  ports:\n    - \"5202:80\"\n  volumes:\n    - ./src/ApiGateways/Web.Bff.Shopping/apigw:/app/configuration\n\nwebmarketingapigw:\n  environment:\n    - ASPNETCORE_ENVIRONMENT=Development\n    - IdentityUrl=http://identity-api\n  ports:\n    - \"5203:80\"\n\n173\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f  volumes:\n    - ./src/ApiGateways/Web.Bff.Marketing/apigw:/app/configuration\n\nBecause of that previous code, and as shown in the Visual Studio Explorer below, the only file needed\nto define each specific business/BFF API Gateway is just a configuration.json file, because the four API\nGateways are based on the same Docker image.\n\nFigure 6-34. The only file needed to define each API Gateway / BFF with Ocelot is a configuration file\n\nBy splitting the API Gateway into multiple API Gateways, different development teams focusing on\ndifferent subsets of microservices can manage their own API Gateways by using independent Ocelot\nconfiguration files. Plus, at the same time they can reuse the same Ocelot Docker image.\n\nNow, if you run eShopOnContainers with the API Gateways (included by default in VS when opening\neShopOnContainers-ServicesAndWebApps.sln solution or if running \u201cdocker-compose up\u201d), the\nfollowing sample routes will be performed.\n\nFor instance, when visiting the upstream URL\nhttp://host.docker.internal:5202/api/v1/c/catalog/items/2/ served by the webshoppingapigw API\nGateway, you get the same result from the internal Downstream URL http://catalog-api/api/v1/2\nwithin the Docker host, as in the following browser.\n\nFigure 6-35. Accessing a microservice through a URL provided by the API Gateway\n\nBecause of testing or debugging reasons, if you wanted to directly access to the Catalog Docker\ncontainer (only at the development environment) without passing through the API Gateway, since\n\u2018catalog-api\u2019 is a DNS resolution internal to the Docker host (service discovery handled by docker-\ncompose service names), the only way to directly access the container is through the external port\npublished in the docker-compose.override.yml, which is provided only for development tests, such as\nhttp://host.docker.internal:5101/api/v1/Catalog/items/1 in the following browser.\n\n174\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-36. Direct access to a microservice for testing purposes\n\nBut the application is configured so it accesses all the microservices through the API Gateways, not\nthrough the direct port \u201cshortcuts\u201d.\n\nThe Gateway aggregation pattern in eShopOnContainers\n\nAs introduced previously, a flexible way to implement requests aggregation is with custom services, by\ncode. You could also implement request aggregation with the Request Aggregation feature in Ocelot,\nbut it might not be as flexible as you need. Therefore, the selected way to implement aggregation in\neShopOnContainers is with an explicit ASP.NET Core Web API service for each aggregator.\n\nAccording to that approach, the API Gateway composition diagram is in reality a bit more extended\nwhen considering the aggregator services that are not shown in the simplified global architecture\ndiagram shown previously.\n\nIn the following diagram, you can also see how the aggregator services work with their related API\nGateways.\n\nFigure 6-37. eShopOnContainers architecture with aggregator services\n\nZooming in further, on the \u201cShopping\u201d business area in the following image, you can see that\nchattiness between the client apps and the microservices is reduced when using the aggregator\nservices in the API Gateways.\n\n175\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-38. Zoom in vision of the Aggregator services\n\nYou can notice how when the diagram shows the possible requests coming from the API Gateways it\ncan get complex. On the other hand, when you use the aggregator pattern, you can see how the\narrows in blue would simplify the communication from a client app perspective. This pattern not only\nhelps to reduce the chattiness and latency in the communication, it also improves the user experience\nsignificantly for the remote apps (mobile and SPA apps).\n\nIn the case of the \u201cMarketing\u201d business area and microservices, it is a simple use case so there was no\nneed to use aggregators, but it could also be possible, if needed.\n\nAuthentication and authorization in Ocelot API Gateways\n\nIn an Ocelot API Gateway, you can sit the authentication service, such as an ASP.NET Core Web API\nservice using IdentityServer providing the auth token, either out or inside the API Gateway.\n\nSince eShopOnContainers is using multiple API Gateways with boundaries based on BFF and business\nareas, the Identity/Auth service is left out of the API Gateways, as highlighted in yellow in the\nfollowing diagram.\n\n176\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fFigure 6-39. Position of the Identity service in eShopOnContainers\n\nHowever, Ocelot also supports sitting the Identity/Auth microservice within the API Gateway\nboundary, as in this other diagram.\n\nFigure 6-40. Authentication in Ocelot\n\nAs the previous diagram shows, when the Identity microservice is beneath the API gateway (AG): 1) AG\nrequests an auth token from identity microservice, 2) The identity microservice returns token to AG, 3-\n4) AG requests from microservices using the auth token. Because eShopOnContainers application has\nsplit the API Gateway into multiple BFF (Backend for Frontend) and business areas API Gateways,\nanother option would have been to create an additional API Gateway for cross-cutting concerns. That\nchoice would be fair in a more complex microservice based architecture with multiple cross-cutting\nconcerns microservices. Since there\u2019s only one cross-cutting concern in eShopOnContainers, it was\ndecided to just handle the security service out of the API Gateway realm, for simplicity\u2019s sake.\n\n177\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fIn any case, if the app is secured at the API Gateway level, the authentication module of the Ocelot\nAPI Gateway is visited at first when trying to use any secured microservice. That redirects the HTTP\nrequest to visit the Identity or auth microservice to get the access token so you can visit the protected\nservices with the access_token.\n\nThe way you secure with authentication any service at the API Gateway level is by setting the\nAuthenticationProviderKey in its related settings at the configuration.json.\n\n    {\n      \"DownstreamPathTemplate\": \"/api/{version}/{everything}\",\n      \"DownstreamScheme\": \"http\",\n      \"DownstreamHostAndPorts\": [\n        {\n          \"Host\": \"basket-api\",\n          \"Port\": 80\n        }\n      ],\n      \"UpstreamPathTemplate\": \"/api/{version}/b/{everything}\",\n      \"UpstreamHttpMethod\": [],\n      \"AuthenticationOptions\": {\n        \"AuthenticationProviderKey\": \"IdentityApiKey\",\n        \"AllowedScopes\": []\n      }\n    }\n\nWhen Ocelot runs, it will look at the ReRoutes AuthenticationOptions.AuthenticationProviderKey and\ncheck that there is an Authentication Provider registered with the given key. If there isn\u2019t, then Ocelot\nwill not start up. If there is, then the ReRoute will use that provider when it executes.\n\nBecause the Ocelot WebHost is configured with the authenticationProviderKey = \"IdentityApiKey\",\nthat will require authentication whenever that service has any requests without any auth token.\n\nnamespace OcelotApiGw\n{\n    public class Startup\n    {\n        private readonly IConfiguration _cfg;\n\n        public Startup(IConfiguration configuration) => _cfg = configuration;\n\n        public void ConfigureServices(IServiceCollection services)\n        {\n            var identityUrl = _cfg.GetValue<string>(\"IdentityUrl\");\n            var authenticationProviderKey = \"IdentityApiKey\";\n                         //\u2026\n            services.AddAuthentication()\n                .AddJwtBearer(authenticationProviderKey, x =>\n                {\n                    x.Authority = identityUrl;\n                    x.RequireHttpsMetadata = false;\n                    x.TokenValidationParameters = new\nMicrosoft.IdentityModel.Tokens.TokenValidationParameters()\n                    {\n                        ValidAudiences = new[] { \"orders\", \"basket\", \"locations\",\n\"marketing\", \"mobileshoppingagg\", \"webshoppingagg\" }\n                    };\n                });\n            //...\n\n178\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\f        }\n    }\n}\n\nThen, you also need to set authorization with the [Authorize] attribute on any resource to be accessed\nlike the microservices, such as in the following Basket microservice controller.\n\nnamespace Microsoft.eShopOnContainers.Services.Basket.API.Controllers\n{\n    [Route(\"api/v1/[controller]\")]\n    [Authorize]\n    public class BasketController : Controller\n    {\n      //...\n    }\n}\n\nThe ValidAudiences such as \u201cbasket\u201d are correlated with the audience defined in each microservice\nwith AddJwtBearer() at the ConfigureServices() of the Startup class, such as in the code below.\n\n// prevent from mapping \"sub\" claim to nameidentifier.\nJwtSecurityTokenHandler.DefaultInboundClaimTypeMap.Clear();\n\nvar identityUrl = Configuration.GetValue<string>(\"IdentityUrl\");\n\nservices.AddAuthentication(options =>\n{\n    options.DefaultAuthenticateScheme = JwtBearerDefaults.AuthenticationScheme;\n    options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\n\n}).AddJwtBearer(options =>\n{\n    options.Authority = identityUrl;\n    options.RequireHttpsMetadata = false;\n    options.Audience = \"basket\";\n});\n\nIf you try to access any secured microservice, like the Basket microservice with a ReRoute URL based\non the API Gateway like http://host.docker.internal:5202/api/v1/b/basket/1, then you\u2019ll get a 401\nUnauthorized unless you provide a valid token. On the other hand, if a ReRoute URL is authenticated,\nOcelot will invoke whatever downstream scheme is associated with it (the internal microservice URL).\n\nAuthorization at Ocelot\u2019s ReRoutes tier. Ocelot supports claims-based authorization evaluated after\nthe authentication. You set the authorization at a route level by adding the following lines to the\nReRoute configuration.\n\n\"RouteClaimsRequirement\": {\n    \"UserType\": \"employee\"\n}\n\nIn that example, when the authorization middleware is called, Ocelot will find if the user has the claim\ntype \u2018UserType\u2019 in the token and if the value of that claim is \u2018employee\u2019. If it isn\u2019t, then the user will not\nbe authorized and the response will be 403 forbidden.\n\n179\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fUsing Kubernetes Ingress plus Ocelot API Gateways\n\nWhen using Kubernetes (like in an Azure Kubernetes Service cluster), you usually unify all the HTTP\nrequests through the Kubernetes Ingress tier based on Nginx.\n\nIn Kubernetes, if you don\u2019t use any ingress approach, then your services and pods have IPs only\nroutable by the cluster network.\n\nBut if you use an ingress approach, you\u2019ll have a middle tier between the Internet and your services\n(including your API Gateways), acting as a reverse proxy.\n\nAs a definition, an Ingress is a collection of rules that allow inbound connections to reach the cluster\nservices. An ingress is configured to provide services externally reachable URLs, load balance traffic,\nSSL termination and more. Users request ingress by POSTing the Ingress resource to the API server.\n\nIn eShopOnContainers, when developing locally and using just your development machine as the\nDocker host, you are not using any ingress but only the multiple API Gateways.\n\nHowever, when targeting a \u201cproduction\u201d environment based on Kubernetes, eShopOnContainers is\nusing an ingress in front of the API gateways. That way, the clients still call the same base URL but the\nrequests are routed to multiple API Gateways or BFF.\n\nAPI Gateways are front-ends or fa\u00e7ades surfacing only the services but not the web applications that\nare usually out of their scope. In addition, the API Gateways might hide certain internal microservices.\n\nThe ingress, however, is just redirecting HTTP requests but not trying to hide any microservice or web\napp.\n\nHaving an ingress Nginx tier in Kubernetes in front of the web applications plus the several Ocelot API\nGateways / BFF is the ideal architecture, as shown in the following diagram.\n\nFigure 6-41. The ingress tier in eShopOnContainers when deployed into Kubernetes\n\nA Kubernetes Ingress acts as a reverse proxy for all traffic to the app, including the web applications,\nthat are out of the Api gateway scope. When you deploy eShopOnContainers into Kubernetes, it\n\n180\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fexposes just a few services or endpoints via ingress, basically the following list of postfixes on the\nURLs:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n/ for the client SPA web application\n\n/webmvc for the client MVC web application\n\n/webstatus for the client web app showing the status/healthchecks\n\n/webshoppingapigw for the web BFF and shopping business processes\n\n/webmarketingapigw for the web BFF and marketing business processes\n\n/mobileshoppingapigw for the mobile BFF and shopping business processes\n\n/mobilemarketingapigw for the mobile BFF and marketing business processes\n\nWhen deploying to Kubernetes, each Ocelot API Gateway is using a different \u201cconfiguration.json\u201d file\nfor each pod running the API Gateways. Those \u201cconfiguration.json\u201d files are provided by mounting\n(originally with the deploy.ps1 script) a volume created based on a Kubernetes config map named\n\u2018ocelot\u2019. Each container mounts its related configuration file in the container\u2019s folder named\n/app/configuration.\n\nIn the source code files of eShopOnContainers, the original \u201cconfiguration.json\u201d files can be found\nwithin the k8s/ocelot/ folder. There\u2019s one file for each BFF/APIGateway.\n\nAdditional cross-cutting features in an Ocelot API Gateway\n\nThere are other important features to research and use, when using an Ocelot API Gateway, described\nin the following links.\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nService discovery in the client side integrating Ocelot with Consul or Eureka\nhttps://ocelot.readthedocs.io/en/latest/features/servicediscovery.html\n\nCaching at the API Gateway tier\nhttps://ocelot.readthedocs.io/en/latest/features/caching.html\n\nLogging at the API Gateway tier\nhttps://ocelot.readthedocs.io/en/latest/features/logging.html\n\nQuality of Service (Retries and Circuit breakers) at the API Gateway tier\nhttps://ocelot.readthedocs.io/en/latest/features/qualityofservice.html\n\nRate limiting\nhttps://ocelot.readthedocs.io/en/latest/features/ratelimiting.html\n\nSwagger for Ocelot\nhttps://github.com/Burgyn/MMLib.SwaggerForOcelot\n\n181\n\nCHAPTER 5 | Designing and Developing Multi-Container and Microservice-Based .NET Applications\n\n\fCHAPTER  6\n\nTackle Business\nComplexity in a\nMicroservice with DDD\nand CQRS Patterns\n\nDesign a domain model for each microservice or Bounded Context that reflects understanding of the\nbusiness domain.\n\nThis section focuses on more advanced microservices that you implement when you need to tackle\ncomplex subsystems, or microservices derived from the knowledge of domain experts with ever-\nchanging business rules. The architecture patterns used in this section are based on domain-driven\ndesign (DDD) and Command and Query Responsibility Segregation (CQRS) approaches, as illustrated\nin Figure 7-1.\n\n182\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-1. External microservice architecture versus internal architecture patterns for each microservice\n\nHowever, most of the techniques for data driven microservices, such as how to implement an ASP.NET\nCore Web API service or how to expose Swagger metadata with Swashbuckle or NSwag, are also\napplicable to the more advanced microservices implemented internally with DDD patterns. This\nsection is an extension of the previous sections, because most of the practices explained earlier also\napply here or for any kind of microservice.\n\nThis section first provides details on the simplified CQRS patterns used in the eShopOnContainers\nreference application. Later, you will get an overview of the DDD techniques that enable you to find\ncommon patterns that you can reuse in your applications.\n\nDDD is a large topic with a rich set of resources for learning. You can start with books like Domain-\nDriven Design by Eric Evans and additional materials from Vaughn Vernon, Jimmy Nilsson, Greg\nYoung, Udi Dahan, Jimmy Bogard, and many other DDD/CQRS experts. But most of all you need to try\nto learn how to apply DDD techniques from the conversations, whiteboarding, and domain modeling\nsessions with the experts in your concrete business domain.\n\nAdditional resources\n\nDDD (Domain-Driven Design)\n\n\u2022\n\nEric Evans. Domain Language\nhttps://domainlanguage.com/\n\n\u2022  Martin Fowler. Domain-Driven Design\n\nhttps://martinfowler.com/tags/domain%20driven%20design.html\n\n\u2022\n\nJimmy Bogard. Strengthening your domain: a primer\nhttps://lostechies.com/jimmybogard/2010/02/04/strengthening-your-domain-a-primer/\n\n183\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fDDD books\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nEric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software\nhttps://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-\nSoftware/dp/0321125215/\n\nEric Evans. Domain-Driven Design Reference: Definitions and Pattern Summaries\nhttps://www.amazon.com/Domain-Driven-Design-Reference-Definitions-2014-09-\n22/dp/B01N8YB4ZO/\n\nVaughn Vernon. Implementing Domain-Driven Design\nhttps://www.amazon.com/Implementing-Domain-Driven-Design-Vaughn-\nVernon/dp/0321834577/\n\nVaughn Vernon. Domain-Driven Design Distilled\nhttps://www.amazon.com/Domain-Driven-Design-Distilled-Vaughn-Vernon/dp/0134434420/\n\nJimmy Nilsson. Applying Domain-Driven Design and Patterns\nhttps://www.amazon.com/Applying-Domain-Driven-Design-Patterns-\nExamples/dp/0321268202/\n\nCesar de la Torre. N-Layered Domain-Oriented Architecture Guide with .NET\nhttps://www.amazon.com/N-Layered-Domain-Oriented-Architecture-Guide-\nNET/dp/8493903612/\n\nAbel Avram and Floyd Marinescu. Domain-Driven Design Quickly\nhttps://www.amazon.com/Domain-Driven-Design-Quickly-Abel-Avram/dp/1411609255/\n\nScott Millett, Nick Tune - Patterns, Principles, and Practices of Domain-Driven Design\nhttps://www.wiley.com/Patterns%2C+Principles%2C+and+Practices+of+Domain+Driven+Des\nign-p-9781118714706\n\nDDD training\n\n\u2022\n\nJulie Lerman and Steve Smith. Domain-Driven Design Fundamentals\nhttps://www.pluralsight.com/courses/fundamentals-domain-driven-design\n\nApply simplified CQRS and DDD patterns in a\nmicroservice\n\nCQRS is an architectural pattern that separates the models for reading and writing data. The related\nterm Command Query Separation (CQS) was originally defined by Bertrand Meyer in his book Object-\nOriented Software Construction. The basic idea is that you can divide a system\u2019s operations into two\nsharply separated categories:\n\n\u2022\n\nQueries. These queries return a result and don\u2019t change the state of the system, and they\u2019re\nfree of side effects.\n\n\u2022\n\nCommands. These commands change the state of a system.\n\n184\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fCQS is a simple concept: it is about methods within the same object being either queries or\ncommands. Each method either returns state or mutates state, but not both. Even a single repository\npattern object can comply with CQS. CQS can be considered a foundational principle for CQRS.\n\nCommand and Query Responsibility Segregation (CQRS) was introduced by Greg Young and strongly\npromoted by Udi Dahan and others. It\u2019s based on the CQS principle, although it\u2019s more detailed. It can\nbe considered a pattern based on commands and events plus optionally on asynchronous messages.\nIn many cases, CQRS is related to more advanced scenarios, like having a different physical database\nfor reads (queries) than for writes (updates). Moreover, a more evolved CQRS system might\nimplement Event-Sourcing (ES) for your updates database, so you would only store events in the\ndomain model instead of storing the current-state data. However, this approach is not used in this\nguide. This guide uses the simplest CQRS approach, which consists of just separating the queries from\nthe commands.\n\nThe separation aspect of CQRS is achieved by grouping query operations in one layer and commands\nin another layer. Each layer has its own data model (note that we say model, not necessarily a different\ndatabase) and is built using its own combination of patterns and technologies. More importantly, the\ntwo layers can be within the same tier or microservice, as in the example (ordering microservice) used\nfor this guide. Or they could be implemented on different microservices or processes so they can be\noptimized and scaled out separately without affecting one another.\n\nCQRS means having two objects for a read/write operation where in other contexts there\u2019s one. There\nare reasons to have a denormalized reads database, which you can learn about in more advanced\nCQRS literature. But we aren\u2019t using that approach here, where the goal is to have more flexibility in\nthe queries instead of limiting the queries with constraints from DDD patterns like aggregates.\n\nAn example of this kind of service is the ordering microservice from the eShopOnContainers reference\napplication. This service implements a microservice based on a simplified CQRS approach. It uses a\nsingle data source or database, but two logical models plus DDD patterns for the transactional\ndomain, as shown in Figure 7-2.\n\n185\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-2. Simplified CQRS- and DDD-based microservice\n\nThe Logical \u201cOrdering\u201d Microservice includes its Ordering database, which can be, but doesn\u2019t have to\nbe, the same Docker host. Having the database in the same Docker host is good for development, but\nnot for production.\n\nThe application layer can be the Web API itself. The important design aspect here is that the\nmicroservice has split the queries and ViewModels (data models especially created for the client\napplications) from the commands, domain model, and transactions following the CQRS pattern. This\napproach keeps the queries independent from restrictions and constraints coming from DDD patterns\nthat only make sense for transactions and updates, as explained in later sections.\n\nAdditional resources\n\n\u2022\n\nGreg Young. Versioning in an Event Sourced System (Free to read online e-book)\nhttps://leanpub.com/esversioning/read\n\nApply CQRS and CQS approaches in a DDD\nmicroservice in eShopOnContainers\n\nThe design of the ordering microservice at the eShopOnContainers reference application is based on\nCQRS principles. However, it uses the simplest approach, which is just separating the queries from the\ncommands and using the same database for both actions.\n\nThe essence of those patterns, and the important point here, is that queries are idempotent: no matter\nhow many times you query a system, the state of that system won\u2019t change. In other words, queries\nare side-effect free.\n\n186\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fTherefore, you could use a different \u201creads\u201d data model than the transactional logic \u201cwrites\u201d domain\nmodel, even though the ordering microservices are using the same database. Hence, this is a\nsimplified CQRS approach.\n\nOn the other hand, commands, which trigger transactions and data updates, change state in the\nsystem. With commands, you need to be careful when dealing with complexity and ever-changing\nbusiness rules. This is where you want to apply DDD techniques to have a better modeled system.\n\nThe DDD patterns presented in this guide should not be applied universally. They introduce\nconstraints on your design. Those constraints provide benefits such as higher quality over time,\nespecially in commands and other code that modifies system state. However, those constraints add\ncomplexity with fewer benefits for reading and querying data.\n\nOne such pattern is the Aggregate pattern, which we examine more in later sections. Briefly, in the\nAggregate pattern, you treat many domain objects as a single unit as a result of their relationship in\nthe domain. You might not always gain advantages from this pattern in queries; it can increase the\ncomplexity of query logic. For read-only queries, you do not get the advantages of treating multiple\nobjects as a single Aggregate. You only get the complexity.\n\nAs shown in Figure 7-2 in the previous section, this guide suggests using DDD patterns only in the\ntransactional/updates area of your microservice (that is, as triggered by commands). Queries can\nfollow a simpler approach and should be separated from commands, following a CQRS approach.\n\nFor implementing the \u201cqueries side\u201d, you can choose between many approaches, from your full-blown\nORM like EF Core, AutoMapper projections, stored procedures, views, materialized views or a micro\nORM.\n\nIn this guide and in eShopOnContainers (specifically the ordering microservice) we chose to\nimplement straight queries using a micro ORM like Dapper. This guide lets you implement any query\nbased on SQL statements to get the best performance, thanks to a light framework with little\noverhead.\n\nWhen you use this approach, any updates to your model that impact how entities are persisted to a\nSQL database also need separate updates to SQL queries used by Dapper or any other separate (non-\nEF) approaches to querying.\n\nCQRS and DDD patterns are not top-level architectures\n\nIt\u2019s important to understand that CQRS and most DDD patterns (like DDD layers or a domain model\nwith aggregates) are not architectural styles, but only architecture patterns. Microservices, SOA, and\nevent-driven architecture (EDA) are examples of architectural styles. They describe a system of many\ncomponents, such as many microservices. CQRS and DDD patterns describe something inside a single\nsystem or component; in this case, something inside a microservice.\n\nDifferent Bounded Contexts (BCs) will employ different patterns. They have different responsibilities,\nand that leads to different solutions. It is worth emphasizing that forcing the same pattern everywhere\nleads to failure. Do not use CQRS and DDD patterns everywhere. Many subsystems, BCs, or\nmicroservices are simpler and can be implemented more easily using simple CRUD services or using\nanother approach.\n\n187\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThere is only one application architecture: the architecture of the system or end-to-end application\nyou are designing (for example, the microservices architecture). However, the design of each Bounded\nContext or microservice within that application reflects its own tradeoffs and internal design decisions\nat an architecture patterns level. Do not try to apply the same architectural patterns as CQRS or DDD\neverywhere.\n\nAdditional resources\n\n\u2022  Martin Fowler. CQRS\n\nhttps://martinfowler.com/bliki/CQRS.html\n\n\u2022\n\n\u2022\n\nGreg Young. CQRS Documents\nhttps://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf\n\nUdi Dahan. Clarified CQRS\nhttps://udidahan.com/2009/12/09/clarified-cqrs/\n\nImplement reads/queries in a CQRS microservice\n\nFor reads/queries, the ordering microservice from the eShopOnContainers reference application\nimplements the queries independently from the DDD model and transactional area. This\nimplementation was done primarily because the demands for queries and for transactions are\ndrastically different. Writes execute transactions that must be compliant with the domain logic.\nQueries, on the other hand, are idempotent and can be segregated from the domain rules.\n\nThe approach is simple, as shown in Figure 7-3. The API interface is implemented by the Web API\ncontrollers using any infrastructure, such as a micro Object Relational Mapper (ORM) like Dapper, and\nreturning dynamic ViewModels depending on the needs of the UI applications.\n\nFigure 7-3. The simplest approach for queries in a CQRS microservice\n\nThe simplest approach for the queries-side in a simplified CQRS approach can be implemented by\nquerying the database with a Micro-ORM like Dapper, returning dynamic ViewModels. The query\n\n188\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fdefinitions query the database and return a dynamic ViewModel built on the fly for each query. Since\nthe queries are idempotent, they won\u2019t change the data no matter how many times you run a query.\nTherefore, you don\u2019t need to be restricted by any DDD pattern used in the transactional side, like\naggregates and other patterns, and that is why queries are separated from the transactional area. You\nquery the database for the data that the UI needs and return a dynamic ViewModel that does not\nneed to be statically defined anywhere (no classes for the ViewModels) except in the SQL statements\nthemselves.\n\nSince this approach is simple, the code required for the queries side (such as code using a micro ORM\nlike Dapper) can be implemented within the same Web API project. Figure 7-4 shows this approach.\nThe queries are defined in the Ordering.API microservice project within the eShopOnContainers\nsolution.\n\nFigure 7-4. Queries in the Ordering microservice in eShopOnContainers\n\nUse ViewModels specifically made for client apps, independent from\ndomain model constraints\n\nSince the queries are performed to obtain the data needed by the client applications, the returned\ntype can be specifically made for the clients, based on the data returned by the queries. These models,\nor Data Transfer Objects (DTOs), are called ViewModels.\n\nThe returned data (ViewModel) can be the result of joining data from multiple entities or tables in the\ndatabase, or even across multiple aggregates defined in the domain model for the transactional area.\nIn this case, because you are creating queries independent of the domain model, the aggregates\nboundaries and constraints are ignored and you\u2019re free to query any table and column you might\nneed. This approach provides great flexibility and productivity for the developers creating or updating\nthe queries.\n\nThe ViewModels can be static types defined in classes (as is implemented in the ordering\nmicroservice). Or they can be created dynamically based on the queries performed, which is agile for\ndevelopers.\n\nUse Dapper as a micro ORM to perform queries\n\nYou can use any micro ORM, Entity Framework Core, or even plain ADO.NET for querying. In the\nsample application, Dapper was selected for the ordering microservice in eShopOnContainers as a\ngood example of a popular micro ORM. It can run plain SQL queries with great performance, because\nit\u2019s a light framework. Using Dapper, you can write a SQL query that can access and join multiple\ntables.\n\n189\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fDapper is an open-source project (original created by Sam Saffron), and is part of the building blocks\nused in Stack Overflow. To use Dapper, you just need to install it through the Dapper NuGet package,\nas shown in the following figure:\n\nYou also need to add a using directive so your code has access to the Dapper extension methods.\n\nWhen you use Dapper in your code, you directly use the SqlConnection class available in the\nMicrosoft.Data.SqlClient namespace. Through the QueryAsync method and other extension methods\nthat extend the SqlConnection class, you can run queries in a straightforward and performant way.\n\nDynamic versus static ViewModels\n\nWhen returning ViewModels from the server-side to client apps, you can think about those\nViewModels as DTOs (Data Transfer Objects) that can be different to the internal domain entities of\nyour entity model because the ViewModels hold the data the way the client app needs. Therefore, in\nmany cases, you can aggregate data coming from multiple domain entities and compose the\nViewModels precisely according to how the client app needs that data.\n\nThose ViewModels or DTOs can be defined explicitly (as data holder classes), like the OrderSummary\nclass shown in a later code snippet. Or, you could just return dynamic ViewModels or dynamic DTOs\nbased on the attributes returned by your queries as a dynamic type.\n\nViewModel as dynamic type\n\nAs shown in the following code, a ViewModel can be directly returned by the queries by just returning\na dynamic type that internally is based on the attributes returned by a query. That means that the\nsubset of attributes to be returned is based on the query itself. Therefore, if you add a new column to\nthe query or join, that data is dynamically added to the returned ViewModel.\n\nusing Dapper;\nusing Microsoft.Extensions.Configuration;\nusing System.Data.SqlClient;\nusing System.Threading.Tasks;\nusing System.Dynamic;\nusing System.Collections.Generic;\n\npublic class OrderQueries : IOrderQueries\n{\n    public async Task<IEnumerable<dynamic>> GetOrdersAsync()\n    {\n        using (var connection = new SqlConnection(_connectionString))\n        {\n            connection.Open();\n            return await connection.QueryAsync<dynamic>(\n                @\"SELECT o.[Id] as ordernumber,\n                o.[OrderDate] as [date],os.[Name] as [status],\n                SUM(oi.units*oi.unitprice) as total\n                FROM [ordering].[Orders] o\n                LEFT JOIN[ordering].[orderitems] oi ON o.Id = oi.orderid\n                LEFT JOIN[ordering].[orderstatus] os on o.OrderStatusId = os.Id\n\n190\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f                GROUP BY o.[Id], o.[OrderDate], os.[Name]\");\n        }\n    }\n}\n\nThe important point is that by using a dynamic type, the returned collection of data is dynamically\nassembled as the ViewModel.\n\nPros: This approach reduces the need to modify static ViewModel classes whenever you update the\nSQL sentence of a query, making this design approach agile when coding, straightforward, and quick\nto evolve in regard to future changes.\n\nCons: In the long term, dynamic types can negatively impact the clarity and the compatibility of a\nservice with client apps. In addition, middleware software like Swashbuckle cannot provide the same\nlevel of documentation on returned types if using dynamic types.\n\nViewModel as predefined DTO classes\n\nPros: Having static, predefined ViewModel classes, like \u201ccontracts\u201d based on explicit DTO classes, is\ndefinitely better for public APIs but also for long-term microservices, even if they are only used by the\nsame application.\n\nIf you want to specify response types for Swagger, you need to use explicit DTO classes as the return\ntype. Therefore, predefined DTO classes allow you to offer richer information from Swagger. That\nimproves the API documentation and compatibility when consuming an API.\n\nCons: As mentioned earlier, when updating the code, it takes some more steps to update the DTO\nclasses.\n\nTip based on our experience: In the queries implemented at the Ordering microservice in\neShopOnContainers, we started developing by using dynamic ViewModels as it was straightforward\nand agile on the early development stages. But, once the development was stabilized, we chose to\nrefactor the APIs and use static or pre-defined DTOs for the ViewModels, because it is clearer for the\nmicroservice\u2019s consumers to know explicit DTO types, used as \u201ccontracts\u201d.\n\nIn the following example, you can see how the query is returning data by using an explicit ViewModel\nDTO class: the OrderSummary class.\n\nusing Dapper;\nusing Microsoft.Extensions.Configuration;\nusing System.Data.SqlClient;\nusing System.Threading.Tasks;\nusing System.Dynamic;\nusing System.Collections.Generic;\n\npublic class OrderQueries : IOrderQueries\n{\n  public async Task<IEnumerable<OrderSummary>> GetOrdersAsync()\n    {\n        using (var connection = new SqlConnection(_connectionString))\n        {\n            connection.Open();\n            return await connection.QueryAsync<OrderSummary>(\n                  @\"SELECT o.[Id] as ordernumber,\n                  o.[OrderDate] as [date],os.[Name] as [status],\n\n191\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f                  SUM(oi.units*oi.unitprice) as total\n                  FROM [ordering].[Orders] o\n                  LEFT JOIN[ordering].[orderitems] oi ON  o.Id = oi.orderid\n                  LEFT JOIN[ordering].[orderstatus] os on o.OrderStatusId = os.Id\n                  GROUP BY o.[Id], o.[OrderDate], os.[Name]\n                  ORDER BY o.[Id]\");\n        }\n    }\n}\n\nDescribe response types of Web APIs\n\nDevelopers consuming web APIs and microservices are most concerned with what is returned\u2014\nspecifically response types and error codes (if not standard). The response types are handled in the\nXML comments and data annotations.\n\nWithout proper documentation in the Swagger UI, the consumer lacks knowledge of what types are\nbeing returned or what HTTP codes can be returned. That problem is fixed by adding the\nMicrosoft.AspNetCore.Mvc.ProducesResponseTypeAttribute, so Swashbuckle can generate richer\ninformation about the API return model and values, as shown in the following code:\n\nnamespace Microsoft.eShopOnContainers.Services.Ordering.API.Controllers\n{\n    [Route(\"api/v1/[controller]\")]\n    [Authorize]\n    public class OrdersController : Controller\n    {\n        //Additional code...\n        [Route(\"\")]\n        [HttpGet]\n        [ProducesResponseType(typeof(IEnumerable<OrderSummary>),\n            (int)HttpStatusCode.OK)]\n        public async Task<IActionResult> GetOrders()\n        {\n            var userid = _identityService.GetUserIdentity();\n            var orders = await _orderQueries\n                .GetOrdersFromUserAsync(Guid.Parse(userid));\n            return Ok(orders);\n        }\n    }\n}\n\nHowever, the ProducesResponseType attribute cannot use dynamic as a type but requires to use\nexplicit types, like the OrderSummary ViewModel DTO, shown in the following example:\n\npublic class OrderSummary\n{\n    public int ordernumber { get; set; }\n    public DateTime date { get; set; }\n    public string status { get; set; }\n    public double total { get; set; }\n}\n// or using C# 8 record types:\npublic record OrderSummary(int ordernumber, DateTime date, string status, double total);\n\n192\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThis is another reason why explicit returned types are better than dynamic types, in the long term.\nWhen using the ProducesResponseType attribute, you can also specify what is the expected outcome\nregarding possible HTTP errors/codes, like 200, 400, etc.\n\nIn the following image, you can see how Swagger UI shows the ResponseType information.\n\nFigure 7-5. Swagger UI showing response types and possible HTTP status codes from a Web API\n\nThe image shows some example values based on the ViewModel types and the possible HTTP status\ncodes that can be returned.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\nDapper\nhttps://github.com/StackExchange/dapper-dot-net\n\nJulie Lerman. Data Points - Dapper, Entity Framework and Hybrid Apps (MSDN\nmagazine article)\n\n193\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fhttps://learn.microsoft.com/archive/msdn-magazine/2016/may/data-points-dapper-entity-\nframework-and-hybrid-apps\n\n\u2022\n\nASP.NET Core Web API Help Pages using Swagger\nhttps://learn.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-\nswagger?tabs=visual-studio\n\n\u2022\n\nCreate record types https://learn.microsoft.com/dotnet/csharp/whats-new/tutorials/records\n\nDesign a DDD-oriented microservice\n\nDomain-driven design (DDD) advocates modeling based on the reality of business as relevant to your\nuse cases. In the context of building applications, DDD talks about problems as domains. It describes\nindependent problem areas as Bounded Contexts (each Bounded Context correlates to a\nmicroservice), and emphasizes a common language to talk about these problems. It also suggests\nmany technical concepts and patterns, like domain entities with rich models (no anemic-domain\nmodel), value objects, aggregates, and aggregate root (or root entity) rules to support the internal\nimplementation. This section introduces the design and implementation of those internal patterns.\n\nSometimes these DDD technical rules and patterns are perceived as obstacles that have a steep\nlearning curve for implementing DDD approaches. But the important part is not the patterns\nthemselves, but organizing the code so it is aligned to the business problems, and using the same\nbusiness terms (ubiquitous language). In addition, DDD approaches should be applied only if you are\nimplementing complex microservices with significant business rules. Simpler responsibilities, like a\nCRUD service, can be managed with simpler approaches.\n\nWhere to draw the boundaries is the key task when designing and defining a microservice. DDD\npatterns help you understand the complexity in the domain. For the domain model for each Bounded\nContext, you identify and define the entities, value objects, and aggregates that model your domain.\nYou build and refine a domain model that is contained within a boundary that defines your context.\nAnd that is explicit in the form of a microservice. The components within those boundaries end up\nbeing your microservices, although in some cases a BC or business microservices can be composed of\nseveral physical services. DDD is about boundaries and so are microservices.\n\nKeep the microservice context boundaries relatively small\n\nDetermining where to place boundaries between Bounded Contexts balances two competing goals.\nFirst, you want to initially create the smallest possible microservices, although that should not be the\nmain driver; you should create a boundary around things that need cohesion. Second, you want to\navoid chatty communications between microservices. These goals can contradict one another. You\nshould balance them by decomposing the system into as many small microservices as you can until\nyou see communication boundaries growing quickly with each additional attempt to separate a new\nBounded Context. Cohesion is key within a single bounded context.\n\nIt is similar to the Inappropriate Intimacy code smell when implementing classes. If two microservices\nneed to collaborate a lot with each other, they should probably be the same microservice.\n\n194\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAnother way to look at this aspect is autonomy. If a microservice must rely on another service to\ndirectly service a request, it is not truly autonomous.\n\nLayers in DDD microservices\n\nMost enterprise applications with significant business and technical complexity are defined by\nmultiple layers. The layers are a logical artifact, and are not related to the deployment of the service.\nThey exist to help developers manage the complexity in the code. Different layers (like the domain\nmodel layer versus the presentation layer, etc.) might have different types, which mandate translations\nbetween those types.\n\nFor example, an entity could be loaded from the database. Then part of that information, or an\naggregation of information including additional data from other entities, can be sent to the client UI\nthrough a REST Web API. The point here is that the domain entity is contained within the domain\nmodel layer and should not be propagated to other areas that it does not belong to, like to the\npresentation layer.\n\nAdditionally, you need to have always-valid entities (see the Designing validations in the domain\nmodel layer section) controlled by aggregate roots (root entities). Therefore, entities should not be\nbound to client views, because at the UI level some data might still not be validated. This reason is\nwhat the ViewModel is for. The ViewModel is a data model exclusively for presentation layer needs.\nThe domain entities do not belong directly to the ViewModel. Instead, you need to translate between\nViewModels and domain entities and vice versa.\n\nWhen tackling complexity, it is important to have a domain model controlled by aggregate roots that\nmake sure that all the invariants and rules related to that group of entities (aggregate) are performed\nthrough a single entry-point or gate, the aggregate root.\n\nFigure 7-5 shows how a layered design is implemented in the eShopOnContainers application.\n\n195\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-5. DDD layers in the ordering microservice in eShopOnContainers\n\nThe three layers in a DDD microservice like Ordering. Each layer is a VS project: Application layer is\nOrdering.API, Domain layer is Ordering.Domain and the Infrastructure layer is Ordering.Infrastructure.\nYou want to design the system so that each layer communicates only with certain other layers. That\napproach may be easier to enforce if layers are implemented as different class libraries, because you\ncan clearly identify what dependencies are set between libraries. For instance, the domain model layer\nshould not take a dependency on any other layer (the domain model classes should be Plain Old Class\nObjects, or POCO, classes). As shown in Figure 7-6, the Ordering.Domain layer library has\ndependencies only on the .NET libraries or NuGet packages, but not on any other custom library, such\nas data library or persistence library.\n\nFigure 7-6. Layers implemented as libraries allow better control of dependencies between layers\n\nThe domain model layer\n\nEric Evans\u2019s excellent book Domain Driven Design says the following about the domain model layer\nand the application layer.\n\nDomain Model Layer: Responsible for representing concepts of the business, information about the\nbusiness situation, and business rules. State that reflects the business situation is controlled and used\nhere, even though the technical details of storing it are delegated to the infrastructure. This layer is\nthe heart of business software.\n\n196\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe domain model layer is where the business is expressed. When you implement a microservice\ndomain model layer in .NET, that layer is coded as a class library with the domain entities that capture\ndata plus behavior (methods with logic).\n\nFollowing the Persistence Ignorance and the Infrastructure Ignorance principles, this layer must\ncompletely ignore data persistence details. These persistence tasks should be performed by the\ninfrastructure layer. Therefore, this layer should not take direct dependencies on the infrastructure,\nwhich means that an important rule is that your domain model entity classes should be POCOs.\n\nDomain entities should not have any direct dependency (like deriving from a base class) on any data\naccess infrastructure framework like Entity Framework or NHibernate. Ideally, your domain entities\nshould not derive from or implement any type defined in any infrastructure framework.\n\nMost modern ORM frameworks like Entity Framework Core allow this approach, so that your domain\nmodel classes are not coupled to the infrastructure. However, having POCO entities is not always\npossible when using certain NoSQL databases and frameworks, like Actors and Reliable Collections in\nAzure Service Fabric.\n\nEven when it is important to follow the Persistence Ignorance principle for your Domain model, you\nshould not ignore persistence concerns. It is still important to understand the physical data model and\nhow it maps to your entity object model. Otherwise you can create impossible designs.\n\nAlso, this aspect does not mean you can take a model designed for a relational database and directly\nmove it to a NoSQL or document-oriented database. In some entity models, the model might fit, but\nusually it does not. There are still constraints that your entity model must adhere to, based both on\nthe storage technology and ORM technology.\n\nThe application layer\n\nMoving on to the application layer, we can again cite Eric Evans\u2019s book Domain Driven Design:\n\nApplication Layer: Defines the jobs the software is supposed to do and directs the expressive domain\nobjects to work out problems. The tasks this layer is responsible for are meaningful to the business or\nnecessary for interaction with the application layers of other systems. This layer is kept thin. It does\nnot contain business rules or knowledge, but only coordinates tasks and delegates work to\ncollaborations of domain objects in the next layer down. It does not have state reflecting the business\nsituation, but it can have state that reflects the progress of a task for the user or the program.\n\nA microservice\u2019s application layer in .NET is commonly coded as an ASP.NET Core Web API project.\nThe project implements the microservice\u2019s interaction, remote network access, and the external Web\nAPIs used from the UI or client apps. It includes queries if using a CQRS approach, commands\naccepted by the microservice, and even the event-driven communication between microservices\n(integration events). The ASP.NET Core Web API that represents the application layer must not contain\nbusiness rules or domain knowledge (especially domain rules for transactions or updates); these\nshould be owned by the domain model class library. The application layer must only coordinate tasks\nand must not hold or define any domain state (domain model). It delegates the execution of business\nrules to the domain model classes themselves (aggregate roots and domain entities), which will\nultimately update the data within those domain entities.\n\n197\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fBasically, the application logic is where you implement all use cases that depend on a given front end.\nFor example, the implementation related to a Web API service.\n\nThe goal is that the domain logic in the domain model layer, its invariants, the data model, and\nrelated business rules must be completely independent from the presentation and application layers.\nMost of all, the domain model layer must not directly depend on any infrastructure framework.\n\nThe infrastructure layer\n\nThe infrastructure layer is how the data that is initially held in domain entities (in memory) is persisted\nin databases or another persistent store. An example is using Entity Framework Core code to\nimplement the Repository pattern classes that use a DBContext to persist data in a relational\ndatabase.\n\nIn accordance with the previously mentioned Persistence Ignorance and Infrastructure Ignorance\nprinciples, the infrastructure layer must not \u201ccontaminate\u201d the domain model layer. You must keep the\ndomain model entity classes agnostic from the infrastructure that you use to persist data (EF or any\nother framework) by not taking hard dependencies on frameworks. Your domain model layer class\nlibrary should have only your domain code, just POCO entity classes implementing the heart of your\nsoftware and completely decoupled from infrastructure technologies.\n\nThus, your layers or class libraries and projects should ultimately depend on your domain model layer\n(library), not vice versa, as shown in Figure 7-7.\n\nFigure 7-7. Dependencies between layers in DDD\n\nDependencies in a DDD Service, the Application layer depends on Domain and Infrastructure, and\nInfrastructure depends on Domain, but Domain doesn\u2019t depend on any layer. This layer design should\nbe independent for each microservice. As noted earlier, you can implement the most complex\nmicroservices following DDD patterns, while implementing simpler data-driven microservices (simple\nCRUD in a single layer) in a simpler way.\n\n198\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nDevIQ. Persistence Ignorance principle\nhttps://deviq.com/persistence-ignorance/\n\nOren Eini. Infrastructure Ignorance\nhttps://ayende.com/blog/3137/infrastructure-ignorance\n\nAngel Lopez. Layered Architecture In Domain-Driven Design\nhttps://ajlopez.wordpress.com/2008/09/12/layered-architecture-in-domain-driven-design/\n\nDesign a microservice domain model\n\nDefine one rich domain model for each business microservice or Bounded Context.\n\nYour goal is to create a single cohesive domain model for each business microservice or Bounded\nContext (BC). Keep in mind, however, that a BC or business microservice could sometimes be\ncomposed of several physical services that share a single domain model. The domain model must\ncapture the rules, behavior, business language, and constraints of the single Bounded Context or\nbusiness microservice that it represents.\n\nThe Domain Entity pattern\n\nEntities represent domain objects and are primarily defined by their identity, continuity, and\npersistence over time, and not only by the attributes that comprise them. As Eric Evans says, \u201can\nobject primarily defined by its identity is called an Entity.\u201d Entities are very important in the domain\nmodel, since they are the base for a model. Therefore, you should identify and design them carefully.\n\nAn entity\u2019s identity can cross multiple microservices or Bounded Contexts.\n\nThe same identity (that is, the same Id value, although perhaps not the same domain entity) can be\nmodeled across multiple Bounded Contexts or microservices. However, that does not imply that the\nsame entity, with the same attributes and logic would be implemented in multiple Bounded Contexts.\nInstead, entities in each Bounded Context limit their attributes and behaviors to those required in that\nBounded Context\u2019s domain.\n\nFor instance, the buyer entity might have most of a person\u2019s attributes that are defined in the user\nentity in the profile or identity microservice, including the identity. But the buyer entity in the ordering\nmicroservice might have fewer attributes, because only certain buyer data is related to the order\nprocess. The context of each microservice or Bounded Context impacts its domain model.\n\nDomain entities must implement behavior in addition to implementing data attributes.\n\nA domain entity in DDD must implement the domain logic or behavior related to the entity data (the\nobject accessed in memory). For example, as part of an order entity class you must have business logic\nand operations implemented as methods for tasks such as adding an order item, data validation, and\ntotal calculation. The entity\u2019s methods take care of the invariants and rules of the entity instead of\nhaving those rules spread across the application layer.\n\n199\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-8 shows a domain entity that implements not only data attributes but operations or methods\nwith related domain logic.\n\nFigure 7-8. Example of a domain entity design implementing data plus behavior\n\nA domain model entity implements behaviors through methods, that is, it\u2019s not an \u201canemic\u201d model. Of\ncourse, sometimes you can have entities that do not implement any logic as part of the entity class.\nThis can happen in child entities within an aggregate if the child entity does not have any special logic\nbecause most of the logic is defined in the aggregate root. If you have a complex microservice that\nhas logic implemented in the service classes instead of in the domain entities, you could be falling\ninto the anemic domain model, explained in the following section.\n\nRich domain model versus anemic domain model\n\nIn his post AnemicDomainModel, Martin Fowler describes an anemic domain model this way:\n\nThe basic symptom of an Anemic Domain Model is that at first blush it looks like the real thing. There\nare objects, many named after the nouns in the domain space, and these objects are connected with\nthe rich relationships and structure that true domain models have. The catch comes when you look at\nthe behavior, and you realize that there is hardly any behavior on these objects, making them little\nmore than bags of getters and setters.\n\nOf course, when you use an anemic domain model, those data models will be used from a set of\nservice objects (traditionally named the business layer) which capture all the domain or business logic.\nThe business layer sits on top of the data model and uses the data model just as data.\n\nThe anemic domain model is just a procedural style design. Anemic entity objects are not real objects\nbecause they lack behavior (methods). They only hold data properties and thus it is not object-\noriented design. By putting all the behavior out into service objects (the business layer), you\nessentially end up with spaghetti code or transaction scripts, and therefore you lose the advantages\nthat a domain model provides.\n\nRegardless, if your microservice or Bounded Context is very simple (a CRUD service), the anemic\ndomain model in the form of entity objects with just data properties might be good enough, and it\nmight not be worth implementing more complex DDD patterns. In that case, it will be simply a\npersistence model, because you have intentionally created an entity with only data for CRUD\npurposes.\n\n200\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThat is why microservices architectures are perfect for a multi-architectural approach depending on\neach Bounded Context. For instance, in eShopOnContainers, the ordering microservice implements\nDDD patterns, but the catalog microservice, which is a simple CRUD service, does not.\n\nSome people say that the anemic domain model is an anti-pattern. It really depends on what you are\nimplementing. If the microservice you are creating is simple enough (for example, a CRUD service),\nfollowing the anemic domain model it is not an anti-pattern. However, if you need to tackle the\ncomplexity of a microservice\u2019s domain that has a lot of ever-changing business rules, the anemic\ndomain model might be an anti-pattern for that microservice or Bounded Context. In that case,\ndesigning it as a rich model with entities containing data plus behavior as well as implementing\nadditional DDD patterns (aggregates, value objects, etc.) might have huge benefits for the long-term\nsuccess of such a microservice.\n\nAdditional resources\n\n\u2022\n\nDevIQ. Domain Entity\nhttps://deviq.com/entity/\n\n\u2022  Martin Fowler. The Domain Model\n\nhttps://martinfowler.com/eaaCatalog/domainModel.html\n\n\u2022  Martin Fowler. The Anemic Domain Model\n\nhttps://martinfowler.com/bliki/AnemicDomainModel.html\n\nThe Value Object pattern\n\nAs Eric Evans has noted, \u201cMany objects do not have conceptual identity. These objects describe\ncertain characteristics of a thing.\u201d\n\nAn entity requires an identity, but there are many objects in a system that do not, like the Value\nObject pattern. A value object is an object with no conceptual identity that describes a domain aspect.\nThese are objects that you instantiate to represent design elements that only concern you temporarily.\nYou care about what they are, not who they are. Examples include numbers and strings, but can also\nbe higher-level concepts like groups of attributes.\n\nSomething that is an entity in a microservice might not be an entity in another microservice, because\nin the second case, the Bounded Context might have a different meaning. For example, an address in\nan e-commerce application might not have an identity at all, since it might only represent a group of\nattributes of the customer\u2019s profile for a person or company. In this case, the address should be\nclassified as a value object. However, in an application for an electric power utility company, the\ncustomer address could be important for the business domain. Therefore, the address must have an\nidentity so the billing system can be directly linked to the address. In that case, an address should be\nclassified as a domain entity.\n\nA person with a name and surname is usually an entity because a person has identity, even if the\nname and surname coincide with another set of values, such as if those names also refer to a different\nperson.\n\nValue objects are hard to manage in relational databases and ORMs like Entity Framework (EF),\nwhereas in document-oriented databases they are easier to implement and use.\n\n201\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fEF Core 2.0 and later versions include the Owned Entities feature that makes it easier to handle value\nobjects, as we\u2019ll see in detail later on.\n\nAdditional resources\n\n\u2022  Martin Fowler. Value Object pattern\n\nhttps://martinfowler.com/bliki/ValueObject.html\n\n\u2022\n\n\u2022\n\n\u2022\n\nValue Object\nhttps://deviq.com/value-object/\n\nValue Objects in Test-Driven Development\nhttps://leanpub.com/tdd-ebook/read#leanpub-auto-value-objects\n\nEric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software. (Book;\nincludes a discussion of value objects)\nhttps://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-\nSoftware/dp/0321125215/\n\nThe Aggregate pattern\n\nA domain model contains clusters of different data entities and processes that can control a\nsignificant area of functionality, such as order fulfillment or inventory. A more fine-grained DDD unit is\nthe aggregate, which describes a cluster or group of entities and behaviors that can be treated as a\ncohesive unit.\n\nYou usually define an aggregate based on the transactions that you need. A classic example is an\norder that also contains a list of order items. An order item will usually be an entity. But it will be a\nchild entity within the order aggregate, which will also contain the order entity as its root entity,\ntypically called an aggregate root.\n\nIdentifying aggregates can be hard. An aggregate is a group of objects that must be consistent\ntogether, but you cannot just pick a group of objects and label them an aggregate. You must start\nwith a domain concept and think about the entities that are used in the most common transactions\nrelated to that concept. Those entities that need to be transactionally consistent are what forms an\naggregate. Thinking about transaction operations is probably the best way to identify aggregates.\n\nThe Aggregate Root or Root Entity pattern\n\nAn aggregate is composed of at least one entity: the aggregate root, also called root entity or primary\nentity. Additionally, it can have multiple child entities and value objects, with all entities and objects\nworking together to implement required behavior and transactions.\n\nThe purpose of an aggregate root is to ensure the consistency of the aggregate; it should be the only\nentry point for updates to the aggregate through methods or operations in the aggregate root class.\nYou should make changes to entities within the aggregate only via the aggregate root. It is the\naggregate\u2019s consistency guardian, considering all the invariants and consistency rules you might need\nto comply with in your aggregate. If you change a child entity or value object independently, the\n\n202\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\faggregate root cannot ensure that the aggregate is in a valid state. It would be like a table with a\nloose leg. Maintaining consistency is the main purpose of the aggregate root.\n\nIn Figure 7-9, you can see sample aggregates like the buyer aggregate, which contains a single entity\n(the aggregate root Buyer). The order aggregate contains multiple entities and a value object.\n\nFigure 7-9. Example of aggregates with multiple or single entities\n\nA DDD domain model is composed from aggregates, an aggregate can have just one entity or more,\nand can include value objects as well. Note that the Buyer aggregate could have additional child\nentities, depending on your domain, as it does in the ordering microservice in the eShopOnContainers\nreference application. Figure 7-9 just illustrates a case in which the buyer has a single entity, as an\nexample of an aggregate that contains only an aggregate root.\n\nIn order to maintain separation of aggregates and keep clear boundaries between them, it is a good\npractice in a DDD domain model to disallow direct navigation between aggregates and only having\nthe foreign key (FK) field, as implemented in the Ordering microservice domain model in\neShopOnContainers. The Order entity only has a foreign key field for the buyer, but not an EF Core\nnavigation property, as shown in the following code:\n\npublic class Order : Entity, IAggregateRoot\n{\n    private DateTime _orderDate;\n    public Address Address { get; private set; }\n    private int? _buyerId; // FK pointing to a different aggregate root\n    public OrderStatus OrderStatus { get; private set; }\n    private readonly List<OrderItem> _orderItems;\n    public IReadOnlyCollection<OrderItem> OrderItems => _orderItems;\n    // ... Additional code\n}\n\n203\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIdentifying and working with aggregates requires research and experience. For more information, see\nthe following Additional resources list.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nVaughn Vernon. Effective Aggregate Design - Part I: Modeling a Single Aggregate (from\nhttps://dddcommunity.org/)\nhttps://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_1.pdf\n\nVaughn Vernon. Effective Aggregate Design - Part II: Making Aggregates Work\nTogether (from https://dddcommunity.org/)\nhttps://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_2.pdf\n\nVaughn Vernon. Effective Aggregate Design - Part III: Gaining Insight Through\nDiscovery (from https://dddcommunity.org/)\nhttps://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_3.pdf\n\nSergey Grybniak. DDD Tactical Design Patterns\nhttps://www.codeproject.com/Articles/1164363/Domain-Driven-Design-Tactical-Design-\nPatterns-Part\n\nChris Richardson. Developing Transactional Microservices Using Aggregates\nhttps://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson\n\nDevIQ. The Aggregate pattern\nhttps://deviq.com/aggregate-pattern/\n\nImplement a microservice domain model with .NET\n\nIn the previous section, the fundamental design principles and patterns for designing a domain model\nwere explained. Now it\u2019s time to explore possible ways to implement the domain model by using .NET\n(plain C# code) and EF Core. Your domain model will be composed simply of your code. It will have\njust the EF Core model requirements, but not real dependencies on EF. You shouldn\u2019t have hard\ndependencies or references to EF Core or any other ORM in your domain model.\n\nDomain model structure in a custom .NET Standard Library\n\nThe folder organization used for the eShopOnContainers reference application demonstrates the DDD\nmodel for the application. You might find that a different folder organization more clearly\ncommunicates the design choices made for your application. As you can see in Figure 7-10, in the\nordering domain model there are two aggregates, the order aggregate and the buyer aggregate. Each\naggregate is a group of domain entities and value objects, although you could have an aggregate\ncomposed of a single domain entity (the aggregate root or root entity) as well.\n\n204\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-10. Domain model structure for the ordering microservice in eShopOnContainers\n\nAdditionally, the domain model layer includes the repository contracts (interfaces) that are the\ninfrastructure requirements of your domain model. In other words, these interfaces express what\nrepositories and the methods the infrastructure layer must implement. It\u2019s critical that the\nimplementation of the repositories be placed outside of the domain model layer, in the infrastructure\nlayer library, so the domain model layer isn\u2019t \u201ccontaminated\u201d by API or classes from infrastructure\ntechnologies, like Entity Framework.\n\nYou can also see a SeedWork folder that contains custom base classes that you can use as a base for\nyour domain entities and value objects, so you don\u2019t have redundant code in each domain\u2019s object\nclass.\n\nStructure aggregates in a custom .NET Standard library\n\nAn aggregate refers to a cluster of domain objects grouped together to match transactional\nconsistency. Those objects could be instances of entities (one of which is the aggregate root or root\nentity) plus any additional value objects.\n\nTransactional consistency means that an aggregate is guaranteed to be consistent and up to date at\nthe end of a business action. For example, the order aggregate from the eShopOnContainers ordering\nmicroservice domain model is composed as shown in Figure 7-11.\n\n205\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-11. The order aggregate in Visual Studio solution\n\nIf you open any of the files in an aggregate folder, you can see how it\u2019s marked as either a custom\nbase class or interface, like entity or value object, as implemented in the SeedWork folder.\n\nImplement domain entities as POCO classes\n\nYou implement a domain model in .NET by creating POCO classes that implement your domain\nentities. In the following example, the Order class is defined as an entity and also as an aggregate\nroot. Because the Order class derives from the Entity base class, it can reuse common code related to\nentities. Bear in mind that these base classes and interfaces are defined by you in the domain model\nproject, so it is your code, not infrastructure code from an ORM like EF.\n\n// COMPATIBLE WITH ENTITY FRAMEWORK CORE 5.0\n// Entity is a custom base class with the ID\npublic class Order : Entity, IAggregateRoot\n{\n    private DateTime _orderDate;\n    public Address Address { get; private set; }\n    private int? _buyerId;\n\n    public OrderStatus OrderStatus { get; private set; }\n    private int _orderStatusId;\n\n    private string _description;\n    private int? _paymentMethodId;\n\n    private readonly List<OrderItem> _orderItems;\n    public IReadOnlyCollection<OrderItem> OrderItems => _orderItems;\n\n    public Order(string userId, Address address, int cardTypeId, string cardNumber, string\ncardSecurityNumber,\n            string cardHolderName, DateTime cardExpiration, int? buyerId = null, int?\npaymentMethodId = null)\n    {\n        _orderItems = new List<OrderItem>();\n        _buyerId = buyerId;\n        _paymentMethodId = paymentMethodId;\n        _orderStatusId = OrderStatus.Submitted.Id;\n        _orderDate = DateTime.UtcNow;\n\n206\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        Address = address;\n\n        // ...Additional code ...\n    }\n\n    public void AddOrderItem(int productId, string productName,\n                            decimal unitPrice, decimal discount,\n                            string pictureUrl, int units = 1)\n    {\n        //...\n        // Domain rules/logic for adding the OrderItem to the order\n        // ...\n\n        var orderItem = new OrderItem(productId, productName, unitPrice, discount,\npictureUrl, units);\n\n        _orderItems.Add(orderItem);\n\n    }\n    // ...\n    // Additional methods with domain rules/logic related to the Order aggregate\n    // ...\n}\n\nIt\u2019s important to note that this is a domain entity implemented as a POCO class. It doesn\u2019t have any\ndirect dependency on Entity Framework Core or any other infrastructure framework. This\nimplementation is as it should be in DDD, just C# code implementing a domain model.\n\nIn addition, the class is decorated with an interface named IAggregateRoot. That interface is an empty\ninterface, sometimes called a marker interface, that\u2019s used just to indicate that this entity class is also\nan aggregate root.\n\nA marker interface is sometimes considered as an anti-pattern; however, it\u2019s also a clean way to mark\na class, especially when that interface might be evolving. An attribute could be the other choice for\nthe marker, but it\u2019s quicker to see the base class (Entity) next to the IAggregate interface instead of\nputting an Aggregate attribute marker above the class. It\u2019s a matter of preferences, in any case.\n\nHaving an aggregate root means that most of the code related to consistency and business rules of\nthe aggregate\u2019s entities should be implemented as methods in the Order aggregate root class (for\nexample, AddOrderItem when adding an OrderItem object to the aggregate). You should not create\nor update OrderItems objects independently or directly; the AggregateRoot class must keep control\nand consistency of any update operation against its child entities.\n\nEncapsulate data in the Domain Entities\n\nA common problem in entity models is that they expose collection navigation properties as publicly\naccessible list types. This allows any collaborator developer to manipulate the contents of these\ncollection types, which may bypass important business rules related to the collection, possibly leaving\nthe object in an invalid state. The solution to this is to expose read-only access to related collections\nand explicitly provide methods that define ways in which clients can manipulate them.\n\nIn the previous code, note that many attributes are read-only or private and are only updatable by the\nclass methods, so any update considers business domain invariants and logic specified within the class\nmethods.\n\n207\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFor example, following DDD patterns, you should not do the following from any command handler\nmethod or application layer class (actually, it should be impossible for you to do so):\n\n// WRONG ACCORDING TO DDD PATTERNS \u2013 CODE AT THE APPLICATION LAYER OR\n// COMMAND HANDLERS\n// Code in command handler methods or Web API controllers\n//... (WRONG) Some code with business logic out of the domain classes ...\nOrderItem myNewOrderItem = new OrderItem(orderId, productId, productName,\n    pictureUrl, unitPrice, discount, units);\n\n//... (WRONG) Accessing the OrderItems collection directly from the application layer // or\ncommand handlers\nmyOrder.OrderItems.Add(myNewOrderItem);\n//...\n\nIn this case, the Add method is purely an operation to add data, with direct access to the OrderItems\ncollection. Therefore, most of the domain logic, rules, or validations related to that operation with the\nchild entities will be spread across the application layer (command handlers and Web API controllers).\n\nIf you go around the aggregate root, the aggregate root cannot guarantee its invariants, its validity, or\nits consistency. Eventually you\u2019ll have spaghetti code or transactional script code.\n\nTo follow DDD patterns, entities must not have public setters in any entity property. Changes in an\nentity should be driven by explicit methods with explicit ubiquitous language about the change\nthey\u2019re performing in the entity.\n\nFurthermore, collections within the entity (like the order items) should be read-only properties (the\nAsReadOnly method explained later). You should be able to update it only from within the aggregate\nroot class methods or the child entity methods.\n\nAs you can see in the code for the Order aggregate root, all setters should be private or at least read-\nonly externally, so that any operation against the entity\u2019s data or its child entities has to be performed\nthrough methods in the entity class. This maintains consistency in a controlled and object-oriented\nway instead of implementing transactional script code.\n\nThe following code snippet shows the proper way to code the task of adding an OrderItem object to\nthe Order aggregate.\n\n// RIGHT ACCORDING TO DDD--CODE AT THE APPLICATION LAYER OR COMMAND HANDLERS\n// The code in command handlers or WebAPI controllers, related only to application stuff\n// There is NO code here related to OrderItem object's business logic\nmyOrder.AddOrderItem(productId, productName, pictureUrl, unitPrice, discount, units);\n\n// The code related to OrderItem params validations or domain rules should\n// be WITHIN the AddOrderItem method.\n\n//...\n\nIn this snippet, most of the validations or logic related to the creation of an OrderItem object will be\nunder the control of the Order aggregate root\u2014in the AddOrderItem method\u2014especially validations\nand logic related to other elements in the aggregate. For instance, you might get the same product\nitem as the result of multiple calls to AddOrderItem. In that method, you could examine the product\nitems and consolidate the same product items into a single OrderItem object with several units.\n\n208\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditionally, if there are different discount amounts but the product ID is the same, you would likely\napply the higher discount. This principle applies to any other domain logic for the OrderItem object.\n\nIn addition, the new OrderItem(params) operation will also be controlled and performed by the\nAddOrderItem method from the Order aggregate root. Therefore, most of the logic or validations\nrelated to that operation (especially anything that impacts the consistency between other child\nentities) will be in a single place within the aggregate root. That is the ultimate purpose of the\naggregate root pattern.\n\nWhen you use Entity Framework Core 1.1 or later, a DDD entity can be better expressed because it\nallows mapping to fields in addition to properties. This is useful when protecting collections of child\nentities or value objects. With this enhancement, you can use simple private fields instead of\nproperties and you can implement any update to the field collection in public methods and provide\nread-only access through the AsReadOnly method.\n\nIn DDD, you want to update the entity only through methods in the entity (or the constructor) in order\nto control any invariant and the consistency of the data, so properties are defined only with a get\naccessor. The properties are backed by private fields. Private members can only be accessed from\nwithin the class. However, there is one exception: EF Core needs to set these fields as well (so it can\nreturn the object with the proper values).\n\nMap properties with only get accessors to the fields in the database table\n\nMapping properties to database table columns is not a domain responsibility but part of the\ninfrastructure and persistence layer. We mention this here just so you\u2019re aware of the new capabilities\nin EF Core 1.1 or later related to how you can model entities. Additional details on this topic are\nexplained in the infrastructure and persistence section.\n\nWhen you use EF Core 1.0 or later, within the DbContext you need to map the properties that are\ndefined only with getters to the actual fields in the database table. This is done with the HasField\nmethod of the PropertyBuilder class.\n\nMap fields without properties\n\nWith the feature in EF Core 1.1 or later to map columns to fields, it\u2019s also possible to not use\nproperties. Instead, you can just map columns from a table to fields. A common use case for this is\nprivate fields for an internal state that doesn\u2019t need to be accessed from outside the entity.\n\nFor example, in the preceding OrderAggregate code example, there are several private fields, like the\n_paymentMethodId field, that have no related property for either a setter or getter. That field could\nalso be calculated within the order\u2019s business logic and used from the order\u2019s methods, but it needs\nto be persisted in the database as well. So in EF Core (since v1.1), there\u2019s a way to map a field without\na related property to a column in the database. This is also explained in the Infrastructure layer section\nof this guide.\n\n209\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nVaughn Vernon. Modeling Aggregates with DDD and Entity Framework. Note that this is\nnot Entity Framework Core.\nhttps://kalele.io/blog-posts/modeling-aggregates-with-ddd-and-entity-framework/\n\nJulie Lerman. Data Points - Coding for Domain-Driven Design: Tips for Data-Focused\nDevs\nhttps://learn.microsoft.com/archive/msdn-magazine/2013/august/data-points-coding-for-\ndomain-driven-design-tips-for-data-focused-devs\n\nUdi Dahan. How to create fully encapsulated Domain Models\nhttps://udidahan.com/2008/02/29/how-to-create-fully-encapsulated-domain-models/\n\nSteve Smith. What is the difference between a DTO and a POCO?  https://ardalis.com/dto-\nor-poco/\n\nSeedwork (reusable base classes and interfaces for\nyour domain model)\n\nThe solution folder contains a SeedWork folder. This folder contains custom base classes that you can\nuse as a base for your domain entities and value objects. Use these base classes so you don\u2019t have\nredundant code in each domain\u2019s object class. The folder for these types of classes is called SeedWork\nand not something like Framework. It\u2019s called SeedWork because the folder contains just a small\nsubset of reusable classes that cannot really be considered a framework. Seedwork is a term\nintroduced by Michael Feathers and popularized by Martin Fowler but you could also name that\nfolder Common, SharedKernel, or similar.\n\nFigure 7-12 shows the classes that form the seedwork of the domain model in the ordering\nmicroservice. It has a few custom base classes like Entity, ValueObject, and Enumeration, plus a few\ninterfaces. These interfaces (IRepository and IUnitOfWork) inform the infrastructure layer about what\nneeds to be implemented. Those interfaces are also used through Dependency Injection from the\napplication layer.\n\nFigure 7-12. A sample set of domain model \u201cseedwork\u201d base classes and interfaces\n\nThis is the type of copy and paste reuse that many developers share between projects, not a formal\nframework. You can have seedworks in any layer or library. However, if the set of classes and\ninterfaces gets large enough, you might want to create a single class library.\n\n210\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe custom Entity base class\n\nThe following code is an example of an Entity base class where you can place code that can be used\nthe same way by any domain entity, such as the entity ID, equality operators, a domain event list per\nentity, etc.\n\n// COMPATIBLE WITH ENTITY FRAMEWORK CORE (1.1 and later)\npublic abstract class Entity\n{\n    int? _requestedHashCode;\n    int _Id;\n    private List<INotification> _domainEvents;\n    public virtual int Id\n    {\n        get\n        {\n            return _Id;\n        }\n        protected set\n        {\n            _Id = value;\n        }\n    }\n\n    public List<INotification> DomainEvents => _domainEvents;\n    public void AddDomainEvent(INotification eventItem)\n    {\n        _domainEvents = _domainEvents ?? new List<INotification>();\n        _domainEvents.Add(eventItem);\n    }\n    public void RemoveDomainEvent(INotification eventItem)\n    {\n        if (_domainEvents is null) return;\n        _domainEvents.Remove(eventItem);\n    }\n\n    public bool IsTransient()\n    {\n        return this.Id == default(Int32);\n    }\n\n    public override bool Equals(object obj)\n    {\n        if (obj == null || !(obj is Entity))\n            return false;\n        if (Object.ReferenceEquals(this, obj))\n            return true;\n        if (this.GetType() != obj.GetType())\n            return false;\n        Entity item = (Entity)obj;\n        if (item.IsTransient() || this.IsTransient())\n            return false;\n        else\n            return item.Id == this.Id;\n    }\n\n    public override int GetHashCode()\n    {\n        if (!IsTransient())\n\n211\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        {\n            if (!_requestedHashCode.HasValue)\n                _requestedHashCode = this.Id.GetHashCode() ^ 31;\n            // XOR for random distribution. See:\n            // https://learn.microsoft.com/archive/blogs/ericlippert/guidelines-and-rules-\nfor-gethashcode\n            return _requestedHashCode.Value;\n        }\n        else\n            return base.GetHashCode();\n    }\n    public static bool operator ==(Entity left, Entity right)\n    {\n        if (Object.Equals(left, null))\n            return (Object.Equals(right, null));\n        else\n            return left.Equals(right);\n    }\n    public static bool operator !=(Entity left, Entity right)\n    {\n        return !(left == right);\n    }\n}\n\nThe previous code using a domain event list per entity will be explained in the next sections when\nfocusing on domain events.\n\nRepository contracts (interfaces) in the domain model layer\n\nRepository contracts are simply .NET interfaces that express the contract requirements of the\nrepositories to be used for each aggregate.\n\nThe repositories themselves, with EF Core code or any other infrastructure dependencies and code\n(Linq, SQL, etc.), must not be implemented within the domain model; the repositories should only\nimplement the interfaces you define in the domain model.\n\nA pattern related to this practice (placing the repository interfaces in the domain model layer) is the\nSeparated Interface pattern. As explained by Martin Fowler, \u201cUse Separated Interface to define an\ninterface in one package but implement it in another. This way a client that needs the dependency to\nthe interface can be completely unaware of the implementation.\u201d\n\nFollowing the Separated Interface pattern enables the application layer (in this case, the Web API\nproject for the microservice) to have a dependency on the requirements defined in the domain model,\nbut not a direct dependency to the infrastructure/persistence layer. In addition, you can use\nDependency Injection to isolate the implementation, which is implemented in the infrastructure/\npersistence layer using repositories.\n\nFor example, the following example with the IOrderRepository interface defines what operations the\nOrderRepository class will need to implement at the infrastructure layer. In the current\nimplementation of the application, the code just needs to add or update orders to the database, since\nqueries are split following the simplified CQRS approach.\n\n// Defined at IOrderRepository.cs\npublic interface IOrderRepository : IRepository<Order>\n{\n\n212\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    Order Add(Order order);\n\n    void Update(Order order);\n\n    Task<Order> GetAsync(int orderId);\n}\n\n// Defined at IRepository.cs (Part of the Domain Seedwork)\npublic interface IRepository<T> where T : IAggregateRoot\n{\n    IUnitOfWork UnitOfWork { get; }\n}\n\nAdditional resources\n\n\u2022  Martin Fowler. Separated Interface.\n\nhttps://www.martinfowler.com/eaaCatalog/separatedInterface.html\n\nImplement value objects\n\nAs discussed in earlier sections about entities and aggregates, identity is fundamental for entities.\nHowever, there are many objects and data items in a system that do not require an identity and\nidentity tracking, such as value objects.\n\nA value object can reference other entities. For example, in an application that generates a route that\ndescribes how to get from one point to another, that route would be a value object. It would be a\nsnapshot of points on a specific route, but this suggested route would not have an identity, even\nthough internally it might refer to entities like City, Road, etc.\n\nFigure 7-13 shows the Address value object within the Order aggregate.\n\n213\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-13. Address value object within the Order aggregate\n\nAs shown in Figure 7-13, an entity is usually composed of multiple attributes. For example, the Order\nentity can be modeled as an entity with an identity and composed internally of a set of attributes such\nas OrderId, OrderDate, OrderItems, etc. But the address, which is simply a complex-value composed of\ncountry/region, street, city, etc., and has no identity in this domain, must be modeled and treated as a\nvalue object.\n\nImportant characteristics of value objects\n\nThere are two main characteristics for value objects:\n\n\u2022\n\n\u2022\n\nThey have no identity.\n\nThey are immutable.\n\nThe first characteristic was already discussed. Immutability is an important requirement. The values of\na value object must be immutable once the object is created. Therefore, when the object is\n\n214\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fconstructed, you must provide the required values, but you must not allow them to change during the\nobject\u2019s lifetime.\n\nValue objects allow you to perform certain tricks for performance, thanks to their immutable nature.\nThis is especially true in systems where there may be thousands of value object instances, many of\nwhich have the same values. Their immutable nature allows them to be reused; they can be\ninterchangeable objects, since their values are the same and they have no identity. This type of\noptimization can sometimes make a difference between software that runs slowly and software with\ngood performance. Of course, all these cases depend on the application environment and deployment\ncontext.\n\nValue object implementation in C#\n\nIn terms of implementation, you can have a value object base class that has basic utility methods like\nequality based on the comparison between all the attributes (since a value object must not be based\non identity) and other fundamental characteristics. The following example shows a value object base\nclass used in the ordering microservice from eShopOnContainers.\n\npublic abstract class ValueObject\n{\n    protected static bool EqualOperator(ValueObject left, ValueObject right)\n    {\n        if (ReferenceEquals(left, null) ^ ReferenceEquals(right, null))\n        {\n            return false;\n        }\n        return ReferenceEquals(left, right) || left.Equals(right);\n    }\n\n    protected static bool NotEqualOperator(ValueObject left, ValueObject right)\n    {\n        return !(EqualOperator(left, right));\n    }\n\n    protected abstract IEnumerable<object> GetEqualityComponents();\n\n    public override bool Equals(object obj)\n    {\n        if (obj == null || obj.GetType() != GetType())\n        {\n            return false;\n        }\n\n        var other = (ValueObject)obj;\n\n        return this.GetEqualityComponents().SequenceEqual(other.GetEqualityComponents());\n    }\n\n    public override int GetHashCode()\n    {\n        return GetEqualityComponents()\n            .Select(x => x != null ? x.GetHashCode() : 0)\n            .Aggregate((x, y) => x ^ y);\n    }\n    // Other utility methods\n}\n\n215\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe ValueObject is an abstract class type, but in this example, it doesn\u2019t overload the == and !=\noperators. You could choose to do so, making comparisons delegate to the Equals override. For\nexample, consider the following operator overloads to the ValueObject type:\n\npublic static bool operator ==(ValueObject one, ValueObject two)\n{\n    return EqualOperator(one, two);\n}\n\npublic static bool operator !=(ValueObject one, ValueObject two)\n{\n    return NotEqualOperator(one, two);\n}\nYou can use this class when implementing your actual value object, as with the Address\nvalue object shown in the following example:\npublic class Address : ValueObject\n{\n    public String Street { get; private set; }\n    public String City { get; private set; }\n    public String State { get; private set; }\n    public String Country { get; private set; }\n    public String ZipCode { get; private set; }\n\n    public Address() { }\n\n    public Address(string street, string city, string state, string country, string\nzipcode)\n    {\n        Street = street;\n        City = city;\n        State = state;\n        Country = country;\n        ZipCode = zipcode;\n    }\n\n    protected override IEnumerable<object> GetEqualityComponents()\n    {\n        // Using a yield return statement to return each element one at a time\n        yield return Street;\n        yield return City;\n        yield return State;\n        yield return Country;\n        yield return ZipCode;\n    }\n}\n\nThis value object implementation of Address has no identity, and therefore no ID field is defined for it,\neither in the Address class definition or the ValueObject class definition.\n\nHaving no ID field in a class to be used by Entity Framework (EF) was not possible until EF Core 2.0,\nwhich greatly helps to implement better value objects with no ID. That is precisely the explanation of\nthe next section.\n\nIt could be argued that value objects, being immutable, should be read-only (that is, have get-only\nproperties), and that\u2019s indeed true. However, value objects are usually serialized and deserialized to go\n\n216\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fthrough message queues, and being read-only stops the deserializer from assigning values, so you\njust leave them as private set, which is read-only enough to be practical.\n\nValue object comparison semantics\n\nTwo instances of the Address type can be compared using all the following methods:\n\nvar one = new Address(\"1 Microsoft Way\", \"Redmond\", \"WA\", \"US\", \"98052\");\nvar two = new Address(\"1 Microsoft Way\", \"Redmond\", \"WA\", \"US\", \"98052\");\n\nConsole.WriteLine(EqualityComparer<Address>.Default.Equals(one, two)); // True\nConsole.WriteLine(object.Equals(one, two)); // True\nConsole.WriteLine(one.Equals(two)); // True\nConsole.WriteLine(one == two); // True\n\nWhen all the values are the same, the comparisons are correctly evaluated as true. If you didn\u2019t choose\nto overload the == and != operators, then the last comparison of one == two would evaluate as false.\nFor more information, see Overload ValueObject equality operators.\n\nHow to persist value objects in the database with EF Core 2.0 and later\n\nYou just saw how to define a value object in your domain model. But how can you actually persist it\ninto the database using Entity Framework Core since it usually targets entities with identity?\n\nBackground and older approaches using EF Core 1.1\n\nAs background, a limitation when using EF Core 1.0 and 1.1 was that you could not use complex types\nas defined in EF 6.x in the traditional .NET Framework. Therefore, if using EF Core 1.0 or 1.1, you\nneeded to store your value object as an EF entity with an ID field. Then, so it looked more like a value\nobject with no identity, you could hide its ID so you make clear that the identity of a value object is\nnot important in the domain model. You could hide that ID by using the ID as a shadow property.\nSince that configuration for hiding the ID in the model is set up in the EF infrastructure level, it would\nbe kind of transparent for your domain model.\n\nIn the initial version of eShopOnContainers (.NET Core 1.1), the hidden ID needed by EF Core\ninfrastructure was implemented in the following way in the DbContext level, using Fluent API at the\ninfrastructure project. Therefore, the ID was hidden from the domain model point of view, but still\npresent in the infrastructure.\n\n// Old approach with EF Core 1.1\n// Fluent API within the OrderingContext:DbContext in the Infrastructure project\nvoid ConfigureAddress(EntityTypeBuilder<Address> addressConfiguration)\n{\n    addressConfiguration.ToTable(\"address\", DEFAULT_SCHEMA);\n\n    addressConfiguration.Property<int>(\"Id\")  // Id is a shadow property\n        .IsRequired();\n    addressConfiguration.HasKey(\"Id\");   // Id is a shadow property\n}\n\nHowever, the persistence of that value object into the database was performed like a regular entity in\na different table.\n\n217\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fWith EF Core 2.0 and later, there are new and better ways to persist value objects.\n\nPersist value objects as owned entity types in EF Core 2.0 and later\n\nEven with some gaps between the canonical value object pattern in DDD and the owned entity type in\nEF Core, it\u2019s currently the best way to persist value objects with EF Core 2.0 and later. You can see\nlimitations at the end of this section.\n\nThe owned entity type feature was added to EF Core since version 2.0.\n\nAn owned entity type allows you to map types that do not have their own identity explicitly defined in\nthe domain model and are used as properties, such as a value object, within any of your entities. An\nowned entity type shares the same CLR type with another entity type (that is, it\u2019s just a regular class).\nThe entity containing the defining navigation is the owner entity. When querying the owner, the\nowned types are included by default.\n\nJust by looking at the domain model, an owned type looks like it doesn\u2019t have any identity. However,\nunder the covers, owned types do have the identity, but the owner navigation property is part of this\nidentity.\n\nThe identity of instances of owned types is not completely their own. It consists of three components:\n\n\u2022\n\n\u2022\n\n\u2022\n\nThe identity of the owner\n\nThe navigation property pointing to them\n\nIn the case of collections of owned types, an independent component (supported in EF Core\n2.2 and later).\n\nFor example, in the Ordering domain model at eShopOnContainers, as part of the Order entity, the\nAddress value object is implemented as an owned entity type within the owner entity, which is the\nOrder entity. Address is a type with no identity property defined in the domain model. It is used as a\nproperty of the Order type to specify the shipping address for a particular order.\n\nBy convention, a shadow primary key is created for the owned type and it will be mapped to the same\ntable as the owner by using table splitting. This allows to use owned types similarly to how complex\ntypes are used in EF6 in the traditional .NET Framework.\n\nIt is important to note that owned types are never discovered by convention in EF Core, so you have\nto declare them explicitly.\n\nIn eShopOnContainers, in the OrderingContext.cs file, within the OnModelCreating() method, multiple\ninfrastructure configurations are applied. One of them is related to the Order entity.\n\n// Part of the OrderingContext.cs class at the Ordering.Infrastructure project\n//\nprotected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n    modelBuilder.ApplyConfiguration(new ClientRequestEntityTypeConfiguration());\n    modelBuilder.ApplyConfiguration(new PaymentMethodEntityTypeConfiguration());\n    modelBuilder.ApplyConfiguration(new OrderEntityTypeConfiguration());\n    modelBuilder.ApplyConfiguration(new OrderItemEntityTypeConfiguration());\n    //...Additional type configurations\n}\n\n218\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIn the following code, the persistence infrastructure is defined for the Order entity:\n\n// Part of the OrderEntityTypeConfiguration.cs class\n//\npublic void Configure(EntityTypeBuilder<Order> orderConfiguration)\n{\n    orderConfiguration.ToTable(\"orders\", OrderingContext.DEFAULT_SCHEMA);\n    orderConfiguration.HasKey(o => o.Id);\n    orderConfiguration.Ignore(b => b.DomainEvents);\n    orderConfiguration.Property(o => o.Id)\n        .ForSqlServerUseSequenceHiLo(\"orderseq\", OrderingContext.DEFAULT_SCHEMA);\n\n    //Address value object persisted as owned entity in EF Core 2.0\n    orderConfiguration.OwnsOne(o => o.Address);\n\n    orderConfiguration.Property<DateTime>(\"OrderDate\").IsRequired();\n\n    //...Additional validations, constraints and code...\n    //...\n}\n\nIn the previous code, the orderConfiguration.OwnsOne(o => o.Address) method specifies that the\nAddress property is an owned entity of the Order type.\n\nBy default, EF Core conventions name the database columns for the properties of the owned entity\ntype as EntityProperty_OwnedEntityProperty. Therefore, the internal properties of Address will appear\nin the Orders table with the names Address_Street, Address_City (and so on for State, Country, and\nZipCode).\n\nYou can append the Property().HasColumnName() fluent method to rename those columns. In the\ncase where Address is a public property, the mappings would be like the following:\n\norderConfiguration.OwnsOne(p => p.Address)\n                            .Property(p=>p.Street).HasColumnName(\"ShippingStreet\");\n\norderConfiguration.OwnsOne(p => p.Address)\n                            .Property(p=>p.City).HasColumnName(\"ShippingCity\");\n\nIt\u2019s possible to chain the OwnsOne method in a fluent mapping. In the following hypothetical\nexample, OrderDetails owns BillingAddress and ShippingAddress, which are both Address types. Then\nOrderDetails is owned by the Order type.\n\norderConfiguration.OwnsOne(p => p.OrderDetails, cb =>\n    {\n        cb.OwnsOne(c => c.BillingAddress);\n        cb.OwnsOne(c => c.ShippingAddress);\n    });\n//...\n//...\npublic class Order\n{\n    public int Id { get; set; }\n    public OrderDetails OrderDetails { get; set; }\n}\n\npublic class OrderDetails\n{\n    public Address BillingAddress { get; set; }\n\n219\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    public Address ShippingAddress { get; set; }\n}\n\npublic class Address\n{\n    public string Street { get; set; }\n    public string City { get; set; }\n}\n\nAdditional details on owned entity types\n\n\u2022\n\n\u2022\n\n\u2022\n\nOwned types are defined when you configure a navigation property to a particular type using\nthe OwnsOne fluent API.\n\nThe definition of an owned type in our metadata model is a composite of: the owner type, the\nnavigation property, and the CLR type of the owned type.\n\nThe identity (key) of an owned type instance in our stack is a composite of the identity of the\nowner type and the definition of the owned type.\n\nOwned entities capabilities\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nOwned types can reference other entities, either owned (nested owned types) or non-owned\n(regular reference navigation properties to other entities).\n\nYou can map the same CLR type as different owned types in the same owner entity through\nseparate navigation properties.\n\nTable splitting is set up by convention, but you can opt out by mapping the owned type to a\ndifferent table using ToTable.\n\nEager loading is performed automatically on owned types, that is, there\u2019s no need to call\n.Include() on the query.\n\nCan be configured with attribute [Owned], using EF Core 2.1 and later.\n\nCan handle collections of owned types (using version 2.2 and later).\n\nOwned entities limitations\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nYou can\u2019t create a DbSet<T> of an owned type (by design).\n\nYou can\u2019t call ModelBuilder.Entity<T>() on owned types (currently by design).\n\nNo support for optional (that is, nullable) owned types that are mapped with the owner in the\nsame table (that is, using table splitting). This is because mapping is done for each property,\nthere is no separate sentinel for the null complex value as a whole.\n\nNo inheritance-mapping support for owned types, but you should be able to map two leaf\ntypes of the same inheritance hierarchies as different owned types. EF Core will not reason\nabout the fact that they are part of the same hierarchy.\n\n220\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fMain differences with EF6\u2019s complex types\n\n\u2022\n\nTable splitting is optional, that is, they can optionally be mapped to a separate table and still\nbe owned types.\n\nAdditional resources\n\n\u2022  Martin Fowler. ValueObject pattern\n\nhttps://martinfowler.com/bliki/ValueObject.html\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nEric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software. (Book;\nincludes a discussion of value objects)\nhttps://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-\nSoftware/dp/0321125215/\n\nVaughn Vernon. Implementing Domain-Driven Design. (Book; includes a discussion of\nvalue objects)\nhttps://www.amazon.com/Implementing-Domain-Driven-Design-Vaughn-\nVernon/dp/0321834577/\n\nOwned Entity Types\nhttps://learn.microsoft.com/ef/core/modeling/owned-entities\n\nShadow Properties\nhttps://learn.microsoft.com/ef/core/modeling/shadow-properties\n\nComplex types and/or value objects. Discussion in the EF Core GitHub repo (Issues tab)\nhttps://github.com/dotnet/efcore/issues/246\n\nValueObject.cs. Base value object class in eShopOnContainers.\nhttps://github.com/dotnet-\narchitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/SeedWor\nk/ValueObject.cs\n\nValueObject.cs. Base value object class in CSharpFunctionalExtensions.\nhttps://github.com/vkhorikov/CSharpFunctionalExtensions/blob/master/CSharpFunctionalExte\nnsions/ValueObject/ValueObject.cs\n\nAddress class. Sample value object class in eShopOnContainers.\nhttps://github.com/dotnet-\narchitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/Aggregat\nesModel/OrderAggregate/Address.cs\n\nUse enumeration classes instead of enum types\n\nEnumerations (or enum types for short) are a thin language wrapper around an integral type. You\nmight want to limit their use to when you are storing one value from a closed set of values.\nClassification based on sizes (small, medium, large) is a good example. Using enums for control flow\nor more robust abstractions can be a code smell. This type of usage leads to fragile code with many\ncontrol flow statements checking values of the enum.\n\n221\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fInstead, you can create Enumeration classes that enable all the rich features of an object-oriented\nlanguage.\n\nHowever, this isn\u2019t a critical topic and in many cases, for simplicity, you can still use regular enum\ntypes if that\u2019s your preference. The use of enumeration classes is more related to business-related\nconcepts.\n\nImplement an Enumeration base class\n\nThe ordering microservice in eShopOnContainers provides a sample Enumeration base class\nimplementation, as shown in the following example:\n\npublic abstract class Enumeration : IComparable\n{\n    public string Name { get; private set; }\n\n    public int Id { get; private set; }\n\n    protected Enumeration(int id, string name) => (Id, Name) = (id, name);\n\n    public override string ToString() => Name;\n\n    public static IEnumerable<T> GetAll<T>() where T : Enumeration =>\n        typeof(T).GetFields(BindingFlags.Public |\n                            BindingFlags.Static |\n                            BindingFlags.DeclaredOnly)\n                 .Select(f => f.GetValue(null))\n                 .Cast<T>();\n\n    public override bool Equals(object obj)\n    {\n        if (obj is not Enumeration otherValue)\n        {\n            return false;\n        }\n\n        var typeMatches = GetType().Equals(obj.GetType());\n        var valueMatches = Id.Equals(otherValue.Id);\n\n        return typeMatches && valueMatches;\n    }\n\n    public int CompareTo(object other) => Id.CompareTo(((Enumeration)other).Id);\n\n    // Other utility methods ...\n}\nYou can use this class as a type in any entity or value object, as for the following\nCardType : Enumeration class:\npublic class CardType\n    : Enumeration\n{\n    public static CardType Amex = new(1, nameof(Amex));\n    public static CardType Visa = new(2, nameof(Visa));\n    public static CardType MasterCard = new(3, nameof(MasterCard));\n\n    public CardType(int id, string name)\n        : base(id, name)\n    {\n\n222\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    }\n}\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nJimmy Bogard. Enumeration classes\nhttps://lostechies.com/jimmybogard/2008/08/12/enumeration-classes/\n\nSteve Smith. Enum Alternatives in C#\nhttps://ardalis.com/enum-alternatives-in-c\n\nEnumeration.cs. Base Enumeration class in eShopOnContainers\nhttps://github.com/dotnet-\narchitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/SeedWor\nk/Enumeration.cs\n\nCardType.cs. Sample Enumeration class in eShopOnContainers.\nhttps://github.com/dotnet-\narchitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/Aggregat\nesModel/BuyerAggregate/CardType.cs\n\nSmartEnum. Ardalis - Classes to help produce strongly typed smarter enums in .NET.\nhttps://www.nuget.org/packages/Ardalis.SmartEnum/\n\nDesign validations in the domain model layer\n\nIn DDD, validation rules can be thought as invariants. The main responsibility of an aggregate is to\nenforce invariants across state changes for all the entities within that aggregate.\n\nDomain entities should always be valid entities. There are a certain number of invariants for an object\nthat should always be true. For example, an order item object always has to have a quantity that must\nbe a positive integer, plus an article name and price. Therefore, invariants enforcement is the\nresponsibility of the domain entities (especially of the aggregate root) and an entity object should not\nbe able to exist without being valid. Invariant rules are simply expressed as contracts, and exceptions\nor notifications are raised when they are violated.\n\nThe reasoning behind this is that many bugs occur because objects are in a state they should never\nhave been in.\n\nLet\u2019s propose we now have a SendUserCreationEmailService that takes a UserProfile \u2026 how can we\nrationalize in that service that Name is not null? Do we check it again? Or more likely \u2026 you just don\u2019t\nbother to check and \u201chope for the best\u201d\u2014you hope that someone bothered to validate it before\nsending it to you. Of course, using TDD one of the first tests we should be writing is that if I send a\ncustomer with a null name that it should raise an error. But once we start writing these kinds of tests\nover and over again we realize \u2026 \u201cwhat if we never allowed name to become null? we wouldn\u2019t have\nall of these tests!\u201d.\n\n223\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fImplement validations in the domain model layer\n\nValidations are usually implemented in domain entity constructors or in methods that can update the\nentity. There are multiple ways to implement validations, such as verifying data and raising exceptions\nif the validation fails. There are also more advanced patterns such as using the Specification pattern\nfor validations, and the Notification pattern to return a collection of errors instead of returning an\nexception for each validation as it occurs.\n\nValidate conditions and throw exceptions\n\nThe following code example shows the simplest approach to validation in a domain entity by raising\nan exception. In the references table at the end of this section you can see links to more advanced\nimplementations based on the patterns we have discussed previously.\n\npublic void SetAddress(Address address)\n{\n    _shippingAddress = address?? throw new ArgumentNullException(nameof(address));\n}\nA better example would demonstrate the need to ensure that either the internal state did\nnot change, or that all the mutations for a method occurred. For example, the following\nimplementation would leave the object in an invalid state:\npublic void SetAddress(string line1, string line2,\n    string city, string state, int zip)\n{\n    _shippingAddress.line1 = line1 ?? throw new ...\n    _shippingAddress.line2 = line2;\n    _shippingAddress.city = city ?? throw new ...\n    _shippingAddress.state = (IsValid(state) ? state : throw new \u2026);\n}\n\nIf the value of the state is invalid, the first address line and the city have already been changed. That\nmight make the address invalid.\n\nA similar approach can be used in the entity\u2019s constructor, raising an exception to make sure that the\nentity is valid once it is created.\n\nUse validation attributes in the model based on data annotations\n\nData annotations, like the Required or MaxLength attributes, can be used to configure EF Core\ndatabase field properties, as explained in detail in the Table mapping section, but they no longer work\nfor entity validation in EF Core (neither does the IValidatableObject.Validate method), as they have\ndone since EF 4.x in .NET Framework.\n\nData annotations and the IValidatableObject interface can still be used for model validation during\nmodel binding, prior to the controller\u2019s actions invocation as usual, but that model is meant to be a\nViewModel or DTO and that\u2019s an MVC or API concern not a domain model concern.\n\nHaving made the conceptual difference clear, you can still use data annotations and\nIValidatableObject in the entity class for validation, if your actions receive an entity class object\nparameter, which is not recommended. In that case, validation will occur upon model binding, just\nbefore invoking the action and you can check the controller\u2019s ModelState.IsValid property to check\n\n224\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fthe result, but then again, it happens in the controller, not before persisting the entity object in the\nDbContext, as it had done since EF 4.x.\n\nYou can still implement custom validation in the entity class using data annotations and the\nIValidatableObject.Validate method, by overriding the DbContext\u2019s SaveChanges method.\n\nYou can see a sample implementation for validating IValidatableObject entities in this comment on\nGitHub. That sample doesn\u2019t do attribute-based validations, but they should be easy to implement\nusing reflection in the same override.\n\nHowever, from a DDD point of view, the domain model is best kept lean with the use of exceptions in\nyour entity\u2019s behavior methods, or by implementing the Specification and Notification patterns to\nenforce validation rules.\n\nIt can make sense to use data annotations at the application layer in ViewModel classes (instead of\ndomain entities) that will accept input, to allow for model validation within the UI layer. However, this\nshould not be done at the exclusion of validation within the domain model.\n\nValidate entities by implementing the Specification pattern and the Notification\npattern\n\nFinally, a more elaborate approach to implementing validations in the domain model is by\nimplementing the Specification pattern in conjunction with the Notification pattern, as explained in\nsome of the additional resources listed later.\n\nIt is worth mentioning that you can also use just one of those patterns\u2014for example, validating\nmanually with control statements, but using the Notification pattern to stack and return a list of\nvalidation errors.\n\nUse deferred validation in the domain\n\nThere are various approaches to deal with deferred validations in the domain. In his book\nImplementing Domain-Driven Design, Vaughn Vernon discusses these in the section on validation.\n\nTwo-step validation\n\nAlso consider two-step validation. Use field-level validation on your command Data Transfer Objects\n(DTOs) and domain-level validation inside your entities. You can do this by returning a result object\ninstead of exceptions in order to make it easier to deal with the validation errors.\n\nUsing field validation with data annotations, for example, you do not duplicate the validation\ndefinition. The execution, though, can be both server-side and client-side in the case of DTOs\n(commands and ViewModels, for instance).\n\nAdditional resources\n\n\u2022\n\n\u2022\n\nRachel Appel. Introduction to model validation in ASP.NET Core MVC\nhttps://learn.microsoft.com/aspnet/core/mvc/models/validation\n\nRick Anderson. Adding validation\nhttps://learn.microsoft.com/aspnet/core/tutorials/first-mvc-app/validation\n\n225\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f\u2022  Martin Fowler. Replacing Throwing Exceptions with Notification in Validations\n\nhttps://martinfowler.com/articles/replaceThrowWithNotification.html\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nSpecification and Notification Patterns\nhttps://www.codeproject.com/Tips/790758/Specification-and-Notification-Patterns\n\nLev Gorodinski. Validation in Domain-Driven Design (DDD)\nhttp://gorodinski.com/blog/2012/05/19/validation-in-domain-driven-design-ddd/\n\nColin Jack. Domain Model Validation\nhttps://colinjack.blogspot.com/2008/03/domain-model-validation.html\n\nJimmy Bogard. Validation in a DDD world\nhttps://lostechies.com/jimmybogard/2009/02/15/validation-in-a-ddd-world/\n\nClient-side validation (validation in the presentation\nlayers)\n\nEven when the source of truth is the domain model and ultimately you must have validation at the\ndomain model level, validation can still be handled at both the domain model level (server side) and\nthe UI (client side).\n\nClient-side validation is a great convenience for users. It saves time they would otherwise spend\nwaiting for a round trip to the server that might return validation errors. In business terms, even a few\nfractions of seconds multiplied hundreds of times each day adds up to a lot of time, expense, and\nfrustration. Straightforward and immediate validation enables users to work more efficiently and\nproduce better quality input and output.\n\nJust as the view model and the domain model are different, view model validation and domain model\nvalidation might be similar but serve a different purpose. If you are concerned about DRY (the Don\u2019t\nRepeat Yourself principle), consider that in this case code reuse might also mean coupling, and in\nenterprise applications it is more important not to couple the server side to the client side than to\nfollow the DRY principle.\n\nEven when using client-side validation, you should always validate your commands or input DTOs in\nserver code, because the server APIs are a possible attack vector. Usually, doing both is your best bet\nbecause if you have a client application, from a UX perspective, it is best to be proactive and not allow\nthe user to enter invalid information.\n\nTherefore, in client-side code you typically validate the ViewModels. You could also validate the client\noutput DTOs or commands before you send them to the services.\n\nThe implementation of client-side validation depends on what kind of client application you are\nbuilding. It will be different if you are validating data in a web MVC web application with most of the\ncode in .NET, a SPA web application with that validation being coded in JavaScript or TypeScript, or a\nmobile app coded with Xamarin and C#.\n\n226\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditional resources\n\nValidation in Xamarin mobile apps\n\n\u2022\n\n\u2022\n\nValidate Text Input and Show Errors\nhttps://developer.xamarin.com/recipes/ios/standard_controls/text_field/validate_input/\n\nValidation Callback\nhttps://developer.xamarin.com/samples/xamarin-forms/XAML/ValidationCallback/\n\nValidation in ASP.NET Core apps\n\n\u2022\n\nRick Anderson. Adding validation\nhttps://learn.microsoft.com/aspnet/core/tutorials/first-mvc-app/validation\n\nValidation in SPA Web apps (Angular 2, TypeScript, JavaScript, Blazor\nWebAssembly)\n\n\u2022\n\n\u2022\n\nForm Validation\nhttps://angular.io/guide/form-validation\n\nValidation. Breeze documentation.\nhttps://breeze.github.io/doc-js/validation.html\n\n\u2022\n\nASP.NET Core Blazor forms and input components\n\nIn summary, these are the most important concepts in regards to validation:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nEntities and aggregates should enforce their own consistency and be \u201calways valid\u201d.\nAggregate roots are responsible for multi-entity consistency within the same aggregate.\n\nIf you think that an entity needs to enter an invalid state, consider using a different object\nmodel\u2014for example, using a temporary DTO until you create the final domain entity.\n\nIf you need to create several related objects, such as an aggregate, and they are only valid\nonce all of them have been created, consider using the Factory pattern.\n\nIn most of the cases, having redundant validation in the client side is good, because the\napplication can be proactive.\n\nDomain events: Design and implementation\n\nUse domain events to explicitly implement side effects of changes within your domain. In other words,\nand using DDD terminology, use domain events to explicitly implement side effects across multiple\naggregates. Optionally, for better scalability and less impact in database locks, use eventual\nconsistency between aggregates within the same domain.\n\n227\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fWhat is a domain event?\n\nAn event is something that has happened in the past. A domain event is, something that happened in\nthe domain that you want other parts of the same domain (in-process) to be aware of. The notified\nparts usually react somehow to the events.\n\nAn important benefit of domain events is that side effects can be expressed explicitly.\n\nFor example, if you\u2019re just using Entity Framework and there has to be a reaction to some event, you\nwould probably code whatever you need close to what triggers the event. So the rule gets coupled,\nimplicitly, to the code, and you have to look into the code to, hopefully, realize the rule is\nimplemented there.\n\nOn the other hand, using domain events makes the concept explicit, because there\u2019s a DomainEvent\nand at least one DomainEventHandler involved.\n\nFor example, in the eShopOnContainers application, when an order is created, the user becomes a\nbuyer, so an OrderStartedDomainEvent is raised and handled in the\nValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler, so the underlying concept is\nevident.\n\nIn short, domain events help you to express, explicitly, the domain rules, based in the ubiquitous\nlanguage provided by the domain experts. Domain events also enable a better separation of concerns\namong classes within the same domain.\n\nIt\u2019s important to ensure that, just like a database transaction, either all the operations related to a\ndomain event finish successfully or none of them do.\n\nDomain events are similar to messaging-style events, with one important difference. With real\nmessaging, message queuing, message brokers, or a service bus using AMQP, a message is always\nsent asynchronously and communicated across processes and machines. This is useful for integrating\nmultiple Bounded Contexts, microservices, or even different applications. However, with domain\nevents, you want to raise an event from the domain operation you\u2019re currently running, but you want\nany side effects to occur within the same domain.\n\nThe domain events and their side effects (the actions triggered afterwards that are managed by event\nhandlers) should occur almost immediately, usually in-process, and within the same domain. Thus,\ndomain events could be synchronous or asynchronous. Integration events, however, should always be\nasynchronous.\n\nDomain events versus integration events\n\nSemantically, domain and integration events are the same thing: notifications about something that\njust happened. However, their implementation must be different. Domain events are just messages\npushed to a domain event dispatcher, which could be implemented as an in-memory mediator based\non an IoC container or any other method.\n\nOn the other hand, the purpose of integration events is to propagate committed transactions and\nupdates to additional subsystems, whether they are other microservices, Bounded Contexts or even\n\n228\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fexternal applications. Hence, they should occur only if the entity is successfully persisted, otherwise it\u2019s\nas if the entire operation never happened.\n\nAs mentioned before, integration events must be based on asynchronous communication between\nmultiple microservices (other Bounded Contexts) or even external systems/applications.\n\nThus, the event bus interface needs some infrastructure that allows inter-process and distributed\ncommunication between potentially remote services. It can be based on a commercial service bus,\nqueues, a shared database used as a mailbox, or any other distributed and ideally push based\nmessaging system.\n\nDomain events as a preferred way to trigger side effects across\nmultiple aggregates within the same domain\n\nIf executing a command related to one aggregate instance requires additional domain rules to be run\non one or more additional aggregates, you should design and implement those side effects to be\ntriggered by domain events. As shown in Figure 7-14, and as one of the most important use cases, a\ndomain event should be used to propagate state changes across multiple aggregates within the same\ndomain model.\n\nFigure 7-14. Domain events to enforce consistency between multiple aggregates within the same domain\n\nFigure 7-14 shows how consistency between aggregates is achieved by domain events. When the user\ninitiates an order, the Order Aggregate sends an OrderStarted domain event. The OrderStarted\ndomain event is handled by the Buyer Aggregate to create a Buyer object in the ordering\nmicroservice, based on the original user info from the identity microservice (with information provided\nin the CreateOrder command).\n\nAlternately, you can have the aggregate root subscribed for events raised by members of its\naggregates (child entities). For instance, each OrderItem child entity can raise an event when the item\nprice is higher than a specific amount, or when the product item amount is too high. The aggregate\nroot can then receive those events and perform a global calculation or aggregation.\n\n229\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIt\u2019s important to understand that this event-based communication is not implemented directly within\nthe aggregates; you need to implement domain event handlers.\n\nHandling the domain events is an application concern. The domain model layer should only focus on\nthe domain logic\u2014things that a domain expert would understand, not application infrastructure like\nhandlers and side-effect persistence actions using repositories. Therefore, the application layer level is\nwhere you should have domain event handlers triggering actions when a domain event is raised.\n\nDomain events can also be used to trigger any number of application actions, and what is more\nimportant, must be open to increase that number in the future in a decoupled way. For instance, when\nthe order is started, you might want to publish a domain event to propagate that info to other\naggregates or even to raise application actions like notifications.\n\nThe key point is the open number of actions to be executed when a domain event occurs. Eventually,\nthe actions and rules in the domain and application will grow. The complexity or number of side-\neffect actions when something happens will grow, but if your code were coupled with \u201cglue\u201d (that is,\ncreating specific objects with new), then every time you needed to add a new action you would also\nneed to change working and tested code.\n\nThis change could result in new bugs and this approach also goes against the Open/Closed principle\nfrom SOLID. Not only that, the original class that was orchestrating the operations would grow and\ngrow, which goes against the Single Responsibility Principle (SRP).\n\nOn the other hand, if you use domain events, you can create a fine-grained and decoupled\nimplementation by segregating responsibilities using this approach:\n\n1.\n\n2.\n\nSend a command (for example, CreateOrder).\n\nReceive the command in a command handler.\n\n\u2013\n\n\u2013\n\nExecute a single aggregate\u2019s transaction.\n\n(Optional) Raise domain events for side effects (for example,\nOrderStartedDomainEvent).\n\n3.  Handle domain events (within the current process) that will execute an open number of side\n\neffects in multiple aggregates or application actions. For example:\n\n\u2013\n\n\u2013\n\n\u2013\n\nVerify or create buyer and payment method.\n\nCreate and send a related integration event to the event bus to propagate states\nacross microservices or trigger external actions like sending an email to the buyer.\n\nHandle other side effects.\n\nAs shown in Figure 7-15, starting from the same domain event, you can handle multiple actions\nrelated to other aggregates in the domain or additional application actions you need to perform\nacross microservices connecting with integration events and the event bus.\n\n230\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-15. Handling multiple actions per domain\n\nThere can be several handlers for the same domain event in the Application Layer, one handler can\nsolve consistency between aggregates and another handler can publish an integration event, so other\nmicroservices can do something with it. The event handlers are typically in the application layer,\nbecause you\u2019ll use infrastructure objects like repositories or an application API for the microservice\u2019s\nbehavior. In that sense, event handlers are similar to command handlers, so both are part of the\napplication layer. The important difference is that a command should be processed only once. A\ndomain event could be processed zero or n times, because it can be received by multiple receivers or\nevent handlers with a different purpose for each handler.\n\nHaving an open number of handlers per domain event allows you to add as many domain rules as\nneeded, without affecting current code. For instance, implementing the following business rule might\nbe as easy as adding a few event handlers (or even just one):\n\nWhen the total amount purchased by a customer in the store, across any number of orders, exceeds\n$6,000, apply a 10% off discount to every new order and notify the customer with an email about that\ndiscount for future orders.\n\nImplement domain events\n\nIn C#, a domain event is simply a data-holding structure or class, like a DTO, with all the information\nrelated to what just happened in the domain, as shown in the following example:\n\npublic class OrderStartedDomainEvent : INotification\n{\n    public string UserId { get; }\n    public string UserName { get; }\n    public int CardTypeId { get; }\n    public string CardNumber { get; }\n    public string CardSecurityNumber { get; }\n\n231\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    public string CardHolderName { get; }\n    public DateTime CardExpiration { get; }\n    public Order Order { get; }\n\n    public OrderStartedDomainEvent(Order order, string userId, string userName,\n                                   int cardTypeId, string cardNumber,\n                                   string cardSecurityNumber, string cardHolderName,\n                                   DateTime cardExpiration)\n    {\n        Order = order;\n        UserId = userId;\n        UserName = userName;\n        CardTypeId = cardTypeId;\n        CardNumber = cardNumber;\n        CardSecurityNumber = cardSecurityNumber;\n        CardHolderName = cardHolderName;\n        CardExpiration = cardExpiration;\n    }\n}\n\nThis is essentially a class that holds all the data related to the OrderStarted event.\n\nIn terms of the ubiquitous language of the domain, since an event is something that happened in the\npast, the class name of the event should be represented as a past-tense verb, like\nOrderStartedDomainEvent or OrderShippedDomainEvent. That\u2019s how the domain event is\nimplemented in the ordering microservice in eShopOnContainers.\n\nAs noted earlier, an important characteristic of events is that since an event is something that\nhappened in the past, it shouldn\u2019t change. Therefore, it must be an immutable class. You can see in\nthe previous code that the properties are read-only. There\u2019s no way to update the object, you can only\nset values when you create it.\n\nIt\u2019s important to highlight here that if domain events were to be handled asynchronously, using a\nqueue that required serializing and deserializing the event objects, the properties would have to be\n\u201cprivate set\u201d instead of read-only, so the deserializer would be able to assign the values upon\ndequeuing. This is not an issue in the Ordering microservice, as the domain event pub/sub is\nimplemented synchronously using MediatR.\n\nRaise domain events\n\nThe next question is how to raise a domain event so it reaches its related event handlers. You can use\nmultiple approaches.\n\nUdi Dahan originally proposed (for example, in several related posts, such as Domain Events \u2013 Take 2)\nusing a static class for managing and raising the events. This might include a static class named\nDomainEvents that would raise domain events immediately when it\u2019s called, using syntax like\nDomainEvents.Raise(Event myEvent). Jimmy Bogard wrote a blog post (Strengthening your domain:\nDomain Events) that recommends a similar approach.\n\nHowever, when the domain events class is static, it also dispatches to handlers immediately. This\nmakes testing and debugging more difficult, because the event handlers with side-effects logic are\nexecuted immediately after the event is raised. When you\u2019re testing and debugging, you just want to\nfocus on what is happening in the current aggregate classes; you don\u2019t want to suddenly be\n\n232\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fredirected to other event handlers for side effects related to other aggregates or application logic.\nThis is why other approaches have evolved, as explained in the next section.\n\nThe deferred approach to raise and dispatch events\n\nInstead of dispatching to a domain event handler immediately, a better approach is to add the\ndomain events to a collection and then to dispatch those domain events right before or right after\ncommitting the transaction (as with SaveChanges in EF). (This approach was described by Jimmy\nBogard in this post A better domain events pattern.)\n\nDeciding if you send the domain events right before or right after committing the transaction is\nimportant, since it determines whether you will include the side effects as part of the same transaction\nor in different transactions. In the latter case, you need to deal with eventual consistency across\nmultiple aggregates. This topic is discussed in the next section.\n\nThe deferred approach is what eShopOnContainers uses. First, you add the events happening in your\nentities into a collection or list of events per entity. That list should be part of the entity object, or\neven better, part of your base entity class, as shown in the following example of the Entity base class:\n\npublic abstract class Entity\n{\n     //...\n     private List<INotification> _domainEvents;\n     public List<INotification> DomainEvents => _domainEvents;\n\n     public void AddDomainEvent(INotification eventItem)\n     {\n         _domainEvents = _domainEvents ?? new List<INotification>();\n         _domainEvents.Add(eventItem);\n     }\n\n     public void RemoveDomainEvent(INotification eventItem)\n     {\n         _domainEvents?.Remove(eventItem);\n     }\n     //... Additional code\n}\n\nWhen you want to raise an event, you just add it to the event collection from code at any method of\nthe aggregate-root entity.\n\nThe following code, part of the Order aggregate-root at eShopOnContainers, shows an example:\n\nvar orderStartedDomainEvent = new OrderStartedDomainEvent(this, //Order object\n                                                          cardTypeId, cardNumber,\n                                                          cardSecurityNumber,\n                                                          cardHolderName,\n                                                          cardExpiration);\nthis.AddDomainEvent(orderStartedDomainEvent);\n\nNotice that the only thing that the AddDomainEvent method is doing is adding an event to the list.\nNo event is dispatched yet, and no event handler is invoked yet.\n\n233\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fYou actually want to dispatch the events later on, when you commit the transaction to the database. If\nyou are using Entity Framework Core, that means in the SaveChanges method of your EF DbContext,\nas in the following code:\n\n// EF Core DbContext\npublic class OrderingContext : DbContext, IUnitOfWork\n{\n    // ...\n    public async Task<bool> SaveEntitiesAsync(CancellationToken cancellationToken =\ndefault(CancellationToken))\n    {\n        // Dispatch Domain Events collection.\n        // Choices:\n        // A) Right BEFORE committing data (EF SaveChanges) into the DB. This makes\n        // a single transaction including side effects from the domain event\n        // handlers that are using the same DbContext with Scope lifetime\n        // B) Right AFTER committing data (EF SaveChanges) into the DB. This makes\n        // multiple transactions. You will need to handle eventual consistency and\n        // compensatory actions in case of failures.\n        await _mediator.DispatchDomainEventsAsync(this);\n\n        // After this line runs, all the changes (from the Command Handler and Domain\n        // event handlers) performed through the DbContext will be committed\n        var result = await base.SaveChangesAsync();\n    }\n}\n\nWith this code, you dispatch the entity events to their respective event handlers.\n\nThe overall result is that you\u2019ve decoupled the raising of a domain event (a simple add into a list in\nmemory) from dispatching it to an event handler. In addition, depending on what kind of dispatcher\nyou are using, you could dispatch the events synchronously or asynchronously.\n\nBe aware that transactional boundaries come into significant play here. If your unit of work and\ntransaction can span more than one aggregate (as when using EF Core and a relational database), this\ncan work well. But if the transaction cannot span aggregates, you have to implement additional steps\nto achieve consistency. This is another reason why persistence ignorance is not universal; it depends\non the storage system you use.\n\nSingle transaction across aggregates versus eventual consistency across\naggregates\n\nThe question of whether to perform a single transaction across aggregates versus relying on eventual\nconsistency across those aggregates is a controversial one. Many DDD authors like Eric Evans and\nVaughn Vernon advocate the rule that one transaction = one aggregate and therefore argue for\neventual consistency across aggregates. For example, in his book Domain-Driven Design, Eric Evans\nsays this:\n\nAny rule that spans Aggregates will not be expected to be up-to-date at all times. Through event\nprocessing, batch processing, or other update mechanisms, other dependencies can be resolved\nwithin some specific time. (page 128)\n\nVaughn Vernon says the following in Effective Aggregate Design. Part II: Making Aggregates Work\nTogether:\n\n234\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThus, if executing a command on one aggregate instance requires that additional business rules\nexecute on one or more aggregates, use eventual consistency [\u2026] There is a practical way to support\neventual consistency in a DDD model. An aggregate method publishes a domain event that is in time\ndelivered to one or more asynchronous subscribers.\n\nThis rationale is based on embracing fine-grained transactions instead of transactions spanning many\naggregates or entities. The idea is that in the second case, the number of database locks will be\nsubstantial in large-scale applications with high scalability needs. Embracing the fact that highly\nscalable applications need not have instant transactional consistency between multiple aggregates\nhelps with accepting the concept of eventual consistency. Atomic changes are often not needed by\nthe business, and it is in any case the responsibility of the domain experts to say whether particular\noperations need atomic transactions or not. If an operation always needs an atomic transaction\nbetween multiple aggregates, you might ask whether your aggregate should be larger or wasn\u2019t\ncorrectly designed.\n\nHowever, other developers and architects like Jimmy Bogard are okay with spanning a single\ntransaction across several aggregates\u2014but only when those additional aggregates are related to side\neffects for the same original command. For instance, in A better domain events pattern, Bogard says\nthis:\n\nTypically, I want the side effects of a domain event to occur within the same logical transaction, but\nnot necessarily in the same scope of raising the domain event [\u2026] Just before we commit our\ntransaction, we dispatch our events to their respective handlers.\n\nIf you dispatch the domain events right before committing the original transaction, it is because you\nwant the side effects of those events to be included in the same transaction. For example, if the EF\nDbContext SaveChanges method fails, the transaction will roll back all changes, including the result of\nany side effect operations implemented by the related domain event handlers. This is because the\nDbContext life scope is by default defined as \u201cscoped.\u201d Therefore, the DbContext object is shared\nacross multiple repository objects being instantiated within the same scope or object graph. This\ncoincides with the HttpRequest scope when developing Web API or MVC apps.\n\nActually, both approaches (single atomic transaction and eventual consistency) can be right. It really\ndepends on your domain or business requirements and what the domain experts tell you. It also\ndepends on how scalable you need the service to be (more granular transactions have less impact\nwith regard to database locks). And it depends on how much investment you\u2019re willing to make in\nyour code, since eventual consistency requires more complex code in order to detect possible\ninconsistencies across aggregates and the need to implement compensatory actions. Consider that if\nyou commit changes to the original aggregate and afterwards, when the events are being dispatched,\nif there\u2019s an issue and the event handlers cannot commit their side effects, you\u2019ll have inconsistencies\nbetween aggregates.\n\nA way to allow compensatory actions would be to store the domain events in additional database\ntables so they can be part of the original transaction. Afterwards, you could have a batch process that\ndetects inconsistencies and runs compensatory actions by comparing the list of events with the\ncurrent state of the aggregates. The compensatory actions are part of a complex topic that will require\ndeep analysis from your side, which includes discussing it with the business user and domain experts.\n\n235\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIn any case, you can choose the approach you need. But the initial deferred approach\u2014raising the\nevents before committing, so you use a single transaction\u2014is the simplest approach when using EF\nCore and a relational database. It\u2019s easier to implement and valid in many business cases. It\u2019s also the\napproach used in the ordering microservice in eShopOnContainers.\n\nBut how do you actually dispatch those events to their respective event handlers? What\u2019s the\n_mediator object you see in the previous example? It has to do with the techniques and artifacts you\nuse to map between events and their event handlers.\n\nThe domain event dispatcher: mapping from events to event handlers\n\nOnce you\u2019re able to dispatch or publish the events, you need some kind of artifact that will publish the\nevent, so that every related handler can get it and process side effects based on that event.\n\nOne approach is a real messaging system or even an event bus, possibly based on a service bus as\nopposed to in-memory events. However, for the first case, real messaging would be overkill for\nprocessing domain events, since you just need to process those events within the same process (that\nis, within the same domain and application layer).\n\nHow to subscribe to domain events\n\nWhen you use MediatR, each event handler must use an event type that is provided on the generic\nparameter of the INotificationHandler interface, as you can see in the following code:\n\npublic class ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler\n  : INotificationHandler<OrderStartedDomainEvent>\n\nBased on the relationship between event and event handler, which can be considered the\nsubscription, the MediatR artifact can discover all the event handlers for each event and trigger each\none of those event handlers.\n\nHow to handle domain events\n\nFinally, the event handler usually implements application layer code that uses infrastructure\nrepositories to obtain the required additional aggregates and to execute side-effect domain logic. The\nfollowing domain event handler code at eShopOnContainers, shows an implementation example.\n\npublic class ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler\n    : INotificationHandler<OrderStartedDomainEvent>\n{\n    private readonly ILogger _logger;\n    private readonly IBuyerRepository _buyerRepository;\n    private readonly IOrderingIntegrationEventService _orderingIntegrationEventService;\n\n    public ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler(\n        ILogger<ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler> logger,\n        IBuyerRepository buyerRepository,\n        IOrderingIntegrationEventService orderingIntegrationEventService)\n    {\n        _buyerRepository = buyerRepository ?? throw new\nArgumentNullException(nameof(buyerRepository));\n        _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new\nArgumentNullException(nameof(orderingIntegrationEventService));\n\n236\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        _logger = logger ?? throw new ArgumentNullException(nameof(logger));\n    }\n\n    public async Task Handle(\n        OrderStartedDomainEvent domainEvent, CancellationToken cancellationToken)\n    {\n        var cardTypeId = domainEvent.CardTypeId != 0 ? domainEvent.CardTypeId : 1;\n        var buyer = await _buyerRepository.FindAsync(domainEvent.UserId);\n        var buyerExisted = buyer is not null;\n\n        if (!buyerExisted)\n        {\n            buyer = new Buyer(domainEvent.UserId, domainEvent.UserName);\n        }\n\n        buyer.VerifyOrAddPaymentMethod(\n            cardTypeId,\n            $\"Payment Method on {DateTime.UtcNow}\",\n            domainEvent.CardNumber,\n            domainEvent.CardSecurityNumber,\n            domainEvent.CardHolderName,\n            domainEvent.CardExpiration,\n            domainEvent.Order.Id);\n\n        var buyerUpdated = buyerExisted ?\n            _buyerRepository.Update(buyer) :\n            _buyerRepository.Add(buyer);\n\n        await _buyerRepository.UnitOfWork\n            .SaveEntitiesAsync(cancellationToken);\n\n        var integrationEvent = new OrderStatusChangedToSubmittedIntegrationEvent(\n            domainEvent.Order.Id, domainEvent.Order.OrderStatus.Name, buyer.Name);\n        await _orderingIntegrationEventService.AddAndSaveEventAsync(integrationEvent);\n\n        OrderingApiTrace.LogOrderBuyerAndPaymentValidatedOrUpdated(\n            _logger, buyerUpdated.Id, domainEvent.Order.Id);\n    }\n}\n\nThe previous domain event handler code is considered application layer code because it uses\ninfrastructure repositories, as explained in the next section on the infrastructure-persistence layer.\nEvent handlers could also use other infrastructure components.\n\nDomain events can generate integration events to be published outside of the\nmicroservice boundaries\n\nFinally, it\u2019s important to mention that you might sometimes want to propagate events across multiple\nmicroservices. That propagation is an integration event, and it could be published through an event\nbus from any specific domain event handler.\n\nConclusions on domain events\n\nAs stated, use domain events to explicitly implement side effects of changes within your domain. To\nuse DDD terminology, use domain events to explicitly implement side effects across one or multiple\naggregates. Additionally, and for better scalability and less impact on database locks, use eventual\nconsistency between aggregates within the same domain.\n\n237\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe reference app uses MediatR to propagate domain events synchronously across aggregates, within\na single transaction. However, you could also use some AMQP implementation like RabbitMQ or\nAzure Service Bus to propagate domain events asynchronously, using eventual consistency but, as\nmentioned above, you have to consider the need for compensatory actions in case of failures.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nGreg Young. What is a Domain Event?\nhttps://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf#page=25\n\nJan Stenberg. Domain Events and Eventual Consistency\nhttps://www.infoq.com/news/2015/09/domain-events-consistency\n\nJimmy Bogard. A better domain events pattern\nhttps://lostechies.com/jimmybogard/2014/05/13/a-better-domain-events-pattern/\n\nVaughn Vernon. Effective Aggregate Design Part II: Making Aggregates Work Together\nhttps://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_2.pdf\n\nJimmy Bogard. Strengthening your domain: Domain Events\nhttps://lostechies.com/jimmybogard/2010/04/08/strengthening-your-domain-domain-\nevents/\n\nUdi Dahan. How to create fully encapsulated Domain Models\nhttps://udidahan.com/2008/02/29/how-to-create-fully-encapsulated-domain-models/\n\nUdi Dahan. Domain Events \u2013 Take 2\nhttps://udidahan.com/2008/08/25/domain-events-take-2/\n\nUdi Dahan. Domain Events \u2013 Salvation\nhttps://udidahan.com/2009/06/14/domain-events-salvation/\n\nCesar de la Torre. Domain Events vs. Integration Events in DDD and microservices\narchitectures\nhttps://devblogs.microsoft.com/cesardelatorre/domain-events-vs-integration-events-in-\ndomain-driven-design-and-microservices-architectures/\n\nDesign the infrastructure persistence layer\n\nData persistence components provide access to the data hosted within the boundaries of a\nmicroservice (that is, a microservice\u2019s database). They contain the actual implementation of\ncomponents such as repositories and Unit of Work classes, like custom Entity Framework (EF)\nDbContext objects. EF DbContext implements both the Repository and the Unit of Work patterns.\n\nThe Repository pattern\n\nThe Repository pattern is a Domain-Driven Design pattern intended to keep persistence concerns\noutside of the system\u2019s domain model. One or more persistence abstractions - interfaces - are defined\n\n238\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fin the domain model, and these abstractions have implementations in the form of persistence-specific\nadapters defined elsewhere in the application.\n\nRepository implementations are classes that encapsulate the logic required to access data sources.\nThey centralize common data access functionality, providing better maintainability and decoupling\nthe infrastructure or technology used to access databases from the domain model. If you use an\nObject-Relational Mapper (ORM) like Entity Framework, the code that must be implemented is\nsimplified, thanks to LINQ and strong typing. This lets you focus on the data persistence logic rather\nthan on data access plumbing.\n\nThe Repository pattern is a well-documented way of working with a data source. In the book Patterns\nof Enterprise Application Architecture, Martin Fowler describes a repository as follows:\n\nA repository performs the tasks of an intermediary between the domain model layers and data\nmapping, acting in a similar way to a set of domain objects in memory. Client objects declaratively\nbuild queries and send them to the repositories for answers. Conceptually, a repository encapsulates a\nset of objects stored in the database and operations that can be performed on them, providing a way\nthat is closer to the persistence layer. Repositories, also, support the purpose of separating, clearly\nand in one direction, the dependency between the work domain and the data allocation or mapping.\n\nDefine one repository per aggregate\n\nFor each aggregate or aggregate root, you should create one repository class. You may be able to\nleverage C# Generics to reduce the total number concrete classes you need to maintain (as\ndemonstrated later in this chapter). In a microservice based on Domain-Driven Design (DDD) patterns,\nthe only channel you should use to update the database should be the repositories. This is because\nthey have a one-to-one relationship with the aggregate root, which controls the aggregate\u2019s\ninvariants and transactional consistency. It\u2019s okay to query the database through other channels (as\nyou can do following a CQRS approach), because queries don\u2019t change the state of the database.\nHowever, the transactional area (that is, the updates) must always be controlled by the repositories\nand the aggregate roots.\n\nBasically, a repository allows you to populate data in memory that comes from the database in the\nform of the domain entities. Once the entities are in memory, they can be changed and then persisted\nback to the database through transactions.\n\nAs noted earlier, if you\u2019re using the CQS/CQRS architectural pattern, the initial queries are performed\nby side queries out of the domain model, performed by simple SQL statements using Dapper. This\napproach is much more flexible than repositories because you can query and join any tables you\nneed, and these queries aren\u2019t restricted by rules from the aggregates. That data goes to the\npresentation layer or client app.\n\nIf the user makes changes, the data to be updated comes from the client app or presentation layer to\nthe application layer (such as a Web API service). When you receive a command in a command\nhandler, you use repositories to get the data you want to update from the database. You update it in\nmemory with the data passed with the commands, and you then add or update the data (domain\nentities) in the database through a transaction.\n\n239\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIt\u2019s important to emphasize again that you should only define one repository for each aggregate root,\nas shown in Figure 7-17. To achieve the goal of the aggregate root to maintain transactional\nconsistency between all the objects within the aggregate, you should never create a repository for\neach table in the database.\n\nFigure 7-17. The relationship between repositories, aggregates, and database tables\n\nThe above diagram shows the relationships between Domain and Infrastructure layers: Buyer\nAggregate depends on the IBuyerRepository and Order Aggregate depends on the IOrderRepository\ninterfaces, these interfaces are implemented in the Infrastructure layer by the corresponding\nrepositories that depend on UnitOfWork, also implemented there, that accesses the tables in the Data\ntier.\n\nEnforce one aggregate root per repository\n\nIt can be valuable to implement your repository design in such a way that it enforces the rule that only\naggregate roots should have repositories. You can create a generic or base repository type that\nconstrains the type of entities it works with to ensure they have the IAggregateRoot marker interface.\n\nThus, each repository class implemented at the infrastructure layer implements its own contract or\ninterface, as shown in the following code:\n\nnamespace Microsoft.eShopOnContainers.Services.Ordering.Infrastructure.Repositories\n{\n    public class OrderRepository : IOrderRepository\n    {\n      // ...\n\n240\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    }\n}\n\nEach specific repository interface implements the generic IRepository interface:\n\npublic interface IOrderRepository : IRepository<Order>\n{\n    Order Add(Order order);\n    // ...\n}\n\nHowever, a better way to have the code enforce the convention that each repository is related to a\nsingle aggregate is to implement a generic repository type. That way, it\u2019s explicit that you\u2019re using a\nrepository to target a specific aggregate. That can be easily done by implementing a generic\nIRepository base interface, as in the following code:\n\npublic interface IRepository<T> where T : IAggregateRoot\n{\n    //....\n}\n\nThe Repository pattern makes it easier to test your application logic\n\nThe Repository pattern allows you to easily test your application with unit tests. Remember that unit\ntests only test your code, not infrastructure, so the repository abstractions make it easier to achieve\nthat goal.\n\nAs noted in an earlier section, it\u2019s recommended that you define and place the repository interfaces in\nthe domain model layer so the application layer, such as your Web API microservice, doesn\u2019t depend\ndirectly on the infrastructure layer where you\u2019ve implemented the actual repository classes. By doing\nthis and using Dependency Injection in the controllers of your Web API, you can implement mock\nrepositories that return fake data instead of data from the database. This decoupled approach allows\nyou to create and run unit tests that focus the logic of your application without requiring connectivity\nto the database.\n\nConnections to databases can fail and, more importantly, running hundreds of tests against a\ndatabase is bad for two reasons. First, it can take a long time because of the large number of tests.\nSecond, the database records might change and impact the results of your tests, especially if your\ntests are running in parallel, so that they might not be consistent. Unit tests typically can run in\nparallel; integration tests may not support parallel execution depending on their implementation.\nTesting against the database isn\u2019t a unit test but an integration test. You should have many unit tests\nrunning fast, but fewer integration tests against the databases.\n\nIn terms of separation of concerns for unit tests, your logic operates on domain entities in memory. It\nassumes the repository class has delivered those. Once your logic modifies the domain entities, it\nassumes the repository class will store them correctly. The important point here is to create unit tests\nagainst your domain model and its domain logic. Aggregate roots are the main consistency\nboundaries in DDD.\n\nThe repositories implemented in eShopOnContainers rely on EF Core\u2019s DbContext implementation of\nthe Repository and Unit of Work patterns using its change tracker, so they don\u2019t duplicate this\nfunctionality.\n\n241\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe difference between the Repository pattern and the legacy Data Access class\n(DAL class) pattern\n\nA typical DAL object directly performs data access and persistence operations against storage, often\nat the level of a single table and row. Simple CRUD operations implemented with a set of DAL classes\nfrequently do not support transactions (though this is not always the case). Most DAL class\napproaches make minimal use of abstractions, resulting in tight coupling between application or\nBusiness Logic Layer (BLL) classes that call the DAL objects.\n\nWhen using repository, the implementation details of persistence are encapsulated away from the\ndomain model. The use of an abstraction provides ease of extending behavior through patterns like\nDecorators or Proxies. For instance, cross-cutting concerns like caching, logging, and error handling\ncan all be applied using these patterns rather than hard-coded in the data access code itself. It\u2019s also\ntrivial to support multiple repository adapters which may be used in different environments, from\nlocal development to shared staging environments to production.\n\nImplementing Unit of Work\n\nA unit of work refers to a single transaction that involves multiple insert, update, or delete operations.\nIn simple terms, it means that for a specific user action, such as a registration on a website, all the\ninsert, update, and delete operations are handled in a single transaction. This is more efficient than\nhandling multiple database operations in a chattier way.\n\nThese multiple persistence operations are performed later in a single action when your code from the\napplication layer commands it. The decision about applying the in-memory changes to the actual\ndatabase storage is typically based on the Unit of Work pattern. In EF, the Unit of Work pattern is\nimplemented by a DbContext and is executed when a call is made to SaveChanges.\n\nIn many cases, this pattern or way of applying operations against the storage can increase application\nperformance and reduce the possibility of inconsistencies. It also reduces transaction blocking in the\ndatabase tables, because all the intended operations are committed as part of one transaction. This is\nmore efficient in comparison to executing many isolated operations against the database. Therefore,\nthe selected ORM can optimize the execution against the database by grouping several update\nactions within the same transaction, as opposed to many small and separate transaction executions.\n\nThe Unit of Work pattern can be implemented with or without using the Repository pattern.\n\nRepositories shouldn\u2019t be mandatory\n\nCustom repositories are useful for the reasons cited earlier, and that is the approach for the ordering\nmicroservice in eShopOnContainers. However, it isn\u2019t an essential pattern to implement in a DDD\ndesign or even in general .NET development.\n\nFor instance, Jimmy Bogard, when providing direct feedback for this guide, said the following:\n\nThis\u2019ll probably be my biggest feedback. I\u2019m really not a fan of repositories, mainly because they hide\nthe important details of the underlying persistence mechanism. It\u2019s why I go for MediatR for\ncommands, too. I can use the full power of the persistence layer, and push all that domain behavior\ninto my aggregate roots. I don\u2019t usually want to mock my repositories \u2013 I still need to have that\n\n242\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fintegration test with the real thing. Going CQRS meant that we didn\u2019t really have a need for\nrepositories any more.\n\nRepositories might be useful, but they are not critical for your DDD design in the way that the\nAggregate pattern and a rich domain model are. Therefore, use the Repository pattern or not, as you\nsee fit.\n\nAdditional resources\n\nRepository pattern\n\n\u2022\n\n\u2022\n\n\u2022\n\nEdward Hieatt and Rob Mee. Repository pattern.\nhttps://martinfowler.com/eaaCatalog/repository.html\n\nThe Repository pattern\nhttps://learn.microsoft.com/previous-versions/msp-n-p/ff649690(v=pandp.10)\n\nEric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software. (Book;\nincludes a discussion of the Repository pattern)\nhttps://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-\nSoftware/dp/0321125215/\n\nUnit of Work pattern\n\n\u2022  Martin Fowler. Unit of Work pattern.\n\nhttps://martinfowler.com/eaaCatalog/unitOfWork.html\n\n\u2022\n\nImplementing the Repository and Unit of Work Patterns in an ASP.NET MVC\nApplication\nhttps://learn.microsoft.com/aspnet/mvc/overview/older-versions/getting-started-with-ef-5-\nusing-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-\napplication\n\nImplement the infrastructure persistence layer with\nEntity Framework Core\n\nWhen you use relational databases such as SQL Server, Oracle, or PostgreSQL, a recommended\napproach is to implement the persistence layer based on Entity Framework (EF). EF supports LINQ and\nprovides strongly typed objects for your model, as well as simplified persistence into your database.\n\nEntity Framework has a long history as part of the .NET Framework. When you use .NET, you should\nalso use Entity Framework Core, which runs on Windows or Linux in the same way as .NET. EF Core is a\ncomplete rewrite of Entity Framework that\u2019s implemented with a much smaller footprint and\nimportant improvements in performance.\n\n243\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIntroduction to Entity Framework Core\n\nEntity Framework (EF) Core is a lightweight, extensible, and cross-platform version of the popular\nEntity Framework data access technology. It was introduced with .NET Core in mid-2016.\n\nSince an introduction to EF Core is already available in Microsoft documentation, here we simply\nprovide links to that information.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nEntity Framework Core\nhttps://learn.microsoft.com/ef/core/\n\nGetting started with ASP.NET Core and Entity Framework Core using Visual Studio\nhttps://learn.microsoft.com/aspnet/core/data/ef-mvc/\n\nDbContext Class\nhttps://learn.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext\n\nCompare EF Core & EF6.x\nhttps://learn.microsoft.com/ef/efcore-and-ef6/index\n\nInfrastructure in Entity Framework Core from a DDD perspective\n\nFrom a DDD point of view, an important capability of EF is the ability to use POCO domain entities,\nalso known in EF terminology as POCO code-first entities. If you use POCO domain entities, your\ndomain model classes are persistence-ignorant, following the Persistence Ignorance and the\nInfrastructure Ignorance principles.\n\nPer DDD patterns, you should encapsulate domain behavior and rules within the entity class itself, so\nit can control invariants, validations, and rules when accessing any collection. Therefore, it is not a\ngood practice in DDD to allow public access to collections of child entities or value objects. Instead,\nyou want to expose methods that control how and when your fields and property collections can be\nupdated, and what behavior and actions should occur when that happens.\n\nSince EF Core 1.1, to satisfy those DDD requirements, you can have plain fields in your entities instead\nof public properties. If you do not want an entity field to be externally accessible, you can just create\nthe attribute or field instead of a property. You can also use private property setters.\n\nIn a similar way, you can now have read-only access to collections by using a public property typed as\nIReadOnlyCollection<T>, which is backed by a private field member for the collection (like a List<T>)\nin your entity that relies on EF for persistence. Previous versions of Entity Framework required\ncollection properties to support ICollection<T>, which meant that any developer using the parent\nentity class could add or remove items through its property collections. That possibility would be\nagainst the recommended patterns in DDD.\n\nYou can use a private collection while exposing a read-only IReadOnlyCollection<T> object, as shown\nin the following code example:\n\npublic class Order : Entity\n{\n\n244\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    // Using private fields, allowed since EF Core 1.1\n    private DateTime _orderDate;\n    // Other fields ...\n\n    private readonly List<OrderItem> _orderItems;\n    public IReadOnlyCollection<OrderItem> OrderItems => _orderItems;\n\n    protected Order() { }\n\n    public Order(int buyerId, int paymentMethodId, Address address)\n    {\n        // Initializations ...\n    }\n\n    public void AddOrderItem(int productId, string productName,\n                             decimal unitPrice, decimal discount,\n                             string pictureUrl, int units = 1)\n    {\n        // Validation logic...\n\n        var orderItem = new OrderItem(productId, productName,\n                                      unitPrice, discount,\n                                      pictureUrl, units);\n        _orderItems.Add(orderItem);\n    }\n}\n\nThe OrderItems property can only be accessed as read-only using IReadOnlyCollection<OrderItem>.\nThis type is read-only so it is protected against regular external updates.\n\nEF Core provides a way to map the domain model to the physical database without \u201ccontaminating\u201d\nthe domain model. It is pure .NET POCO code, because the mapping action is implemented in the\npersistence layer. In that mapping action, you need to configure the fields-to-database mapping. In\nthe following example of the OnModelCreating method from OrderingContext and the\nOrderEntityTypeConfiguration class, the call to SetPropertyAccessMode tells EF Core to access the\nOrderItems property through its field.\n\n// At OrderingContext.cs from eShopOnContainers\nprotected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n   // ...\n   modelBuilder.ApplyConfiguration(new OrderEntityTypeConfiguration());\n   // Other entities' configuration ...\n}\n\n// At OrderEntityTypeConfiguration.cs from eShopOnContainers\nclass OrderEntityTypeConfiguration : IEntityTypeConfiguration<Order>\n{\n    public void Configure(EntityTypeBuilder<Order> orderConfiguration)\n    {\n        orderConfiguration.ToTable(\"orders\", OrderingContext.DEFAULT_SCHEMA);\n        // Other configuration\n\n        var navigation =\n              orderConfiguration.Metadata.FindNavigation(nameof(Order.OrderItems));\n\n        //EF access the OrderItem collection property through its backing field\n        navigation.SetPropertyAccessMode(PropertyAccessMode.Field);\n\n245\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        // Other configuration\n    }\n}\n\nWhen you use fields instead of properties, the OrderItem entity is persisted as if it had a\nList<OrderItem> property. However, it exposes a single accessor, the AddOrderItem method, for\nadding new items to the order. As a result, behavior and data are tied together and will be consistent\nthroughout any application code that uses the domain model.\n\nImplement custom repositories with Entity Framework Core\n\nAt the implementation level, a repository is simply a class with data persistence code coordinated by a\nunit of work (DBContext in EF Core) when performing updates, as shown in the following class:\n\n// using directives...\nnamespace Microsoft.eShopOnContainers.Services.Ordering.Infrastructure.Repositories\n{\n    public class BuyerRepository : IBuyerRepository\n    {\n        private readonly OrderingContext _context;\n        public IUnitOfWork UnitOfWork\n        {\n            get\n            {\n                return _context;\n            }\n        }\n\n        public BuyerRepository(OrderingContext context)\n        {\n            _context = context ?? throw new ArgumentNullException(nameof(context));\n        }\n\n        public Buyer Add(Buyer buyer)\n        {\n            return _context.Buyers.Add(buyer).Entity;\n        }\n\n        public async Task<Buyer> FindAsync(string buyerIdentityGuid)\n        {\n            var buyer = await _context.Buyers\n                .Include(b => b.Payments)\n                .Where(b => b.FullName == buyerIdentityGuid)\n                .SingleOrDefaultAsync();\n\n            return buyer;\n        }\n    }\n}\n\nThe IBuyerRepository interface comes from the domain model layer as a contract. However, the\nrepository implementation is done at the persistence and infrastructure layer.\n\nThe EF DbContext comes through the constructor through Dependency Injection. It is shared between\nmultiple repositories within the same HTTP request scope, thanks to its default lifetime\n(ServiceLifetime.Scoped) in the IoC container (which can also be explicitly set with\nservices.AddDbContext<>).\n\n246\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fMethods to implement in a repository (updates or transactions versus queries)\n\nWithin each repository class, you should put the persistence methods that update the state of entities\ncontained by its related aggregate. Remember there is one-to-one relationship between an aggregate\nand its related repository. Consider that an aggregate root entity object might have embedded child\nentities within its EF graph. For example, a buyer might have multiple payment methods as related\nchild entities.\n\nSince the approach for the ordering microservice in eShopOnContainers is also based on CQS/CQRS,\nmost of the queries are not implemented in custom repositories. Developers have the freedom to\ncreate the queries and joins they need for the presentation layer without the restrictions imposed by\naggregates, custom repositories per aggregate, and DDD in general. Most of the custom repositories\nsuggested by this guide have several update or transactional methods but just the query methods\nneeded to get data to be updated. For example, the BuyerRepository repository implements a\nFindAsync method, because the application needs to know whether a particular buyer exists before\ncreating a new buyer related to the order.\n\nHowever, the real query methods to get data to send to the presentation layer or client apps are\nimplemented, as mentioned, in the CQRS queries based on flexible queries using Dapper.\n\nUsing a custom repository versus using EF DbContext directly\n\nThe Entity Framework DbContext class is based on the Unit of Work and Repository patterns and can\nbe used directly from your code, such as from an ASP.NET Core MVC controller. The Unit of Work and\nRepository patterns result in the simplest code, as in the CRUD catalog microservice in\neShopOnContainers. In cases where you want the simplest code possible, you might want to directly\nuse the DbContext class, as many developers do.\n\nHowever, implementing custom repositories provides several benefits when implementing more\ncomplex microservices or applications. The Unit of Work and Repository patterns are intended to\nencapsulate the infrastructure persistence layer so it is decoupled from the application and domain-\nmodel layers. Implementing these patterns can facilitate the use of mock repositories simulating\naccess to the database.\n\nIn Figure 7-18, you can see the differences between not using repositories (directly using the EF\nDbContext) versus using repositories, which makes it easier to mock those repositories.\n\n247\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-18. Using custom repositories versus a plain DbContext\n\nFigure 7-18 shows that using a custom repository adds an abstraction layer that can be used to ease\ntesting by mocking the repository. There are multiple alternatives when mocking. You could mock just\nrepositories or you could mock a whole unit of work. Usually mocking just the repositories is enough,\nand the complexity to abstract and mock a whole unit of work is usually not needed.\n\nLater, when we focus on the application layer, you will see how Dependency Injection works in\nASP.NET Core and how it is implemented when using repositories.\n\nIn short, custom repositories allow you to test code more easily with unit tests that are not impacted\nby the data tier state. If you run tests that also access the actual database through the Entity\nFramework, they are not unit tests but integration tests, which are a lot slower.\n\nIf you were using DbContext directly, you would have to mock it or to run unit tests by using an in-\nmemory SQL Server with predictable data for unit tests. But mocking the DbContext or controlling\nfake data requires more work than mocking at the repository level. Of course, you could always test\nthe MVC controllers.\n\nEF DbContext and IUnitOfWork instance lifetime in your IoC container\n\nThe DbContext object (exposed as an IUnitOfWork object) should be shared among multiple\nrepositories within the same HTTP request scope. For example, this is true when the operation being\nexecuted must deal with multiple aggregates, or simply because you are using multiple repository\ninstances. It is also important to mention that the IUnitOfWork interface is part of your domain layer,\nnot an EF Core type.\n\nIn order to do that, the instance of the DbContext object has to have its service lifetime set to\nServiceLifetime.Scoped. This is the default lifetime when registering a DbContext with\nbuilder.Services.AddDbContext in your IoC container from the Program.cs file in your ASP.NET Core\nWeb API project. The following code illustrates this.\n\n248\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f// Add framework services.\nbuilder.Services.AddMvc(options =>\n{\n    options.Filters.Add(typeof(HttpGlobalExceptionFilter));\n}).AddControllersAsServices();\n\nbuilder.Services.AddEntityFrameworkSqlServer()\n    .AddDbContext<OrderingContext>(options =>\n    {\n        options.UseSqlServer(Configuration[\"ConnectionString\"],\n                            sqlOptions =>\nsqlOptions.MigrationsAssembly(typeof(Startup).GetTypeInfo().\n\nAssembly.GetName().Name));\n    },\n    ServiceLifetime.Scoped // Note that Scoped is the default choice\n                            // in AddDbContext. It is shown here only for\n                            // pedagogic purposes.\n    );\n\nThe DbContext instantiation mode should not be configured as ServiceLifetime.Transient or\nServiceLifetime.Singleton.\n\nThe repository instance lifetime in your IoC container\n\nIn a similar way, repository\u2019s lifetime should usually be set as scoped (InstancePerLifetimeScope in\nAutofac). It could also be transient (InstancePerDependency in Autofac), but your service will be more\nefficient in regards to memory when using the scoped lifetime.\n\n// Registering a Repository in Autofac IoC container\nbuilder.RegisterType<OrderRepository>()\n    .As<IOrderRepository>()\n    .InstancePerLifetimeScope();\n\nUsing the singleton lifetime for the repository could cause you serious concurrency problems when\nyour DbContext is set to scoped (InstancePerLifetimeScope) lifetime (the default lifetimes for a\nDBContext). As long as your service lifetimes for your repositories and your DbContext are both\nScoped, you\u2019ll avoid these issues.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nImplementing the Repository and Unit of Work Patterns in an ASP.NET MVC\nApplication\nhttps://www.asp.net/mvc/overview/older-versions/getting-started-with-ef-5-using-mvc-\n4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-application\n\nJonathan Allen. Implementation Strategies for the Repository Pattern with Entity\nFramework, Dapper, and Chain\nhttps://www.infoq.com/articles/repository-implementation-strategies\n\nCesar de la Torre. Comparing ASP.NET Core IoC container service lifetimes with Autofac\nIoC container instance scopes\nhttps://devblogs.microsoft.com/cesardelatorre/comparing-asp-net-core-ioc-service-life-\ntimes-and-autofac-ioc-instance-scopes/\n\n249\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fTable mapping\n\nTable mapping identifies the table data to be queried from and saved to the database. Previously you\nsaw how domain entities (for example, a product or order domain) can be used to generate a related\ndatabase schema. EF is strongly designed around the concept of conventions. Conventions address\nquestions like \u201cWhat will the name of a table be?\u201d or \u201cWhat property is the primary key?\u201d Conventions\nare typically based on conventional names. For example, it is typical for the primary key to be a\nproperty that ends with Id.\n\nBy convention, each entity will be set up to map to a table with the same name as the DbSet<TEntity>\nproperty that exposes the entity on the derived context. If no DbSet<TEntity> value is provided for\nthe given entity, the class name is used.\n\nData Annotations versus Fluent API\n\nThere are many additional EF Core conventions, and most of them can be changed by using either\ndata annotations or Fluent API, implemented within the OnModelCreating method.\n\nData annotations must be used on the entity model classes themselves, which is a more intrusive way\nfrom a DDD point of view. This is because you are contaminating your model with data annotations\nrelated to the infrastructure database. On the other hand, Fluent API is a convenient way to change\nmost conventions and mappings within your data persistence infrastructure layer, so the entity model\nwill be clean and decoupled from the persistence infrastructure.\n\nFluent API and the OnModelCreating method\n\nAs mentioned, in order to change conventions and mappings, you can use the OnModelCreating\nmethod in the DbContext class.\n\nThe ordering microservice in eShopOnContainers implements explicit mapping and configuration,\nwhen needed, as shown in the following code.\n\n// At OrderingContext.cs from eShopOnContainers\nprotected override void OnModelCreating(ModelBuilder modelBuilder)\n{\n   // ...\n   modelBuilder.ApplyConfiguration(new OrderEntityTypeConfiguration());\n   // Other entities' configuration ...\n}\n\n// At OrderEntityTypeConfiguration.cs from eShopOnContainers\nclass OrderEntityTypeConfiguration : IEntityTypeConfiguration<Order>\n{\n    public void Configure(EntityTypeBuilder<Order> orderConfiguration)\n    {\n        orderConfiguration.ToTable(\"orders\", OrderingContext.DEFAULT_SCHEMA);\n\n        orderConfiguration.HasKey(o => o.Id);\n\n        orderConfiguration.Ignore(b => b.DomainEvents);\n\n        orderConfiguration.Property(o => o.Id)\n            .UseHiLo(\"orderseq\", OrderingContext.DEFAULT_SCHEMA);\n\n250\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        //Address value object persisted as owned entity type supported since EF Core 2.0\n        orderConfiguration\n            .OwnsOne(o => o.Address, a =>\n            {\n                a.WithOwner();\n            });\n\n        orderConfiguration\n            .Property<int?>(\"_buyerId\")\n            .UsePropertyAccessMode(PropertyAccessMode.Field)\n            .HasColumnName(\"BuyerId\")\n            .IsRequired(false);\n\n        orderConfiguration\n            .Property<DateTime>(\"_orderDate\")\n            .UsePropertyAccessMode(PropertyAccessMode.Field)\n            .HasColumnName(\"OrderDate\")\n            .IsRequired();\n\n        orderConfiguration\n            .Property<int>(\"_orderStatusId\")\n            .UsePropertyAccessMode(PropertyAccessMode.Field)\n            .HasColumnName(\"OrderStatusId\")\n            .IsRequired();\n\n        orderConfiguration\n            .Property<int?>(\"_paymentMethodId\")\n            .UsePropertyAccessMode(PropertyAccessMode.Field)\n            .HasColumnName(\"PaymentMethodId\")\n            .IsRequired(false);\n\n        orderConfiguration.Property<string>(\"Description\").IsRequired(false);\n\n        var navigation =\norderConfiguration.Metadata.FindNavigation(nameof(Order.OrderItems));\n\n        // DDD Patterns comment:\n        //Set as field (New since EF 1.1) to access the OrderItem collection property\nthrough its field\n        navigation.SetPropertyAccessMode(PropertyAccessMode.Field);\n\n        orderConfiguration.HasOne<PaymentMethod>()\n            .WithMany()\n            .HasForeignKey(\"_paymentMethodId\")\n            .IsRequired(false)\n            .OnDelete(DeleteBehavior.Restrict);\n\n        orderConfiguration.HasOne<Buyer>()\n            .WithMany()\n            .IsRequired(false)\n            .HasForeignKey(\"_buyerId\");\n\n        orderConfiguration.HasOne(o => o.OrderStatus)\n            .WithMany()\n            .HasForeignKey(\"_orderStatusId\");\n    }\n}\n\nYou could set all the Fluent API mappings within the same OnModelCreating method, but it\u2019s\nadvisable to partition that code and have multiple configuration classes, one per entity, as shown in\n\n251\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fthe example. Especially for large models, it is advisable to have separate configuration classes for\nconfiguring different entity types.\n\nThe code in the example shows a few explicit declarations and mapping. However, EF Core\nconventions do many of those mappings automatically, so the actual code you would need in your\ncase might be smaller.\n\nThe Hi/Lo algorithm in EF Core\n\nAn interesting aspect of code in the preceding example is that it uses the Hi/Lo algorithm as the key\ngeneration strategy.\n\nThe Hi/Lo algorithm is useful when you need unique keys before committing changes. As a summary,\nthe Hi-Lo algorithm assigns unique identifiers to table rows while not depending on storing the row in\nthe database immediately. This lets you start using the identifiers right away, as happens with regular\nsequential database IDs.\n\nThe Hi/Lo algorithm describes a mechanism for getting a batch of unique IDs from a related database\nsequence. These IDs are safe to use because the database guarantees the uniqueness, so there will be\nno collisions between users. This algorithm is interesting for these reasons:\n\n\u2022\n\n\u2022\n\n\u2022\n\nIt does not break the Unit of Work pattern.\n\nIt gets sequence IDs in batches, to minimize round trips to the database.\n\nIt generates a human readable identifier, unlike techniques that use GUIDs.\n\nEF Core supports HiLo with the UseHiLo method, as shown in the preceding example.\n\nMap fields instead of properties\n\nWith this feature, available since EF Core 1.1, you can directly map columns to fields. It is possible to\nnot use properties in the entity class, and just to map columns from a table to fields. A common use\nfor that would be private fields for any internal state that do not need to be accessed from outside the\nentity.\n\nYou can do this with single fields or also with collections, like a List<> field. This point was mentioned\nearlier when we discussed modeling the domain model classes, but here you can see how that\nmapping is performed with the PropertyAccessMode.Field configuration highlighted in the previous\ncode.\n\nUse shadow properties in EF Core, hidden at the infrastructure level\n\nShadow properties in EF Core are properties that do not exist in your entity class model. The values\nand states of these properties are maintained purely in the ChangeTracker class at the infrastructure\nlevel.\n\n252\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fImplement the Query Specification pattern\n\nAs introduced earlier in the design section, the Query Specification pattern is a Domain-Driven Design\npattern designed as the place where you can put the definition of a query with optional sorting and\npaging logic.\n\nThe Query Specification pattern defines a query in an object. For example, in order to encapsulate a\npaged query that searches for some products you can create a PagedProduct specification that takes\nthe necessary input parameters (pageNumber, pageSize, filter, etc.). Then, within any Repository\nmethod (usually a List() overload) it would accept an IQuerySpecification and run the expected query\nbased on that specification.\n\nAn example of a generic Specification interface is the following code, which is similar to code used in\nthe eShopOnWeb reference application.\n\n// GENERIC SPECIFICATION INTERFACE\n// https://github.com/dotnet-architecture/eShopOnWeb\n\npublic interface ISpecification<T>\n{\n    Expression<Func<T, bool>> Criteria { get; }\n    List<Expression<Func<T, object>>> Includes { get; }\n    List<string> IncludeStrings { get; }\n}\n\nThen, the implementation of a generic specification base class is the following.\n\n// GENERIC SPECIFICATION IMPLEMENTATION (BASE CLASS)\n// https://github.com/dotnet-architecture/eShopOnWeb\n\npublic abstract class BaseSpecification<T> : ISpecification<T>\n{\n    public BaseSpecification(Expression<Func<T, bool>> criteria)\n    {\n        Criteria = criteria;\n    }\n    public Expression<Func<T, bool>> Criteria { get; }\n\n    public List<Expression<Func<T, object>>> Includes { get; } =\n                                           new List<Expression<Func<T, object>>>();\n\n    public List<string> IncludeStrings { get; } = new List<string>();\n\n    protected virtual void AddInclude(Expression<Func<T, object>> includeExpression)\n    {\n        Includes.Add(includeExpression);\n    }\n\n    // string-based includes allow for including children of children\n    // e.g. Basket.Items.Product\n    protected virtual void AddInclude(string includeString)\n    {\n        IncludeStrings.Add(includeString);\n    }\n}\n\n253\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe following specification loads a single basket entity given either the basket\u2019s ID or the ID of the\nbuyer to whom the basket belongs. It will eagerly load the basket\u2019s Items collection.\n\n// SAMPLE QUERY SPECIFICATION IMPLEMENTATION\n\npublic class BasketWithItemsSpecification : BaseSpecification<Basket>\n{\n    public BasketWithItemsSpecification(int basketId)\n        : base(b => b.Id == basketId)\n    {\n        AddInclude(b => b.Items);\n    }\n\n    public BasketWithItemsSpecification(string buyerId)\n        : base(b => b.BuyerId == buyerId)\n    {\n        AddInclude(b => b.Items);\n    }\n}\n\nAnd finally, you can see below how a generic EF Repository can use such a specification to filter and\neager-load data related to a given entity type T.\n\n// GENERIC EF REPOSITORY WITH SPECIFICATION\n// https://github.com/dotnet-architecture/eShopOnWeb\n\npublic IEnumerable<T> List(ISpecification<T> spec)\n{\n    // fetch a Queryable that includes all expression-based includes\n    var queryableResultWithIncludes = spec.Includes\n        .Aggregate(_dbContext.Set<T>().AsQueryable(),\n            (current, include) => current.Include(include));\n\n    // modify the IQueryable to include any string-based include statements\n    var secondaryResult = spec.IncludeStrings\n        .Aggregate(queryableResultWithIncludes,\n            (current, include) => current.Include(include));\n\n    // return the result of the query using the specification's criteria expression\n    return secondaryResult\n                    .Where(spec.Criteria)\n                    .AsEnumerable();\n}\n\nIn addition to encapsulating filtering logic, the specification can specify the shape of the data to be\nreturned, including which properties to populate.\n\nAlthough we don\u2019t recommend returning IQueryable from a repository, it\u2019s perfectly fine to use them\nwithin the repository to build up a set of results. You can see this approach used in the List method\nabove, which uses intermediate IQueryable expressions to build up the query\u2019s list of includes before\nexecuting the query with the specification\u2019s criteria on the last line.\n\nLearn how the specification pattern is applied in the eShopOnWeb sample.\n\n254\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nTable Mapping\nhttps://learn.microsoft.com/ef/core/modeling/relational/tables\n\nUse HiLo to generate keys with Entity Framework Core\nhttps://www.talkingdotnet.com/use-hilo-to-generate-keys-with-entity-framework-core/\n\nBacking Fields\nhttps://learn.microsoft.com/ef/core/modeling/backing-field\n\nSteve Smith. Encapsulated Collections in Entity Framework Core\nhttps://ardalis.com/encapsulated-collections-in-entity-framework-core\n\nShadow Properties\nhttps://learn.microsoft.com/ef/core/modeling/shadow-properties\n\nThe Specification pattern\nhttps://deviq.com/specification-pattern/\n\nArdalis.Specification NuGet Package Used by\neShopOnWeb.  https://www.nuget.org/packages/Ardalis.Specification\n\nUse NoSQL databases as a persistence infrastructure\n\nWhen you use NoSQL databases for your infrastructure data tier, you typically do not use an ORM like\nEntity Framework Core. Instead you use the API provided by the NoSQL engine, such as Azure Cosmos\nDB, MongoDB, Cassandra, RavenDB, CouchDB, or Azure Storage Tables.\n\nHowever, when you use a NoSQL database, especially a document-oriented database like Azure\nCosmos DB, CouchDB, or RavenDB, the way you design your model with DDD aggregates is partially\nsimilar to how you can do it in EF Core, in regards to the identification of aggregate roots, child entity\nclasses, and value object classes. But, ultimately, the database selection will impact in your design.\n\nWhen you use a document-oriented database, you implement an aggregate as a single document,\nserialized in JSON or another format. However, the use of the database is transparent from a domain\nmodel code point of view. When using a NoSQL database, you still are using entity classes and\naggregate root classes, but with more flexibility than when using EF Core because the persistence is\nnot relational.\n\nThe difference is in how you persist that model. If you implemented your domain model based on\nPOCO entity classes, agnostic to the infrastructure persistence, it might look like you could move to a\ndifferent persistence infrastructure, even from relational to NoSQL. However, that should not be your\ngoal. There are always constraints and trade-offs in the different database technologies, so you will\nnot be able to have the same model for relational or NoSQL databases. Changing persistence models\nis not a trivial task, because transactions and persistence operations will be very different.\n\nFor example, in a document-oriented database, it is okay for an aggregate root to have multiple child\ncollection properties. In a relational database, querying multiple child collection properties is not\n\n255\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\feasily optimized, because you get a UNION ALL SQL statement back from EF. Having the same\ndomain model for relational databases or NoSQL databases is not simple, and you should not try to\ndo it. You really have to design your model with an understanding of how the data is going to be\nused in each particular database.\n\nA benefit when using NoSQL databases is that the entities are more denormalized, so you do not set a\ntable mapping. Your domain model can be more flexible than when using a relational database.\n\nWhen you design your domain model based on aggregates, moving to NoSQL and document-\noriented databases might be even easier than using a relational database, because the aggregates\nyou design are similar to serialized documents in a document-oriented database. Then you can\ninclude in those \u201cbags\u201d all the information you might need for that aggregate.\n\nFor instance, the following JSON code is a sample implementation of an order aggregate when using\na document-oriented database. It is similar to the order aggregate we implemented in the\neShopOnContainers sample, but without using EF Core underneath.\n\n{\n    \"id\": \"2024001\",\n    \"orderDate\": \"2/25/2024\",\n    \"buyerId\": \"1234567\",\n    \"address\": [\n        {\n        \"street\": \"100 One Microsoft Way\",\n        \"city\": \"Redmond\",\n        \"state\": \"WA\",\n        \"zip\": \"98052\",\n        \"country\": \"U.S.\"\n        }\n    ],\n    \"orderItems\": [\n        {\"id\": 20240011, \"productId\": \"123456\", \"productName\": \".NET T-Shirt\",\n        \"unitPrice\": 25, \"units\": 2, \"discount\": 0},\n        {\"id\": 20240012, \"productId\": \"123457\", \"productName\": \".NET Mug\",\n        \"unitPrice\": 15, \"units\": 1, \"discount\": 0}\n    ]\n}\n\nIntroduction to Azure Cosmos DB and the native Cosmos DB API\n\nAzure Cosmos DB is Microsoft\u2019s globally distributed database service for mission-critical applications.\nAzure Cosmos DB provides turn-key global distribution, elastic scaling of throughput and storage\nworldwide, single-digit millisecond latencies at the 99th percentile, five well-defined consistency\nlevels, and guaranteed high availability, all backed by industry-leading SLAs. Azure Cosmos DB\nautomatically indexes data without requiring you to deal with schema and index management. It is\nmulti-model and supports document, key-value, graph, and columnar data models.\n\n256\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-19. Azure Cosmos DB global distribution\n\nWhen you use a C# model to implement the aggregate to be used by the Azure Cosmos DB API, the\naggregate can be similar to the C# POCO classes used with EF Core. The difference is in the way to\nuse them from the application and infrastructure layers, as in the following code:\n\n// C# EXAMPLE OF AN ORDER AGGREGATE BEING PERSISTED WITH AZURE COSMOS DB API\n// *** Domain Model Code ***\n// Aggregate: Create an Order object with its child entities and/or value objects.\n// Then, use AggregateRoot's methods to add the nested objects so invariants and\n// logic is consistent across the nested properties (value objects and entities).\n\nOrder orderAggregate = new Order\n{\n    Id = \"2024001\",\n    OrderDate = new DateTime(2005, 7, 1),\n    BuyerId = \"1234567\",\n    PurchaseOrderNumber = \"PO18009186470\"\n}\n\nAddress address = new Address\n{\n    Street = \"100 One Microsoft Way\",\n    City = \"Redmond\",\n    State = \"WA\",\n    Zip = \"98052\",\n    Country = \"U.S.\"\n}\n\norderAggregate.UpdateAddress(address);\n\nOrderItem orderItem1 = new OrderItem\n{\n\n257\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    Id = 20240011,\n    ProductId = \"123456\",\n    ProductName = \".NET T-Shirt\",\n    UnitPrice = 25,\n    Units = 2,\n    Discount = 0;\n};\n\n//Using methods with domain logic within the entity. No anemic-domain model\norderAggregate.AddOrderItem(orderItem1);\n// *** End of Domain Model Code ***\n\n// *** Infrastructure Code using Cosmos DB Client API ***\nUri collectionUri = UriFactory.CreateDocumentCollectionUri(databaseName,\n    collectionName);\n\nawait client.CreateDocumentAsync(collectionUri, orderAggregate);\n\n// As your app evolves, let's say your object has a new schema. You can insert\n// OrderV2 objects without any changes to the database tier.\nOrder2 newOrder = GetOrderV2Sample(\"IdForSalesOrder2\");\nawait client.CreateDocumentAsync(collectionUri, newOrder);\n\nYou can see that the way you work with your domain model can be similar to the way you use it in\nyour domain model layer when the infrastructure is EF. You still use the same aggregate root methods\nto ensure consistency, invariants, and validations within the aggregate.\n\nHowever, when you persist your model into the NoSQL database, the code and API change\ndramatically compared to EF Core code or any other code related to relational databases.\n\nImplement .NET code targeting MongoDB and Azure Cosmos DB\n\nUse Azure Cosmos DB from .NET containers\n\nYou can access Azure Cosmos DB databases from .NET code running in containers, like from any other\n.NET application. For instance, the Locations.API and Marketing.API microservices in\neShopOnContainers are implemented so they can consume Azure Cosmos DB databases.\n\nHowever, there\u2019s a limitation in Azure Cosmos DB from a Docker development environment point of\nview. Even though there\u2019s an on-premises Azure Cosmos DB Emulator that can run in a local\ndevelopment machine, it only supports Windows. Linux and macOS aren\u2019t supported.\n\nThere\u2019s also the possibility to run this emulator on Docker, but just on Windows Containers, not with\nLinux Containers. That\u2019s an initial handicap for the development environment if your application is\ndeployed as Linux containers, since, currently, you can\u2019t deploy Linux and Windows Containers on\nDocker for Windows at the same time. Either all containers being deployed have to be for Linux or for\nWindows.\n\nThe ideal and more straightforward deployment for a dev/test solution is to be able to deploy your\ndatabase systems as containers along with your custom containers so your dev/test environments are\nalways consistent.\n\n258\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fUse MongoDB API for local dev/test Linux/Windows containers plus Azure Cosmos\nDB\n\nCosmos DB databases support MongoDB API for .NET as well as the native MongoDB wire protocol.\nThis means that by using existing drivers, your application written for MongoDB can now\ncommunicate with Cosmos DB and use Cosmos DB databases instead of MongoDB databases, as\nshown in Figure 7-20.\n\nFigure 7-20. Using MongoDB API and protocol to access Azure Cosmos DB\n\nThis is a very convenient approach for proof of concepts in Docker environments with Linux\ncontainers because the MongoDB Docker image is a multi-arch image that supports Docker Linux\ncontainers and Docker Windows containers.\n\nAs shown in the following image, by using the MongoDB API, eShopOnContainers supports MongoDB\nLinux and Windows containers for the local development environment but then, you can move to a\nscalable, PaaS cloud solution as Azure Cosmos DB by simply changing the MongoDB connection\nstring to point to Azure Cosmos DB.\n\n259\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-21. eShopOnContainers using MongoDB containers for dev-env or Azure Cosmos DB for production\n\nThe production Azure Cosmos DB would be running in Azure\u2019s cloud as a PaaS and scalable service.\n\nYour custom .NET containers can run on a local development Docker host (that is using Docker for\nWindows in a Windows 10 machine) or be deployed into a production environment, like Kubernetes in\nAzure AKS or Azure Service Fabric. In this second environment, you would deploy only the .NET\ncustom containers but not the MongoDB container since you\u2019d be using Azure Cosmos DB in the\ncloud for handling the data in production.\n\nA clear benefit of using the MongoDB API is that your solution could run in both database engines,\nMongoDB or Azure Cosmos DB, so migrations to different environments should be easy. However,\nsometimes it is worthwhile to use a native API (that is the native Cosmos DB API) in order to take full\nadvantage of the capabilities of a specific database engine.\n\nFor further comparison between simply using MongoDB versus Cosmos DB in the cloud, see the\nBenefits of using Azure Cosmos DB in this page.\n\nAnalyze your approach for production applications: MongoDB API vs. Cosmos DB\nAPI\n\nIn eShopOnContainers, we\u2019re using MongoDB API because our priority was fundamentally to have a\nconsistent dev/test environment using a NoSQL database that could also work with Azure Cosmos DB.\n\n260\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fHowever, if you are planning to use MongoDB API to access Azure Cosmos DB in Azure for\nproduction applications, you should analyze the differences in capabilities and performance when\nusing MongoDB API to access Azure Cosmos DB databases compared to using the native Azure\nCosmos DB API. If it is similar you can use MongoDB API and you get the benefit of supporting two\nNoSQL database engines at the same time.\n\nYou could also use MongoDB clusters as the production database in Azure\u2019s cloud, too, with\nMongoDB Azure Service. But that is not a PaaS service provided by Microsoft. In this case, Azure is just\nhosting that solution coming from MongoDB.\n\nBasically, this is just a disclaimer stating that you shouldn\u2019t always use MongoDB API against Azure\nCosmos DB, as we did in eShopOnContainers because it was a convenient choice for Linux containers.\nThe decision should be based on the specific needs and tests you need to do for your production\napplication.\n\nThe code: Use MongoDB API in .NET applications\n\nMongoDB API for .NET is based on NuGet packages that you need to add to your projects, like in the\nLocations.API project shown in the following figure.\n\n261\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-22. MongoDB API NuGet packages references in a .NET project\n\nLet\u2019s investigate the code in the following sections.\n\nA Model used by MongoDB API\n\nFirst, you need to define a model that will hold the data coming from the database in your\napplication\u2019s memory space. Here\u2019s an example of the model used for Locations at\neShopOnContainers.\n\nusing MongoDB.Bson;\nusing MongoDB.Bson.Serialization.Attributes;\nusing MongoDB.Driver.GeoJsonObjectModel;\nusing System.Collections.Generic;\n\npublic class Locations\n{\n\n262\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    [BsonId]\n    [BsonRepresentation(BsonType.ObjectId)]\n    public string Id { get; set; }\n    public int LocationId { get; set; }\n    public string Code { get; set; }\n    [BsonRepresentation(BsonType.ObjectId)]\n    public string Parent_Id { get; set; }\n    public string Description { get; set; }\n    public double Latitude { get; set; }\n    public double Longitude { get; set; }\n    public GeoJsonPoint<GeoJson2DGeographicCoordinates> Location\n                                                             { get; private set; }\n    public GeoJsonPolygon<GeoJson2DGeographicCoordinates> Polygon\n                                                             { get; private set; }\n    public void SetLocation(double lon, double lat) => SetPosition(lon, lat);\n    public void SetArea(List<GeoJson2DGeographicCoordinates> coordinatesList)\n                                                    => SetPolygon(coordinatesList);\n\n    private void SetPosition(double lon, double lat)\n    {\n        Latitude = lat;\n        Longitude = lon;\n        Location = new GeoJsonPoint<GeoJson2DGeographicCoordinates>(\n            new GeoJson2DGeographicCoordinates(lon, lat));\n    }\n\n    private void SetPolygon(List<GeoJson2DGeographicCoordinates> coordinatesList)\n    {\n        Polygon = new GeoJsonPolygon<GeoJson2DGeographicCoordinates>(\n                  new GeoJsonPolygonCoordinates<GeoJson2DGeographicCoordinates>(\n                  new GeoJsonLinearRingCoordinates<GeoJson2DGeographicCoordinates>(\n                                                                 coordinatesList)));\n    }\n}\n\nYou can see there are a few attributes and types coming from the MongoDB NuGet packages.\n\nNoSQL databases are usually very well suited for working with non-relational hierarchical data. In this\nexample, we are using MongoDB types especially made for geo-locations, like\nGeoJson2DGeographicCoordinates.\n\nRetrieve the database and the collection\n\nIn eShopOnContainers, we have created a custom database context where we implement the code to\nretrieve the database and the MongoCollections, as in the following code.\n\npublic class LocationsContext\n{\n    private readonly IMongoDatabase _database = null;\n\n    public LocationsContext(IOptions<LocationSettings> settings)\n    {\n        var client = new MongoClient(settings.Value.ConnectionString);\n        if (client != null)\n            _database = client.GetDatabase(settings.Value.Database);\n    }\n\n    public IMongoCollection<Locations> Locations\n    {\n\n263\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        get\n        {\n            return _database.GetCollection<Locations>(\"Locations\");\n        }\n    }\n}\n\nRetrieve the data\n\nIn C# code, like Web API controllers or custom Repositories implementation, you can write similar\ncode to the following when querying through the MongoDB API. Note that the _context object is an\ninstance of the previous LocationsContext class.\n\npublic async Task<Locations> GetAsync(int locationId)\n{\n    var filter = Builders<Locations>.Filter.Eq(\"LocationId\", locationId);\n    return await _context.Locations\n                            .Find(filter)\n                            .FirstOrDefaultAsync();\n}\n\nUse an env-var in the docker-compose.override.yml file for the MongoDB connection\nstring\n\nWhen creating a MongoClient object, it needs a fundamental parameter which is precisely the\nConnectionString parameter pointing to the right database. In the case of eShopOnContainers, the\nconnection string can point to a local MongoDB Docker container or to a \u201cproduction\u201d Azure Cosmos\nDB database. That connection string comes from the environment variables defined in the docker-\ncompose.override.yml files used when deploying with docker-compose or Visual Studio, as in the\nfollowing yml code.\n\n# docker-compose.override.yml\nversion: '3.4'\nservices:\n  # Other services\n  locations-api:\n    environment:\n      # Other settings\n      - ConnectionString=${ESHOP_AZURE_COSMOSDB:-mongodb://nosqldata}\n\nThe ConnectionString environment variable is resolved this way: If the ESHOP_AZURE_COSMOSDB\nglobal variable is defined in the .env file with the Azure Cosmos DB connection string, it will use it to\naccess the Azure Cosmos DB database in the cloud. If it\u2019s not defined, it will take the\nmongodb://nosqldata value and use the development MongoDB container.\n\nThe following code shows the .env file with the Azure Cosmos DB connection string global\nenvironment variable, as implemented in eShopOnContainers:\n\n# .env file, in eShopOnContainers root folder\n# Other Docker environment variables\n\nESHOP_EXTERNAL_DNS_NAME_OR_IP=host.docker.internal\nESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP=<YourDockerHostIP>\n\n#ESHOP_AZURE_COSMOSDB=<YourAzureCosmosDBConnData>\n\n264\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f#Other environment variables for additional Azure infrastructure assets\n#ESHOP_AZURE_REDIS_BASKET_DB=<YourAzureRedisBasketInfo>\n#ESHOP_AZURE_STORAGE_CATALOG_URL=<YourAzureStorage_Catalog_BLOB_URL>\n#ESHOP_AZURE_SERVICE_BUS=<YourAzureServiceBusInfo>\n\nUncomment the ESHOP_AZURE_COSMOSDB line and update it with your Azure Cosmos DB\nconnection string obtained from the Azure portal as explained in Connect a MongoDB application to\nAzure Cosmos DB.\n\nIf the ESHOP_AZURE_COSMOSDB global variable is empty, meaning it\u2019s commented out in the .env\nfile, then the container uses a default MongoDB connection string. This connection string points to\nthe local MongoDB container deployed in eShopOnContainers that is named nosqldata and was\ndefined at the docker-compose file, as shown in the following .yml code:\n\n# docker-compose.yml\nversion: '3.4'\nservices:\n  # ...Other services...\n  nosqldata:\n    image: mongo\n\nAdditional resources\n\n\u2022  Modeling document data for NoSQL databases\n\nhttps://learn.microsoft.com/azure/cosmos-db/modeling-data\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nVaughn Vernon. The Ideal Domain-Driven Design Aggregate Store?\nhttps://kalele.io/blog-posts/the-ideal-domain-driven-design-aggregate-store/\n\nIntroduction to Azure Cosmos DB: API for MongoDB\nhttps://learn.microsoft.com/azure/cosmos-db/mongodb-introduction\n\nAzure Cosmos DB: Build a MongoDB API web app with .NET and the Azure portal\nhttps://learn.microsoft.com/azure/cosmos-db/create-mongodb-dotnet\n\nUse the Azure Cosmos DB Emulator for local development and testing\nhttps://learn.microsoft.com/azure/cosmos-db/local-emulator\n\nConnect a MongoDB application to Azure Cosmos DB\nhttps://learn.microsoft.com/azure/cosmos-db/connect-mongodb-account\n\nThe Cosmos DB Emulator Docker image (Windows Container)\nhttps://hub.docker.com/r/microsoft/azure-cosmosdb-emulator/\n\nThe MongoDB Docker image (Linux and Windows Container)\nhttps://hub.docker.com/_/mongo/\n\nUse MongoChef (Studio 3T) with an Azure Cosmos DB: API for MongoDB account\nhttps://learn.microsoft.com/azure/cosmos-db/mongodb-mongochef\n\n265\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fDesign the microservice application layer and Web\nAPI\n\nUse SOLID principles and Dependency Injection\n\nSOLID principles are critical techniques to be used in any modern and mission-critical application,\nsuch as developing a microservice with DDD patterns. SOLID is an acronym that groups five\nfundamental principles:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nSingle Responsibility principle\n\nOpen/closed principle\n\nLiskov substitution principle\n\nInterface Segregation principle\n\nDependency Inversion principle\n\nSOLID is more about how you design your application or microservice internal layers and about\ndecoupling dependencies between them. It is not related to the domain, but to the application\u2019s\ntechnical design. The final principle, the Dependency Inversion principle, allows you to decouple the\ninfrastructure layer from the rest of the layers, which allows a better decoupled implementation of the\nDDD layers.\n\nDependency Injection (DI) is one way to implement the Dependency Inversion principle. It is a\ntechnique for achieving loose coupling between objects and their dependencies. Rather than directly\ninstantiating collaborators, or using static references (that is, using new\u2026), the objects that a class\nneeds in order to perform its actions are provided to (or \u201cinjected into\u201d) the class. Most often, classes\nwill declare their dependencies via their constructor, allowing them to follow the Explicit\nDependencies principle. Dependency Injection is usually based on specific Inversion of Control (IoC)\ncontainers. ASP.NET Core provides a simple built-in IoC container, but you can also use your favorite\nIoC container, like Autofac or Ninject.\n\nBy following the SOLID principles, your classes will tend naturally to be small, well-factored, and easily\ntested. But how can you know if too many dependencies are being injected into your classes? If you\nuse DI through the constructor, it will be easy to detect that by just looking at the number of\nparameters for your constructor. If there are too many dependencies, this is generally a sign (a code\nsmell) that your class is trying to do too much, and is probably violating the Single Responsibility\nprinciple.\n\nIt would take another guide to cover SOLID in detail. Therefore, this guide requires you to have only a\nminimum knowledge of these topics.\n\nAdditional resources\n\n\u2022\n\nSOLID: Fundamental OOP Principles\nhttps://deviq.com/solid/\n\n266\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f\u2022\n\n\u2022\n\nInversion of Control Containers and the Dependency Injection pattern\nhttps://martinfowler.com/articles/injection.html\n\nSteve Smith. New is Glue\nhttps://ardalis.com/new-is-glue\n\nImplement the microservice application layer using\nthe Web API\n\nUse Dependency Injection to inject infrastructure objects into your\napplication layer\n\nAs mentioned previously, the application layer can be implemented as part of the artifact (assembly)\nyou are building, such as within a Web API project or an MVC web app project. In the case of a\nmicroservice built with ASP.NET Core, the application layer will usually be your Web API library. If you\nwant to separate what is coming from ASP.NET Core (its infrastructure plus your controllers) from your\ncustom application layer code, you could also place your application layer in a separate class library,\nbut that is optional.\n\nFor instance, the application layer code of the ordering microservice is directly implemented as part of\nthe Ordering.API project (an ASP.NET Core Web API project), as shown in Figure 7-23.\n\nFigure 7-23. The application layer in the Ordering.API ASP.NET Core Web API project\n\nASP.NET Core includes a simple built-in IoC container (represented by the IServiceProvider interface)\nthat supports constructor injection by default, and ASP.NET makes certain services available through\nDI. ASP.NET Core uses the term service for any of the types you register that will be injected through\nDI. You configure the built-in container\u2019s services in your application\u2019s Program.cs file. Your\n\n267\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fdependencies are implemented in the services that a type needs and that you register in the IoC\ncontainer.\n\nTypically, you want to inject dependencies that implement infrastructure objects. A typical\ndependency to inject is a repository. But you could inject any other infrastructure dependency that\nyou may have. For simpler implementations, you could directly inject your Unit of Work pattern object\n(the EF DbContext object), because the DBContext is also the implementation of your infrastructure\npersistence objects.\n\nIn the following example, you can see how .NET is injecting the required repository objects through\nthe constructor. The class is a command handler, which will get covered in the next section.\n\npublic class CreateOrderCommandHandler\n        : IRequestHandler<CreateOrderCommand, bool>\n{\n    private readonly IOrderRepository _orderRepository;\n    private readonly IIdentityService _identityService;\n    private readonly IMediator _mediator;\n    private readonly IOrderingIntegrationEventService _orderingIntegrationEventService;\n    private readonly ILogger<CreateOrderCommandHandler> _logger;\n\n    // Using DI to inject infrastructure persistence Repositories\n    public CreateOrderCommandHandler(IMediator mediator,\n        IOrderingIntegrationEventService orderingIntegrationEventService,\n        IOrderRepository orderRepository,\n        IIdentityService identityService,\n        ILogger<CreateOrderCommandHandler> logger)\n    {\n        _orderRepository = orderRepository ?? throw new\nArgumentNullException(nameof(orderRepository));\n        _identityService = identityService ?? throw new\nArgumentNullException(nameof(identityService));\n        _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator));\n        _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new\nArgumentNullException(nameof(orderingIntegrationEventService));\n        _logger = logger ?? throw new ArgumentNullException(nameof(logger));\n    }\n\n    public async Task<bool> Handle(CreateOrderCommand message, CancellationToken\ncancellationToken)\n    {\n        // Add Integration event to clean the basket\n        var orderStartedIntegrationEvent = new\nOrderStartedIntegrationEvent(message.UserId);\n        await\n_orderingIntegrationEventService.AddAndSaveEventAsync(orderStartedIntegrationEvent);\n\n        // Add/Update the Buyer AggregateRoot\n        // DDD patterns comment: Add child entities and value-objects through the Order\nAggregate-Root\n        // methods and constructor so validations, invariants and business logic\n        // make sure that consistency is preserved across the whole aggregate\n        var address = new Address(message.Street, message.City, message.State,\nmessage.Country, message.ZipCode);\n        var order = new Order(message.UserId, message.UserName, address,\nmessage.CardTypeId, message.CardNumber, message.CardSecurityNumber, message.CardHolderName,\nmessage.CardExpiration);\n\n        foreach (var item in message.OrderItems)\n\n268\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        {\n            order.AddOrderItem(item.ProductId, item.ProductName, item.UnitPrice,\nitem.Discount, item.PictureUrl, item.Units);\n        }\n\n        _logger.LogInformation(\"----- Creating Order - Order: {@Order}\", order);\n\n        _orderRepository.Add(order);\n\n        return await _orderRepository.UnitOfWork\n            .SaveEntitiesAsync(cancellationToken);\n    }\n}\n\nThe class uses the injected repositories to execute the transaction and persist the state changes. It\ndoes not matter whether that class is a command handler, an ASP.NET Core Web API controller\nmethod, or a DDD Application Service. It is ultimately a simple class that uses repositories, domain\nentities, and other application coordination in a fashion similar to a command handler. Dependency\nInjection works the same way for all the mentioned classes, as in the example using DI based on the\nconstructor.\n\nRegister the dependency implementation types and interfaces or abstractions\n\nBefore you use the objects injected through constructors, you need to know where to register the\ninterfaces and classes that produce the objects injected into your application classes through DI. (Like\nDI based on the constructor, as shown previously.)\n\nUse the built-in IoC container provided by ASP.NET Core\n\nWhen you use the built-in IoC container provided by ASP.NET Core, you register the types you want\nto inject in the Program.cs file, as in the following code:\n\n// Register out-of-the-box framework services.\nbuilder.Services.AddDbContext<CatalogContext>(c =>\n    c.UseSqlServer(Configuration[\"ConnectionString\"]),\n    ServiceLifetime.Scoped);\n\nbuilder.Services.AddMvc();\n// Register custom application dependencies.\nbuilder.Services.AddScoped<IMyCustomRepository, MyCustomSQLRepository>();\n\nThe most common pattern when registering types in an IoC container is to register a pair of types\u2014an\ninterface and its related implementation class. Then when you request an object from the IoC\ncontainer through any constructor, you request an object of a certain type of interface. For instance, in\nthe previous example, the last line states that when any of your constructors have a dependency on\nIMyCustomRepository (interface or abstraction), the IoC container will inject an instance of the\nMyCustomSQLServerRepository implementation class.\n\nUse the Scrutor library for automatic types registration\n\nWhen using DI in .NET, you might want to be able to scan an assembly and automatically register its\ntypes by convention. This feature is not currently available in ASP.NET Core. However, you can use the\n\n269\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fScrutor library for that. This approach is convenient when you have dozens of types that need to be\nregistered in your IoC container.\n\nAdditional resources\n\n\u2022  Matthew King. Registering services with Scrutor\n\nhttps://www.mking.net/blog/registering-services-with-scrutor\n\n\u2022\n\nKristian Hellang. Scrutor. GitHub repo.\nhttps://github.com/khellang/Scrutor\n\nUse Autofac as an IoC container\n\nYou can also use additional IoC containers and plug them into the ASP.NET Core pipeline, as in the\nordering microservice in eShopOnContainers, which uses Autofac. When using Autofac you typically\nregister the types via modules, which allow you to split the registration types between multiple files\ndepending on where your types are, just as you could have the application types distributed across\nmultiple class libraries.\n\nFor example, the following is the Autofac application module for the Ordering.API Web API project\nwith the types you will want to inject.\n\npublic class ApplicationModule : Autofac.Module\n{\n    public string QueriesConnectionString { get; }\n    public ApplicationModule(string qconstr)\n    {\n        QueriesConnectionString = qconstr;\n    }\n\n    protected override void Load(ContainerBuilder builder)\n    {\n        builder.Register(c => new OrderQueries(QueriesConnectionString))\n            .As<IOrderQueries>()\n            .InstancePerLifetimeScope();\n        builder.RegisterType<BuyerRepository>()\n            .As<IBuyerRepository>()\n            .InstancePerLifetimeScope();\n        builder.RegisterType<OrderRepository>()\n            .As<IOrderRepository>()\n            .InstancePerLifetimeScope();\n        builder.RegisterType<RequestManager>()\n            .As<IRequestManager>()\n            .InstancePerLifetimeScope();\n   }\n}\n\nAutofac also has a feature to scan assemblies and register types by name conventions.\n\nThe registration process and concepts are very similar to the way you can register types with the built-\nin ASP.NET Core IoC container, but the syntax when using Autofac is a bit different.\n\nIn the example code, the abstraction IOrderRepository is registered along with the implementation\nclass OrderRepository. This means that whenever a constructor is declaring a dependency through the\n\n270\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fIOrderRepository abstraction or interface, the IoC container will inject an instance of the\nOrderRepository class.\n\nThe instance scope type determines how an instance is shared between requests for the same service\nor dependency. When a request is made for a dependency, the IoC container can return the following:\n\n\u2022\n\n\u2022\n\n\u2022\n\nA single instance per lifetime scope (referred to in the ASP.NET Core IoC container as scoped).\n\nA new instance per dependency (referred to in the ASP.NET Core IoC container as transient).\n\nA single instance shared across all objects using the IoC container (referred to in the ASP.NET\nCore IoC container as singleton).\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nIntroduction to Dependency Injection in ASP.NET Core\nhttps://learn.microsoft.com/aspnet/core/fundamentals/dependency-injection\n\nAutofac. Official documentation.\nhttps://docs.autofac.org/en/latest/\n\nComparing ASP.NET Core IoC container service lifetimes with Autofac IoC container\ninstance scopes - Cesar de la Torre.\nhttps://devblogs.microsoft.com/cesardelatorre/comparing-asp-net-core-ioc-service-life-\ntimes-and-autofac-ioc-instance-scopes/\n\nImplement the Command and Command Handler patterns\n\nIn the DI-through-constructor example shown in the previous section, the IoC container was injecting\nrepositories through a constructor in a class. But exactly where were they injected? In a simple Web\nAPI (for example, the catalog microservice in eShopOnContainers), you inject them at the MVC\ncontrollers\u2019 level, in a controller constructor, as part of the request pipeline of ASP.NET Core. However,\nin the initial code of this section (the CreateOrderCommandHandler class from the Ordering.API\nservice in eShopOnContainers), the injection of dependencies is done through the constructor of a\nparticular command handler. Let us explain what a command handler is and why you would want to\nuse it.\n\nThe Command pattern is intrinsically related to the CQRS pattern that was introduced earlier in this\nguide. CQRS has two sides. The first area is queries, using simplified queries with the Dapper micro\nORM, which was explained previously. The second area is commands, which are the starting point for\ntransactions, and the input channel from outside the service.\n\nAs shown in Figure 7-24, the pattern is based on accepting commands from the client-side,\nprocessing them based on the domain model rules, and finally persisting the states with transactions.\n\n271\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-24. High-level view of the commands or \u201ctransactional side\u201d in a CQRS pattern\n\nFigure 7-24 shows that the UI app sends a command through the API that gets to a\nCommandHandler, that depends on the Domain model and the Infrastructure, to update the\ndatabase.\n\nThe command class\n\nA command is a request for the system to perform an action that changes the state of the system.\nCommands are imperative, and should be processed just once.\n\nSince commands are imperatives, they are typically named with a verb in the imperative mood (for\nexample, \u201ccreate\u201d or \u201cupdate\u201d), and they might include the aggregate type, such as\nCreateOrderCommand. Unlike an event, a command is not a fact from the past; it is only a request,\nand thus may be refused.\n\nCommands can originate from the UI as a result of a user initiating a request, or from a process\nmanager when the process manager is directing an aggregate to perform an action.\n\nAn important characteristic of a command is that it should be processed just once by a single receiver.\nThis is because a command is a single action or transaction you want to perform in the application.\nFor example, the same order creation command should not be processed more than once. This is an\nimportant difference between commands and events. Events may be processed multiple times,\nbecause many systems or microservices might be interested in the event.\n\nIn addition, it is important that a command be processed only once in case the command is not\nidempotent. A command is idempotent if it can be executed multiple times without changing the\nresult, either because of the nature of the command, or because of the way the system handles the\ncommand.\n\nIt is a good practice to make your commands and updates idempotent when it makes sense under\nyour domain\u2019s business rules and invariants. For instance, to use the same example, if for any reason\n(retry logic, hacking, etc.) the same CreateOrder command reaches your system multiple times, you\nshould be able to identify it and ensure that you do not create multiple orders. To do so, you need to\n\n272\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fattach some kind of identity in the operations and identify whether the command or update was\nalready processed.\n\nYou send a command to a single receiver; you do not publish a command. Publishing is for events\nthat state a fact\u2014that something has happened and might be interesting for event receivers. In the\ncase of events, the publisher has no concerns about which receivers get the event or what they do it.\nBut domain or integration events are a different story already introduced in previous sections.\n\nA command is implemented with a class that contains data fields or collections with all the\ninformation that is needed in order to execute that command. A command is a special kind of Data\nTransfer Object (DTO), one that is specifically used to request changes or transactions. The command\nitself is based on exactly the information that is needed for processing the command, and nothing\nmore.\n\nThe following example shows the simplified CreateOrderCommand class. This is an immutable\ncommand that is used in the ordering microservice in eShopOnContainers.\n\n// DDD and CQRS patterns comment: Note that it is recommended to implement immutable\nCommands\n// In this case, its immutability is achieved by having all the setters as private\n// plus only being able to update the data just once, when creating the object through its\nconstructor.\n// References on Immutable Commands:\n// http://cqrs.nu/Faq\n// https://docs.spine3.org/motivation/immutability.html\n// http://blog.gauffin.org/2012/06/griffin-container-introducing-command-support/\n// https://learn.microsoft.com/dotnet/csharp/programming-guide/classes-and-structs/how-to-\nimplement-a-lightweight-class-with-auto-implemented-properties\n\n[DataContract]\npublic class CreateOrderCommand\n    : IRequest<bool>\n{\n    [DataMember]\n    private readonly List<OrderItemDTO> _orderItems;\n\n    [DataMember]\n    public string UserId { get; private set; }\n\n    [DataMember]\n    public string UserName { get; private set; }\n\n    [DataMember]\n    public string City { get; private set; }\n\n    [DataMember]\n    public string Street { get; private set; }\n\n    [DataMember]\n    public string State { get; private set; }\n\n    [DataMember]\n    public string Country { get; private set; }\n\n    [DataMember]\n    public string ZipCode { get; private set; }\n\n    [DataMember]\n\n273\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    public string CardNumber { get; private set; }\n\n    [DataMember]\n    public string CardHolderName { get; private set; }\n\n    [DataMember]\n    public DateTime CardExpiration { get; private set; }\n\n    [DataMember]\n    public string CardSecurityNumber { get; private set; }\n\n    [DataMember]\n    public int CardTypeId { get; private set; }\n\n    [DataMember]\n    public IEnumerable<OrderItemDTO> OrderItems => _orderItems;\n\n    public CreateOrderCommand()\n    {\n        _orderItems = new List<OrderItemDTO>();\n    }\n\n    public CreateOrderCommand(List<BasketItem> basketItems, string userId, string userName,\nstring city, string street, string state, string country, string zipcode,\n        string cardNumber, string cardHolderName, DateTime cardExpiration,\n        string cardSecurityNumber, int cardTypeId) : this()\n    {\n        _orderItems = basketItems.ToOrderItemsDTO().ToList();\n        UserId = userId;\n        UserName = userName;\n        City = city;\n        Street = street;\n        State = state;\n        Country = country;\n        ZipCode = zipcode;\n        CardNumber = cardNumber;\n        CardHolderName = cardHolderName;\n        CardExpiration = cardExpiration;\n        CardSecurityNumber = cardSecurityNumber;\n        CardTypeId = cardTypeId;\n        CardExpiration = cardExpiration;\n    }\n\n    public class OrderItemDTO\n    {\n        public int ProductId { get; set; }\n\n        public string ProductName { get; set; }\n\n        public decimal UnitPrice { get; set; }\n\n        public decimal Discount { get; set; }\n\n        public int Units { get; set; }\n\n        public string PictureUrl { get; set; }\n    }\n}\n\n274\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fBasically, the command class contains all the data you need for performing a business transaction by\nusing the domain model objects. Thus, commands are simply data structures that contain read-only\ndata, and no behavior. The command\u2019s name indicates its purpose. In many languages like C#,\ncommands are represented as classes, but they are not true classes in the real object-oriented sense.\n\nAs an additional characteristic, commands are immutable, because the expected usage is that they are\nprocessed directly by the domain model. They do not need to change during their projected lifetime.\nIn a C# class, immutability can be achieved by not having any setters or other methods that change\nthe internal state.\n\nKeep in mind that if you intend or expect commands to go through a serializing/deserializing process,\nthe properties must have a private setter, and the [DataMember] (or [JsonProperty]) attribute.\nOtherwise, the deserializer won\u2019t be able to reconstruct the object at the destination with the required\nvalues. You can also use truly read-only properties if the class has a constructor with parameters for all\nproperties, with the usual camelCase naming convention, and annotate the constructor as\n[JsonConstructor]. However, this option requires more code.\n\nFor example, the command class for creating an order is probably similar in terms of data to the order\nyou want to create, but you probably do not need the same attributes. For instance,\nCreateOrderCommand does not have an order ID, because the order has not been created yet.\n\nMany command classes can be simple, requiring only a few fields about some state that needs to be\nchanged. That would be the case if you are just changing the status of an order from \u201cin process\u201d to\n\u201cpaid\u201d or \u201cshipped\u201d by using a command similar to the following:\n\n[DataContract]\npublic class UpdateOrderStatusCommand\n    :IRequest<bool>\n{\n    [DataMember]\n    public string Status { get; private set; }\n\n    [DataMember]\n    public string OrderId { get; private set; }\n\n    [DataMember]\n    public string BuyerIdentityGuid { get; private set; }\n}\n\nSome developers make their UI request objects separate from their command DTOs, but that is just a\nmatter of preference. It is a tedious separation with not much additional value, and the objects are\nalmost exactly the same shape. For instance, in eShopOnContainers, some commands come directly\nfrom the client-side.\n\nThe Command handler class\n\nYou should implement a specific command handler class for each command. That is how the pattern\nworks, and it\u2019s where you\u2019ll use the command object, the domain objects, and the infrastructure\nrepository objects. The command handler is in fact the heart of the application layer in terms of CQRS\nand DDD. However, all the domain logic should be contained in the domain classes\u2014within the\naggregate roots (root entities), child entities, or domain services, but not within the command handler,\nwhich is a class from the application layer.\n\n275\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fThe command handler class offers a strong stepping stone in the way to achieve the Single\nResponsibility Principle (SRP) mentioned in a previous section.\n\nA command handler receives a command and obtains a result from the aggregate that is used. The\nresult should be either successful execution of the command, or an exception. In the case of an\nexception, the system state should be unchanged.\n\nThe command handler usually takes the following steps:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nIt receives the command object, like a DTO (from the mediator or other infrastructure object).\n\nIt validates that the command is valid (if not validated by the mediator).\n\nIt instantiates the aggregate root instance that is the target of the current command.\n\nIt executes the method on the aggregate root instance, getting the required data from the\ncommand.\n\nIt persists the new state of the aggregate to its related database. This last operation is the\nactual transaction.\n\nTypically, a command handler deals with a single aggregate driven by its aggregate root (root entity).\nIf multiple aggregates should be impacted by the reception of a single command, you could use\ndomain events to propagate states or actions across multiple aggregates.\n\nThe important point here is that when a command is being processed, all the domain logic should be\ninside the domain model (the aggregates), fully encapsulated and ready for unit testing. The\ncommand handler just acts as a way to get the domain model from the database, and as the final\nstep, to tell the infrastructure layer (repositories) to persist the changes when the model is changed.\nThe advantage of this approach is that you can refactor the domain logic in an isolated, fully\nencapsulated, rich, behavioral domain model without changing code in the application or\ninfrastructure layers, which are the plumbing level (command handlers, Web API, repositories, etc.).\n\nWhen command handlers get complex, with too much logic, that can be a code smell. Review them,\nand if you find domain logic, refactor the code to move that domain behavior to the methods of the\ndomain objects (the aggregate root and child entity).\n\nAs an example of a command handler class, the following code shows the same\nCreateOrderCommandHandler class that you saw at the beginning of this chapter. In this case, it also\nhighlights the Handle method and the operations with the domain model objects/aggregates.\n\npublic class CreateOrderCommandHandler\n        : IRequestHandler<CreateOrderCommand, bool>\n{\n    private readonly IOrderRepository _orderRepository;\n    private readonly IIdentityService _identityService;\n    private readonly IMediator _mediator;\n    private readonly IOrderingIntegrationEventService _orderingIntegrationEventService;\n    private readonly ILogger<CreateOrderCommandHandler> _logger;\n\n    // Using DI to inject infrastructure persistence Repositories\n    public CreateOrderCommandHandler(IMediator mediator,\n        IOrderingIntegrationEventService orderingIntegrationEventService,\n        IOrderRepository orderRepository,\n\n276\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        IIdentityService identityService,\n        ILogger<CreateOrderCommandHandler> logger)\n    {\n        _orderRepository = orderRepository ?? throw new\nArgumentNullException(nameof(orderRepository));\n        _identityService = identityService ?? throw new\nArgumentNullException(nameof(identityService));\n        _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator));\n        _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new\nArgumentNullException(nameof(orderingIntegrationEventService));\n        _logger = logger ?? throw new ArgumentNullException(nameof(logger));\n    }\n\n    public async Task<bool> Handle(CreateOrderCommand message, CancellationToken\ncancellationToken)\n    {\n        // Add Integration event to clean the basket\n        var orderStartedIntegrationEvent = new\nOrderStartedIntegrationEvent(message.UserId);\n        await\n_orderingIntegrationEventService.AddAndSaveEventAsync(orderStartedIntegrationEvent);\n\n        // Add/Update the Buyer AggregateRoot\n        // DDD patterns comment: Add child entities and value-objects through the Order\nAggregate-Root\n        // methods and constructor so validations, invariants and business logic\n        // make sure that consistency is preserved across the whole aggregate\n        var address = new Address(message.Street, message.City, message.State,\nmessage.Country, message.ZipCode);\n        var order = new Order(message.UserId, message.UserName, address,\nmessage.CardTypeId, message.CardNumber, message.CardSecurityNumber, message.CardHolderName,\nmessage.CardExpiration);\n\n        foreach (var item in message.OrderItems)\n        {\n            order.AddOrderItem(item.ProductId, item.ProductName, item.UnitPrice,\nitem.Discount, item.PictureUrl, item.Units);\n        }\n\n        _logger.LogInformation(\"----- Creating Order - Order: {@Order}\", order);\n\n        _orderRepository.Add(order);\n\n        return await _orderRepository.UnitOfWork\n            .SaveEntitiesAsync(cancellationToken);\n    }\n}\n\nThese are additional steps a command handler should take:\n\n\u2022\n\n\u2022\n\n\u2022\n\nUse the command\u2019s data to operate with the aggregate root\u2019s methods and behavior.\n\nInternally within the domain objects, raise domain events while the transaction is executed,\nbut that is transparent from a command handler point of view.\n\nIf the aggregate\u2019s operation result is successful and after the transaction is finished, raise\nintegration events. (These might also be raised by infrastructure classes like repositories.)\n\n277\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditional resources\n\n\u2022  Mark Seemann. At the Boundaries, Applications are Not Object-Oriented\n\nhttps://blog.ploeh.dk/2011/05/31/AttheBoundaries,ApplicationsareNotObject-Oriented/\n\n\u2022\n\nCommands and events\nhttps://cqrs.nu/faq/Command%20and%20Events\n\n\u2022  What does a command handler do?\n\nhttps://cqrs.nu/faq/Command%20Handlers\n\n\u2022\n\n\u2022\n\nJimmy Bogard. Domain Command Patterns \u2013 Handlers\nhttps://jimmybogard.com/domain-command-patterns-handlers/\n\nJimmy Bogard. Domain Command Patterns \u2013 Validation\nhttps://jimmybogard.com/domain-command-patterns-validation/\n\nThe Command process pipeline: how to trigger a command handler\n\nThe next question is how to invoke a command handler. You could manually call it from each related\nASP.NET Core controller. However, that approach would be too coupled and is not ideal.\n\nThe other two main options, which are the recommended options, are:\n\n\u2022\n\nThrough an in-memory Mediator pattern artifact.\n\n\u2022  With an asynchronous message queue, in between controllers and handlers.\n\nUse the Mediator pattern (in-memory) in the command pipeline\n\nAs shown in Figure 7-25, in a CQRS approach you use an intelligent mediator, similar to an in-memory\nbus, which is smart enough to redirect to the right command handler based on the type of the\ncommand or DTO being received. The single black arrows between components represent the\ndependencies between objects (in many cases, injected through DI) with their related interactions.\n\n278\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fFigure 7-25. Using the Mediator pattern in process in a single CQRS microservice\n\nThe above diagram shows a zoom-in from image 7-24: the ASP.NET Core controller sends the\ncommand to MediatR\u2019s command pipeline, so they get to the appropriate handler.\n\nThe reason that using the Mediator pattern makes sense is that in enterprise applications, the\nprocessing requests can get complicated. You want to be able to add an open number of cross-\ncutting concerns like logging, validations, audit, and security. In these cases, you can rely on a\nmediator pipeline (see Mediator pattern) to provide a means for these extra behaviors or cross-\ncutting concerns.\n\nA mediator is an object that encapsulates the \u201chow\u201d of this process: it coordinates execution based on\nstate, the way a command handler is invoked, or the payload you provide to the handler. With a\nmediator component, you can apply cross-cutting concerns in a centralized and transparent way by\napplying decorators (or pipeline behaviors since MediatR 3). For more information, see the Decorator\npattern.\n\nDecorators and behaviors are similar to Aspect Oriented Programming (AOP), only applied to a\nspecific process pipeline managed by the mediator component. Aspects in AOP that implement cross-\ncutting concerns are applied based on aspect weavers injected at compilation time or based on object\ncall interception. Both typical AOP approaches are sometimes said to work \u201clike magic,\u201d because it is\nnot easy to see how AOP does its work. When dealing with serious issues or bugs, AOP can be difficult\nto debug. On the other hand, these decorators/behaviors are explicit and applied only in the context\nof the mediator, so debugging is much more predictable and easy.\n\nFor example, in the eShopOnContainers ordering microservice, has an implementation of two sample\nbehaviors, a LogBehavior class and a ValidatorBehavior class. The implementation of the behaviors is\nexplained in the next section by showing how eShopOnContainers uses MediatR behaviors.\n\n279\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fUse message queues (out-of-proc) in the command\u2019s pipeline\n\nAnother choice is to use asynchronous messages based on brokers or message queues, as shown in\nFigure 7-26. That option could also be combined with the mediator component right before the\ncommand handler.\n\nFigure 7-26. Using message queues (out of the process and inter-process communication) with CQRS commands\n\nCommand\u2019s pipeline can also be handled by a high availability message queue to deliver the\ncommands to the appropriate handler. Using message queues to accept the commands can further\ncomplicate your command\u2019s pipeline, because you will probably need to split the pipeline into two\nprocesses connected through the external message queue. Still, it should be used if you need to have\nimproved scalability and performance based on asynchronous messaging. Consider that in the case of\nFigure 7-26, the controller just posts the command message into the queue and returns. Then the\ncommand handlers process the messages at their own pace. That is a great benefit of queues: the\nmessage queue can act as a buffer in cases when hyper scalability is needed, such as for stocks or any\nother scenario with a high volume of ingress data.\n\nHowever, because of the asynchronous nature of message queues, you need to figure out how to\ncommunicate with the client application about the success or failure of the command\u2019s process. As a\nrule, you should never use \u201cfire and forget\u201d commands. Every business application needs to know if a\ncommand was processed successfully, or at least validated and accepted.\n\nThus, being able to respond to the client after validating a command message that was submitted to\nan asynchronous queue adds complexity to your system, as compared to an in-process command\nprocess that returns the operation\u2019s result after running the transaction. Using queues, you might\nneed to return the result of the command process through other operation result messages, which will\nrequire additional components and custom communication in your system.\n\n280\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fAdditionally, async commands are one-way commands, which in many cases might not be needed, as\nis explained in the following interesting exchange between Burtsev Alexey and Greg Young in an\nonline conversation:\n\n[Burtsev Alexey] I find lots of code where people use async command handling or one-way command\nmessaging without any reason to do so (they are not doing some long operation, they are not\nexecuting external async code, they do not even cross-application boundary to be using message\nbus). Why do they introduce this unnecessary complexity? And actually, I haven\u2019t seen a CQRS code\nexample with blocking command handlers so far, though it will work just fine in most cases.\n\n[Greg Young] [\u2026] an asynchronous command doesn\u2019t exist; it\u2019s actually another event. If I must accept\nwhat you send me and raise an event if I disagree, it\u2019s no longer you telling me to do something [that\nis, it\u2019s not a command]. It\u2019s you telling me something has been done. This seems like a slight\ndifference at first, but it has many implications.\n\nAsynchronous commands greatly increase the complexity of a system, because there is no simple way\nto indicate failures. Therefore, asynchronous commands are not recommended other than when\nscaling requirements are needed or in special cases when communicating the internal microservices\nthrough messaging. In those cases, you must design a separate reporting and recovery system for\nfailures.\n\nIn the initial version of eShopOnContainers, it was decided to use synchronous command processing,\nstarted from HTTP requests and driven by the Mediator pattern. That easily allows you to return the\nsuccess or failure of the process, as in the CreateOrderCommandHandler implementation.\n\nIn any case, this should be a decision based on your application\u2019s or microservice\u2019s business\nrequirements.\n\nImplement the command process pipeline with a mediator pattern\n(MediatR)\n\nAs a sample implementation, this guide proposes using the in-process pipeline based on the Mediator\npattern to drive command ingestion and route commands, in memory, to the right command\nhandlers. The guide also proposes applying behaviors in order to separate cross-cutting concerns.\n\nFor implementation in .NET, there are multiple open-source libraries available that implement the\nMediator pattern. The library used in this guide is the MediatR open-source library (created by Jimmy\nBogard), but you could use another approach. MediatR is a small and simple library that allows you to\nprocess in-memory messages like a command, while applying decorators or behaviors.\n\nUsing the Mediator pattern helps you to reduce coupling and to isolate the concerns of the requested\nwork, while automatically connecting to the handler that performs that work\u2014in this case, to\ncommand handlers.\n\nAnother good reason to use the Mediator pattern was explained by Jimmy Bogard when reviewing\nthis guide:\n\n281\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fI think it might be worth mentioning testing here \u2013 it provides a nice consistent window into the\nbehavior of your system. Request-in, response-out. We\u2019ve found that aspect quite valuable in building\nconsistently behaving tests.\n\nFirst, let\u2019s look at a sample WebAPI controller where you actually would use the mediator object. If\nyou weren\u2019t using the mediator object, you\u2019d need to inject all the dependencies for that controller,\nthings like a logger object and others. Therefore, the constructor would be complicated. On the other\nhand, if you use the mediator object, the constructor of your controller can be a lot simpler, with just a\nfew dependencies instead of many dependencies if you had one per cross-cutting operation, as in the\nfollowing example:\n\npublic class MyMicroserviceController : Controller\n{\n    public MyMicroserviceController(IMediator mediator,\n                                    IMyMicroserviceQueries microserviceQueries)\n    {\n        // ...\n    }\n}\n\nYou can see that the mediator provides a clean and lean Web API controller constructor. In addition,\nwithin the controller methods, the code to send a command to the mediator object is almost one line:\n\n[Route(\"new\")]\n[HttpPost]\npublic async Task<IActionResult> ExecuteBusinessOperation([FromBody]RunOpCommand\n                                                               runOperationCommand)\n{\n    var commandResult = await _mediator.SendAsync(runOperationCommand);\n\n    return commandResult ? (IActionResult)Ok() : (IActionResult)BadRequest();\n}\n\nImplement idempotent Commands\n\nIn eShopOnContainers, a more advanced example than the above is submitting a\nCreateOrderCommand object from the Ordering microservice. But since the Ordering business\nprocess is a bit more complex and, in our case, it actually starts in the Basket microservice, this action\nof submitting the CreateOrderCommand object is performed from an integration-event handler\nnamed UserCheckoutAcceptedIntegrationEventHandler instead of a simple WebAPI controller called\nfrom the client App as in the previous simpler example.\n\nNevertheless, the action of submitting the Command to MediatR is pretty similar, as shown in the\nfollowing code.\n\nvar createOrderCommand = new CreateOrderCommand(eventMsg.Basket.Items,\n                                                eventMsg.UserId, eventMsg.City,\n                                                eventMsg.Street, eventMsg.State,\n                                                eventMsg.Country, eventMsg.ZipCode,\n                                                eventMsg.CardNumber,\n                                                eventMsg.CardHolderName,\n                                                eventMsg.CardExpiration,\n                                                eventMsg.CardSecurityNumber,\n                                                eventMsg.CardTypeId);\n\n282\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fvar requestCreateOrder = new IdentifiedCommand<CreateOrderCommand,bool>(createOrderCommand,\n\neventMsg.RequestId);\nresult = await _mediator.Send(requestCreateOrder);\n\nHowever, this case is also slightly more advanced because we\u2019re also implementing idempotent\ncommands. The CreateOrderCommand process should be idempotent, so if the same message comes\nduplicated through the network, because of any reason, like retries, the same business order will be\nprocessed just once.\n\nThis is implemented by wrapping the business command (in this case CreateOrderCommand) and\nembedding it into a generic IdentifiedCommand, which is tracked by an ID of every message coming\nthrough the network that has to be idempotent.\n\nIn the code below, you can see that the IdentifiedCommand is nothing more than a DTO with and ID\nplus the wrapped business command object.\n\npublic class IdentifiedCommand<T, R> : IRequest<R>\n    where T : IRequest<R>\n{\n    public T Command { get; }\n    public Guid Id { get; }\n    public IdentifiedCommand(T command, Guid id)\n    {\n        Command = command;\n        Id = id;\n    }\n}\n\nThen the CommandHandler for the IdentifiedCommand named IdentifiedCommandHandler.cs will\nbasically check if the ID coming as part of the message already exists in a table. If it already exists, that\ncommand won\u2019t be processed again, so it behaves as an idempotent command. That infrastructure\ncode is performed by the _requestManager.ExistAsync method call below.\n\n// IdentifiedCommandHandler.cs\npublic class IdentifiedCommandHandler<T, R> : IRequestHandler<IdentifiedCommand<T, R>, R>\n        where T : IRequest<R>\n{\n    private readonly IMediator _mediator;\n    private readonly IRequestManager _requestManager;\n    private readonly ILogger<IdentifiedCommandHandler<T, R>> _logger;\n\n    public IdentifiedCommandHandler(\n        IMediator mediator,\n        IRequestManager requestManager,\n        ILogger<IdentifiedCommandHandler<T, R>> logger)\n    {\n        _mediator = mediator;\n        _requestManager = requestManager;\n        _logger = logger ?? throw new System.ArgumentNullException(nameof(logger));\n    }\n\n    /// <summary>\n    /// Creates the result value to return if a previous request was found\n    /// </summary>\n    /// <returns></returns>\n\n283\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    protected virtual R CreateResultForDuplicateRequest()\n    {\n        return default(R);\n    }\n\n    /// <summary>\n    /// This method handles the command. It just ensures that no other request exists with\nthe same ID, and if this is the case\n    /// just enqueues the original inner command.\n    /// </summary>\n    /// <param name=\"message\">IdentifiedCommand which contains both original command &\nrequest ID</param>\n    /// <returns>Return value of inner command or default value if request same ID was\nfound</returns>\n    public async Task<R> Handle(IdentifiedCommand<T, R> message, CancellationToken\ncancellationToken)\n    {\n        var alreadyExists = await _requestManager.ExistAsync(message.Id);\n        if (alreadyExists)\n        {\n            return CreateResultForDuplicateRequest();\n        }\n        else\n        {\n            await _requestManager.CreateRequestForCommandAsync<T>(message.Id);\n            try\n            {\n                var command = message.Command;\n                var commandName = command.GetGenericTypeName();\n                var idProperty = string.Empty;\n                var commandId = string.Empty;\n\n                switch (command)\n                {\n                    case CreateOrderCommand createOrderCommand:\n                        idProperty = nameof(createOrderCommand.UserId);\n                        commandId = createOrderCommand.UserId;\n                        break;\n\n                    case CancelOrderCommand cancelOrderCommand:\n                        idProperty = nameof(cancelOrderCommand.OrderNumber);\n                        commandId = $\"{cancelOrderCommand.OrderNumber}\";\n                        break;\n\n                    case ShipOrderCommand shipOrderCommand:\n                        idProperty = nameof(shipOrderCommand.OrderNumber);\n                        commandId = $\"{shipOrderCommand.OrderNumber}\";\n                        break;\n\n                    default:\n                        idProperty = \"Id?\";\n                        commandId = \"n/a\";\n                        break;\n                }\n\n                _logger.LogInformation(\n                    \"----- Sending command: {CommandName} - {IdProperty}: {CommandId}\n({@Command})\",\n                    commandName,\n                    idProperty,\n                    commandId,\n\n284\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f                    command);\n\n                // Send the embedded business command to mediator so it runs its related\nCommandHandler\n                var result = await _mediator.Send(command, cancellationToken);\n\n                _logger.LogInformation(\n                    \"----- Command result: {@Result} - {CommandName} - {IdProperty}:\n{CommandId} ({@Command})\",\n                    result,\n                    commandName,\n                    idProperty,\n                    commandId,\n                    command);\n\n                return result;\n            }\n            catch\n            {\n                return default(R);\n            }\n        }\n    }\n}\n\nSince the IdentifiedCommand acts like a business command\u2019s envelope, when the business command\nneeds to be processed because it is not a repeated ID, then it takes that inner business command and\nresubmits it to Mediator, as in the last part of the code shown above when running\n_mediator.Send(message.Command), from the IdentifiedCommandHandler.cs.\n\nWhen doing that, it will link and run the business command handler, in this case, the\nCreateOrderCommandHandler, which is running transactions against the Ordering database, as shown\nin the following code.\n\n// CreateOrderCommandHandler.cs\npublic class CreateOrderCommandHandler\n        : IRequestHandler<CreateOrderCommand, bool>\n{\n    private readonly IOrderRepository _orderRepository;\n    private readonly IIdentityService _identityService;\n    private readonly IMediator _mediator;\n    private readonly IOrderingIntegrationEventService _orderingIntegrationEventService;\n    private readonly ILogger<CreateOrderCommandHandler> _logger;\n\n    // Using DI to inject infrastructure persistence Repositories\n    public CreateOrderCommandHandler(IMediator mediator,\n        IOrderingIntegrationEventService orderingIntegrationEventService,\n        IOrderRepository orderRepository,\n        IIdentityService identityService,\n        ILogger<CreateOrderCommandHandler> logger)\n    {\n        _orderRepository = orderRepository ?? throw new\nArgumentNullException(nameof(orderRepository));\n        _identityService = identityService ?? throw new\nArgumentNullException(nameof(identityService));\n        _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator));\n        _orderingIntegrationEventService = orderingIntegrationEventService ?? throw new\nArgumentNullException(nameof(orderingIntegrationEventService));\n        _logger = logger ?? throw new ArgumentNullException(nameof(logger));\n\n285\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    }\n\n    public async Task<bool> Handle(CreateOrderCommand message, CancellationToken\ncancellationToken)\n    {\n        // Add Integration event to clean the basket\n        var orderStartedIntegrationEvent = new\nOrderStartedIntegrationEvent(message.UserId);\n        await\n_orderingIntegrationEventService.AddAndSaveEventAsync(orderStartedIntegrationEvent);\n\n        // Add/Update the Buyer AggregateRoot\n        // DDD patterns comment: Add child entities and value-objects through the Order\nAggregate-Root\n        // methods and constructor so validations, invariants and business logic\n        // make sure that consistency is preserved across the whole aggregate\n        var address = new Address(message.Street, message.City, message.State,\nmessage.Country, message.ZipCode);\n        var order = new Order(message.UserId, message.UserName, address,\nmessage.CardTypeId, message.CardNumber, message.CardSecurityNumber, message.CardHolderName,\nmessage.CardExpiration);\n\n        foreach (var item in message.OrderItems)\n        {\n            order.AddOrderItem(item.ProductId, item.ProductName, item.UnitPrice,\nitem.Discount, item.PictureUrl, item.Units);\n        }\n\n        _logger.LogInformation(\"----- Creating Order - Order: {@Order}\", order);\n\n        _orderRepository.Add(order);\n\n        return await _orderRepository.UnitOfWork\n            .SaveEntitiesAsync(cancellationToken);\n    }\n}\n\nRegister the types used by MediatR\n\nIn order for MediatR to be aware of your command handler classes, you need to register the mediator\nclasses and the command handler classes in your IoC container. By default, MediatR uses Autofac as\nthe IoC container, but you can also use the built-in ASP.NET Core IoC container or any other container\nsupported by MediatR.\n\nThe following code shows how to register Mediator\u2019s types and commands when using Autofac\nmodules.\n\npublic class MediatorModule : Autofac.Module\n{\n    protected override void Load(ContainerBuilder builder)\n    {\n        builder.RegisterAssemblyTypes(typeof(IMediator).GetTypeInfo().Assembly)\n            .AsImplementedInterfaces();\n\n        // Register all the Command classes (they implement IRequestHandler)\n        // in assembly holding the Commands\n        builder.RegisterAssemblyTypes(typeof(CreateOrderCommand).GetTypeInfo().Assembly)\n                .AsClosedTypesOf(typeof(IRequestHandler<,>));\n        // Other types registration\n\n286\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        //...\n    }\n}\n\nThis is where \u201cthe magic happens\u201d with MediatR.\n\nAs each command handler implements the generic IRequestHandler<T> interface, when you register\nthe assemblies using RegisteredAssemblyTypes method all the types marked as IRequestHandler also\ngets registered with their Commands. For example:\n\npublic class CreateOrderCommandHandler\n  : IRequestHandler<CreateOrderCommand, bool>\n{\n\nThat is the code that correlates commands with command handlers. The handler is just a simple class,\nbut it inherits from RequestHandler<T>, where T is the command type, and MediatR makes sure it is\ninvoked with the correct payload (the command).\n\nApply cross-cutting concerns when processing commands with the\nBehaviors in MediatR\n\nThere is one more thing: being able to apply cross-cutting concerns to the mediator pipeline. You can\nalso see at the end of the Autofac registration module code how it registers a behavior type,\nspecifically, a custom LoggingBehavior class and a ValidatorBehavior class. But you could add other\ncustom behaviors, too.\n\npublic class MediatorModule : Autofac.Module\n{\n    protected override void Load(ContainerBuilder builder)\n    {\n        builder.RegisterAssemblyTypes(typeof(IMediator).GetTypeInfo().Assembly)\n            .AsImplementedInterfaces();\n\n        // Register all the Command classes (they implement IRequestHandler)\n        // in assembly holding the Commands\n        builder.RegisterAssemblyTypes(\n                              typeof(CreateOrderCommand).GetTypeInfo().Assembly).\n                                   AsClosedTypesOf(typeof(IRequestHandler<,>));\n        // Other types registration\n        //...\n        builder.RegisterGeneric(typeof(LoggingBehavior<,>)).\n                                                   As(typeof(IPipelineBehavior<,>));\n        builder.RegisterGeneric(typeof(ValidatorBehavior<,>)).\n                                                   As(typeof(IPipelineBehavior<,>));\n    }\n}\n\nThat LoggingBehavior class can be implemented as the following code, which logs information about\nthe command handler being executed and whether it was successful or not.\n\npublic class LoggingBehavior<TRequest, TResponse>\n         : IPipelineBehavior<TRequest, TResponse>\n{\n    private readonly ILogger<LoggingBehavior<TRequest, TResponse>> _logger;\n    public LoggingBehavior(ILogger<LoggingBehavior<TRequest, TResponse>> logger) =>\n                                                                  _logger = logger;\n\n287\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f    public async Task<TResponse> Handle(TRequest request,\n                                        RequestHandlerDelegate<TResponse> next)\n    {\n        _logger.LogInformation($\"Handling {typeof(TRequest).Name}\");\n        var response = await next();\n        _logger.LogInformation($\"Handled {typeof(TResponse).Name}\");\n        return response;\n    }\n}\n\nJust by implementing this behavior class and by registering it in the pipeline (in the MediatorModule\nabove), all the commands processed through MediatR will be logging information about the\nexecution.\n\nThe eShopOnContainers ordering microservice also applies a second behavior for basic validations,\nthe ValidatorBehavior class that relies on the FluentValidation library, as shown in the following code:\n\npublic class ValidatorBehavior<TRequest, TResponse>\n         : IPipelineBehavior<TRequest, TResponse>\n{\n    private readonly IValidator<TRequest>[] _validators;\n    public ValidatorBehavior(IValidator<TRequest>[] validators) =>\n                                                         _validators = validators;\n\n    public async Task<TResponse> Handle(TRequest request,\n                                        RequestHandlerDelegate<TResponse> next)\n    {\n        var failures = _validators\n            .Select(v => v.Validate(request))\n            .SelectMany(result => result.Errors)\n            .Where(error => error != null)\n            .ToList();\n\n        if (failures.Any())\n        {\n            throw new OrderingDomainException(\n                $\"Command Validation Errors for type {typeof(TRequest).Name}\",\n                        new ValidationException(\"Validation exception\", failures));\n        }\n\n        var response = await next();\n        return response;\n    }\n}\n\nHere the behavior is raising an exception if validation fails, but you could also return a result object,\ncontaining the command result if it succeeded or the validation messages in case it didn\u2019t. This would\nprobably make it easier to display validation results to the user.\n\nThen, based on the FluentValidation library, you would create validation for the data passed with\nCreateOrderCommand, as in the following code:\n\npublic class CreateOrderCommandValidator : AbstractValidator<CreateOrderCommand>\n{\n    public CreateOrderCommandValidator()\n    {\n        RuleFor(command => command.City).NotEmpty();\n        RuleFor(command => command.Street).NotEmpty();\n\n288\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f        RuleFor(command => command.State).NotEmpty();\n        RuleFor(command => command.Country).NotEmpty();\n        RuleFor(command => command.ZipCode).NotEmpty();\n        RuleFor(command => command.CardNumber).NotEmpty().Length(12, 19);\n        RuleFor(command => command.CardHolderName).NotEmpty();\n        RuleFor(command =>\ncommand.CardExpiration).NotEmpty().Must(BeValidExpirationDate).WithMessage(\"Please specify\na valid card expiration date\");\n        RuleFor(command => command.CardSecurityNumber).NotEmpty().Length(3);\n        RuleFor(command => command.CardTypeId).NotEmpty();\n        RuleFor(command => command.OrderItems).Must(ContainOrderItems).WithMessage(\"No\norder items found\");\n    }\n\n    private bool BeValidExpirationDate(DateTime dateTime)\n    {\n        return dateTime >= DateTime.UtcNow;\n    }\n\n    private bool ContainOrderItems(IEnumerable<OrderItemDTO> orderItems)\n    {\n        return orderItems.Any();\n    }\n}\n\nYou could create additional validations. This is a very clean and elegant way to implement your\ncommand validations.\n\nIn a similar way, you could implement other behaviors for additional aspects or cross-cutting concerns\nthat you want to apply to commands when handling them.\n\nAdditional resources\n\nThe mediator pattern\n\n\u2022  Mediator pattern\n\nhttps://en.wikipedia.org/wiki/Mediator_pattern\n\nThe decorator pattern\n\n\u2022\n\nDecorator pattern\nhttps://en.wikipedia.org/wiki/Decorator_pattern\n\nMediatR (Jimmy Bogard)\n\n\u2022  MediatR. GitHub repo.\n\nhttps://github.com/jbogard/MediatR\n\n\u2022\n\n\u2022\n\nCQRS with MediatR and AutoMapper\nhttps://lostechies.com/jimmybogard/2015/05/05/cqrs-with-mediatr-and-automapper/\n\nPut your controllers on a diet: POSTs and commands.\nhttps://lostechies.com/jimmybogard/2013/12/19/put-your-controllers-on-a-diet-posts-and-\ncommands/\n\n289\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\f\u2022\n\n\u2022\n\nTackling cross-cutting concerns with a mediator pipeline\nhttps://lostechies.com/jimmybogard/2014/09/09/tackling-cross-cutting-concerns-with-a-\nmediator-pipeline/\n\nCQRS and REST: the perfect match\nhttps://lostechies.com/jimmybogard/2016/06/01/cqrs-and-rest-the-perfect-match/\n\n\u2022  MediatR Pipeline Examples\n\nhttps://lostechies.com/jimmybogard/2016/10/13/mediatr-pipeline-examples/\n\n\u2022\n\nVertical Slice Test Fixtures for MediatR and ASP.NET Core\nhttps://lostechies.com/jimmybogard/2016/10/24/vertical-slice-test-fixtures-for-mediatr-and-\nasp-net-core/\n\n\u2022  MediatR Extensions for Microsoft Dependency Injection Released\n\nhttps://lostechies.com/jimmybogard/2016/07/19/mediatr-extensions-for-microsoft-\ndependency-injection-released/\n\nFluent validation\n\n\u2022\n\nJeremy Skinner. FluentValidation. GitHub repo.\nhttps://github.com/JeremySkinner/FluentValidation\n\n290\n\nCHAPTER 6 | Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\n\n\fCHAPTER  7\n\nImplement resilient\napplications\n\nYour microservice and cloud-based applications must embrace the partial failures that will certainly\noccur eventually. You must design your application to be resilient to those partial failures.\n\nResiliency is the ability to recover from failures and continue to function. It isn\u2019t about avoiding\nfailures but accepting the fact that failures will happen and responding to them in a way that avoids\ndowntime or data loss. The goal of resiliency is to return the application to a fully functioning state\nafter a failure.\n\nIt\u2019s challenging enough to design and deploy a microservices-based application. But you also need to\nkeep your application running in an environment where some sort of failure is certain. Therefore, your\napplication should be resilient. It should be designed to cope with partial failures, like network\noutages or nodes or VMs crashing in the cloud. Even microservices (containers) being moved to a\ndifferent node within a cluster can cause intermittent short failures within the application.\n\nThe many individual components of your application should also incorporate health monitoring\nfeatures. By following the guidelines in this chapter, you can create an application that can work\nsmoothly in spite of transient downtime or the normal hiccups that occur in complex and cloud-based\ndeployments.\n\nImportant\n\neShopOnContainer had been using the Polly library to implement resiliency using Typed Clients up\nuntil the release 3.0.0.\n\nStarting with release 3.0.0, the HTTP calls resiliency is implemented using a Linkerd mesh, that handles\nretries in a transparent and configurable fashion, within a Kubernetes cluster, without having to\nhandle those concerns in the code.\n\nThe Polly library is still used to add resilience to database connections, specially while starting up the\nservices.\n\nWarning\n\nAll code samples and images in this section were valid before using Linkerd and are not updated to\nreflect the current actual code. So they make sense in the context of this section.\n\n291\n\nCHAPTER 7 | Implement resilient applications\n\n\fHandle partial failure\n\nIn distributed systems like microservices-based applications, there\u2019s an ever-present risk of partial\nfailure. For instance, a single microservice/container can fail or might not be available to respond for a\nshort time, or a single VM or server can crash. Since clients and services are separate processes, a\nservice might not be able to respond in a timely way to a client\u2019s request. The service might be\noverloaded and responding very slowly to requests or might simply not be accessible for a short time\nbecause of network issues.\n\nFor example, consider the Order details page from the eShopOnContainers sample application. If the\nordering microservice is unresponsive when the user tries to submit an order, a bad implementation\nof the client process (the MVC web application)\u2014for example, if the client code were to use\nsynchronous RPCs with no timeout\u2014would block threads indefinitely waiting for a response. Besides\ncreating a bad user experience, every unresponsive wait consumes or blocks a thread, and threads are\nextremely valuable in highly scalable applications. If there are many blocked threads, eventually the\napplication\u2019s runtime can run out of threads. In that case, the application can become globally\nunresponsive instead of just partially unresponsive, as shown in Figure 8-1.\n\nFigure 8-1. Partial failures because of dependencies that impact service thread availability\n\nIn a large microservices-based application, any partial failure can be amplified, especially if most of\nthe internal microservices interaction is based on synchronous HTTP calls (which is considered an anti-\npattern). Think about a system that receives millions of incoming calls per day. If your system has a\nbad design that\u2019s based on long chains of synchronous HTTP calls, these incoming calls might result in\nmany more millions of outgoing calls (let\u2019s suppose a ratio of 1:4) to dozens of internal microservices\nas synchronous dependencies. This situation is shown in Figure 8-2, especially dependency #3, that\nstarts a chain, calling dependency #4, which then calls #5.\n\n292\n\nCHAPTER 7 | Implement resilient applications\n\n\fFigure 8-2. The impact of having an incorrect design featuring long chains of HTTP requests\n\nIntermittent failure is guaranteed in a distributed and cloud-based system, even if every dependency\nitself has excellent availability. It\u2019s a fact you need to consider.\n\nIf you do not design and implement techniques to ensure fault tolerance, even small downtimes can\nbe amplified. As an example, 50 dependencies each with 99.99% of availability would result in several\nhours of downtime each month because of this ripple effect. When a microservice dependency fails\nwhile handling a high volume of requests, that failure can quickly saturate all available request threads\nin each service and crash the whole application.\n\nFigure 8-3. Partial failure amplified by microservices with long chains of synchronous HTTP calls\n\n293\n\nCHAPTER 7 | Implement resilient applications\n\n\fTo minimize this problem, in the section Asynchronous microservice integration enforce microservice\u2019s\nautonomy, this guide encourages you to use asynchronous communication across the internal\nmicroservices.\n\nIn addition, it\u2019s essential that you design your microservices and client applications to handle partial\nfailures\u2014that is, to build resilient microservices and client applications.\n\nStrategies to handle partial failure\n\nTo deal with partial failures, use one of the strategies described here.\n\nUse asynchronous communication (for example, message-based communication) across\ninternal microservices. It\u2019s highly advisable not to create long chains of synchronous HTTP calls\nacross the internal microservices because that incorrect design will eventually become the main cause\nof bad outages. On the contrary, except for the front-end communications between the client\napplications and the first level of microservices or fine-grained API Gateways, it\u2019s recommended to use\nonly asynchronous (message-based) communication once past the initial request/response cycle,\nacross the internal microservices. Eventual consistency and event-driven architectures will help to\nminimize ripple effects. These approaches enforce a higher level of microservice autonomy and\ntherefore prevent against the problem noted here.\n\nUse retries with exponential backoff. This technique helps to avoid short and intermittent failures\nby performing call retries a certain number of times, in case the service was not available only for a\nshort time. This might occur due to intermittent network issues or when a microservice/container is\nmoved to a different node in a cluster. However, if these retries are not designed properly with circuit\nbreakers, it can aggravate the ripple effects, ultimately even causing a Denial of Service (DoS).\n\nWork around network timeouts. In general, clients should be designed not to block indefinitely and\nto always use timeouts when waiting for a response. Using timeouts ensures that resources are never\ntied up indefinitely.\n\nUse the Circuit Breaker pattern. In this approach, the client process tracks the number of failed\nrequests. If the error rate exceeds a configured limit, a \u201ccircuit breaker\u201d trips so that further attempts\nfail immediately. (If a large number of requests are failing, that suggests the service is unavailable and\nthat sending requests is pointless.) After a timeout period, the client should try again and, if the new\nrequests are successful, close the circuit breaker.\n\nProvide fallbacks. In this approach, the client process performs fallback logic when a request fails,\nsuch as returning cached data or a default value. This is an approach suitable for queries, and is more\ncomplex for updates or commands.\n\nLimit the number of queued requests. Clients should also impose an upper bound on the number\nof outstanding requests that a client microservice can send to a particular service. If the limit has been\nreached, it\u2019s probably pointless to make additional requests, and those attempts should fail\nimmediately. In terms of implementation, the Polly Bulkhead Isolation policy can be used to fulfill this\nrequirement. This approach is essentially a parallelization throttle with SemaphoreSlim as the\nimplementation. It also permits a \u201cqueue\u201d outside the bulkhead. You can proactively shed excess load\neven before execution (for example, because capacity is deemed full). This makes its response to\n\n294\n\nCHAPTER 7 | Implement resilient applications\n\n\fcertain failure scenarios faster than a circuit breaker would be, since the circuit breaker waits for the\nfailures. The BulkheadPolicy object in Polly exposes how full the bulkhead and queue are, and offers\nevents on overflow so can also be used to drive automated horizontal scaling.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nResiliency patterns\nhttps://learn.microsoft.com/azure/architecture/framework/resiliency/reliability-patterns\n\nAdding Resilience and Optimizing Performance\nhttps://learn.microsoft.com/previous-versions/msp-n-p/jj591574(v=pandp.10)\n\nBulkhead. GitHub repo. Implementation with Polly policy.\nhttps://github.com/App-vNext/Polly/wiki/Bulkhead\n\nDesigning resilient applications for Azure\nhttps://learn.microsoft.com/azure/architecture/framework/resiliency/app-design\n\nTransient fault handling\nhttps://learn.microsoft.com/azure/architecture/best-practices/transient-faults\n\nImplement retries with exponential backoff\n\nRetries with exponential backoff is a technique that retries an operation, with an exponentially\nincreasing wait time, up to a maximum retry count has been reached (the exponential backoff). This\ntechnique embraces the fact that cloud resources might intermittently be unavailable for more than a\nfew seconds for any reason. For example, an orchestrator might be moving a container to another\nnode in a cluster for load balancing. During that time, some requests might fail. Another example\ncould be a database like SQL Azure, where a database can be moved to another server for load\nbalancing, causing the database to be unavailable for a few seconds.\n\nThere are many approaches to implement retries logic with exponential backoff.\n\nImplement resilient Entity Framework Core SQL\nconnections\n\nFor Azure SQL DB, Entity Framework (EF) Core already provides internal database connection resiliency\nand retry logic. But you need to enable the Entity Framework execution strategy for each DbContext\nconnection if you want to have resilient EF Core connections.\n\nFor instance, the following code at the EF Core connection level enables resilient SQL connections that\nare retried if the connection fails.\n\n// Program.cs from any ASP.NET Core Web API\n// Other code ...\nbuilder.Services.AddDbContext<CatalogContext>(options =>\n    {\n\n295\n\nCHAPTER 7 | Implement resilient applications\n\n\f        options.UseSqlServer(builder.Configuration[\"ConnectionString\"],\n        sqlServerOptionsAction: sqlOptions =>\n        {\n            sqlOptions.EnableRetryOnFailure(\n            maxRetryCount: 10,\n            maxRetryDelay: TimeSpan.FromSeconds(30),\n            errorNumbersToAdd: null);\n        });\n    });\n\nExecution strategies and explicit transactions using BeginTransaction\nand multiple DbContexts\n\nWhen retries are enabled in EF Core connections, each operation you perform using EF Core becomes\nits own retryable operation. Each query and each call to SaveChanges will be retried as a unit if a\ntransient failure occurs.\n\nHowever, if your code initiates a transaction using BeginTransaction, you\u2019re defining your own group\nof operations that need to be treated as a unit. Everything inside the transaction has to be rolled back\nif a failure occurs.\n\nIf you try to execute that transaction when using an EF execution strategy (retry policy) and you call\nSaveChanges from multiple DbContexts, you\u2019ll get an exception like this one:\n\nSystem.InvalidOperationException: The configured execution strategy\n\u2018SqlServerRetryingExecutionStrategy\u2019 does not support user initiated transactions. Use the execution\nstrategy returned by \u2018DbContext.Database.CreateExecutionStrategy()\u2019 to execute all the operations in\nthe transaction as a retriable unit.\n\nThe solution is to manually invoke the EF execution strategy with a delegate representing everything\nthat needs to be executed. If a transient failure occurs, the execution strategy will invoke the delegate\nagain. For example, the following code shows how it\u2019s implemented in eShopOnContainers with two\nmultiple DbContexts (_catalogContext and the IntegrationEventLogContext) when updating a product\nand then saving the ProductPriceChangedIntegrationEvent object, which needs to use a different\nDbContext.\n\npublic async Task<IActionResult> UpdateProduct(\n    [FromBody]CatalogItem productToUpdate)\n{\n    // Other code ...\n\n    var oldPrice = catalogItem.Price;\n    var raiseProductPriceChangedEvent = oldPrice != productToUpdate.Price;\n\n    // Update current product\n    catalogItem = productToUpdate;\n\n    // Save product's data and publish integration event through the Event Bus\n    // if price has changed\n    if (raiseProductPriceChangedEvent)\n    {\n        //Create Integration Event to be published through the Event Bus\n        var priceChangedEvent = new ProductPriceChangedIntegrationEvent(\n          catalogItem.Id, productToUpdate.Price, oldPrice);\n\n296\n\nCHAPTER 7 | Implement resilient applications\n\n\f       // Achieving atomicity between original Catalog database operation and the\n       // IntegrationEventLog thanks to a local transaction\n       await _catalogIntegrationEventService.SaveEventAndCatalogContextChangesAsync(\n           priceChangedEvent);\n\n       // Publish through the Event Bus and mark the saved event as published\n       await _catalogIntegrationEventService.PublishThroughEventBusAsync(\n           priceChangedEvent);\n    }\n    // Just save the updated product because the Product's Price hasn't changed.\n    else\n    {\n        await _catalogContext.SaveChangesAsync();\n    }\n}\n\nThe first DbContext is _catalogContext and the second DbContext is within the\n_catalogIntegrationEventService object. The Commit action is performed across all DbContext objects\nusing an EF execution strategy.\n\nTo achieve this multiple DbContext commit, the SaveEventAndCatalogContextChangesAsync uses a\nResilientTransaction class, as shown in the following code:\n\npublic class CatalogIntegrationEventService : ICatalogIntegrationEventService\n{\n    //\u2026\n    public async Task SaveEventAndCatalogContextChangesAsync(\n        IntegrationEvent evt)\n    {\n        // Use of an EF Core resiliency strategy when using multiple DbContexts\n        // within an explicit BeginTransaction():\n        // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency\n        await ResilientTransaction.New(_catalogContext).ExecuteAsync(async () =>\n        {\n            // Achieving atomicity between original catalog database\n            // operation and the IntegrationEventLog thanks to a local transaction\n            await _catalogContext.SaveChangesAsync();\n            await _eventLogService.SaveEventAsync(evt,\n                _catalogContext.Database.CurrentTransaction.GetDbTransaction());\n        });\n    }\n}\n\nThe ResilientTransaction.ExecuteAsync method basically begins a transaction from the passed\nDbContext (_catalogContext) and then makes the EventLogService use that transaction to save\nchanges from the IntegrationEventLogContext and then commits the whole transaction.\n\npublic class ResilientTransaction\n{\n    private DbContext _context;\n    private ResilientTransaction(DbContext context) =>\n        _context = context ?? throw new ArgumentNullException(nameof(context));\n\n    public static ResilientTransaction New (DbContext context) =>\n        new ResilientTransaction(context);\n\n    public async Task ExecuteAsync(Func<Task> action)\n    {\n\n297\n\nCHAPTER 7 | Implement resilient applications\n\n\f        // Use of an EF Core resiliency strategy when using multiple DbContexts\n        // within an explicit BeginTransaction():\n        // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency\n        var strategy = _context.Database.CreateExecutionStrategy();\n        await strategy.ExecuteAsync(async () =>\n        {\n            await using var transaction = await _context.Database.BeginTransactionAsync();\n            await action();\n            await transaction.CommitAsync();\n        });\n    }\n}\n\nAdditional resources\n\n\u2022\n\n\u2022\n\nConnection Resiliency and Command Interception with EF in an ASP.NET MVC\nApplication\nhttps://learn.microsoft.com/aspnet/mvc/overview/getting-started/getting-started-with-ef-\nusing-mvc/connection-resiliency-and-command-interception-with-the-entity-framework-in-\nan-asp-net-mvc-application\n\nCesar de la Torre. Using Resilient Entity Framework Core SQL Connections and\nTransactions\nhttps://devblogs.microsoft.com/cesardelatorre/using-resilient-entity-framework-core-sql-\nconnections-and-transactions-retries-with-exponential-backoff/\n\nUse IHttpClientFactory to implement resilient HTTP\nrequests\n\nIHttpClientFactory is a contract implemented by DefaultHttpClientFactory, an opinionated factory,\navailable since .NET Core 2.1, for creating HttpClient instances to be used in your applications.\n\nIssues with the original HttpClient class available in .NET\n\nThe original and well-known HttpClient class can be easily used, but in some cases, it isn\u2019t being\nproperly used by many developers.\n\nThough this class implements IDisposable, declaring and instantiating it within a using statement is\nnot preferred because when the HttpClient object gets disposed of, the underlying socket is not\nimmediately released, which can lead to a socket exhaustion problem. For more information about this\nissue, see the blog post You\u2019re using HttpClient wrong and it\u2019s destabilizing your software.\n\nTherefore, HttpClient is intended to be instantiated once and reused throughout the life of an\napplication. Instantiating an HttpClient class for every request will exhaust the number of sockets\navailable under heavy loads. That issue will result in SocketException errors. Possible approaches to\nsolve that problem are based on the creation of the HttpClient object as singleton or static, as\nexplained in this Microsoft article on HttpClient usage. This can be a good solution for short-lived\nconsole apps or similar, that run a few times a day.\n\n298\n\nCHAPTER 7 | Implement resilient applications\n\n\fAnother issue that developers run into is when using a shared instance of HttpClient in long-running\nprocesses. In a situation where the HttpClient is instantiated as a singleton or a static object, it fails to\nhandle the DNS changes as described in this issue of the dotnet/runtime GitHub repository.\n\nHowever, the issue isn\u2019t really with HttpClient per se, but with the default constructor for HttpClient,\nbecause it creates a new concrete instance of HttpMessageHandler, which is the one that has sockets\nexhaustion and DNS changes issues mentioned above.\n\nTo address the issues mentioned above and to make HttpClient instances manageable, .NET Core 2.1\nintroduced two approaches, one of them being IHttpClientFactory. It\u2019s an interface that\u2019s used to\nconfigure and create HttpClient instances in an app through Dependency Injection (DI). It also\nprovides extensions for Polly-based middleware to take advantage of delegating handlers in\nHttpClient.\n\nThe alternative is to use SocketsHttpHandler with configured PooledConnectionLifetime. This\napproach is applied to long-lived, static or singleton HttpClient instances. To learn more about\ndifferent strategies, see HttpClient guidelines for .NET.\n\nPolly is a transient-fault-handling library that helps developers add resiliency to their applications, by\nusing some pre-defined policies in a fluent and thread-safe manner.\n\nBenefits of using IHttpClientFactory\n\nThe current implementation of IHttpClientFactory, that also implements IHttpMessageHandlerFactory,\noffers the following benefits:\n\n\u2022\n\n\u2022\n\n\u2022\n\nProvides a central location for naming and configuring logical HttpClient objects. For\nexample, you may configure a client (Service Agent) that\u2019s pre-configured to access a specific\nmicroservice.\n\nCodify the concept of outgoing middleware via delegating handlers in HttpClient and\nimplementing Polly-based middleware to take advantage of Polly\u2019s policies for resiliency.\n\nHttpClient already has the concept of delegating handlers that could be linked together for\noutgoing HTTP requests. You can register HTTP clients into the factory and you can use a\nPolly handler to use Polly policies for Retry, CircuitBreakers, and so on.\n\n\u2022  Manage the lifetime of HttpMessageHandler to avoid the mentioned problems/issues that\n\ncan occur when managing HttpClient lifetimes yourself.\n\nTip\n\nThe HttpClient instances injected by DI can be disposed of safely, because the associated\nHttpMessageHandler is managed by the factory. Injected HttpClient instances are Transient from a DI\nperspective, while HttpMessageHandler instances can be regarded as Scoped. HttpMessageHandler\ninstances have their own DI scopes, separate from the application scopes (for example, ASP.NET\nincoming request scopes). For more information, see Using HttpClientFactory in .NET.\n\n299\n\nCHAPTER 7 | Implement resilient applications\n\n\fNote\n\nThe implementation of IHttpClientFactory (DefaultHttpClientFactory) is tightly tied to the DI\nimplementation in the Microsoft.Extensions.DependencyInjection NuGet package. If you need to use\nHttpClient without DI or with other DI implementations, consider using a static or singleton HttpClient\nwith PooledConnectionLifetime set up. For more information, see HttpClient guidelines for .NET.\n\nMultiple ways to use IHttpClientFactory\n\nThere are several ways that you can use IHttpClientFactory in your application:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nBasic usage\n\nUse Named Clients\n\nUse Typed Clients\n\nUse Generated Clients\n\nFor the sake of brevity, this guidance shows the most structured way to use IHttpClientFactory, which\nis to use Typed Clients (Service Agent pattern). However, all options are documented and are currently\nlisted in this article covering the IHttpClientFactory usage.\n\nNote\n\nIf your app requires cookies, it might be better to avoid using IHttpClientFactory in your app. For\nalternative ways of managing clients, see Guidelines for using HTTP clients\n\nHow to use Typed Clients with IHttpClientFactory\n\nSo, what\u2019s a \u201cTyped Client\u201d? It\u2019s just an HttpClient that\u2019s pre-configured for some specific use. This\nconfiguration can include specific values such as the base server, HTTP headers or time outs.\n\nThe following diagram shows how Typed Clients are used with IHttpClientFactory:\n\n300\n\nCHAPTER 7 | Implement resilient applications\n\n\fFigure 8-4. Using IHttpClientFactory with Typed Client classes.\n\nIn the above image, a ClientService (used by a controller or client code) uses an HttpClient created by\nthe registered IHttpClientFactory. This factory assigns an HttpMessageHandler from a pool to the\nHttpClient. The HttpClient can be configured with Polly\u2019s policies when registering the\nIHttpClientFactory in the DI container with the extension method AddHttpClient.\n\nTo configure the above structure, add IHttpClientFactory in your application by installing the\nMicrosoft.Extensions.Http NuGet package that includes the AddHttpClient extension method for\nIServiceCollection. This extension method registers the internal DefaultHttpClientFactory class to be\nused as a singleton for the interface IHttpClientFactory. It defines a transient configuration for the\nHttpMessageHandlerBuilder. This message handler (HttpMessageHandler object), taken from a pool,\nis used by the HttpClient returned from the factory.\n\nIn the next snippet, you can see how AddHttpClient() can be used to register Typed Clients (Service\nAgents) that need to use HttpClient.\n\n// Program.cs\n//Add http client services at ConfigureServices(IServiceCollection services)\nbuilder.Services.AddHttpClient<ICatalogService, CatalogService>();\nbuilder.Services.AddHttpClient<IBasketService, BasketService>();\nbuilder.Services.AddHttpClient<IOrderingService, OrderingService>();\n\n301\n\nCHAPTER 7 | Implement resilient applications\n\n\fRegistering the client services as shown in the previous snippet, makes the DefaultClientFactory create\na standard HttpClient for each service. The typed client is registered as transient with DI container. In\nthe preceding code, AddHttpClient() registers CatalogService, BasketService, OrderingService as\ntransient services so they can be injected and consumed directly without any need for additional\nregistrations.\n\nYou could also add instance-specific configuration in the registration to, for example, configure the\nbase address, and add some resiliency policies, as shown in the following:\n\nbuilder.Services.AddHttpClient<ICatalogService, CatalogService>(client =>\n{\n    client.BaseAddress = new Uri(builder.Configuration[\"BaseUrl\"]);\n})\n    .AddPolicyHandler(GetRetryPolicy())\n    .AddPolicyHandler(GetCircuitBreakerPolicy());\n\nIn this next example, you can see the configuration of one of the above policies:\n\nstatic IAsyncPolicy<HttpResponseMessage> GetRetryPolicy()\n{\n    return HttpPolicyExtensions\n        .HandleTransientHttpError()\n        .OrResult(msg => msg.StatusCode == System.Net.HttpStatusCode.NotFound)\n        .WaitAndRetryAsync(6, retryAttempt => TimeSpan.FromSeconds(Math.Pow(2,\nretryAttempt)));\n}\n\nYou can find more details about using Polly in the Next article.\n\nHttpClient lifetimes\n\nEach time you get an HttpClient object from the IHttpClientFactory, a new instance is returned. But\neach HttpClient uses an HttpMessageHandler that\u2019s pooled and reused by the IHttpClientFactory to\nreduce resource consumption, as long as the HttpMessageHandler\u2019s lifetime hasn\u2019t expired.\n\nPooling of handlers is desirable as each handler typically manages its own underlying HTTP\nconnections; creating more handlers than necessary can result in connection delays. Some handlers\nalso keep connections open indefinitely, which can prevent the handler from reacting to DNS\nchanges.\n\nThe HttpMessageHandler objects in the pool have a lifetime that\u2019s the length of time that an\nHttpMessageHandler instance in the pool can be reused. The default value is two minutes, but it can\nbe overridden per Typed Client. To override it, call SetHandlerLifetime() on the IHttpClientBuilder\nthat\u2019s returned when creating the client, as shown in the following code:\n\n//Set 5 min as the lifetime for the HttpMessageHandler objects in the pool used for the\nCatalog Typed Client\nbuilder.Services.AddHttpClient<ICatalogService, CatalogService>()\n    .SetHandlerLifetime(TimeSpan.FromMinutes(5));\n\nEach Typed Client can have its own configured handler lifetime value. Set the lifetime to\nInfiniteTimeSpan to disable handler expiry.\n\n302\n\nCHAPTER 7 | Implement resilient applications\n\n\fImplement your Typed Client classes that use the injected and configured\nHttpClient\n\nAs a previous step, you need to have your Typed Client classes defined, such as the classes in the\nsample code, like \u2018BasketService\u2019, \u2018CatalogService\u2019, \u2018OrderingService\u2019, etc. \u2013 A Typed Client is a class\nthat accepts an HttpClient object (injected through its constructor) and uses it to call some remote\nHTTP service. For example:\n\npublic class CatalogService : ICatalogService\n{\n    private readonly HttpClient _httpClient;\n    private readonly string _remoteServiceBaseUrl;\n\n    public CatalogService(HttpClient httpClient)\n    {\n        _httpClient = httpClient;\n    }\n\n    public async Task<Catalog> GetCatalogItems(int page, int take,\n                                               int? brand, int? type)\n    {\n        var uri = API.Catalog.GetAllCatalogItems(_remoteServiceBaseUrl,\n                                                 page, take, brand, type);\n\n        var responseString = await _httpClient.GetStringAsync(uri);\n\n        var catalog = JsonConvert.DeserializeObject<Catalog>(responseString);\n        return catalog;\n    }\n}\n\nThe Typed Client (CatalogService in the example) is activated by DI (Dependency Injection), which\nmeans it can accept any registered service in its constructor, in addition to HttpClient.\n\nA Typed Client is effectively a transient object, that means a new instance is created each time one is\nneeded. It receives a new HttpClient instance each time it\u2019s constructed. However, the\nHttpMessageHandler objects in the pool are the objects that are reused by multiple HttpClient\ninstances.\n\nUse your Typed Client classes\n\nFinally, once you have your typed classes implemented, you can have them registered and configured\nwith AddHttpClient(). After that you can use them wherever services are injected by DI, such as in\nRazor page code or an MVC web app controller, shown in the below code from eShopOnContainers:\n\nnamespace Microsoft.eShopOnContainers.WebMVC.Controllers\n{\n    public class CatalogController : Controller\n    {\n        private ICatalogService _catalogSvc;\n\n        public CatalogController(ICatalogService catalogSvc) =>\n                                                           _catalogSvc = catalogSvc;\n\n        public async Task<IActionResult> Index(int? BrandFilterApplied,\n                                               int? TypesFilterApplied,\n\n303\n\nCHAPTER 7 | Implement resilient applications\n\n\f                                               int? page,\n                                               [FromQuery]string errorMsg)\n        {\n            var itemsPage = 10;\n            var catalog = await _catalogSvc.GetCatalogItems(page ?? 0,\n                                                            itemsPage,\n                                                            BrandFilterApplied,\n                                                            TypesFilterApplied);\n            //\u2026 Additional code\n        }\n\n        }\n}\n\nUp to this point, the above code snippet only shows the example of performing regular HTTP\nrequests. But the \u2018magic\u2019 comes in the following sections where it shows how all the HTTP requests\nmade by HttpClient can have resilient policies such as retries with exponential backoff, circuit\nbreakers, security features using auth tokens, or even any other custom feature. And all of these can\nbe done just by adding policies and delegating handlers to your registered Typed Clients.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nHttpClient guidelines for .NET\nhttps://learn.microsoft.com/en-us/dotnet/fundamentals/networking/http/httpclient-\nguidelines\n\nUsing HttpClientFactory in .NET\nhttps://learn.microsoft.com/en-us/dotnet/core/extensions/httpclient-factory\n\nUsing HttpClientFactory in ASP.NET Core\nhttps://learn.microsoft.com/aspnet/core/fundamentals/http-requests\n\nHttpClientFactory source code in the dotnet/runtime GitHub repository\nhttps://github.com/dotnet/runtime/tree/release/7.0/src/libraries/Microsoft.Extensions.Http/\n\nPolly (.NET resilience and transient-fault-handling library)\nhttps://thepollyproject.azurewebsites.net/\n\nImplement HTTP call retries with exponential backoff\nwith IHttpClientFactory and Polly policies\n\nThe recommended approach for retries with exponential backoff is to take advantage of more\nadvanced .NET libraries like the open-source Polly library.\n\nPolly is a .NET library that provides resilience and transient-fault handling capabilities. You can\nimplement those capabilities by applying Polly policies such as Retry, Circuit Breaker, Bulkhead\nIsolation, Timeout, and Fallback. Polly targets .NET Framework 4.x and .NET Standard 1.0, 1.1, and 2.0\n(which supports .NET Core and later).\n\n304\n\nCHAPTER 7 | Implement resilient applications\n\n\fThe following steps show how you can use Http retries with Polly integrated into IHttpClientFactory,\nwhich is explained in the previous section.\n\nInstall .NET packages\n\nFirst, you will need to install the Microsoft.Extensions.Http.Polly package.\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nInstall with Visual Studio\n\nInstall with dotnet CLI\n\nInstall with nuget.exe CLI\n\nInstall with Package Manager Console (PowerShell)\n\nReference the .NET 7 packages\n\nIHttpClientFactory is available since .NET Core 2.1, however, we recommend you use the latest .NET 7\npackages from NuGet in your project. You typically also need to reference the extension package\nMicrosoft.Extensions.Http.Polly.\n\nConfigure a client with Polly\u2019s Retry policy, in app startup\n\nThe AddPolicyHandler() method is what adds policies to the HttpClient objects you\u2019ll use. In this\ncase, it\u2019s adding a Polly\u2019s policy for Http Retries with exponential backoff.\n\nTo have a more modular approach, the Http Retry Policy can be defined in a separate method within\nthe Program.cs file, as shown in the following code:\n\nstatic IAsyncPolicy<HttpResponseMessage> GetRetryPolicy()\n{\n    return HttpPolicyExtensions\n        .HandleTransientHttpError()\n        .OrResult(msg => msg.StatusCode == System.Net.HttpStatusCode.NotFound)\n        .WaitAndRetryAsync(6, retryAttempt => TimeSpan.FromSeconds(Math.Pow(2,\n                                                                    retryAttempt)));\n}\n\nAs shown in previous sections, you need to define a named or typed client HttpClient configuration in\nyour standard Program.cs app configuration. Now you add incremental code specifying the policy for\nthe Http retries with exponential backoff, as follows:\n\n// Program.cs\nbuilder.Services.AddHttpClient<IBasketService, BasketService>()\n        .SetHandlerLifetime(TimeSpan.FromMinutes(5))  //Set lifetime to five minutes\n        .AddPolicyHandler(GetRetryPolicy());\n\nWith Polly, you can define a Retry policy with the number of retries, the exponential backoff\nconfiguration, and the actions to take when there\u2019s an HTTP exception, such as logging the error. In\nthis case, the policy is configured to try six times with an exponential retry, starting at two seconds.\n\nAdd a jitter strategy to the retry policy\n\nA regular Retry policy can affect your system in cases of high concurrency and scalability and under\nhigh contention. To overcome peaks of similar retries coming from many clients in partial outages, a\ngood workaround is to add a jitter strategy to the retry algorithm/policy. This strategy can improve\nthe overall performance of the end-to-end system. As recommended in Polly: Retry with Jitter, a good\n\n305\n\nCHAPTER 7 | Implement resilient applications\n\n\fjitter strategy can be implemented by smooth and evenly distributed retry intervals applied with a\nwell-controlled median initial retry delay on an exponential backoff. This approach helps to spread out\nthe spikes when the issue arises. The principle is illustrated by the following example:\n\nvar delay = Backoff.DecorrelatedJitterBackoffV2(medianFirstRetryDelay:\nTimeSpan.FromSeconds(1), retryCount: 5);\n\nvar retryPolicy = Policy\n    .Handle<FooException>()\n    .WaitAndRetryAsync(delay);\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nRetry pattern https://learn.microsoft.com/azure/architecture/patterns/retry\n\nPolly and IHttpClientFactory https://github.com/App-vNext/Polly/wiki/Polly-and-\nHttpClientFactory\n\nPolly (.NET resilience and transient-fault-handling library) https://github.com/App-\nvNext/Polly\n\n\u2022\n\nPolly: Retry with Jitter https://github.com/App-vNext/Polly/wiki/Retry-with-jitter\n\n\u2022  Marc Brooker. Jitter: Making Things Better With Randomness\n\nhttps://brooker.co.za/blog/2015/03/21/backoff.html\n\nImplement the Circuit Breaker pattern\n\nAs noted earlier, you should handle faults that might take a variable amount of time to recover from,\nas might happen when you try to connect to a remote service or resource. Handling this type of fault\ncan improve the stability and resiliency of an application.\n\nIn a distributed environment, calls to remote resources and services can fail due to transient faults,\nsuch as slow network connections and timeouts, or if resources are responding slowly or are\ntemporarily unavailable. These faults typically correct themselves after a short time, and a robust cloud\napplication should be prepared to handle them by using a strategy like the \u201cRetry pattern\u201d.\n\nHowever, there can also be situations where faults are due to unanticipated events that might take\nmuch longer to fix. These faults can range in severity from a partial loss of connectivity to the\ncomplete failure of a service. In these situations, it might be pointless for an application to continually\nretry an operation that\u2019s unlikely to succeed.\n\nInstead, the application should be coded to accept that the operation has failed and handle the failure\naccordingly.\n\nUsing Http retries carelessly could result in creating a Denial of Service (DoS) attack within your own\nsoftware. As a microservice fails or performs slowly, multiple clients might repeatedly retry failed\nrequests. That creates a dangerous risk of exponentially increasing traffic targeted at the failing\nservice.\n\n306\n\nCHAPTER 7 | Implement resilient applications\n\n\fTherefore, you need some kind of defense barrier so that excessive requests stop when it isn\u2019t worth\nto keep trying. That defense barrier is precisely the circuit breaker.\n\nThe Circuit Breaker pattern has a different purpose than the \u201cRetry pattern\u201d. The \u201cRetry pattern\u201d\nenables an application to retry an operation in the expectation that the operation will eventually\nsucceed. The Circuit Breaker pattern prevents an application from performing an operation that\u2019s\nlikely to fail. An application can combine these two patterns. However, the retry logic should be\nsensitive to any exception returned by the circuit breaker, and it should abandon retry attempts if the\ncircuit breaker indicates that a fault is not transient.\n\nImplement Circuit Breaker pattern with IHttpClientFactory and Polly\n\nAs when implementing retries, the recommended approach for circuit breakers is to take advantage of\nproven .NET libraries like Polly and its native integration with IHttpClientFactory.\n\nAdding a circuit breaker policy into your IHttpClientFactory outgoing middleware pipeline is as simple\nas adding a single incremental piece of code to what you already have when using IHttpClientFactory.\n\nThe only addition here to the code used for HTTP call retries is the code where you add the Circuit\nBreaker policy to the list of policies to use, as shown in the following incremental code.\n\n// Program.cs\nvar retryPolicy = GetRetryPolicy();\nvar circuitBreakerPolicy = GetCircuitBreakerPolicy();\n\nbuilder.Services.AddHttpClient<IBasketService, BasketService>()\n        .SetHandlerLifetime(TimeSpan.FromMinutes(5))  // Sample: default lifetime is 2\nminutes\n        .AddHttpMessageHandler<HttpClientAuthorizationDelegatingHandler>()\n        .AddPolicyHandler(retryPolicy)\n        .AddPolicyHandler(circuitBreakerPolicy);\n\nThe AddPolicyHandler() method is what adds policies to the HttpClient objects you\u2019ll use. In this case,\nit\u2019s adding a Polly policy for a circuit breaker.\n\nTo have a more modular approach, the Circuit Breaker Policy is defined in a separate method called\nGetCircuitBreakerPolicy(), as shown in the following code:\n\n// also in Program.cs\nstatic IAsyncPolicy<HttpResponseMessage> GetCircuitBreakerPolicy()\n{\n    return HttpPolicyExtensions\n        .HandleTransientHttpError()\n        .CircuitBreakerAsync(5, TimeSpan.FromSeconds(30));\n}\n\nIn the code example above, the circuit breaker policy is configured so it breaks or opens the circuit\nwhen there have been five consecutive faults when retrying the Http requests. When that happens,\nthe circuit will break for 30 seconds: in that period, calls will be failed immediately by the circuit-\nbreaker rather than actually be placed. The policy automatically interprets relevant exceptions and\nHTTP status codes as faults.\n\nCircuit breakers should also be used to redirect requests to a fallback infrastructure if you had issues\nin a particular resource that\u2019s deployed in a different environment than the client application or\n\n307\n\nCHAPTER 7 | Implement resilient applications\n\n\fservice that\u2019s performing the HTTP call. That way, if there\u2019s an outage in the datacenter that impacts\nonly your backend microservices but not your client applications, the client applications can redirect\nto the fallback services. Polly is planning a new policy to automate this failover policy scenario.\n\nAll those features are for cases where you\u2019re managing the failover from within the .NET code, as\nopposed to having it managed automatically for you by Azure, with location transparency.\n\nFrom a usage point of view, when using HttpClient, there\u2019s no need to add anything new here\nbecause the code is the same than when using HttpClient with IHttpClientFactory, as shown in\nprevious sections.\n\nTest Http retries and circuit breakers in eShopOnContainers\n\nWhenever you start the eShopOnContainers solution in a Docker host, it needs to start multiple\ncontainers. Some of the containers are slower to start and initialize, like the SQL Server container. This\nis especially true the first time you deploy the eShopOnContainers application into Docker because it\nneeds to set up the images and the database. The fact that some containers start slower than others\ncan cause the rest of the services to initially throw HTTP exceptions, even if you set dependencies\nbetween containers at the docker-compose level, as explained in previous sections. Those docker-\ncompose dependencies between containers are just at the process level. The container\u2019s entry point\nprocess might be started, but SQL Server might not be ready for queries. The result can be a cascade\nof errors, and the application can get an exception when trying to consume that particular container.\n\nYou might also see this type of error on startup when the application is deploying to the cloud. In that\ncase, orchestrators might be moving containers from one node or VM to another (that is, starting new\ninstances) when balancing the number of containers across the cluster\u2019s nodes.\n\nThe way \u2018eShopOnContainers\u2019 solves those issues when starting all the containers is by using the Retry\npattern illustrated earlier.\n\nTest the circuit breaker in eShopOnContainers\n\nThere are a few ways you can break/open the circuit and test it with eShopOnContainers.\n\nOne option is to lower the allowed number of retries to 1 in the circuit breaker policy and redeploy\nthe whole solution into Docker. With a single retry, there\u2019s a good chance that an HTTP request will\nfail during deployment, the circuit breaker will open, and you get an error.\n\nAnother option is to use custom middleware that\u2019s implemented in the Basket microservice. When\nthis middleware is enabled, it catches all HTTP requests and returns status code 500. You can enable\nthe middleware by making a GET request to the failing URI, like the following:\n\n\u2022\n\n\u2022\n\n308\n\nGET http://localhost:5103/failing\nThis request returns the current state of the middleware. If the middleware is enabled, the\nrequest return status code 500. If the middleware is disabled, there\u2019s no response.\n\nGET http://localhost:5103/failing?enable\nThis request enables the middleware.\n\nCHAPTER 7 | Implement resilient applications\n\n\f\u2022\n\nGET http://localhost:5103/failing?disable\nThis request disables the middleware.\n\nFor instance, once the application is running, you can enable the middleware by making a request\nusing the following URI in any browser. Note that the ordering microservice uses port 5103.\n\nhttp://localhost:5103/failing?enable\n\nYou can then check the status using the URI http://localhost:5103/failing, as shown in Figure 8-5.\n\nFigure 8-5. Checking the state of the \u201cFailing\u201d ASP.NET middleware \u2013 In this case, disabled.\n\nAt this point, the Basket microservice responds with status code 500 whenever you call invoke it.\n\nOnce the middleware is running, you can try making an order from the MVC web application. Because\nthe requests fail, the circuit will open.\n\nIn the following example, you can see that the MVC web application has a catch block in the logic for\nplacing an order. If the code catches an open-circuit exception, it shows the user a friendly message\ntelling them to wait.\n\npublic class CartController : Controller\n{\n    //\u2026\n    public async Task<IActionResult> Index()\n    {\n        try\n        {\n            var user = _appUserParser.Parse(HttpContext.User);\n            //Http requests using the Typed Client (Service Agent)\n            var vm = await _basketSvc.GetBasket(user);\n            return View(vm);\n        }\n        catch (BrokenCircuitException)\n        {\n            // Catches error when Basket.api is in circuit-opened mode\n            HandleBrokenCircuitException();\n        }\n        return View();\n    }\n\n    private void HandleBrokenCircuitException()\n    {\n        TempData[\"BasketInoperativeMsg\"] = \"Basket Service is inoperative, please try later\non. (Business message due to Circuit-Breaker)\";\n    }\n}\n\nHere\u2019s a summary. The Retry policy tries several times to make the HTTP request and gets HTTP errors.\nWhen the number of retries reaches the maximum number set for the Circuit Breaker policy (in this\ncase, 5), the application throws a BrokenCircuitException. The result is a friendly message, as shown in\nFigure 8-6.\n\n309\n\nCHAPTER 7 | Implement resilient applications\n\n\fFigure 8-6. Circuit breaker returning an error to the UI\n\nYou can implement different logic for when to open/break the circuit. Or you can try an HTTP request\nagainst a different back-end microservice if there\u2019s a fallback datacenter or redundant back-end\nsystem.\n\nFinally, another possibility for the CircuitBreakerPolicy is to use Isolate (which forces open and holds\nopen the circuit) and Reset (which closes it again). These could be used to build a utility HTTP\nendpoint that invokes Isolate and Reset directly on the policy. Such an HTTP endpoint could also be\nused, suitably secured, in production for temporarily isolating a downstream system, such as when\nyou want to upgrade it. Or it could trip the circuit manually to protect a downstream system you\nsuspect to be faulting.\n\nAdditional resources\n\n\u2022\n\nCircuit Breaker pattern\nhttps://learn.microsoft.com/azure/architecture/patterns/circuit-breaker\n\nHealth monitoring\n\nHealth monitoring can allow near-real-time information about the state of your containers and\nmicroservices. Health monitoring is critical to multiple aspects of operating microservices and is\nespecially important when orchestrators perform partial application upgrades in phases, as explained\nlater.\n\nMicroservices-based applications often use heartbeats or health checks to enable their performance\nmonitors, schedulers, and orchestrators to keep track of the multitude of services. If services cannot\nsend some sort of \u201cI\u2019m alive\u201d signal, either on demand or on a schedule, your application might face\nrisks when you deploy updates, or it might just detect failures too late and not be able to stop\ncascading failures that can end up in major outages.\n\nIn the typical model, services send reports about their status, and that information is aggregated to\nprovide an overall view of the state of health of your application. If you\u2019re using an orchestrator, you\ncan provide health information to your orchestrator\u2019s cluster, so that the cluster can act accordingly. If\nyou invest in high-quality health reporting that\u2019s customized for your application, you can detect and\nfix issues for your running application much more easily.\n\n310\n\nCHAPTER 7 | Implement resilient applications\n\n\fImplement health checks in ASP.NET Core services\n\nWhen developing an ASP.NET Core microservice or web application, you can use the built-in health\nchecks feature that was released in ASP .NET Core 2.2\n(Microsoft.Extensions.Diagnostics.HealthChecks). Like many ASP.NET Core features, health checks\ncome with a set of services and a middleware.\n\nHealth check services and middleware are easy to use and provide capabilities that let you validate if\nany external resource needed for your application (like a SQL Server database or a remote API) is\nworking properly. When you use this feature, you can also decide what it means that the resource is\nhealthy, as we explain later.\n\nTo use this feature effectively, you need to first configure services in your microservices. Second, you\nneed a front-end application that queries for the health reports. That front-end application could be a\ncustom reporting application, or it could be an orchestrator itself that can react accordingly to the\nhealth states.\n\nUse the HealthChecks feature in your back-end ASP.NET microservices\n\nIn this section, you\u2019ll learn how to implement the HealthChecks feature in a sample ASP.NET Core 7.0\nWeb API application when using the Microsoft.Extensions.Diagnostics.HealthChecks package. The\nImplementation of this feature in a large-scale microservices like the eShopOnContainers is explained\nin the next section.\n\nTo begin, you need to define what constitutes a healthy status for each microservice. In the sample\napplication, we define the microservice is healthy if its API is accessible via HTTP and its related SQL\nServer database is also available.\n\nIn .NET 7, with the built-in APIs, you can configure the services, add a Health Check for the\nmicroservice and its dependent SQL Server database in this way:\n\n// Program.cs from .NET 7 Web API sample\n\n//...\n// Registers required services for health checks\nbuilder.Services.AddHealthChecks()\n    // Add a health check for a SQL Server database\n    .AddCheck(\n        \"OrderingDB-check\",\n        new SqlConnectionHealthCheck(builder.Configuration[\"ConnectionString\"]),\n        HealthStatus.Unhealthy,\n        new string[] { \"orderingdb\" });\n\nIn the previous code, the services.AddHealthChecks() method configures a basic HTTP check that\nreturns a status code 200 with \u201cHealthy\u201d. Further, the AddCheck() extension method configures a\ncustom SqlConnectionHealthCheck that checks the related SQL Database\u2019s health.\n\nThe AddCheck() method adds a new health check with a specified name and the implementation of\ntype IHealthCheck. You can add multiple Health Checks using AddCheck method, so a microservice\nwon\u2019t provide a \u201chealthy\u201d status until all its checks are healthy.\n\n311\n\nCHAPTER 7 | Implement resilient applications\n\n\fSqlConnectionHealthCheck is a custom class that implements IHealthCheck, which takes a connection\nstring as a constructor parameter and executes a simple query to check if the connection to the SQL\ndatabase is successful. It returns HealthCheckResult.Healthy() if the query was executed successfully\nand a FailureStatus with the actual exception when it fails.\n\n// Sample SQL Connection Health Check\npublic class SqlConnectionHealthCheck : IHealthCheck\n{\n    private const string DefaultTestQuery = \"Select 1\";\n\n    public string ConnectionString { get; }\n\n    public string TestQuery { get; }\n\n    public SqlConnectionHealthCheck(string connectionString)\n        : this(connectionString, testQuery: DefaultTestQuery)\n    {\n    }\n\n    public SqlConnectionHealthCheck(string connectionString, string testQuery)\n    {\n        ConnectionString = connectionString ?? throw new\nArgumentNullException(nameof(connectionString));\n        TestQuery = testQuery;\n    }\n\n    public async Task<HealthCheckResult> CheckHealthAsync(HealthCheckContext context,\nCancellationToken cancellationToken = default(CancellationToken))\n    {\n        using (var connection = new SqlConnection(ConnectionString))\n        {\n            try\n            {\n                await connection.OpenAsync(cancellationToken);\n\n                if (TestQuery != null)\n                {\n                    var command = connection.CreateCommand();\n                    command.CommandText = TestQuery;\n\n                    await command.ExecuteNonQueryAsync(cancellationToken);\n                }\n            }\n            catch (DbException ex)\n            {\n                return new HealthCheckResult(status: context.Registration.FailureStatus,\nexception: ex);\n            }\n        }\n\n        return HealthCheckResult.Healthy();\n    }\n}\n\nNote that in the previous code, Select 1 is the query used to check the Health of the database. To\nmonitor the availability of your microservices, orchestrators like Kubernetes periodically perform\nhealth checks by sending requests to test the microservices. It\u2019s important to keep your database\nqueries efficient so that these operations are quick and don\u2019t result in a higher utilization of resources.\n\n312\n\nCHAPTER 7 | Implement resilient applications\n\n\fFinally, add a middleware that responds to the url path /hc:\n\n// Program.cs from .NET 7 Web Api sample\n\napp.MapHealthChecks(\"/hc\");\n\nWhen the endpoint <yourmicroservice>/hc is invoked, it runs all the health checks that are configured\nin the AddHealthChecks() method in the Startup class and shows the result.\n\nHealthChecks implementation in eShopOnContainers\n\nMicroservices in eShopOnContainers rely on multiple services to perform its task. For example, the\nCatalog.API microservice from eShopOnContainers depends on many services, such as Azure Blob\nStorage, SQL Server, and RabbitMQ. Therefore, it has several health checks added using the\nAddCheck() method. For every dependent service, a custom IHealthCheck implementation that\ndefines its respective health status would need to be added.\n\nThe open-source project AspNetCore.Diagnostics.HealthChecks solves this problem by providing\ncustom health check implementations for each of these enterprise services, that are built on top of\n.NET 7. Each health check is available as an individual NuGet package that can be easily added to the\nproject. eShopOnContainers uses them extensively in all its microservices.\n\nFor instance, in the Catalog.API microservice, the following NuGet packages were added:\n\nFigure 8-7. Custom Health Checks implemented in Catalog.API using AspNetCore.Diagnostics.HealthChecks\n\nIn the following code, the health check implementations are added for each dependent service and\nthen the middleware is configured:\n\n// Extension method from Catalog.api microservice\n//\npublic static IServiceCollection AddCustomHealthCheck(this IServiceCollection services,\nIConfiguration configuration)\n{\n    var accountName = configuration.GetValue<string>(\"AzureStorageAccountName\");\n    var accountKey = configuration.GetValue<string>(\"AzureStorageAccountKey\");\n\n    var hcBuilder = services.AddHealthChecks();\n\n313\n\nCHAPTER 7 | Implement resilient applications\n\n\f    hcBuilder\n        .AddSqlServer(\n            configuration[\"ConnectionString\"],\n            name: \"CatalogDB-check\",\n            tags: new string[] { \"catalogdb\" });\n\n    if (!string.IsNullOrEmpty(accountName) && !string.IsNullOrEmpty(accountKey))\n    {\n        hcBuilder\n            .AddAzureBlobStorage(\n\n$\"DefaultEndpointsProtocol=https;AccountName={accountName};AccountKey={accountKey};Endpoint\nSuffix=core.windows.net\",\n                name: \"catalog-storage-check\",\n                tags: new string[] { \"catalogstorage\" });\n    }\n    if (configuration.GetValue<bool>(\"AzureServiceBusEnabled\"))\n    {\n        hcBuilder\n            .AddAzureServiceBusTopic(\n                configuration[\"EventBusConnection\"],\n                topicName: \"eshop_event_bus\",\n                name: \"catalog-servicebus-check\",\n                tags: new string[] { \"servicebus\" });\n    }\n    else\n    {\n        hcBuilder\n            .AddRabbitMQ(\n                $\"amqp://{configuration[\"EventBusConnection\"]}\",\n                name: \"catalog-rabbitmqbus-check\",\n                tags: new string[] { \"rabbitmqbus\" });\n    }\n\n    return services;\n}\n\nFinally, add the HealthCheck middleware to listen to \u201c/hc\u201d endpoint:\n\n// HealthCheck middleware\napp.UseHealthChecks(\"/hc\", new HealthCheckOptions()\n{\n    Predicate = _ => true,\n    ResponseWriter = UIResponseWriter.WriteHealthCheckUIResponse\n});\n\nQuery your microservices to report about their health status\n\nWhen you\u2019ve configured health checks as described in this article and you have the microservice\nrunning in Docker, you can directly check from a browser if it\u2019s healthy. You have to publish the\ncontainer port in the Docker host, so you can access the container through the external Docker host IP\nor through host.docker.internal, as shown in figure 8-8.\n\n314\n\nCHAPTER 7 | Implement resilient applications\n\n\fFigure 8-8. Checking health status of a single service from a browser\n\nIn that test, you can see that the Catalog.API microservice (running on port 5101) is healthy, returning\nHTTP status 200 and status information in JSON. The service also checked the health of its SQL Server\ndatabase dependency and RabbitMQ, so the health check reported itself as healthy.\n\nUse watchdogs\n\nA watchdog is a separate service that can watch health and load across services, and report health\nabout the microservices by querying with the HealthChecks library introduced earlier. This can help\nprevent errors that would not be detected based on the view of a single service. Watchdogs also are a\ngood place to host code that can perform remediation actions for known conditions without user\ninteraction.\n\nThe eShopOnContainers sample contains a web page that displays sample health check reports, as\nshown in Figure 8-9. This is the simplest watchdog you could have since it only shows the state of the\nmicroservices and web applications in eShopOnContainers. Usually a watchdog also takes actions\nwhen it detects unhealthy states.\n\nFortunately, AspNetCore.Diagnostics.HealthChecks also provides AspNetCore.HealthChecks.UI NuGet\npackage that can be used to display the health check results from the configured URIs.\n\n315\n\nCHAPTER 7 | Implement resilient applications\n\n\fFigure 8-9. Sample health check report in eShopOnContainers\n\nIn summary, this watchdog service queries each microservice\u2019s \u201c/hc\u201d endpoint. This will execute all the\nhealth checks defined within it and return an overall health state depending on all those checks. The\nHealthChecksUI is easy to consume with a few configuration entries and two lines of code that needs\nto be added into the Startup.cs of the watchdog service.\n\nSample configuration file for health check UI:\n\n// Configuration\n{\n  \"HealthChecksUI\": {\n    \"HealthChecks\": [\n      {\n        \"Name\": \"Ordering HTTP Check\",\n        \"Uri\": \"http://host.docker.internal:5102/hc\"\n      },\n      {\n        \"Name\": \"Ordering HTTP Background Check\",\n        \"Uri\": \"http://host.docker.internal:5111/hc\"\n      },\n      //...\n    ]}\n}\n\nProgram.cs file that adds HealthChecksUI:\n\n316\n\nCHAPTER 7 | Implement resilient applications\n\n\f// Program.cs from WebStatus(Watch Dog) service\n//\n// Registers required services for health checks\nbuilder.Services.AddHealthChecksUI();\n// build the app, register other middleware\napp.UseHealthChecksUI(config => config.UIPath = \"/hc-ui\");\n\nHealth checks when using orchestrators\n\nTo monitor the availability of your microservices, orchestrators like Kubernetes and Service Fabric\nperiodically perform health checks by sending requests to test the microservices. When an\norchestrator determines that a service/container is unhealthy, it stops routing requests to that\ninstance. It also usually creates a new instance of that container.\n\nFor instance, most orchestrators can use health checks to manage zero-downtime deployments. Only\nwhen the status of a service/container changes to healthy will the orchestrator start routing traffic to\nservice/container instances.\n\nHealth monitoring is especially important when an orchestrator performs an application upgrade.\nSome orchestrators (like Azure Service Fabric) update services in phases\u2014for example, they might\nupdate one-fifth of the cluster surface for each application upgrade. The set of nodes that\u2019s upgraded\nat the same time is referred to as an upgrade domain. After each upgrade domain has been upgraded\nand is available to users, that upgrade domain must pass health checks before the deployment moves\nto the next upgrade domain.\n\nAnother aspect of service health is reporting metrics from the service. This is an advanced capability of\nthe health model of some orchestrators, like Service Fabric. Metrics are important when using an\norchestrator because they are used to balance resource usage. Metrics also can be an indicator of\nsystem health. For example, you might have an application that has many microservices, and each\ninstance reports a requests-per-second (RPS) metric. If one service is using more resources (memory,\nprocessor, etc.) than another service, the orchestrator could move service instances around in the\ncluster to try to maintain even resource utilization.\n\nNote that Azure Service Fabric provides its own Health Monitoring model, which is more advanced\nthan simple health checks.\n\nAdvanced monitoring: visualization, analysis, and alerts\n\nThe final part of monitoring is visualizing the event stream, reporting on service performance, and\nalerting when an issue is detected. You can use different solutions for this aspect of monitoring.\n\nYou can use simple custom applications showing the state of your services, like the custom page\nshown when explaining the AspNetCore.Diagnostics.HealthChecks. Or you could use more advanced\ntools like Azure Monitor to raise alerts based on the stream of events.\n\nFinally, if you\u2019re storing all the event streams, you can use Microsoft Power BI or other solutions like\nKibana or Splunk to visualize the data.\n\n317\n\nCHAPTER 7 | Implement resilient applications\n\n\fAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\nHealthChecks and HealthChecks UI for ASP.NET Core\nhttps://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks\n\nIntroduction to Service Fabric health monitoring\nhttps://learn.microsoft.com/azure/service-fabric/service-fabric-health-introduction\n\nAzure Monitor\nhttps://azure.microsoft.com/services/monitor/\n\n318\n\nCHAPTER 7 | Implement resilient applications\n\n\fCHAPTER  8\n\nMake secure .NET\nMicroservices and Web\nApplications\n\nThere are so many aspects about security in microservices and web applications that the topic could\neasily take several books like this one. So, in this section, we\u2019ll focus on authentication, authorization,\nand application secrets.\n\nImplement authentication in .NET microservices and\nweb applications\n\nIt\u2019s often necessary for resources and APIs published by a service to be limited to certain trusted users\nor clients. The first step to making these sorts of API-level trust decisions is authentication.\nAuthentication is the process of reliably verifying a user\u2019s identity.\n\nIn microservice scenarios, authentication is typically handled centrally. If you\u2019re using an API Gateway,\nthe gateway is a good place to authenticate, as shown in Figure 9-1. If you use this approach, make\nsure that the individual microservices cannot be reached directly (without the API Gateway) unless\nadditional security is in place to authenticate messages whether they come from the gateway or not.\n\nFigure 9-1. Centralized authentication with an API Gateway\n\nWhen the API Gateway centralizes authentication, it adds user information when forwarding requests\nto the microservices. If services can be accessed directly, an authentication service like Azure Active\n\n319\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fDirectory or a dedicated authentication microservice acting as a security token service (STS) can be\nused to authenticate users. Trust decisions are shared between services with security tokens or\ncookies. (These tokens can be shared between ASP.NET Core applications, if needed, by implementing\ncookie sharing.) This pattern is illustrated in Figure 9-2.\n\nFigure 9-2. Authentication by identity microservice; trust is shared using an authorization token\n\nWhen microservices are accessed directly, trust, that includes authentication and authorization, is\nhandled by a security token issued by a dedicated microservice, shared between microservices.\n\nAuthenticate with ASP.NET Core Identity\n\nThe primary mechanism in ASP.NET Core for identifying an application\u2019s users is the ASP.NET Core\nIdentity membership system. ASP.NET Core Identity stores user information (including sign-in\ninformation, roles, and claims) in a data store configured by the developer. Typically, the ASP.NET\nCore Identity data store is an Entity Framework store provided in the\nMicrosoft.AspNetCore.Identity.EntityFrameworkCore package. However, custom stores or other third-\nparty packages can be used to store identity information in Azure Table Storage, CosmosDB, or other\nlocations.\n\nTip\n\nASP.NET Core 2.1 and later provides ASP.NET Core Identity as a Razor Class Library, so you won\u2019t see\nmuch of the necessary code in your project, as was the case for previous versions. For details on how\nto customize the Identity code to suit your needs, see Scaffold Identity in ASP.NET Core projects.\n\nThe following code is taken from the ASP.NET Core Web Application MVC 3.1 project template with\nindividual user account authentication selected. It shows how to configure ASP.NET Core Identity\nusing Entity Framework Core in the Program.cs file.\n\n//...\nbuilder.Services.AddDbContext<ApplicationDbContext>(options =>\n    options.UseSqlServer(\n        builder.Configuration.GetConnectionString(\"DefaultConnection\")));\n\n320\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fbuilder.Services.AddDefaultIdentity<IdentityUser>(options =>\n    options.SignIn.RequireConfirmedAccount = true)\n        .AddEntityFrameworkStores<ApplicationDbContext>();\n\nbuilder.Services.AddRazorPages();\n//...\nOnce ASP.NET Core Identity is configured, you enable it by adding the\napp.UseAuthentication() and endpoints.MapRazorPages() as shown in the following code in the\nservice\u2019s Program.cs file:\n//...\napp.UseRouting();\n\napp.UseAuthentication();\napp.UseAuthorization();\n\napp.UseEndpoints(endpoints =>\n{\n    endpoints.MapRazorPages();\n});\n//...\n\nImportant\n\nThe lines in the preceding code MUST BE IN THE ORDER SHOWN for Identity to work correctly.\n\nUsing ASP.NET Core Identity enables several scenarios:\n\n\u2022\n\n\u2022\n\n\u2022\n\nCreate new user information using the UserManager type (userManager.CreateAsync).\n\nAuthenticate users using the SignInManager type. You can use signInManager.SignInAsync to\nsign in directly, or signInManager.PasswordSignInAsync to confirm the user\u2019s password is\ncorrect and then sign them in.\n\nIdentify a user based on information stored in a cookie (which is read by ASP.NET Core\nIdentity middleware) so that subsequent requests from a browser will include a signed-in\nuser\u2019s identity and claims.\n\nASP.NET Core Identity also supports two-factor authentication.\n\nFor authentication scenarios that make use of a local user data store and that persist identity between\nrequests using cookies (as is typical for MVC web applications), ASP.NET Core Identity is a\nrecommended solution.\n\nAuthenticate with external providers\n\nASP.NET Core also supports using external authentication providers to let users sign in via OAuth 2.0\nflows. This means that users can sign in using existing authentication processes from providers like\nMicrosoft, Google, Facebook, or Twitter and associate those identities with an ASP.NET Core identity\nin your application.\n\nTo use external authentication, besides including the authentication middleware as mentioned before,\nusing the app.UseAuthentication() method, you also have to register the external provider in\nProgram.cs as shown in the following example:\n\n321\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\f//...\nservices.AddDefaultIdentity<IdentityUser>(options => options.SignIn.RequireConfirmedAccount\n= true)\n    .AddEntityFrameworkStores<ApplicationDbContext>();\n\nservices.AddAuthentication()\n    .AddMicrosoftAccount(microsoftOptions =>\n    {\n        microsoftOptions.ClientId =\nbuilder.Configuration[\"Authentication:Microsoft:ClientId\"];\n        microsoftOptions.ClientSecret =\nbuilder.Configuration[\"Authentication:Microsoft:ClientSecret\"];\n    })\n    .AddGoogle(googleOptions => { ... })\n    .AddTwitter(twitterOptions => { ... })\n    .AddFacebook(facebookOptions => { ... });\n//...\n\nPopular external authentication providers and their associated NuGet packages are shown in the\nfollowing table:\n\nProvider\n\nPackage\n\nMicrosoft\n\nMicrosoft.AspNetCore.Authentication.MicrosoftAccount\n\nGoogle\n\nMicrosoft.AspNetCore.Authentication.Google\n\nFacebook\n\nMicrosoft.AspNetCore.Authentication.Facebook\n\nTwitter\n\nMicrosoft.AspNetCore.Authentication.Twitter\n\nIn all cases, you must complete an application registration procedure that is vendor dependent and\nthat usually involves:\n\n1.\n\n2.\n\n3.\n\nGetting a Client Application ID.\n\nGetting a Client Application Secret.\n\nConfiguring a redirection URL, that\u2019s handled by the authorization middleware and the\nregistered provider\n\n4.  Optionally, configuring a sign-out URL to properly handle sign out in a Single Sign On (SSO)\n\nscenario.\n\nFor details on configuring your app for an external provider, see the External provider authentication\nin the ASP.NET Core documentation).\n\nTip\n\nAll details are handled by the authorization middleware and services previously mentioned. So, you\njust have to choose the Individual User Account authentication option when you create the ASP.NET\nCore web application project in Visual Studio, as shown in Figure 9-3, besides registering the\nauthentication providers previously mentioned.\n\n322\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fFigure 9-3. Selecting the Individual User Accounts option, for using external authentication, when creating a web\napplication project in Visual Studio 2019.\n\nIn addition to the external authentication providers listed previously, third-party packages are\navailable that provide middleware for using many more external authentication providers. For a list,\nsee the AspNet.Security.OAuth.Providers repository on GitHub.\n\nYou can also create your own external authentication middleware to solve some special need.\n\nAuthenticate with bearer tokens\n\nAuthenticating with ASP.NET Core Identity (or Identity plus external authentication providers) works\nwell for many web application scenarios in which storing user information in a cookie is appropriate.\nIn other scenarios, though, cookies are not a natural means of persisting and transmitting data.\n\nFor example, in an ASP.NET Core Web API that exposes RESTful endpoints that might be accessed by\nSingle Page Applications (SPAs), by native clients, or even by other Web APIs, you typically want to\nuse bearer token authentication instead. These types of applications do not work with cookies, but\ncan easily retrieve a bearer token and include it in the authorization header of subsequent requests.\nTo enable token authentication, ASP.NET Core supports several options for using OAuth 2.0 and\nOpenID Connect.\n\n323\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fAuthenticate with an OpenID Connect or OAuth 2.0 Identity provider\n\nIf user information is stored in Azure Active Directory or another identity solution that supports\nOpenID Connect or OAuth 2.0, you can use the\nMicrosoft.AspNetCore.Authentication.OpenIdConnect package to authenticate using the OpenID\nConnect workflow. For example, to authenticate to the Identity.Api microservice in\neShopOnContainers, an ASP.NET Core web application can use middleware from that package as\nshown in the following simplified example in Program.cs:\n\n// Program.cs\n\nvar identityUrl = builder.Configuration.GetValue<string>(\"IdentityUrl\");\nvar callBackUrl = builder.Configuration.GetValue<string>(\"CallBackUrl\");\nvar sessionCookieLifetime = builder.Configuration.GetValue(\"SessionCookieLifetimeMinutes\",\n60);\n\n// Add Authentication services\n\nservices.AddAuthentication(options =>\n{\n    options.DefaultScheme = CookieAuthenticationDefaults.AuthenticationScheme;\n    options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\n})\n.AddCookie(setup => setup.ExpireTimeSpan = TimeSpan.FromMinutes(sessionCookieLifetime))\n.AddOpenIdConnect(options =>\n{\n    options.SignInScheme = CookieAuthenticationDefaults.AuthenticationScheme;\n    options.Authority = identityUrl.ToString();\n    options.SignedOutRedirectUri = callBackUrl.ToString();\n    options.ClientId = useLoadTest ? \"mvctest\" : \"mvc\";\n    options.ClientSecret = \"secret\";\n    options.ResponseType = useLoadTest ? \"code id_token token\" : \"code id_token\";\n    options.SaveTokens = true;\n    options.GetClaimsFromUserInfoEndpoint = true;\n    options.RequireHttpsMetadata = false;\n    options.Scope.Add(\"openid\");\n    options.Scope.Add(\"profile\");\n    options.Scope.Add(\"orders\");\n    options.Scope.Add(\"basket\");\n    options.Scope.Add(\"marketing\");\n    options.Scope.Add(\"locations\");\n    options.Scope.Add(\"webshoppingagg\");\n    options.Scope.Add(\"orders.signalrhub\");\n});\n\n// Build the app\n//\u2026\napp.UseAuthentication();\n//\u2026\napp.UseEndpoints(endpoints =>\n{\n    //...\n});\n\nWhen you use this workflow, the ASP.NET Core Identity middleware is not needed, because all user\ninformation storage and authentication is handled by the Identity service.\n\n324\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fIssue security tokens from an ASP.NET Core service\n\nIf you prefer to issue security tokens for local ASP.NET Core Identity users rather than using an\nexternal identity provider, you can take advantage of some good third-party libraries.\n\nIdentityServer4 and OpenIddict are OpenID Connect providers that integrate easily with ASP.NET Core\nIdentity to let you issue security tokens from an ASP.NET Core service. The IdentityServer4\ndocumentation has in-depth instructions for using the library. However, the basic steps to using\nIdentityServer4 to issue tokens are as follows.\n\n1.\n\n2.\n\nYou configure IdentityServer4 in Program.cs by making a call to\nbuilder.Services.AddIdentityServer.\n\nYou call app.UseIdentityServer in Program.cs to add IdentityServer4 to the application\u2019s HTTP\nrequest processing pipeline. This lets the library serve requests to OpenID Connect and\nOAuth2 endpoints like /connect/token.\n\n3.\n\nYou configure identity server by setting the following data:\n\n\u2013\n\n\u2013\n\n\u2013\n\n\u2013\n\nThe credentials to use for signing.\n\nThe Identity and API resources that users might request access to:\n\n\u2022\n\n\u2022\n\nAPI resources represent protected data or functionality that a user can access\nwith an access token. An example of an API resource would be a web API (or\nset of APIs) that requires authorization.\n\nIdentity resources represent information (claims) that are given to a client to\nidentify a user. The claims might include the user name, email address, and so\non.\n\nThe clients that will be connecting in order to request tokens.\n\nThe storage mechanism for user information, such as ASP.NET Core Identity or an\nalternative.\n\nWhen you specify clients and resources for IdentityServer4 to use, you can pass an IEnumerable\ncollection of the appropriate type to methods that take in-memory client or resource stores. Or for\nmore complex scenarios, you can provide client or resource provider types via Dependency Injection.\n\nA sample configuration for IdentityServer4 to use in-memory resources and clients provided by a\ncustom IClientStore type might look like the following example:\n\n// Program.cs\n\nbuilder.Services.AddSingleton<IClientStore, CustomClientStore>();\nbuilder.Services.AddIdentityServer()\n    .AddSigningCredential(\"CN=sts\")\n    .AddInMemoryApiResources(MyApiResourceProvider.GetAllResources())\n    .AddAspNetIdentity<ApplicationUser>();\n//...\n\n325\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fConsume security tokens\n\nAuthenticating against an OpenID Connect endpoint or issuing your own security tokens covers some\nscenarios. But what about a service that simply needs to limit access to those users who have valid\nsecurity tokens that were provided by a different service?\n\nFor that scenario, authentication middleware that handles JWT tokens is available in the\nMicrosoft.AspNetCore.Authentication.JwtBearer package. JWT stands for \u201cJSON Web Token\u201d and\nis a common security token format (defined by RFC 7519) for communicating security claims. A\nsimplified example of how to use middleware to consume such tokens might look like this code\nfragment, taken from the Ordering.Api microservice of eShopOnContainers.\n\n// Program.cs\n\nvar identityUrl = builder.Configuration.GetValue<string>(\"IdentityUrl\");\n\n// Add Authentication services\n\nbuilder.Services.AddAuthentication(options =>\n{\n    options.DefaultAuthenticateScheme =\nAspNetCore.Authentication.JwtBearer.JwtBearerDefaults.AuthenticationScheme;\n    options.DefaultChallengeScheme =\nAspNetCore.Authentication.JwtBearer.JwtBearerDefaults.AuthenticationScheme;\n\n}).AddJwtBearer(options =>\n{\n    options.Authority = identityUrl;\n    options.RequireHttpsMetadata = false;\n    options.Audience = \"orders\";\n});\n\n// Build the app\n\napp.UseAuthentication();\n//\u2026\napp.UseEndpoints(endpoints =>\n{\n    //...\n});\n\nThe parameters in this usage are:\n\n\u2022\n\n\u2022\n\nAudience represents the receiver of the incoming token or the resource that the token grants\naccess to. If the value specified in this parameter does not match the parameter in the token,\nthe token will be rejected.\n\nAuthority is the address of the token-issuing authentication server. The JWT bearer\nauthentication middleware uses this URI to get the public key that can be used to validate the\ntoken\u2019s signature. The middleware also confirms that the iss parameter in the token matches\nthis URI.\n\nAnother parameter, RequireHttpsMetadata, is useful for testing purposes; you set this parameter to\nfalse so you can test in environments where you don\u2019t have certificates. In real-world deployments,\nJWT bearer tokens should always be passed only over HTTPS.\n\n326\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fWith this middleware in place, JWT tokens are automatically extracted from authorization headers.\nThey are then deserialized, validated (using the values in the Audience and Authority parameters), and\nstored as user information to be referenced later by MVC actions or authorization filters.\n\nThe JWT bearer authentication middleware can also support more advanced scenarios, such as using a\nlocal certificate to validate a token if the authority is not available. For this scenario, you can specify a\nTokenValidationParameters object in the JwtBearerOptions object.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nSharing cookies between applications\nhttps://learn.microsoft.com/aspnet/core/security/cookie-sharing\n\nIntroduction to Identity\nhttps://learn.microsoft.com/aspnet/core/security/authentication/identity\n\nRick Anderson. Two-factor authentication with SMS\nhttps://learn.microsoft.com/aspnet/core/security/authentication/2fa\n\nEnabling authentication using Facebook, Google and other external providers\nhttps://learn.microsoft.com/aspnet/core/security/authentication/social/\n\n\u2022  Michell Anicas. An Introduction to OAuth 2\n\nhttps://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2\n\n\u2022\n\n\u2022\n\nAspNet.Security.OAuth.Providers (GitHub repo for ASP.NET OAuth providers)\nhttps://github.com/aspnet-contrib/AspNet.Security.OAuth.Providers/tree/dev/src\n\nIdentityServer4. Official documentation\nhttps://identityserver4.readthedocs.io/en/latest/\n\nAbout authorization in .NET microservices and web\napplications\n\nAfter authentication, ASP.NET Core Web APIs need to authorize access. This process allows a service\nto make APIs available to some authenticated users, but not to all. Authorization can be done based\non users\u2019 roles or based on custom policy, which might include inspecting claims or other heuristics.\n\nRestricting access to an ASP.NET Core MVC route is as easy as applying an Authorize attribute to the\naction method (or to the controller\u2019s class if all the controller\u2019s actions require authorization), as\nshown in following example:\n\npublic class AccountController : Controller\n{\n    public ActionResult Login()\n    {\n    }\n\n    [Authorize]\n\n327\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\f    public ActionResult Logout()\n    {\n    }\n}\n\nBy default, adding an Authorize attribute without parameters will limit access to authenticated users\nfor that controller or action. To further restrict an API to be available for only specific users, the\nattribute can be expanded to specify required roles or policies that users must satisfy.\n\nImplement role-based authorization\n\nASP.NET Core Identity has a built-in concept of roles. In addition to users, ASP.NET Core Identity\nstores information about different roles used by the application and keeps track of which users are\nassigned to which roles. These assignments can be changed programmatically with the RoleManager\ntype that updates roles in persisted storage, and the UserManager type that can grant or revoke roles\nfrom users.\n\nIf you\u2019re authenticating with JWT bearer tokens, the ASP.NET Core JWT bearer authentication\nmiddleware will populate a user\u2019s roles based on role claims found in the token. To limit access to an\nMVC action or controller to users in specific roles, you can include a Roles parameter in the Authorize\nannotation (attribute), as shown in the following code fragment:\n\n[Authorize(Roles = \"Administrator, PowerUser\")]\npublic class ControlPanelController : Controller\n{\n    public ActionResult SetTime()\n    {\n    }\n\n    [Authorize(Roles = \"Administrator\")]\n    public ActionResult ShutDown()\n    {\n    }\n}\n\nIn this example, only users in the Administrator or PowerUser roles can access APIs in the\nControlPanel controller (such as executing the SetTime action). The ShutDown API is further restricted\nto allow access only to users in the Administrator role.\n\nTo require a user be in multiple roles, you use multiple Authorize attributes, as shown in the following\nexample:\n\n[Authorize(Roles = \"Administrator, PowerUser\")]\n[Authorize(Roles = \"RemoteEmployee \")]\n[Authorize(Policy = \"CustomPolicy\")]\npublic ActionResult API1 ()\n{\n}\n\nIn this example, to call API1, a user must:\n\n\u2022\n\n\u2022\n\n328\n\nBe in the Administrator or PowerUser role, and\n\nBe in the RemoteEmployee role, and\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\f\u2022\n\nSatisfy a custom handler for CustomPolicy authorization.\n\nImplement policy-based authorization\n\nCustom authorization rules can also be written using authorization policies. This section provides an\noverview. For more information, see the ASP.NET Authorization Workshop.\n\nCustom authorization policies are registered in the Startup.ConfigureServices method using the\nservice.AddAuthorization method. This method takes a delegate that configures an\nAuthorizationOptions argument.\n\nservices.AddAuthorization(options =>\n{\n    options.AddPolicy(\"AdministratorsOnly\", policy =>\n        policy.RequireRole(\"Administrator\"));\n\n    options.AddPolicy(\"EmployeesOnly\", policy =>\n        policy.RequireClaim(\"EmployeeNumber\"));\n\n    options.AddPolicy(\"Over21\", policy =>\n        policy.Requirements.Add(new MinimumAgeRequirement(21)));\n});\n\nAs shown in the example, policies can be associated with different types of requirements. After the\npolicies are registered, they can be applied to an action or controller by passing the policy\u2019s name as\nthe Policy argument of the Authorize attribute (for example, [Authorize(Policy=\"EmployeesOnly\")])\nPolicies can have multiple requirements, not just one (as shown in these examples).\n\nIn the previous example, the first AddPolicy call is just an alternative way of authorizing by role. If\n[Authorize(Policy=\"AdministratorsOnly\")] is applied to an API, only users in the Administrator role will\nbe able to access it.\n\nThe second AddPolicy call demonstrates an easy way to require that a particular claim should be\npresent for the user. The RequireClaim method also optionally takes expected values for the claim. If\nvalues are specified, the requirement is met only if the user has both a claim of the correct type and\none of the specified values. If you\u2019re using the JWT bearer authentication middleware, all JWT\nproperties will be available as user claims.\n\nThe most interesting policy shown here is in the third AddPolicy method, because it uses a custom\nauthorization requirement. By using custom authorization requirements, you can have a great deal of\ncontrol over how authorization is performed. For this to work, you must implement these types:\n\n\u2022\n\n\u2022\n\nA Requirements type that derives from IAuthorizationRequirement and that contains fields\nspecifying the details of the requirement. In the example, this is an age field for the sample\nMinimumAgeRequirement type.\n\nA handler that implements AuthorizationHandler, where T is the type of\nIAuthorizationRequirement that the handler can satisfy. The handler must implement the\nHandleRequirementAsync method, which checks whether a specified context that contains\ninformation about the user satisfies the requirement.\n\n329\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fIf the user meets the requirement, a call to context.Succeed will indicate that the user is authorized. If\nthere are multiple ways that a user might satisfy an authorization requirement, multiple handlers can\nbe created.\n\nIn addition to registering custom policy requirements with AddPolicy calls, you also need to register\ncustom requirement handlers via Dependency Injection (services.AddTransient<IAuthorizationHandler,\nMinimumAgeHandler>()).\n\nAn example of a custom authorization requirement and handler for checking a user\u2019s age (based on a\nDateOfBirth claim) is available in the ASP.NET Core authorization documentation.\n\nAuthorization and minimal apis\n\nASP.NET supports minimal APIs as an alternative to controller-based APIs. Authorization policies are\nthe recommended way to configure authorization for minimal APIs, as this example demonstrates:\n\n// Program.cs\nbuilder.Services.AddAuthorizationBuilder()\n  .AddPolicy(\"admin_greetings\", policy =>\n        policy\n            .RequireRole(\"admin\")\n            .RequireScope(\"greetings_api\"));\n\n// build the app\n\napp.MapGet(\"/hello\", () => \"Hello world!\")\n  .RequireAuthorization(\"admin_greetings\");\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nASP.NET Core Authentication\nhttps://learn.microsoft.com/aspnet/core/security/authentication/identity\n\nASP.NET Core Authorization\nhttps://learn.microsoft.com/aspnet/core/security/authorization/introduction\n\nRole-based Authorization\nhttps://learn.microsoft.com/aspnet/core/security/authorization/roles\n\nCustom Policy-Based Authorization\nhttps://learn.microsoft.com/aspnet/core/security/authorization/policies\n\nAuthentication and authorization in minimal\nAPIs  https://learn.microsoft.com/aspnet/core/fundamentals/minimal-apis/security\n\nStore application secrets safely during development\n\nTo connect with protected resources and other services, ASP.NET Core applications typically need to\nuse connection strings, passwords, or other credentials that contain sensitive information. These\nsensitive pieces of information are called secrets. It\u2019s a best practice to not include secrets in source\n\n330\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fcode and making sure not to store secrets in source control. Instead, you should use the ASP.NET\nCore configuration model to read the secrets from more secure locations.\n\nYou must separate the secrets for accessing development and staging resources from the ones used\nfor accessing production resources, because different individuals will need access to those different\nsets of secrets. To store secrets used during development, common approaches are to either store\nsecrets in environment variables or by using the ASP.NET Core Secret Manager tool. For more secure\nstorage in production environments, microservices can store secrets in an Azure Key Vault.\n\nStore secrets in environment variables\n\nOne way to keep secrets out of source code is for developers to set string-based secrets as\nenvironment variables on their development machines. When you use environment variables to store\nsecrets with hierarchical names, such as the ones nested in configuration sections, you must name the\nvariables to include the complete hierarchy of its sections, delimited with colons (:).\n\nFor example, setting an environment variable Logging:LogLevel:Default to Debug value would be\nequivalent to a configuration value from the following JSON file:\n\n{\n    \"Logging\": {\n        \"LogLevel\": {\n            \"Default\": \"Debug\"\n        }\n    }\n}\n\nTo access these values from environment variables, the application just needs to call\nAddEnvironmentVariables on its ConfigurationBuilder when constructing an IConfigurationRoot\nobject.\n\nNote\n\nEnvironment variables are commonly stored as plain text, so if the machine or process with the\nenvironment variables is compromised, the environment variable values will be visible.\n\nStore secrets with the ASP.NET Core Secret Manager\n\nThe ASP.NET Core Secret Manager tool provides another method of keeping secrets out of source\ncode during development. To use the Secret Manager tool, install the package\nMicrosoft.Extensions.Configuration.SecretManager in your project file. Once that dependency is\npresent and has been restored, the dotnet user-secrets command can be used to set the value of\nsecrets from the command line. These secrets will be stored in a JSON file in the user\u2019s profile\ndirectory (details vary by OS), away from source code.\n\nSecrets set by the Secret Manager tool are organized by the UserSecretsId property of the project\nthat\u2019s using the secrets. Therefore, you must be sure to set the UserSecretsId property in your project\nfile, as shown in the snippet below. The default value is a GUID assigned by Visual Studio, but the\nactual string is not important as long as it\u2019s unique in your computer.\n\n331\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\f<PropertyGroup>\n    <UserSecretsId>UniqueIdentifyingString</UserSecretsId>\n</PropertyGroup>\n\nUsing secrets stored with Secret Manager in an application is accomplished by calling\nAddUserSecrets<T> on the ConfigurationBuilder instance to include secrets for the application in its\nconfiguration. The generic parameter T should be a type from the assembly that the UserSecretId was\napplied to. Usually, using AddUserSecrets<Startup> is fine.\n\nThe AddUserSecrets<Startup>() is included in the default options for the Development environment\nwhen using the CreateDefaultBuilder method in Program.cs.\n\nUse Azure Key Vault to protect secrets at production\ntime\n\nSecrets stored as environment variables or stored by the Secret Manager tool are still stored locally\nand unencrypted on the machine. A more secure option for storing secrets is Azure Key Vault, which\nprovides a secure, central location for storing keys and secrets.\n\nThe Azure.Extensions.AspNetCore.Configuration.Secrets package allows an ASP.NET Core\napplication to read configuration information from Azure Key Vault. To start using secrets from an\nAzure Key Vault, you follow these steps:\n\n1.\n\nRegister your application as an Azure AD application. (Access to key vaults is managed by\nAzure AD.) This can be done through the Azure management portal.\n\nAlternatively, if you want your application to authenticate using a certificate instead of a\npassword or client secret, you can use the New-AzADApplication PowerShell cmdlet. The\ncertificate that you register with Azure Key Vault needs only your public key. Your application\nwill use the private key.\n\n2.\n\nGive the registered application access to the key vault by creating a new service principal. You\ncan do this using the following PowerShell commands:\n\n$sp = New-AzADServicePrincipal -ApplicationId \"<Application ID guid>\"\nSet-AzKeyVaultAccessPolicy -VaultName \"<VaultName>\" -ServicePrincipalName\n$sp.ServicePrincipalNames[0] -PermissionsToSecrets all -ResourceGroupName \"<KeyVault\nResource Group>\"\n\n3.\n\nInclude the key vault as a configuration source in your application by calling the\nAzureKeyVaultConfigurationExtensions.AddAzureKeyVault extension method when you create\nan IConfigurationRoot instance.\n\nNote that calling AddAzureKeyVault requires the application ID that was registered and given access\nto the key vault in the previous steps. Or you can firstly running the Azure CLI command: az login,\nthen using an overload of AddAzureKeyVault that takes a DefaultAzureCredential in place of the\nclient.\n\n332\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fImportant\n\nWe recommend that you register Azure Key Vault as the last configuration provider, so it can override\nconfiguration values from previous providers.\n\nAdditional resources\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nUsing Azure Key Vault to protect application secrets\nhttps://learn.microsoft.com/azure/architecture/multitenant-identity\n\nSafe storage of app secrets during development\nhttps://learn.microsoft.com/aspnet/core/security/app-secrets\n\nConfiguring data protection\nhttps://learn.microsoft.com/aspnet/core/security/data-protection/configuration/overview\n\nData Protection key management and lifetime in ASP.NET Core\nhttps://learn.microsoft.com/aspnet/core/security/data-protection/configuration/default-\nsettings\n\n333\n\nCHAPTER 8 | Make secure .NET Microservices and Web Applications\n\n\fCHAPTER  9\n\n.NET Microservices\nArchitecture key takeaways\n\nAs a summary and key takeaways, the following are the most important conclusions from this guide.\n\nBenefits of using containers. Container-based solutions provide important cost savings because\nthey help reduce deployment problems caused by failing dependencies in production environments.\nContainers significantly improve DevOps and production operations.\n\nContainers will be ubiquitous. Docker-based containers are becoming the de facto standard in the\nindustry, supported by key vendors in the Windows and Linux ecosystems, such as Microsoft, Amazon\nAWS, Google, and IBM. Docker will probably soon be ubiquitous in both the cloud and on-premises\ndatacenters.\n\nContainers as a unit of deployment. A Docker container is becoming the standard unit of\ndeployment for any server-based application or service.\n\nMicroservices. The microservices architecture is becoming the preferred approach for distributed and\nlarge or complex mission-critical applications based on many independent subsystems in the form of\nautonomous services. In a microservice-based architecture, the application is built as a collection of\nservices that are developed, tested, versioned, deployed, and scaled independently. Each service can\ninclude any related autonomous database.\n\nDomain-driven design and SOA. The microservices architecture patterns derive from service-\noriented architecture (SOA) and domain-driven design (DDD). When you design and develop\nmicroservices for environments with evolving business needs and rules, it\u2019s important to consider DDD\napproaches and patterns.\n\nMicroservices challenges. Microservices offer many powerful capabilities, like independent\ndeployment, strong subsystem boundaries, and technology diversity. However, they also raise many\nnew challenges related to distributed application development, such as fragmented and independent\ndata models, resilient communication between microservices, eventual consistency, and operational\ncomplexity that results from aggregating logging and monitoring information from multiple\nmicroservices. These aspects introduce a much higher complexity level than a traditional monolithic\napplication. As a result, only specific scenarios are suitable for microservice-based applications. These\ninclude large and complex applications with multiple evolving subsystems. In these cases, it\u2019s worth\ninvesting in a more complex software architecture, because it will provide better long-term agility and\napplication maintenance.\n\n334\n\nCHAPTER 9 | .NET Microservices Architecture key takeaways\n\n\fContainers for any application. Containers are convenient for microservices, but can also be useful\nfor monolithic applications based on the traditional .NET Framework, when using Windows\nContainers. The benefits of using Docker, such as solving many deployment-to-production issues and\nproviding state-of-the-art Dev and Test environments, apply to many different types of applications.\n\nCLI versus IDE. With Microsoft tools, you can develop containerized .NET applications using your\npreferred approach. You can develop with a CLI and an editor-based environment by using the Docker\nCLI and Visual Studio Code. Or you can use an IDE-focused approach with Visual Studio and its unique\nfeatures for Docker, such as multi-container debugging.\n\nResilient cloud applications. In cloud-based systems and distributed systems in general, there is\nalways the risk of partial failure. Since clients and services are separate processes (containers), a\nservice might not be able to respond in a timely way to a client\u2019s request. For example, a service might\nbe down because of a partial failure or for maintenance; the service might be overloaded and\nresponding slowly to requests; or it might not be accessible for a short time because of network\nissues. Therefore, a cloud-based application must embrace those failures and have a strategy in place\nto respond to those failures. These strategies can include retry policies (resending messages or\nretrying requests) and implementing circuit-breaker patterns to avoid exponential load of repeated\nrequests. Basically, cloud-based applications must have resilient mechanisms\u2014either based on cloud\ninfrastructure or custom, as the high-level ones provided by orchestrators or service buses.\n\nSecurity. Our modern world of containers and microservices can expose new vulnerabilities. There are\nseveral ways to implement basic application security, based on authentication and authorization.\nHowever, container security must consider additional key components that result in inherently safer\napplications. A critical element of building safer apps is having a secure way of communicating with\nother apps and systems, something that often requires credentials, tokens, passwords, and the like,\ncommonly referred to as application secrets. Any secure solution must follow security best practices,\nsuch as encrypting secrets while in transit and at rest, and preventing secrets from leaking when\nconsumed by the final application. Those secrets need to be stored and kept safely, as when using\nAzure Key Vault.\n\nOrchestrators. Container-based orchestrators, such as Azure Kubernetes Service and Azure Service\nFabric are key part of any significant microservice and container-based application. These applications\ncarry with them high complexity, scalability needs, and go through constant evolution. This guide has\nintroduced orchestrators and their role in microservice-based and container-based solutions. If your\napplication needs are moving you toward complex containerized apps, you will find it useful to seek\nout additional resources for learning more about orchestrators.\n\n335\n\nCHAPTER 9 | .NET Microservices Architecture key takeaways\n```"}