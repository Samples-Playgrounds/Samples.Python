"[\"<!-- image -->\\n\\n## .NET Microservices: Architecture for Containerized NET Applications\\n\\n<!-- image -\", \"->\\n\\nCesar de la Torre Bill Wagner Mike Rousos\\n\\nMicrosoft Corporation\\n\\nEDITION v7.0 -Updated to ASP.N\", \"ET Core 7.0\\n\\nRefer changelog for the book updates and community contributions.\\n\\nThis guide is an int\", \"roduction to developing microservices-based applications and managing them using containers. It disc\", \"usses architectural design and implementation approaches using .NET and Docker containers.\\n\\nTo make \", \"it easier to get started, the guide focuses on a reference containerized and microservicebased appli\", \"cation that you can explore. The reference application is available at the eShopOnContainers GitHub \", \"repo.\\n\\n## Action links\\n\\n- This e -book is also available in a PDF format (English version only) Down\", \"load\\n- Clone/Fork the reference application eShopOnContainers on GitHub\\n- Watch the introductory vid\", \"eo\\n- Get to know the Microservices Architecture right away\\n\\n## Introduction\\n\\nEnterprises are increas\", \"ingly realizing cost savings, solving deployment problems, and improving DevOps and production opera\", \"tions by using containers. Microsoft has been releasing container innovations for Windows and Linux \", \"by creating products like Azure Kubernetes Service and Azure Service Fabric, and by partnering with \", \"industry leaders like Docker, Mesosphere, and Kubernetes. These products deliver container solutions\", \" that help companies build and deploy applications at cloud speed and scale, whatever their choice o\", \"f platform or tools.\\n\\nDocker is becoming the de facto standard in the container industry, supported \", \"by the most significant vendors in the Windows and Linux ecosystems. (Microsoft is one of the main c\", \"loud vendors supporting Docker). In the future, Docker will probably be ubiquitous in any datacenter\", \" in the cloud or on-premises.\\n\\nIn addition, the microservices architecture is emerging as an importa\", \"nt approach for distributed mission -critical applications. In a microservice-based architecture, th\", \"e application is built on a collection of services that can be developed, tested, deployed, and vers\", \"ioned independently.\\n\\n## About this guide\\n\\nThis guide is an introduction to developing microservices\", \"-based applications and managing them using containers. It discusses architectural design and implem\", \"entation approaches using .NET and Docker containers. To make it easier to get started with containe\", \"rs and microservices, the guide focuses on a reference containerized and microservice -based applica\", \"tion that you can explore. The sample application is available at the eShopOnContainers GitHub repo.\", \"\\n\\n<!-- image -->\\n\\n<!-- image -->\\n\\nThis guide provides foundational development and architectural gui\", \"dance primarily at a development environment level with a focus on two technologies: Docker and .NET\", \". Our intention is that you read this guide when thinking about your application design without focu\", \"sing on the infrastructure (cloud or on-premises) of your production environment. You will make deci\", \"sions about your infrastructure later, when you create your production-ready applications. Therefore\", \", this guide is intended to be infrastructure agnostic and more development-environment-centric.\\n\\nAf\", \"ter you have studied this guide, your next step would be to learn about production-ready microservic\", \"es on Microsoft Azure.\\n\\n## Version\\n\\nThis guide has been revised to cover .NET 7 version along with m\", \"any additional updates related to the same \\\"wave\\\" of technologies (that is, Azure and additional thi\", \"rd-party technologies) coinciding in time with the .NET 7 release. That's why the book version has a\", \"lso been updated to version 7.0 .\\n\\n## What this guide does not cover\\n\\nThis guide does not focus on t\", \"he application lifecycle, DevOps, CI/CD pipelines, or team work. The complementary guide Containeriz\", \"ed Docker Application Lifecycle with Microsoft Platform and Tools focuses on that subject. The curre\", \"nt guide also does not provide implementation details on Azure infrastructure, such as information o\", \"n specific orchestrators.\\n\\n## Additional resources\\n\\n- Containerized Docker Application Lifecycle wit\", \"h Microsoft Platform and Tools (downloadable e-book) https://aka.ms/dockerlifecycleebook\\n\\n## Who sho\", \"uld use this guide\\n\\nWe wrote this guide for developers and solution architects who are new to Docker\", \"-based application development and to microservices-based architecture. This guide is for you if you\", \" want to learn how to architect, design, and implement proof-of-concept applications with Microsoft \", \"development technologies (with special focus on .NET) and with Docker containers.\\n\\nYou will also fin\", \"d this guide useful if you are a technical decision maker, such as an enterprise architect, who want\", \"s an architecture and technology overview before you decide on what approach to select for new and m\", \"odern distributed applications.\\n\\n## How to use this guide\\n\\nThe first part of this guide introduces D\", \"ocker containers, discusses how to choose between .NET 7 and the .NET Framework as a development fra\", \"mework, and provides an overview of microservices. This content is for architects and technical deci\", \"sion makers who want an overview but don't need to focus on code implementation details.\\n\\n<!-- image\", \" -->\\n\\nThe second part of the guide starts with the Development process for Docker based applications\", \" section. It focuses on the development and microservice patterns for implementing applications usin\", \"g .NET and Docker. This section will be of most interest to developers and architects who want to fo\", \"cus on code and on patterns and implementation details.\\n\\n## Related microservice and container -base\", \"d reference application: eShopOnContainers\\n\\nThe eShopOnContainers application is an open-source refe\", \"rence app for .NET and microservices that is designed to be deployed using Docker containers. The ap\", \"plication consists of multiple subsystems, including several e-store UI front-ends (a Web MVC app, a\", \" Web SPA, and a native mobile app). It also includes the back -end microservices and containers for \", \"all required server-side operations.\\n\\nThe purpose of the application is to showcase architectural pa\", \"tterns. IT IS NOT A PRODUCTIONREADY TEMPLATE to start real -world applications. In fact, the applica\", \"tion is in a permanent beta state, as it's also used to test new potentially interesting technologie\", \"s as they show up.\\n\\n## Credits\\n\\nCo -Authors:\\n\\nCesar de la Torre, Sr. PM, .NET product team, Microsof\", \"t Corp.\\n\\nBill Wagner, Sr. Content Developer, C+E, Microsoft Corp.\\n\\nMike Rousos, Principal Software E\", \"ngineer, DevDiv CAT team, Microsoft\\n\\nEditors:\\n\\nMike Pope\\n\\nSteve Hoag\\n\\nParticipants and reviewers:\\n\\nJ\", \"effrey Richter, Partner Software Eng, Azure team, Microsoft\\n\\nJimmy Bogard, Chief Architect at Headsp\", \"ring\\n\\nUdi Dahan, Founder &amp; CEO, Particular Software\\n\\nJimmy Nilsson, Co-founder and CEO of Factor\", \"10\\n\\nGlenn Condron, Sr. Program Manager, ASP.NET team\\n\\nMark Fussell, Principal PM Lead, Azure Service\", \" Fabric team, Microsoft\\n\\nDiego Vega, PM Lead, Entity Framework team, Microsoft\\n\\nBarry Dorrans, Sr. S\", \"ecurity Program Manager\\n\\nRowan Miller, Sr. Program Manager, Microsoft\\n\\nAnkit Asthana, Principal PM M\", \"anager, .NET team, Microsoft\\n\\nScott Hunter, Partner Director PM, .NET team, Microsoft\\n\\nNish Anil, Sr\", \". Program Manager, .NET team, Microsoft\\n\\nDylan Reisenberger, Architect and Dev Lead at Polly\\n\\nSteve \", \"\\u201cardalis\\u201d Smith - Software Architect and Trainer - Ardalis.com\\n\\nIan Cooper, Coding Architect at Brig\", \"hter\\n\\nUnai Zorrilla, Architect and Dev Lead at Plain Concepts\\n\\nEduard Tomas, Dev Lead at Plain Conce\", \"pts\\n\\nRamon Tomas, Developer at Plain Concepts\\n\\nDavid Sanz, Developer at Plain Concepts\\n\\nJavier Valer\", \"o, Chief Operating Officer at Grupo Solutio\\n\\nPierre Millet , Sr. Consultant, Microsoft\\n\\nMichael Frii\", \"s, Product Manager, Docker Inc\\n\\nCharles Lowell, Software Engineer, VS CAT team, Microsoft\\n\\nMiguel Ve\", \"loso, Software Development Engineer at Plain Concepts\\n\\nSumit Ghosh, Principal Consultant at Neudesic\", \"\\n\\n## Copyright\\n\\nPUBLISHED BY\\n\\nMicrosoft Developer Division, .NET and Visual Studio product teams\\n\\nA \", \"division of Microsoft Corporation\\n\\nOne Microsoft Way\\n\\nRedmond, Washington 98052-6399\\n\\nCopyright \\u00a9 20\", \"23 by Microsoft Corporation\\n\\nAll rights reserved. No part of the contents of this book may be reprod\", \"uced or transmitted in any form or by any means without the written permission of the publisher.\\n\\nTh\", \"is book is provided \\\"as-is\\\" and expresses the author's views and opinions. The views, opinions and i\", \"nformation expressed in this book, including URL and other Internet website references, may change w\", \"ithout notice.\\n\\nSome examples depicted herein are provided for illustration only and are fictitious.\", \" No real association or connection is intended or should be inferred.\\n\\n<!-- image -->\\n\\n<!-- image --\", \">\\n\\nMicrosoft and the trademarks listed at https://www.microsoft.com on the \\\"Trademarks\\\" webpage are \", \"trademarks of the Microsoft group of companies.\\n\\nMac and macOS are trademarks of Apple Inc.\\n\\nThe Doc\", \"ker whale logo is a registered trademark of Docker, Inc. Used by permission.\\n\\nAll other marks and lo\", \"gos are property of their respective owners.\\n\\n## Contents\\n\\n| Introduction to Containers and Docker .\", \"...............................................................................  1                  \", \"                                                   |\\n|----------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"---------------------------------------------|\\n| What is Docker? ................................ ..\", \".............................................................. .....................................\", \"........................... ........ 2 |\\n| Comparing Docker containers with virtual machines .......\", \"......................................................... ........................... 3             \", \"                                 |\\n| A simple analogy ................................ .............\", \"................................................... ................................ ...............\", \".................. 4       |\\n| Docker terminology  ................................ ................\", \"................................................ ................................ ..................\", \".............. 5     |\\n| Docker containers, images, and registries  ................................\", \" ................................................................ ..................... 7           \", \"               |\\n| Choosing Between .NET and .NET Framework for Docker Containers  .................\", \".............  9                                                                                    \", \"         |\\n| General guidance ................................ .....................................\", \"........................... ................................................................ ..... 9\", \"   |\\n| When to choose .NET for Docker containers ................................ ..................\", \".............................................. .............  10                                 |\\n|\", \" Developing and deploying cross platform  ................................ .........................\", \"....................................... ............  10                                   |\\n| Using\", \" containers for new (\\u201cgreen-field\\u201d) projects ................................ ......................\", \".......... ...............................  11                                       |\\n| Create and \", \"deploy microservices on containers ................................ ................................\", \"................................ ..  11                                        |\\n| Deploying high de\", \"nsity in scalable systems ................................ .........................................\", \"....................... ..........  11                                   |\\n| When to choose .NET Fra\", \"mework for Docker containers ................................................................ ......\", \"...............  12                                                |\\n| Using third-party .NET librar\", \"ies or NuGet packages not available for .NET 7  ................................ .........  12      \", \"                                                             |\\n| Using .NET technologies not availab\", \"le for .NET 7 ................................ ................................ ....................\", \"...........  12                                        |\\n| Using a platform or API that doesn\\u2019t supp\", \"ort .NET 7  ................................................................ .......................\", \".  13                                            |\\n| Porting existing ASP.NET application to .NET 7 \", \"................................ ................................................................ ..\", \".  13                                      |\\n| Decision table: .NET implementations to use for Docke\", \"r ................................................................ .....................  13        \", \"                                     |\\n| What OS to target with .NET containers ....................\", \"............ ................................................................ ......................\", \"  14                           |\\n| Official .NET Docker images  ....................................\", \"............................ ................................................................ ......\", \".......  16              |\\n| Container design principles  ..........................................\", \"...................... ................................................................ ............\", \"..  18             |\\n| Containerizing monolithic applications  ................................ ....\", \"............................................................ .......................  19            \", \"             |\\n| Deploying a monolithic application as a container ................................ \", \"................................ ............................  21                                   \", \"       |\\n| Publishing a single-container-based application to Azure App Service ....................\", \"............ ....................  21                                                               \", \" |\\n\\n| Manage state and data in Docker applications  ................................ ...............\", \"................................................. ........                        | 22              \", \"        |\\n|-----------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------|-----------\", \"--------------|\\n| Service - oriented architecture ..................................................\", \".............. ................................................................ ...........   | 25  \", \"                    |\\n| Microservices architecture .................................................\", \"............... ................................................................ .................  \", \"| 25                      |\\n| Additional resources  ................................................\", \"................ ................................................................ ..................\", \"..... | 27                      |\\n| Data sovereignty per microservice  .............................\", \"................................... ................................ ...............................\", \".           | 27                      |\\n| The relationship between microservices and the Bounded Con\", \"text pattern ................................ ...........                                           \", \"                  | 29                      |\\n| Logical architecture versus physical architecture ..\", \".............................. ................................................................     \", \"                        | .....  30               |\\n| Challenges and solutions for distributed data \", \"management ................................................................ ..............          \", \"                              | 31                      |\\n| Challenge #1: How to define the boundari\", \"es of each microservice ................................ ............................               \", \"                                    | 31                      |\\n| Challenge #2: How to create querie\", \"s that retrieve data from several microservices ............................                        \", \"                                          | 32                      |\\n| Challenge #3: How to achieve\", \" consistency across multiple microservices  ................................ ...............        \", \"                                                | 33                      |\\n| Challenge #4: How to d\", \"esign communication across microservice boundaries  ................................                \", \"                                                      | ....  35                |\\n| Additional resou\", \"rces  ................................................................ .............................\", \"................................... ....................... | 36                      |\\n| Identify d\", \"omain-model boundaries for each microservice .......................................................\", \"......... ..................                                      | 36                      |\\n| The \", \"API gateway pattern versus the Direct client-to-microservice communication .........................\", \".......                                                                 | ..  40                  |\\n\", \"| Direct client - to - microservice communication  ................................ ................\", \"................................................                              | ......  40          \", \"    |\\n| Why consider API Gateways instead of direct client-to-microservice communication  ..........\", \".............                                                                       | 41            \", \"          |\\n| What is the API Gateway pattern? .....................................................\", \"........... ................................ .............................                | 42      \", \"                |\\n| Main features in the API Gateway pattern  ................................ .....\", \"........................................................... .............                       | 44\", \"                      |\\n| Using products with API Gateway features ................................ \", \"................................................................ ............                       \", \"  | 45                      |\\n| Drawbacks of the API Gateway pattern ...............................\", \". ................................................................ ...................              \", \"        | 47                      |\\n| Additional resources  ........................................\", \"........................ ................................................................ ..........\", \"............. | 48                      |\\n| Communication in a microservice architecture  ..........\", \"...................... ................................................................ ........    \", \"                    | 48                      |\\n| Communication types  .............................\", \"................................... ................................................................\", \" ....................     | 49                      |\\n| Asynchronous microservice integration enforc\", \"es microservice\\u2019s autonomy  ................................ ...........                            \", \"                                | 50                      |\\n| Communication styles  ................\", \"................................................ ...................................................\", \"............. ....................    | 52                      |\\n| Asynchronous message-based commu\", \"nication ................................ ..........................................................\", \"...... .......                              | 54                      |\\n| Single-receiver message-ba\", \"sed communication  ................................ ................................ ...............\", \".................                                 | 55                      |\\n| Multiple-receivers m\", \"essage-based communication  ................................................................ .......\", \"...................                                     | 56                      |\\n| Asynchronous e\", \"vent-driven communication ................................ .........................................\", \".......................                                       | ........  56            |\\n| A note a\", \"bout messaging technologies for production systems  ................................................\", \"................                                                    | ...  57                 |\\n| Re\", \"siliently publishing to the event bus  ................................ ............................\", \"....................................                                      | ...................  58 \", \"|\\n\\n| Additional resources  ................................................................ ........\", \"........................................................ .......................                    \", \"       | 58                                                                              |\\n|--------\", \"----------------------------------------------------------------------------------------------------\", \"-----------------------------------------------------------------------------------------------|----\", \"-----------------------------------------------------------------------------|\\n| Creating, evolving,\", \" and versioning microservice APIs and contracts  ................................ ..................\", \".............                                                                      | 59             \", \"                                                                 |\\n| Additional resources  .........\", \"....................................................... ............................................\", \".................... .......................                           | 59                         \", \"                                                     |\\n| Microservices addressability and the servic\", \"e registry  ................................ ................................ ......................\", \"......                                                     | 60                                     \", \"                                         |\\n| Additional resources  .................................\", \"............................... ................................................................ ...\", \"....................                           | 60                                                 \", \"                             |\\n| Creating composite UI based on microservices  .....................\", \"........... ................................................................ .......                \", \"                                   | 60                                                             \", \"                 |\\n| Additional resources  .........................................................\", \"....... ................................................................ .......................    \", \"                       | 62                                                                         \", \"     |\\n| Resiliency and high availability in microservices  ................................ .......\", \"......................................................... ......                                    \", \"           | 63                                                                              |\\n| Hea\", \"lth management and diagnostics in microservices  ...................................................\", \"............. ....................                                                                 |\", \" 63                                                                              |\\n| Additional reso\", \"urces  ................................................................ ............................\", \".................................... .......................                           | 65         \", \"                                                                     |\\n| Orchestrate microservices a\", \"nd multi - container applications for high scalability and availability  .......                    \", \"                                                                           | 66                     \", \"                                                         |\\n| Software platforms for container cluste\", \"ring, orchestration, and scheduling ................................ ...........                    \", \"                                                               | 68                                 \", \"                                             |\\n| Using container-based orchestrators in Microsoft Az\", \"ure ................................................................ ................               \", \"                                                   | 68                                             \", \"                                 |\\n| Using Azure Kubernetes Service  ...............................\", \"................................. ................................ ................................ \", \"                                       | 69                                                         \", \"                     |\\n| Development environment for Kubernetes ................................ ...\", \"............................................................. ...........                           \", \"                           | 70                                                                     \", \"         |\\n| Getting started with Azure Kubernetes Service (AKS)  ..................................\", \".............................. .......................                                              \", \"               | 70                                                                              |\\n|\", \" Deploy with Helm charts into Kubernetes clusters ................................ .................\", \"............... .............................                                                       \", \"   | 71                                                                              |\\n| Additional \", \"resources  ................................................................ ........................\", \"........................................                                                   | .......\", \"................  71                                                     |\\n| Development process for\", \" Docker-based applications .......................................................                  \", \"                                                                               | 72                 \", \"                                                             |\\n| Development environment for Docker \", \"apps  ................................ .............................................................\", \"... .............                                                  | 72                             \", \"                                                 |\\n| Development tool choices: IDE or editor .......\", \"......................... ................................................................ .........\", \".......                                                | 72                                         \", \"                                     |\\n| Additional resources  .....................................\", \"........................... ................................................................ .......\", \"................                           | 73                                                     \", \"                         |\\n| .NET languages and frameworks for Docker containers  ..................\", \".............................................. .......................                              \", \"                               | 73                                                                 \", \"             |\\n| Development workflow for Docker apps ................................ .............\", \"...................................................                                                 \", \"                   | .....................  73                                                      \", \" |\\n| Workflow for developing Docker container-based applications  ..................................\", \"..............................                                                                      \", \"       | ..  73                                                                          |\\n| Step 1.\", \" Start coding and create your initial application or service baseline ..............................\", \".. .............                                                                               | 75 \", \"                                                                             |\\n| Step 2. Create a Do\", \"ckerfile related to an existing .NET base image ................................ ...................\", \".........                                                                          | 76             \", \"                                                                 |\\n| Step 3. Create your custom Dock\", \"er images and embed your application or service in them ..........                                  \", \"                                                                       | 83                         \", \"                                                     |\\n| Step 4. Define your services in docker-comp\", \"ose.yml when building a multi-container Docker  application  ................................ ......\", \".......................................................... | .......................................\", \"......................... ..........  84 |\\n| Step 5. Build and run your Docker application  ........\", \"........................ ................................................................           \", \"                                               | ....  87                                           \", \"                             |\\n| Step 6. Test your Docker application using your local Docker host .\", \"...............................                                                                     \", \"                                   | ............................  89                               \", \"                 |\\n\\n| Simplified workflow when developing containers with Visual Studio  ...........\", \"..................... ........................                                                      \", \"| 90                                                                                   |\\n|----------\", \"----------------------------------------------------------------------------------------------------\", \"---------------------------------------------------------------------|------------------------------\", \"--------------------------------------------------------|\\n| Using PowerShell commands in a Dockerfil\", \"e to set up Windows Containers ................................ .........                           \", \"                                      | 91                                                          \", \"                         |\\n| Designing and Developing Multi-Container and Microservice-Based .NET Ap\", \"plications 93                                                                                       \", \"       | Designing and Developing Multi-Container and Microservice-Based .NET Applications 93 |\\n| De\", \"sign a microservice-oriented application  ................................ .........................\", \"....................................... ..............                      | 93                    \", \"                                                               |\\n| Application specifications  .....\", \"........................................................... ........................................\", \"........................ .............       | 93                                                   \", \"                                |\\n| Development team context  ......................................\", \".......................... ................................................................         \", \"              | .........  94                                                                       \", \" |\\n| Choosing an architecture ................................................................ .....\", \"........................................................... ..............         | 94             \", \"                                                                      |\\n| Benefits of a microservice\", \" - based solution  ................................ ................................................\", \"................ .............                      | 97                                            \", \"                                       |\\n| Downsides of a microservice - based solution  ...........\", \"..................... ................................................................              \", \"                     | .......  98                                                                  \", \"        |\\n| External versus internal architecture and design patterns ..............................\", \".................................. ...............                                        | 99      \", \"                                                                             |\\n| The new world: mult\", \"iple architectural patterns and polyglot microservices ................................ ..........  \", \"                                                           | 100                                    \", \"                                              |\\n| Creating a simple data-driven CRUD microservice  .\", \"............................... ...............................................................     \", \"                            | 102                                                                   \", \"               |\\n| Designing a simple CRUD microservice  ................................ ..........\", \"......................................................                                           | .\", \"............... 102                                                                 |\\n| Implementing\", \" a simple CRUD microservice with ASP.NET Core ......................................................\", \"..........                                                        | . 103                           \", \"                                                     |\\n| The DB connection string and environment va\", \"riables used by Docker containers  .............................                                    \", \"                                   | 109                                                            \", \"                      |\\n| Generating Swagger description metadata from your ASP.NET Core Web API  ..\", \"..............................                                                                      \", \"    | ... 111                                                                              |\\n| Defin\", \"ing your multi-container application with docker-compose.yml  ................................ .....\", \"....................                                                     | 116                      \", \"                                                            |\\n| Use a database server running as a c\", \"ontainer  ................................ .........................................................\", \".......                                   | ........ 127                                            \", \"                             |\\n| SQL Server running as a container with a microservice-related datab\", \"ase ................................ ..............                                                 \", \"           | 128                                                                                  |\\n\", \"| Seeding with test data on Web application startup ................................................\", \"................ .........................                                      | 129               \", \"                                                                   |\\n| EF Core InMemory database ver\", \"sus SQL Server running as a container ................................ .................            \", \"                                                 | 132                                              \", \"                                    |\\n| Using a Redis cache service running in a container .........\", \".......................................................                                             \", \"                  | ......................... 132                                                   \", \"     |\\n| Implementing event-based communication between microservices (integration events)  ........\", \"...........                                                                            | 133        \", \"                                                                          |\\n| Using message brokers \", \"and service buses for production systems  ................................ .........................\", \".                                                       | 134                                       \", \"                                           |\\n| Integration events ................................ .\", \"............................................................... ................................ ...\", \".......................  | 135                                                                      \", \"            |\\n| The event bus  ................................ ....................................\", \"............................ ................................................................ | .. 1\", \"36                                                                               |\\n| Additional reso\", \"urces  ................................................................ ............................\", \"....................................                           | ..................... 138          \", \"                                                  |\\n| Implementing an event bus with RabbitMQ for th\", \"e development or test environment .......................                                           \", \"                                | 138                                                               \", \"                   |\\n| Implementing a simple publish method with RabbitMQ ..........................\", \"...................................... ...............                                              \", \" | 139                                                                                  |\\n| Implemen\", \"ting the subscription code with the RabbitMQ API ...................................................\", \".............                                                         | ..... 140                   \", \"                                                         |\\n| Additional resources  .................\", \"............................................... ....................................................\", \"............                           | ..................... 141                                  \", \"                          |\\n\\n| Subscribing to events ................................ ..............\", \".................. ................................................................ ................\", \"........                                                 | 141                                      \", \"                                  |\\n|---------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------|-----------------------------------\", \"-----------------------------------------|\\n| Publishing events through the event bus ...............\", \"................. ................................................................ .............    \", \"                                                                       | 142                        \", \"                                                |\\n| Idempotency in update message events ...........\", \"..................... ................................................................ .............\", \".                                                                             | 149                 \", \"                                                       |\\n| Deduplicating integration event messages \", \" ................................ ................................................................ .\", \"........                                                                             | 150          \", \"                                                              |\\n| Testing ASP.NET Core services and \", \"web apps  ................................ .........................................................\", \"....... ........                                                                            | 152   \", \"                                                                     |\\n| Testing in eShopOnContainer\", \"s ................................................................ .................................\", \"...............................                                                                    |\", \" . 155                                                                      |\\n| Implement background\", \" tasks in microservices with IHostedService and the BackgroundService class ........................\", \"........................................ ...........................................................\", \"..... | ................................................................ .... 157  |\\n| Registering h\", \"osted services in your WebHost or Host  ............................................................\", \".... ...................                                                                            \", \"             | 159                                                                        |\\n| The IH\", \"ostedService interface ................................................................ ............\", \"....................................................                                                \", \"                    | ..... 159                                                                  |\\n|\", \" Implementing IHostedService with a custom hosted service class deriving from the  BackgroundService\", \" base class ................................................................                        \", \"                           | ................................................................ ... 16\", \"0   |\\n| Additional resources  ................................................................ .....\", \"........................................................... .....................                   \", \"                                  | 163                                                             \", \"           |\\n| Implement API Gateways with Ocelot ................................ .................\", \"............................................... ........................                            \", \"                                         | 163                                                      \", \"                  |\\n| Architect and design your API Gateways ................................ ......\", \".......................................................... ..............                           \", \"                                                | 163                                               \", \"                         |\\n| Implementing your API Gateways with Ocelot  ...........................\", \"..... ................................................................                              \", \"                                                       | .. 168                                     \", \"                                |\\n| Using Kubernetes Ingress plus Ocelot API Gateways  .............\", \"................................................... ......................                          \", \"                                                              | 180                                 \", \"                                       |\\n| Additional cross - cutting features in an Ocelot API Gate\", \"way  ................................................................                               \", \"                                                                     | ....... 181                  \", \"                                              |\\n| Tackle Business Complexity in a Microservice with \", \"DDD and CQRS Patterns  ..............                                                               \", \"                                                                            | 182                   \", \"                                                     |\\n| Apply simplified CQRS and DDD patterns in a\", \" microservice ................................................................ .............        \", \"                                                                                   | 184            \", \"                                                            |\\n| Additional resources  ..............\", \".................................................. .................................................\", \"............... .....................                                                     | 186     \", \"                                                                   |\\n| Apply CQRS and CQS approaches\", \" in a DDD microservice in eShopOnContainers  ................................                       \", \"                                                                                                 | .\", \" 186                                                                      |\\n| CQRS and DDD patterns \", \"are not top-level architectures ................................................................ ...\", \"............                                                                                        \", \"    | 187                                                                        |\\n| Implement reads\", \"/queries in a CQRS microservice  ................................ ..................................\", \"..............................                                                                      \", \"           | 188                                                                        |\\n| Use View\", \"Models specifically made for client apps, independent from domain model constraints ................\", \"................................................ ...................................................\", \".............     | ............................................................... 189        |\\n| U\", \"se Dapper as a micro ORM to perform queries ................................ .......................\", \"......... ..............................                                                            \", \"                         | 189                                                                      \", \"  |\\n| Dynamic versus static ViewModels  ................................ ...........................\", \"..................................... .........................                                     \", \"                                | 190                                                               \", \"         |\\n| Additional resources  ................................................................ \", \"................................................................ .....................              \", \"                                       | 193                                                        \", \"                |\\n| Design a DDD-oriented microservice  ................................ ...........\", \"..................................................... .........................                     \", \"                                              | 194                                                 \", \"                       |\\n| Keep the microservice context boundaries relatively small  ..............\", \"..................................................                                                  \", \"                                                     | .......... 194                               \", \"                              |\\n| Layers in DDD microservices  .....................................\", \"...........................                                                                         \", \"                                                            | ......................................\", \".......................... ..... 195 |\\n\\n| Design a microservice domain model  ......................\", \".......... ................................................................ ........................\", \"                                                 | 199                                              \", \"                                       |\\n|----------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"--------------------------------------------------|-------------------------------------------------\", \"----------------------------------------|\\n| The Domain Entity pattern  .............................\", \"................................... ................................................................\", \"                                                   | ......... 199                                  \", \"                                         |\\n| Implement a microservice domain model with .NET .......\", \"......................... ................................ ............................             \", \"                                                    | 204                                           \", \"                                          |\\n| Domain model structure in a custom .NET Standard Libra\", \"ry  ................................................................                                \", \"                                                     | ....... 204                                  \", \"                                           |\\n| Structure aggregates in a custom .NET Standard librar\", \"y  ................................................................ ...............                 \", \"                                                      | 205                                         \", \"                                            |\\n| Implement domain entities as POCO classes  .........\", \"....................... ................................................................            \", \"                                                       | ..... 206                                  \", \"                                             |\\n| Encapsulate data in the Domain Entities  ..........\", \"......................                                                                              \", \"                                                        | ..........................................\", \"...................... .............. 207     |\\n| Seedwork (reusable base classes and interfaces for\", \" your domain model) ................................ ..................                             \", \"                                                         | 210                                      \", \"                                               |\\n| The custom Entity base class ....................\", \"............................................                                                        \", \"                                                          | ........................................\", \"........................ ..... 211              |\\n| Repository contracts (interfaces) in the domain \", \"model layer  ................................................................                       \", \"                                                           | ...... 212                             \", \"                                                 |\\n| Additional resources  .........................\", \"....................................... ............................................................\", \".... .....................                                  | 213                                   \", \"                                                  |\\n| Implement value objects ......................\", \"..........................................                                                          \", \"                                                             | .....................................\", \"........................... .................. 213 |\\n| Important characteristics of value objects  .\", \"............................... ................................................................    \", \"                                                              | ........... 214                     \", \"                                                    |\\n| Value object implementation in C# ..........\", \"...................... ................................................................             \", \"                                                               | ........................ 215       \", \"                                                     |\\n| How to persist value objects in the databas\", \"e with EF Core 2.0 and later ................................ ................                      \", \"                                                                | 217                               \", \"                                                      |\\n| Persist value objects as owned entity type\", \"s in EF Core 2.0 and later ................................ ........................                \", \"                                                                 | 218                              \", \"                                                       |\\n| Additional resources  ...................\", \"............................................. ......................................................\", \"..........                                                        | ..................... 221       \", \"                                                        |\\n| Use enumeration classes instead of enum \", \"types  ................................ ............................................................\", \"....                                                               | ... 221                        \", \"                                                         |\\n| Implement an Enumeration base class ...\", \"............................. ................................................................ .....\", \".............                                                       | 222                           \", \"                                                          |\\n| Additional resources  ................\", \"................................................ ...................................................\", \"............. .....................                                  | 223                          \", \"                                                           |\\n| Design validations in the domain mode\", \"l layer  ................................ ..........................................................\", \"......                                                                | ....... 223                 \", \"                                                            |\\n| Implement validations in the domain \", \"model layer ................................ ................................ ......................\", \".....                                                                  | 224                        \", \"                                                             |\\n| Additional resources  .............\", \"................................................... ................................................\", \"................                                                        | ..................... 225 \", \"                                                              |\\n| Client - side validation (validati\", \"on in the presentation layers) ................................................................     \", \"                                                                         | ............ 226         \", \"                                                               |\\n| Additional resources  ...........\", \"..................................................... ..............................................\", \".................. .....................                                  | 227                     \", \"                                                                |\\n| Domain events: Design and implem\", \"entation  ................................ .........................................................\", \".......                                                                    | .......... 227         \", \"                                                                 |\\n| What is a domain event? .......\", \".........................................................                                           \", \"                                                                            | ......................\", \".......................................... ............. 228      |\\n| Domain events versus integrati\", \"on events  ................................ ........................................................\", \"........ ............                                                        | 228                  \", \"                                                                   |\\n| Domain events as a preferred \", \"way to trigger side effects across multiple aggregates within the  same domain  ....................\", \"............ ................................................................ | ....................\", \"............................................ ... 229                |\\n| Implement domain events ....\", \"............................................................ .......................................\", \".........................                                                      | .......... 231     \", \"                                                                     |\\n| Conclusions on domain event\", \"s ................................................................                                  \", \"                                                                                | ..................\", \".............................................. . 237                  |\\n\\n| Additional resources  ...\", \"............................................................. ......................................\", \".......................... .....................   | 238                                            \", \"                                                |\\n|-------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------|-----------------------------------------------------------------------\", \"-------------------------|\\n| Design the infrastructure persistence layer ...........................\", \"..... ................................................................ ..............               \", \"     | 238                                                                                          \", \"  |\\n| The Repository pattern  ................................................................ .....\", \"........................................................... ................      | 238             \", \"                                                                               |\\n| Additional resour\", \"ces  ................................................................ ..............................\", \".................................. .....................   | 243                                    \", \"                                                        |\\n| Implement the infrastructure persistence\", \" layer with Entity Framework Core  ................................ ............                    \", \"                                    | 243                                                           \", \"                                 |\\n| Introduction to Entity Framework Core .........................\", \"....... ................................................................ .................          \", \"             | 244                                                                                  \", \"          |\\n| Infrastructure in Entity Framework Core from a DDD perspective .......................\", \"......... .............................                                                   | 244     \", \"                                                                                       |\\n| Implement\", \" custom repositories with Entity Framework Core ....................................................\", \"............                                                       | ...... 246                     \", \"                                                                |\\n| EF DbContext and IUnitOfWork ins\", \"tance lifetime in your IoC container ................................ .................             \", \"                                            | 248                                                   \", \"                                         |\\n| The repository instance lifetime in your IoC container \", \"................................................................ ...................                \", \"                     | 249                                                                          \", \"                  |\\n| Table mapping  ................................ ..............................\", \"..................................                                                                | \", \"................................................................ 250                           |\\n| I\", \"mplement the Query Specification pattern ................................ ..........................\", \"......................................                                     | ........ 253           \", \"                                                                        |\\n| Use NoSQL databases as a\", \" persistence infrastructure ................................................................ .......\", \"..................                                  | 255                                           \", \"                                                 |\\n| Introduction to Azure Cosmos DB and the native \", \"Cosmos DB API  ................................ ...........................                         \", \"                             | 256                                                                  \", \"                          |\\n| Implement .NET code targeting MongoDB and Azure Cosmos DB  ...........\", \"..................... ..........................                                                    \", \"      | 258                                                                                         \", \"   |\\n| Design the microservice application layer and Web API  ......................................\", \".......................... ....................                                    | 266            \", \"                                                                                |\\n| Use SOLID princi\", \"ples and Dependency Injection ................................ ................................ ....\", \"..........................                                  | 266                                   \", \"                                                         |\\n| Implement the microservice application \", \"layer using the Web API  ................................................................           \", \"                                     | . 267                                                        \", \"                                  |\\n| Use Dependency Injection to inject infrastructure objects into\", \" your application layer .....................                                                       \", \"              | 267                                                                                 \", \"           |\\n| Implement the Command and Command Handler patterns  .................................\", \"...............................                                                            | .......\", \" 271                                                                                    |\\n| The Comm\", \"and process pipeline: how to trigger a command handler ................................ ............\", \".........                                                           | 278                           \", \"                                                                 |\\n| Implement the command process p\", \"ipeline with a mediator pattern (MediatR)  ................................                         \", \"                                             | .. 281                                               \", \"                                          |\\n| Apply cross-cutting concerns when processing commands \", \"with the Behaviors in MediatR  ..........                                                           \", \"                      | 287                                                                         \", \"                   |\\n| Implement resilient applications  ...........................................\", \"............................................                                                       |\", \" 291                                                                                            |\\n| \", \"Handle partial failure ................................ ................................            \", \"                                                                            | ......................\", \".......................................... ......................... 292 |\\n| Strategies to handle pa\", \"rtial failure ................................................................ .....................\", \"..........................................           | 294                                          \", \"                                                  |\\n| Additional resources  ........................\", \"........................................                                                            \", \"                              | ................................................................ ...\", \".................. 295     |\\n| Implement retries with exponential backoff  .........................\", \".......                                                                                             \", \"       | ................................................................ ............ 295          \", \"    |\\n| Implement resilient Entity Framework Core SQL connections ..................................\", \"..............................                                                      | .......... 295\", \"                                                                                 |\\n| Execution strat\", \"egies and explicit transactions using BeginTransaction and multiple DbContexts296                   \", \"                                                             |                                      \", \"                                                          |\\n| Additional resources  ................\", \"................................................ ...................................................\", \"............. .....................   | 298                                                         \", \"                                   |\\n| Use IHttpClientFactory to implement resilient HTTP requests  \", \"                                                                                                    \", \"               | ................................................................ ......... 298     \", \"            |\\n\\n| Issues with the original HttpClient class available in .NET .......................\", \"......................................... .............                                           | \", \"298                                                                                        |\\n|------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------|-----------------------\", \"---------------------------------------------------------------------|\\n| Benefits of using IHttpClie\", \"ntFactory ................................ .........................................................\", \"....... .......................                       | 299                                         \", \"                                               |\\n| Multiple ways to use IHttpClientFactory .........\", \"....................... ................................................................ ...........\", \"....                            | 300                                                               \", \"                         |\\n| How to use Typed Clients with IHttpClientFactory ......................\", \".......... ................................ ...........................                             \", \"          | 300                                                                                     \", \"   |\\n| Additional resources  ................................................................ ......\", \".......................................................... .....................        | 304       \", \"                                                                                 |\\n| Implement HTTP \", \"call retries with exponential backoff with IHttpClientFactory and Polly policies  ...               \", \"                                                                  | 304                             \", \"                                                           |\\n| Add a jitter strategy to the retry po\", \"licy  ................................ .............................................................\", \"... .................                       | 305                                                   \", \"                                     |\\n| Additional resources  .....................................\", \"........................... ................................................................ .......\", \"..............        | 306                                                                         \", \"               |\\n| Implement the Circuit Breaker pattern ................................ ..........\", \"...................................................... .......................                      \", \"| 306                                                                                        |\\n| Imp\", \"lement Circuit Breaker pattern with IHttpClientFactory and Polly  ................................ .\", \"....................                                                          | 307                 \", \"                                                                       |\\n| Test Http retries and cir\", \"cuit breakers in eShopOnContainers ................................................................ \", \"                                                        | ...... 308                                \", \"                                                 |\\n| Additional resources  .........................\", \"....................................... ............................................................\", \".... .....................        | 310                                                             \", \"                           |\\n| Health monitoring  ................................ .................\", \"............................................... ................................ ...................\", \"........... | 310                                                                                   \", \"     |\\n| Implement health checks in ASP.NET Core services  .........................................\", \"....................... ........................                                          | 311     \", \"                                                                                   |\\n| Use watchdogs\", \" ................................ ................................................................ .\", \"...............................................................     | 315                           \", \"                                                             |\\n| Health checks when using orchestrat\", \"ors  ................................ ..............................................................\", \".. ..............                             | 317                                                 \", \"                                       |\\n| Advanced monitoring: visualization, analysis, and alerts \", \" ................................................................ ...............                   \", \"                        | 317                                                                       \", \"                 |\\n| Additional resources  .........................................................\", \"....... ................................................................ .....................      \", \"  | 318                                                                                        |\\n| M\", \"ake secure .NET Microservices and Web Applications .................................................\", \"                                                                                | 319               \", \"                                                                         |\\n| Implement authenticatio\", \"n in .NET microservices and web applications  ................................ .....................\", \".                                                         | 319                                     \", \"                                                   |\\n| Authenticate with ASP.NET Core Identity .....\", \"........................... ................................................................ .......\", \"......                              | 320                                                           \", \"                             |\\n| Authenticate with external providers  .............................\", \"... ................................................................ .....................          \", \"              | 321                                                                                 \", \"       |\\n| Authenticate with bearer tokens .........................................................\", \"....... ................................ ..............................                     | 323   \", \"                                                                                     |\\n| Authenticat\", \"e with an OpenID Connect or OAuth 2.0 Identity provider ................................ ...........\", \"........                                                              | 324                         \", \"                                                               |\\n| Issue security tokens from an ASP\", \".NET Core service  ................................................................ ................\", \".......                                         | 325                                               \", \"                                         |\\n| Consume security tokens ...............................\", \"................................. ................................................................ .\", \"...........               | 326                                                                     \", \"                   |\\n| Additional resources ................................ .......................\", \"......................................... ................................                          \", \"    | .......................... 327                                                             |\\n|\", \" About authorization in .NET microservices and web applications  ...................................\", \".............................                                                     | .. 327          \", \"                                                                           |\\n| Implement role-based \", \"authorization  ................................ ....................................................\", \"............ .....................                          | 328                                   \", \"                                                     |\\n| Implement policy-based authorization  .....\", \"........................... ................................................................ .......\", \"..........                            | 329                                                         \", \"                               |\\n| Authorization and minimal apis  .................................\", \"............................... ...............................................................     \", \"                | 330                                                                               \", \"         |\\n| Additional resources  ................................................................ \", \"                                                                                              | ....\", \"............................................................ ..................... 330 |\\n\\n| Store ap\", \"plication secrets safely during development ................................ .......................\", \"......... ..........................                              |   330 |\\n|-----------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------|-------|\\n| Store secrets in environment variabl\", \"es  ................................ ...............................................................\", \". .................                   |   331 |\\n| Store secrets with the ASP.NET Core Secret Manager\", \"  ................................................................ ....................             \", \"                        |   331 |\\n| Use Azure Key Vault to protect secrets at production time  .....\", \"........................................................... ..............                          \", \"          |   332 |\\n| Additional resources  ........................................................\", \"........ ................................................................ ..................... |   \", \"333 |\\n| .NET Microservices Architecture key takeaways  .............................................\", \".................                                                                 |   334 |\\n\\nDocker \", \"Host\\n\\nApp 1\\n\\nApp 2\\n\\n## Introduction to Containers and Docker\\n\\nContainerization is an approach to sof\", \"tware development in which an application or service, its dependencies, and its configuration (abstr\", \"acted as deployment manifest files) are packaged together as a container image. The containerized ap\", \"plication can be tested as a unit and deployed as a container image instance to the host operating s\", \"ystem (OS).\\n\\nJust as shipping containers allow goods to be transported by ship, train, or truck rega\", \"rdless of the cargo inside, software containers act as a standard unit of software deployment that c\", \"an contain different code and dependencies. Containerizing software this way enables developers and \", \"IT professionals to deploy them across environments with little or no modification.\\n\\nContainers also\", \" isolate applications from each other on a shared OS. Containerized applications run on top of a con\", \"tainer host that in turn runs on the OS (Linux or Windows). Containers therefore have a significantl\", \"y smaller footprint than virtual machine (VM) images.\\n\\nEach container can run a whole web applicatio\", \"n or a service, as shown in Figure 2-1. In this example, Docker host is a container host, and App1, \", \"App2, Svc 1, and Svc 2 are containerized applications or services.\\n\\nFigure 2 -1. Multiple containers\", \" running on a container host\\n\\n<!-- image -->\\n\\nCustomer\\n\\nDatacenter\\n\\nAnother benefit of containerizat\", \"ion is scalability. You can scale out quickly by creating new containers for short -term tasks. From\", \" an application point of view, instantiating an image (creating a container) is similar to instantia\", \"ting a process like a service or a web app. For reliability, however, when you run multiple instance\", \"s of the same image across multiple host servers, you typically want each container (image instance)\", \" to run in a different host server or VM in different fault domains.\\n\\nIn short, containers offer the\", \" benefits of isolation, portability, agility, scalability, and control across the whole application \", \"lifecycle workflow. The most important benefit is the environment's isolation provided between Dev a\", \"nd Ops. Azure Provider\\n\\nDocker\\n\\n## What is Docker?\\n\\nDocker is an open-source project for automating \", \"the deployment of applications as portable, selfsufficient containers that can run on the cloud or o\", \"n -premises. Docker is also a company that promotes and evolves this technology, working in collabor\", \"ation with cloud, Linux, and Windows vendors, including Microsoft.\\n\\nFigure 2 -2. Docker deploys cont\", \"ainers at all layers of the hybrid cloud.\\n\\n<!-- image -->\\n\\nDocker containers can run anywhere, on-pr\", \"emises in the customer datacenter, in an external service provider or in the cloud, on Azure. Docker\", \" image containers can run natively on Linux and Windows. However, Windows images can run only on Win\", \"dows hosts and Linux images can run on Linux hosts and Windows hosts (using a Hyper-V Linux VM, so f\", \"ar), where host means a server or a VM.\\n\\nDevelopers can use development environments on Windows, Lin\", \"ux, or macOS. On the development computer, the developer runs a Docker host where Docker images are \", \"deployed, including the app and its dependencies. Developers who work on Linux or on macOS use a Doc\", \"ker host that is Linux based, and they can create images only for Linux containers. (Developers work\", \"ing on macOS can edit code or run the Docker CLI from macOS, but as of the time of this writing, con\", \"tainers don't run\\n\\nApp 1\\n\\nBins/Libs\\n\\nGuest OS\\n\\nApp 2\\n\\nBins/Libs\\n\\nApp 3\\n\\nBins/Libs\\n\\nApp 1\\n\\nBins/Libs\\n\", \"\\nApp 2\\n\\nBins/Libs\\n\\nApp 3\\n\\nBins/Libs directly on macOS.) Developers who work on Windows can create im\", \"ages for either Linux or Windows Containers.\\n\\nTo host containers in development environments and pro\", \"vide additional developer tools, Docker ships Docker Desktop for Windows or for macOS. These product\", \"s install the necessary VM (the Docker host) to host the containers.\\n\\nTo run Windows Containers, the\", \"re are two types of runtimes:\\n\\n- Windows Server Containers provide application isolation through pro\", \"cess and namespace isolation technology. A Windows Server Container shares a kernel with the contain\", \"er host and with all containers running on the host.\\n- Hyper -V Containers expand on the isolation p\", \"rovided by Windows Server Containers by running each container in a highly optimized virtual machine\", \". In this configuration, the kernel of the container host isn't shared with the Hyper-V Containers, \", \"providing better isolation.\\n\\nThe images for these containers are created the same way and function t\", \"he same. The difference is in how the container is created from the image running a Hyper-V Containe\", \"r requires an extra parameter. For details, see Hyper-V Containers .\\n\\n## Comparing Docker containers\", \" with virtual machines\\n\\nFigure 2-3 shows a comparison between VMs and Docker containers.\\n\\n<!-- image\", \" -->\\n\\nFigure 2 -3. Comparison of traditional virtual machines to Docker containers\\n\\n| Virtual Machin\", \"es                                                                                                  \", \"                                                                     | Docker Containers            \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                              |\\n|-------------------\", \"----------------------------------------------------------------------------------------------------\", \"-----------------------------------------------------------------|----------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"--------------------------------------------------------------------------|\\n| Virtual machines inclu\", \"de the application, the  required libraries or binaries, and a full guest  operating system. Full vi\", \"rtualization requires  more resources than containerization. | Containers include the application an\", \"d all its  dependencies. However, they share the OS kernel  with other containers, running as isolat\", \"ed  processes in user space on the host operating  system. (Except in Hyper-V containers, where  eac\", \"h container runs inside of a special virtual  machine per container.) |\\n\\nFor VMs, there are three ba\", \"se layers in the host server, from the bottom-up: infrastructure, Host Operating System and a Hyperv\", \"isor and on top of all that each VM has its own OS and all necessary libraries. For Docker, the host\", \" server only has the infrastructure and the OS and on top of that, the container engine, that keeps \", \"container isolated but sharing the base OS services.\\n\\nBecause containers require far fewer resources\", \" (for example, they don't need a full OS), they're easy to deploy and they start fast. This allows y\", \"ou to have higher density, meaning that it allows you to run more services on the same hardware unit\", \", thereby reducing costs.\\n\\nAs a side effect of running on the same kernel, you get less isolation th\", \"an VMs.\\n\\nThe main goal of an image is that it makes the environment (dependencies) the same across d\", \"ifferent deployments. This means that you can debug it on your machine and then deploy it to another\", \" machine with the same environment guaranteed.\\n\\nA container image is a way to package an app or serv\", \"ice and deploy it in a reliable and reproducible way. You could say that Docker isn't only a technol\", \"ogy but also a philosophy and a process.\\n\\nWhen using Docker, you won't hear developers say, \\\"It work\", \"s on my machine, why not in production?\\\" They can simply say, \\\"It runs on Docker\\\", because the packa\", \"ged Docker application can be executed on any supported Docker environment, and it runs the way it w\", \"as intended to on all deployment targets (such as Dev, QA, staging, and production).\\n\\n## A simple an\", \"alogy\\n\\nPerhaps a simple analogy can help getting the grasp of the core concept of Docker.\\n\\nLet's go \", \"back in time to the 1950s for a moment. There were no word processors, and the photocopiers were use\", \"d everywhere (kind of).\\n\\nImagine you're responsible for quickly issuing batches of letters as requir\", \"ed, to mail them to customers, using real paper and envelopes, to be delivered physically to each cu\", \"stomer's address (there was no email back then).\\n\\nAt some point, you realize the letters are just a \", \"composition of a large set of paragraphs, which are picked and arranged as needed, according to the \", \"purpose of the letter, so you devise a system to issue letters quickly, expecting to get a hefty rai\", \"se.\\n\\nThe system is simple:\\n\\n1. You begin with a deck of transparent sheets containing one paragraph \", \"each.\\n\\n2. To issue a set of letters, you pick the sheets with the paragraphs you need, then you stac\", \"k and align them so they look and read fine.\\n3. Finally, you place the set in the photocopier and pr\", \"ess start to produce as many letters as required.\\n\\nSo, simplifying, that\\u2019s the core idea of Docker.\\n\", \"\\nIn Docker, each layer is the resulting set of changes that happen to the filesystem after executing\", \" a command, such as, installing a program.\\n\\nSo, when you \\\"look\\\" at the filesystem after the layer ha\", \"s been copied, you see all the files, included in the layer when the program was installed.\\n\\nYou can\", \" think of an image as an auxiliary read-only hard disk ready to be installed in a \\\"computer\\\" where t\", \"he operating system is already installed.\\n\\nSimilarly, you can think of a container as the \\\"computer\\\"\", \" with the image hard disk installed. The container, just like a computer, can be powered on or off.\\n\", \"\\n## Docker terminology\\n\\nThis section lists terms and definitions you should be familiar with before \", \"getting deeper into Docker. For further definitions, see the extensive glossary provided by Docker.\\n\", \"\\nContainer image: A package with all the dependencies and information needed to create a container. \", \"An image includes all the dependencies (such as frameworks) plus deployment and execution configurat\", \"ion to be used by a container runtime. Usually, an image derives from multiple base images that are \", \"layers stacked on top of each other to form the container's filesystem. An image is immutable once i\", \"t has been created.\\n\\nDockerfile: A text file that contains instructions for building a Docker image.\", \" It's like a batch script, the first line states the base image to begin with and then follow the in\", \"structions to install required programs, copy files, and so on, until you get the working environmen\", \"t you need.\\n\\nBuild: The action of building a container image based on the information and context pr\", \"ovided by its Dockerfile, plus additional files in the folder where the image is built. You can buil\", \"d images with the following Docker command:\\n\\ndocker build\\n\\nContainer: An instance of a Docker image.\", \" A container represents the execution of a single application, process, or service. It consists of t\", \"he contents of a Docker image, an execution environment, and a standard set of instructions. When sc\", \"aling a service, you create multiple instances of a container from the same image. Or a batch job ca\", \"n create multiple containers from the same image, passing different parameters to each instance.\\n\\nVo\", \"lumes: Offer a writable filesystem that the container can use. Since images are read-only but most p\", \"rograms need to write to the filesystem, volumes add a writable layer, on top of the container image\", \", so the programs have access to a writable filesystem. The program doesn't know it's accessing a\\n\\nl\", \"ayered filesystem, it's just the filesystem as usual. Volumes live in the host system and are manage\", \"d by Docker.\\n\\nTag: A mark or label you can apply to images so that different images or versions of t\", \"he same image (depending on the version number or the target environment) can be identified.\\n\\nMulti \", \"-stage Build: Is a feature, since Docker 17.05 or higher, that helps to reduce the size of the final\", \" images. For example, a large base image, containing the SDK can be used for compiling and publishin\", \"g and then a small runtime-only base image can be used to host the application.\\n\\nRepository (repo): \", \"A collection of related Docker images, labeled with a tag that indicates the image version. Some rep\", \"os contain multiple variants of a specific image, such as an image containing SDKs (heavier), an ima\", \"ge containing only runtimes (lighter), etc. Those variants can be marked with tags. A single repo ca\", \"n contain platform variants, such as a Linux image and a Windows image.\\n\\nRegistry: A service that pr\", \"ovides access to repositories. The default registry for most public images is Docker Hub (owned by D\", \"ocker as an organization). A registry usually contains repositories from multiple teams. Companies o\", \"ften have private registries to store and manage images they've created. Azure Container Registry is\", \" another example.\\n\\nMulti -arch image: For multi-architecture (or multi-platform), it's a Docker feat\", \"ure that simplifies the selection of the appropriate image, according to the platform where Docker i\", \"s running. For example, when a Dockerfile requests a base image FROM mcr.microsoft.com/dotnet/sdk:7.\", \"0 from the registry, it actually gets 7.0-nanoserver-ltsc2022 , 7.0 -nanoserver-1809 or 7.0 -bullsey\", \"e-slim , depending on the operating system and version where Docker is running.\\n\\nDocker Hub: A publi\", \"c registry to upload images and work with them. Docker Hub provides Docker image hosting, public or \", \"private registries, build triggers and web hooks, and integration with GitHub and Bitbucket.\\n\\nAzure \", \"Container Registry: A public resource for working with Docker images and its components in Azure. Th\", \"is provides a registry that's close to your deployments in Azure and that gives you control over acc\", \"ess, making it possible to use your Azure Active Directory groups and permissions.\\n\\nDocker Trusted R\", \"egistry (DTR): A Docker registry service (from Docker) that can be installed onpremises so it lives \", \"within the organization's datacenter and network. It's convenient for private images that should be \", \"managed within the enterprise. Docker Trusted Registry is included as part of the Docker Datacenter \", \"product.\\n\\nDocker Desktop: Development tools for Windows and macOS for building, running, and testing\", \" containers locally. Docker Desktop for Windows provides development environments for both Linux and\", \" Windows Containers. The Linux Docker host on Windows is based on a Hyper-V virtual machine. The hos\", \"t for Windows Containers is directly based on Windows. Docker Desktop for Mac is based on the Apple \", \"Hypervisor framework and the xhyve hypervisor, which provides a Linux Docker host virtual machine on\", \" macOS. Docker Desktop for Windows and for Mac replaces Docker Toolbox, which was based on Oracle Vi\", \"rtualBox.\\n\\nCompose: A command-line tool and YAML file format with metadata for defining and running \", \"multicontainer applications. You define a single application based on multiple images with one or mo\", \"re .yml files that can override values depending on the environment. After you've created the defini\", \"tions,\\n\\nyou can deploy the whole multi-container application with a single command (docker-compose u\", \"p) that creates a container per image on the Docker host.\\n\\nCluster: A collection of Docker hosts exp\", \"osed as if it were a single virtual Docker host, so that the application can scale to multiple insta\", \"nces of the services spread across multiple hosts within the cluster. Docker clusters can be created\", \" with Kubernetes, Azure Service Fabric, Docker Swarm and Mesosphere DC/OS.\\n\\nOrchestrator: A tool tha\", \"t simplifies the management of clusters and Docker hosts. Orchestrators enable you to manage their i\", \"mages, containers, and hosts through a command-line interface (CLI) or a graphical UI. You can manag\", \"e container networking, configurations, load balancing, service discovery, high availability, Docker\", \" host configuration, and more. An orchestrator is responsible for running, distributing, scaling, an\", \"d healing workloads across a collection of nodes. Typically, orchestrator products are the same prod\", \"ucts that provide cluster infrastructure, like Kubernetes and Azure Service Fabric, among other offe\", \"rings in the market.\\n\\n## Docker containers, images, and registries\\n\\nWhen using Docker, a developer c\", \"reates an app or service and packages it and its dependencies into a container image. An image is a \", \"static representation of the app or service and its configuration and dependencies.\\n\\nTo run the app \", \"or service, the app's image is instantiated to create a container, which will be running on the Dock\", \"er host. Containers are initially tested in a development environment or PC.\\n\\nDevelopers should stor\", \"e images in a registry, which acts as a library of images and is needed when deploying to production\", \" orchestrators. Docker maintains a public registry via Docker Hub; other vendors provide registries \", \"for different collections of images, including Azure Container Registry . Alternatively, enterprises\", \" can have a private registry on-premises for their own Docker images.\\n\\nFigure 2-4 shows how images a\", \"nd registries in Docker relate to other components. It also shows the multiple registry offerings fr\", \"om vendors.\\n\\nBasic taxonomy in Docker\\n\\nRegistry\\n\\nHosted Docker\\n\\nRegistry\\n\\nDocker Trusted\\n\\nRegistry o\", \"n-prem.\\n\\nI'n' private organizations)\\n\\nFigure 2 -4. Taxonomy of Docker terms and concepts\\n\\n<!-- image\", \" -->\\n\\nThe registry is like a bookshelf where images are stored and available to be pulled for buildi\", \"ng containers to run services or web apps. There are private Docker registries on-premises and on th\", \"e public cloud. Docker Hub is a public registry maintained by Docker, along the Docker Trusted Regis\", \"try an enterprise-grade solution, Azure offers the Azure Container Registry. AWS, Google, and others\", \" also have container registries.\\n\\nPutting images in a registry lets you store static and immutable a\", \"pplication bits, including all their dependencies at a framework level. Those images can then be ver\", \"sioned and deployed in multiple environments and therefore provide a consistent deployment unit.\\n\\nPr\", \"ivate image registries, either hosted on-premises or in the cloud, are recommended when:\\n\\n- Your ima\", \"ges must not be shared publicly due to confidentiality.\\n- You want to have minimum network latency b\", \"etween your images and your chosen deployment environment. For example, if your production environme\", \"nt is Azure cloud, you probably want to store your images in Azure Container Registry so that networ\", \"k latency will be minimal. In a similar way, if your production environment is on-premises, you migh\", \"t want to have an on -premises Docker Trusted Registry available within the same local network.\\n\\nOn-\", \"premises\\n\\n## Choosing Between .NET and .NET Framework for Docker Containers\\n\\nThere are two supported\", \" frameworks for building server-side containerized Docker applications with .NET: .NET Framework and\", \" .NET 7. They share many .NET platform components, and you can share code across the two. However, t\", \"here are fundamental differences between them, and which framework you use will depend on what you w\", \"ant to accomplish. This section provides guidance on when to choose each framework.\\n\\n## General guid\", \"ance\\n\\nThis section provides a summary of when to choose .NET 7 or .NET Framework. We provide more de\", \"tails about these choices in the sections that follow.\\n\\nUse .NET 7, with Linux or Windows Containers\", \", for your containerized Docker server application when:\\n\\n- You have cross -platform needs. For exam\", \"ple, you want to use both Linux and Windows Containers.\\n- Your application architecture is based on \", \"microservices.\\n- You need to start containers fast and want a small footprint per container to achie\", \"ve better density or more containers per hardware unit in order to lower your costs.\\n\\nIn short, when\", \" you create new containerized .NET applications, you should consider .NET 7 as the default choice. I\", \"t has many benefits and fits best with the containers philosophy and style of working.\\n\\nAn extra ben\", \"efit of using .NET 7 is that you can run side-by-side .NET versions for applications within the same\", \" machine. This benefit is more important for servers or VMs that do not use containers, because cont\", \"ainers isolate the versions of .NET that the app needs. (As long as they are compatible with the und\", \"erlying OS.)\\n\\nUse .NET Framework for your containerized Docker server application when:\\n\\n- Your appl\", \"ication currently uses .NET Framework and has strong dependencies on Windows.\\n\\n- You need to use Win\", \"dows APIs that are not supported by .NET 7.\\n- You need to use third -party .NET libraries or NuGet p\", \"ackages that are not available for .NET 7.\\n\\nUsing .NET Framework on Docker can improve your deployme\", \"nt experiences by minimizing deployment issues. This \\\"lift and shift\\\" scenario is important for cont\", \"ainerizing legacy applications that were originally developed with the traditional .NET Framework, l\", \"ike ASP.NET WebForms, MVC web apps, or WCF (Windows Communication Foundation) services.\\n\\n## Addition\", \"al resources\\n\\n- E -book: Modernize existing .NET Framework applications with Azure and Windows Conta\", \"iners https://aka.ms/liftandshiftwithcontainersebook\\n- Sample apps: Modernization of legacy ASP.NET \", \"web apps by using Windows Containers https://aka.ms/eshopmodernizing\\n\\n## When to choose .NET for Doc\", \"ker containers\\n\\nThe modularity and lightweight nature of .NET 7 makes it perfect for containers. Whe\", \"n you deploy and start a container, its image is far smaller with .NET 7 than with .NET Framework. I\", \"n contrast, to use .NET Framework for a container, you must base your image on the Windows Server Co\", \"re image, which is a lot heavier than the Windows Nano Server or Linux images that you use for .NET \", \"7.\\n\\nAdditionally, .NET 7 is cross-platform, so you can deploy server apps with Linux or Windows cont\", \"ainer images. However, if you are using the traditional .NET Framework, you can only deploy images b\", \"ased on Windows Server Core.\\n\\nThe following is a more detailed explanation of why to choose .NET 7.\\n\", \"\\n## Developing and deploying cross platform\\n\\nClearly, if your goal is to have an application (web ap\", \"p or service) that can run on multiple platforms supported by Docker (Linux and Windows), the right \", \"choice is .NET 7, because .NET Framework only supports Windows.\\n\\n.NET 7 also supports macOS as a dev\", \"elopment platform. However, when you deploy containers to a Docker host, that host must (currently) \", \"be based on Linux or Windows. For example, in a development environment, you could use a Linux VM ru\", \"nning on a Mac.\\n\\nVisual Studio provides an integrated development environment (IDE) for Windows and \", \"supports Docker development.\\n\\nVisual Studio for Mac is an IDE, evolution of Xamarin Studio, that run\", \"s on macOS and supports Docker -based application development. This tool should be the preferred cho\", \"ice for developers working in Mac machines who also want to use a powerful IDE.\\n\\nYou can also use Vi\", \"sual Studio Code on macOS, Linux, and Windows. Visual Studio Code fully supports .NET 7, including I\", \"ntelliSense and debugging. Because VS Code is a lightweight editor, you\\n\\ncan use it to develop conta\", \"inerized apps on the machine in conjunction with the Docker CLI and the .NET CLI. You can also targe\", \"t .NET 7 with most third-party editors like Sublime, Emacs, vi, and the open -source OmniSharp proje\", \"ct, which also provides IntelliSense support.\\n\\nIn addition to the IDEs and editors, you can use the \", \".NET CLI for all supported platforms.\\n\\n## Using containers for new (\\u201cgreen-field\\u201d) projects\\n\\nContain\", \"ers are commonly used in conjunction with a microservices architecture, although they can also be us\", \"ed to containerize web apps or services that follow any architectural pattern. You can use .NET Fram\", \"ework on Windows Containers, but the modularity and lightweight nature of .NET 7 makes it perfect fo\", \"r containers and microservices architectures. When you create and deploy a container, its image is f\", \"ar smaller with .NET 7 than with .NET Framework.\\n\\n## Create and deploy microservices on containers\\n\\n\", \"You could use the traditional .NET Framework for building microservices-based applications (without \", \"containers) by using plain processes. That way, because the .NET Framework is already installed and \", \"shared across processes, processes are light and fast to start. However, if you are using containers\", \", the image for the traditional .NET Framework is also based on Windows Server Core and that makes i\", \"t too heavy for a microservices-on-containers approach. However, teams have been looking for opportu\", \"nities to improve the experience for .NET Framework users as well. Recently, size of the Windows Ser\", \"ver Core container images have been reduced to &gt;40% smaller .\\n\\nOn the other hand, .NET 7 is the b\", \"est candidate if you're embracing a microservices-oriented system that is based on containers becaus\", \"e .NET 7 is lightweight. In addition, its related container images, for either Linux or Windows Nano\", \" Server, are lean and small, making containers light and fast to start.\\n\\nA microservice is meant to \", \"be as small as possible: to be light when spinning up, to have a small footprint, to have a small Bo\", \"unded Context (check DDD, Domain-Driven Design), to represent a small area of concerns, and to be ab\", \"le to start and stop fast. For those requirements, you will want to use small and fast -to -instanti\", \"ate container images like the .NET 7 container image.\\n\\nA microservices architecture also allows you \", \"to mix technologies across a service boundary. This approach enables a gradual migration to .NET 7 f\", \"or new microservices that work in conjunction with other microservices or with services developed wi\", \"th Node.js, Python, Java, GoLang, or other technologies.\\n\\n## Deploying high density in scalable syst\", \"ems\\n\\nWhen your container-based system needs the best possible density, granularity, and performance,\", \" .NET and ASP.NET Core are your best options. ASP.NET Core is up to 10 times faster than ASP.NET in \", \"the traditional .NET Framework, and it leads to other popular industry technologies for microservice\", \"s, such as Java servlets, Go, and Node.js.\\n\\nThis approach is especially relevant for microservices a\", \"rchitectures, where you could have hundreds of microservices (containers) running. With ASP.NET Core\", \" images (based on the .NET runtime) on Linux or Windows Nano, you can run your system with a much lo\", \"wer number of servers or VMs, ultimately saving costs in infrastructure and hosting.\\n\\n## When to cho\", \"ose .NET Framework for Docker containers\\n\\nWhile .NET 7 offers significant benefits for new applicati\", \"ons and application patterns, .NET Framework will continue to be a good choice for many existing sce\", \"narios.\\n\\n## Migrating existing applications directly to a Windows Server container\\n\\nYou might want t\", \"o use Docker containers just to simplify deployment, even if you are not creating microservices. For\", \" example, perhaps you want to improve your DevOps workflow with Docker\\u2014 containers can give you bett\", \"er isolated test environments and can also eliminate deployment issues caused by missing dependencie\", \"s when you move to a production environment. In cases like these, even if you are deploying a monoli\", \"thic application, it makes sense to use Docker and Windows Containers for your current .NET Framewor\", \"k applications.\\n\\nIn most cases for this scenario, you will not need to migrate your existing applica\", \"tions to .NET 7; you can use Docker containers that include the traditional .NET Framework. However,\", \" a recommended approach is to use .NET 7 as you extend an existing application, such as writing a ne\", \"w service in ASP.NET Core.\\n\\n## Using third-party .NET libraries or NuGet packages not available for \", \".NET 7\\n\\nThird -party libraries are quickly embracing .NET Standard, which enables code sharing acros\", \"s all .NET flavors, including .NET 7. With .NET Standard 2.0 and later, the API surface compatibilit\", \"y across different frameworks has become significantly larger. Even more, .NET Core 2.x and newer ap\", \"plications can also directly reference existing .NET Framework libraries (see .NET Framework 4.6.1 s\", \"upporting .NET Standard 2.0).\\n\\nIn addition, the Windows Compatibility Pack extends the API surface a\", \"vailable for .NET Standard 2.0 on Windows. This pack allows recompiling most existing code to .NET S\", \"tandard 2.x with little or no modification, to run on Windows.\\n\\nHowever, even with that exceptional \", \"progression since .NET Standard 2.0 and .NET Core 2.1 or later, there might be cases where certain N\", \"uGet packages need Windows to run and might not support .NET Core or later. If those packages are cr\", \"itical for your application, then you will need to use .NET Framework on Windows Containers.\\n\\n## Usi\", \"ng .NET technologies not available for .NET 7\\n\\nSome .NET Framework technologies aren't available in \", \".NET 7. Some of them might become available in later releases, but others don't fit the new applicat\", \"ion patterns targeted by .NET Core and might never be available.\\n\\nThe following list shows most of t\", \"he technologies that aren\\u2019t available in .NET 7:\\n\\n- ASP.NET Web Forms. This technology is only avail\", \"able on .NET Framework. Currently there are no plans to bring ASP.NET Web Forms to .NET or later.\\n- \", \"Workflow -related services. Windows Workflow Foundation (WF), Workflow Services (WCF + WF in a singl\", \"e service), and WCF Data Services (formerly known as ADO.NET Data Services) are only available on .N\", \"ET Framework. There are currently no plans to bring them to .NET 7.\\n\\nIn addition to the technologies\", \" listed in the official .NET roadmap, other features might be ported to the new unified .NET platfor\", \"m. You might consider participating in the discussions on GitHub so that your voice can be heard. An\", \"d if you think something is missing, file a new issue in the dotnet/runtime GitHub repository.\\n\\n## U\", \"sing a platform or API that doesn\\u2019t support .NET 7\\n\\nSome Microsoft and third -party platforms don't \", \"support .NET 7. For example, some Azure services provide an SDK that isn't yet available for consump\", \"tion on .NET 7 yet. Most Azure SDK should eventually be ported to .NET 7/.NET Standard, but some mig\", \"ht not for several reasons. You can see the available Azure SDKs in the Azure SDK Latest Releases pa\", \"ge.\\n\\nIn the meantime, if any platform or service in Azure still doesn't support .NET 7 with its clie\", \"nt API, you can use the equivalent REST API from the Azure service or the client SDK on .NET Framewo\", \"rk.\\n\\n## Porting existing ASP.NET application to .NET 7\\n\\n.NET Core is a revolutionary step forward fr\", \"om .NET Framework. It offers a host of advantages over .NET Framework across the board from producti\", \"vity to performance, and from cross-platform support to developer satisfaction. If you are using .NE\", \"T Framework and planning to migrate your application to .NET Core or .NET 5+, see Porting Existing A\", \"SP.NET Apps to .NET Core .\\n\\n## Additional resources\\n\\n- .NET fundamentals https://learn.microsoft.com\", \"/dotnet/fundamentals\\n- Porting Projects to .NET 5 https://learn.microsoft.com/events/dotnetconf-2020\", \"/porting-projects-to-net-5\\n- .NET on Docker Guide https://learn.microsoft.com/dotnet/core/docker/int\", \"roduction\\n\\n## Decision table: .NET implementations to use for Docker\\n\\nThe following decision table s\", \"ummarizes whether to use .NET Framework or .NET 7. Remember that for Linux containers, you need Linu\", \"x-based Docker hosts (VMs or servers), and that for Windows Containers, you need Windows Server-base\", \"d Docker hosts (VMs or servers).\\n\\n## Important\\n\\nYour development machines will run one Docker host, \", \"either Linux or Windows. Related microservices that you want to run and test together in one solutio\", \"n will all need to run on the same container platform.\\n\\n| Architecture / App Type                   \", \"                         | Linux containers                                                         \", \"     | Windows Containers                                                                           \", \"|\\n|--------------------------------------------------------------------|----------------------------\", \"---------------------------------------------------|------------------------------------------------\", \"----------------------------------------------|\\n| Microservices on containers                       \", \"                 | .NET 7                                                                        | .\", \"NET 7                                                                                       |\\n| Mono\", \"lithic app                                                     | .NET 7                             \", \"                                           | .NET Framework .NET 7                                  \", \"                                      |\\n| Best - in - class performance and  scalability            \", \"         | .NET 7                                                                        | .NET 7   \", \"                                                                                    |\\n| Windows Serv\", \"er legacy app (\\u201cbrown\\u0002 field\\u201d) migration to containers | \\u2013                                          \", \"                                   | .NET Framework                                                 \", \"                              |\\n| New container - based development  (\\u201cgreen - field\\u201d)              \", \" | .NET 7                                                                        | .NET 7           \", \"                                                                            |\\n| ASP.NET Core        \", \"                                               | .NET 7                                             \", \"                           | .NET 7 (recommended) .NET Framework                                    \", \"                      |\\n| ASP.NET 4 (MVC 5, Web API 2, and  Web Forms)                       | \\u2013    \", \"                                                                         | .NET Framework           \", \"                                                                    |\\n| SignalR services            \", \"                                       | .NET Core 2.1 or higher  version                           \", \"                   | .NET Framework .NET Core 2.1 or higher  version                                \", \"              |\\n| WCF, WF, and other legacy  frameworks                              | WCF in .NET C\", \"ore (client  library only) or CoreWCF                            | .NET Framework WCF in .NET 7 (cli\", \"ent library  only) or CoreWCF                               |\\n| Consumption of Azure services       \", \"                               | .NET 7 (eventually most Azure  services will provide client  SDKs f\", \"or .NET 7) | .NET Framework .NET 7 (eventually most Azure  services will provide client  SDKs for .N\", \"ET 7) |\\n\\n## What OS to target with .NET containers\\n\\nGiven the diversity of operating systems support\", \"ed by Docker and the differences between .NET Framework and .NET 7, you should target a specific OS \", \"and specific versions depending on the framework you are using.\\n\\nFor Windows, you can use Windows Se\", \"rver Core or Windows Nano Server. These Windows versions provide different characteristics (IIS in W\", \"indows Server Core versus a self-hosted web server like Kestrel in Nano Server) that might be needed\", \" by .NET Framework or .NET 7, respectively.\\n\\nFor Linux, multiple distros are available and supported\", \" in official .NET Docker images (like Debian).\\n\\nExisting\\n\\n\\u2022NET\\n\\napps\\n\\nWhat OS to target with .NET co\", \"ntainers\\n\\nIn Figure 3-1, you can see the possible OS version depending on the .NET framework used.\\n\\n\", \"3.5, 4.x\\n\\nServer Core\\n\\nIIS\\n\\nLarger Image\\n\\nFigure 3 -1. Operating systems to target depending on vers\", \"ions of the .NET framework\\n\\n<!-- image -->\\n\\nWhen deploying legacy .NET Framework applications you ha\", \"ve to target Windows Server Core, compatible with legacy apps and IIS, but it has a larger image. Wh\", \"en deploying .NET 7 applications, you can target Windows Nano Server, which is cloud optimized, uses\", \" Kestrel and is smaller and starts faster. You can also target Linux, supporting Debian, Alpine, and\", \" others.\\n\\nYou can also create your own Docker image in cases where you want to use a different Linux\", \" distro or where you want an image with versions not provided by Microsoft. For example, you might c\", \"reate an image with ASP.NET Core running on the traditional .NET Framework and Windows Server Core, \", \"which is a not -so-common scenario for Docker.\\n\\nWhen you add the image name to your Dockerfile file,\", \" you can select the operating system and version depending on the tag you use, as in the following e\", \"xamples:\\n\\n| Image                                                | Comments                         \", \"                                                                                                    \", \"                                      |\\n|------------------------------------------------------|----\", \"----------------------------------------------------------------------------------------------------\", \"--------------------------------------------------------------------|\\n| mcr.microsoft.com/dotnet/run\", \"time:7.0                 | .NET 7 multi - architecture: Supports Linux and Windows  Nano Server depe\", \"nding on the Docker host.                                                                         |\\n\", \"| mcr.microsoft.com/dotnet/aspnet:7.0                  | ASP.NET Core 7.0 multi-architecture: Suppor\", \"ts Linux and  Windows Nano Server depending on the Docker host. The aspnetcore image has a few optim\", \"izations for  ASP.NET Core. |\\n| mcr.microsoft.com/dotnet/aspnet:7.0- bullseye-slim   | .NET 7 runtim\", \"e - only on Linux Debian distro                                                                     \", \"                                                          |\\n| mcr.microsoft.com/dotnet/aspnet:7.0- n\", \"anoserver-1809 | .NET 7 runtime - only on Windows Nano Server (Windows  Server version 1809)        \", \"                                                                                        |\\n\\n## Offici\", \"al .NET Docker images\\n\\nThe Official .NET Docker images are Docker images created and optimized by Mi\", \"crosoft. They're publicly available on Microsoft Artifact Registry. You can search over the catalog \", \"to find all .NET image repositories, for example .NET SDK repository.\\n\\nEach repository can contain m\", \"ultiple images, depending on .NET versions, and depending on the OS and versions (Linux Debian, Linu\", \"x Alpine, Windows Nano Server, Windows Server Core, and so on). Image repositories provide extensive\", \" tagging to help you select not just a specific framework version, but also to choose an OS (Linux d\", \"istribution or Windows version).\\n\\n## .NET and Docker image optimizations for development versus prod\", \"uction\\n\\nWhen building Docker images for developers, Microsoft focused on the following main scenario\", \"s:\\n\\n- Images used to develop and build .NET apps.\\n- Images used to run .NET apps.\\n\\nWhy multiple imag\", \"es? When developing, building, and running containerized applications, you usually have different pr\", \"iorities. By providing different images for these separate tasks, Microsoft helps optimize the separ\", \"ate processes of developing, building, and deploying apps.\\n\\n## During development and build\\n\\nDuring \", \"development, what is important is how fast you can iterate changes, and the ability to debug the cha\", \"nges. The size of the image isn't as important as the ability to make changes to your code and see t\", \"he changes quickly. Some tools and \\\"build-agent containers\\\", use the development .NET image (mcr.mic\", \"rosoft.com/dotnet/sdk:7.0) during development and build process. When building inside a Docker conta\", \"iner, the important aspects are the elements that are needed to compile your app. This includes the \", \"compiler and any other .NET dependencies.\\n\\nWhy is this type of build image important? You don't depl\", \"oy this image to production. Instead, it's an image that you use to build the content you place into\", \" a production image. This image would be used in your continuous integration (CI) environment or bui\", \"ld environment when using Docker multi-stage builds.\\n\\n## In production\\n\\nWhat is important in product\", \"ion is how fast you can deploy and start your containers based on a production .NET image. Therefore\", \", the runtime-only image based on mcr.microsoft.com/dotnet/aspnet:7.0 is small so that it can travel\", \" quickly across the network from your Docker registry to your Docker hosts. The contents are ready t\", \"o run, enabling the fastest time from starting the container to processing results. In the Docker mo\", \"del, there is no need for compilation from C# code, as there's when you run dotnet build or dotnet p\", \"ublish when using the build container.\\n\\nIn this optimized image, you put only the binaries and other\", \" content needed to run the application. For example, the content created by dotnet publish contains \", \"only the compiled .NET binaries, images, .js, and .css files. Over time, you'll see images that cont\", \"ain pre-jitted (the compilation from IL to native that occurs at run time) packages.\\n\\nAlthough there\", \" are multiple versions of the .NET and ASP.NET Core images, they all share one or more layers, inclu\", \"ding the base layer. Therefore, the amount of disk space needed to store an image is small; it consi\", \"sts only of the delta between your custom image and its base image. The result is that it's quick to\", \" pull the image from your registry.\\n\\nWhen you explore the .NET image repositories at Microsoft Artif\", \"act Registry, you'll find multiple image versions classified or marked with tags. These tags help to\", \" decide which one to use, depending on the version you need, like those in the following table:\\n\\n| I\", \"mage                               | Comments                                                       \", \"                                    |\\n|-------------------------------------|-----------------------\", \"-----------------------------------------------------------------------------|\\n| mcr.microsoft.com/d\", \"otnet/aspnet:7.0 | ASP.NET Core, with runtime only and ASP.NET Core  optimizations, on Linux and Win\", \"dows (multi-arch) |\\n| mcr.microsoft.com/dotnet/sdk:7.0    | .NET 7, with SDKs included, on Linux and\", \" Windows  (multi-arch)                                     |\\n\\nYou can find all the available docker \", \"images in dotnet-docker and also refer to the latest preview releases by using nightly build mcr.mic\", \"rosoft.com/dotnet/nightly/*\\n\\n## Architecting container and microservice -based applications\\n\\nMicrose\", \"rvices offer great benefits but also raise huge new challenges. Microservice architecture patterns a\", \"re fundamental pillars when creating a microservice-based application.\\n\\nEarlier in this guide, you l\", \"earned basic concepts about containers and Docker. That information was the minimum you needed to ge\", \"t started with containers. Even though containers are enablers of, and a great fit for microservices\", \", they aren't mandatory for a microservice architecture. Many architectural concepts in this archite\", \"cture section could be applied without containers. However, this guide focuses on the intersection o\", \"f both due to the already introduced importance of containers.\\n\\nEnterprise applications can be compl\", \"ex and are often composed of multiple services instead of a single service-based application. For th\", \"ose cases, you need to understand other architectural approaches, such as the microservices and cert\", \"ain Domain-Driven Design (DDD) patterns plus container orchestration concepts. Note that this chapte\", \"r describes not just microservices on containers, but any containerized application, as well.\\n\\n## Co\", \"ntainer design principles\\n\\nIn the container model, a container image instance represents a single pr\", \"ocess. By defining a container image as a process boundary, you can create primitives that can be us\", \"ed to scale or batch the process.\\n\\nWhen you design a container image, you'll see an ENTRYPOINT defin\", \"ition in the Dockerfile. This definition defines the process whose lifetime controls the lifetime of\", \" the container. When the process completes, the container lifecycle ends. Containers might represent\", \" long-running processes like web servers, but can also represent short-lived processes like batch jo\", \"bs, which formerly might have been implemented as Azure WebJobs .\\n\\nIf the process fails, the contain\", \"er ends, and the orchestrator takes over. If the orchestrator was configured to keep five instances \", \"running and one fails, the orchestrator will create another container instance to replace the failed\", \" process. In a batch job, the process is started with parameters. When the process completes, the wo\", \"rk is complete. This guidance drills-down on orchestrators, later on.\\n\\nMonolithic Containerized appl\", \"ication\\n\\nApp 1 = 1 Container\\n\\nA monolithic application has most of its functionality within\\n\\na singl\", \"e process/container that is componentized with internal\\n\\nYou might find a scenario where you want mu\", \"ltiple processes running in a single container. For that scenario, since there can be only one entry\", \" point per container, you could run a script within the container that launches as many programs as \", \"needed. For example, you can use Supervisor or a similar tool to take care of launching multiple pro\", \"cesses inside a single container. However, even though you can find architectures that hold multiple\", \" processes per container, that approach isn't very common.\\n\\n## Containerizing monolithic application\", \"s\\n\\nYou might want to build a single, monolithically deployed web application or service and deploy i\", \"t as a container. The application itself might not be internally monolithic, but structured as sever\", \"al libraries, components, or even layers (application layer, domain layer, data-access layer, etc.).\", \" Externally, however, it's a single container\\u2014a single process, a single web application, or a singl\", \"e service.\\n\\nTo manage this model, you deploy a single container to represent the application. To inc\", \"rease capacity, you scale out, that is, just add more copies with a load balancer in front. The simp\", \"licity comes from managing a single deployment in a single container or VM.\\n\\nFigure 4 -1. Example of\", \" the architecture of a containerized monolithic application\\n\\n<!-- image -->\\n\\nYou can include multipl\", \"e components, libraries, or internal layers in each container, as illustrated in Figure 4-1. A monol\", \"ithic containerized application has most of its functionality within a single container, with intern\", \"al layers or libraries, and scales out by cloning the container on multiple servers/VMs. However, th\", \"is monolithic pattern might conflict with the container principle \\\"a container does one thing, and d\", \"oes it in one process\\\", but might be ok for some cases.\\n\\nThe downside of this approach becomes evide\", \"nt if the application grows, requiring it to scale. If the entire application can scale, it isn't re\", \"ally a problem. However, in most cases, just a few parts of the application are the choke points tha\", \"t require scaling, while other components are used less.\\n\\nFor example, in a typical e-commerce appli\", \"cation, you likely need to scale the product information subsystem, because many more customers brow\", \"se products than purchase them. More customers use\\n\\nHost 1\\n\\n(Server/VM)\\n\\nHost 2\\n\\n(Server/VM)\\n\\nHost 3\", \"\\n\\n(Server/VM)\\n\\nHost running multiple apps/containers their basket than use the payment pipeline. Few\", \"er customers add comments or view their purchase history. And you might have only a handful of emplo\", \"yees that need to manage the content and marketing campaigns. If you scale the monolithic design, al\", \"l the code for these different tasks is deployed multiple times and scaled at the same grade.\\n\\nThere\", \" are multiple ways to scale an application-horizontal duplication, splitting different areas of the \", \"application, and partitioning similar business concepts or data. But, in addition to the problem of \", \"scaling all components, changes to a single component require complete retesting of the entire appli\", \"cation, and a complete redeployment of all the instances.\\n\\nHowever, the monolithic approach is commo\", \"n, because the development of the application is initially easier than for microservices approaches.\", \" Thus, many organizations develop using this architectural approach. While some organizations have h\", \"ad good enough results, others are hitting limits. Many organizations designed their applications us\", \"ing this model because tools and infrastructure made it too difficult to build service -oriented arc\", \"hitectures (SOA) years ago, and they did not see the needuntil the application grew.\\n\\nFrom an infras\", \"tructure perspective, each server can run many applications within the same host and have an accepta\", \"ble ratio of efficiency in resources usage, as shown in Figure 4-2.\\n\\nFigure 4 -2. Monolithic approac\", \"h: Host running multiple apps, each app running as a container\\n\\n<!-- image -->\\n\\nMonolithic applicati\", \"ons in Microsoft Azure can be deployed using dedicated VMs for each instance. Additionally, using Az\", \"ure virtual machine scale sets, you can easily scale the VMs. Azure App Service can also run monolit\", \"hic applications and easily scale instances without requiring you to manage the VMs. Since 2016, Azu\", \"re App Services can run single instances of Docker containers as well, simplifying deployment.\\n\\nAs a\", \" QA environment or a limited production environment, you can deploy multiple Docker host VMs and bal\", \"ance them using the Azure balancer, as shown in Figure 4-3. This lets you manage scaling with a coar\", \"se-grain approach, because the whole application lives within a single container.\\n\\nBrowser or\\n\\nClien\", \"t app\\n\\nArchitecture in Docker infrastructure for monolithic applications\\n\\nMicrosoft\\n\\nAzure\\n\\n\\u2022\\n\\n## Ho\", \"st 1 (VM) Qor l\\n\\nFigure 4 -3. Example of multiple hosts scaling up a single container application\\n\\n<\", \"!-- image -->\\n\\nDeployment to the various hosts can be managed with traditional deployment techniques\", \". Docker hosts can be managed with commands like docker run or docker-compose performed manually, or\", \" through automation such as continuous delivery (CD) pipelines.\\n\\n## Deploying a monolithic applicati\", \"on as a container\\n\\nThere are benefits to using containers to manage monolithic application deploymen\", \"ts. Scaling container instances is far faster and easier than deploying additional VMs. Even if you \", \"use virtual machine scale sets, VMs take time to start. When deployed as traditional application ins\", \"tances instead of containers, the configuration of the application is managed as part of the VM, whi\", \"ch isn't ideal.\\n\\nDeploying updates as Docker images is far faster and network efficient. Docker imag\", \"es typically start in seconds, which speeds rollouts. Tearing down a Docker image instance is as eas\", \"y as issuing a docker stop command, and typically completes in less than a second.\\n\\nBecause containe\", \"rs are immutable by design, you never need to worry about corrupted VMs. In contrast, update scripts\", \" for a VM might forget to account for some specific configuration or file left on disk.\\n\\nWhile monol\", \"ithic applications can benefit from Docker, we're touching only on the benefits. Additional benefits\", \" of managing containers come from deploying with container orchestrators, which manage the various i\", \"nstances and lifecycle of each container instance. Breaking up the monolithic application into subsy\", \"stems that can be scaled, developed, and deployed individually is your entry point into the realm of\", \" microservices.\\n\\n## Publishing a single-container-based application to Azure App Service\\n\\nWhether yo\", \"u want to get validation of a container deployed to Azure or when an application is simply a single-\", \"container application, Azure App Service provides a great way to provide scalable singlecontainer -b\", \"ased services. Using Azure App Service is simple. It provides great integration with Git to make it \", \"easy to take your code, build it in Visual Studio, and deploy it directly to Azure.\\n\\nPublish\\n\\nSelect\", \" existing or cre\\n\\nTarget\\n\\nSpecific target\\n\\nApp Service\\n\\nContainer Registry\\n\\nAPI Management\\n\\nFigure 4\", \" -4. Publishing a single -container application to Azure App Service from Visual Studio 2022\\n\\n<!-- i\", \"mage -->\\n\\nWithout Docker, if you needed other capabilities, frameworks, or dependencies that aren't \", \"supported in Azure App Service, you had to wait until the Azure team updated those dependencies in A\", \"pp Service. Or you had to switch to other services like Azure Cloud Services or VMs, where you had f\", \"urther control and you could install a required component or framework for your application.\\n\\nContai\", \"ner support in Visual Studio 2017 and later gives you the ability to include whatever you want in yo\", \"ur application environment, as shown in Figure 4-4. Since you're running it in a container, if you a\", \"dd a dependency to your application, you can include the dependency in your Dockerfile or Docker ima\", \"ge.\\n\\nAs also shown in Figure 4-4, the publish flow pushes an image through a container registry. Thi\", \"s can be the Azure Container Registry (a registry close to your deployments in Azure and secured by \", \"Azure Active Directory groups and accounts), or any other Docker registry, like Docker Hub or an onp\", \"remises registry.\\n\\n## Manage state and data in Docker applications\\n\\nIn most cases, you can think of \", \"a container as an instance of a process. A process doesn't maintain persistent state. While a contai\", \"ner can write to its local storage, assuming that an instance will be around indefinitely would be l\", \"ike assuming that a single location in memory will be durable. You\\n\\nAzure Container Registry\\n\\nCreate\", \" new\\n\\nshould assume that container images, like processes, have multiple instances or will eventuall\", \"y be killed. If they're managed with a container orchestrator, you should assume that they might get\", \" moved from one node or VM to another.\\n\\nThe following solutions are used to manage data in Docker ap\", \"plications:\\n\\nFrom the Docker host, as Docker Volumes:\\n\\n- Volumes are stored in an area of the host f\", \"ilesystem that's managed by Docker.\\n- Bind mounts can map to any folder in the host filesystem, so a\", \"ccess can't be controlled from Docker process and can pose a security risk as a container could acce\", \"ss sensitive OS folders.\\n- tmpfs mounts are like virtual folders that only exist in the host's memor\", \"y and are never written to the filesystem.\\n\\n## From remote storage:\\n\\n- Azure Storage, which provides\", \" geo-distributable storage, providing a good long-term persistence solution for containers.\\n- Remote\", \" relational databases like Azure SQL Database or NoSQL databases like Azure Cosmos DB, or cache serv\", \"ices like Redis .\\n\\n## From the Docker container:\\n\\n- Overlay File System. This Docker feature impleme\", \"nts a copy-on-write task that stores updated information to the root file system of the container. T\", \"hat information is \\\"on top\\\" of the original image on which the container is based. If the container \", \"is deleted from the system, those changes are lost. Therefore, while it's possible to save the state\", \" of a container within its local storage, designing a system around this would conflict with the pre\", \"mise of container design, which by default is stateless.\\n\\nHowever, using Docker Volumes is now the p\", \"referred way to handle local data in Docker. If you need more information about storage in container\", \"s check on Docker storage drivers and About storage drivers .\\n\\nThe following provides more detail ab\", \"out these options:\\n\\nVolumes are directories mapped from the host OS to directories in containers. Wh\", \"en code in the container has access to the directory, that access is actually to a directory on the \", \"host OS. This directory is not tied to the lifetime of the container itself, and the directory is ma\", \"naged by Docker and isolated from the core functionality of the host machine. Thus, data volumes are\", \" designed to persist data independently of the life of the container. If you delete a container or a\", \"n image from the Docker host, the data persisted in the data volume isn't deleted.\\n\\nVolumes can be n\", \"amed or anonymous (the default). Named volumes are the evolution of Data Volume Containers and make \", \"it easy to share data between containers. Volumes also support volume drivers that allow you to stor\", \"e data on remote hosts, among other options.\\n\\nBind mounts are available since a long time ago and al\", \"low the mapping of any folder to a mount point in a container. Bind mounts have more limitations tha\", \"n volumes and some important security issues, so volumes are the recommended option.\\n\\nBrowser or \\u00b0\\n\\n\", \"Client app\\n\\nData Volume and Data Volume Container\\n\\nMicrosoft\\n\\nAzure tmpfs mounts are basically virtu\", \"al folders that live only in the host's memory and are never written to the filesystem. They are fas\", \"t and secure but use memory and are only meant for temporary, nonpersistent data.\\n\\nAs shown in Figur\", \"e 4-5, regular Docker volumes can be stored outside of the containers themselves but within the phys\", \"ical boundaries of the host server or VM. However, Docker containers can't access a volume from one \", \"host server or VM to another. In other words, with these volumes, it isn't possible to manage data s\", \"hared between containers that run on different Docker hosts, although it could be achieved with a vo\", \"lume driver that supports remote hosts.\\n\\nData Volume Container\\n\\n## Stateless container\\n\\nFigure 4 -5.\", \" Volumes and external data sources for container-based applications\\n\\n<!-- image -->\\n\\nVolumes can be \", \"shared between containers, but only in the same host, unless you use a remote driver that supports r\", \"emote hosts. In addition, when Docker containers are managed by an orchestrator, containers might \\\"m\", \"ove\\\" between hosts, depending on the optimizations performed by the cluster. Therefore, it isn't rec\", \"ommended that you use data volumes for business data. But they're a good mechanism to work with trac\", \"e files, temporal files, or similar that will not impact business data consistency.\\n\\nRemote data sou\", \"rces and cache tools like Azure SQL Database, Azure Cosmos DB, or a remote cache like Redis can be u\", \"sed in containerized applications the same way they are used when developing without containers. Thi\", \"s is a proven way to store business application data.\\n\\nAzure Storage. Business data usually will nee\", \"d to be placed in external resources or databases, like Azure Storage. Azure Storage, in concrete, p\", \"rovides the following services in the cloud:\\n\\n- Blob storage stores unstructured object data. A blob\", \" can be any type of text or binary data, such as document or media files (images, audio, and video f\", \"iles). Blob storage is also referred to as Object storage.\\n\\n- File storage offers shared storage for\", \" legacy applications using standard SMB protocol. Azure virtual machines and cloud services can shar\", \"e file data across application components via mounted shares. On -premises applications can access f\", \"ile data in a share via the File service REST API.\\n- Table storage stores structured datasets. Table\", \" storage is a NoSQL key-attribute data store, which allows rapid development and fast access to larg\", \"e quantities of data.\\n\\nRelational databases and NoSQL databases. There are many choices for external\", \" databases, from relational databases like SQL Server, PostgreSQL, Oracle, or NoSQL databases like A\", \"zure Cosmos DB, MongoDB, etc. These databases are not going to be explained as part of this guide si\", \"nce they are in a completely different subject.\\n\\n## Service -oriented architecture\\n\\nService -oriente\", \"d architecture (SOA) was an overused term and has meant different things to different people. But as\", \" a common denominator, SOA means that you structure your application by decomposing it into multiple\", \" services (most commonly as HTTP services) that can be classified as different types like subsystems\", \" or tiers.\\n\\nThose services can now be deployed as Docker containers, which solves deployment issues,\", \" because all the dependencies are included in the container image. However, when you need to scale u\", \"p SOA applications, you might have scalability and availability challenges if you're deploying based\", \" on single Docker hosts. This is where Docker clustering software or an orchestrator can help you, a\", \"s explained in later sections where deployment approaches for microservices are described.\\n\\nDocker c\", \"ontainers are useful (but not required) for both traditional service-oriented architectures and the \", \"more advanced microservices architectures.\\n\\nMicroservices derive from SOA, but SOA is different from\", \" microservices architecture. Features like large central brokers, central orchestrators at the organ\", \"ization level, and the Enterprise Service Bus (ESB) are typical in SOA. But in most cases, these are\", \" anti-patterns in the microservice community. In fact, some people argue that \\\"The microservice arch\", \"itecture is SOA done right.\\\"\\n\\nThis guide focuses on microservices, because a SOA approach is less pr\", \"escriptive than the requirements and techniques used in a microservice architecture. If you know how\", \" to build a microservice -based application, you also know how to build a simpler service-oriented a\", \"pplication.\\n\\n## Microservices architecture\\n\\nAs the name implies, a microservices architecture is an \", \"approach to building a server application as a set of small services. That means a microservices arc\", \"hitecture is mainly oriented to the back-end, although the approach is also being used for the front\", \" end. Each service runs in its own process and communicates with other processes using protocols suc\", \"h as HTTP/HTTPS, WebSockets, or AMQP . Each microservice implements a specific end-to-end domain or \", \"business capability within a certain context boundary, and each must be developed autonomously and b\", \"e deployable independently. Finally, each microservice should own its related domain data model and \", \"domain logic (sovereignty\\n\\nMonolithic deployment approach\\n\\n\\u2022 A traditional application has most of i\", \"ts functionality within a\\n\\nfew processes that are libraries.\\n\\nMicroservices application approach\\n\\n\\u2022 \", \"A microservice application segregates functionality into\\n\\nseparate smaller services.\\n\\nand decentrali\", \"zed data management) and could be based on different data storage technologies (SQL, NoSQL) and diff\", \"erent programming languages.\\n\\nWhat size should a microservice be? When developing a microservice, si\", \"ze shouldn't be the important point. Instead, the important point should be to create loosely couple\", \"d services so you have autonomy of development, deployment, and scale, for each service. Of course, \", \"when identifying and designing microservices, you should try to make them as small as possible as lo\", \"ng as you don't have too many direct dependencies with other microservices. More important than the \", \"size of the microservice is the internal cohesion it must have and its independence from other servi\", \"ces.\\n\\nWhy a microservices architecture? In short, it provides long-term agility. Microservices enabl\", \"e better maintainability in complex, large, and highly-scalable systems by letting you create applic\", \"ations based on many independently deployable services that each have granular and autonomous lifecy\", \"cles.\\n\\nAs an additional benefit, microservices can scale out independently. Instead of having a sing\", \"le monolithic application that you must scale out as a unit, you can instead scale out specific micr\", \"oservices. That way, you can scale just the functional area that needs more processing power or netw\", \"ork bandwidth to support demand, rather than scaling out other areas of the application that don't n\", \"eed to be scaled. That means cost savings because you need less hardware.\\n\\nFigure 4 -6. Monolithic d\", \"eployment versus the microservices approach\\n\\n<!-- image -->\\n\\nAs Figure 4-6 shows, in the traditional\", \" monolithic approach, the application scales by cloning the whole app in several servers/VM. In the \", \"microservices approach, functionality is segregated in smaller services, so each service can scale i\", \"ndependently. The microservices approach allows agile changes and rapid iteration of each microservi\", \"ce, because you can change specific, small areas of complex, large, and scalable applications.\\n\\nArch\", \"itecting fine-grained microservices-based applications enables continuous integration and continuous\", \" delivery practices. It also accelerates delivery of new functions into the application. Finegrained\", \" composition of applications also allows you to run and test microservices in isolation, and to\\n\\nApp\", \" 1\\n\\nApp 2\\n\\nApp 1\\n\\nevolve them autonomously while maintaining clear contracts between them. As long a\", \"s you don't change the interfaces or contracts, you can change the internal implementation of any mi\", \"croservice or add new functionality without breaking other microservices.\\n\\nThe following are importa\", \"nt aspects to enable success in going into production with a microservicesbased system:\\n\\n- Monitorin\", \"g and health checks of the services and infrastructure.\\n- Scalable infrastructure for the services (\", \"that is, cloud and orchestrators).\\n- Security design and implementation at multiple levels: authenti\", \"cation, authorization, secrets management, secure communication, etc.\\n- Rapid application delivery, \", \"usually with different teams focusing on different microservices.\\n- DevOps and CI/CD practices and i\", \"nfrastructure.\\n\\nOf these, only the first three are covered or introduced in this guide. The last two\", \" points, which are related to application lifecycle, are covered in the additional Containerized Doc\", \"ker Application Lifecycle with Microsoft Platform and Tools e-book.\\n\\n## Additional resources\\n\\n- Mark\", \" Russinovich. Microservices: An application revolution powered by the cloud https://azure.microsoft.\", \"com/blog/microservices-an-application-revolution-powered-by-thecloud/\\n- Martin Fowler. Microservices\", \" https://www.martinfowler.com/articles/microservices.html\\n- Martin Fowler. Microservice Prerequisite\", \"s https://martinfowler.com/bliki/MicroservicePrerequisites.html\\n- Jimmy Nilsson. Chunk Cloud Computi\", \"ng https://www.infoq.com/articles/CCC-Jimmy-Nilsson\\n- Cesar de la Torre. Containerized Docker Applic\", \"ation Lifecycle with Microsoft Platform and Tools (downloadable e-book) https://aka.ms/dockerlifecyc\", \"leebook\\n\\n## Data sovereignty per microservice\\n\\nAn important rule for microservices architecture is t\", \"hat each microservice must own its domain data and logic. Just as a full application owns its logic \", \"and data, so must each microservice own its logic and data under an autonomous lifecycle, with indep\", \"endent deployment per microservice.\\n\\nThis means that the conceptual model of the domain will differ \", \"between subsystems or microservices. Consider enterprise applications, where customer relationship m\", \"anagement (CRM) applications,\\n\\nData in Traditional approach\\n\\n: Single monolithic database\\n\\nTiers of \", \"specific technologies\\n\\nWeb Tier\\n\\nServices Tier\\n\\nCache Tier\\n\\nData Tier\\n\\nData in Microservices approac\", \"h\\n\\n\\u2022 Graph of interconnected microservices\\n\\nState typically scoped to the microservice transactional\", \" purchase subsystems, and customer support subsystems each call on unique customer entity attributes\", \" and data, and where each employs a different Bounded Context (BC).\\n\\nMy Web Application 1 =\\n\\nThis pr\", \"inciple is similar in Domain-driven design (DDD), where each Bounded Context or autonomous subsystem\", \" or service must own its domain model (data plus logic and behavior). Each DDD Bounded Context corre\", \"lates to one business microservice (one or several services). This point about the Bounded Context p\", \"attern is expanded in the next section. services\\n\\nOn the other hand, the traditional (monolithic dat\", \"a) approach used in many applications is to have a single centralized database or just a few databas\", \"es. This is often a normalized SQL database that's used for the whole application and all its intern\", \"al subsystems, as shown in Figure 4-7.\\n\\nNo-SQL\\n\\nMonolithic Databases are shared across services.\\n\\n##\", \" , SQL\\n\\nStateless services with\\n\\nseparate store\\n\\n<!-- image -->\\n\\nEach microservice owns its model/da\", \"ta!\\n\\nFigure 4 -7. Data sovereignty comparison: monolithic database versus microservices\\n\\n<!-- image \", \"-->\\n\\nIn the traditional approach, there's a single database shared across all services, typically in\", \" a tiered architecture. In the microservices approach, each microservice owns its model/data. The ce\", \"ntralized database approach initially looks simpler and seems to enable reuse of entities in differe\", \"nt subsystems to make everything consistent. But the reality is you end up with huge tables that ser\", \"ve many different subsystems, and that include attributes and columns that aren't needed in most cas\", \"es. It's like trying to use the same physical map for hiking a short trail, taking a day-long car tr\", \"ip, and learning geography.\\n\\nA monolithic application with typically a single relational database ha\", \"s two important benefits: ACID transactions and the SQL language, both working across all the tables\", \" and data related to your application. This approach provides a way to easily write a query that com\", \"bines data from multiple tables.\\n\\nHowever, data access becomes much more complicated when you move t\", \"o a microservices architecture. Even when using ACID transactions within a microservice or Bounded C\", \"ontext, it is crucial to consider that the data owned by each microservice is private to that micros\", \"ervice and should only services\\n\\nbe accessed either synchronously through its API endpoints(REST, gR\", \"PC, SOAP, etc) or asynchronously via messaging(AMQP or similar).\\n\\nEncapsulating the data ensures tha\", \"t the microservices are loosely coupled and can evolve independently of one another. If multiple ser\", \"vices were accessing the same data, schema updates would require coordinated updates to all the serv\", \"ices. This would break the microservice lifecycle autonomy. But distributed data structures mean tha\", \"t you can't make a single ACID transaction across microservices. This in turn means you must use eve\", \"ntual consistency when a business process spans multiple microservices. This is much harder to imple\", \"ment than simple SQL joins, because you can't create integrity constraints or use distributed transa\", \"ctions between separate databases, as we'll explain later on. Similarly, many other relational datab\", \"ase features aren't available across multiple microservices.\\n\\nGoing even further, different microser\", \"vices often use different kinds of databases. Modern applications store and process diverse kinds of\", \" data, and a relational database isn't always the best choice. For some use cases, a NoSQL database \", \"such as Azure CosmosDB or MongoDB might have a more convenient data model and offer better performan\", \"ce and scalability than a SQL database like SQL Server or Azure SQL Database. In other cases, a rela\", \"tional database is still the best approach. Therefore, microservices -based applications often use a\", \" mixture of SQL and NoSQL databases, which is sometimes called the polyglot persistence approach.\\n\\nA\", \" partitioned, polyglot-persistent architecture for data storage has many benefits. These include loo\", \"sely coupled services and better performance, scalability, costs, and manageability. However, it can\", \" introduce some distributed data management challenges, as explained in \\\"Identifying domain-model bo\", \"undaries\\\" later in this chapter.\\n\\n## The relationship between microservices and the Bounded Context \", \"pattern\\n\\nThe concept of microservice derives from the Bounded Context (BC) pattern in domain-driven \", \"design (DDD). DDD deals with large models by dividing them into multiple BCs and being explicit abou\", \"t their boundaries. Each BC must have its own model and database; likewise, each microservice owns i\", \"ts related data. In addition, each BC usually has its own ubiquitous language to help communication \", \"between software developers and domain experts.\\n\\nThose terms (mainly domain entities) in the ubiquit\", \"ous language can have different names in different Bounded Contexts, even when different domain enti\", \"ties share the same identity (that is, the unique ID that's used to read the entity from storage). F\", \"or instance, in a user-profile Bounded Context, the User domain entity might share identity with the\", \" Buyer domain entity in the ordering Bounded Context.\\n\\nA microservice is therefore like a Bounded Co\", \"ntext, but it also specifies that it's a distributed service. It's built as a separate process for e\", \"ach Bounded Context, and it must use the distributed protocols noted earlier, like HTTP/HTTPS, WebSo\", \"ckets, or AMQP. The Bounded Context pattern, however, doesn't specify whether the Bounded Context is\", \" a distributed service or if it's simply a logical boundary (such as a generic subsystem) within a m\", \"onolithic-deployment application.\\n\\nIt's important to highlight that defining a service for each Boun\", \"ded Context is a good place to start. But you don't have to constrain your design to it. Sometimes y\", \"ou must design a Bounded Context or\\n\\nbusiness microservice composed of several physical services. Bu\", \"t ultimately, both patterns -Bounded Context and microservice -are closely related.\\n\\nDDD benefits fr\", \"om microservices by getting real boundaries in the form of distributed microservices. But ideas like\", \" not sharing the model between microservices are what you also want in a Bounded Context.\\n\\n## Additi\", \"onal resources\\n\\n- Chris Richardson. Pattern: Database per service https://microservices.io/patterns/\", \"data/database-per-service.html\\n- Martin Fowler. BoundedContext https://martinfowler.com/bliki/Bounde\", \"dContext.html\\n- Martin Fowler. PolyglotPersistence https://martinfowler.com/bliki/PolyglotPersistenc\", \"e.html\\n- Alberto Brandolini. Strategic Domain Driven Design with Context Mapping https://www.infoq.c\", \"om/articles/ddd-contextmapping\\n\\n## Logical architecture versus physical architecture\\n\\nIt's useful at\", \" this point to stop and discuss the distinction between logical architecture and physical architectu\", \"re, and how this applies to the design of microservice-based applications.\\n\\nTo begin, building micro\", \"services doesn't require the use of any specific technology. For instance, Docker containers aren't \", \"mandatory to create a microservice-based architecture. Those microservices could also be run as plai\", \"n processes. Microservices is a logical architecture.\\n\\nMoreover, even when a microservice could be p\", \"hysically implemented as a single service, process, or container (for simplicity's sake, that's the \", \"approach taken in the initial version of eShopOnContainers), this parity between business microservi\", \"ce and physical service or container isn't necessarily required in all cases when you build a large \", \"and complex application composed of many dozens or even hundreds of services.\\n\\nThis is where there's\", \" a difference between an application's logical architecture and physical architecture. The logical a\", \"rchitecture and logical boundaries of a system do not necessarily map oneto -one to the physical or \", \"deployment architecture. It can happen, but it often doesn't.\\n\\nAlthough you might have identified ce\", \"rtain business microservices or Bounded Contexts, it doesn't mean that the best way to implement the\", \"m is always by creating a single service (such as an ASP.NET Web API) or single Docker container for\", \" each business microservice. Having a rule saying each business microservice has to be implemented u\", \"sing a single service or container is too rigid.\\n\\nTherefore, a business microservice or Bounded Cont\", \"ext is a logical architecture that might coincide (or not) with physical architecture. The important\", \" point is that a business microservice or Bounded Context must be autonomous by allowing code and st\", \"ate to be independently versioned, deployed, and scaled.\\n\\n\\u2022 -\\n\\n-\\n\\nCatalog business microservice\\n\\nWeb\", \" API\\n\\nAs Figure 4-8 shows, the catalog business microservice could be composed of several services o\", \"r processes. These could be multiple ASP.NET Web API services or any other kind of services using HT\", \"TP or any other protocol. More importantly, the services could share the same data, as long as these\", \" services are cohesive with respect to the same business domain.\\n\\nSearch\\n\\nService\\n\\nFigure 4 -8. Busi\", \"ness microservice with several physical services\\n\\n<!-- image -->\\n\\nThe services in the example share \", \"the same data model because the Web API service targets the same data as the Search service. So, in \", \"the physical implementation of the business microservice, you're splitting that functionality so you\", \" can scale each of those internal services up or down as needed. Maybe the Web API service usually n\", \"eeds more instances than the Search service, or vice versa.\\n\\nIn short, the logical architecture of m\", \"icroservices doesn't always have to coincide with the physical deployment architecture. In this guid\", \"e, whenever we mention a microservice, we mean a business or logical microservice that could map to \", \"one or more (physical) services. In most cases, this will be a single service, but it might be more.\", \"\\n\\n## Challenges and solutions for distributed data management\\n\\n## Challenge #1: How to define the bo\", \"undaries of each microservice\\n\\nDefining microservice boundaries is probably the first challenge anyo\", \"ne encounters. Each microservice has to be a piece of your application and each microservice should \", \"be autonomous with all the benefits and challenges that it conveys. But how do you identify those bo\", \"undaries?\\n\\nFirst, you need to focus on the application's logical domain models and related data. Try\", \" to identify decoupled islands of data and different contexts within the same application. Each cont\", \"ext could have a different business language (different business terms). The contexts should be defi\", \"ned and managed independently. The terms and entities that are used in those different contexts migh\", \"t sound similar, but you might discover that in a particular context, a business concept with one is\", \" used for a different\\n\\npurpose in another context, and might even have a different name. For instanc\", \"e, a user can be referred as a user in the identity or membership context, as a customer in a CRM co\", \"ntext, as a buyer in an ordering context, and so forth.\\n\\nThe way you identify boundaries between mul\", \"tiple application contexts with a different domain for each context is exactly how you can identify \", \"the boundaries for each business microservice and its related domain model and data. You always atte\", \"mpt to minimize the coupling between those microservices. This guide goes into more detail about thi\", \"s identification and domain model design in the section Identifying domain-model boundaries for each\", \" microservice later.\\n\\n## Challenge #2: How to create queries that retrieve data from several microse\", \"rvices\\n\\nA second challenge is how to implement queries that retrieve data from several microservices\", \", while avoiding chatty communication to the microservices from remote client apps. An example could\", \" be a single screen from a mobile app that needs to show user information that's owned by the basket\", \", catalog, and user identity microservices. Another example would be a complex report involving many\", \" tables located in multiple microservices. The right solution depends on the complexity of the queri\", \"es. But in any case, you'll need a way to aggregate information if you want to improve the efficienc\", \"y in the communications of your system. The most popular solutions are the following.\\n\\nAPI Gateway. \", \"For simple data aggregation from multiple microservices that own different databases, the recommende\", \"d approach is an aggregation microservice referred to as an API Gateway. However, you need to be car\", \"eful about implementing this pattern, because it can be a choke point in your system, and it can vio\", \"late the principle of microservice autonomy. To mitigate this possibility, you can have multiple fin\", \"ed-grained API Gateways each one focusing on a vertical \\\"slice\\\" or business area of the system. The \", \"API Gateway pattern is explained in more detail in the API Gateway section later.\\n\\nGraphQL Federatio\", \"n One option to consider if your microservices are already using GraphQL is GraphQL Federation. Fede\", \"ration allows you to define \\\"subgraphs\\\" from other services and compose them into an aggregate \\\"supe\", \"rgraph\\\" that acts as a standalone schema.\\n\\nCQRS with query/reads tables. Another solution for aggreg\", \"ating data from multiple microservices is the Materialized View pattern. In this approach, you gener\", \"ate, in advance (prepare denormalized data before the actual queries happen), a read-only table with\", \" the data that's owned by multiple microservices. The table has a format suited to the client app's \", \"needs.\\n\\nConsider something like the screen for a mobile app. If you have a single database, you migh\", \"t pull together the data for that screen using a SQL query that performs a complex join involving mu\", \"ltiple tables. However, when you have multiple databases, and each database is owned by a different \", \"microservice, you cannot query those databases and create a SQL join. Your complex query becomes a c\", \"hallenge. You can address the requirement using a CQRS approach\\u2014you create a denormalized table in a\", \" different database that's used just for queries. The table can be designed specifically for the dat\", \"a you need for the complex query, with a one-to-one relationship between fields needed by your appli\", \"cation's screen and the columns in the query table. It could also serve for reporting purposes.\\n\\nThi\", \"s approach not only solves the original problem (how to query and join across microservices), but it\", \" also improves performance considerably when compared with a complex join, because you already\\n\\nhave\", \" the data that the application needs in the query table. Of course, using Command and Query Responsi\", \"bility Segregation (CQRS) with query/reads tables means additional development work, and you'll need\", \" to embrace eventual consistency. Nonetheless, requirements on performance and high scalability in c\", \"ollaborative scenarios (or competitive scenarios, depending on the point of view) are where you shou\", \"ld apply CQRS with multiple databases.\\n\\n\\\"Cold data\\\" in central databases. For complex reports and qu\", \"eries that might not require real-time data, a common approach is to export your \\\"hot data\\\" (transac\", \"tional data from the microservices) as \\\"cold data\\\" into large databases that are used only for repor\", \"ting. That central database system can be a Big Data-based system, like Hadoop; a data warehouse lik\", \"e one based on Azure SQL Data Warehouse; or even a single SQL database that's used just for reports \", \"(if size won't be an issue).\\n\\nKeep in mind that this centralized database would be used only for que\", \"ries and reports that do not need real -time data. The original updates and transactions, as your so\", \"urce of truth, have to be in your microservices data. The way you would synchronize data would be ei\", \"ther by using event-driven communication (covered in the next sections) or by using other database i\", \"nfrastructure import/export tools. If you use event-driven communication, that integration process w\", \"ould be similar to the way you propagate data as described earlier for CQRS query tables.\\n\\nHowever, \", \"if your application design involves constantly aggregating information from multiple microservices f\", \"or complex queries, it might be a symptom of a bad design -a microservice should be as isolated as p\", \"ossible from other microservices. (This excludes reports/analytics that always should use cold -data\", \" central databases.) Having this problem often might be a reason to merge microservices. You need to\", \" balance the autonomy of evolution and deployment of each microservice with strong dependencies, coh\", \"esion, and data aggregation.\\n\\n## Challenge #3: How to achieve consistency across multiple microservi\", \"ces\\n\\nAs stated previously, the data owned by each microservice is private to that microservice and c\", \"an only be accessed using its microservice API. Therefore, a challenge presented is how to implement\", \" end-toend business processes while keeping consistency across multiple microservices.\\n\\nTo analyze t\", \"his problem, let's look at an example from the eShopOnContainers reference application . The Catalog\", \" microservice maintains information about all the products, including the product price. The Basket \", \"microservice manages temporal data about product items that users are adding to their shopping baske\", \"ts, which includes the price of the items at the time they were added to the basket. When a product'\", \"s price is updated in the catalog, that price should also be updated in the active baskets that hold\", \" that same product, plus the system should probably warn the user saying that a particular item's pr\", \"ice has changed since they added it to their basket.\\n\\nIn a hypothetical monolithic version of this a\", \"pplication, when the price changes in the products table, the catalog subsystem could simply use an \", \"ACID transaction to update the current price in the Basket table.\\n\\nHowever, in a microservices-based\", \" application, the Product and Basket tables are owned by their respective microservices. No microser\", \"vice should ever include tables/storage owned by another microservice in its own transactions, not e\", \"ven in direct queries, as shown in Figure 4-9.\\n\\nCatalog microservice\\n\\nCatalog.APl\\n\\nID\\n\\nProdPrice in \", \"Catalog DB\\n\\nEventual consistency\\n\\nBasket microservice\\n\\nBasket.API\\n\\nFigure 4 -9. A microservice can't\", \" directly access a table in another microservice\\n\\n<!-- image -->\\n\\nThe Catalog microservice shouldn't\", \" update the Basket table directly, because the Basket table is owned by the Basket microservice. To \", \"make an update to the Basket microservice, the Catalog microservice should use eventual consistency \", \"probably based on asynchronous communication such as integration events (message and event-based com\", \"munication). This is how the eShopOnContainers reference application performs this type of consisten\", \"cy across microservices.\\n\\nAs stated by the CAP theorem, you need to choose between availability and \", \"ACID strong consistency. Most microservice -based scenarios demand availability and high scalability\", \" as opposed to strong consistency. Mission-critical applications must remain up and running, and dev\", \"elopers can work around strong consistency by using techniques for working with weak or eventual con\", \"sistency. This is the approach taken by most microservice-based architectures.\\n\\nMoreover, ACID-style\", \" or two-phase commit transactions are not just against microservices principles; most NoSQL database\", \"s (like Azure Cosmos DB, MongoDB, etc.) do not support two-phase commit transactions, typical in dis\", \"tributed databases scenarios. However, maintaining data consistency across services and databases is\", \" essential. This challenge is also related to the question of how to propagate changes across multip\", \"le microservices when certain data needs to be redundant\\u2014for example, when you need to have the prod\", \"uct's name or description in the Catalog microservice and the Basket microservice.\\n\\nA good solution \", \"for this problem is to use eventual consistency between microservices articulated through event-driv\", \"en communication and a publish-and-subscribe system. These topics are covered in the section Asynchr\", \"onous event-driven communication later in this guide.\\n\\n## Challenge #4: How to design communication \", \"across microservice boundaries\\n\\nCommunicating across microservice boundaries is a real challenge. In\", \" this context, communication doesn't refer to what protocol you should use (HTTP and REST, AMQP, mes\", \"saging, and so on). Instead, it addresses what communication style you should use, and especially ho\", \"w coupled your microservices should be. Depending on the level of coupling, when failure occurs, the\", \" impact of that failure on your system will vary significantly.\\n\\nIn a distributed system like a micr\", \"oservices-based application, with so many artifacts moving around and with distributed services acro\", \"ss many servers or hosts, components will eventually fail. Partial failure and even larger outages w\", \"ill occur, so you need to design your microservices and the communication across them considering th\", \"e common risks in this type of distributed system.\\n\\nA popular approach is to implement HTTP (REST)-b\", \"ased microservices, due to their simplicity. An HTTP -based approach is perfectly acceptable; the is\", \"sue here is related to how you use it. If you use HTTP requests and responses just to interact with \", \"your microservices from client applications or from API Gateways, that's fine. But if you create lon\", \"g chains of synchronous HTTP calls across microservices, communicating across their boundaries as if\", \" the microservices were objects in a monolithic application, your application will eventually run in\", \"to problems.\\n\\nFor instance, imagine that your client application makes an HTTP API call to an indivi\", \"dual microservice like the Ordering microservice. If the Ordering microservice in turn calls additio\", \"nal microservices using HTTP within the same request/response cycle, you're creating a chain of HTTP\", \" calls. It might sound reasonable initially. However, there are important points to consider when go\", \"ing down this path:\\n\\n- Blocking and low performance. Due to the synchronous nature of HTTP, the orig\", \"inal request doesn't get a response until all the internal HTTP calls are finished. Imagine if the n\", \"umber of these calls increases significantly and at the same time one of the intermediate HTTP calls\", \" to a microservice is blocked. The result is that performance is impacted, and the overall scalabili\", \"ty will be exponentially affected as additional HTTP requests increase.\\n- Coupling microservices wit\", \"h HTTP. Business microservices shouldn't be coupled with other business microservices. Ideally, they\", \" shouldn't \\\"know\\\" about the existence of other microservices. If your application relies on coupling\", \" microservices as in the example, achieving autonomy per microservice will be almost impossible.\\n- F\", \"ailure in any one microservice. If you implemented a chain of microservices linked by HTTP calls, wh\", \"en any of the microservices fails (and eventually they will fail) the whole chain of microservices w\", \"ill fail. A microservice -based system should be designed to continue to work as well as possible du\", \"ring partial failures. Even if you implement client logic that uses retries with exponential backoff\", \" or circuit breaker mechanisms, the more complex the HTTP call chains are, the more complex it is to\", \" implement a failure strategy based on HTTP.\\n\\nIn fact, if your internal microservices are communicat\", \"ing by creating chains of HTTP requests as described, it could be argued that you have a monolithic \", \"application, but one based on HTTP between processes instead of intra-process communication mechanis\", \"ms .\\n\\nTherefore, in order to enforce microservice autonomy and have better resiliency, you should mi\", \"nimize the use of chains of request/response communication across microservices. It's recommended th\", \"at you use only asynchronous interaction for inter-microservice communication, either by using async\", \"hronous message- and event-based communication, or by using (asynchronous) HTTP polling independentl\", \"y of the original HTTP request/response cycle.\\n\\nThe use of asynchronous communication is explained w\", \"ith additional details later in this guide in the sections Asynchronous microservice integration enf\", \"orces microservice's autonomy and Asynchronous message -based communication .\\n\\n## Additional resourc\", \"es\\n\\n- CAP theorem https://en.wikipedia.org/wiki/CAP\\\\_theorem\\n- Eventual consistency https://en.wikip\", \"edia.org/wiki/Eventual\\\\_consistency\\n- Data Consistency Primer https://learn.microsoft.com/previous-v\", \"ersions/msp-n-p/dn589800(v=pandp.10)\\n- Martin Fowler. CQRS (Command and Query Responsibility Segrega\", \"tion) https://martinfowler.com/bliki/CQRS.html\\n- Materialized View https://learn.microsoft.com/azure\", \"/architecture/patterns/materialized-view\\n- Charles Row. ACID vs. BASE: The Shifting pH of Database T\", \"ransaction Processing https://www.dataversity.net/acid-vs-base-the-shifting-ph-of-database-transacti\", \"onprocessing/\\n- Compensating Transaction https://learn.microsoft.com/azure/architecture/patterns/com\", \"pensating-transaction\\n- Udi Dahan. Service Oriented Composition https://udidahan.com/2014/07/30/serv\", \"ice-oriented-composition-with-video/\\n\\n## Identify domain -model boundaries for each microservice\\n\\nTh\", \"e goal when identifying model boundaries and size for each microservice isn't to get to the most gra\", \"nular separation possible, although you should tend toward small microservices if possible. Instead,\", \" your goal should be to get to the most meaningful separation guided by your domain knowledge. The e\", \"mphasis isn't on the size, but instead on business capabilities. In addition, if there's clear cohes\", \"ion needed for a certain area of the application based on a high number of dependencies, that indica\", \"tes the need for a single microservice, too. Cohesion is a way to identify how to break apart\\n\\nor gr\", \"oup together microservices. Ultimately, while you gain more knowledge about the domain, you should a\", \"dapt the size of your microservice, iteratively. Finding the right size isn't a one-shot process.\\n\\nS\", \"am Newman, a recognized promoter of microservices and author of the book Building Microservices , hi\", \"ghlights that you should design your microservices based on the Bounded Context (BC) pattern (part o\", \"f domain-driven design), as introduced earlier. Sometimes, a BC could be composed of several physica\", \"l services, but not vice versa.\\n\\nA domain model with specific domain entities applies within a concr\", \"ete BC or microservice. A BC delimits the applicability of a domain model and gives developer team m\", \"embers a clear and shared understanding of what must be cohesive and what can be developed independe\", \"ntly. These are the same goals for microservices.\\n\\nAnother tool that informs your design choice is C\", \"onway's law, which states that an application will reflect the social boundaries of the organization\", \" that produced it. But sometimes the opposite is true the company's organization is formed by the so\", \"ftware. You might need to reverse Conway's law and build the boundaries the way you want the company\", \" to be organized, leaning toward business process consulting.\\n\\nTo identify bounded contexts, you can\", \" use a DDD pattern called the Context Mapping pattern. With Context Mapping, you identify the variou\", \"s contexts in the application and their boundaries. It's common to have a different context and boun\", \"dary for each small subsystem, for instance. The Context Map is a way to define and make explicit th\", \"ose boundaries between domains. A BC is autonomous and includes the details of a single domain -deta\", \"ils like the domain entities- and defines integration contracts with other BCs. This is similar to t\", \"he definition of a microservice: it's autonomous, it implements certain domain capability, and it mu\", \"st provide interfaces. This is why Context Mapping and the Bounded Context pattern are good approach\", \"es for identifying the domain model boundaries of your microservices.\\n\\nWhen designing a large applic\", \"ation, you'll see how its domain model can be fragmented - a domain expert from the catalog domain w\", \"ill name entities differently in the catalog and inventory domains than a shipping domain expert, fo\", \"r instance. Or the user domain entity might be different in size and number of attributes when deali\", \"ng with a CRM expert who wants to store every detail about the customer than for an ordering domain \", \"expert who just needs partial data about the customer. It's very hard to disambiguate all domain ter\", \"ms across all the domains related to a large application. But the most important thing is that you s\", \"houldn't try to unify the terms. Instead, accept the differences and richness provided by each domai\", \"n. If you try to have a unified database for the whole application, attempts at a unified vocabulary\", \" will be awkward and won't sound right to any of the multiple domain experts. Therefore, BCs (implem\", \"ented as microservices) will help you to clarify where you can use certain domain terms and where yo\", \"u'll need to split the system and create additional BCs with different domains.\\n\\nYou'll know that yo\", \"u got the right boundaries and sizes of each BC and domain model if you have few strong relationship\", \"s between domain models, and you do not usually need to merge information from multiple domain model\", \"s when performing typical application operations.\\n\\nPerhaps the best answer to the question of how la\", \"rge a domain model for each microservice should be is the following: it should have an autonomous BC\", \", as isolated as possible, that enables you to work without having to constantly switch to other con\", \"texts (other microservice's models). In Figure 4-\\n\\nIdentifying a Domain Model per Microservice or Bo\", \"unded Context\\n\\nConferences Management\\n\\nOrders and Registration\\n\\n10, you can see how multiple microse\", \"rvices (multiple BCs) each has their own model and how their entities can be defined, depending on t\", \"he specific requirements for each of the identified domains in your application.\\n\\nOrders\\n\\n## Attende\", \"es Seat\\n\\nassignments\\n\\n<!-- image -->\\n\\nUsers\\n\\nPricing and Marketing\\n\\nConferences\\n\\nPromotions\\n\\nSeats\\n\\n\", \"Figure 4 -10. Identifying entities and microservice model boundaries\\n\\nFigure 4-10 illustrates a samp\", \"le scenario related to an online conference management system. The same entity appears as \\\"Users\\\", \\\"\", \"Buyers\\\", \\\"Payers\\\", and \\\"Customers\\\" depending on the bounded context. You've identified several BCs t\", \"hat could be implemented as microservices, based on domains that domain experts defined for you. As \", \"you can see, there are entities that are present just in a single microservice model, like Payments \", \"in the Payment microservice. Those will be easy to implement.\\n\\nHowever, you might also have entities\", \" that have a different shape but share the same identity across the multiple domain models from the \", \"multiple microservices. For example, the User entity is identified in the Conferences Management mic\", \"roservice. That same user, with the same identity, is the one named Buyers in the Ordering microserv\", \"ice, or the one named Payer in the Payment microservice, and even the one named Customer in the Cust\", \"omer Service microservice. This is because, depending on the ubiquitous language that each domain ex\", \"pert is using, a user might have a different perspective even with different attributes. The user en\", \"tity in the microservice model named Conferences Management might have most of its personal data att\", \"ributes. However, that same user in the shape of Payer in the microservice Payment or in the shape o\", \"f Customer in the microservice Customer Service might not need the same list of attributes.\\n\\nA simil\", \"ar approach is illustrated in Figure 4-11.\\n\\nReservations\\n\\nDecomposing a traditional data model into \", \"multiple domain models\\n\\n(One domain model per microservice or Bounded-Context)\\n\\npublic class User\\n\\nC\", \"onferences\\n\\nManagement\\n\\n\\u2192 User\\n\\nStatus FirstName LastName\\n\\n## FirstName LastName ID\\n\\n<!-- image -->\\n\", \"\\nTraditional\\n\\nEntities with \\\"All\\n\\npossible\\\"\\n\\nattributes\\n\\nFigure 4 -11. Decomposing traditional data \", \"models into multiple domain models\\n\\nWhen decomposing a traditional data model between bounded contex\", \"ts, you can have different entities that share the same identity (a buyer is also a user) with diffe\", \"rent attributes in each bounded context. You can see how the user is present in the Conferences Mana\", \"gement microservice model as the User entity and is also present in the form of the Buyer entity in \", \"the Pricing microservice, with alternate attributes or details about the user when it's actually a b\", \"uyer. Each microservice or BC might not need all the data related to a User entity, just part of it,\", \" depending on the problem to solve or the context. For instance, in the Pricing microservice model, \", \"you do not need the address or the name of the user, just the ID (as identity) and Status, which wil\", \"l have an impact on discounts when pricing the seats per buyer.\\n\\nThe Seat entity has the same name b\", \"ut different attributes in each domain model. However, Seat shares identity based on the same ID, as\", \" happens with User and Buyer.\\n\\nBasically, there's a shared concept of a user that exists in multiple\", \" services (domains), which all share the identity of that user. But in each domain model there might\", \" be additional or different details about the user entity. Therefore, there needs to be a way to map\", \" a user entity from one domain (microservice) to another.\\n\\nThere are several benefits to not sharing\", \" the same user entity with the same number of attributes across domains. One benefit is to reduce du\", \"plication, so that microservice models do not have any data that they do not need. Another benefit i\", \"s having a primary microservice that owns a certain type of data per entity so that updates and quer\", \"ies for that type of data are driven only by that microservice.\\n\\nDirect Client-To-Microservice commu\", \"nication\\n\\nArchitecture\\n\\nMicroservices\\n\\n## The API gateway pattern versus the Direct client -to -micr\", \"oservice communication Microservice\\n\\nIn a microservices architecture, each microservice exposes a se\", \"t of (typically) fine-grained endpoints. This fact can impact the client-to-microservice communicati\", \"on, as explained in this section.\\n\\n## Direct client -to -microservice communication\\n\\nA possible appr\", \"oach is to use a direct client-to-microservice communication architecture. In this approach, a clien\", \"t app can make requests directly to some of the microservices, as shown in Figure 412.\\n\\nFigure 4 -12\", \". Using a direct client -to -microservice communication architecture\\n\\n<!-- image -->\\n\\nIn this approa\", \"ch, each microservice has a public endpoint, sometimes with a different TCP port for each microservi\", \"ce. An example of a URL for a particular service could be the following URL in Azure:\\n\\nhttp://eshopo\", \"ncontainers.westus.cloudapp.azure.com:88/\\n\\nIn a production environment based on a cluster, that URL \", \"would map to the load balancer used in the cluster, which in turn distributes the requests across th\", \"e microservices. In production environments, you could have an Application Delivery Controller (ADC)\", \" like Azure Application Gateway between your microservices and the Internet. This layer acts as a tr\", \"ansparent tier that not only performs load balancing, but secures your services by offering SSL term\", \"ination. This approach improves the load of your hosts by offloading CPU-intensive SSL termination a\", \"nd other routing duties to the Azure Application Gateway. In any case, a load balancer and ADC are t\", \"ransparent from a logical application architecture point of view.\\n\\nA direct client -to -microservice\", \" communication architecture could be good enough for a small microservice -based application, especi\", \"ally if the client app is a server-side web application like an ASP.NET MVC app. However, when you b\", \"uild large and complex microservice-based applications (for example, when handling dozens of microse\", \"rvice types), and especially when the client apps are remote mobile apps or SPA web applications, th\", \"at approach faces a few issues.\\n\\nClient Apps\\n\\nMobile\\n\\nApp\\n\\nWeb\\n\\nApp\\n\\nConsider the following question\", \"s when developing a large application based on microservices:\\n\\n- How can client apps minimize the nu\", \"mber of requests to the back end and reduce chatty communication to multiple microservices?\\n\\nInterac\", \"ting with multiple microservices to build a single UI screen increases the number of round trips acr\", \"oss the Internet. This approach increases latency and complexity on the UI side. Ideally, responses \", \"should be efficiently aggregated in the server side. This approach reduces latency, since multiple p\", \"ieces of data come back in parallel and some UI can show data as soon as it's ready.\\n\\n- How can you \", \"handle cross-cutting concerns such as authorization, data transformations, and dynamic request dispa\", \"tching?\\n\\nImplementing security and cross-cutting concerns like security and authorization on every m\", \"icroservice can require significant development effort. A possible approach is to have those service\", \"s within the Docker host or internal cluster to restrict direct access to them from the outside, and\", \" to implement those cross-cutting concerns in a centralized place, like an API Gateway.\\n\\n- How can c\", \"lient apps communicate with services that use non-Internet-friendly protocols?\\n\\nProtocols used on th\", \"e server side (like AMQP or binary protocols) are not supported in client apps. Therefore, requests \", \"must be performed through protocols like HTTP/HTTPS and translated to the other protocols afterwards\", \". A man-in-the-middle approach can help in this situation.\\n\\n- How can you shape a facade especially \", \"made for mobile apps?\\n\\nThe API of multiple microservices might not be well designed for the needs of\", \" different client applications. For instance, the needs of a mobile app might be different than the \", \"needs of a web app. For mobile apps, you might need to optimize even further so that data responses \", \"can be more efficient. You might do this functionality by aggregating data from multiple microservic\", \"es and returning a single set of data, and sometimes eliminating any data in the response that isn't\", \" needed by the mobile app. And, of course, you might compress that data. Again, a facade or API in b\", \"etween the mobile app and the microservices can be convenient for this scenario.\\n\\n## Why consider AP\", \"I Gateways instead of direct client-to-microservice communication\\n\\nIn a microservices architecture, \", \"the client apps usually need to consume functionality from more than one microservice. If that consu\", \"mption is performed directly, the client needs to handle multiple calls to microservice endpoints. W\", \"hat happens when the application evolves and new microservices are introduced or existing microservi\", \"ces are updated? If your application has many microservices, handling so many endpoints from the cli\", \"ent apps can be a nightmare. Since the client app would be coupled to those internal endpoints, evol\", \"ving the microservices in the future can cause high impact for the client apps.\\n\\nTherefore, having a\", \"n intermediate level or tier of indirection (Gateway) can be convenient for microservice -based appl\", \"ications. If you don't have API Gateways, the client apps must send requests directly to the microse\", \"rvices and that raises problems, such as the following issues:\\n\\n- Coupling: Without the API Gateway \", \"pattern, the client apps are coupled to the internal microservices. The client apps need to know how\", \" the multiple areas of the application are decomposed in microservices. When evolving and refactorin\", \"g the internal microservices, those actions impact maintenance because they cause breaking changes t\", \"o the client apps due to the direct reference to the internal microservices from the client apps. Cl\", \"ient apps need to be updated frequently, making the solution harder to evolve.\\n- Too many round trip\", \"s: A single page/screen in the client app might require several calls to multiple services. That app\", \"roach can result in multiple network round trips between the client and the server, adding significa\", \"nt latency. Aggregation handled in an intermediate level could improve the performance and user expe\", \"rience for the client app.\\n- Security issues: Without a gateway, all the microservices must be expos\", \"ed to the \\\"external world\\\", making the attack surface larger than if you hide internal microservices\", \" that aren't directly used by the client apps. The smaller the attack surface is, the more secure yo\", \"ur application can be.\\n- Cross -cutting concerns: Each publicly published microservice must handle c\", \"oncerns such as authorization and SSL. In many situations, those concerns could be handled in a sing\", \"le tier so the internal microservices are simplified.\\n\\n## What is the API Gateway pattern?\\n\\nWhen you\", \" design and build large or complex microservice-based applications with multiple client apps, a good\", \" approach to consider can be an API Gateway. This pattern is a service that provides a single-entry \", \"point for certain groups of microservices. It's similar to the Facade pattern from objectoriented de\", \"sign, but in this case, it's part of a distributed system. The API Gateway pattern is also sometimes\", \" known as the \\\"backend for frontend\\\" (BFF) because you build it while thinking about the needs of th\", \"e client app.\\n\\nTherefore, the API gateway sits between the client apps and the microservices. It act\", \"s as a reverse proxy, routing requests from clients to services. It can also provide other cross-cut\", \"ting features such as authentication, SSL termination, and cache.\\n\\nFigure 4-13 shows how a custom AP\", \"I Gateway can fit into a simplified microservice-based architecture with just a few microservices.\\n\\n\", \"Using a single custom API Gateway service\\n\\nClient mobile app\\n\\nJSON\\n\\nClient SPA Web app\\n\\nJavaScript/A\", \"ngularjs\\n\\nTraditional Web app\\n\\nBrowser +\\n\\nHTML\\n\\n## Microservice 1 Web API\\n\\nontaine\\n\\nFigure 4 -13. Us\", \"ing an API Gateway implemented as a custom service\\n\\n<!-- image -->\\n\\nApps connect to a single endpoin\", \"t, the API Gateway, that's configured to forward requests to individual microservices. In this examp\", \"le, the API Gateway would be implemented as a custom ASP.NET Core WebHost service running as a conta\", \"iner.\\n\\nIt's important to highlight that in that diagram, you would be using a single custom API Gate\", \"way service facing multiple and different client apps. That fact can be an important risk because yo\", \"ur API Gateway service will be growing and evolving based on many different requirements from the cl\", \"ient apps. Eventually, it will be bloated because of those different needs and effectively it could \", \"be similar to a monolithic application or monolithic service. That's why it's very much recommended \", \"to split the API Gateway in multiple services or multiple smaller API Gateways, one per client app f\", \"orm-factor type, for instance.\\n\\nYou need to be careful when implementing the API Gateway pattern. Us\", \"ually it isn't a good idea to have a single API Gateway aggregating all the internal microservices o\", \"f your application. If it does, it acts as a monolithic aggregator or orchestrator and violates micr\", \"oservice autonomy by coupling all the microservices.\\n\\nTherefore, the API Gateways should be segregat\", \"ed based on business boundaries and the client apps and not act as a single aggregator for all the i\", \"nternal microservices.\\n\\nWhen splitting the API Gateway tier into multiple API Gateways, if your appl\", \"ication has multiple client apps, that can be a primary pivot when identifying the multiple API Gate\", \"ways types, so that you can have a different facade for the needs of each client app. This case is a\", \" pattern named \\\"Backend for Frontend\\\" (BFF) where each API Gateway can provide a different API tailo\", \"red for each client app type, possibly even based on the client form factor by implementing specific\", \" adapter code which underneath calls multiple internal microservices, as shown in the following imag\", \"e:\\n\\n--\\n\\nBack end\\n\\n-\\n\\nClient mobile app\\n\\nJSON\\n\\nClient SPA Web app\\n\\nJavaScript/Angular.js\\n\\nTraditional\", \" Web app\\n\\nBrowser\\n\\nHTML\\n\\n+\\n\\ncontainer\\n\\nFigure 4 -13.1. Using multiple custom API Gateways\\n\\n<!-- imag\", \"e -->\\n\\nFigure 4-13.1 shows API Gateways that are segregated by client type; one for mobile clients a\", \"nd one for web clients. A traditional web app connects to an MVC microservice that uses the web API \", \"Gateway. The example depicts a simplified architecture with multiple fine-grained API Gateways. In t\", \"his case, the boundaries identified for each API Gateway are based purely on the \\\"Backend for Fronte\", \"nd\\\" (BFF) pattern, hence based just on the API needed per client app. But in larger applications you\", \" should also go further and create other API Gateways based on business boundaries as a second desig\", \"n pivot.\\n\\n## Main features in the API Gateway pattern\\n\\nAn API Gateway can offer multiple features. D\", \"epending on the product it might offer richer or simpler features, however, the most important and f\", \"oundational features for any API Gateway are the following design patterns:\\n\\nReverse proxy or gatewa\", \"y routing. The API Gateway offers a reverse proxy to redirect or route requests (layer 7 routing, us\", \"ually HTTP requests) to the endpoints of the internal microservices. The gateway provides a single e\", \"ndpoint or URL for the client apps and then internally maps the requests to a group of internal micr\", \"oservices. This routing feature helps to decouple the client apps from the microservices but it's al\", \"so convenient when modernizing a monolithic API by sitting the API Gateway in between the monolithic\", \" API and the client apps, then you can add new APIs as new microservices while still using the legac\", \"y monolithic API until it's split into many microservices in the future. Because of the API Gateway,\", \" the client apps won't notice if the APIs being used are implemented as internal microservices or a \", \"monolithic API and more importantly, when evolving and refactoring the monolithic API into microserv\", \"ices, thanks to the API Gateway routing, client apps won't be impacted with any URI change.\\n\\nUsing m\", \"ultiple API Gateways / BFF\\n\\n- \\u2014\\n\\nBack end\\n\\n## Microservice 1 Web AP API Gateway - Mobile\\n\\n| ASPNET C\", \"ore\\n\\nWebHost\\n\\nFor more information, see Gateway routing pattern .\\n\\nRequests aggregation. As part of \", \"the gateway pattern you can aggregate multiple client requests (usually HTTP requests) targeting mul\", \"tiple internal microservices into a single client request. This pattern is especially convenient whe\", \"n a client page/screen needs information from several microservices. With this approach, the client \", \"app sends a single request to the API Gateway that dispatches several requests to the internal micro\", \"services and then aggregates the results and sends everything back to the client app. The main benef\", \"it and goal of this design pattern is to reduce chattiness between the client apps and the backend A\", \"PI, which is especially important for remote apps out of the datacenter where the microservices live\", \", like mobile apps or requests coming from SPA apps that come from JavaScript in client remote brows\", \"ers. For regular web apps performing the requests in the server environment (like an ASP.NET Core MV\", \"C web app), this pattern is not so important as the latency is very much smaller than for remote cli\", \"ent apps.\\n\\nDepending on the API Gateway product you use, it might be able to perform this aggregatio\", \"n. However, in many cases it's more flexible to create aggregation microservices under the scope of \", \"the API Gateway, so you define the aggregation in code (that is, C# code):\\n\\nFor more information, se\", \"e Gateway aggregation pattern .\\n\\nCross -cutting concerns or gateway offloading. Depending on the fea\", \"tures offered by each API Gateway product, you can offload functionality from individual microservic\", \"es to the gateway, which simplifies the implementation of each microservice by consolidating cross-c\", \"utting concerns into one tier. This approach is especially convenient for specialized features that \", \"can be complex to implement properly in every internal microservice, such as the following functiona\", \"lity:\\n\\n- Authentication and authorization\\n- Service discovery integration\\n- Response caching\\n- Retry\", \" policies, circuit breaker, and QoS\\n- Rate limiting and throttling\\n- Load balancing\\n- Logging, traci\", \"ng, correlation\\n- Headers, query strings, and claims transformation\\n- IP allowlisting\\n\\nFor more info\", \"rmation, see Gateway offloading pattern .\\n\\n## Using products with API Gateway features\\n\\nThere can be\", \" many more cross-cutting concerns offered by the API Gateways products depending on each implementat\", \"ion. We'll explore here:\\n\\n- Azure API Management\\n- Ocelot\\n\\nAP! Gateway with Azure API Management\\n\\nAr\", \"chitecture\\n\\n------\\n\\n## Azure API Management\\n\\nAzure API Management (as shown in Figure 4-14) not only\", \" solves your API Gateway needs but provides features like gathering insights from your APIs. If you'\", \"re using an API management solution, an API Gateway is only a component within that full API managem\", \"ent solution.\\n\\nDeveloper container\\n\\n## portal Microservice 2\\n\\nWeb AP\\n\\ncontainer\\n\\n<!-- image -->\\n\\nCli\", \"ent SPA Web app\\n\\nJavaScript/Angularjs\\n\\nTraditional Web app\\n\\nBrowser\\n\\nHTML\\n\\nFigure 4 -14. Using Azure\", \" API Management for your API Gateway\\n\\nAzure API Management solves both your API Gateway and Manageme\", \"nt needs like logging, security, metering, etc. In this case, when using a product like Azure API Ma\", \"nagement, the fact that you might have a single API Gateway is not so risky because these kinds of A\", \"PI Gateways are \\\"thinner\\\", meaning that you don't implement custom C# code that could evolve towards\", \" a monolithic component.\\n\\nThe API Gateway products usually act like a reverse proxy for ingress comm\", \"unication, where you can also filter the APIs from the internal microservices plus apply authorizati\", \"on to the published APIs in this single tier.\\n\\nThe insights available from an API Management system \", \"help you get an understanding of how your APIs are being used and how they are performing. They do t\", \"his activity by letting you view near realtime analytics reports and identifying trends that might i\", \"mpact your business. Plus, you can have logs about request and response activity for further online \", \"and offline analysis.\\n\\nWith Azure API Management, you can secure your APIs using a key, a token, and\", \" IP filtering. These features let you enforce flexible and fine-grained quotas and rate limits, modi\", \"fy the shape and behavior of your APIs using policies, and improve performance with response caching\", \".\\n\\nIn this guide and the reference sample application (eShopOnContainers), the architecture is limit\", \"ed to a simpler and custom-made containerized architecture in order to focus on plain containers wit\", \"hout\\n\\nAPI Gateway\\n\\nusing PaaS products like Azure API Management. But for large microservice-based a\", \"pplications that are deployed into Microsoft Azure, we encourage you to evaluate Azure API Managemen\", \"t as the base for your API Gateways in production.\\n\\n## Ocelot\\n\\nOcelot is a lightweight API Gateway, \", \"recommended for simpler approaches. Ocelot is an Open Source .NET Core -based API Gateway especially\", \" made for microservices architectures that need unified points of entry into their systems. It's lig\", \"htweight, fast, and scalable and provides routing and authentication among many other features.\\n\\nThe\", \" main reason to choose Ocelot for the eShopOnContainers reference application 2.0 is because Ocelot \", \"is a .NET Core lightweight API Gateway that you can deploy into the same application deployment envi\", \"ronment where you're deploying your microservices/containers, such as a Docker Host, Kubernetes, etc\", \". And since it's based on .NET Core, it's cross -platform allowing you to deploy on Linux or Windows\", \".\\n\\nThe previous diagrams showing custom API Gateways running in containers are precisely how you can\", \" also run Ocelot in a container and microservice -based application.\\n\\nIn addition, there are many ot\", \"her products in the market offering API Gateways features, such as Apigee, Kong, MuleSoft, WSO2, and\", \" other products like Linkerd and Istio for service mesh ingress controller features.\\n\\nAfter the init\", \"ial architecture and patterns explanation sections, the next sections explain how to implement API G\", \"ateways with Ocelot .\\n\\n## Drawbacks of the API Gateway pattern\\n\\n- The most important drawback is tha\", \"t when you implement an API Gateway, you're coupling that tier with the internal microservices. Coup\", \"ling like this might introduce serious difficulties for your application. Clemens Vaster, architect \", \"at the Azure Service Bus team, refers to this potential difficulty as \\\"the new ESB\\\" in the \\\"Messagin\", \"g and Microservices\\\" session at GOTO 2016.\\n- Using a microservices API Gateway creates an additional\", \" possible single point of failure.\\n- An API Gateway can introduce increased response time due to the\", \" additional network call. However, this extra call usually has less impact than having a client inte\", \"rface that's too chatty directly calling the internal microservices.\\n- If not scaled out properly, t\", \"he API Gateway can become a bottleneck.\\n- An API Gateway requires additional development cost and fu\", \"ture maintenance if it includes custom logic and data aggregation. Developers must update the API Ga\", \"teway in order to expose each microservice's endpoints. Moreover, implementation changes in the inte\", \"rnal microservices might cause code changes at the API Gateway level. However, if the API Gateway is\", \" just applying security, logging, and versioning (as when using Azure API Management), this addition\", \"al development cost might not apply.\\n\\n- If the API Gateway is developed by a single team, there can \", \"be a development bottleneck. This aspect is another reason why a better approach is to have several \", \"fined-grained API Gateways that respond to different client needs. You could also segregate the API \", \"Gateway internally into multiple areas or layers that are owned by the different teams working on th\", \"e internal microservices.\\n\\n## Additional resources\\n\\n- Chris Richardson. Pattern: API Gateway / Backe\", \"nd for Front-End https://microservices.io/patterns/apigateway.html\\n- API Gateway pattern https://lea\", \"rn.microsoft.com/azure/architecture/microservices/gateway\\n- Aggregation and composition pattern http\", \"s://microservices.io/patterns/data/api-composition.html\\n- Azure API Management https://azure.microso\", \"ft.com/services/api-management/\\n- Udi Dahan. Service Oriented Composition https://udidahan.com/2014/\", \"07/30/service-oriented-composition-with-video/\\n- Clemens Vasters. Messaging and Microservices at GOT\", \"O 2016 (video) https://www.youtube.com/watch?v=rXi5CLjIQ9k\\n- API Gateway in a Nutshell (ASP.NET Core\", \" API Gateway Tutorial Series) https://www.pogsdotnet.com/2018/08/api-gateway-in-nutshell.html\\n\\n## Co\", \"mmunication in a microservice architecture\\n\\nIn a monolithic application running on a single process,\", \" components invoke one another using language-level method or function calls. These can be strongly \", \"coupled if you're creating objects with code (for example, new ClassName()), or can be invoked in a \", \"decoupled way if you're using Dependency Injection by referencing abstractions rather than concrete \", \"object instances. Either way, the objects are running within the same process. The biggest challenge\", \" when changing from a monolithic application to a microservices-based application lies in changing t\", \"he communication mechanism. A direct conversion from in -process method calls into RPC calls to serv\", \"ices will cause a chatty and not efficient communication that won't perform well in distributed envi\", \"ronments. The challenges of designing distributed system properly are well enough known that there's\", \" even a canon known as the Fallacies of distributed computing that lists assumptions that developers\", \" often make when moving from monolithic to distributed designs.\\n\\nThere isn't one solution, but sever\", \"al. One solution involves isolating the business microservices as much as possible. You then use asy\", \"nchronous communication between the internal microservices and replace fine-grained communication th\", \"at's typical in intra-process communication between objects with coarser -grained communication. You\", \" can do this by grouping calls, and by returning data that aggregates the results of multiple intern\", \"al calls, to the client.\\n\\nA microservices -based application is a distributed system running on mult\", \"iple processes or services, usually even across multiple servers or hosts. Each service instance is \", \"typically a process. Therefore, services must interact using an inter-process communication protocol\", \" such as HTTP, AMQP, or a binary protocol like TCP, depending on the nature of each service.\\n\\nThe mi\", \"croservice community promotes the philosophy of \\\"smart endpoints and dumb pipes\\\". This slogan encour\", \"ages a design that's as decoupled as possible between microservices, and as cohesive as possible wit\", \"hin a single microservice. As explained earlier, each microservice owns its own data and its own dom\", \"ain logic. But the microservices composing an end-to-end application are usually simply choreographe\", \"d by using REST communications rather than complex protocols such as WS-* and flexible event -driven\", \" communications instead of centralized business -process -orchestrators.\\n\\nThe two commonly used prot\", \"ocols are HTTP request/response with resource APIs (when querying most of all), and lightweight asyn\", \"chronous messaging when communicating updates across multiple microservices. These are explained in \", \"more detail in the following sections.\\n\\n## Communication types\\n\\nClient and services can communicate \", \"through many different types of communication, each one targeting a different scenario and goals. In\", \"itially, those types of communications can be classified in two axes.\\n\\nThe first axis defines if the\", \" protocol is synchronous or asynchronous:\\n\\n- Synchronous protocol. HTTP is a synchronous protocol. T\", \"he client sends a request and waits for a response from the service. That's independent of the clien\", \"t code execution that could be synchronous (thread is blocked) or asynchronous (thread isn't blocked\", \", and the response will reach a callback eventually). The important point here is that the protocol \", \"(HTTP/HTTPS) is synchronous and the client code can only continue its task when it receives the HTTP\", \" server response.\\n- Asynchronous protocol. Other protocols like AMQP (a protocol supported by many o\", \"perating systems and cloud environments) use asynchronous messages. The client code or message sende\", \"r usually doesn't wait for a response. It just sends the message as when sending a message to a Rabb\", \"itMQ queue or any other message broker.\\n\\nThe second axis defines if the communication has a single r\", \"eceiver or multiple receivers:\\n\\n- Single receiver. Each request must be processed by exactly one rec\", \"eiver or service. An example of this communication is the Command pattern .\\n- Multiple receivers. Ea\", \"ch request can be processed by zero to multiple receivers. This type of communication must be asynch\", \"ronous. An example is the publish/subscribe mechanism used in patterns like Event-driven architectur\", \"e. This is based on an event-bus interface or message broker when propagating data updates between m\", \"ultiple microservices through events; it's usually implemented through a service bus or similar arti\", \"fact like Azure Service Bus by using topics and subscriptions .\\n\\nA microservice -based application w\", \"ill often use a combination of these communication styles. The most common type is single-receiver c\", \"ommunication with a synchronous protocol like HTTP/HTTPS when invoking a regular Web API HTTP servic\", \"e. Microservices also typically use messaging protocols for asynchronous communication between micro\", \"services.\\n\\nThese axes are good to know so you have clarity on the possible communication mechanisms,\", \" but they're not the important concerns when building microservices. Neither the asynchronous nature\", \" of client thread execution nor the asynchronous nature of the selected protocol are the important p\", \"oints when integrating microservices. What is important is being able to integrate your microservice\", \"s asynchronously while maintaining the independence of microservices, as explained in the following \", \"section.\\n\\n## Asynchronous microservice integration enforces microservice's autonomy\\n\\nAs mentioned, t\", \"he important point when building a microservices-based application is the way you integrate your mic\", \"roservices. Ideally, you should try to minimize the communication between the internal microservices\", \". The fewer communications between microservices, the better. But in many cases, you'll have to some\", \"how integrate the microservices. When you need to do that, the critical rule here is that the commun\", \"ication between the microservices should be asynchronous. That doesn't mean that you have to use a s\", \"pecific protocol (for example, asynchronous messaging versus synchronous HTTP). It just means that t\", \"he communication between microservices should be done only by propagating data asynchronously, but t\", \"ry not to depend on other internal microservices as part of the initial service's HTTP request/respo\", \"nse operation.\\n\\nIf possible, never depend on synchronous communication (request/response) between mu\", \"ltiple microservices, not even for queries. The goal of each microservice is to be autonomous and av\", \"ailable to the client consumer, even if the other services that are part of the end-to-end applicati\", \"on are down or unhealthy. If you think you need to make a call from one microservice to other micros\", \"ervices (like performing an HTTP request for a data query) to be able to provide a response to a cli\", \"ent application, you have an architecture that won't be resilient when some microservices fail.\\n\\nMor\", \"eover, having HTTP dependencies between microservices, like when creating long request/response cycl\", \"es with HTTP request chains, as shown in the first part of the Figure 4-15, not only makes your micr\", \"oservices not autonomous but also their performance is impacted as soon as one of the services in th\", \"at chain isn't performing well.\\n\\nThe more you add synchronous dependencies between microservices, su\", \"ch as query requests, the worse the overall response time gets for the client apps.\\n\\nSynchronous vs.\", \" async communication across microservices\\n\\nAnti-pattern\\n\\nSynchronous all request/response\\n\\ncycle\\n\\nAs\", \"ynchronous\\n\\nComm. across internal microservices\\n\\n(EventBus: like AMQP)\\n\\n\\\"Asynchronous\\\"\\n\\nComm. across\", \" internal microservices\\n\\n(Polling: Http)\\n\\nClient\\n\\nHttp sync.\\n\\nrequest\\n\\nHttp sync.\\n\\nrequest\\n\\nHttp syn\", \"c.\\n\\nrequest\\n\\nHttp sync.\\n\\nrequest\\n\\nBasket\\n\\nOrdering\\n\\nCatalog\\n\\n## Such as MVC app Http sync. response \", \"Http sync. Http sync. response response Http sync. response\\n\\nAPI Gateway\\n\\nSame http request/response\", \" cycle!\\n\\nFigure 4 -15. Anti -patterns and patterns in communication between microservices\\n\\n<!-- imag\", \"e -->\\n\\nAs shown in the above diagram, in synchronous communication a \\\"chain\\\" of requests is created \", \"between microservices while serving the client request. This is an anti-pattern. In asynchronous com\", \"munication microservices use asynchronous messages or http polling to communicate with other microse\", \"rvices, but the client request is served right away.\\n\\nIf your microservice needs to raise an additio\", \"nal action in another microservice, if possible, do not perform that action synchronously and as par\", \"t of the original microservice request and reply operation. Instead, do it asynchronously (using asy\", \"nchronous messaging or integration events, queues, etc.). But, as much as possible, do not invoke th\", \"e action synchronously as part of the original synchronous request and reply operation.\\n\\nAnd finally\", \" (and this is where most of the issues arise when building microservices), if your initial microserv\", \"ice needs data that's originally owned by other microservices, do not rely on making synchronous req\", \"uests for that data. Instead, replicate or propagate that data (only the attributes you need) into t\", \"he initial service's database by using eventual consistency (typically by using integration events, \", \"as explained in upcoming sections).\\n\\nAs noted earlier in the Identifying domain-model boundaries for\", \" each microservice section, duplicating some data across several microservices isn't an incorrect de\", \"sign\\u2014on the contrary, when doing that you can translate the data into the specific language or terms\", \" of that additional domain or Bounded Context. For instance, in the eShopOnContainers application yo\", \"u have a microservice named identity-api that's in charge of most of the user's data with an entity \", \"named User. However, when you need to store data about the user within the Ordering microservice, yo\", \"u store it as a different entity named Buyer. The Buyer entity shares the same identity with the ori\", \"ginal User entity, but it might have only the few attributes needed by the Ordering domain, and not \", \"the whole user profile.\\n\\nOther\\n\\nRequest/response communication for live queries and updates\\n\\nHTTP-ba\", \"sed Services\\n\\nYou might use any protocol to communicate and propagate data asynchronously across mic\", \"roservices in order to have eventual consistency. As mentioned, you could use integration events usi\", \"ng an event bus or message broker or you could even use HTTP by polling the other services instead. \", \"It doesn't matter. The important rule is to not create synchronous dependencies between your microse\", \"rvices. eb API\\n\\nrequest/response\\n\\nWeb API\\n\\nThe following sections explain the multiple communication\", \" styles you can consider using in a microservice -based application. Redis cache\\n\\nCg\\n\\n## Communicati\", \"on styles\\n\\nThere are many protocols and choices you can use for communication, depending on the comm\", \"unication type you want to use. If you're using a synchronous request/response-based communication m\", \"echanism, protocols such as HTTP and REST approaches are the most common, especially if you're publi\", \"shing your services outside the Docker host or microservice cluster. If you're communicating between\", \" services internally (within your Docker host or microservices cluster), you might also want to use \", \"binary format communication mechanisms (like WCF using TCP and binary format). Alternatively, you ca\", \"n use asynchronous, message-based communication mechanisms such as AMQP.\\n\\nThere are also multiple me\", \"ssage formats like JSON or XML, or even binary formats, which can be more efficient. If your chosen \", \"binary format isn't a standard, it's probably not a good idea to publicly publish your services usin\", \"g that format. You could use a non-standard format for internal communication between your microserv\", \"ices. You might do this when communicating between microservices within your Docker host or microser\", \"vice cluster (for example, Docker orchestrators), or for proprietary client applications that talk t\", \"o the microservices.\\n\\n## Request/response communication with HTTP and REST\\n\\nWhen a client uses reque\", \"st/response communication, it sends a request to a service, then the service processes the request a\", \"nd sends back a response. Request/response communication is especially well suited for querying data\", \" for a real-time UI (a live user interface) from client apps. Therefore, in a microservice architect\", \"ure you'll probably use this communication mechanism for most queries, as shown in Figure 4-16.\\n\\nFig\", \"ure 4 -16. Using HTTP request/response communication (synchronous or asynchronous)\\n\\n<!-- image -->\\n\\n\", \"When a client uses request/response communication, it assumes that the response will arrive in a sho\", \"rt time, typically less than a second, or a few seconds at most. For delayed responses, you need to \", \"implement asynchronous communication based on messaging patterns and messaging technologies , which \", \"is a different approach that we explain in the next section.\\n\\nA popular architectural style for requ\", \"est/response communication is REST. This approach is based on, and tightly coupled to, the HTTP prot\", \"ocol, embracing HTTP verbs like GET, POST, and PUT. REST is the most commonly used architectural com\", \"munication approach when creating services. You can implement REST services when you develop ASP.NET\", \" Core Web API services.\\n\\nThere's additional value when using HTTP REST services as your interface de\", \"finition language. For instance, if you use Swagger metadata to describe your service API, you can u\", \"se tools that generate client stubs that can directly discover and consume your services.\\n\\n## Additi\", \"onal resources\\n\\n- Martin Fowler. Richardson Maturity Model A description of the REST model. https://\", \"martinfowler.com/articles/richardsonMaturityModel.html\\n- Swagger The official site. https://swagger.\", \"io/\\n\\n## Push and real -time communication based on HTTP\\n\\nAnother possibility (usually for different \", \"purposes than REST) is a real-time and one-to-many communication with higher-level frameworks such a\", \"s ASP.NET SignalR and protocols such as WebSockets .\\n\\nAs Figure 4-17 shows, real-time HTTP communica\", \"tion means that you can have server code pushing content to connected clients as the data becomes av\", \"ailable, rather than having the server wait for a client to request new data.\\n\\nPush and real-time co\", \"mmunication based on HTTP\\n\\nOne-to-many communication\\n\\nClient-1 WebApp SPA\\n\\nJavaScript / Angular.js\\n\\n\", \"Client-2 WebApp SPA \\\\\\n\\nJavaScript / Angular.js\\n\\n## 1 Back end\\n\\nFigure 4 -17. One -to -many real -tim\", \"e asynchronous message communication\\n\\n<!-- image -->\\n\\nSignalR is a good way to achieve real-time com\", \"munication for pushing content to the clients from a back -end server. Since communication is in rea\", \"l time, client apps show the changes almost instantly. This is usually handled by a protocol such as\", \" WebSockets, using many WebSockets connections (one per client). A typical example is when a service\", \" communicates a change in the score of a sports game to many client web apps simultaneously.\\n\\n## Asy\", \"nchronous message -based communication\\n\\nAsynchronous messaging and event-driven communication are cr\", \"itical when propagating changes across multiple microservices and their related domain models. As me\", \"ntioned earlier in the discussion microservices and Bounded Contexts (BCs), models (User, Customer, \", \"Product, Account, etc.) can mean different things to different microservices or BCs. That means that\", \" when changes occur, you need some way to reconcile changes across the different models. A solution \", \"is eventual consistency and event -driven communication based on asynchronous messaging.\\n\\nWhen using\", \" messaging, processes communicate by exchanging messages asynchronously. A client makes a command or\", \" a request to a service by sending it a message. If the service needs to reply, it sends a different\", \" message back to the client. Since it's a message-based communication, the client assumes that the r\", \"eply won't be received immediately, and that there might be no response at all.\\n\\nA message is compos\", \"ed by a header (metadata such as identification or security information) and a body. Messages are us\", \"ually sent through asynchronous protocols like AMQP.\\n\\nThe preferred infrastructure for this type of \", \"communication in the microservices community is a lightweight message broker, which is different tha\", \"n the large brokers and orchestrators used in SOA. In a lightweight message broker, the infrastructu\", \"re is typically \\\"dumb,\\\" acting only as a message broker, with simple implementations such as RabbitM\", \"Q or a scalable service bus in the cloud like\\n\\nBack end\\n\\n(1)\\n\\ncommand\\n\\nJ\\n\\nSingle receiver message-ba\", \"sed communication\\n\\n(i.e. Message-based Commands)\\n\\n-\\n\\n--\\n\\n-\\n\\nAzure Service Bus. In this scenario, mos\", \"t of the \\\"smart\\\" thinking still lives in the endpoints that are producing and consuming messages-tha\", \"t is, in the microservices.\\n\\nAnother rule you should try to follow, as much as possible, is to use o\", \"nly asynchronous messaging between the internal services, and to use synchronous communication (such\", \" as HTTP) only from the client apps to the front-end services (API Gateways plus the first level of \", \"microservices).\\n\\nThere are two kinds of asynchronous messaging communication: single receiver messag\", \"e-based communication, and multiple receivers message-based communication. The following sections pr\", \"ovide details about them.\\n\\nCache\\n\\nDatabase\\n\\n## Single-receiver message-based communication\\n\\nMessage \", \"-based asynchronous communication with a single receiver means there's point-to-point communication \", \"that delivers a message to exactly one of the consumers that's reading from the channel, and that th\", \"e message is processed just once. However, there are special situations. For instance, in a cloud sy\", \"stem that tries to automatically recover from failures, the same message could be sent multiple time\", \"s. Due to network or other failures, the client has to be able to retry sending messages, and the se\", \"rver has to implement an operation to be idempotent in order to process a particular message just on\", \"ce.\\n\\nSingle-receiver message-based communication is especially well suited for sending asynchronous \", \"commands from one microservice to another as shown in Figure 4-18 that illustrates this approach.\\n\\nO\", \"nce you start sending message-based communication (either with commands or events), you should avoid\", \" mixing message-based communication with synchronous HTTP communication.\\n\\nFigure 4 -18. A single mic\", \"roservice receiving an asynchronous message\\n\\n<!-- image -->\\n\\nWhen the commands come from client appl\", \"ications, they can be implemented as HTTP synchronous commands. Use message-based commands when you \", \"need higher scalability or when you're already in a message-based business process.\\n\\n## Multiple-rec\", \"eivers message-based communication\\n\\nAs a more flexible approach, you might also want to use a publis\", \"h/subscribe mechanism so that your communication from the sender will be available to additional sub\", \"scriber microservices or to external applications. Thus, it helps you to follow the open/closed prin\", \"ciple in the sending service. That way, additional subscribers can be added in the future without th\", \"e need to modify the sender service.\\n\\nWhen you use a publish/subscribe communication, you might be u\", \"sing an event bus interface to publish events to any subscriber.\\n\\n## Asynchronous event-driven commu\", \"nication\\n\\nWhen using asynchronous event-driven communication, a microservice publishes an integratio\", \"n event when something happens within its domain and another microservice needs to be aware of it, l\", \"ike a price change in a product catalog microservice. Additional microservices subscribe to the even\", \"ts so they can receive them asynchronously. When that happens, the receivers might update their own \", \"domain entities, which can cause more integration events to be published. This publish/subscribe sys\", \"tem is performed by using an implementation of an event bus. The event bus can be designed as an abs\", \"traction or interface, with the API that's needed to subscribe or unsubscribe to events and to publi\", \"sh events. The event bus can also have one or more implementations based on any inter-process and me\", \"ssaging broker, like a messaging queue or service bus that supports asynchronous communication and a\", \" publish/subscribe model.\\n\\nIf a system uses eventual consistency driven by integration events, it's \", \"recommended that this approach is made clear to the end user. The system shouldn't use an approach t\", \"hat mimics integration events, like SignalR or polling systems from the client. The end user and the\", \" business owner have to explicitly embrace eventual consistency in the system and realize that in ma\", \"ny cases the business doesn't have any problem with this approach, as long as it's explicit. This ap\", \"proach is important because users might expect to see some results immediately and this aspect might\", \" not happen with eventual consistency.\\n\\nAs noted earlier in the Challenges and solutions for distrib\", \"uted data management section, you can use integration events to implement business tasks that span m\", \"ultiple microservices. Thus, you'll have eventual consistency between those services. An eventually \", \"consistent transaction is made up of a collection of distributed actions. At each action, the relate\", \"d microservice updates a domain entity and publishes another integration event that raises the next \", \"action within the same end-to-end business task.\\n\\nAn important point is that you might want to commu\", \"nicate to multiple microservices that are subscribed to the same event. To do so, you can use publis\", \"h/subscribe messaging based on eventdriven communication, as shown in Figure 4-19. This publish/subs\", \"cribe mechanism isn't exclusive to the microservice architecture. It's similar to the way Bounded Co\", \"ntexts in DDD should communicate, or to the way you propagate updates from the write database to the\", \" read database in the Command\\n\\nAsynchronous event-driven communication\\n\\nMultiple receivers\\n\\n--\\n\\n---\\n\", \"\\n-\\n\\nand Query Responsibility Segregation (CQRS) architecture pattern. The goal is to have eventual c\", \"onsistency between multiple data sources across your distributed system.\\n\\nBack end\\n\\nUser-Profile Mic\", \"roservice\\n\\n1\\n\\nWeb API Service\\n\\nUpdateUser command\\n\\n## Service X UserUpdated event \\u2192 Buyer info 3 Eve\", \"nt Bus\\n\\nUserUpdated event!\\n\\n(Publish Action)\\n\\n<!-- image -->\\n\\n2\\n\\nDatabase\\n\\nEventual consistency acro\", \"ss\\n\\nFigure 4 -19. Asynchronous event -driven message communication\\n\\nIn asynchronous event-driven com\", \"munication, one microservice publishes events to an event bus and many microservices can subscribe t\", \"o it, to get notified and act on it. Your implementation will determine what protocol to use for eve\", \"nt-driven, message-based communications. AMQP can help achieve reliable queued communication.\\n\\nWhen \", \"you use an event bus, you might want to use an abstraction level (like an event bus interface) based\", \" on a related implementation in classes with code using the API from a message broker like RabbitMQ \", \"or a service bus like Azure Service Bus with Topics. Alternatively, you might want to use a higher-l\", \"evel service bus like NServiceBus , MassTransit, or Brighter to articulate your event bus and publis\", \"h/subscribe system.\\n\\n## A note about messaging technologies for production systems\\n\\nThe messaging te\", \"chnologies available for implementing your abstract event bus are at different levels. For instance,\", \" products like RabbitMQ (a messaging broker transport) and Azure Service Bus sit at a lower level th\", \"an other products like NServiceBus , MassTransit, or Brighter, which can work on top of RabbitMQ and\", \" Azure Service Bus. Your choice depends on how many rich features at the application level and out -\", \"of -the -box scalability you need for your application. For implementing just a proof-ofconcept even\", \"t bus for your development environment, as it was done in the eShopOnContainers sample, a simple imp\", \"lementation on top of RabbitMQ running on a Docker container might be enough.\\n\\nHowever, for mission \", \"-critical and production systems that need hyper-scalability, you might want to evaluate Azure Servi\", \"ce Bus. For high-level abstractions and features that make the development of distributed applicatio\", \"ns easier, we recommend that you evaluate other commercial and open-source service buses, such as NS\", \"erviceBus , MassTransit, and Brighter. Of course, you can build your own\\n\\nCache\\n\\n(Publish/subscribe \", \"channel) 4 X UserUpdated event \\u2192 Buyer info\\n\\nservice -bus features on top of lower-level technologie\", \"s like RabbitMQ and Docker. But that plumbing work might cost too much for a custom enterprise appli\", \"cation.\\n\\n## Resiliently publishing to the event bus\\n\\nA challenge when implementing an event-driven a\", \"rchitecture across multiple microservices is how to atomically update state in the original microser\", \"vice while resiliently publishing its related integration event into the event bus, somehow based on\", \" transactions. The following are a few ways to accomplish this functionality, although there could b\", \"e additional approaches as well.\\n\\n- Using a transactional (DTC-based) queue like MSMQ. (However, thi\", \"s is a legacy approach.)\\n- Using transaction log mining.\\n- Using full Event Sourcing pattern.\\n- Usin\", \"g the Outbox pattern: a transactional database table as a message queue that will be the base for an\", \" event -creator component that would create the event and publish it.\\n\\nFor a more complete descripti\", \"on of the challenges in this space, including how messages with potentially incorrect data can end u\", \"p being published, see Data platform for mission-critical workloads on Azure: Every message must be \", \"processed .\\n\\nAdditional topics to consider when using asynchronous communication are message idempot\", \"ence and message deduplication. These topics are covered in the section Implementing event-based com\", \"munication between microservices (integration events) later in this guide.\\n\\n## Additional resources\\n\", \"\\n- Event Driven Messaging https://patterns.arcitura.com/soa-patterns/design\\\\_patterns/event\\\\_driven\\\\\", \"_messaging\\n- Publish/Subscribe Channel https://www.enterpriseintegrationpatterns.com/patterns/messag\", \"ing/PublishSubscribeChannel. html\\n- Udi Dahan. Clarified CQRS https://udidahan.com/2009/12/09/clarif\", \"ied-cqrs/\\n- Command and Query Responsibility Segregation (CQRS) https://learn.microsoft.com/azure/ar\", \"chitecture/patterns/cqrs\\n- Communicating Between Bounded Contexts https://learn.microsoft.com/previo\", \"us-versions/msp-n-p/jj591572(v=pandp.10)\\n- Eventual consistency https://en.wikipedia.org/wiki/Eventu\", \"al\\\\_consistency\\n- Jimmy Bogard. Refactoring Towards Resilience: Evaluating Coupling https://jimmybog\", \"ard.com/refactoring-towards-resilience-evaluating-coupling/\\n\\n## Creating, evolving, and versioning m\", \"icroservice APIs and contracts\\n\\nA microservice API is a contract between the service and its clients\", \". You'll be able to evolve a microservice independently only if you do not break its API contract, w\", \"hich is why the contract is so important. If you change the contract, it will impact your client app\", \"lications or your API Gateway.\\n\\nThe nature of the API definition depends on which protocol you're us\", \"ing. For instance, if you're using messaging, like AMQP, the API consists of the message types. If y\", \"ou're using HTTP and RESTful services, the API consists of the URLs and the request and response JSO\", \"N formats.\\n\\nHowever, even if you're thoughtful about your initial contract, a service API will need \", \"to change over time. When that happens\\u2014and especially if your API is a public API consumed by multip\", \"le client applications \\u2014 you typically can't force all clients to upgrade to your new API contract. \", \"You usually need to incrementally deploy new versions of a service in a way that both old and new ve\", \"rsions of a service contract are running simultaneously. Therefore, it's important to have a strateg\", \"y for your service versioning.\\n\\nWhen the API changes are small, like if you add attributes or parame\", \"ters to your API, clients that use an older API should switch and work with the new version of the s\", \"ervice. You might be able to provide default values for any missing attributes that are required, an\", \"d the clients might be able to ignore any extra response attributes.\\n\\nHowever, sometimes you need to\", \" make major and incompatible changes to a service API. Because you might not be able to force client\", \" applications or services to upgrade immediately to the new version, a service must support older ve\", \"rsions of the API for some period. If you're using an HTTPbased mechanism such as REST, one approach\", \" is to embed the API version number in the URL or into an HTTP header. Then you can decide between i\", \"mplementing both versions of the service simultaneously within the same service instance, or deployi\", \"ng different instances that each handle a version of the API. A good approach for this functionality\", \" is the Mediator pattern (for example, MediatR library) to decouple the different implementation ver\", \"sions into independent handlers.\\n\\nFinally, if you're using a REST architecture, Hypermedia is the be\", \"st solution for versioning your services and allowing evolvable APIs.\\n\\n## Additional resources\\n\\n- Sc\", \"ott Hanselman. ASP.NET Core RESTful Web API versioning made easy https://www.hanselman.com/blog/ASPN\", \"ETCoreRESTfulWebAPIVersioningMadeEasy.aspx\\n- Versioning a RESTful web API https://learn.microsoft.co\", \"m/azure/architecture/best-practices/api-design#versioning-arestful -web -api\\n- Roy Fielding. Version\", \"ing, Hypermedia, and REST https://www.infoq.com/articles/roy-fielding-on-versioning\\n\\n## Microservice\", \"s addressability and the service registry\\n\\nEach microservice has a unique name (URL) that's used to \", \"resolve its location. Your microservice needs to be addressable wherever it's running. If you have t\", \"o think about which computer is running a particular microservice, things can go bad quickly. In the\", \" same way that DNS resolves a URL to a particular computer, your microservice needs to have a unique\", \" name so that its current location is discoverable. Microservices need addressable names that make t\", \"hem independent from the infrastructure that they're running on. This approach implies that there's \", \"an interaction between how your service is deployed and how it's discovered, because there needs to \", \"be a service registry. In the same vein, when a computer fails, the registry service must be able to\", \" indicate where the service is now running.\\n\\nThe service registry pattern is a key part of service d\", \"iscovery. The registry is a database containing the network locations of service instances. A servic\", \"e registry needs to be highly available and up-to-date. Clients could cache network locations obtain\", \"ed from the service registry. However, that information eventually goes out of date and clients can \", \"no longer discover service instances. So, a service registry consists of a cluster of servers that u\", \"se a replication protocol to maintain consistency.\\n\\nIn some microservice deployment environments (ca\", \"lled clusters, to be covered in a later section), service discovery is built in. For example, an Azu\", \"re Kubernetes Service (AKS) environment can handle service instance registration and deregistration.\", \" It also runs a proxy on each cluster host that plays the role of server -side discovery router.\\n\\n##\", \" Additional resources\\n\\n- Chris Richardson. Pattern: Service registry https://microservices.io/patter\", \"ns/service-registry.html\\n- Auth0. The Service Registry https://auth0.com/blog/an-introduction-to-mic\", \"roservices-part-3-the-service-registry/\\n- Gabriel Schenker. Service discovery https://lostechies.com\", \"/gabrielschenker/2016/01/27/service-discovery/\\n\\n## Creating composite UI based on microservices\\n\\nMic\", \"roservices architecture often starts with the server -side handling data and logic, but, in many cas\", \"es, the UI is still handled as a monolith. However, a more advanced approach, called micro frontends\", \", is to design your application UI based on microservices as well. That means having a composite UI \", \"produced by the microservices, instead of having microservices on the server and just a monolithic c\", \"lient app consuming the microservices. With this approach, the microservices you build can be comple\", \"te with both logic and visual representation.\\n\\nFigure 4-20 shows the simpler approach of just consum\", \"ing microservices from a monolithic client application. Of course, you could have an ASP.NET MVC ser\", \"vice in between producing the HTML and JavaScript. The figure is a simplification that highlights th\", \"at you have a single (monolithic) client UI\\n\\nMonolithic Ul consuming microservices\\n\\n- --\\n\\nMonolithic\", \" Ul: Visual layout, shapes and styles are defined in\\n\\nthe client app, and don't depend\\n\\n\\u00bf on the mic\", \"roservices\\n\\nMicroservices consuming the microservices, which just focus on logic and data and not on\", \" the UI shape (HTML and JavaScript). Web API\\n\\n## Data container Microservice 2\\n\\nFigure 4 -20. A mono\", \"lithic UI application consuming back -end microservices\\n\\n<!-- image -->\\n\\nIn contrast, a composite UI\", \" is precisely generated and composed by the microservices themselves. Some of the microservices driv\", \"e the visual shape of specific areas of the UI. The key difference is that you have client UI compon\", \"ents (TypeScript classes, for example) based on templates, and the datashaping-UI ViewModel for thos\", \"e templates comes from each microservice.\\n\\nAt client application start-up time, each of the client U\", \"I components (TypeScript classes, for example) registers itself with an infrastructure microservice \", \"capable of providing ViewModels for a given scenario. If the microservice changes the shape, the UI \", \"changes also.\\n\\nFigure 4-21 shows a version of this composite UI approach. This approach is simplifie\", \"d because you might have other microservices that are aggregating granular parts that are based on d\", \"ifferent techniques. It depends on whether you're building a traditional web approach (ASP.NET MVC) \", \"or an SPA (Single Page Application).\\n\\nData\\n\\namazon\\n\\nComposite Ul generated by microservices\\n\\n\\u0413\\n\\n-\\n\\nB\", \"ackend Microservices\\n\\nJSON\\n\\nThe Easter Shop\\n\\n## Try Primed composed ViewModel DTOs Microservice 1 Ul\", \" Composition Microservice 2\\n\\nComposed\\n\\nUl Composition\\n\\nContainer\\n\\n<!-- image -->\\n\\nComposite Ul\\n\\nBook\", \"s at Amazon\\n\\nAward winners\\n\\nMore from the Amazon Book Editors\\n\\nFigure 4 -21. Example of a composite \", \"UI application shaped by back -end microservices\\n\\nEach of those UI composition microservices would b\", \"e similar to a small API Gateway. But in this case, each one is responsible for a small UI area.\\n\\nA \", \"composite UI approach that's driven by microservices can be more challenging or less so, depending o\", \"n what UI technologies you're using. For instance, you won't use the same techniques for building a \", \"traditional web application that you use for building an SPA or for native mobile app (as when devel\", \"oping Xamarin apps, which can be more challenging for this approach).\\n\\nThe eShopOnContainers sample \", \"application uses the monolithic UI approach for multiple reasons. First, it's an introduction to mic\", \"roservices and containers. A composite UI is more advanced but also requires further complexity when\", \" designing and developing the UI. Second, eShopOnContainers also provides a native mobile app based \", \"on Xamarin, which would make it more complex on the client C# side.\\n\\nHowever, we encourage you to us\", \"e the following references to learn more about composite UI based on microservices.\\n\\n## Additional r\", \"esources\\n\\n- Micro Frontends (Martin Fowler's blog) https://martinfowler.com/articles/micro-frontends\", \".html\\n- Micro Frontends (Michael Geers site) https://micro-frontends.org/\\n- Composite UI using ASP.N\", \"ET (Particular's Workshop) https://github.com/Particular/Workshop/tree/master/demos/asp-net-core\\n- R\", \"uben Oostinga. The Monolithic Frontend in the Microservices Architecture https://xebia.com/blog/the-\", \"monolithic-frontend-in-the-microservices-architecture/\\n\\n- -\\n\\n-\\n\\n- Mauro Servienti. The secret of bet\", \"ter UI composition https://particular.net/blog/secret-of-better-ui-composition\\n- Viktor Farcic. Incl\", \"uding Front-End Web Components Into Microservices https://technologyconversations.com/2015/08/09/inc\", \"luding-front-end-web-componentsinto -microservices/\\n- Managing Frontend in the Microservices Archite\", \"cture https://allegro.tech/2016/03/Managing-Frontend-in-the-microservices-architecture.html\\n\\n## Resi\", \"liency and high availability in microservices\\n\\nDealing with unexpected failures is one of the hardes\", \"t problems to solve, especially in a distributed system. Much of the code that developers write invo\", \"lves handling exceptions, and this is also where the most time is spent in testing. The problem is m\", \"ore involved than writing code to handle failures. What happens when the machine where the microserv\", \"ice is running fails? Not only do you need to detect this microservice failure (a hard problem on it\", \"s own), but you also need something to restart your microservice.\\n\\nA microservice needs to be resili\", \"ent to failures and to be able to restart often on another machine for availability. This resiliency\", \" also comes down to the state that was saved on behalf of the microservice, where the microservice c\", \"an recover this state from, and whether the microservice can restart successfully. In other words, t\", \"here needs to be resiliency in the compute capability (the process can restart at any time) as well \", \"as resilience in the state or data (no data loss, and the data remains consistent).\\n\\nThe problems of\", \" resiliency are compounded during other scenarios, such as when failures occur during an application\", \" upgrade. The microservice, working with the deployment system, needs to determine whether it can co\", \"ntinue to move forward to the newer version or instead roll back to a previous version to maintain a\", \" consistent state. Questions such as whether enough machines are available to keep moving forward an\", \"d how to recover previous versions of the microservice need to be considered. This approach requires\", \" the microservice to emit health information so that the overall application and orchestrator can ma\", \"ke these decisions.\\n\\nIn addition, resiliency is related to how cloud-based systems must behave. As m\", \"entioned, a cloudbased system must embrace failures and must try to automatically recover from them.\", \" For instance, in case of network or container failures, client apps or client services must have a \", \"strategy to retry sending messages or to retry requests, since in many cases failures in the cloud a\", \"re partial. The Implementing Resilient Applications section in this guide addresses how to handle pa\", \"rtial failure. It describes techniques like retries with exponential backoff or the Circuit Breaker \", \"pattern in .NET by using libraries like Polly, which offers a large variety of policies to handle th\", \"is subject.\\n\\n## Health management and diagnostics in microservices\\n\\nIt may seem obvious, and it's of\", \"ten overlooked, but a microservice must report its health and diagnostics. Otherwise, there's little\", \" insight from an operations perspective. Correlating diagnostic events across a set of independent s\", \"ervices and dealing with machine clock skews to make sense of\\n\\nthe event order is challenging. In th\", \"e same way that you interact with a microservice over agreedupon protocols and data formats, there's\", \" a need for standardization in how to log health and diagnostic events that ultimately end up in an \", \"event store for querying and viewing. In a microservices approach, it's key that different teams agr\", \"ee on a single logging format. There needs to be a consistent approach to viewing diagnostic events \", \"in the application.\\n\\n## Health checks\\n\\nHealth is different from diagnostics. Health is about the mic\", \"roservice reporting its current state to take appropriate actions. A good example is working with up\", \"grade and deployment mechanisms to maintain availability. Although a service might currently be unhe\", \"althy due to a process crash or machine reboot, the service might still be operational. The last thi\", \"ng you need is to make this worse by performing an upgrade. The best approach is to do an investigat\", \"ion first or allow time for the microservice to recover. Health events from a microservice help us m\", \"ake informed decisions and, in effect, help create self-healing services.\\n\\nIn the Implementing healt\", \"h checks in ASP.NET Core services section of this guide, we explain how to use a new ASP.NET HealthC\", \"hecks library in your microservices so they can report their state to a monitoring service to take a\", \"ppropriate actions.\\n\\nYou also have the option of using an excellent open-source library called AspNe\", \"tCore.Diagnostics.HealthChecks, available on GitHub and as a NuGet package. This library also does h\", \"ealth checks, with a twist, it handles two types of checks:\\n\\n- Liveness: Checks if the microservice \", \"is alive, that is, if it's able to accept requests and respond.\\n- Readiness: Checks if the microserv\", \"ice's dependencies (Database, queue services, etc.) are themselves ready, so the microservice can do\", \" what it's supposed to do.\\n\\n## Using diagnostics and logs event streams\\n\\nLogs provide information ab\", \"out how an application or service is running, including exceptions, warnings, and simple information\", \"al messages. Usually, each log is in a text format with one line per event, although exceptions also\", \" often show the stack trace across multiple lines.\\n\\nIn monolithic server -based applications, you ca\", \"n write logs to a file on disk (a logfile) and then analyze it with any tool. Since application exec\", \"ution is limited to a fixed server or VM, it generally isn't too complex to analyze the flow of even\", \"ts. However, in a distributed application where multiple services are executed across many nodes in \", \"an orchestrator cluster, being able to correlate distributed events is a challenge.\\n\\nA microservice \", \"-based application should not try to store the output stream of events or logfiles by itself, and no\", \"t even try to manage the routing of the events to a central place. It should be transparent, meaning\", \" that each process should just write its event stream to a standard output that underneath will be c\", \"ollected by the execution environment infrastructure where it's running. An example of these event s\", \"tream routers is Microsoft.Diagnostic.EventFlow, which collects event streams from multiple sources \", \"and publishes it to output systems. These can include simple standard output for a development envir\", \"onment or cloud systems like Azure Monitor and Azure Diagnostics. There are also good third-party lo\", \"g analysis platforms and tools that can search, alert, report, and monitor logs, even in real time, \", \"like Splunk .\\n\\nLifecycle\\n\\nMgmt\\n\\nYour microservices\\n\\n## Orchestrators managing health and diagnostics\", \" information\\n\\nWhen you create a microservice-based application, you need to deal with complexity. Of\", \" course, a single microservice is simple to deal with, but dozens or hundreds of types and thousands\", \" of instances of microservices is a complex problem. It isn't just about building your microservice \", \"architecture \\u2014 you also need high availability, addressability, resiliency, health, and diagnostics \", \"if you intend to have a stable and cohesive system.\\n\\n## (Orchestrators/Clusters)\\n\\nFigure 4 -22. A Mi\", \"croservice Platform is fundamental for an application's health management\\n\\n<!-- image -->\\n\\nThe compl\", \"ex problems shown in Figure 4-22 are hard to solve by yourself. Development teams should focus on so\", \"lving business problems and building custom applications with microservice-based approaches. They sh\", \"ould not focus on solving complex infrastructure problems; if they did, the cost of any microservice\", \"-based application would be huge. Therefore, there are microservice-oriented platforms, referred to \", \"as orchestrators or microservice clusters, that try to solve the hard problems of building and runni\", \"ng a service and using infrastructure resources efficiently. This approach reduces the complexities \", \"of building applications that use a microservices approach.\\n\\nDifferent orchestrators might sound sim\", \"ilar, but the diagnostics and health checks offered by each of them differ in features and state of \", \"maturity, sometimes depending on the OS platform, as explained in the next section.\\n\\n## Additional r\", \"esources\\n\\n- The Twelve -Factor App. XI. Logs: Treat logs as event streams https://12factor.net/logs\\n\", \"- Microsoft Diagnostic EventFlow Library GitHub repo. https://github.com/Azure/diagnostics-eventflow\", \"\\n- What is Azure Diagnostics https://learn.microsoft.com/azure/azure-diagnostics\\n\\n- Connect Windows \", \"computers to the Azure Monitor service https://learn.microsoft.com/azure/azure-monitor/platform/agen\", \"t-windows\\n- Logging What You Mean: Using the Semantic Logging Application Block https://learn.micros\", \"oft.com/previous-versions/msp-n-p/dn440729(v=pandp.60)\\n- Splunk Official site. https://www.splunk.co\", \"m/\\n- EventSource Class API for events tracing for Windows (ETW) https://learn.microsoft.com/dotnet/a\", \"pi/system.diagnostics.tracing.eventsource\\n\\n## Orchestrate microservices and multi -container applica\", \"tions for high scalability and availability\\n\\nUsing orchestrators for production-ready applications i\", \"s essential if your application is based on microservices or simply split across multiple containers\", \". As introduced previously, in a microservicebased approach, each microservice owns its model and da\", \"ta so that it will be autonomous from a development and deployment point of view. But even if you ha\", \"ve a more traditional application that's composed of multiple services (like SOA), you'll also have \", \"multiple containers or services comprising a single business application that need to be deployed as\", \" a distributed system. These kinds of systems are complex to scale out and manage; therefore, you ab\", \"solutely need an orchestrator if you want to have a production-ready and scalable multi-container ap\", \"plication.\\n\\nFigure 4-23 illustrates deployment into a cluster of an application composed of multiple\", \" microservices (containers).\\n\\nComposed Docker Applications in a Cluster\\n\\n\\u2022 For each service instance\", \" you use one container\\n\\n\\u2022 Docker images/containers are \\\"units of deployment\\\"\\n\\nA container is an inst\", \"ance of a Docker Image\\n\\n## \\u2022 A host (VM/server) handles many containers\\n\\nFigure 4 -23. A cluster of \", \"containers\\n\\n<!-- image -->\\n\\nYou use one container for each service instance. Docker containers are \\\"\", \"units of deployment\\\" and a container is an instance of a Docker. A host handles many containers. It \", \"looks like a logical approach. But how are you handling load-balancing, routing, and orchestrating t\", \"hese composed applications?\\n\\nThe plain Docker Engine in single Docker hosts meets the needs of manag\", \"ing single image instances on one host, but it falls short when it comes to managing multiple contai\", \"ners deployed on multiple hosts for more complex distributed applications. In most cases, you need a\", \" management platform that will automatically start containers, scale out containers with multiple in\", \"stances per image, suspend them or shut them down when needed, and ideally also control how they acc\", \"ess resources like the network and data storage.\\n\\nTo go beyond the management of individual containe\", \"rs or simple composed apps and move toward larger enterprise applications with microservices, you mu\", \"st turn to orchestration and clustering platforms.\\n\\nFrom an architecture and development point of vi\", \"ew, if you're building large enterprise composed of microservices -based applications, it's importan\", \"t to understand the following platforms and products that support advanced scenarios:\\n\\nClusters and \", \"orchestrators. When you need to scale out applications across many Docker hosts, as when a large mic\", \"roservice-based application, it's critical to be able to manage all those hosts as a single cluster \", \"by abstracting the complexity of the underlying platform. That's what the container clusters and orc\", \"hestrators provide. Kubernetes is an example of an orchestrator, and is available in Azure through A\", \"zure Kubernetes Service.\\n\\nSchedulers. Scheduling means to have the capability for an administrator t\", \"o launch containers in a cluster so they also provide a UI. A cluster scheduler has several responsi\", \"bilities: to use the cluster's\\n\\nApp 1\\n\\nApp?\\n\\nServices\\n\\nMy Docker Images\\n\\nresources efficiently, to s\", \"et the constraints provided by the user, to efficiently load-balance containers across nodes or host\", \"s, and to be robust against errors while providing high availability.\\n\\nThe concepts of a cluster and\", \" a scheduler are closely related, so the products provided by different vendors often provide both s\", \"ets of capabilities. The following list shows the most important platform and software choices you h\", \"ave for clusters and schedulers. These orchestrators are generally offered in public clouds like Azu\", \"re.\\n\\n## Software platforms for container clustering, orchestration, and scheduling\\n\\n<!-- image -->\\n\\n\", \"<!-- image -->\\n\\n<!-- image -->\\n\\n| Platform                       | Description                      \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"                                                 |\\n|--------------------------------|---------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"--------------------------------------------------------------------|\\n| Kubernetes                  \", \"   | Kubernetes is an open-source  product that provides functionality  that ranges from cluster  in\", \"frastructure and container  scheduling to orchestrating  capabilities. It lets you automate  deploym\", \"ent, scaling, and  operations of application  containers across clusters of hosts. Kubernetes provid\", \"es a container\\u0002 centric infrastructure that groups  application containers into logical  units for e\", \"asy management and  discovery. Kubernetes is mature in Linux, less  mature in Windows. |\\n| Azure Kub\", \"ernetes Service (AKS) | AKS is a managed Kubernetes  container orchestration service in  Azure that \", \"simplifies Kubernetes  cluster\\u2019s management, deployment,  and operations.                           \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"      |\\n| Azure Container Apps           | Azure Container Apps is a managed  serverless container s\", \"ervice for  building and deploying modern  apps at scale.                                           \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"                         |\\n\\n## Using container-based orchestrators in Microsoft Azure\\n\\nSeveral cloud\", \" vendors offer Docker containers support plus Docker clusters and orchestration support, including M\", \"icrosoft Azure, Amazon EC2 Container Service, and Google Container Engine. Microsoft Azure provides \", \"Docker cluster and orchestrator support through Azure Kubernetes Service (AKS).\\n\\nAKS\\n\\nKubernetes\\n\\nNo\", \"de\\n\\nNode\\n\\nMaster Node\\n\\n## Using Azure Kubernetes Service DNS\\n\\nA Kubernetes cluster pools multiple Do\", \"cker hosts and exposes them as a single virtual Docker host, so you can deploy multiple containers i\", \"nto the cluster and scale-out with any number of container instances. The cluster will handle all th\", \"e complex management plumbing, like scalability, health, and so forth.\\n\\nAKS provides a way to simpli\", \"fy the creation, configuration, and management of a cluster of virtual machines in Azure that are pr\", \"econfigured to run containerized applications. Using an optimized configuration of popular open-sour\", \"ce scheduling and orchestration tools, AKS enables you to use your existing skills or draw on a larg\", \"e and growing body of community expertise to deploy and manage container-based applications on Micro\", \"soft Azure.\\n\\nAzure Kubernetes Service optimizes the configuration of popular Docker clustering open-\", \"source tools and technologies specifically for Azure. You get an open solution that offers portabili\", \"ty for both your containers and your application configuration. You select the size, the number of h\", \"osts, and the orchestrator tools, and AKS handles everything else.\\n\\nFigure 4 -24. Kubernetes cluster\", \"'s simplified structure and topology\\n\\n<!-- image -->\\n\\nPC/Development Environment\\n\\nWindows 10 or macO\", \"S\\n\\nDocker Registry\\n\\nDocker Hub\\n\\nOr ACR\\n\\nIn figure 4-24, you can see the structure of a Kubernetes cl\", \"uster where a master node (VM) controls most of the coordination of the cluster and you can deploy c\", \"ontainers to the rest of the nodes, which are managed as a single pool from an application point of \", \"view and allows you to scale to thousands or even tens of thousands of containers. Docker Image Sche\", \"dulee\\n\\nRepository\\n\\n## Development environment for Kubernetes\\n\\nKubernetes cluster\\n\\nIn the development\", \" environment, Docker announced in July 2018 that Kubernetes can also run in a single development mac\", \"hine (Windows 10 or macOS) by installing Docker Desktop. You can later deploy to the cloud (AKS) for\", \" further integration tests, as shown in figure 4-25.\\n\\nFigure 4 -25. Running Kubernetes in dev machin\", \"e and the cloud\\n\\n<!-- image -->\\n\\n## Getting started with Azure Kubernetes Service (AKS)\\n\\nTo begin us\", \"ing AKS, you deploy an AKS cluster from the Azure portal or by using the CLI. For more information o\", \"n deploying a Kubernetes cluster in Azure, see Deploy an Azure Kubernetes Service (AKS) cluster .\\n\\nT\", \"here are no fees for any of the software installed by default as part of AKS. All default options ar\", \"e implemented with open-source software. AKS is available for multiple virtual machines in Azure. Yo\", \"u're charged only for the compute instances you choose, and the other underlying infrastructure reso\", \"urces consumed, such as storage and networking. There are no incremental charges for AKS itself.\\n\\nTh\", \"e default production deployment option for Kubernetes is to use Helm charts, which are introduced in\", \" the next section.\\n\\nA Azure\\n\\nAzure Kubernetes Service (AKS)\\n\\n\\\"Managed Kubernetes\\\" for production\\n\\nNo\", \"de\\n\\n## Deploy with Helm charts into Kubernetes clusters\\n\\nWhen deploying an application to a Kubernet\", \"es cluster, you can use the original kubectl.exe CLI tool using deployment files based on the native\", \" format (.yaml files), as already mentioned in the previous section. However, for more complex Kuber\", \"netes applications such as when deploying complex microservice -based applications, it's recommended\", \" to use Helm .\\n\\nHelm Charts helps you define, version, install, share, upgrade, or rollback even the\", \" most complex Kubernetes application.\\n\\nGoing further, Helm usage is also recommended because other K\", \"ubernetes environments in Azure, such as Azure Dev Spaces are also based on Helm charts.\\n\\nHelm is ma\", \"intained by the Cloud Native Computing Foundation (CNCF) in collaboration with Microsoft, Google, Bi\", \"tnami, and the Helm contributor community.\\n\\nFor more implementation information on Helm charts and K\", \"ubernetes, see the Using Helm Charts to deploy eShopOnContainers to AKS post.\\n\\n## Additional resourc\", \"es\\n\\n- Getting started with Azure Kubernetes Service (AKS) https://learn.microsoft.com/azure/aks/kube\", \"rnetes-walkthrough-portal\\n- Azure Dev Spaces https://learn.microsoft.com/azure/dev-spaces/azure-dev-\", \"spaces\\n- Kubernetes The official site. https://kubernetes.io/\\n\\n## Development process for Docker -ba\", \"sed applications\\n\\nDevelop containerized .NET applications the way you like, either Integrated Develo\", \"pment Environment (IDE) focused with Visual Studio and Visual Studio tools for Docker or CLI/Editor \", \"focused with Docker CLI and Visual Studio Code.\\n\\n## Development environment for Docker apps\\n\\n## Deve\", \"lopment tool choices: IDE or editor\\n\\nWhether you prefer a full and powerful IDE or a lightweight and\", \" agile editor, Microsoft has tools that you can use for developing Docker applications.\\n\\nVisual Stud\", \"io (for Windows). Docker-based .NET 7 application development with Visual Studio requires Visual Stu\", \"dio 2022 version 17.0 or later. Visual Studio 2022 comes with tools for Docker already built in. The\", \" tools for Docker let you develop, run, and validate your applications directly in the target Docker\", \" environment. You can press F5 to run and debug your application (single container or multiple conta\", \"iners) directly into a Docker host, or press CTRL + F5 to edit and refresh your application without \", \"having to rebuild the container. This IDE is the most powerful development choice for Docker -based \", \"apps.\\n\\nVisual Studio for Mac. It's an IDE, evolution of Xamarin Studio, running in macOS. This tool \", \"should be the preferred choice for developers working in macOS machines who also want to use a power\", \"ful IDE.\\n\\nVisual Studio Code and Docker CLI. If you prefer a lightweight and cross-platform editor t\", \"hat supports any development language, you can use Visual Studio Code and the Docker CLI. This IDE i\", \"s a cross-platform development approach for macOS, Linux, and Windows. Additionally, Visual Studio C\", \"ode supports extensions for Docker such as IntelliSense for Dockerfiles and shortcut tasks to run Do\", \"cker commands from the editor.\\n\\nBy installing Docker Desktop, you can use a single Docker CLI to bui\", \"ld apps for both Windows and Linux.\\n\\n## Additional resources\\n\\n- Visual Studio. Official site. https:\", \"//visualstudio.microsoft.com/vs/\\n- Visual Studio Code. Official site. https://code.visualstudio.com/\", \"download\\n- Docker Desktop for Windows https://hub.docker.com/editions/community/docker-ce-desktop-wi\", \"ndows\\n- Docker Desktop for Mac https://hub.docker.com/editions/community/docker-ce-desktop-mac\\n\\n## .\", \"NET languages and frameworks for Docker containers\\n\\nAs mentioned in earlier sections of this guide, \", \"you can use .NET Framework, .NET 7, or the opensource Mono project when developing Docker containeri\", \"zed .NET applications. You can develop in C#, F#, or Visual Basic when targeting Linux or Windows Co\", \"ntainers, depending on which .NET framework is in use. For more details about.NET languages, see the\", \" blog post The .NET Language Strategy .\\n\\n## Development workflow for Docker apps\\n\\nThe application de\", \"velopment life cycle starts at your computer, as a developer, where you code the application using y\", \"our preferred language and test it locally. With this workflow, no matter which language, framework,\", \" and platform you choose, you're always developing and testing Docker containers, but doing so local\", \"ly.\\n\\nEach container (an instance of a Docker image) includes the following components:\\n\\n- An operati\", \"ng system selection, for example, a Linux distribution, Windows Nano Server, or Windows Server Core.\", \"\\n- Files added during development, for example, source code and application binaries.\\n- Configuratio\", \"n information, such as environment settings and dependencies.\\n\\n## Workflow for developing Docker con\", \"tainer-based applications\\n\\nThis section describes the inner -loop development workflow for Docker co\", \"ntainer-based applications. The inner -loop workflow means it's not considering the broader DevOps w\", \"orkflow, which can include up to production deployment, and just focuses on the development work don\", \"e on the developer's computer. The initial steps to set up the environment aren't included, since th\", \"ose steps are done only once.\\n\\nInner-Loop development workflow for Docker apps\\n\\n3\\n\\nCreate Images det\", \"ined at\\n\\nWrite\\n\\nDockerfile/s\\n\\n4. (Opt.in)\\n\\nDefine services by writing\\n\\nRun\\n\\nContainers /\\n\\nTest\\n\\nAn a\", \"pplication is composed of your own services plus additional libraries (dependencies). The following \", \"are the basic steps you usually take when building a Docker application, as illustrated in Figure 5-\", \"1. docker build f docker run / Docker-compose up http access... VM\\n\\n## Base Images My Images My Cont\", \"ainer 2 Container 2)\\n\\nRemote\\n\\nDocker Registry\\n\\nLocal\\n\\nDocker\\n\\n<!-- image -->\\n\\nCode your app\\n\\n=\\n\\ngit \", \"push\\n\\n7.\\n\\nPush or\\n\\nContinue developing\\n\\nFigure 5 -1. Step -by -step workflow for developing Docker c\", \"ontainerized apps\\n\\nIn this section, this whole process is detailed and every major step is explained\", \" by focusing on a Visual Studio environment.\\n\\nWhen you're using an editor/CLI development approach (\", \"for example, Visual Studio Code plus Docker CLI on macOS or Windows), you need to know every step, g\", \"enerally in more detail than if you're using Visual Studio. For more information about working in a \", \"CLI environment, see the e-book Containerized Docker Application lifecycle with Microsoft Platforms \", \"and Tools .\\n\\nWhen you're using Visual Studio 2022, many of those steps are handled for you, which dr\", \"amatically improves your productivity. This is especially true when you're using Visual Studio 2022 \", \"and targeting multi -container applications. For instance, with just one mouse click, Visual Studio \", \"adds the Dockerfile and docker -compose.yml file to your projects with the configuration for your ap\", \"plication. When you run the application in Visual Studio, it builds the Docker image and runs the mu\", \"lti-container application directly in Docker; it even allows you to debug several containers at once\", \". These features will boost your development speed.\\n\\nHowever, just because Visual Studio makes those\", \" steps automatic doesn't mean that you don't need to know what's going on underneath with Docker. Th\", \"erefore, the following guidance details every step.\\n\\nLiAuAl ChuA:\\n\\nLand\\n\\nInstalling - Visual Studio \", \"Enterprise 2022 - 17.0.1\\n\\nCode\\n\\nWorkloads your app\\n\\nIndividual components\\n\\nLanguage packs\\n\\nWeb &amp;\", \" Cloud (4)\\n\\nASP.NET and web development\\n\\nBuild web applications using ASP.NET Core, ASP.NET,\\n\\nHTML/J\", \"avaScript, and Containers including Docker supp...\\n\\nPython development\\n\\nEditing, debugging, interact\", \"ive development and source control for Python.\\n\\n<!-- image -->\\n\\nDesktop &amp; Mobile (5)\\n\\n## Step 1.\", \" Start coding and create your initial application or service baseline\\n\\nDesktop development with C++\\n\", \"\\nUniversal Windows Platform development\\n\\nDeveloping a Docker application is similar to the way you d\", \"evelop an application without Docker. The difference is that while developing for Docker, you're dep\", \"loying and testing your application or services running within Docker containers in your local envir\", \"onment (either a Linux VM setup by Docker or directly Windows if using Windows Containers). Install \", \"while downloadingInstall\\n\\n## Set up your local environment with Visual Studio\\n\\nTo begin, make sure y\", \"ou have Docker Desktop for Windows for Windows installed, as explained in the following instructions\", \":\\n\\n## Get started with Docker Desktop for Windows\\n\\nIn addition, you need Visual Studio 2022 version \", \"17.0, with the .ASP.NET and web development workload installed, as shown in Figure 5-2.\\n\\nFigure 5 -2\", \". Selecting the ASP.NET and web development workload during Visual Studio 2022 setup\\n\\n<!-- image -->\", \"\\n\\nYou can start coding your application in plain .NET (usually in .NET Core or later if you're plann\", \"ing to use containers) even before enabling Docker in your application and deploying and testing in \", \"Docker. However, it is recommended that you start working on Docker as soon as possible, because tha\", \"t will be the real environment and any issues can be discovered as soon as possible. This is encoura\", \"ged\\n\\nLocation\\n\\nInstallation locations\\n\\nAzure development\\n\\nAzure SKs, tools, and projects for develop\", \"ing cloud apps and creating resources using .NET and .NET Framework.....\\n\\nNode.js development\\n\\nBuild\", \" scalable network applications using Node.js, an asynchronous event-driven JavaScript runtime.\\n\\n*\\n\\nI\", \"nstallation details\\n\\n\\u2022 Visual Studio core editor\\n\\n\\u2022 ASP.NET and web development\\n\\n\\u2022 Individual compon\", \"ents\\n\\nNET 6.0 Runtime\\n\\n- \\u2022\\n\\nX\\n\\nbecause Visual Studio makes it so easy to work with Docker that it al\", \"most feels transparent\\u2014the best example when debugging multi-container applications from Visual Stud\", \"io.\\n\\n## Additional resources\\n\\n- Get started with Docker Desktop for Windows https://docs.docker.com/\", \"docker-for-windows/\\n- Visual Studio 2022 https://visualstudio.microsoft.com/downloads/\\n\\n<!-- image -\", \"->\\n\\n## Step 2. Create a Dockerfile related to an existing .NET base image\\n\\nYou need a Dockerfile for\", \" each custom image you want to build; you also need a Dockerfile for each container to be deployed, \", \"whether you deploy automatically from Visual Studio or manually using the Docker CLI (docker run and\", \" docker-compose commands). If your application contains a single custom service, you need a single D\", \"ockerfile. If your application contains multiple services (as in a microservices architecture), you \", \"need one Dockerfile for each service.\\n\\nThe Dockerfile is placed in the root folder of your applicati\", \"on or service. It contains the commands that tell Docker how to set up and run your application or s\", \"ervice in a container. You can manually create a Dockerfile in code and add it to your project along\", \" with your .NET dependencies.\\n\\nWith Visual Studio and its tools for Docker, this task requires only \", \"a few mouse clicks. When you create a new project in Visual Studio 2022, there's an option named Ena\", \"ble Docker Support, as shown in Figure 5-3.\\n\\nNew Item...\\n\\nAdditional information\\n\\nExisting Item...\\n\\n\", \"New Scaffolded Item...\\n\\nASP.NET Core Web API\\n\\nNew Folder\\n\\nFramework\\n\\nContainer Orchestrator Support.\", \"..\\n\\n\\u2022NET 6.0 (Long-term support)\\n\\nDocker Support...\\n\\nAuthentication type\\n\\nNone\\n\\nApplication Insights\", \" Telemetry...\\n\\n\\u2022 Configure for HTTPS O\\n\\nClient-Side Library...\\n\\nL Enable Docker O\\n\\nDocker OS O\\n\\nNew \", \"Azure WebJob Project\\n\\nExisting Project as Azure WebJob\\n\\nLinux\\n\\nI Use controllers (uncheck to use min\", \"imal APIs) O\\n\\nReference...\\n\\nEnable OpenAPI support O\\n\\nService Reference...\\n\\nConnected Service\\n\\nClass\", \"...\\n\\nC#\\n\\nLinux macos\\n\\nCtrl+Shift+A\\n\\nShift+Alt+A\\n\\nWindows\\n\\nCloud\\n\\nAdd\\n\\nManage NuGet Packages...\\n\\nMana\", \"ge Client-Side Libraries...\\n\\nService\\n\\nWeb\\n\\nManage User Secrets\\n\\nFigure 5 -3. Enabling Docker Support\", \" when creating a new ASP.NET Core project in Visual Studio 2022\\n\\n<!-- image -->\\n\\nYou can also enable\", \" Docker support on an existing ASP.NET Core web app project by right-clicking the project in Solutio\", \"n Explorer and selecting Add &gt; Docker Support\\u2026, as shown in Figure 5-4.\\n\\nFigure 5 -4. Enabling Do\", \"cker support in an existing Visual Studio 2022 project\\n\\n<!-- image -->\\n\\nThis action adds a Dockerfil\", \"e to the project with the required configuration, and is only available on ASP.NET Core projects.\\n\\nI\", \"n a similar fashion, Visual Studio can also add a docker -compose.yml file for the whole solution wi\", \"th the option Add &gt; Container Orchestrator Support\\u2026. In step 4, we'll explore this option in grea\", \"ter detail.\\n\\n## Using an existing official .NET Docker image\\n\\nYou usually build a custom image for y\", \"our container on top of a base image you get from an official repository like the Docker Hub registr\", \"y. That is precisely what happens under the covers when you enable Docker support in Visual Studio. \", \"Your Dockerfile will use an existing dotnet/core/aspnet image.\\n\\nEarlier we explained which Docker im\", \"ages and repos you can use, depending on the framework and OS you have chosen. For instance, if you \", \"want to use ASP.NET Core (Linux or Windows), the image to use is mcr.microsoft.com/dotnet/aspnet:7.0\", \". Therefore, you just need to specify what base Docker image you will use for your container. You do\", \" that by adding FROM\\n\\nmcr.microsoft.com/dotnet/aspnet:7.0 to your Dockerfile. This will be automatic\", \"ally performed by Visual Studio, but if you were to update the version, you update this value.\\n\\nUsin\", \"g an official .NET image repository from Docker Hub with a version number ensures that the same lang\", \"uage features are available on all machines (including development, testing, and production).\\n\\nThe f\", \"ollowing example shows a sample Dockerfile for an ASP.NET Core container.\\n\\n```\\nFROM mcr.microsoft.co\", \"m/dotnet/aspnet:7.0 ARG source WORKDIR /app EXPOSE 80 COPY ${source:-obj/Docker/publish} . ENTRYPOIN\", \"T [\\\"dotnet\\\" , \\\" MySingleContainerWebApp.dll \\\"]\\n```\\n\\nIn this case, the image is based on version 7.0 \", \"of the official ASP.NET Core Docker image (multi-arch for Linux and Windows). This is the setting FR\", \"OM mcr.microsoft.com/dotnet/aspnet:7.0. (For more information about this base image, see the ASP.NET\", \" Core Docker Image page.) In the Dockerfile, you also need to instruct Docker to listen on the TCP p\", \"ort you will use at runtime (in this case, port 80, as configured with the EXPOSE setting).\\n\\nYou can\", \" specify additional configuration settings in the Dockerfile, depending on the language and framewor\", \"k you're using. For instance, the ENTRYPOINT line with [\\\"dotnet\\\",\\n\\n\\\"MySingleContainerWebApp.dll\\\"] te\", \"lls Docker to run a .NET application. If you're using the SDK and the .NET CLI (dotnet CLI) to build\", \" and run the .NET application, this setting would be different. The bottom line is that the ENTRYPOI\", \"NT line and other settings will be different depending on the language and platform you choose for y\", \"our application.\\n\\n## Additional resources\\n\\n- Building Docker Images for ASP.NET Core Applications ht\", \"tps://learn.microsoft.com/dotnet/core/docker/building-net-docker-images\\n\\n- Build your own image. In \", \"the official Docker documentation. https://docs.docker.com/engine/tutorials/dockerimages/\\n- Staying \", \"up-to-date with .NET Container Images https://devblogs.microsoft.com/dotnet/staying-up-to-date-with-\", \"net-container-images/\\n- Using .NET and Docker Together - DockerCon 2018 Update https://devblogs.micr\", \"osoft.com/dotnet/using-net-and-docker-together-dockercon-2018update/\\n\\n## Using multi-arch image repo\", \"sitories\\n\\nA single repo can contain platform variants, such as a Linux image and a Windows image. Th\", \"is feature allows vendors like Microsoft (base image creators) to create a single repo to cover mult\", \"iple platforms (that is Linux and Windows). For example, the .NET repository available in the Docker\", \" Hub registry provides support for Linux and Windows Nano Server by using the same repo name.\\n\\nIf yo\", \"u specify a tag, targeting a platform that is explicit like in the following cases:\\n\\n- mcr.microsoft\", \".com/dotnet/aspnet:7.0-bullseye-slim Targets: .NET 7 runtime-only on Linux\\n- mcr.microsoft.com/dotne\", \"t/aspnet:7.0-nanoserver-ltsc2022 Targets: .NET 7 runtime-only on Windows Nano Server\\n\\nBut, if you sp\", \"ecify the same image name, even with the same tag, the multi-arch images (like the aspnet image) wil\", \"l use the Linux or Windows version depending on the Docker host OS you're deploying, as shown in the\", \" following example:\\n\\n- mcr.microsoft.com/dotnet/aspnet:7.0 Multi -arch: .NET 7 runtime -only on Linu\", \"x or Windows Nano Server depending on the Docker host OS\\n\\nThis way, when you pull an image from a Wi\", \"ndows host, it will pull the Windows variant, and pulling the same image name from a Linux host will\", \" pull the Linux variant.\\n\\n## Multi -stage builds in Dockerfile\\n\\nThe Dockerfile is similar to a batch\", \" script. Similar to what you would do if you had to set up the machine from the command line.\\n\\nIt st\", \"arts with a base image that sets up the initial context, it's like the startup filesystem, that sits\", \" on top of the host OS. It's not an OS, but you can think of it like \\\"the\\\" OS inside the container.\\n\", \"\\nThe execution of every command line creates a new layer on the filesystem with the changes from the\", \" previous one, so that, when combined, produce the resulting filesystem.\\n\\nSince every new layer \\\"res\", \"ts\\\" on top of the previous one and the resulting image size increases with every command, images can\", \" get very large if they have to include, for example, the SDK needed to build and publish an applica\", \"tion.\\n\\nThis is where multi -stage builds get into the plot (from Docker 17.05 and higher) to do thei\", \"r magic.\\n\\nThe core idea is that you can separate the Dockerfile execution process in stages, where a\", \" stage is an initial image followed by one or more commands, and the last stage determines the final\", \" image size.\\n\\nIn short, multi-stage builds allow splitting the creation in different \\\"phases\\\" and th\", \"en assemble the final image taking only the relevant directories from the intermediate stages. The g\", \"eneral strategy to use this feature is:\\n\\n1. Use a base SDK image (doesn't matter how large), with ev\", \"erything needed to build and publish the application to a folder and then\\n2. Use a base, small, runt\", \"ime-only image and copy the publishing folder from the previous stage to produce a small final image\", \".\\n\\nProbably the best way to understand multi-stage is going through a Dockerfile in detail, line by \", \"line, so let's begin with the initial Dockerfile created by Visual Studio when adding Docker support\", \" to a project and will get into some optimizations later.\\n\\nThe initial Dockerfile might look somethi\", \"ng like this:\\n\\n```\\n1 FROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base 2 WORKDIR /app 3 EXPOSE 80 4 5\", \" FROM mcr.microsoft.com/dotnet/sdk:7.0 AS build 6 WORKDIR /src 7 COPY src/Services/Catalog/Catalog.A\", \"PI/Catalog.API.csproj \\u2026 8 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.AspNetCore.HealthChecks\", \" \\u2026 9 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions.HealthChecks \\u2026 10 COPY src/Buildi\", \"ngBlocks/EventBus/IntegrationEventLogEF/ \\u2026 11 COPY src/BuildingBlocks/EventBus/EventBus/EventBus.csp\", \"roj \\u2026 12 COPY src/BuildingBlocks/EventBus/EventBusRabbitMQ/EventBusRabbitMQ.csproj \\u2026 13 COPY src/Bui\", \"ldingBlocks/EventBus/EventBusServiceBus/EventBusServiceBus.csproj \\u2026 14 COPY src/BuildingBlocks/WebHo\", \"stCustomization/WebHost.Customization \\u2026 15 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extens\", \"ions \\u2026 16 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions \\u2026 17 RUN dotnet restore src/\", \"Services/Catalog/Catalog.API/Catalog.API.csproj 18 COPY . . 19 WORKDIR /src/src/Services/Catalog/Cat\", \"alog.API 20 RUN dotnet build Catalog.API.csproj -c Release -o /app 21 22 FROM build AS publish 23 RU\", \"N dotnet publish Catalog.API.csproj -c Release -o /app 24 25 FROM base AS final 26 WORKDIR /app 27 C\", \"OPY --from=publish /app . 28 ENTRYPOINT [\\\"dotnet\\\" , \\\"Catalog.API.dll\\\"]\\n```\\n\\nAnd these are the detail\", \"s, line by line:\\n\\n- Line #1: Begin a stage with a \\\"small\\\" runtime-only base image, call it base for \", \"reference.\\n- Line #2: Create the /app directory in the image.\\n- Line #3: Expose port 80 .\\n\\n- Line #5\", \": Begin a new stage with the \\\"large\\\" image for building/publishing. Call it build for reference.\\n- L\", \"ine #6: Create directory /src in the image.\\n- Line #7: Up to line 16, copy referenced .csproj projec\", \"t files to be able to restore packages later.\\n- Line #17: Restore packages for the Catalog.API proje\", \"ct and the referenced projects.\\n- Line #18: Copy all directory tree for the solution (except the fil\", \"es/directories included in the .dockerignore file) to the /src directory in the image.\\n- Line #19: C\", \"hange the current folder to the Catalog.API project.\\n- Line #20: Build the project (and other projec\", \"t dependencies) and output to the /app directory in the image.\\n- Line #22: Begin a new stage continu\", \"ing from the build. Call it publish for reference.\\n- Line #23: Publish the project (and dependencies\", \") and output to the /app directory in the image.\\n- Line #25: Begin a new stage continuing from base \", \"and call it final .\\n- Line #26: Change the current directory to /app .\\n- Line #27: Copy the /app dir\", \"ectory from stage publish to the current directory.\\n- Line #28: Define the command to run when the c\", \"ontainer is started.\\n\\nNow let's explore some optimizations to improve the whole process performance \", \"that, in the case of eShopOnContainers, means about 22 minutes or more to build the complete solutio\", \"n in Linux containers.\\n\\nYou'll take advantage of Docker's layer cache feature, which is quite simple\", \": if the base image and the commands are the same as some previously executed, it can just use the r\", \"esulting layer without the need to execute the commands, thus saving some time.\\n\\nSo, let's focus on \", \"the build stage, lines 5-6 are mostly the same, but lines 7-17 are different for every service from \", \"eShopOnContainers, so they have to execute every single time, however if you changed lines 7 -16 to:\", \"\\n\\n## COPY . .\\n\\nThen it would be just the same for every service, it would copy the whole solution an\", \"d would create a larger layer but:\\n\\n1. The copy process would only be executed the first time (and w\", \"hen rebuilding if a file is changed) and would use the cache for all other services and\\n2. Since the\", \" larger image occurs in an intermediate stage, it doesn't affect the final image size.\\n\\nThe next sig\", \"nificant optimization involves the restore command executed in line 17, which is also different for \", \"every service of eShopOnContainers. If you change that line to just:\\n\\nRUN dotnet restore\\n\\nIt would r\", \"estore the packages for the whole solution, but then again, it would do it just once, instead of the\", \" 15 times with the current strategy.\\n\\nHowever, dotnet restore only runs if there's a single project \", \"or solution file in the folder, so achieving this is a bit more complicated and the way to solve it,\", \" without getting into too many details, is this:\\n\\n1. Add the following lines to .dockerignore:\\n2. \\u2013 \", \"*.sln, to ignore all solution files in the main folder tree\\n3. \\u2013 !eShopOnContainers-ServicesAndWebAp\", \"ps.sln, to include only this solution file.\\n2. Include the /ignoreprojectextensions:.dcproj argument\", \" to dotnet restore, so it also ignores the docker -compose project and only restores the packages fo\", \"r the eShopOnContainersServicesAndWebApps solution.\\n\\nFor the final optimization, it just happens tha\", \"t line 20 is redundant, as line 23 also builds the application and comes, in essence, right after li\", \"ne 20, so there goes another time-consuming command.\\n\\nThe resulting file is then:\\n\\n```\\n1 FROM mcr.mi\", \"crosoft.com/dotnet/aspnet:7.0 AS base 2 WORKDIR /app 3 EXPOSE 80 4 5 FROM mcr.microsoft.com/dotnet/s\", \"dk:7.0 AS publish 6 WORKDIR /src 7 COPY . . 8 RUN dotnet restore /ignoreprojectextensions:.dcproj 9 \", \"WORKDIR /src/src/Services/Catalog/Catalog.API 10 RUN dotnet publish Catalog.API.csproj -c Release -o\", \" /app 11 12 FROM base AS final 13 WORKDIR /app 14 COPY --from=publish /app . 15 ENTRYPOINT [\\\"dotnet\\\"\", \" , \\\"Catalog.API.dll\\\"]\\n```\\n\\n## Creating your base image from scratch\\n\\nYou can create your own Docker \", \"base image from scratch. This scenario is not recommended for someone who is starting with Docker, b\", \"ut if you want to set the specific bits of your own base image, you can do so.\\n\\n## Additional resour\", \"ces\\n\\n- Multi -arch .NET Core images .\\n\\nhttps://github.com/dotnet/announcements/issues/14\\n\\nPS C: \\\\dev\", \"\\\\netcore-webapi-microservice-docker&gt; docker build -t cesardl/netcore-webapi-microservice-docker:f\", \"irst \\u2022\\n\\n3.\\n\\nStep 1 : FROM microsoft/dotnet: latest\\n\\nSending build context to Docker\\\\_daemon 1.148 MB\", \"\\n\\nlatest: Pulling from microsoft/dotnet\\n\\n5c90d4a2d1a8: Downloading\\n\\nCreate Images ab30c63719b1: Down\", \"loading\\n\\ndefined at c6072700a242: Downloading\\n\\nDockerfile/s eb57cf4f29ee: Waiting\\n\\n121d7eef6c20: wai\", \"ting b2c5ae2d325b: waiting\\n\\n- Create a base image. Official Docker documentation. https://docs.docke\", \"r.com/develop/develop-images/baseimages/\\n\\n<!-- image -->\\n\\n## Step 3. Create your custom Docker image\", \"s and embed your application or service in them\\n\\nFor each service in your application, you need to c\", \"reate a related image. If your application is made up of a single service or web application, you ju\", \"st need a single image.\\n\\nNote that the Docker images are built automatically for you in Visual Studi\", \"o. The following steps are only needed for the editor/CLI workflow and explained for clarity about w\", \"hat happens underneath.\\n\\nYou, as a developer, need to develop and test locally until you push a comp\", \"leted feature or change to your source control system (for example, to GitHub). This means that you \", \"need to create the Docker images and deploy containers to a local Docker host (Windows or Linux VM) \", \"and run, test, and debug against those local containers.\\n\\nTo create a custom image in your local env\", \"ironment by using Docker CLI and your Dockerfile, you can use the docker build command, as in Figure\", \" 5-5.\\n\\nFigure 5 -5. Creating a custom Docker image\\n\\n<!-- image -->\\n\\nOptionally, instead of directly \", \"running docker build from the project folder, you can first generate a deployable folder with the re\", \"quired .NET libraries and binaries by running dotnet publish, and then use the docker build command.\", \"\\n\\nThis will create a Docker image with the name cesardl/netcore-webapi-microservice-docker:first. In\", \" this case, :first is a tag that represents a specific version. You can repeat this step for each cu\", \"stom image you need to create for your composed Docker application.\\n\\nWhen an application is made of \", \"multiple containers (that is, it is a multi-container application), you can also use the docker -com\", \"pose up --build command to build all the related images with a single command by using the metadata \", \"exposed in the related docker-compose.yml files.\\n\\nYou can find the existing images in your local rep\", \"ository by using the docker images command, as shown in Figure 5-6.\\n\\n===&gt;\\n\\n18.34 MB/51.35\\n\\n18.34 \", \"MB/42.53\\n\\n18.48 MB/18.55\\n\\nMB\\n\\nMB\\n\\nPS C: \\\\dev\\\\netcore-webapi-microservice-docker&gt; docker images\\n\\nR\", \"EPOSITORY\\n\\n4. (opt)\\n\\nmicrosoft/dotnet\\n\\nDefine services hello-world\\n\\ncesardl/netcore-webapi-microserv\", \"ice-docker ubuntu\\n\\nby writing docker-compose.yml\\n\\nTAG\\n\\nfirst latest\\n\\nlatest latest\\n\\nIMAGE ID\\n\\n384c4a\", \"c1809b\\n\\n49aaf5daa850\\n\\nc54a2cc56cbb cf62323 fa025\\n\\nCREATED\\n\\n30 hours ago\\n\\n4 minutes ago ago\\n\\n12 days \", \"ago\\n\\n5 days\\n\\nSIZE\\n\\n548.6 MB\\n\\n579.8 MB\\n\\n125 MB\\n\\n1.848 kB\\n\\nFigure 5 -6. Viewing existing images using \", \"the docker images command\\n\\n<!-- image -->\\n\\n## Creating Docker images with Visual Studio\\n\\nWhen you us\", \"e Visual Studio to create a project with Docker support, you don't explicitly create an image. Inste\", \"ad, the image is created for you when you press F5 (or Ctrl+F5) to run the dockerized application or\", \" service. This step is automatic in Visual Studio and you won't see it happen, but it's important th\", \"at you know what's going on underneath.\\n\\n<!-- image -->\\n\\n## Step 4. Define your services in docker-c\", \"ompose.yml when building a multi -container Docker application\\n\\nThe docker -compose.yml file lets yo\", \"u define a set of related services to be deployed as a composed application with deployment commands\", \". It also configures its dependency relations and runtime configuration.\\n\\nTo use a docker -compose.y\", \"ml file, you need to create the file in your main or root solution folder, with content similar to t\", \"hat in the following example:\\n\\n```\\nversion: '3.4' services: webmvc: image: eshop/web environment: -C\", \"atalogUrl=http://catalog-api -OrderingUrl=http://ordering-api ports: -\\\"80:80\\\" depends_on: -catalog-a\", \"pi -ordering-api catalog-api: image: eshop/catalog-api environment: -ConnectionString=Server=sqldata\", \";Port=1433;Database=CatalogDB;\\u2026 ports: -\\\"81:80\\\" depends_on: -sqldata ordering-api: image: eshop/orde\", \"ring-api\\n```\\n\\n```\\nenvironment: -ConnectionString=Server=sqldata;Database=OrderingDb;\\u2026 ports: -\\\"82:80\", \"\\\" extra_hosts: -\\\"CESARDLBOOKVHD:10.0.75.1\\\" depends_on: -sqldata sqldata: image: mcr.microsoft.com/ms\", \"sql/server:latest environment: -SA_PASSWORD=Pass@word -ACCEPT_EULA=Y ports: -\\\"5433:1433\\\"\\n```\\n\\nThis d\", \"ocker -compose.yml file is a simplified and merged version. It contains static configuration data fo\", \"r each container (like the name of the custom image), which is always required, and configuration in\", \"formation that might depend on the deployment environment, like the connection string. In later sect\", \"ions, you will learn how to split the docker-compose.yml configuration into multiple dockercompose f\", \"iles and override values depending on the environment and execution type (debug or release).\\n\\nThe do\", \"cker -compose.yml file example defines four services: the webmvc service (a web application), two mi\", \"croservices (ordering-api and basket-api), and one data source container, sqldata, based on SQL Serv\", \"er for Linux running as a container. Each service will be deployed as a container, so a Docker image\", \" is required for each.\\n\\nThe docker -compose.yml file specifies not only what containers are being us\", \"ed, but how they are individually configured. For instance, the webmvc container definition in the .\", \"yml file:\\n\\n- Uses a pre -built eshop/web:latest image. However, you could also configure the image t\", \"o be built as part of the docker-compose execution with an additional configuration based on a build\", \": section in the docker -compose file.\\n- Initializes two environment variables (CatalogUrl and Order\", \"ingUrl).\\n- Forwards the exposed port 80 on the container to the external port 80 on the host machine\", \".\\n- Links the web app to the catalog and ordering service with the depends\\\\_on setting. This causes \", \"the service to wait until those services are started.\\n\\nWe will revisit the docker -compose.yml file \", \"in a later section when we cover how to implement microservices and multi -container apps.\\n\\n## Worki\", \"ng with docker-compose.yml in Visual Studio 2022\\n\\nBesides adding a Dockerfile to a project, as we me\", \"ntioned before, Visual Studio 2017 (from version 15.8 on) can add orchestrator support for Docker Co\", \"mpose to a solution.\\n\\nWhen you add container orchestrator support, as shown in Figure 5-7, for the f\", \"irst time, Visual Studio creates the Dockerfile for the project and creates a new (service section) \", \"project in your solution with\\n\\n1 E docker-compose\\n\\n\\u2022 .dockerignore\\n\\nNew Item...\\n\\nExisting Item...\\n\\nV\", \"i docker-compose.yml\\n\\nNew Scaffolded Item...\\n\\nCtrI+Shift+A\\n\\nShift+Alt+A\\n\\n\\u2022 docker-compose.override.y\", \"ml\\n\\nNew Folder\\n\\n1\\n\\nAdd\\n\\nManage NuGet Package\\n\\nManage Client-Side Libraries...\\n\\nseveral global docker\", \"-compose*.yml files, and then adds the project to those files. You can then open the docker -compose\", \".yml files and update them with additional features.\\n\\nRepeat this operation for every project you wa\", \"nt to include in the docker-compose.yml file.\\n\\nAt the time of this writing, Visual Studio supports D\", \"ocker Compose orchestrators.\\n\\nNew Azure Weblob Project\\n\\nExisting Project as Azure WebJob\\n\\nReference.\", \"..\\n\\nService Reference...\\n\\nConnected Service\\n\\nClass...\\n\\nFigure 5 -7. Adding Docker support in Visual \", \"Studio 2022 by right -clicking an ASP.NET Core project\\n\\n<!-- image -->\\n\\nAfter you add orchestrator s\", \"upport to your solution in Visual Studio, you will also see a new node (in the docker -compose.dcpro\", \"j project file) in Solution Explorer that contains the added dockercompose.yml files, as shown in Fi\", \"gure 5-8.\\n\\nFigure 5 -8. The docker -compose tree node added in Visual Studio 2022 Solution Explorer\\n\", \"\\n<!-- image -->\\n\\nYou could deploy a multi-container application with a single docker-compose.yml fil\", \"e by using the docker -compose up command. However, Visual Studio adds a group of them so you can ov\", \"erride values depending on the environment (development or production) and execution type (release o\", \"r debug). This capability will be explained in later sections.\\n\\nPS C: dev netcore-webapi-microservic\", \"e-docker\\u00bb docker run -t -d i\\n\\nRun\\n\\n5.\\n\\nd96975a683b0a9411595816f63be6c135801878b8a85181a4d86dc848ea4c\", \"a6\\\"P 80:5000 cesardl/netcore-webapi-microservice-docker:first\\n\\nContainers /\\n\\nCompose app\\n\\n<!-- image\", \" -->\\n\\n## Step 5. Build and run your Docker application\\n\\nIf your application only has a single contai\", \"ner, you can run it by deploying it to your Docker host (VM or physical server). However, if your ap\", \"plication contains multiple services, you can deploy it as a composed application, either using a si\", \"ngle CLI command (docker-compose up), or with Visual Studio, which will use that command under the c\", \"overs. Let's look at the different options.\\n\\n## Option A: Running a single-container application\\n\\n##\", \" Using Docker CLI\\n\\nYou can run a Docker container using the docker run command, as shown in Figure 5\", \"-9:\\n\\ndocker run -t -d -p 80:5000 cesardl/netcore-webapi-microservice-docker:first\\n\\nThe above command\", \" will create a new container instance from the specified image, every time it's run. You can use the\", \" --name parameter to give a name to the container and then use docker start {name} (or use the conta\", \"iner ID or automatic name) to run an existing container instance.\\n\\nFigure 5 -9. Running a Docker con\", \"tainer using the docker run command\\n\\nIn this case, the command binds the internal port 5000 of the c\", \"ontainer to port 80 of the host machine. This means that the host is listening on port 80 and forwar\", \"ding to port 5000 on the container.\\n\\nThe hash shown is the container ID and it's also assigned a ran\", \"dom readable name if the --name option is not used.\\n\\n## Using Visual Studio\\n\\nIf you haven't added co\", \"ntainer orchestrator support, you can also run a single container app in Visual Studio by pressing C\", \"trl+F5 and you can also use F5 to debug the application within the container. The container runs loc\", \"ally using docker run.\\n\\n## Option B: Running a multi-container application\\n\\nIn most enterprise scena\", \"rios, a Docker application will be composed of multiple services, which means you need to run a mult\", \"i-container application, as shown in Figure 5-10.\\n\\nPS C: \\\\Dev\\\\webApplication\\u00bb docker-compose up\\n\\nRec\", \"reating webapplication\\\\_webapplication\\\\_1\\n\\nAttaching to webapplication\\\\_webapplication\\\\_1\\n\\nwebapplic\", \"ation\\\\_1\\n\\nwebapplication\\\\_1\\n\\nwebapplication\\\\_1\\n\\nVM\\n\\nhosting environment: Production\\n\\nContent root pa\", \"th: /app\\n\\nApplication started. Press Ctrl+C to shut down.\\n\\nNow listening on: http://*:80\\n\\nMy webappl\", \"ication\\\\_1\\n\\n<!-- image -->\\n\\nContainer | Container 2\\n\\nFigure 5 -10. VM with Docker containers deploye\", \"d\\n\\n## Using Docker CLI\\n\\nTo run a multi -container application with the Docker CLI, you use the docke\", \"r-compose up command. This command uses the docker -compose.yml file that you have at the solution l\", \"evel to deploy a multi -container application. Figure 5-11 shows the results when running the comman\", \"d from your main solution directory, which contains the docker-compose.yml file.\\n\\nFigure 5 -11. Exam\", \"ple results when running the docker -compose up command\\n\\n<!-- image -->\\n\\nAfter the docker -compose u\", \"p command runs, the application and its related containers are deployed into your Docker host, as de\", \"picted in Figure 5-10.\\n\\n## Using Visual Studio\\n\\nRunning a multi-container application using Visual S\", \"tudio 2019 can't get any simpler. You just press Ctrl+F5 to run or F5 to debug, as usual, setting up\", \" the docker-compose project as the startup project. Visual Studio handles all needed setup, so you c\", \"an create breakpoints as usual and debug what finally become independent processes running in \\\"remot\", \"e servers\\\", with the debugger already attached, just like that.\\n\\nAs mentioned before, each time you \", \"add Docker solution support to a project within a solution, that project is configured in the global\", \" (solution-level) docker-compose.yml file, which lets you run or debug the whole solution at once. V\", \"isual Studio will start one container for each project that has Docker solution support enabled, and\", \" perform all the internal steps for you (dotnet publish, docker build, etc.).\\n\\nIf you want to take a\", \" peek at all the drudgery, take a look at the file:\\n\\n{root solution folder}\\\\obj\\\\Docker\\\\docker-compos\", \"e.vs.debug.g.yml\\n\\nToolbo\\n\\nFILE EDIT VIEW PROJECT BUILD DEBUG\\n\\n\\u2022 -\\n\\nTEST\\n\\nANALYZE\\n\\n0 \\u2039 \\u2039\\n\\nTest\\n\\nC\\n\\nyo\", \"ur appor microservices\\n\\nDebug\\n\\nAny CPU\\n\\nB localhost/API/values docker-compose.yml \\u2192 \\u00d7 docker-compose\", \".override.yml\\n\\n1 Mercian.\\n\\nThe important point here is that, as shown in Figure 5-12, in Visual Stud\", \"io 2019 there is an additional Docker command for the F5 key action. This option lets you run or deb\", \"ug a multi-container application by running all the containers that are defined in the docker-compos\", \"e.yml files at the solution level. The ability to debug multiple-container solutions means that you \", \"can set several breakpoints, each breakpoint in a different project (container), and while debugging\", \" from Visual Studio you will stop at breakpoints defined in different projects and running on differ\", \"ent containers.\\n\\nFigure 5 -12. Running multi -container apps in Visual Studio 2022\\n\\n<!-- image -->\\n\\n\", \"## Additional resources\\n\\n- Deploy an ASP.NET container to a remote Docker host https://learn.microso\", \"ft.com/visualstudio/containers/hosting-web-apps-in-docker\\n\\n## A note about testing and deploying wit\", \"h orchestrators\\n\\nThe docker -compose up and docker run commands (or running and debugging the contai\", \"ners in Visual Studio) are adequate for testing containers in your development environment. But you \", \"should not use this approach for production deployments, where you should target orchestrators like \", \"Kubernetes or Service Fabric. If you're using Kubernetes, you have to use pods to organize container\", \"s and services to network them. You also use deployments to organize pod creation and modification.\\n\", \"\\n<!-- image -->\\n\\n## Step 6. Test your Docker application using your local Docker host\\n\\nThis step wil\", \"l vary depending on what your application is doing. In a simple .NET Web application that is deploye\", \"d as a single container or service, you can access the service by opening a browser on the Docker ho\", \"st and navigating to that site, as shown in Figure 5-13. (If the configuration in the Dockerfile map\", \"s the container to a port on the host that is anything other than 80, include the host port in the U\", \"RL.)\\n\\nFigure 5 -13. Example of testing your Docker application locally using localhost\\n\\n<!-- image -\", \"->\\n\\nTOOLS\\n\\nEXTENSIONS\\n\\ndocker-compose\\n\\nWINDOW HELP\\n\\nSearch (CtrI+Q)\\n\\n- \\u2022 Docker Compose\\n\\nPS C: \\\\dev \", \"netcore-webapi-microservice-docker&gt; curl http://10.0.75.1/API/values\\n\\nStatusCode\\n\\nContent\\n\\nStatus\", \"Description : oK\\n\\n: 200\\n\\n: [\\\"Howdy!\\\", \\\"Cheers mate!\\\"]\\n\\nRaContent\\n\\n: HTTP/1.1 200 OK\\n\\nIf localhost is\", \" not pointing to the Docker host IP (by default, when using Docker CE, it should), to navigate to yo\", \"ur service, use the IP address of your machine's network card.\\n\\nThis URL in the browser uses port 80\", \" for the particular container example being discussed. However, internally the requests are being re\", \"directed to port 5000, because that was how it was deployed with the docker run command, as explaine\", \"d in a previous step .\\n\\nYou can also test the application using curl from the terminal, as shown in \", \"Figure 5-14. In a Docker installation on Windows, the default Docker Host IP is always 10.0.75.1 in \", \"addition to your machine's actual IP address.\\n\\nFigure 5 -14. Example of testing your Docker applicat\", \"ion locally using curl\\n\\n<!-- image -->\\n\\n## Testing and debugging containers with Visual Studio 2022\\n\", \"\\nWhen running and debugging the containers with Visual Studio 2022, you can debug the .NET applicati\", \"on in much the same way as you would when running without containers.\\n\\n## Testing and debugging with\", \"out Visual Studio\\n\\nIf you're developing using the editor/CLI approach, debugging containers is more \", \"difficult and you'll probably want to debug by generating traces.\\n\\n## Additional resources\\n\\n- Quicks\", \"tart: Docker in Visual Studio.\\n\\nhttps://learn.microsoft.com/visualstudio/containers/container-tools\\n\", \"\\n- Debugging apps in a local Docker container\\n\\nhttps://learn.microsoft.com/visualstudio/containers/e\", \"dit-and-refresh\\n\\n## Simplified workflow when developing containers with Visual Studio\\n\\nEffectively, \", \"the workflow when using Visual Studio is a lot simpler than if you use the editor/CLI approach. Most\", \" of the steps required by Docker related to the Dockerfile and docker-compose.yml files are hidden o\", \"r simplified by Visual Studio, as shown in Figure 5-15.\\n\\nForms\\n\\nHeaders\\n\\nImages\\n\\nLinks\\n\\nInputFields\\n\", \"\\nParsedHtml\\n\\nVS development workflow for Docker apps\\n\\n1.\\n\\nCode your app\\n\\ngit push\\n\\n5.\\n\\nPush or\\n\\nCont\", \"inue developing\\n\\n2. (Once)\\n\\nAdd Docker\\n\\nRun/Debug\\n\\nContainers /\\n\\n4.\\n\\nTest\\n\\n## support to projects Co\", \"mpose app your appor microservices\\n\\nMy\\n\\nContainers\\n\\nFigure 5 -15. Simplified workflow when developin\", \"g with Visual Studio\\n\\n<!-- image -->\\n\\nIn addition, you need to perform step 2 (adding Docker support\", \" to your projects) just once. Therefore, the workflow is similar to your usual development tasks whe\", \"n using .NET for any other development. You need to know what is going on under the covers (the imag\", \"e build process, what base images you're using, deployment of containers, etc.) and sometimes you wi\", \"ll also need to edit the Dockerfile or docker -compose.yml file to customize behaviors. But most of \", \"the work is greatly simplified by using Visual Studio, making you a lot more productive.\\n\\n## Using P\", \"owerShell commands in a Dockerfile to set up Windows Containers\\n\\nWindows Containers allow you to con\", \"vert your existing Windows applications into Docker images and deploy them with the same tools as th\", \"e rest of the Docker ecosystem. To use Windows Containers, you run PowerShell commands in the Docker\", \"file, as shown in the following example:\\n\\n```\\nFROM mcr.microsoft.com/windows/servercore LABEL Descri\", \"ption=\\\"IIS\\\" Vendor=\\\"Microsoft\\\" Version=\\\"10\\\" RUN powershell -Command Add-WindowsFeature Web-Server CM\", \"D [ \\\"ping\\\" , \\\"localhost\\\" , \\\" -t\\\" ]\\n```\\n\\nIn this case, we are using a Windows Server Core base image \", \"(the FROM setting) and installing IIS with a PowerShell command (the RUN setting). In a similar way,\", \" you could also use PowerShell commands to set up additional components like ASP.NET 4.x, .NET Frame\", \"work 4.6, or any other Windows software. For example, the following command in a Dockerfile sets up \", \"ASP.NET 4.5:\\n\\nRUN powershell add-windowsfeature web-asp-net45\\n\\n## Additional resources\\n\\n- aspnet -do\", \"cker/Dockerfile. Example PowerShell commands to run from dockerfiles to include Windows features.\\n\\nh\", \"ttps://github.com/Microsoft/aspnet-docker/blob/master/4.7.1-windowsservercoreltsc2016/runtime/Docker\", \"file\\n\\n## Designing and Developing Multi -Container and Microservice -Based .NET Applications\\n\\nDevelo\", \"ping containerized microservice applications means you are building multi-container applications. Ho\", \"wever, a multi-container application could also be simpler\\u2014for example, a three-tier application\\u2014and\", \" might not be built using a microservice architecture.\\n\\nEarlier we raised the question \\\"Is Docker ne\", \"cessary when building a microservice architecture?\\\" The answer is a clear no. Docker is an enabler a\", \"nd can provide significant benefits, but containers and Docker are not a hard requirement for micros\", \"ervices. As an example, you could create a microservices -based application with or without Docker w\", \"hen using Azure Service Fabric, which supports microservices running as simple processes or as Docke\", \"r containers.\\n\\nHowever, if you know how to design and develop a microservices-based application that\", \" is also based on Docker containers, you will be able to design and develop any other, simpler appli\", \"cation model. For example, you might design a three-tier application that also requires a multi-cont\", \"ainer approach. Because of that, and because microservice architectures are an important trend withi\", \"n the container world, this section focuses on a microservice architecture implementation using Dock\", \"er containers.\\n\\n## Design a microservice -oriented application\\n\\nThis section focuses on developing a\", \" hypothetical server-side enterprise application.\\n\\n## Application specifications\\n\\nThe hypothetical a\", \"pplication handles requests by executing business logic, accessing databases, and then returning HTM\", \"L, JSON, or XML responses. We will say that the application must support various clients, including \", \"desktop browsers running Single Page Applications (SPAs), traditional web apps, mobile web apps, and\", \" native mobile apps. The application might also expose an API for third parties\\n\\nto consume. It shou\", \"ld also be able to integrate its microservices or external applications asynchronously, so that appr\", \"oach will help resiliency of the microservices in the case of partial failures.\\n\\nThe application wil\", \"l consist of these types of components:\\n\\n- Presentation components. These components are responsible\", \" for handling the UI and consuming remote services.\\n- Domain or business logic. This component is th\", \"e application's domain logic.\\n- Database access logic. This component consists of data access compon\", \"ents responsible for accessing databases (SQL or NoSQL).\\n- Application integration logic. This compo\", \"nent includes a messaging channel, based on message brokers.\\n\\nThe application will require high scal\", \"ability, while allowing its vertical subsystems to scale out autonomously, because certain subsystem\", \"s will require more scalability than others.\\n\\nThe application must be able to be deployed in multipl\", \"e infrastructure environments (multiple public clouds and on -premises) and ideally should be cross-\", \"platform, able to move from Linux to Windows (or vice versa) easily.\\n\\n## Development team context\\n\\nW\", \"e also assume the following about the development process for the application:\\n\\n- You have multiple \", \"dev teams focusing on different business areas of the application.\\n- New team members must become pr\", \"oductive quickly, and the application must be easy to understand and modify.\\n- The application will \", \"have a long-term evolution and ever-changing business rules.\\n- You need good long-term maintainabili\", \"ty, which means having agility when implementing new changes in the future while being able to updat\", \"e multiple subsystems with minimum impact on the other subsystems.\\n- You want to practice continuous\", \" integration and continuous deployment of the application.\\n- You want to take advantage of emerging \", \"technologies (frameworks, programming languages, etc.) while evolving the application. You do not wa\", \"nt to make full migrations of the application when moving to new technologies, because that would re\", \"sult in high costs and impact the predictability and stability of the application.\\n\\n## Choosing an a\", \"rchitecture\\n\\nWhat should the application deployment architecture be? The specifications for the appl\", \"ication, along with the development context, strongly suggest that you should architect the applicat\", \"ion by decomposing it into autonomous subsystems in the form of collaborating microservices and cont\", \"ainers, where a microservice is a container.\\n\\nIn this approach, each service (container) implements \", \"a set of cohesive and narrowly related functions. For example, an application might consist of servi\", \"ces such as the catalog service, ordering service, basket service, user profile service, etc.\\n\\nMicro\", \"services communicate using protocols such as HTTP (REST), but also asynchronously (for example, usin\", \"g AMQP) whenever possible, especially when propagating updates with integration events.\\n\\nMicroservic\", \"es are developed and deployed as containers independently of one another. This approach means that a\", \" development team can be developing and deploying a certain microservice without impacting other sub\", \"systems.\\n\\nEach microservice has its own database, allowing it to be fully decoupled from other micro\", \"services. When necessary, consistency between databases from different microservices is achieved usi\", \"ng application-level integration events (through a logical event bus), as handled in Command and Que\", \"ry Responsibility Segregation (CQRS). Because of that, the business constraints must embrace eventua\", \"l consistency between the multiple microservices and related databases.\\n\\n## eShopOnContainers: A ref\", \"erence application for .NET and microservices deployed using containers\\n\\nSo that you can focus on th\", \"e architecture and technologies instead of thinking about a hypothetical business domain that you mi\", \"ght not know, we have selected a well-known business domain\\u2014namely, a simplified e-commerce (e-shop)\", \" application that presents a catalog of products, takes orders from customers, verifies inventory, a\", \"nd performs other business functions. This container-based application source code is available in t\", \"he eShopOnContainers GitHub repo.\\n\\nThe application consists of multiple subsystems, including severa\", \"l store UI front ends (a Web application and a native mobile app), along with the back-end microserv\", \"ices and containers for all the required server-side operations with several API Gateways as consoli\", \"dated entry points to the internal microservices. Figure 6-1 shows the architecture of the reference\", \" application.\\n\\n\\u0433 -\\n\\nClient apps eShop mobile app\\n\\nXamarin.Forms\\n\\nC#\\n\\nxPlat. OS:\\n\\niOS\\n\\nAndroid\\n\\nWindo\", \"ws eShop traditional Web app\\n\\nK\\n\\neShop SPA Web app\\n\\nTypeScript/Angular 2\\n\\nIdentity microservice (STS\", \"+users)\\n\\n\\u2022\\n\\nSQL Server database\\n\\nFigure 6 -1. The eShopOnContainers reference application architectu\", \"re for development environment\\n\\n<!-- image -->\\n\\nThe above diagram shows that Mobile and SPA clients \", \"communicate to single API gateway endpoints, that then communicate to microservices. Traditional web\", \" clients communicate to MVC microservice, that communicates to microservices through the API gateway\", \".\\n\\nHosting environment. In Figure 6-1, you see several containers deployed within a single Docker ho\", \"st. That would be the case when deploying to a single Docker host with the docker-compose up command\", \". However, if you are using an orchestrator or container cluster, each container could be running in\", \" a different host (node), and any node could be running any number of containers, as we explained ea\", \"rlier in the architecture section.\\n\\nCommunication architecture. The eShopOnContainers application us\", \"es two communication types, depending on the kind of the functional action (queries versus updates a\", \"nd transactions):\\n\\n- Http client-to-microservice communication through API Gateways. This approach i\", \"s used for queries and when accepting update or transactional commands from the client apps. The app\", \"roach using API Gateways is explained in detail in later sections.\\n- Asynchronous event-based commun\", \"ication. This communication occurs through an event bus to propagate updates across microservices or\", \" to integrate with external applications. The event bus can be implemented with any messaging-broker\", \" infrastructure technology like RabbitMQ, or using higher-level (abstraction-level) service buses li\", \"ke Azure Service Bus, NServiceBus, MassTransit, or Brighter.\\n\\nThe application is deployed as a set o\", \"f microservices in the form of containers. Client apps can communicate with those microservices runn\", \"ing as containers through the public URLs published by the API Gateways.\\n\\neShopOnContainers referenc\", \"e application\\n\\n(Development environment architecture)\\n\\n-\\n\\n\\u2022 Docker Host\\n\\nAPI Gateways/BFF\\n\\n## Data s\", \"overeignty per microservice\\n\\nIn the sample application, each microservice owns its own database or d\", \"ata source, although all SQL Server databases are deployed as a single container. This design decisi\", \"on was made only to make it easy for a developer to get the code from GitHub, clone it, and open it \", \"in Visual Studio or Visual Studio Code. Or alternatively, it makes it easy to compile the custom Doc\", \"ker images using the .NET CLI and the Docker CLI, and then deploy and run them in a Docker developme\", \"nt environment. Either way, using containers for data sources lets developers build and deploy in a \", \"matter of minutes without having to provision an external database or any other data source with har\", \"d dependencies on infrastructure (cloud or on-premises).\\n\\nIn a real production environment, for high\", \" availability and for scalability, the databases should be based on database servers in the cloud or\", \" on -premises, but not in containers.\\n\\nTherefore, the units of deployment for microservices (and eve\", \"n for databases in this application) are Docker containers, and the reference application is a multi\", \"-container application that embraces microservices principles.\\n\\n## Additional resources\\n\\n- eShopOnCo\", \"ntainers GitHub repo. Source code for the reference application https://aka.ms/eShopOnContainers/\\n\\n#\", \"# Benefits of a microservice -based solution\\n\\nA microservice -based solution like this has many bene\", \"fits:\\n\\nEach microservice is relatively small\\u2014easy to manage and evolve. Specifically:\\n\\n- It is easy \", \"for a developer to understand and get started quickly with good productivity.\\n- Containers start fas\", \"t, which makes developers more productive.\\n- An IDE like Visual Studio can load smaller projects fas\", \"t, making developers productive.\\n- Each microservice can be designed, developed, and deployed indepe\", \"ndently of other microservices, which provide agility because it is easier to deploy new versions of\", \" microservices frequently.\\n\\nIt is possible to scale out individual areas of the application. For ins\", \"tance, the catalog service or the basket service might need to be scaled out, but not the ordering p\", \"rocess. A microservices infrastructure will be much more efficient with regard to the resources used\", \" when scaling out than a monolithic architecture would be.\\n\\nYou can divide the development work betw\", \"een multiple teams. Each service can be owned by a single development team. Each team can manage, de\", \"velop, deploy, and scale their service independently of the rest of the teams.\\n\\nIssues are more isol\", \"ated. If there is an issue in one service, only that service is initially impacted (except when the \", \"wrong design is used, with direct dependencies between microservices), and other services can contin\", \"ue to handle requests. In contrast, one malfunctioning component in a monolithic\\n\\ndeployment archite\", \"cture can bring down the entire system, especially when it involves resources, such as a memory leak\", \". Additionally, when an issue in a microservice is resolved, you can deploy just the affected micros\", \"ervice without impacting the rest of the application.\\n\\nYou can use the latest technologies. Because \", \"you can start developing services independently and run them side by side (thanks to containers and \", \".NET), you can start using the latest technologies and frameworks expediently instead of being stuck\", \" on an older stack or framework for the whole application.\\n\\n## Downsides of a microservice -based so\", \"lution\\n\\nA microservice -based solution like this also has some drawbacks:\\n\\nDistributed application. \", \"Distributing the application adds complexity for developers when they are designing and building the\", \" services. For example, developers must implement inter-service communication using protocols like H\", \"TTP or AMQP, which adds complexity for testing and exception handling. It also adds latency to the s\", \"ystem.\\n\\nDeployment complexity. An application that has dozens of microservices types and needs high \", \"scalability (it needs to be able to create many instances per service and balance those services acr\", \"oss many hosts) means a high degree of deployment complexity for IT operations and management. If yo\", \"u are not using a microservice-oriented infrastructure (like an orchestrator and scheduler), that ad\", \"ditional complexity can require far more development efforts than the business application itself.\\n\\n\", \"Atomic transactions. Atomic transactions between multiple microservices usually are not possible. Th\", \"e business requirements have to embrace eventual consistency between multiple microservices.\\n\\nIncrea\", \"sed global resource needs (total memory, drives, and network resources for all the servers or hosts)\", \". In many cases, when you replace a monolithic application with a microservices approach, the amount\", \" of initial global resources needed by the new microservice-based application will be larger than th\", \"e infrastructure needs of the original monolithic application. This approach is because the higher d\", \"egree of granularity and distributed services requires more global resources. However, given the low\", \" cost of resources in general and the benefit of being able to scale out certain areas of the applic\", \"ation compared to long-term costs when evolving monolithic applications, the increased use of resour\", \"ces is usually a good tradeoff for large, long-term applications.\\n\\nIssues with direct client -to-mic\", \"roservice communication. When the application is large, with dozens of microservices, there are chal\", \"lenges and limitations if the application requires direct clientto -microservice communications. One\", \" problem is a potential mismatch between the needs of the client and the APIs exposed by each of the\", \" microservices. In certain cases, the client application might need to make many separate requests t\", \"o compose the UI, which can be inefficient over the Internet and would be impractical over a mobile \", \"network. Therefore, requests from the client application to the back -end system should be minimized\", \".\\n\\nAnother problem with direct client-to-microservice communications is that some microservices migh\", \"t be using protocols that are not Web-friendly. One service might use a binary protocol, while anoth\", \"er service might use AMQP messaging. Those protocols are not firewall-friendly and are best used int\", \"ernally. Usually, an application should use protocols such as HTTP and WebSockets for communication \", \"outside of the firewall.\\n\\nYet another drawback with this direct client -to -service approach is that\", \" it makes it difficult to refactor the contracts for those microservices. Over time developers might\", \" want to change how the system is partitioned into services. For example, they might merge two servi\", \"ces or split a service into two or more services. However, if clients communicate directly with the \", \"services, performing this kind of refactoring can break compatibility with client apps.\\n\\nAs mentione\", \"d in the architecture section, when designing and building a complex application based on microservi\", \"ces, you might consider the use of multiple fine-grained API Gateways instead of the simpler direct \", \"client-to-microservice communication approach .\\n\\nPartitioning the microservices. Finally, no matter,\", \" which approach you take for your microservice architecture, another challenge is deciding how to pa\", \"rtition an end-to-end application into multiple microservices. As noted in the architecture section \", \"of the guide, there are several techniques and approaches you can take. Basically, you need to ident\", \"ify areas of the application that are decoupled from the other areas and that have a low number of h\", \"ard dependencies. In many cases, this approach is aligned to partitioning services by use case. For \", \"example, in our e-shop application, we have an ordering service that is responsible for all the busi\", \"ness logic related to the order process. We also have the catalog service and the basket service tha\", \"t implement other capabilities. Ideally, each service should have only a small set of responsibiliti\", \"es. This approach is similar to the single responsibility principle (SRP) applied to classes, which \", \"states that a class should only have one reason to change. But in this case, it is about microservic\", \"es, so the scope will be larger than a single class. Most of all, a microservice has to be autonomou\", \"s, end to end, including responsibility for its own data sources.\\n\\n## External versus internal archi\", \"tecture and design patterns\\n\\nThe external architecture is the microservice architecture composed by \", \"multiple services, following the principles described in the architecture section of this guide. How\", \"ever, depending on the nature of each microservice, and independently of high-level microservice arc\", \"hitecture you choose, it is common and sometimes advisable to have different internal architectures,\", \" each based on different patterns, for different microservices. The microservices can even use diffe\", \"rent technologies and programming languages. Figure 6-2 illustrates this diversity.\\n\\nExternal archit\", \"ecture per application\\n\\nBack end\\n\\n## Microservice 1\\n\\nFigure 6 -2. External versus internal architect\", \"ure and design\\n\\n<!-- image -->\\n\\nFor instance, in our eShopOnContainers sample, the catalog, basket, \", \"and user profile microservices are simple (basically, CRUD subsystems). Therefore, their internal ar\", \"chitecture and design is straightforward. However, you might have other microservices, such as the o\", \"rdering microservice, which is more complex and represents ever-changing business rules with a high \", \"degree of domain complexity. In cases like these, you might want to implement more advanced patterns\", \" within a particular microservice, like the ones defined with domain-driven design (DDD) approaches,\", \" as we are doing in the eShopOnContainers ordering microservice. (We will review these DDD patterns \", \"in the section later that explains the implementation of the eShopOnContainers ordering microservice\", \".)\\n\\nAnother reason for a different technology per microservice might be the nature of each microserv\", \"ice. For example, it might be better to use a functional programming language like F#, or even a lan\", \"guage like R if you are targeting AI and machine learning domains, instead of a more object-oriented\", \" programming language like C#.\\n\\nThe bottom line is that each microservice can have a different inter\", \"nal architecture based on different design patterns. Not all microservices should be implemented usi\", \"ng advanced DDD patterns, because that would be over -engineering them. Similarly, complex microserv\", \"ices with ever-changing business logic should not be implemented as CRUD components, or you can end \", \"up with low-quality code.\\n\\n## The new world: multiple architectural patterns and polyglot microservi\", \"ces\\n\\nThere are many architectural patterns used by software architects and developers. The following\", \" are a few (mixing architecture styles and architecture patterns):\\n\\n- Simple CRUD, single-tier, sing\", \"le-layer.\\n\\nClient apps\\n\\nMobile app\\n\\nSPA\\n\\nWeb app\\n\\nInternal architecture per microservice\\n\\nThe Multi-\", \"Architectural-Patterns and polyglot microservices world\\n\\nMicroservice 1\\n\\nContainer\\n\\n\\u2022 ASP.NET Core\\n\\n\", \"\\u2022 Simple CRUD Design\\n\\nEntity Framework Core\\n\\nMicroservice 4\\n\\nContainer\\n\\nNancyFX (.NET Core)\\n\\nSimple \", \"CRUD Design\\n\\nMassive\\n\\nMicroservice 7\\n\\nContainer\\n\\nSQL Server database\\n\\n- Traditional N -Layered .\\n\\nDD\", \"D &amp; CQRS patterns\\n\\n' Microservice 3\\n\\nContainer\\n\\n\\u2022\\n\\nASP.NET Core\\n\\nQueries projection\\n\\n- Domain -D\", \"riven Design N-layered .\\n\\nDocDB/MongoDB API\\n\\n- Clean Architecture (as used with eShopOnWeb)\\n- Comman\", \"d and Query Responsibility Segregation (CQRS).\\n\\nASP.NET Core\\n\\n- Event -Driven Architecture (EDA).\\n\\n\\u2022\", \" Simple CRUD Design\\n\\nYou can also build microservices with many technologies and languages, such as \", \"ASP.NET Core Web APIs, NancyFx, ASP.NET Core SignalR (available with .NET Core 2 or later), F#, Node\", \".js, Python, Java, C++, GoLang, and more. Container\\n\\nPython\\n\\nMicroservice 10\\n\\nJava\\n\\n\\u2022 ASP.NET Core\\n\\n\", \"The important point is that no particular architecture pattern or style, nor any particular technolo\", \"gy, is right for all situations. Figure 6-3 shows some approaches and technologies (although not in \", \"any particular order) that could be used in different microservices. \\u00b7 GoLang\\n\\nContainer\\n\\n\\u2022 Hub for \", \"Real Time comm.\\n\\n\\u2022 i.e. Calculus focused\\n\\n## Container Container\\n\\n\\u2022 Stateless process\\n\\nFigure 6 -3. \", \"Multi -architectural patterns and the polyglot microservices world\\n\\n<!-- image -->\\n\\nMulti -architect\", \"ural pattern and polyglot microservices means you can mix and match languages and technologies to th\", \"e needs of each microservice and still have them talking to each other. As shown in Figure 6-3, in a\", \"pplications composed of many microservices (Bounded Contexts in domain-driven design terminology, or\", \" simply \\\"subsystems\\\" as autonomous microservices), you might implement each microservice in a differ\", \"ent way. Each might have a different architecture pattern and use different languages and databases \", \"depending on the application's nature, business requirements, and priorities. In some cases, the mic\", \"roservices might be similar. But that is not usually the case, because each subsystem's context boun\", \"dary and requirements are usually different.\\n\\n\\u2022 Node.js\\n\\nMicroservice 2\\n\\nSQL Server database\\n\\n\\u2022 (}\\n\\n\", \"DocDB /\\n\\nMongoDB\\n\\n\\u2022 MySql\\n\\n\\u2022 database\\n\\nClient apps\\n\\nMobile app\\n\\nSPA\\n\\nWeb app\\n\\nBack end\\n\\nAPI Gateway\\n\", \"\\nMicroservice 1\\n\\nWeb AP!\\n\\nMicroservice 2\\n\\nWeb API\\n\\nSimple CRUD\\n\\nFor instance, for a simple CRUD main\", \"tenance application, it might not make sense to design and implement DDD patterns. But for your core\", \" domain or core business, you might need to apply more advanced patterns to tackle business complexi\", \"ty with ever-changing business rules.\\n\\nEspecially when you deal with large applications composed by \", \"multiple subsystems, you should not apply a single top-level architecture based on a single architec\", \"ture pattern. For instance, CQRS should not be applied as a top-level architecture for a whole appli\", \"cation, but might be useful for a specific set of services.\\n\\nThere is no silver bullet or a right ar\", \"chitecture pattern for every given case. You cannot have \\\"one architecture pattern to rule them all.\", \"\\\" Depending on the priorities of each microservice, you must choose a different approach for each, a\", \"s explained in the following sections.\\n\\n## Creating a simple data -driven CRUD microservice\\n\\nThis se\", \"ction outlines how to create a simple microservice that performs create, read, update, and delete (C\", \"RUD) operations on a data source.\\n\\n## Designing a simple CRUD microservice\\n\\nFrom a design point of v\", \"iew, this type of containerized microservice is very simple. Perhaps the problem to solve is simple,\", \" or perhaps the implementation is only a proof of concept.\\n\\nFigure 6 -4. Internal design for simple \", \"CRUD microservices\\n\\n<!-- image -->\\n\\nAn example of this kind of simple data-drive service is the cata\", \"log microservice from the eShopOnContainers sample application. This type of service implements all \", \"its functionality in a single ASP.NET Core Web API project that includes classes for its data model,\", \" its business logic, and its data access code. It also stores its related data in a database running\", \" in SQL Server (as another container for dev/test purposes), but could also be any regular SQL Serve\", \"r host, as shown in Figure 6-5.\\n\\nData-Driven/CRUD microservice container\\n\\nDocker Host\\n\\n## Logical \\\"C\", \"atalog\\\" Microservice \\\"Catalog\\\" API\\n\\nFigure 6 -5. Simple data -driven/CRUD microservice design\\n\\n<!-- \", \"image -->\\n\\nThe previous diagram shows the logical Catalog microservice, that includes its Catalog da\", \"tabase, which can be or not in the same Docker host. Having the database in the same Docker host mig\", \"ht be good for development, but not for production. When you are developing this kind of service, yo\", \"u only need ASP.NET Core and a data -access API or ORM like Entity Framework Core. You could also ge\", \"nerate Swagger metadata automatically through Swashbuckle to provide a description of what your serv\", \"ice offers, as explained in the next section.\\n\\nNote that running a database server like SQL Server w\", \"ithin a Docker container is great for development environments, because you can have all your depend\", \"encies up and running without needing to provision a database in the cloud or on-premises. This appr\", \"oach is convenient when running integration tests. However, for production environments, running a d\", \"atabase server in a container is not recommended, because you usually do not get high availability w\", \"ith that approach. For a production environment in Azure, it is recommended that you use Azure SQL D\", \"B or any other database technology that can provide high availability and high scalability. For exam\", \"ple, for a NoSQL approach, you might choose CosmosDB.\\n\\nFinally, by editing the Dockerfile and docker\", \"-compose.yml metadata files, you can configure how the image of this container will be created\\u2014what \", \"base image it will use, plus design settings such as internal and external names and TCP ports.\\n\\n## \", \"Implementing a simple CRUD microservice with ASP.NET Core\\n\\nTo implement a simple CRUD microservice u\", \"sing .NET and Visual Studio, you start by creating a simple ASP.NET Core Web API project (running on\", \" .NET so it can run on a Linux Docker host), as shown in Figure 6-6.\\n\\nExternal IP, and Port\\n\\nCreate \", \"a new project\\n\\nRecent project templates\\n\\nASP.NET Core Web Application\\n\\n\\u2022 Windows Forms App (.NET)\\n\\n0\", \" gRPC Service\\n\\nConsole App (.NET Core)\\n\\nxUnit Test Project (.NET Core)\\n\\n&amp; Class Library (.NET Co\", \"re)\\n\\nSearch for templates (Alt+S)\\n\\nAll languages\\n\\nAll platforms\\n\\nASP.NET Core Web Application\\n\\nProje\", \"ct templates for creating ASP.NET Core web apps and web APls for Windows, Linux and macOS using .NET\", \" Core or.NET Framework. Create web apps with\\n\\nRazor Danac MVC or Sinnla Dana Anne /SDA)ncinn Annular\", \" React or React + Radi\\n\\nFigure 6 -6. Creating an ASP.NET Core Web API project in Visual Studio 2019\\n\", \"\\n<!-- image -->\\n\\nTo create an ASP.NET Core Web API Project, first select an ASP.NET Core Web Applica\", \"tion and then select the API type. After creating the project, you can implement your MVC controller\", \"s as you would in any other Web API project, using the Entity Framework API or other API. In a new W\", \"eb API project, you can see that the only dependency you have in that microservice is on ASP.NET Cor\", \"e itself. Internally, within the Microsoft.AspNetCore.All dependency, it is referencing Entity Frame\", \"work and many other .NET NuGet packages, as shown in Figure 6-7.\\n\\nP-\\n\\nAll project types\\n\\nCatalog.API\", \"\\n\\nC Connected Services\\n\\n#* Dependencies\\n\\nAnalyzers\\n\\nFrameworks\\n\\nFigure 6 -7. Dependencies in a simpl\", \"e CRUD Web API microservice\\n\\n<!-- image -->\\n\\nThe API project includes references to Microsoft.AspNet\", \"Core.App NuGet package, that includes references to all essential packages. It could include some ot\", \"her packages as well.\\n\\n## Implementing CRUD Web API services with Entity Framework Core\\n\\nEntity Fram\", \"ework (EF) Core is a lightweight, extensible, and cross-platform version of the popular Entity Frame\", \"work data access technology. EF Core is an object-relational mapper (ORM) that enables .NET develope\", \"rs to work with a database using .NET objects.\\n\\nThe catalog microservice uses EF and the SQL Server \", \"provider because its database is running in a container with the SQL Server for Linux Docker image. \", \"However, the database could be deployed into\\n\\nany SQL Server, such as Windows on-premises or Azure S\", \"QL DB. The only thing you would need to change is the connection string in the ASP.NET Web API micro\", \"service.\\n\\n## The data model\\n\\nWith EF Core, data access is performed by using a model. A model is mad\", \"e up of (domain model) entity classes and a derived context (DbContext) that represents a session wi\", \"th the database, allowing you to query and save data. You can generate a model from an existing data\", \"base, manually code a model to match your database, or use EF migrations technique to create a datab\", \"ase from your model, using the code-first approach (that makes it easy to evolve the database as you\", \"r model changes over time). For the catalog microservice, the last approach has been used. You can s\", \"ee an example of the CatalogItem entity class in the following code example, which is a simple Plain\", \" Old Class Object (POCO) entity class.\\n\\n```\\npublic class CatalogItem { public int Id { get; set; } p\", \"ublic string Name { get; set; } public string Description { get; set; } public decimal Price { get; \", \"set; } public string PictureFileName { get; set; } public string PictureUri { get; set; } public int\", \" CatalogTypeId { get; set; } public CatalogType CatalogType { get; set; } public int CatalogBrandId \", \"{ get; set; } public CatalogBrand CatalogBrand { get; set; } public int AvailableStock { get; set; }\", \" public int RestockThreshold { get; set; } public int MaxStockThreshold { get; set; } public bool On\", \"Reorder { get; set; } public CatalogItem() { } // Additional code ... }\\n```\\n\\nYou also need a DbConte\", \"xt that represents a session with the database. For the catalog microservice, the CatalogContext cla\", \"ss derives from the DbContext base class, as shown in the following example:\\n\\n```\\npublic class Catal\", \"ogContext : DbContext { public CatalogContext(DbContextOptions<CatalogContext> options) : base(optio\", \"ns) { } public DbSet<CatalogItem> CatalogItems { get; set; } public DbSet<CatalogBrand> CatalogBrand\", \"s { get; set; } public DbSet<CatalogType> CatalogTypes { get; set; } // Additional code ... }\\n```\\n\\nY\", \"ou can have additional DbContext implementations. For example, in the sample Catalog.API microservic\", \"e, there's a second DbContext named CatalogContextSeed where it automatically populates the sample d\", \"ata the first time it tries to access the database. This method is useful for demo data and for auto\", \"mated testing scenarios, as well.\\n\\nWithin the DbContext, you use the OnModelCreating method to custo\", \"mize object/database entity mappings and other EF extensibility points .\\n\\n## Querying data from Web \", \"API controllers\\n\\nInstances of your entity classes are typically retrieved from the database using La\", \"nguage-Integrated Query (LINQ), as shown in the following example:\\n\\n```\\n[Route(\\\"api/v1/[controller]\\\"\", \")] public class CatalogController : ControllerBase { private readonly CatalogContext _catalogContext\", \"; private readonly CatalogSettings _settings; private readonly ICatalogIntegrationEventService _cata\", \"logIntegrationEventService; public CatalogController( CatalogContext context , IOptionsSnapshot<Cata\", \"logSettings> settings , ICatalogIntegrationEventService catalogIntegrationEventService) { _catalogCo\", \"ntext = context ?? throw new ArgumentNullException(nameof(context)); _catalogIntegrationEventService\", \" = catalogIntegrationEventService ?? throw new ArgumentNullException(nameof(catalogIntegrationEventS\", \"ervice)); _settings = settings . Value; context . ChangeTracker . QueryTrackingBehavior = QueryTrack\", \"ingBehavior . NoTracking; } // GET api/v1/[controller]/items[?pageSize=3&pageIndex=10] [HttpGet] [Ro\", \"ute(\\\"items\\\")] [ProducesResponseType(typeof(PaginatedItemsViewModel<CatalogItem>), (int)HttpStatusCod\", \"e . OK)] [ProducesResponseType(typeof(IEnumerable<CatalogItem>), (int)HttpStatusCode . OK)] [Produce\", \"sResponseType((int)HttpStatusCode . BadRequest)] public async Task<IActionResult> ItemsAsync( [FromQ\", \"uery]int pageSize = 10 , [FromQuery]int pageIndex = 0 , string ids = null) { if (!string . IsNullOrE\", \"mpty(ids)) { var items = await GetItemsByIdsAsync(ids); if (!items . Any()) { return BadRequest(\\\"ids\", \" value invalid. Must be comma-separated list of numbers\\\"); } return Ok(items); } var totalItems = aw\", \"ait _catalogContext . CatalogItems . LongCountAsync(); var itemsOnPage = await _catalogContext . Cat\", \"alogItems . OrderBy(c => c . Name) . Skip(pageSize * pageIndex)\\n```\\n\\n```\\n. Take(pageSize) . ToListAs\", \"ync(); itemsOnPage = ChangeUriPlaceholder(itemsOnPage); var model = new PaginatedItemsViewModel<Cata\", \"logItem>( pageIndex , pageSize , totalItems , itemsOnPage); return Ok(model); } //... }\\n```\\n\\n## Savi\", \"ng data\\n\\nData is created, deleted, and modified in the database using instances of your entity class\", \"es. You could add code like the following hard-coded example (mock data, in this case) to your Web A\", \"PI controllers.\\n\\n```\\nvar catalogItem = new CatalogItem() {CatalogTypeId=2 , CatalogBrandId=2 , Name=\", \"\\\"Roslyn T-Shirt\\\" , Price = 12}; _context . Catalog . Add(catalogItem); _context . SaveChanges();\\n```\", \"\\n\\n## Dependency Injection in ASP.NET Core and Web API controllers\\n\\nIn ASP.NET Core, you can use Depe\", \"ndency Injection (DI) out of the box. You do not need to set up a third -party Inversion of Control \", \"(IoC) container, although you can plug your preferred IoC container into the ASP.NET Core infrastruc\", \"ture if you want. In this case, it means that you can directly inject the required EF DBContext or a\", \"dditional repositories through the controller constructor.\\n\\nIn the CatalogController class mentioned\", \" earlier, CatalogContext (which inherits from DbContext) type is injected along with the other requi\", \"red objects in the CatalogController() constructor.\\n\\nAn important configuration to set up in the Web\", \" API project is the DbContext class registration into the service's IoC container. You typically do \", \"so in the Program.cs file by calling the builder.Services.AddDbContext&lt;CatalogContext&gt;() metho\", \"d, as shown in the following simplified example:\\n\\n```\\n// Additional code... builder . Services . Add\", \"DbContext<CatalogContext>(options => { options . UseSqlServer(builder . Configuration[\\\"ConnectionStr\", \"ing\\\"], sqlServerOptionsAction: sqlOptions => { sqlOptions . MigrationsAssembly( typeof(Program).GetT\", \"ypeInfo().Assembly . GetName().Name); //Configuring Connection Resiliency: sqlOptions . EnableRetryO\", \"nFailure(maxRetryCount: 5 , maxRetryDelay: TimeSpan . FromSeconds(30), errorNumbersToAdd: null); });\", \"\\n```\\n\\n```\\n// Changing default behavior when client evaluation occurs to throw. // Default in EFCore \", \"would be to log warning when client evaluation is done. options . ConfigureWarnings(warnings => warn\", \"ings . Throw( RelationalEventId . QueryClientEvaluationWarning)); });\\n```\\n\\n## Additional resources\\n\\n\", \"- Querying Data\\n\\nhttps://learn.microsoft.com/ef/core/querying/index\\n\\n- Saving Data\\n- https://learn.m\", \"icrosoft.com/ef/core/saving/index\\n\\n## The DB connection string and environment variables used by Doc\", \"ker containers\\n\\nYou can use the ASP.NET Core settings and add a ConnectionString property to your se\", \"ttings.json file as shown in the following example:\\n\\n```\\n{ \\\"ConnectionString\\\": \\\"Server=tcp:127.0.0.1\", \",5433;Initial Catalog=Microsoft.eShopOnContainers.Services.CatalogDb;User Id=sa;Password=[PLACEHOLDE\", \"R]\\\" , \\\"ExternalCatalogBaseUrl\\\": \\\"http://host.docker.internal:5101\\\" , \\\"Logging\\\": { \\\"IncludeScopes\\\": f\", \"alse , \\\"LogLevel\\\": { \\\"Default\\\": \\\"Debug\\\" , \\\"System\\\": \\\"Information\\\" , \\\"Microsoft\\\": \\\"Information\\\" } } }\", \"\\n```\\n\\nThe settings.json file can have default values for the ConnectionString property or for any ot\", \"her property. However, those properties will be overridden by the values of environment variables th\", \"at you specify in the docker-compose.override.yml file, when using Docker.\\n\\nFrom your docker-compose\", \".yml or docker-compose.override.yml files, you can initialize those environment variables so that Do\", \"cker will set them up as OS environment variables for you, as shown in the following docker-compose.\", \"override.yml file (the connection string and other lines wrap in this example, but it would not wrap\", \" in your own file) .\\n\\n```\\n# docker -compose.override.yml # catalog-api: environment: -ConnectionStri\", \"ng=Server=sqldata;Database=Microsoft.eShopOnContainers.Services.CatalogDb;Use r Id=sa;Password=[PLAC\", \"EHOLDER] # Additional environment variables for this service ports: -\\\"5101:80\\\"\\n```\\n\\nThe docker -comp\", \"ose.yml files at the solution level are not only more flexible than configuration files at the proje\", \"ct or microservice level, but also more secure if you override the environment variables declared at\", \" the docker -compose files with values set from your deployment tools, like from Azure DevOps Servic\", \"es Docker deployment tasks.\\n\\nFinally, you can get that value from your code by using builder.Configu\", \"ration\\\\[\\\"ConnectionString\\\"\\\\], as shown in an earlier code example.\\n\\nHowever, for production environm\", \"ents, you might want to explore additional ways on how to store secrets like the connection strings.\", \" An excellent way to manage application secrets is using Azure Key Vault .\\n\\nAzure Key Vault helps to\", \" store and safeguard cryptographic keys and secrets used by your cloud applications and services. A \", \"secret is anything you want to keep strict control of, like API keys, connection strings, passwords,\", \" etc. and strict control includes usage logging, setting expiration, managing access, among others .\", \"\\n\\nAzure Key Vault allows a detailed control level of the application secrets usage without the need \", \"to let anyone know them. The secrets can even be rotated for enhanced security without disrupting de\", \"velopment or operations.\\n\\nApplications have to be registered in the organization's Active Directory,\", \" so they can use the Key Vault.\\n\\nYou can check the Key Vault Concepts documentation for more details\", \".\\n\\n## Implementing versioning in ASP.NET Web APIs\\n\\nAs business requirements change, new collections \", \"of resources may be added, the relationships between resources might change, and the structure of th\", \"e data in resources might be amended. Updating a Web API to handle new requirements is a relatively \", \"straightforward process, but you must consider the effects that such changes will have on client app\", \"lications consuming the Web API. Although the developer designing and implementing a Web API has ful\", \"l control over that API, the developer does not have the same degree of control over client applicat\", \"ions that might be built by third -party organizations operating remotely.\\n\\nVersioning enables a Web\", \" API to indicate the features and resources that it exposes. A client application can then submit re\", \"quests to a specific version of a feature or resource. There are several approaches to implement ver\", \"sioning:\\n\\n- URI versioning\\n- Query string versioning\\n- Header versioning\\n\\nQuery string and URI versi\", \"oning are the simplest to implement. Header versioning is a good approach. However, header versionin\", \"g is not as explicit and straightforward as URI versioning. Because URL versioning is the simplest a\", \"nd most explicit, the eShopOnContainers sample application uses URI versioning.\\n\\nWith URI versioning\", \", as in the eShopOnContainers sample application, each time you modify the Web API or change the sch\", \"ema of resources, you add a version number to the URI for each resource. Existing URIs should contin\", \"ue to operate as before, returning resources that conform to the schema that matches the requested v\", \"ersion.\\n\\nAs shown in the following code example, the version can be set by using the Route attribute\", \" in the Web API controller, which makes the version explicit in the URI (v1 in this case).\\n\\n```\\n[Rou\", \"te(\\\"api/v1/[controller]\\\")] public class CatalogController : ControllerBase { // Implementation ...\\n`\", \"``\\n\\nThis versioning mechanism is simple and depends on the server routing the request to the appropr\", \"iate endpoint. However, for a more sophisticated versioning and the best method when using REST, you\", \" should use hypermedia and implement HATEOAS (Hypertext as the Engine of Application State) .\\n\\n## Ad\", \"ditional resources\\n\\n- ASP.NET API Versioning https://github.com/dotnet/aspnet-api-versioning\\n- Scott\", \" Hanselman. ASP.NET Core RESTful Web API versioning made easy https://www.hanselman.com/blog/ASPNETC\", \"oreRESTfulWebAPIVersioningMadeEasy.aspx\\n- Versioning a RESTful web API https://learn.microsoft.com/a\", \"zure/architecture/best-practices/api-design#versioning-arestful -web -api\\n- Roy Fielding. Versioning\", \", Hypermedia, and REST https://www.infoq.com/articles/roy-fielding-on-versioning\\n\\n## Generating Swag\", \"ger description metadata from your ASP.NET Core Web API\\n\\nSwagger is a commonly used open source fram\", \"ework backed by a large ecosystem of tools that helps you design, build, document, and consume your \", \"RESTful APIs. It is becoming the standard for the APIs description metadata domain. You should inclu\", \"de Swagger description metadata with any kind of microservice, either data-driven microservices or m\", \"ore advanced domain-driven microservices (as explained in the following section).\\n\\nThe heart of Swag\", \"ger is the Swagger specification, which is API description metadata in a JSON or YAML file. The spec\", \"ification creates the RESTful contract for your API, detailing all its resources and operations in b\", \"oth a human- and machine-readable format for easy development, discovery, and integration.\\n\\nThe spec\", \"ification is the basis of the OpenAPI Specification (OAS) and is developed in an open, transparent, \", \"and collaborative community to standardize the way RESTful interfaces are defined.\\n\\nThe specificatio\", \"n defines the structure for how a service can be discovered and how its capabilities understood. For\", \" more information, including a web editor and examples of Swagger specifications from companies like\", \" Spotify, Uber, Slack, and Microsoft, see the Swagger site (https://swagger.io).\\n\\n## Why use Swagger\", \"?\\n\\nThe main reasons to generate Swagger metadata for your APIs are the following.\\n\\nAbility for other\", \" products to automatically consume and integrate your APIs. Dozens of products and commercial tools \", \"and many libraries and frameworks support Swagger. Microsoft has high-level products and tools that \", \"can automatically consume Swagger-based APIs, such as the following:\\n\\n- AutoRest. You can automatica\", \"lly generate .NET client classes for calling Swagger. This tool can be used from the CLI and it also\", \" integrates with Visual Studio for easy use through the GUI.\\n- Microsoft Flow. You can automatically\", \" use and integrate your API into a high-level Microsoft Flow workflow, with no programming skills re\", \"quired.\\n- Microsoft PowerApps. You can automatically consume your API from PowerApps mobile apps bui\", \"lt with PowerApps Studio, with no programming skills required.\\n- Azure App Service Logic Apps. You c\", \"an automatically use and integrate your API into an Azure App Service Logic App, with no programming\", \" skills required.\\n\\nAbility to automatically generate API documentation. When you create large-scale \", \"RESTful APIs, such as complex microservice-based applications, you need to handle many endpoints wit\", \"h different data models used in the request and response payloads. Having proper documentation and h\", \"aving a solid API explorer, as you get with Swagger, is key for the success of your API and adoption\", \" by developers.\\n\\nSwagger's metadata is what Microsoft Flow, PowerApps, and Azure Logic Apps use to u\", \"nderstand how to use APIs and connect to them.\\n\\nThere are several options to automate Swagger metada\", \"ta generation for ASP.NET Core REST API applications, in the form of functional API help pages, base\", \"d on swagger-ui .\\n\\nProbably the best know is Swashbuckle, which is currently used in eShopOnContaine\", \"rs and we'll cover in some detail in this guide but there's also the option to use NSwag, which can \", \"generate Typescript and C# API clients, as well as C# controllers, from a Swagger or OpenAPI specifi\", \"cation and even by scanning the .dll that contains the controllers, using NSwagStudio .\\n\\n## How to a\", \"utomate API Swagger metadata generation with the Swashbuckle NuGet package\\n\\nGenerating Swagger metad\", \"ata manually (in a JSON or YAML file) can be tedious work. However, you can automate API discovery o\", \"f ASP.NET Web API services by using the Swashbuckle NuGet package to dynamically generate Swagger AP\", \"I metadata.\\n\\nSwashbuckle automatically generates Swagger metadata for your ASP.NET Web API projects.\", \" It supports ASP.NET Core Web API projects and the traditional ASP.NET Web API and any other flavor,\", \"\\n\\n\\u2022 \\u2022 Swagger UI\\n\\n+\\n\\nA Not secure I hot docker intermal:101/wager/nde.tml\\n\\n6) Swagger.\\n\\nwindly SMART\", \"BEAR\\n\\nServers\\n\\n/catalog-api v\\n\\nCatalog\\n\\nSelect a definition\\n\\nCatalog.AP| V1\\n\\nsuch as Azure API App, \", \"Azure Mobile App, Azure Service Fabric microservices based on ASP.NET. It also supports plain Web AP\", \"I deployed on containers, as in for the reference application.\\n\\nSwashbuckle combines API Explorer an\", \"d Swagger or swagger-ui to provide a rich discovery and documentation experience for your API consum\", \"ers. In addition to its Swagger metadata generator engine, Swashbuckle also contains an embedded ver\", \"sion of swagger-ui, which it will automatically serve up once Swashbuckle is installed.\\n\\nGET\\n\\nPUT\\n\\nP\", \"OST\\n\\n/api/v1/Catalog/items\\n\\nV\\n\\nThis means you can complement your API with a nice discovery UI to he\", \"lp developers to use your API. It requires a small amount of code and maintenance because it is auto\", \"matically generated, allowing you to focus on building your API. The result for the API Explorer loo\", \"ks like Figure 6-8.\\n\\nGET /api/v1/Catalog/items/{id}\\n\\nGET\\n\\n/api/v1/Catalog/items/withname/{name}\\n\\nFig\", \"ure 6 -8. Swashbuckle API Explorer based on Swagger metadata \\u2014 eShopOnContainers catalog microservic\", \"e\\n\\n<!-- image -->\\n\\nThe Swashbuckle generated Swagger UI API documentation includes all published act\", \"ions. The API explorer is not the most important thing here. Once you have a Web API that can descri\", \"be itself in Swagger metadata, your API can be used seamlessly from Swagger-based tools, including c\", \"lient proxy -class code generators that can target many platforms. For example, as mentioned, AutoRe\", \"st automatically generates .NET client classes. But additional tools like swagger-codegen are also a\", \"vailable, which allow code generation of API client libraries, server stubs, and documentation autom\", \"atically.\\n\\nCurrently, Swashbuckle consists of five internal NuGet packages under the high-level meta\", \"package Swashbuckle.AspNetCore for ASP.NET Core applications.\\n\\nAfter you have installed these NuGet \", \"packages in your Web API project, you need to configure Swagger in the Program.cs class, as in the f\", \"ollowing simplified code:\\n\\n```\\n// Add framework services. builder . Services . AddSwaggerGen(options\", \" => {\\n```\\n\\nInPrivate g\\n\\n```\\noptions . DescribeAllEnumsAsStrings(); options . SwaggerDoc(\\\"v1\\\" , new O\", \"penApiInfo { Title = \\\"eShopOnContainers -Catalog HTTP API\\\" , Version = \\\"v1\\\" , Description = \\\"The Cat\", \"alog Microservice HTTP API. This is a Data-Driven/CRUD microservice sample\\\" }); }); // Other startup\", \" code... app.UseSwagger() . UseSwaggerUI(c => { c . SwaggerEndpoint(\\\"/swagger/v1/swagger.json\\\" , \\\"My\", \" API V1\\\"); }); ``` ::: Once this is done , you can start your application and browse the following S\", \"wagger JSON and UI endpoints using URLs like these: :::{custom-style=CodeBox} ```console http://<you\", \"r-root-url>/swagger/v1/swagger.json http://<your-root-url>/swagger/\\n```\\n\\nYou previously saw the gene\", \"rated UI created by Swashbuckle for a URL like http://&lt;your-rooturl&gt;/swagger. In Figure 6-9, y\", \"ou can also see how you can test any API method.\\n\\n&lt; \\u2192\\n\\nCatalog\\n\\nGET\\n\\nParameters\\n\\nName pageSize\\n\\ni\", \"nteger\\n\\n(query)\\n\\nids string\\n\\n(query)\\n\\nResponses\\n\\nCurl curl -X GET \\\"http://localhost:5101/spi/v1/Cata\", \"log/itens?pogeSize=12&amp;pogeIndex=0\\\" -H \\\"accept: text/plain\\\"\\n\\nRequest URL\\n\\nhttp://locelhost:5101/s\", \"pi/v1/Catalog/itens7pageSize=12&amp;pageIndex=0\\n\\nServer response\\n\\nCode\\n\\n200\\n\\nSwagger Ul\\n\\n\\u00d7 + V\\n\\nloca\", \"lhost:5101/swagger/|\\n\\n/api/v1/Catalog/items\\n\\n-\\n\\nV\\n\\nFigure 6 -9. Swashbuckle UI testing the Catalog/I\", \"tems API method\\n\\n<!-- image -->\\n\\nThe Swagger UI API detail shows a sample of the response and can be\", \" used to execute the real API, which is great for developer discovery. Figure 6-10 shows the Swagger\", \" JSON metadata generated from the eShopOnContainers microservice (which is what the tools use undern\", \"eath) when you request http://&lt;your-root-url&gt;/swagger/v1/swagger.json using Postman .\\n\\nX\\n\\nRunn\", \"er\\n\\nFilter\\n\\nHistory\\n\\nCollections\\n\\nToday\\n\\nGET http://localhost:5101/swagger/v1/swagl ger.json\\n\\nNovemb\", \"er 8\\n\\nGET http://10.120.158.69:8740/api/vehicle s/?tenantid=CDLTLL\\n\\nGET http://10.106.144.95:8740/ap\", \"i/vehicle s/?tenantid=CDLTLL|\\n\\nGET http://myworld-cluster.westus.cloudap p.azure.com:8740/api/vehicl\", \"es/?tenant\\n\\nid=CDLTLL|\\n\\nNovember 6\\n\\nGET http://myworld-cluster.westus.cloudap p.azure.com:8740/api/v\", \"ehicles/\\n\\nSYNC OFF\\n\\nNo Environment\\n\\nParams\\n\\nSave\\n\\nFigure 6 -10. Swagger JSON metadata\\n\\n<!-- image --\", \">\\n\\nIt is that simple. And because it is automatically generated, the Swagger metadata will grow when\", \" you add more functionality to your API.\\n\\n## Additional resources\\n\\n- ASP.NET Web API Help Pages usin\", \"g Swagger https://learn.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger\\n- Get s\", \"tarted with Swashbuckle and ASP.NET Core https://learn.microsoft.com/aspnet/core/tutorials/getting-s\", \"tarted-with-swashbuckle\\n- Get started with NSwag and ASP.NET Core https://learn.microsoft.com/aspnet\", \"/core/tutorials/getting-started-with-nswag\\n\\n## Defining your multi -container application with docke\", \"r -compose.yml\\n\\nIn this guide, the docker-compose.yml file was introduced in the section Step 4. Def\", \"ine your services in docker -compose.yml when building a multi-container Docker application. However\", \", there are additional ways to use the docker-compose files that are worth exploring in further deta\", \"il.\\n\\nFor example, you can explicitly describe how you want to deploy your multi-container applicatio\", \"n in the docker -compose.yml file. Optionally, you can also describe how you are going to build your\", \" custom Docker images. (Custom Docker images can also be built with the Docker CLI.)\\n\\nSend\\n\\nImport h\", \"ttp://localhost:5101/ X\\n\\nBuilder\\n\\nTeam Library\\n\\n+\\n\\nGET Y\\n\\nhttp://localhost:5101/swagger/v1/swagger.j\", \"son\\n\\nSign In\\n\\n-\\n\\nX\\n\\nBasically, you define each of the containers you want to deploy plus certain cha\", \"racteristics for each container deployment. Once you have a multi-container deployment description f\", \"ile, you can deploy the whole solution in a single action orchestrated by the docker-compose up CLI \", \"command, or you can deploy it transparently from Visual Studio. Otherwise, you would need to use the\", \" Docker CLI to deploy container-by-container in multiple steps by using the docker run command from \", \"the command line. Therefore, each service defined in docker -compose.yml must specify exactly one im\", \"age or build. Other keys are optional, and are analogous to their docker run command-line counterpar\", \"ts.\\n\\nThe following YAML code is the definition of a possible global but single docker-compose.yml fi\", \"le for the eShopOnContainers sample. This code is not the actual docker-compose file from eShopOnCon\", \"tainers. Instead, it is a simplified and consolidated version in a single file, which is not the bes\", \"t way to work with docker-compose files, as will be explained later.\\n\\n```\\nversion: '3.4' services: w\", \"ebmvc: image: eshop/webmvc environment: -CatalogUrl=http://catalog-api -OrderingUrl=http://ordering-\", \"api -BasketUrl=http://basket-api ports: -\\\"5100:80\\\" depends_on: -catalog-api -ordering-api -basket -a\", \"pi catalog-api: image: eshop/catalog-api environment: -ConnectionString=Server=sqldata;Initial Catal\", \"og=CatalogData;User Id=sa;Password=[PLACEHOLDER] expose: -\\\"80\\\" ports: -\\\"5101:80\\\" #extra hosts can be\", \" used for standalone SQL Server or services at the dev PC extra_hosts: -\\\"CESARDLSURFBOOK:10.0.75.1\\\" \", \"depends_on: -sqldata ordering-api: image: eshop/ordering-api environment: -ConnectionString=Server=s\", \"qldata;Database=Services.OrderingDb;User Id=sa;Password=[PLACEHOLDER] ports: -\\\"5102:80\\\" #extra hosts\", \" can be used for standalone SQL Server or services at the dev PC extra_hosts: -\\\"CESARDLSURFBOOK:10.0\", \".75.1\\\" depends_on: -sqldata\\n```\\n\\n```\\nbasket -api: image: eshop/basket-api environment: -ConnectionSt\", \"ring=sqldata ports: -\\\"5103:80\\\" depends_on: -sqldata sqldata: environment: -SA_PASSWORD=[PLACEHOLDER]\", \" -ACCEPT_EULA=Y ports: -\\\"5434:1433\\\" basketdata: image: redis\\n```\\n\\nThe root key in this file is servi\", \"ces. Under that key, you define the services you want to deploy and run when you execute the docker-\", \"compose up command or when you deploy from Visual Studio by using this docker -compose.yml file. In \", \"this case, the docker-compose.yml file has multiple services defined, as described in the following \", \"table.\\n\\n| Service name   | Description                                                              \", \"                              |\\n|----------------|--------------------------------------------------\", \"------------------------------------------------------|\\n| webmvc         | Container including the A\", \"SP.NET Core MVC  application consuming the microservices from  server-side C# |\\n| catalog-api    | C\", \"ontainer including the Catalog ASP.NET Core  Web API microservice                                   \", \"  |\\n| ordering-api   | Container including the Ordering ASP.NET  Core Web API microservice          \", \"                          |\\n| sqldata        | Container running SQL Server for Linux,  holding the \", \"microservices databases                           |\\n| basket - api   | Container with the Basket ASP\", \".NET Core Web  API microservice                                           |\\n| basketdata     | Conta\", \"iner running the REDIS cache service,  with the basket database as a REDIS cache                  |\\n\", \"\\n## A simple Web Service API container\\n\\nFocusing on a single container, the catalog-api container-mi\", \"croservice has a straightforward definition:\\n\\n```\\ncatalog-api: image: eshop/catalog-api environment:\", \" -ConnectionString=Server=sqldata;Initial Catalog=CatalogData;User Id=sa;Password=[PLACEHOLDER] expo\", \"se:\\n```\\n\\n```\\n-\\\"80\\\" ports: -\\\"5101:80\\\" #extra hosts can be used for standalone SQL Server or services \", \"at the dev PC extra_hosts: -\\\"CESARDLSURFBOOK:10.0.75.1\\\" depends_on: -sqldata\\n```\\n\\nThis containerized\", \" service has the following basic configuration:\\n\\n- It is based on the custom eshop/catalog-api image\", \". For simplicity's sake, there is no build: key setting in the file. This means that the image must \", \"have been previously built (with docker build) or have been downloaded (with the docker pull command\", \") from any Docker registry.\\n- It defines an environment variable named ConnectionString with the con\", \"nection string to be used by Entity Framework to access the SQL Server instance that contains the ca\", \"talog data model. In this case, the same SQL Server container is holding multiple databases. Therefo\", \"re, you need less memory in your development machine for Docker. However, you could also deploy one \", \"SQL Server container for each microservice database.\\n- The SQL Server name is sqldata, which is the \", \"same name used for the container that is running the SQL Server instance for Linux. This is convenie\", \"nt; being able to use this name resolution (internal to the Docker host) will resolve the network ad\", \"dress so you don't need to know the internal IP for the containers you are accessing from other cont\", \"ainers.\\n\\nBecause the connection string is defined by an environment variable, you could set that var\", \"iable through a different mechanism and at a different time. For example, you could set a different \", \"connection string when deploying to production in the final hosts, or by doing it from your CI/CD pi\", \"pelines in Azure DevOps Services or your preferred DevOps system.\\n\\n- It exposes port 80 for internal\", \" access to the catalog-api service within the Docker host. The host is currently a Linux VM because \", \"it is based on a Docker image for Linux, but you could configure the container to run on a Windows i\", \"mage instead.\\n- It forwards the exposed port 80 on the container to port 5101 on the Docker host mac\", \"hine (the Linux VM).\\n- It links the web service to the sqldata service (the SQL Server instance for \", \"Linux database running in a container). When you specify this dependency, the catalog-api container \", \"will not start until the sqldata container has already started; this aspect is important because cat\", \"alogapi needs to have the SQL Server database up and running first. However, this kind of container \", \"dependency is not enough in many cases, because Docker checks only at the container level. Sometimes\", \" the service (in this case SQL Server) might still not be ready, so it is advisable to implement ret\", \"ry logic with exponential backoff in your client microservices. That way, if a dependency container \", \"is not ready for a short time, the application will still be resilient.\\n- It is configured to allow \", \"access to external servers: the extra\\\\_hosts setting allows you to access external servers or machin\", \"es outside of the Docker host (that is, outside the default Linux VM,\\n\\nwhich is a development Docker\", \" host), such as a local SQL Server instance on your development PC.\\n\\nThere are also other, more adva\", \"nced docker-compose.yml settings that we'll discuss in the following sections.\\n\\n## Using docker-comp\", \"ose files to target multiple environments\\n\\nThe docker -compose.*.yml files are definition files and \", \"can be used by multiple infrastructures that understand that format. The most straightforward tool i\", \"s the docker-compose command.\\n\\nTherefore, by using the docker-compose command you can target the fol\", \"lowing main scenarios.\\n\\n## Development environments\\n\\nWhen you develop applications, it is important \", \"to be able to run an application in an isolated development environment. You can use the docker-comp\", \"ose CLI command to create that environment or Visual Studio, which uses docker-compose under the cov\", \"ers.\\n\\nThe docker -compose.yml file allows you to configure and document all your application's servi\", \"ce dependencies (other services, cache, databases, queues, etc.). Using the docker-compose CLI comma\", \"nd, you can create and start one or more containers for each dependency with a single command (docke\", \"r-compose up).\\n\\nThe docker -compose.yml files are configuration files interpreted by Docker engine b\", \"ut also serve as convenient documentation files about the composition of your multi-container applic\", \"ation.\\n\\n## Testing environments\\n\\nAn important part of any continuous deployment (CD) or continuous i\", \"ntegration (CI) process are the unit tests and integration tests. These automated tests require an i\", \"solated environment so they are not impacted by the users or any other change in the application's d\", \"ata.\\n\\nWith Docker Compose, you can create and destroy that isolated environment very easily in a few\", \" commands from your command prompt or scripts, like the following commands:\\n\\ndocker -compose -f dock\", \"er -compose.yml -f docker-compose-test.override.yml up -d ./run\\\\_unit\\\\_tests docker -compose -f dock\", \"er -compose.yml -f docker-compose-test.override.yml down\\n\\n## Production deployments\\n\\nYou can also us\", \"e Compose to deploy to a remote Docker Engine. A typical case is to deploy to a single Docker host i\", \"nstance (like a production VM or server provisioned with Docker Machine).\\n\\nIf you are using any othe\", \"r orchestrator (Azure Service Fabric, Kubernetes, etc.), you might need to add setup and metadata co\", \"nfiguration settings like those in docker-compose.yml, but in the format required by the other orche\", \"strator.\\n\\nIn any case, docker-compose is a convenient tool and metadata format for development, test\", \"ing and production workflows, although the production workflow might vary on the orchestrator you ar\", \"e using.\\n\\n## Using multiple docker-compose files to handle several environments\\n\\nWhen targeting diff\", \"erent environments, you should use multiple compose files. This approach lets you create multiple co\", \"nfiguration variants depending on the environment.\\n\\n## Overriding the base docker-compose file\\n\\nYou \", \"could use a single docker-compose.yml file as in the simplified examples shown in previous sections.\", \" However, that is not recommended for most applications.\\n\\nBy default, Compose reads two files, a doc\", \"ker-compose.yml and an optional dockercompose.override.yml file. As shown in Figure 6-11, when you a\", \"re using Visual Studio and enabling Docker support, Visual Studio also creates an additional docker-\", \"compose.vs.debug.g.yml file for debugging the application, you can take a look at this file in folde\", \"r obj\\\\Docker\\\\ in the main solution folder.\\n\\nFigure 6 -11. docker -compose files in Visual Studio 201\", \"9\\n\\n<!-- image -->\\n\\ndocker -compose project file structure:\\n\\n- .dockerignore - used to ignore files\\n-\", \" docker -compose.yml - used to compose microservices\\n- docker -compose.override.yml - used to config\", \"ure microservices environment\\n\\nYou can edit the docker -compose files with any editor, like Visual S\", \"tudio Code or Sublime, and run the application with the docker-compose up command.\\n\\nBy convention, t\", \"he docker-compose.yml file contains your base configuration and other static settings. That means th\", \"at the service configuration should not change depending on the deployment environment you are targe\", \"ting.\\n\\nThe docker -compose.override.yml file, as its name suggests, contains configuration settings \", \"that override the base configuration, such as configuration that depends on the deployment environme\", \"nt. You can have multiple override files with different names also. The override files usually conta\", \"in additional information needed by the application but specific to an environment or to a deploymen\", \"t.\\n\\n## Targeting multiple environments\\n\\nA typical use case is when you define multiple compose files\", \" so you can target multiple environments, like production, staging, CI, or development. To support t\", \"hese differences, you can split your Compose configuration into multiple files, as shown in Figure 6\", \"-12.\\n\\nMultiple docker-compose files docker-compose.override.yml\\n\\ndocker-compose.prod.yml docker-comp\", \"ose.staging.yml\\n\\nFigure 6 -12. Multiple docker -compose files overriding values in the base docker -\", \"compose.yml file\\n\\n<!-- image -->\\n\\nYou can combine multiple docker-compose*.yml files to handle diffe\", \"rent environments. You start with the base docker -compose.yml file. This base file contains the bas\", \"e or static configuration settings that do not change depending on the environment. For example, the\", \" eShopOnContainers app has the following docker-compose.yml file (simplified with fewer services) as\", \" the base file.\\n\\n```\\n#docker -compose.yml (Base) version: '3.4' services: basket -api: image: eshop/\", \"basket-api:${TAG:-latest} build: context: . dockerfile: src/Services/Basket/Basket.API/Dockerfile de\", \"pends_on: -basketdata -identity-api -rabbitmq catalog-api: image: eshop/catalog-api:${TAG:-latest} b\", \"uild: context: . dockerfile: src/Services/Catalog/Catalog.API/Dockerfile depends_on: -sqldata -rabbi\", \"tmq marketing-api: image: eshop/marketing-api:${TAG:-latest} build: context: . dockerfile: src/Servi\", \"ces/Marketing/Marketing.API/Dockerfile depends_on: -sqldata -nosqldata -identity-api -rabbitmq webmv\", \"c: image: eshop/webmvc:${TAG:-latest}\\n```\\n\\n```\\nbuild: context: . dockerfile: src/Web/WebMVC/Dockerfi\", \"le depends_on: -catalog-api -ordering-api -identity-api -basket -api -marketing-api sqldata: image: \", \"mcr.microsoft.com/mssql/server:2019-latest nosqldata: image: mongo basketdata: image: redis rabbitmq\", \": image: rabbitmq:3-management\\n```\\n\\nThe values in the base docker -compose.yml file should not chang\", \"e because of different target deployment environments.\\n\\nIf you focus on the webmvc service definitio\", \"n, for instance, you can see how that information is much the same no matter what environment you mi\", \"ght be targeting. You have the following information:\\n\\n- The service name: webmvc.\\n- The container's\", \" custom image: eshop/webmvc.\\n- The command to build the custom Docker image, indicating which Docker\", \"file to use.\\n- Dependencies on other services, so this container does not start until the other depe\", \"ndency containers have started.\\n\\nYou can have additional configuration, but the important point is t\", \"hat in the base dockercompose.yml file, you just want to set the information that is common across e\", \"nvironments. Then in the docker -compose.override.yml or similar files for production or staging, yo\", \"u should place configuration that is specific for each environment.\\n\\nUsually, the docker-compose.ove\", \"rride.yml is used for your development environment, as in the following example from eShopOnContaine\", \"rs:\\n\\n```\\n#docker -compose.override.yml (Extended config for DEVELOPMENT env.) version: '3.4' service\", \"s: # Simplified number of services here: basket -api: environment: -ASPNETCORE_ENVIRONMENT=Developme\", \"nt -ASPNETCORE_URLS=http://0.0.0.0:80 -ConnectionString=${ESHOP_AZURE_REDIS_BASKET_DB:-basketdata} -\", \"identityUrl=http://identity-api -IdentityUrlExternal=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5105\\n``\", \"`\\n\\n```\\n-EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq} -EventBusUserName=${ESHOP_SERVICE_BU\", \"S_USERNAME} -EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD} -AzureServiceBusEnabled=False -Applicati\", \"onInsights__InstrumentationKey=${INSTRUMENTATION_KEY} -OrchestratorType=${ORCHESTRATOR_TYPE} -UseLoa\", \"dTest=${USE_LOADTEST:-False} ports: -\\\"5103:80\\\" catalog-api: environment: -ASPNETCORE_ENVIRONMENT=Dev\", \"elopment -ASPNETCORE_URLS=http://0.0.0.0:80 -ConnectionString=${ESHOP_AZURE_CATALOG_DB:-Server=sqlda\", \"ta;Database=Microsoft.eShopOnContainers.Services.CatalogDb;User Id=sa;Password=[PLACEHOLDER]} -PicBa\", \"seUrl=${ESHOP_AZURE_STORAGE_CATALOG_URL:-http://host.docker.internal:5202/api/v1/catalog/items/[0]/p\", \"ic/} -EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq} -EventBusUserName=${ESHOP_SERVICE_BUS_\", \"USERNAME} -EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD} -AzureStorageAccountName=${ESHOP_AZURE_STO\", \"RAGE_CATALOG_NAME} -AzureStorageAccountKey=${ESHOP_AZURE_STORAGE_CATALOG_KEY} -UseCustomizationData=\", \"True -AzureServiceBusEnabled=False -AzureStorageEnabled=False -ApplicationInsights__InstrumentationK\", \"ey=${INSTRUMENTATION_KEY} -OrchestratorType=${ORCHESTRATOR_TYPE} ports: -\\\"5101:80\\\" marketing-api: en\", \"vironment: -ASPNETCORE_ENVIRONMENT=Development -ASPNETCORE_URLS=http://0.0.0.0:80 -ConnectionString=\", \"${ESHOP_AZURE_MARKETING_DB:-Server=sqldata;Database=Microsoft.eShopOnContainers.Services.MarketingDb\", \";User Id=sa;Password=[PLACEHOLDER]} -MongoConnectionString=${ESHOP_AZURE_COSMOSDB:-mongodb://nosqlda\", \"ta} -MongoDatabase=MarketingDb -EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq} -EventBusUse\", \"rName=${ESHOP_SERVICE_BUS_USERNAME} -EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD} -identityUrl=htt\", \"p://identity-api -IdentityUrlExternal=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5105 -CampaignDetailFu\", \"nctionUri=${ESHOP_AZUREFUNC_CAMPAIGN_DETAILS_URI} -PicBaseUrl=${ESHOP_AZURE_STORAGE_MARKETING_URL:-h\", \"ttp://host.docker.internal:5110/api/v1/campaigns/[0]/pic/} -AzureStorageAccountName=${ESHOP_AZURE_ST\", \"ORAGE_MARKETING_NAME} -AzureStorageAccountKey=${ESHOP_AZURE_STORAGE_MARKETING_KEY} -AzureServiceBusE\", \"nabled=False -AzureStorageEnabled=False -ApplicationInsights__InstrumentationKey=${INSTRUMENTATION_K\", \"EY} -OrchestratorType=${ORCHESTRATOR_TYPE} -UseLoadTest=${USE_LOADTEST:-False} ports: -\\\"5110:80\\\" web\", \"mvc:\\n```\\n\\n```\\nenvironment: -ASPNETCORE_ENVIRONMENT=Development -ASPNETCORE_URLS=http://0.0.0.0:80 -P\", \"urchaseUrl=http://webshoppingapigw -IdentityUrl=http://10.0.75.1:5105 -MarketingUrl=http://webmarket\", \"ingapigw -CatalogUrlHC=http://catalog-api/hc -OrderingUrlHC=http://ordering-api/hc -IdentityUrlHC=ht\", \"tp://identity-api/hc -BasketUrlHC=http://basket-api/hc -MarketingUrlHC=http://marketing-api/hc -Paym\", \"entUrlHC=http://payment-api/hc -SignalrHubUrl=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5202 -UseCusto\", \"mizationData=True -ApplicationInsights__InstrumentationKey=${INSTRUMENTATION_KEY} -OrchestratorType=\", \"${ORCHESTRATOR_TYPE} -UseLoadTest=${USE_LOADTEST:-False} ports: -\\\"5100:80\\\" sqldata: environment: -SA\", \"_PASSWORD=[PLACEHOLDER] -ACCEPT_EULA=Y ports: -\\\"5433:1433\\\" nosqldata: ports: -\\\"27017:27017\\\" basketda\", \"ta: ports: -\\\"6379:6379\\\" rabbitmq: ports: -\\\"15672:15672\\\" -\\\"5672:5672\\\"\\n```\\n\\nIn this example, the devel\", \"opment override configuration exposes some ports to the host, defines environment variables with red\", \"irect URLs, and specifies connection strings for the development environment. These settings are all\", \" just for the development environment.\\n\\nWhen you run docker-compose up (or launch it from Visual Stu\", \"dio), the command reads the overrides automatically as if it were merging both files.\\n\\nSuppose that \", \"you want another Compose file for the production environment, with different configuration values, p\", \"orts, or connection strings. You can create another override file, like file named docker -compose.p\", \"rod.yml with different settings and environment variables. That file might be stored in a different \", \"Git repo or managed and secured by a different team.\\n\\n## How to deploy with a specific override file\", \"\\n\\nTo use multiple override files, or an override file with a different name, you can use the -f opti\", \"on with the docker -compose command and specify the files. Compose merges files in the order they ar\", \"e specified on the command line. The following example shows how to deploy with override files.\\n\\n```\", \"\\ndocker -compose -f docker -compose.yml -f docker-compose.prod.yml up -d\\n```\\n\\n## Using environment v\", \"ariables in docker-compose files\\n\\nIt is convenient, especially in production environments, to be abl\", \"e to get configuration information from environment variables, as we have shown in previous examples\", \". You can reference an environment variable in your docker-compose files using the syntax ${MY\\\\_VAR}\", \". The following line from a docker -compose.prod.yml file shows how to reference the value of an env\", \"ironment variable.\\n\\nIdentityUrl=http://${ESHOP\\\\_PROD\\\\_EXTERNAL\\\\_DNS\\\\_NAME\\\\_OR\\\\_IP}:5105\\n\\nEnvironment\", \" variables are created and initialized in different ways, depending on your host environment (Linux,\", \" Windows, Cloud cluster, etc.). However, a convenient approach is to use an .env file. The docker -c\", \"ompose files support declaring default environment variables in the .env file. These values for the \", \"environment variables are the default values. But they can be overridden by the values you might hav\", \"e defined in each of your environments (host OS or environment variables from your cluster). You pla\", \"ce this .env file in the folder where the docker-compose command is executed from.\\n\\nThe following ex\", \"ample shows an .env file like the .env file for the eShopOnContainers application.\\n\\n```\\n# .env file \", \"ESHOP_EXTERNAL_DNS_NAME_OR_IP=host.docker.internal ESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP=10.121.122.92\\n\", \"```\\n\\nDocker -compose expects each line in an .env file to be in the format &lt;variable&gt;=&lt;valu\", \"e&gt;.\\n\\nThe values set in the run -time environment always override the values defined inside the .e\", \"nv file. In a similar way, values passed via command-line arguments also override the default values\", \" set in the .env file.\\n\\n## Additional resources\\n\\n- Overview of Docker Compose\\n- https://docs.docker.\", \"com/compose/overview/\\n- Multiple Compose files\\n\\nhttps://docs.docker.com/compose/multiple-compose-fil\", \"es/\\n\\n## Building optimized ASP.NET Core Docker images\\n\\nIf you are exploring Docker and .NET on sourc\", \"es on the Internet, you will find Dockerfiles that demonstrate the simplicity of building a Docker i\", \"mage by copying your source into a container. These examples suggest that by using a simple configur\", \"ation, you can have a Docker image with the environment packaged with your application. The followin\", \"g example shows a simple Dockerfile in this vein.\\n\\n```\\nFROM mcr.microsoft.com/dotnet/sdk:7.0 WORKDIR\", \" /app ENV ASPNETCORE_URLS http://+:80 EXPOSE 80 COPY . . RUN dotnet restore ENTRYPOINT [\\\"dotnet\\\" , \\\"\", \"run\\\"]\\n```\\n\\nA Dockerfile like this will work. However, you can substantially optimize your images, es\", \"pecially your production images.\\n\\nIn the container and microservices model, you are constantly start\", \"ing containers. The typical way of using containers does not restart a sleeping container, because t\", \"he container is disposable. Orchestrators (like Kubernetes and Azure Service Fabric) create new inst\", \"ances of images. What this means is that you would need to optimize by precompiling the application \", \"when it is built so the instantiation process will be faster. When the container is started, it shou\", \"ld be ready to run. Don't restore and compile at run time using the dotnet restore and dotnet build \", \"CLI commands as you may see in blog posts about .NET and Docker.\\n\\nThe .NET team has been doing impor\", \"tant work to make .NET and ASP.NET Core a container-optimized framework. Not only is .NET a lightwei\", \"ght framework with a small memory footprint; the team has focused on optimized Docker images for thr\", \"ee main scenarios and published them in the Docker Hub registry at dotnet/, beginning with version 2\", \".1:\\n\\n1. Development: The priority is the ability to quickly iterate and debug changes, and where siz\", \"e is secondary.\\n2. Build: The priority is compiling the application, and the image includes binaries\", \" and other dependencies to optimize binaries.\\n3. Production: The focus is fast deploying and startin\", \"g of containers, so these images are limited to the binaries and content needed to run the applicati\", \"on.\\n\\nThe .NET team provides four basic variants in dotnet/ (at Docker Hub):\\n\\n1. sdk: for development\", \" and build scenarios\\n2. aspnet: for ASP.NET production scenarios\\n3. runtime: for .NET production sce\", \"narios\\n4. runtime -deps: for production scenarios of self-contained applications\\n\\nFor faster startup\", \", runtime images also automatically set aspnetcore\\\\_urls to port 80 and use Ngen to create a native \", \"image cache of assemblies.\\n\\n## Additional resources\\n\\n- Building Optimized Docker Images with ASP.NET\", \" Core\\n\\nhttps://learn.microsoft.com/archive/blogs/stevelasker/building-optimized-docker-imageswith -a\", \"sp -net -core\\n\\n- Building Docker Images for .NET Applications\\n\\nhttps://learn.microsoft.com/dotnet/co\", \"re/docker/building-net-docker-images\\n\\n## Use a database server running as a container\\n\\nYou can have \", \"your databases (SQL Server, PostgreSQL, MySQL, etc.) on regular standalone servers, in on-premises c\", \"lusters, or in PaaS services in the cloud like Azure SQL DB. However, for development and test envir\", \"onments, having your databases running as containers is convenient, because you don't\\n\\nhave any exte\", \"rnal dependency and simply running the docker-compose up command starts the whole application. Havin\", \"g those databases as containers is also great for integration tests, because the database is started\", \" in the container and is always populated with the same sample data, so tests can be more predictabl\", \"e.\\n\\n## SQL Server running as a container with a microservice-related database\\n\\nIn eShopOnContainers,\", \" there's a container named sqldata, as defined in the docker-compose.yml file, that runs a SQL Serve\", \"r for Linux instance with the SQL databases for all microservices that need one.\\n\\nA key point in mic\", \"roservices is that each microservice owns its related data, so it should have its own database. Howe\", \"ver, the databases can be anywhere. In this case, they are all in the same container to keep Docker \", \"memory requirements as low as possible. Keep in mind that this is a good-enough solution for develop\", \"ment and, perhaps, testing but not for production.\\n\\nThe SQL Server container in the sample applicati\", \"on is configured with the following YAML code in the docker -compose.yml file, which is executed whe\", \"n you run docker-compose up. Note that the YAML code has consolidated configuration information from\", \" the generic docker-compose.yml file and the docker -compose.override.yml file. (Usually you would s\", \"eparate the environment settings from the base or static information related to the SQL Server image\", \".)\\n\\n```\\nsqldata: image: mcr.microsoft.com/mssql/server:2017-latest environment: -SA_PASSWORD=Pass@wo\", \"rd -ACCEPT_EULA=Y ports: -\\\"5434:1433\\\"\\n```\\n\\nIn a similar way, instead of using docker-compose, the fo\", \"llowing docker run command can run that container:\\n\\n```\\ndocker run -e 'ACCEPT_EULA=Y' -e 'SA_PASSWOR\", \"D=Pass@word' -p 5433:1433 -d mcr . microsoft . com/mssql/server:2017-latest\\n```\\n\\nHowever, if you are\", \" deploying a multi-container application like eShopOnContainers, it is more convenient to use the do\", \"cker -compose up command so that it deploys all the required containers for the application.\\n\\nWhen y\", \"ou start this SQL Server container for the first time, the container initializes SQL Server with the\", \" password that you provide. Once SQL Server is running as a container, you can update the database b\", \"y connecting through any regular SQL connection, such as from SQL Server Management Studio, Visual S\", \"tudio, or C# code.\\n\\nThe eShopOnContainers application initializes each microservice database with sa\", \"mple data by seeding it with data on startup, as explained in the following section.\\n\\nHaving SQL Ser\", \"ver running as a container is not just useful for a demo where you might not have access to an insta\", \"nce of SQL Server. As noted, it is also great for development and testing\\n\\nenvironments so that you \", \"can easily run integration tests starting from a clean SQL Server image and known data by seeding ne\", \"w sample data.\\n\\n## Additional resources\\n\\n- Run the SQL Server Docker image on Linux, Mac, or Windows\", \" https://learn.microsoft.com/sql/linux/sql-server-linux-setup-docker\\n- Connect and query SQL Server \", \"on Linux with sqlcmd\\n\\nhttps://learn.microsoft.com/sql/linux/sql-server-linux-connect-and-query-sqlcm\", \"d\\n\\n## Seeding with test data on Web application startup\\n\\nTo add data to the database when the applic\", \"ation starts up, you can add code like the following to the Main method in the Program class of the \", \"Web API project:\\n\\n```\\npublic static int Main(string[] args) { var configuration = GetConfiguration()\", \"; Log . Logger = CreateSerilogLogger(configuration); try { Log . Information(\\\"Configuring web host (\", \"{ApplicationContext})...\\\" , AppName); var host = CreateHostBuilder(configuration , args); Log . Info\", \"rmation(\\\"Applying migrations ({ApplicationContext})...\\\" , AppName); host . MigrateDbContext<CatalogC\", \"ontext>((context , services) => { var env = services . GetService<IWebHostEnvironment>(); var settin\", \"gs = services . GetService<IOptions<CatalogSettings>>(); var logger = services . GetService<ILogger<\", \"CatalogContextSeed>>(); new CatalogContextSeed() . SeedAsync(context , env , settings , logger) . Wa\", \"it(); }) . MigrateDbContext<IntegrationEventLogContext>(( _ , __ ) => { }); Log . Information(\\\"Start\", \"ing web host ({ApplicationContext})...\\\" , AppName); host . Run(); return 0; } catch (Exception ex) {\", \" Log . Fatal(ex , \\\"Program terminated unexpectedly ({ApplicationContext})!\\\" , AppName); return 1; } \", \"finally { Log . CloseAndFlush(); } }\\n```\\n\\nThere's an important caveat when applying migrations and s\", \"eeding a database during container startup. Since the database server might not be available for wha\", \"tever reason, you must handle retries while waiting for the server to be available. This retry logic\", \" is handled by the MigrateDbContext() extension method, as shown in the following code:\\n\\n```\\npublic \", \"static IWebHost MigrateDbContext<TContext>( this IWebHost host , Action<TContext , IServiceProvider>\", \" seeder) where TContext : DbContext { var underK8s = host . IsInKubernetes(); using (var scope = hos\", \"t . Services . CreateScope()) { var services = scope . ServiceProvider; var logger = services . GetR\", \"equiredService<ILogger<TContext>>(); var context = services . GetService<TContext>(); try { logger .\", \" LogInformation(\\\"Migrating database associated with context {DbContextName}\\\" , typeof(TContext).Name\", \"); if (underK8s) { InvokeSeeder(seeder , context , services); } else { var retry = Policy . Handle<S\", \"qlException>() . WaitAndRetry(new TimeSpan[] { TimeSpan . FromSeconds(3), TimeSpan . FromSeconds(5),\", \" TimeSpan . FromSeconds(8), }); //if the sql server container is not created on run docker compose t\", \"his //migration can't fail for network related exception. The retry options for DbContext only //app\", \"ly to transient exceptions // Note that this is NOT applied when running some orchestrators (let the\", \" orchestrator to recreate the failing service) retry . Execute(() => InvokeSeeder(seeder , context ,\", \" services)); } logger . LogInformation(\\\"Migrated database associated with context {DbContextName}\\\" ,\", \" typeof(TContext).Name); } catch (Exception ex) { logger . LogError(ex , \\\"An error occurred while mi\", \"grating the database used on context {DbContextName}\\\" , typeof(TContext).Name); if (underK8s) { thro\", \"w; // Rethrow under k8s because we rely on k8s to re-run the\\n```\\n\\n```\\npod } } } return host; }\\n```\\n\\n\", \"The following code in the custom CatalogContextSeed class populates the data.\\n\\n```\\npublic class Cata\", \"logContextSeed { public static async Task SeedAsync(IApplicationBuilder applicationBuilder) { var co\", \"ntext = (CatalogContext)applicationBuilder . ApplicationServices . GetService(typeof(CatalogContext)\", \"); using (context) { context . Database . Migrate(); if (!context . CatalogBrands . Any()) { context\", \" . CatalogBrands . AddRange( GetPreconfiguredCatalogBrands()); await context . SaveChangesAsync(); }\", \" if (!context . CatalogTypes . Any()) { context . CatalogTypes . AddRange( GetPreconfiguredCatalogTy\", \"pes()); await context . SaveChangesAsync(); } } } static IEnumerable<CatalogBrand> GetPreconfiguredC\", \"atalogBrands() { return new List<CatalogBrand>() { new CatalogBrand() { Brand = \\\"Azure\\\"}, new Catalo\", \"gBrand() { Brand = \\\".NET\\\" }, new CatalogBrand() { Brand = \\\"Visual Studio\\\" }, new CatalogBrand() { Br\", \"and = \\\"SQL Server\\\" } }; } static IEnumerable<CatalogType> GetPreconfiguredCatalogTypes() { return ne\", \"w List<CatalogType>() { new CatalogType() { Type = \\\"Mug\\\"}, new CatalogType() { Type = \\\"T-Shirt\\\" }, n\", \"ew CatalogType() { Type = \\\"Backpack\\\" }, new CatalogType() { Type = \\\"USB Memory Stick\\\" } }; } }\\n```\\n\\n\", \"When you run integration tests, having a way to generate data consistent with your integration tests\", \" is useful. Being able to create everything from scratch, including an instance of SQL Server runnin\", \"g on a container, is great for test environments.\\n\\n## EF Core InMemory database versus SQL Server ru\", \"nning as a container\\n\\nAnother good choice when running tests is to use the Entity Framework InMemory\", \" database provider. You can specify that configuration in the ConfigureServices method of the Startu\", \"p class in your Web API project:\\n\\n```\\npublic class Startup { // Other Startup code ... public void C\", \"onfigureServices(IServiceCollection services) { services . AddSingleton<IConfiguration>(Configuratio\", \"n); // DbContext using an InMemory database provider services . AddDbContext<CatalogContext>(opt => \", \"opt . UseInMemoryDatabase()); //(Alternative: DbContext using a SQL Server provider //services.AddDb\", \"Context<CatalogContext>(c => //{ // c.UseSqlServer(Configuration[\\\"ConnectionString\\\"]); // //}); } //\", \" Other Startup code ... }\\n```\\n\\nThere is an important catch, though. The in-memory database does not \", \"support many constraints that are specific to a particular database. For instance, you might add a u\", \"nique index on a column in your EF Core model and write a test against your in-memory database to ch\", \"eck that it does not let you add a duplicate value. But when you are using the in-memory database, y\", \"ou cannot handle unique indexes on a column. Therefore, the in -memory database does not behave exac\", \"tly the same as a real SQL Server database \\u2014 it does not emulate database -specific constraints.\\n\\nEv\", \"en so, an in-memory database is still useful for testing and prototyping. But if you want to create \", \"accurate integration tests that take into account the behavior of a specific database implementation\", \", you need to use a real database like SQL Server. For that purpose, running SQL Server in a contain\", \"er is a great choice and more accurate than the EF Core InMemory database provider.\\n\\n## Using a Redi\", \"s cache service running in a container\\n\\nYou can run Redis on a container, especially for development\", \" and testing and for proof-of-concept scenarios. This scenario is convenient, because you can have a\", \"ll your dependencies running on containers \\u2014 not just for your local development machines, but for y\", \"our testing environments in your CI/CD pipelines.\\n\\nHowever, when you run Redis in production, it is \", \"better to look for a high-availability solution like Redis Microsoft Azure, which runs as a PaaS (Pl\", \"atform as a Service). In your code, you just need to change your connection strings.\\n\\nRedis provides\", \" a Docker image with Redis. That image is available from Docker Hub at this URL:\\n\\nhttps://hub.docker\", \".com/\\\\_/redis/\\n\\nYou can directly run a Docker Redis container by executing the following Docker CLI \", \"command in your command prompt:\\n\\n```\\ndocker run --name some-redis -d redis\\n```\\n\\nThe Redis image incl\", \"udes expose:6379 (the port used by Redis), so standard container linking will make it automatically \", \"available to the linked containers.\\n\\nIn eShopOnContainers, the basket-api microservice uses a Redis \", \"cache running as a container. That basketdata container is defined as part of the multi-container do\", \"cker-compose.yml file, as shown in the following example:\\n\\n```\\n#docker -compose.yml file #... basket\", \"data: image: redis expose: -\\\"6379\\\"\\n```\\n\\nThis code in the docker -compose.yml defines a container nam\", \"ed basketdata based on the redis image and publishing the port 6379 internally. This configuration m\", \"eans that it will only be accessible from other containers running within the Docker host.\\n\\nFinally,\", \" in the docker-compose.override.yml file, the basket-api microservice for the eShopOnContainers samp\", \"le defines the connection string to use for that Redis container:\\n\\n```\\nbasket -api: environment: # O\", \"ther data ... -ConnectionString=basketdata -EventBusConnection=rabbitmq\\n```\\n\\nAs mentioned before, th\", \"e name of the microservice basketdata is resolved by Docker's internal network DNS.\\n\\n## Implementing\", \" event -based communication between microservices (integration events)\\n\\nAs described earlier, when y\", \"ou use event-based communication, a microservice publishes an event when something notable happens, \", \"such as when it updates a business entity. Other microservices subscribe to those events. When a mic\", \"roservice receives an event, it can update its own business entities, which might lead to more event\", \"s being published. This is the essence of the eventual consistency concept. This publish/subscribe s\", \"ystem is usually performed by using an implementation of an event bus. The event bus can be designed\", \" as an interface with the API needed to subscribe and unsubscribe to events and to publish events. I\", \"t can also have one or more implementations based on any inter-process or messaging communication, s\", \"uch as a messaging queue or a service bus that supports asynchronous communication and a publish/sub\", \"scribe model.\\n\\nYou can use events to implement business transactions that span multiple services, wh\", \"ich give you eventual consistency between those services. An eventually consistent transaction consi\", \"sts of a series\\n\\nImplementing asynchronous event-driven communication with an event bus\\n\\n- -\\n\\n1\\n\\nof \", \"distributed actions. At each action, the microservice updates a business entity and publishes an eve\", \"nt that triggers the next action. Figure 6-18 below, shows a PriceUpdated event published through an\", \" event bus, so the price update is propagated to the Basket and other microservices.\\n\\nBackend\\n\\nCatal\", \"og microservice\\n\\n1\\n\\nWeb API Service\\n\\nUpdatePrice command\\n\\nService\\n\\n## 3 Event bus PriceUpdated event\", \" ! * (Publish/Subscribe channel) (Publish action) Event bus abstractions/interface \\u00b7 PriceUpdated ev\", \"ent \\u2192 Existing basket items 4 \\u00b7 PriceUpdated event \\u2192 Other related features\\n\\nDatabase\\n\\nDB update\\n\\nFi\", \"gure 6 -18. Event -driven communication based on an event bus\\n\\n<!-- image -->\\n\\nThis section describe\", \"s how you can implement this type of communication with .NET by using a generic event bus interface,\", \" as shown in Figure 6-18. There are multiple potential implementations, each using a different techn\", \"ology or infrastructure such as RabbitMQ, Azure Service Bus, or any other third -party open -source \", \"or commercial service bus.\\n\\n## Using message brokers and service buses for production systems\\n\\nAs no\", \"ted in the architecture section, you can choose from multiple messaging technologies for implementin\", \"g your abstract event bus. But these technologies are at different levels. For instance, RabbitMQ, a\", \" messaging broker transport, is at a lower level than commercial products like Azure Service Bus, NS\", \"erviceBus, MassTransit, or Brighter. Most of these products can work on top of either RabbitMQ or Az\", \"ure Service Bus. Your choice of product depends on how many features and how much out -of -the -box \", \"scalability you need for your application.\\n\\nFor implementing just an event bus proof-of-concept for \", \"your development environment, as in the eShopOnContainers sample, a simple implementation on top of \", \"RabbitMQ running as a container might be enough. But for mission-critical and production systems tha\", \"t need high scalability, you might want to evaluate and use Azure Service Bus.\\n\\nIf you require high-\", \"level abstractions and richer features like Sagas for long-running processes that make distributed d\", \"evelopment easier, other commercial and open-source service buses like NServiceBus, MassTransit, and\", \" Brighter are worth evaluating. In this case, the abstractions and API to use would usually be direc\", \"tly the ones provided by those high-level service buses instead of your own abstractions (like the s\", \"imple event bus abstractions provided at eShopOnContainers). For that matter,\\n\\nyou can research the \", \"forked eShopOnContainers using NServiceBus (additional derived sample implemented by Particular Soft\", \"ware).\\n\\nOf course, you could always build your own service bus features on top of lower-level techno\", \"logies like RabbitMQ and Docker, but the work needed to \\\"reinvent the wheel\\\" might be too costly for\", \" a custom enterprise application.\\n\\nTo reiterate: the sample event bus abstractions and implementatio\", \"n showcased in the eShopOnContainers sample are intended to be used only as a proof of concept. Once\", \" you have decided that you want to have asynchronous and event-driven communication, as explained in\", \" the current section, you should choose the service bus product that best fits your needs for produc\", \"tion.\\n\\n## Integration events\\n\\nIntegration events are used for bringing domain state in sync across m\", \"ultiple microservices or external systems. This functionality is done by publishing integration even\", \"ts outside the microservice. When an event is published to multiple receiver microservices (to as ma\", \"ny microservices as are subscribed to the integration event), the appropriate event handler in each \", \"receiver microservice handles the event.\\n\\nAn integration event is basically a data-holding class, as\", \" in the following example:\\n\\n```\\npublic class ProductPriceChangedIntegrationEvent : IntegrationEvent \", \"{ public int ProductId { get; private set; } public decimal NewPrice { get; private set; } public de\", \"cimal OldPrice { get; private set; } public ProductPriceChangedIntegrationEvent(int productId , deci\", \"mal newPrice , decimal oldPrice) { ProductId = productId; NewPrice = newPrice; OldPrice = oldPrice; \", \"} }\\n```\\n\\nThe integration events can be defined at the application level of each microservice, so the\", \"y are decoupled from other microservices, in a way comparable to how ViewModels are defined in the s\", \"erver and client. What is not recommended is sharing a common integration events library across mult\", \"iple microservices; doing that would be coupling those microservices with a single event definition \", \"data library. You do not want to do that for the same reasons that you do not want to share a common\", \" domain model across multiple microservices: microservices must be completely autonomous. For more i\", \"nformation, see this blog post on the amount of data to put in events. Be careful not to take this t\", \"oo far, as this other blog post describes the problem data deficient messages can produce. Your desi\", \"gn of your events should aim to be \\\"just right\\\" for the needs of their consumers.\\n\\nThere are only a \", \"few kinds of libraries you should share across microservices. One is libraries that are final applic\", \"ation blocks, like the Event Bus client API, as in eShopOnContainers. Another is libraries that cons\", \"titute tools that could also be shared as NuGet components, like JSON serializers.\\n\\n## The event bus\", \"\\n\\nEvent Bus\\n\\nEvent\\n\\nAn event bus allows publish/subscribe-style communication between microservices \", \"without requiring the components to explicitly be aware of each other, as shown in Figure 6-19.\\n\\nFig\", \"ure 6 -19. Publish/subscribe basics with an event bus\\n\\n<!-- image -->\\n\\nThe above diagram shows that \", \"microservice A publishes to Event Bus, which distributes to subscribing microservices B and C, witho\", \"ut the publisher needing to know the subscribers. The event bus is related to the Observer pattern a\", \"nd the publish-subscribe pattern.\\n\\n## Observer pattern\\n\\nIn the Observer pattern, your primary object\", \" (known as the Observable) notifies other interested objects (known as Observers) with relevant info\", \"rmation (events).\\n\\n## Publish/Subscribe (Pub/Sub) pattern\\n\\nThe purpose of the Publish/Subscribe patt\", \"ern is the same as the Observer pattern: you want to notify other services when certain events take \", \"place. But there is an important difference between the Observer and Pub/Sub patterns. In the observ\", \"er pattern, the broadcast is performed directly from the observable to the observers, so they \\\"know\\\"\", \" each other. But when using a Pub/Sub pattern, there is a third component, called broker, or message\", \" broker or event bus, which is known by both the publisher and subscriber. Therefore, when using the\", \" Pub/Sub pattern the publisher and the subscribers are precisely decoupled thanks to the mentioned e\", \"vent bus or message broker.\\n\\n## The middleman or event bus\\n\\nHow do you achieve anonymity between pub\", \"lisher and subscriber? An easy way is let a middleman take care of all the communication. An event b\", \"us is one such middleman.\\n\\nAn event bus is typically composed of two parts:\\n\\n- The abstraction or in\", \"terface.\\n- One or more implementations.\\n\\nMicroservice\\n\\nB\\n\\nEvent bus\\n\\nLupublish/subscribe channel\\n\\nIn\", \" Figure 6-19 you can see how, from an application point of view, the event bus is nothing more than \", \"a Pub/Sub channel. The way you implement this asynchronous communication can vary. It can have multi\", \"ple implementations so that you can swap between them, depending on the environment requirements (fo\", \"r example, production versus development environments).\\n\\nIn Figure 6-20, you can see an abstraction \", \"of an event bus with multiple implementations based on infrastructure messaging technologies like Ra\", \"bbitMQ, Azure Service Bus, or another event/message broker. Service message /\\n\\nBus event broker\\n\\nFig\", \"ure 6 -20. Multiple implementations of an event bus\\n\\n<!-- image -->\\n\\nIt's good to have the event bus\", \" defined through an interface so it can be implemented with several technologies, like RabbitMQ, Azu\", \"re Service bus or others. However, and as mentioned previously, using your own abstractions (the eve\", \"nt bus interface) is good only if you need basic event bus features supported by your abstractions. \", \"If you need richer service bus features, you should probably use the API and abstractions provided b\", \"y your preferred commercial service bus instead of your own abstractions.\\n\\n## Defining an event bus \", \"interface\\n\\nLet's start with some implementation code for the event bus interface and possible implem\", \"entations for exploration purposes. The interface should be generic and straightforward, as in the f\", \"ollowing interface.\\n\\n```\\npublic interface IEventBus { void Publish(IntegrationEvent @event); void Su\", \"bscribe<T , TH>() where T : IntegrationEvent where TH : IIntegrationEventHandler<T>; void SubscribeD\", \"ynamic<TH>(string eventName) where TH : IDynamicIntegrationEventHandler;\\n```\\n\\n```\\nvoid UnsubscribeDy\", \"namic<TH>(string eventName) where TH : IDynamicIntegrationEventHandler; void Unsubscribe<T , TH>() w\", \"here TH : IIntegrationEventHandler<T> where T : IntegrationEvent; }\\n```\\n\\nThe Publish method is strai\", \"ghtforward. The event bus will broadcast the integration event passed to it to any microservice, or \", \"even an external application, subscribed to that event. This method is used by the microservice that\", \" is publishing the event.\\n\\nThe Subscribe methods (you can have several implementations depending on \", \"the arguments) are used by the microservices that want to receive events. This method has two argume\", \"nts. The first is the integration event to subscribe to (IntegrationEvent). The second argument is t\", \"he integration event handler (or callback method), named IIntegrationEventHandler&lt;T&gt;, to be ex\", \"ecuted when the receiver microservice gets that integration event message.\\n\\n## Additional resources\\n\", \"\\nSome production-ready messaging solutions:\\n\\n- \\u2022\\n- Azure Service Bus\\n\\nhttps://learn.microsoft.com/az\", \"ure/service-bus-messaging/\\n\\n- NServiceBus\\n\\nhttps://particular.net/nservicebus\\n\\n- \\u2022\\n- MassTransit\\n\\nht\", \"tps://masstransit-project.com/\\n\\n## Implementing an event bus with RabbitMQ for the development or te\", \"st environment\\n\\nWe should start by saying that if you create your custom event bus based on RabbitMQ\", \" running in a container, as the eShopOnContainers application does, it should be used only for your \", \"development and test environments. Don't use it for your production environment, unless you are buil\", \"ding it as a part of a production-ready service bus as described in the Additional resources section\", \" below. A simple custom event bus might be missing many production-ready critical features that a co\", \"mmercial service bus has.\\n\\nOne of the event bus custom implementations in eShopOnContainers is basic\", \"ally a library using the RabbitMQ API. (There's another implementation based on Azure Service Bus.)\\n\", \"\\nThe event bus implementation with RabbitMQ lets microservices subscribe to events, publish events, \", \"and receive events, as shown in Figure 6-21.\\n\\nMessage\\n\\nSender\\n\\nMicroservice\\n\\nOrigin\\n\\nRabbitMQ\\n\\n(Serv\", \"er/Container)\\n\\nFigure 6 -21. RabbitMQ implementation of an event bus\\n\\n<!-- image -->\\n\\nRabbitMQ funct\", \"ions as an intermediary between message publisher and subscribers, to handle distribution. In the co\", \"de, the EventBusRabbitMQ class implements the generic IEventBus interface. This implementation is ba\", \"sed on Dependency Injection so that you can swap from this dev/test version to a production version.\", \"\\n\\n```\\npublic class EventBusRabbitMQ : IEventBus , IDisposable { // Implementation using RabbitMQ API\", \" //... }\\n```\\n\\nThe RabbitMQ implementation of a sample dev/test event bus is boilerplate code. It has\", \" to handle the connection to the RabbitMQ server and provide code for publishing a message event to \", \"the queues. It also has to implement a dictionary of collections of integration event handlers for e\", \"ach event type; these event types can have a different instantiation and different subscriptions for\", \" each receiver microservice, as shown in Figure 6-21.\\n\\n## Implementing a simple publish method with \", \"RabbitMQ\\n\\nThe following code is a simplified version of an event bus implementation for RabbitMQ, to\", \" showcase the whole scenario. You don't really handle the connection this way. To see the full imple\", \"mentation, see the actual code in the dotnet-architecture/eShopOnContainers repository.\\n\\n```\\npublic \", \"class EventBusRabbitMQ : IEventBus , IDisposable { // Member objects and other methods ... // ... pu\", \"blic void Publish(IntegrationEvent @event) { var eventName = @event . GetType().Name; var factory = \", \"new ConnectionFactory() { HostName = _connectionString }; using (var connection = factory . CreateCo\", \"nnection()) using (var channel = connection . CreateModel())\\n```\\n\\nMessage\\n\\nReceivers\\n\\nMicroservice A\", \"\\n\\n```\\n{ channel . ExchangeDeclare(exchange: _brokerName , type: \\\"direct\\\"); string message = JsonConv\", \"ert . SerializeObject(@event); var body = Encoding . UTF8 . GetBytes(message); channel . BasicPublis\", \"h(exchange: _brokerName , routingKey: eventName , basicProperties: null , body: body); } } }\\n```\\n\\nTh\", \"e actual code of the Publish method in the eShopOnContainers application is improved by using a Poll\", \"y retry policy, which retries the task some times in case the RabbitMQ container is not ready. This \", \"scenario can occur when docker -compose is starting the containers; for example, the RabbitMQ contai\", \"ner might start more slowly than the other containers.\\n\\nAs mentioned earlier, there are many possibl\", \"e configurations in RabbitMQ, so this code should be used only for dev/test environments.\\n\\n## Implem\", \"enting the subscription code with the RabbitMQ API\\n\\nAs with the publish code, the following code is \", \"a simplification of part of the event bus implementation for RabbitMQ. Again, you usually do not nee\", \"d to change it unless you are improving it.\\n\\n```\\npublic class EventBusRabbitMQ : IEventBus , IDispos\", \"able { // Member objects and other methods ... // ... public void Subscribe<T , TH>() where T : Inte\", \"grationEvent where TH : IIntegrationEventHandler<T> { var eventName = _subsManager . GetEventKey<T>(\", \"); var containsKey = _subsManager . HasSubscriptionsForEvent(eventName); if (!containsKey) { if (!_p\", \"ersistentConnection . IsConnected) { _persistentConnection . TryConnect(); } using (var channel = _p\", \"ersistentConnection . CreateModel()) { channel . QueueBind(queue: _queueName , exchange: BROKER_NAME\", \" , routingKey: eventName); } } _subsManager . AddSubscription<T , TH>(); } }\\n```\\n\\nEach event type ha\", \"s a related channel to get events from RabbitMQ. You can then have as many event handlers per channe\", \"l and event type as needed.\\n\\nThe Subscribe method accepts an IIntegrationEventHandler object, which \", \"is like a callback method in the current microservice, plus its related IntegrationEvent object. The\", \" code then adds that event handler to the list of event handlers that each integration event type ca\", \"n have per client microservice. If the client code has not already been subscribed to the event, the\", \" code creates a channel for the event type so it can receive events in a push style from RabbitMQ wh\", \"en that event is published from any other service.\\n\\nAs mentioned above, the event bus implemented in\", \" eShopOnContainers has only an educational purpose, since it only handles the main scenarios, so it'\", \"s not ready for production.\\n\\nFor production scenarios check the additional resources below, specific\", \" for RabbitMQ, and the Implementing event-based communication between microservices section.\\n\\n## Add\", \"itional resources\\n\\nA production-ready solution with support for RabbitMQ.\\n\\n- NServiceBus -Fully-supp\", \"orted commercial service bus with advanced management and monitoring tooling for .NET https://partic\", \"ular.net/\\n- EasyNetQ -Open Source .NET API client for RabbitMQ https://easynetq.com/\\n- MassTransit -\", \"Free, open -source distributed application framework for .NET https://masstransit-project.com/\\n- Reb\", \"us -Open source .NET Service Bus https://github.com/rebus-org/Rebus\\n\\n## Subscribing to events\\n\\nThe f\", \"irst step for using the event bus is to subscribe the microservices to the events they want to recei\", \"ve. That functionality should be done in the receiver microservices.\\n\\nThe following simple code show\", \"s what each receiver microservice needs to implement when starting the service (that is, in the Star\", \"tup class) so it subscribes to the events it needs. In this case, the basketapi microservice needs t\", \"o subscribe to ProductPriceChangedIntegrationEvent and the OrderStartedIntegrationEvent messages.\\n\\nF\", \"or instance, when subscribing to the ProductPriceChangedIntegrationEvent event, that makes the baske\", \"t microservice aware of any changes to the product price and lets it warn the user about the change \", \"if that product is in the user's basket.\\n\\n```\\nvar eventBus = app . ApplicationServices . GetRequired\", \"Service<IEventBus>();\\n```\\n\\neventBus . Subscribe&lt;ProductPriceChangedIntegrationEvent ,\\n\\n```\\nProduc\", \"tPriceChangedIntegrationEventHandler>(); eventBus . Subscribe<OrderStartedIntegrationEvent , OrderSt\", \"artedIntegrationEventHandler>();\\n```\\n\\nAfter this code runs, the subscriber microservice will be list\", \"ening through RabbitMQ channels. When any message of type ProductPriceChangedIntegrationEvent arrive\", \"s, the code invokes the event handler that is passed to it and processes the event.\\n\\n## Publishing e\", \"vents through the event bus\\n\\nFinally, the message sender (origin microservice) publishes the integra\", \"tion events with code similar to the following example. (This approach is a simplified example that \", \"does not take atomicity into account.) You would implement similar code whenever an event must be pr\", \"opagated across multiple microservices, usually right after committing data or transactions from the\", \" origin microservice.\\n\\nFirst, the event bus implementation object (based on RabbitMQ or based on a s\", \"ervice bus) would be injected at the controller constructor, as in the following code:\\n\\n```\\n[Route(\\\"\", \"api/v1/[controller]\\\")] public class CatalogController : ControllerBase { private readonly CatalogCon\", \"text _context; private readonly IOptionsSnapshot<Settings> _settings; private readonly IEventBus _ev\", \"entBus; public CatalogController(CatalogContext context , IOptionsSnapshot<Settings> settings , IEve\", \"ntBus eventBus) { _context = context; _settings = settings; _eventBus = eventBus; } // ... }\\n```\\n\\nTh\", \"en you use it from your controller\\u2019s methods, like in the UpdateProduct method:\\n\\n```\\n[Route(\\\"items\\\")\", \"] [HttpPost] public async Task<IActionResult> UpdateProduct([FromBody]CatalogItem product) { var ite\", \"m = await _context . CatalogItems . SingleOrDefaultAsync( i => i . Id == product . Id); // ... if (i\", \"tem . Price != product . Price) { var oldPrice = item . Price; item . Price = product . Price; _cont\", \"ext . CatalogItems . Update(item); var @event = new ProductPriceChangedIntegrationEvent(item . Id , \", \"item . Price , oldPrice); // Commit changes in original transaction await _context . SaveChangesAsyn\", \"c(); // Publish integration event to the event bus // (RabbitMQ or a service bus underneath)\\n```\\n\\n``\", \"`\\n_eventBus . Publish(@event); // ... } // ... }\\n```\\n\\nIn this case, since the origin microservice is\", \" a simple CRUD microservice, that code is placed right into a Web API controller.\\n\\nIn more advanced \", \"microservices, like when using CQRS approaches, it can be implemented in the CommandHandler class, w\", \"ithin the Handle() method.\\n\\n## Designing atomicity and resiliency when publishing to the event bus\\n\\n\", \"When you publish integration events through a distributed messaging system like your event bus, you \", \"have the problem of atomically updating the original database and publishing an event (that is, eith\", \"er both operations complete or none of them). For instance, in the simplified example shown earlier,\", \" the code commits data to the database when the product price is changed and then publishes a Produc\", \"tPriceChangedIntegrationEvent message. Initially, it might look essential that these two operations \", \"be performed atomically. However, if you are using a distributed transaction involving the database \", \"and the message broker, as you do in older systems like Microsoft Message Queuing (MSMQ), this appro\", \"ach is not recommended for the reasons described by the CAP theorem .\\n\\nBasically, you use microservi\", \"ces to build scalable and highly available systems. Simplifying somewhat, the CAP theorem says that \", \"you cannot build a (distributed) database (or a microservice that owns its model) that's continually\", \" available, strongly consistent, and tolerant to any partition. You must choose two of these three p\", \"roperties.\\n\\nIn microservices -based architectures, you should choose availability and tolerance, and\", \" you should de -emphasize strong consistency. Therefore, in most modern microservice-based applicati\", \"ons, you usually do not want to use distributed transactions in messaging, as you do when you implem\", \"ent distributed transactions based on the Windows Distributed Transaction Coordinator (DTC) with MSM\", \"Q .\\n\\nLet's go back to the initial issue and its example. If the service crashes after the database i\", \"s updated (in this case, right after the line of code with \\\\_context.SaveChangesAsync()), but before\", \" the integration event is published, the overall system could become inconsistent. This approach mig\", \"ht be business critical, depending on the specific business operation you are dealing with.\\n\\nAs ment\", \"ioned earlier in the architecture section, you can have several approaches for dealing with this iss\", \"ue:\\n\\n- Using the full Event Sourcing pattern .\\n- Using transaction log mining.\\n- Using the Outbox pa\", \"ttern. This is a transactional table to store the integration events (extending the local transactio\", \"n).\\n\\nFor this scenario, using the full Event Sourcing (ES) pattern is one of the best approaches, if\", \" not the best. However, in many application scenarios, you might not be able to implement a full ES \", \"system. ES means storing only domain events in your transactional database, instead of storing curre\", \"nt state\\n\\ndata. Storing only domain events can have great benefits, such as having the history of yo\", \"ur system available and being able to determine the state of your system at any moment in the past. \", \"However, implementing a full ES system requires you to rearchitect most of your system and introduce\", \"s many other complexities and requirements. For example, you would want to use a database specifical\", \"ly made for event sourcing, such as Event Store, or a document-oriented database such as Azure Cosmo\", \"s DB, MongoDB, Cassandra, CouchDB, or RavenDB. ES is a great approach for this problem, but not the \", \"easiest solution unless you are already familiar with event sourcing.\\n\\nThe option to use transaction\", \" log mining initially looks transparent. However, to use this approach, the microservice has to be c\", \"oupled to your RDBMS transaction log, such as the SQL Server transaction log. This approach is proba\", \"bly not desirable. Another drawback is that the low-level updates recorded in the transaction log mi\", \"ght not be at the same level as your high-level integration events. If so, the process of reverse-en\", \"gineering those transaction log operations can be difficult.\\n\\nA balanced approach is a mix of a tran\", \"sactional database table and a simplified ES pattern. You can use a state such as \\\"ready to publish \", \"the event,\\\" which you set in the original event when you commit it to the integration events table. \", \"You then try to publish the event to the event bus. If the publishevent action succeeds, you start a\", \"nother transaction in the origin service and move the state from \\\"ready to publish the event\\\" to \\\"ev\", \"ent already published.\\\"\\n\\nIf the publish-event action in the event bus fails, the data still will not\", \" be inconsistent within the origin microservice \\u2014 it is still marked as \\\"ready to publish the event,\", \"\\\" and with respect to the rest of the services, it will eventually be consistent. You can always hav\", \"e background jobs checking the state of the transactions or integration events. If the job finds an \", \"event in the \\\"ready to publish the event\\\" state, it can try to republish that event to the event bus\", \".\\n\\nNotice that with this approach, you are persisting only the integration events for each origin mi\", \"croservice, and only the events that you want to communicate to other microservices or external syst\", \"ems. In contrast, in a full ES system, you store all domain events as well.\\n\\nTherefore, this balance\", \"d approach is a simplified ES system. You need a list of integration events with their current state\", \" (\\\"ready to publish\\\" versus \\\"published\\\"). But you only need to implement these states for the integr\", \"ation events. And in this approach, you do not need to store all your domain data as events in the t\", \"ransactional database, as you would in a full ES system.\\n\\nIf you are already using a relational data\", \"base, you can use a transactional table to store integration events. To achieve atomicity in your ap\", \"plication, you use a two-step process based on local transactions. Basically, you have an Integratio\", \"nEvent table in the same database where you have your domain entities. That table works as an insura\", \"nce for achieving atomicity so that you include persisted integration events into the same transacti\", \"ons that are committing your domain data.\\n\\nStep by step, the process goes like this:\\n\\n1. The applica\", \"tion begins a local database transaction.\\n2. It then updates the state of your domain entities and i\", \"nserts an event into the integration event table.\\n3. Finally, it commits the transaction, so you get\", \" the desired atomicity and then\\n\\nUpdate product\\n\\n4. You publish the event somehow (next).\\n\\nProductPr\", \"iceChanged\\n\\nRaise\\n\\nUl business warning\\n\\n5\\n\\nBasket\\n\\n, microservice\\n\\nWhen implementing the steps of pu\", \"blishing the events, you have these choices:\\n\\n(Publish/Susbcribe channel)\\n\\n- Publish the integration\", \" event right after committing the transaction and use another local transaction to mark the events i\", \"n the table as being published. Then, use the table just as an artifact to track the integration eve\", \"nts in case of issues in the remote microservices, and perform compensatory actions based on the sto\", \"red integration events. ----\\n\\nProduct Table\\n\\nID\\n\\nName\\n\\nPrice\\n\\n//etc.\\n\\nEventLog Table\\n\\n- Use the tabl\", \"e as a kind of queue. A separate application thread or process queries the integration event table, \", \"publishes the events to the event bus, and then uses a local transaction to mark the events as publi\", \"shed.\\n\\nFigure 6-22 shows the architecture for the first of these approaches.\\n\\nFigure 6 -22. Atomicit\", \"y when publishing events to the event bus\\n\\n<!-- image -->\\n\\nThe approach illustrated in Figure 6-22 i\", \"s missing an additional worker microservice that is in charge of checking and confirming the success\", \" of the published integration events. In case of failure, that additional checker worker microservic\", \"e can read events from the table and republish them, that is, repeat step number 2.\\n\\nAbout the secon\", \"d approach: you use the EventLog table as a queue and always use a worker microservice to publish th\", \"e messages. In that case, the process is like that shown in Figure 6-23. This shows an additional mi\", \"croservice, and the table is the single source when publishing events.\\n\\nUpdate\\n\\nProduct\\n\\nProduct tab\", \"le\\n\\nID\\n\\nName\\n\\nPrice\\n\\n//etc\\n\\nK\\n\\nRaise r, Ul Business\\n\\nwarning\\n\\nBasket\\n\\nFigure 6 -23. Atomicity when p\", \"ublishing events to the event bus with a worker microservice\\n\\n<!-- image -->\\n\\nFor simplicity, the eS\", \"hopOnContainers sample uses the first approach (with no additional processes or checker microservice\", \"s) plus the event bus. However, the eShopOnContainers sample is not handling all possible failure ca\", \"ses. In a real application deployed to the cloud, you must embrace the fact that issues will arise e\", \"ventually, and you must implement that check and resend logic. Using the table as a queue can be mor\", \"e effective than the first approach if you have that table as a single source of events when publish\", \"ing them (with the worker) through the event bus.\\n\\n## Implementing atomicity when publishing integra\", \"tion events through the event bus\\n\\nThe following code shows how you can create a single transaction \", \"involving multiple DbContext objects\\u2014one context related to the original data being updated, and the\", \" second context related to the IntegrationEventLog table.\\n\\nThe transaction in the example code below\", \" will not be resilient if connections to the database have any issue at the time when the code is ru\", \"nning. This can happen in cloud-based systems like Azure SQL DB, which might move databases across s\", \"ervers. For implementing resilient transactions across multiple contexts, see the Implementing resil\", \"ient Entity Framework Core SQL connections section later in this guide.\\n\\nFor clarity, the following \", \"example shows the whole process in a single piece of code. However, the eShopOnContainers implementa\", \"tion is refactored and splits this logic into multiple classes so it's easier to maintain.\\n\\n```\\n// U\", \"pdate Product from the Catalog microservice // public async Task<IActionResult> UpdateProduct([FromB\", \"ody]CatalogItem productToUpdate) { var catalogItem = await _catalogContext . CatalogItems . SingleOr\", \"DefaultAsync(i => i . Id == productToUpdate . Id);\\n```\\n\\n```\\nif (catalogItem == null) return NotFound\", \"(); bool raiseProductPriceChangedEvent = false; IntegrationEvent priceChangedEvent = null; if (catal\", \"ogItem . Price != productToUpdate . Price) raiseProductPriceChangedEvent = true; if (raiseProductPri\", \"ceChangedEvent) // Create event if price has changed { var oldPrice = catalogItem . Price; priceChan\", \"gedEvent = new ProductPriceChangedIntegrationEvent(catalogItem . Id , productToUpdate . Price , oldP\", \"rice); } // Update current product catalogItem = productToUpdate; // Just save the updated product i\", \"f the Product's Price hasn't changed. if (!raiseProductPriceChangedEvent) { await _catalogContext . \", \"SaveChangesAsync(); } else // Publish to event bus only if product price changed { // Achieving atom\", \"icity between original DB and the IntegrationEventLog // with a local transaction using (var transac\", \"tion = _catalogContext . Database . BeginTransaction()) { _catalogContext . CatalogItems . Update(ca\", \"talogItem); await _catalogContext . SaveChangesAsync(); await _integrationEventLogService . SaveEven\", \"tAsync(priceChangedEvent); transaction . Commit(); } // Publish the integration event through the ev\", \"ent bus _eventBus . Publish(priceChangedEvent); _integrationEventLogService . MarkEventAsPublishedAs\", \"ync( priceChangedEvent); } return Ok(); }\\n```\\n\\nAfter the ProductPriceChangedIntegrationEvent integra\", \"tion event is created, the transaction that stores the original domain operation (update the catalog\", \" item) also includes the persistence of the event in the EventLog table. This makes it a single tran\", \"saction, and you will always be able to check whether event messages were sent.\\n\\nThe event log table\", \" is updated atomically with the original database operation, using a local transaction against the s\", \"ame database. If any of the operations fail, an exception is thrown and the transaction rolls back a\", \"ny completed operation, thus maintaining consistency between the domain operations and the event mes\", \"sages saved to the table.\\n\\n## Receiving messages from subscriptions: event handlers in receiver micr\", \"oservices\\n\\nIn addition to the event subscription logic, you need to implement the internal code for \", \"the integration event handlers (like a callback method). The event handler is where you specify wher\", \"e the event messages of a certain type will be received and processed.\\n\\nAn event handler first recei\", \"ves an event instance from the event bus. Then it locates the component to be processed related to t\", \"hat integration event, propagating and persisting the event as a change in state in the receiver mic\", \"roservice. For example, if a ProductPriceChanged event originates in the catalog microservice, it is\", \" handled in the basket microservice and changes the state in this receiver basket microservice as we\", \"ll, as shown in the following code.\\n\\n```\\nnamespace Microsoft . eShopOnContainers . Services . Basket\", \" . API . IntegrationEvents . EventHandling { public class ProductPriceChangedIntegrationEventHandler\", \" : IIntegrationEventHandler<ProductPriceChangedIntegrationEvent> { private readonly IBasketRepositor\", \"y _repository; public ProductPriceChangedIntegrationEventHandler( IBasketRepository repository) { _r\", \"epository = repository; } public async Task Handle(ProductPriceChangedIntegrationEvent @event) { var\", \" userIds = await _repository . GetUsers(); foreach (var id in userIds) { var basket = await _reposit\", \"ory . GetBasket(id); await UpdatePriceInBasketItems(@event . ProductId , @event . NewPrice , basket)\", \"; } } private async Task UpdatePriceInBasketItems(int productId , decimal newPrice , CustomerBasket \", \"basket) { var itemsToUpdate = basket?.Items?.Where(x => int . Parse(x . ProductId) == productId).ToL\", \"ist(); if (itemsToUpdate != null) { foreach (var item in itemsToUpdate) { if(item . UnitPrice != new\", \"Price) { var originalPrice = item . UnitPrice; item . UnitPrice = newPrice; item . OldUnitPrice = or\", \"iginalPrice; } } await _repository . UpdateBasket(basket); } } } }\\n```\\n\\n- \\u2192\\n\\nA Not secure host.docke\", \"r.internal:5104/basket\\n\\n[eShop]\\n\\non containers\\n\\n+\\n\\nThe event handler needs to verify whether the pro\", \"duct exists in any of the basket instances. It also updates the item price for each related basket l\", \"ine item. Finally, it creates an alert to be displayed to the user about the price change, as shown \", \"in Figure 6-24.\\n\\n[ SHOPPING BAG ]\\n\\n.NET BLACK &amp; WHITE MUG\\n\\nS10.00 - 1 +\\n\\nNote that the price of \", \"this article changed in our Catalog. The old price when you\\n\\noriginally added it to the basket was S\", \" 8.5\\n\\nFigure 6 -24. Displaying an item price change in a basket, as communicated by integration even\", \"ts\\n\\n<!-- image -->\\n\\n## Idempotency in update message events\\n\\nAn important aspect of update message e\", \"vents is that a failure at any point in the communication should cause the message to be retried. Ot\", \"herwise a background task might try to publish an event that has already been published, creating a \", \"race condition. Make sure that the updates are either idempotent or that they provide enough informa\", \"tion to ensure that you can detect a duplicate, discard it, and send back only one response.\\n\\nAs not\", \"ed earlier, idempotency means that an operation can be performed multiple times without changing the\", \" result. In a messaging environment, as when communicating events, an event is idempotent if it can \", \"be delivered multiple times without changing the result for the receiver microservice. This may be n\", \"ecessary because of the nature of the event itself, or because of the way the system handles the eve\", \"nt. Message idempotency is important in any application that uses messaging, not just in application\", \"s that implement the event bus pattern.\\n\\nAn example of an idempotent operation is a SQL statement th\", \"at inserts data into a table only if that data is not already in the table. It does not matter how m\", \"any times you run that insert SQL statement; the result will be the same \\u2014 the table will contain th\", \"at data. Idempotency like this can also be necessary when dealing with messages if the messages coul\", \"d potentially be sent and therefore eShopOnContainers - SPA\\n\\ndemouser@microsoft.com\\n\\n0\\n\\nprocessed mo\", \"re than once. For instance, if retry logic causes a sender to send exactly the same message more tha\", \"n once, you need to make sure that it is idempotent.\\n\\nIt is possible to design idempotent messages. \", \"For example, you can create an event that says \\\"set the product price to $25\\\" instead of \\\"add $5 to \", \"the product price.\\\" You could safely process the first message any number of times and the result wi\", \"ll be the same. That is not true for the second message. But even in the first case, you might not w\", \"ant to process the first event, because the system could also have sent a newer price-change event a\", \"nd you would be overwriting the new price.\\n\\nAnother example might be an order-completed event that's\", \" propagated to multiple subscribers. The app has to make sure that order information is updated in o\", \"ther systems only once, even if there are duplicated message events for the same order-completed eve\", \"nt.\\n\\nIt is convenient to have some kind of identity per event so that you can create logic that enfo\", \"rces that each event is processed only once per receiver.\\n\\nSome message processing is inherently ide\", \"mpotent. For example, if a system generates image thumbnails, it might not matter how many times the\", \" message about the generated thumbnail is processed; the outcome is that the thumbnails are generate\", \"d and they are the same every time. On the other hand, operations such as calling a payment gateway \", \"to charge a credit card may not be idempotent at all. In these cases, you need to ensure that proces\", \"sing a message multiple times has the effect that you expect.\\n\\n## Additional resources\\n\\n- Honoring m\", \"essage idempotency https://learn.microsoft.com/previous-versions/msp-n-p/jj591565(v=pandp.10)#honori\", \"ngmessage -idempotency\\n\\n## Deduplicating integration event messages\\n\\nYou can make sure that message \", \"events are sent and processed only once per subscriber at different levels. One way is to use a dedu\", \"plication feature offered by the messaging infrastructure you are using. Another is to implement cus\", \"tom logic in your destination microservice. Having validations at both the transport level and the a\", \"pplication level is your best bet.\\n\\n## Deduplicating message events at the EventHandler level\\n\\nOne w\", \"ay to make sure that an event is processed only once by any receiver is by implementing certain logi\", \"c when processing the message events in event handlers. For example, that is the approach used in th\", \"e eShopOnContainers application, as you can see in the source code of the\\n\\nUserCheckoutAcceptedInteg\", \"rationEventHandler class when it receives a\\n\\nUserCheckoutAcceptedIntegrationEvent integration event.\", \" (In this case, the CreateOrderCommand is wrapped with an IdentifiedCommand, using the eventMsg.Requ\", \"estId as an identifier, before sending it to the command handler).\\n\\n## Deduplicating messages when u\", \"sing RabbitMQ\\n\\nWhen intermittent network failures happen, messages can be duplicated, and the messag\", \"e receiver must be ready to handle these duplicated messages. If possible, receivers should handle m\", \"essages in an idempotent way, which is better than explicitly handling them with deduplication.\\n\\nAcc\", \"ording to the RabbitMQ documentation, \\\"If a message is delivered to a consumer and then requeued (be\", \"cause it was not acknowledged before the consumer connection dropped, for example) then RabbitMQ wil\", \"l set the redelivered flag on it when it is delivered again (whether to the same consumer or a diffe\", \"rent one).\\n\\nIf the \\\"redelivered\\\" flag is set, the receiver must take that into account, because the \", \"message might already have been processed. But that is not guaranteed; the message might never have \", \"reached the receiver after it left the message broker, perhaps because of network issues. On the oth\", \"er hand, if the \\\"redelivered\\\" flag is not set, it is guaranteed that the message has not been sent m\", \"ore than once. Therefore, the receiver needs to deduplicate messages or process messages in an idemp\", \"otent way only if the \\\"redelivered\\\" flag is set in the message.\\n\\n## Additional resources\\n\\n- Forked e\", \"ShopOnContainers using NServiceBus (Particular Software) https://go.particular.net/eShopOnContainers\", \"\\n- Event Driven Messaging https://patterns.arcitura.com/soa-patterns/design\\\\_patterns/event\\\\_driven\\\\\", \"_messaging\\n- Jimmy Bogard. Refactoring Towards Resilience: Evaluating Coupling https://jimmybogard.c\", \"om/refactoring-towards-resilience-evaluating-coupling/\\n- Publish -Subscribe channel https://www.ente\", \"rpriseintegrationpatterns.com/patterns/messaging/PublishSubscribeChannel. html\\n- Communicating Betwe\", \"en Bounded Contexts https://learn.microsoft.com/previous-versions/msp-n-p/jj591572(v=pandp.10)\\n\\n\\u2022\\n\\nE\", \"ventual Consistency https://en.wikipedia.org/wiki/Eventual\\\\_consistency\\n\\n- Philip Brown. Strategies \", \"for Integrating Bounded Contexts https://www.culttt.com/2014/11/26/strategies-integrating-bounded-co\", \"ntexts/\\n- Chris Richardson. Developing Transactional Microservices Using Aggregates, Event Sourcing \", \"and CQRS Part 2 https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-2-richardso\", \"n\\n- Chris Richardson. Event Sourcing pattern https://microservices.io/patterns/data/event-sourcing.h\", \"tml\\n- Introducing Event Sourcing https://learn.microsoft.com/previous-versions/msp-n-p/jj591559(v=pa\", \"ndp.10)\\n\\n- Event Store database. Official site. https://geteventstore.com/\\n- Patrick Nommensen. Even\", \"t -Driven Data Management for Microservices https://dzone.com/articles/event-driven-data-management-\", \"for-microservices-1\\n- The CAP Theorem https://en.wikipedia.org/wiki/CAP\\\\_theorem\\n- What is CAP Theor\", \"em? https://www.quora.com/What-Is-CAP-Theorem-1\\n- Data Consistency Primer https://learn.microsoft.co\", \"m/previous-versions/msp-n-p/dn589800(v=pandp.10)\\n- Rick Saling. The CAP Theorem: Why \\\"Everything is \", \"Different\\\" with the Cloud and Internet https://learn.microsoft.com/archive/blogs/rickatmicrosoft/the\", \"-cap-theorem-why-everythingis -different -with -the -cloud -and -internet/\\n- Eric Brewer. CAP Twelve\", \" Years Later: How the \\\"Rules\\\" Have Changed https://www.infoq.com/articles/cap-twelve-years-later-how\", \"-the-rules-have-changed\\n- CAP, PACELC, and Microservices https://ardalis.com/cap-pacelc-and-microser\", \"vices/\\n- Azure Service Bus. Brokered Messaging: Duplicate Detection https://github.com/microsoftarch\", \"ive/msdn-code-gallerymicrosoft/tree/master/Windows%20Azure%20Product%20Team/Brokered%20Messaging%20 \", \"Duplicate%20Detection\\n- Reliability Guide (RabbitMQ documentation) https://www.rabbitmq.com/reliabil\", \"ity.html#consumer\\n\\n## Testing ASP.NET Core services and web apps\\n\\nControllers are a central part of \", \"any ASP.NET Core API service and ASP.NET MVC Web application. As such, you should have confidence th\", \"ey behave as intended for your application. Automated tests can provide you with this confidence and\", \" can detect errors before they reach production.\\n\\nYou need to test how the controller behaves based \", \"on valid or invalid inputs, and test controller responses based on the result of the business operat\", \"ion it performs. However, you should have these types of tests for your microservices:\\n\\n- Unit tests\", \". These tests ensure that individual components of the application work as expected. Assertions test\", \" the component API.\\n- Integration tests. These tests ensure that component interactions work as expe\", \"cted against external artifacts like databases. Assertions can test component API, UI, or the side e\", \"ffects of actions like database I/O, logging, etc.\\n\\n- Functional tests for each microservice. These \", \"tests ensure that the application works as expected from the user's perspective.\\n- Service tests. Th\", \"ese tests ensure that end -to -end service use cases, including testing multiple services at the sam\", \"e time, are tested. For this type of testing, you need to prepare the environment first. In this cas\", \"e, it means starting the services (for example, by using dockercompose up).\\n\\n## Implementing unit te\", \"sts for ASP.NET Core Web APIs\\n\\nUnit testing involves testing a part of an application in isolation f\", \"rom its infrastructure and dependencies. When you unit test controller logic, only the content of a \", \"single action or method is tested, not the behavior of its dependencies or of the framework itself. \", \"Unit tests do not detect issues in the interaction between components\\u2014that is the purpose of integra\", \"tion testing.\\n\\nAs you unit test your controller actions, make sure you focus only on their behavior.\", \" A controller unit test avoids things like filters, routing, or model binding (the mapping of reques\", \"t data to a ViewModel or DTO). Because they focus on testing just one thing, unit tests are generall\", \"y simple to write and quick to run. A well-written set of unit tests can be run frequently without m\", \"uch overhead.\\n\\nUnit tests are implemented based on test frameworks like xUnit.net, MSTest, Moq, or N\", \"Unit. For the eShopOnContainers sample application, we are using xUnit.\\n\\nWhen you write a unit test \", \"for a Web API controller, you instantiate the controller class directly using the new keyword in C#,\", \" so that the test will run as fast as possible. The following example shows how to do this when usin\", \"g xUnit as the Test framework.\\n\\n```\\n[Fact] public async Task Get_order_detail_success() { //Arrange \", \"var fakeOrderId = \\\"12\\\"; var fakeOrder = GetFakeOrder(); //... //Act var orderController = new OrderC\", \"ontroller( _orderServiceMock . Object , _basketServiceMock . Object , _identityParserMock . Object);\", \" orderController . ControllerContext . HttpContext = _contextMock . Object; var actionResult = await\", \" orderController . Detail(fakeOrderId); //Assert var viewResult = Assert . IsType<ViewResult>(action\", \"Result); Assert . IsAssignableFrom<Order>(viewResult . ViewData . Model); }\\n```\\n\\n## Implementing int\", \"egration and functional tests for each microservice\\n\\nAs noted, integration tests and functional test\", \"s have different purposes and goals. However, the way you implement both when testing ASP.NET Core c\", \"ontrollers is similar, so in this section we concentrate on integration tests.\\n\\nIntegration testing \", \"ensures that an application's components function correctly when assembled. ASP.NET Core supports in\", \"tegration testing using unit test frameworks and a built-in test web host that can be used to handle\", \" requests without network overhead.\\n\\nUnlike unit testing, integration tests frequently involve appli\", \"cation infrastructure concerns, such as a database, file system, network resources, or web requests \", \"and responses. Unit tests use fakes or mock objects in place of these concerns. But the purpose of i\", \"ntegration tests is to confirm that the system works as expected with these systems, so for integrat\", \"ion testing you do not use fakes or mock objects. Instead, you include the infrastructure, like data\", \"base access or service invocation from other services.\\n\\nBecause integration tests exercise larger se\", \"gments of code than unit tests, and because integration tests rely on infrastructure elements, they \", \"tend to be orders of magnitude slower than unit tests. Thus, it is a good idea to limit how many int\", \"egration tests you write and run.\\n\\nASP.NET Core includes a built -in test web host that can be used \", \"to handle HTTP requests without network overhead, meaning that you can run those tests faster than w\", \"hen using a real web host. The test web host (TestServer) is available in a NuGet component as Micro\", \"soft.AspNetCore.TestHost. It can be added to integration test projects and used to host ASP.NET Core\", \" applications.\\n\\nAs you can see in the following code, when you create integration tests for ASP.NET \", \"Core controllers, you instantiate the controllers through the test host. This functionality is compa\", \"rable to an HTTP request, but it runs faster.\\n\\n```\\npublic class PrimeWebDefaultRequestShould { priva\", \"te readonly TestServer _server; private readonly HttpClient _client; public PrimeWebDefaultRequestSh\", \"ould() { // Arrange _server = new TestServer(new WebHostBuilder() . UseStartup<Startup>()); _client \", \"= _server . CreateClient(); } [Fact] public async Task ReturnHelloWorld() { // Act var response = aw\", \"ait _client . GetAsync(\\\"/\\\"); response.EnsureSuccessStatusCode(); var responseString = await response\", \" . Content . ReadAsStringAsync(); // Assert Assert . Equal(\\\"Hello World!\\\" , responseString); } }\\n```\", \"\\n\\n## Additional resources\\n\\n- Steve Smith. Testing controllers (ASP.NET Core) https://learn.microsoft\", \".com/aspnet/core/mvc/controllers/testing\\n- Steve Smith. Integration testing (ASP.NET Core) https://l\", \"earn.microsoft.com/aspnet/core/test/integration-tests\\n- Unit testing in .NET using dotnet test https\", \"://learn.microsoft.com/dotnet/core/testing/unit-testing-with-dotnet-test\\n- xUnit.net. Official site.\", \" https://xunit.net/\\n- Unit Test Basics. https://learn.microsoft.com/visualstudio/test/unit-test-basi\", \"cs\\n- Moq. GitHub repo. https://github.com/moq/moq\\n- NUnit. Official site. https://nunit.org/\\n\\n## Imp\", \"lementing service tests on a multi-container application\\n\\nAs noted earlier, when you test multi-cont\", \"ainer applications, all the microservices need to be running within the Docker host or container clu\", \"ster. End -to -end service tests that include multiple operations involving several microservices re\", \"quire you to deploy and start the whole application in the Docker host by running docker-compose up \", \"(or a comparable mechanism if you are using an orchestrator). Once the whole application and all its\", \" services is running, you can execute end-to-end integration and functional tests.\\n\\nThere are a few \", \"approaches you can use. In the docker-compose.yml file that you use to deploy the application at the\", \" solution level you can expand the entry point to use dotnet test. You can also use another compose \", \"file that would run your tests in the image you are targeting. By using another compose file for int\", \"egration tests that includes your microservices and databases on containers, you can make sure that \", \"the related data is always reset to its original state before running the tests.\\n\\nOnce the compose a\", \"pplication is up and running, you can take advantage of breakpoints and exceptions if you are runnin\", \"g Visual Studio. Or you can run the integration tests automatically in your CI pipeline in Azure Dev\", \"Ops Services or any other CI/CD system that supports Docker containers.\\n\\n## Testing in eShopOnContai\", \"ners\\n\\nThe reference application (eShopOnContainers) tests were recently restructured and now there a\", \"re four categories:\\n\\n1. Unit tests, just plain old regular unit tests, contained in the {Microservic\", \"eName}.UnitTests projects\\n\\n1\\n\\nSolution 'eShopOnContainers-ServicesAndWebApps' (34 of 34 projects)\\n\\nS\", \"olution Items\\n\\nSrC\\n\\nApiGateways\\n\\n2. Microservice functional/integration tests, with test cases invol\", \"ving the infrastructure for each microservice but isolated from the others and are contained in the \", \"{MicroserviceName}.FunctionalTests projects. Basket Catalog\\n3. Application functional/integration te\", \"sts, which focus on microservices integration, with test cases that exert several microservices. The\", \"se tests are located in project Application.FunctionalTests .\\n\\nWhile unit and integration tests are \", \"organized in a test folder within the microservice project, application and load tests are managed s\", \"eparately under the root folder, as shown in Figure 6-25.\\n\\nMarketing\\n\\n5 Ordering\\n\\n\\u2022 Web Apps tests\\n\\n\", \"ServiceTests docker-compose\\n\\nFigure 6 -25. Test folder structure in eShopOnContainers\\n\\n<!-- image --\", \">\\n\\nMicroservice and Application functional/integration tests are run from Visual Studio, using the r\", \"egular tests runner, but first you need to start the required infrastructure services, with a set of\", \" dockercompose files contained in the solution test folder:\\n\\n## docker -compose -test.yml\\n\\n```\\nversi\", \"on: '3.4' services: redis.data: image: redis:alpine rabbitmq: image: rabbitmq:3-management-alpine sq\", \"ldata:\\n```\\n\\nP\\n\\nP\\n\\n```\\nimage: mcr.microsoft.com/mssql/server:2017-latest nosqldata: image: mongo\\n```\\n\", \"\\n## docker -compose -test.override.yml\\n\\n```\\nversion: '3.4' services: redis.data: ports: -\\\"6379:6379\\\"\", \" rabbitmq: ports: -\\\"15672:15672\\\" -\\\"5672:5672\\\" sqldata: environment: -SA_PASSWORD=Pass@word -ACCEPT_E\", \"ULA=Y ports: -\\\"5433:1433\\\" nosqldata: ports: -\\\"27017:27017\\\"\\n```\\n\\nSo, to run the functional/integratio\", \"n tests you must first run this command, from the solution test folder:\\n\\ndocker -compose -f docker -\", \"compose -test.yml -f docker-compose-test.override.yml up\\n\\nAs you can see, these docker-compose files\", \" only start the Redis, RabbitMQ, SQL Server, and MongoDB microservices.\\n\\n## Additional resources\\n\\n- \", \"Unit &amp; Integration testing on the eShopOnContainers https://github.com/dotnet-architecture/eShop\", \"OnContainers/wiki/Unit-and-integrationtesting\\n- Load testing on the eShopOnContainers https://github\", \".com/dotnet-architecture/eShopOnContainers/wiki/Load-testing\\n\\n## Implement background tasks in micro\", \"services with IHostedService and the BackgroundService class\\n\\nBackground tasks and scheduled jobs ar\", \"e something you might need to use in any application, whether or not it follows the microservices ar\", \"chitecture pattern. The difference when using a microservices architecture is that you can implement\", \" the background task in a separate process/container for hosting so you can scale it down/up based o\", \"n your need.\\n\\nImplementing background tasks with HostedService in NET Core\\n\\nAs optional container\\n\\nF\", \"rom a generic point of view, in .NET we called these type of tasks Hosted Services, because they are\", \" services/logic that you host within your host/application/microservice. Note that in this case, the\", \" hosted service simply means a class with the background task logic.\\n\\nHttp\\n\\n(*) In\\n\\n. NET Core:\\n\\nIwe\", \"bHost implemented in Microsoft. AspNetCore.Hosting\\n\\nHostedService implemented in Microsoft. Extensio\", \"ns.Hosting\\n\\nSince .NET Core 2.0, the framework provides a new interface named IHostedService helping\", \" you to easily implement hosted services. The basic idea is that you can register multiple backgroun\", \"d tasks (hosted services) that run in the background while your web host or host is running, as show\", \"n in the image 6-26.\\n\\nMylHostedService 'n? Background task\\n\\nMylHostedService 'n? / Background task\\n\\n\", \"Figure 6 -26. Using IHostedService in a WebHost vs. a Host\\n\\n<!-- image -->\\n\\nASP.NET Core 1.x and 2.x\", \" support IWebHost for background processes in web apps. .NET Core 2.1 and later versions support IHo\", \"st for background processes with plain console apps. Note the difference made between WebHost and Ho\", \"st.\\n\\nA WebHost (base class implementing IWebHost) in ASP.NET Core 2.0 is the infrastructure artifact\", \" you use to provide HTTP server features to your process, such as when you're implementing an MVC we\", \"b app or Web API service. It provides all the new infrastructure goodness in ASP.NET Core, enabling \", \"you to use dependency injection, insert middlewares in the request pipeline, and similar. The WebHos\", \"t uses these very same IHostedServices for background tasks.\\n\\nA Host (base class implementing IHost)\", \" was introduced in .NET Core 2.1. Basically, a Host allows you to have a similar infrastructure than\", \" what you have with WebHost (dependency injection, hosted services, etc.), but in this case, you jus\", \"t want to have a simple and lighter process as the host, with nothing related to MVC, Web API or HTT\", \"P server features.\\n\\nTherefore, you can choose and either create a specialized host-process with IHos\", \"t to handle the hosted services and nothing else, such a microservice made just for hosting the IHos\", \"tedServices, or you can alternatively extend an existing ASP.NET Core WebHost, such as an existing A\", \"SP.NET Core Web API or MVC app.\\n\\nEach approach has pros and cons depending on your business and scal\", \"ability needs. The bottom line is basically that if your background tasks have nothing to do with HT\", \"TP (IWebHost) you should use IHost.\\n\\n## Registering hosted services in your WebHost or Host\\n\\nLet's d\", \"rill down further on the IHostedService interface since its usage is pretty similar in a WebHost or \", \"in a Host.\\n\\nSignalR is one example of an artifact using hosted services, but you can also use it for\", \" much simpler things like:\\n\\n- A background task polling a database looking for changes.\\n- A schedule\", \"d task updating some cache periodically.\\n- An implementation of QueueBackgroundWorkItem that allows \", \"a task to be executed on a background thread.\\n- Processing messages from a message queue in the back\", \"ground of a web app while sharing common services such as ILogger.\\n- A background task started with \", \"Task.Run().\\n\\nYou can basically offload any of those actions to a background task that implements IHo\", \"stedService.\\n\\nThe way you add one or multiple IHostedServices into your WebHost or Host is by regist\", \"ering them up through the AddHostedService extension method in an ASP.NET Core WebHost (or in a Host\", \" in .NET Core 2.1 and above). Basically, you have to register the hosted services within application\", \" startup in Program.cs .\\n\\n```\\n//Other DI registrations; // Register Hosted Services builder . Servic\", \"es . AddHostedService<GracePeriodManagerService>(); builder . Services . AddHostedService<MyHostedSe\", \"rviceB>(); builder . Services . AddHostedService<MyHostedServiceC>(); //...\\n```\\n\\nIn that code, the G\", \"racePeriodManagerService hosted service is real code from the Ordering business microservice in eSho\", \"pOnContainers, while the other two are just two additional samples.\\n\\nThe IHostedService background t\", \"ask execution is coordinated with the lifetime of the application (host or microservice, for that ma\", \"tter). You register tasks when the application starts and you have the opportunity to do some gracef\", \"ul action or clean-up when the application is shutting down.\\n\\nWithout using IHostedService, you coul\", \"d always start a background thread to run any task. The difference is precisely at the app's shutdow\", \"n time when that thread would simply be killed without having the opportunity to run graceful clean-\", \"up actions.\\n\\n## The IHostedService interface\\n\\nWhen you register an IHostedService, .NET calls the St\", \"artAsync() and StopAsync() methods of your IHostedService type during application start and stop res\", \"pectively. For more details, see IHostedService interface .\\n\\nAs you can imagine, you can create mult\", \"iple implementations of IHostedService and register each of them in Program.cs, as shown previously.\", \" All those hosted services will be started and stopped along with the application/microservice.\\n\\nAs \", \"a developer, you are responsible for handling the stopping action of your services when StopAsync() \", \"method is triggered by the host.\\n\\n## Implementing IHostedService with a custom hosted service class \", \"deriving from the BackgroundService base class\\n\\nYou could go ahead and create your custom hosted ser\", \"vice class from scratch and implement the IHostedService, as you need to do when using .NET Core 2.0\", \" and later.\\n\\nHowever, since most background tasks will have similar needs in regard to the cancellat\", \"ion tokens management and other typical operations, there is a convenient abstract base class you ca\", \"n derive from, named BackgroundService (available since .NET Core 2.1).\\n\\nThat class provides the mai\", \"n work needed to set up the background task.\\n\\nThe next code is the abstract BackgroundService base c\", \"lass as implemented in .NET.\\n\\n```\\n// Copyright (c) .NET Foundation. Licensed under the Apache Licens\", \"e, Version 2.0. /// <summary> /// Base class for implementing a long running <see cref=\\\"IHostedServi\", \"ce\\\"/> . /// </summary> public abstract class BackgroundService : IHostedService , IDisposable { priv\", \"ate Task _executingTask; private readonly CancellationTokenSource _stoppingCts = new CancellationTok\", \"enSource(); protected abstract Task ExecuteAsync(CancellationToken stoppingToken); public virtual Ta\", \"sk StartAsync(CancellationToken cancellationToken) { // Store the task we're executing _executingTas\", \"k = ExecuteAsync(_stoppingCts . Token); // If the task is completed then return it, // this will bub\", \"ble cancellation and failure to the caller if (_executingTask . IsCompleted) { return _executingTask\", \"; } // Otherwise it's running return Task . CompletedTask; } public virtual async Task StopAsync(Can\", \"cellationToken cancellationToken) { // Stop called without start if (_executingTask == null) { retur\", \"n; }\\n```\\n\\n```\\ntry { // Signal cancellation to the executing method _stoppingCts . Cancel(); } finall\", \"y { // Wait until the task completes or the stop token triggers await Task . WhenAny(_executingTask \", \", Task . Delay(Timeout . Infinite , cancellationToken)); } } public virtual void Dispose() { _stoppi\", \"ngCts . Cancel(); } }\\n```\\n\\nWhen deriving from the previous abstract base class, thanks to that inher\", \"ited implementation, you just need to implement the ExecuteAsync() method in your own custom hosted \", \"service class, as in the following simplified code from eShopOnContainers which is polling a databas\", \"e and publishing integration events into the Event Bus when needed.\\n\\n```\\npublic class GracePeriodMan\", \"agerService : BackgroundService { private readonly ILogger<GracePeriodManagerService> _logger; priva\", \"te readonly OrderingBackgroundSettings _settings; private readonly IEventBus _eventBus; public Grace\", \"PeriodManagerService(IOptions<OrderingBackgroundSettings> settings , IEventBus eventBus , ILogger<Gr\", \"acePeriodManagerService> logger) { // Constructor's parameters validations... } protected override a\", \"sync Task ExecuteAsync(CancellationToken stoppingToken) { _logger . LogDebug($\\\"GracePeriodManagerSer\", \"vice is starting.\\\"); stoppingToken . Register(() => _logger . LogDebug($\\\" GracePeriod background tas\", \"k is stopping.\\\")); while (!stoppingToken . IsCancellationRequested) { _logger . LogDebug($\\\"GracePeri\", \"od task doing background work.\\\"); // This eShopOnContainers method is querying a database table // a\", \"nd publishing events into the Event Bus (RabbitMQ / ServiceBus) CheckConfirmedGracePeriodOrders(); t\", \"ry { await Task . Delay(_settings . CheckUpdateTime , stoppingToken); } catch (TaskCanceledException\", \" exception) { _logger . LogCritical(exception , \\\"TaskCanceledException Error\\\" , exception . Message)\", \";\\n```\\n\\nClass diagram with a custom HostedService and related classes and interfaces\\n\\n```\\n} } _logger\", \" . LogDebug($\\\"GracePeriod background task is stopping.\\\"); } .../... } or \\\"' MyHostedServiceA class B\", \"ackground task |HostedService interface \\u00b7 BackgroundService base class (*)\\n```\\n\\nIn this specific cas\", \"e for eShopOnContainers, it's executing an application method that's querying a database table looki\", \"ng for orders with a specific state and when applying changes, it is publishing integration events t\", \"hrough the event bus (underneath it can be using RabbitMQ or Azure Service Bus).\\n\\n(**) IHost and Bac\", \"kgroundService are implemented\\n\\nOf course, you could run any other business background task, instead\", \".\\n\\nBy default, the cancellation token is set with a 5 seconds timeout, although you can change that \", \"value when building your WebHost using the UseShutdownTimeout extension of the IWebHostBuilder. This\", \" means that our service is expected to cancel within 5 seconds otherwise it will be more abruptly ki\", \"lled.\\n\\nThe following code would be changing that time to 10 seconds.\\n\\n```\\nWebHost . CreateDefaultBui\", \"lder(args) . UseShutdownTimeout(TimeSpan . FromSeconds(10))\\n```\\n\\n...\\n\\n## Summary class diagram\\n\\nThe \", \"following image shows a visual summary of the classes and interfaces involved when implementing IHos\", \"tedServices.\\n\\nFigure 6 -27. Class diagram showing the multiple classes and interfaces related to IHo\", \"stedService\\n\\n<!-- image -->\\n\\nClass diagram: IWebHost and IHost can host many services, which inherit\", \" from BackgroundService, which implements IHostedService.\\n\\n## Deployment considerations and takeaway\", \"s\\n\\nIt is important to note that the way you deploy your ASP.NET Core WebHost or .NET Host might impa\", \"ct the final solution. For instance, if you deploy your WebHost on IIS or a regular Azure App Servic\", \"e, your host can be shut down because of app pool recycles. But if you are deploying your host as a \", \"container into an orchestrator like Kubernetes, you can control the assured number of live instances\", \" of your host. In addition, you could consider other approaches in the cloud especially made for the\", \"se scenarios, like Azure Functions. Finally, if you need the service to be running all the time and \", \"are deploying on a Windows Server you could use a Windows Service.\\n\\nBut even for a WebHost deployed \", \"into an app pool, there are scenarios like repopulating or flushing application's in-memory cache th\", \"at would be still applicable.\\n\\nThe IHostedService interface provides a convenient way to start backg\", \"round tasks in an ASP.NET Core web application (in .NET Core 2.0 and later versions) or in any proce\", \"ss/host (starting in .NET Core 2.1 with IHost). Its main benefit is the opportunity you get with the\", \" graceful cancellation to clean-up the code of your background tasks when the host itself is shuttin\", \"g down.\\n\\n## Additional resources\\n\\n- Building a scheduled task in ASP.NET Core/Standard 2.0 https://b\", \"log.maartenballiauw.be/post/2017/08/01/building-a-scheduled-cache-updater-inaspnet -core-2.html\\n- Im\", \"plementing IHostedService in ASP.NET Core 2.0 https://www.stevejgordon.co.uk/asp-net-core-2-ihosteds\", \"ervice\\n- GenericHost Sample using ASP.NET Core 2.1 https://github.com/aspnet/Hosting/tree/release/2.\", \"1/samples/GenericHostSample\\n\\n## Implement API Gateways with Ocelot\\n\\n## Important\\n\\nThe reference micr\", \"oservice application eShopOnContainers is currently using features provided by Envoy to implement th\", \"e API Gateway instead of the earlier referenced Ocelot. We made this design choice because of Envoy'\", \"s built-in support for the WebSocket protocol, required by the new gRPC inter -service communication\", \"s implemented in eShopOnContainers. However, we've retained this section in the guide so you can con\", \"sider Ocelot as a simple, capable, and lightweight API Gateway suitable for production-grade scenari\", \"os. Also, latest Ocelot version contains a breaking change on its json schema. Consider using Ocelot\", \" &lt; v16.0.0, or use the key Routes instead of ReRoutes.\\n\\n## Architect and design your API Gateways\", \"\\n\\nThe following architecture diagram shows how API Gateways were implemented with Ocelot in eShopOnC\", \"ontainers.\\n\\nF--\\n\\n- \\u2014\\u2014\\n\\nClient apps eShop mobile app\\n\\nXamarin.Forms\\n\\nC#\\n\\nxPlat. OS:\\n\\niOS\\n\\nAndroid\\n\\nWi\", \"ndows eShop traditional Web app\\n\\neShop SPA Web app\\n\\nTypeScript/Angular eShopOnContainers reference a\", \"pplication\\n\\n(Development environment architecture)\\n\\nDocker Host\\n\\nIdentity microservice (STS+ users)\\n\", \"\\nSQL Server database\\n\\n## Catalog microservice\\n\\n\\u00b7 0SQL Server database\\n\\nRabbitMQ\\n\\nFigure 6 -28. eShop\", \"OnContainers architecture with API Gateways\\n\\n<!-- image -->\\n\\nThat diagram shows how the whole applic\", \"ation is deployed into a single Docker host or development PC with \\\"Docker for Windows\\\" or \\\"Docker f\", \"or Mac\\\". However, deploying into any orchestrator would be similar, but any container in the diagram\", \" could be scaled out in the orchestrator.\\n\\nIn addition, the infrastructure assets such as databases,\", \" cache, and message brokers should be offloaded from the orchestrator and deployed into high availab\", \"le systems for infrastructure, like Azure SQL Database, Azure Cosmos DB, Azure Redis, Azure Service \", \"Bus, or any HA clustering solution onpremises.\\n\\nAs you can also notice in the diagram, having severa\", \"l API Gateways allows multiple development teams to be autonomous (in this case Marketing features v\", \"s. Shopping features) when developing and deploying their microservices plus their own related API G\", \"ateways.\\n\\nIf you had a single monolithic API Gateway that would mean a single point to be updated by\", \" several development teams, which could couple all the microservices with a single part of the appli\", \"cation.\\n\\nGoing much further in the design, sometimes a fine-grained API Gateway can also be limited \", \"to a single business microservice depending on the chosen architecture. Having the API Gateway's bou\", \"ndaries dictated by the business or domain will help you to get a better design.\\n\\nFor instance, fine\", \" granularity in the API Gateway tier can be especially useful for more advanced composite UI applica\", \"tions that are based on microservices, because the concept of a fine-grained API Gateway is similar \", \"to a UI composition service.\\n\\nWe delve into more details in the previous section Creating composite \", \"UI based on microservices .\\n\\nAs a key takeaway, for many medium- and large-size applications, using \", \"a custom-built API Gateway product is usually a good approach, but not as a single monolithic aggreg\", \"ator or unique central custom API Gateway unless that API Gateway allows multiple independent config\", \"uration areas for the several development teams creating autonomous microservices.\\n\\nAPI Gateways / B\", \"FF\\n\\n(Publish/Subscribe\\n\\n\\u2022\\n\\nServices\\n\\nCatalog\\n\\nBasket\\n\\nCatalog\\n\\nCatalog API\\n\\nC Connected Services\\n\\nId\", \"entity\\n\\nLocation\\n\\nDependencies\\n\\n## Sample microservices/containers to reroute through the API Gatewa\", \"ys\\n\\nAs an example, eShopOnContainers has around six internal microservice-types that have to be publ\", \"ished through the API Gateways, as shown in the following image.\\n\\nFigure 6 -29. Microservice folders\", \" in eShopOnContainers solution in Visual Studio\\n\\n<!-- image -->\\n\\nAbout the Identity service, in the \", \"design it's left out of the API Gateway routing because it's the only cross-cutting concern in the s\", \"ystem, although with Ocelot it's also possible to include it as part of the rerouting lists.\\n\\nAll th\", \"ose services are currently implemented as ASP.NET Core Web API services, as you can tell from the co\", \"de. Let's focus on one of the microservices like the Catalog microservice code.\\n\\nD\\n\\nC*\\n\\nFigure 6 -30\", \". Sample Web API microservice (Catalog microservice)\\n\\n<!-- image -->\\n\\nYou can see that the Catalog m\", \"icroservice is a typical ASP.NET Core Web API project with several controllers and methods like in t\", \"he following code.\\n\\n```\\n[HttpGet] [Route(\\\"items/{id:int}\\\")] [ProducesResponseType((int)HttpStatusCod\", \"e . BadRequest)] [ProducesResponseType((int)HttpStatusCode . NotFound)] [ProducesResponseType(typeof\", \"(CatalogItem),(int)HttpStatusCode . OK)] public async Task<IActionResult> GetItemById(int id) { if (\", \"id <= 0) { return BadRequest(); } var item = await _catalogContext . CatalogItems . SingleOrDefaultA\", \"sync(ci => ci . Id == id); //\\u2026 if (item != null) { return Ok(item); } return NotFound(); }\\n```\\n\\nThe \", \"HTTP request will end up running that kind of C# code accessing the microservice database and any ad\", \"ditional required action.\\n\\nRegarding the microservice URL, when the containers are deployed in your \", \"local development PC (local Docker host), each microservice's container always has an internal port \", \"(usually port 80) specified in its dockerfile, as in the following dockerfile:\\n\\n```\\nFROM mcr.microso\", \"ft.com/dotnet/aspnet:7.0 AS base WORKDIR /app EXPOSE 80\\n```\\n\\nThe port 80 shown in the code is intern\", \"al within the Docker host, so it can\\u2019t be reached by client apps.\\n\\nClient apps can access only the e\", \"xternal ports (if any) published when deploying with dockercompose.\\n\\nThose external ports shouldn't \", \"be published when deploying to a production environment. For this specific reason, why you want to u\", \"se the API Gateway, to avoid the direct communication between the client apps and the microservices.\", \"\\n\\nHowever, when developing, you want to access the microservice/container directly and run it throug\", \"h Swagger. That's why in eShopOnContainers, the external ports are still specified even when they wo\", \"n't be used by the API Gateway or the client apps.\\n\\nHere\\u2019s an example of the docker-compose.override\", \".yml file for the Catalog microservice:\\n\\n```\\ncatalog-api: environment: -ASPNETCORE_ENVIRONMENT=Devel\", \"opment -ASPNETCORE_URLS=http://0.0.0.0:80 -ConnectionString=YOUR_VALUE -... Other Environment Variab\", \"les\\n```\\n\\n```\\nports: -\\\"5101:80\\\" # Important: In a production environment you should remove the extern\", \"al port (5101) kept here for microservice debugging purposes. # The API Gateway redirects and access\", \" through the internal port (80).\\n```\\n\\nYou can see how in the docker -compose.override.yml configurat\", \"ion the internal port for the Catalog container is port 80, but the port for external access is 5101\", \". But this port shouldn't be used by the application when using an API Gateway, only to debug, run, \", \"and test just the Catalog microservice.\\n\\nNormally, you won't be deploying with docker-compose into a\", \" production environment because the right production deployment environment for microservices is an \", \"orchestrator like Kubernetes or Service Fabric. When deploying to those environments you use differe\", \"nt configuration files where you won't publish directly any external port for the microservices but,\", \" you'll always use the reverse proxy from the API Gateway.\\n\\nRun the catalog microservice in your loc\", \"al Docker host. Either run the full eShopOnContainers solution from Visual Studio (it runs all the s\", \"ervices in the docker-compose files), or start the Catalog microservice with the following docker-co\", \"mpose command in CMD or PowerShell positioned at the folder where the docker -compose.yml and docker\", \"-compose.override.yml are placed.\\n\\n```\\ndocker -compose run --service -ports catalog-api\\n```\\n\\nThis co\", \"mmand only runs the catalog-api service container plus dependencies that are specified in the docker\", \" -compose.yml. In this case, the SQL Server container and RabbitMQ container.\\n\\nThen, you can directl\", \"y access the Catalog microservice and see its methods through the Swagger UI accessing directly thro\", \"ugh that \\\"external\\\" port, in this case http://host.docker.internal:5101/swagger:\\n\\n&lt; \\u2192 localhost:5\", \"101/swagger/\\n\\nswagger\\n\\nSelect a spec eShoponContainers - Catalog HTTP API\\u00b0\\n\\n/swagger/v1/swagger.ison\", \"\\n\\nThe Catalog Microservice HTTP API. This is a Data-Driven/CRUD microservice sample\\n\\nTerms of servic\", \"e\\n\\nCatalog\\n\\nGET\\n\\nPUT\\n\\nPOST\\n\\nGET\\n\\nGET\\n\\nGET\\n\\nGET\\n\\nGET\\n\\nDELETE\\n\\n/api/v1/Catalog/items\\n\\n/api/v1/Catalog/\", \"items\\n\\n/api/v1/Catalog/items\\n\\n/api/v1/Catalog/items/{id}\\n\\n/api/v1/Catalog/CatalogTypes\\n\\n/api/v1/Cata\", \"log/CatalogBrands\\n\\n/api/v1/Catalog/{id}\\n\\nCatalog.API V1\\n\\nFigure 6 -31. Testing the Catalog microserv\", \"ice with its Swagger UI\\n\\n<!-- image -->\\n\\nAt this point, you could set a breakpoint in C# code in Vis\", \"ual Studio, test the microservice with the methods exposed in Swagger UI, and finally clean-up every\", \"thing with the docker-compose down command.\\n\\nHowever, direct-access communication to the microservic\", \"e, in this case through the external port 5101, is precisely what you want to avoid in your applicat\", \"ion. And you can avoid that by setting the additional level of indirection of the API Gateway (Ocelo\", \"t, in this case). That way, the client app won't directly access the microservice.\\n\\n## Implementing \", \"your API Gateways with Ocelot\\n\\nOcelot is basically a set of middleware that you can apply in a speci\", \"fic order.\\n\\nOcelot is designed to work with ASP.NET Core only. The latest version of the package is \", \"18.0 which targets .NET 6 and hence is not suitable for .NET Framework applications.\\n\\nApiGw-Base\\n\\n\\u2022 \", \"-e OcelotApiGw\\n\\n\\u2022- Connected Services\\n\\n* Dependencies\\n\\nYou install Ocelot and its dependencies in yo\", \"ur ASP.NET Core project with Ocelot's NuGet package , from Visual Studio.\\n\\n## Install -Package Ocelo\", \"t\\n\\nSDK\\n\\nIn eShopOnContainers, its API Gateway implementation is a simple ASP.NET Core WebHost projec\", \"t, and Ocelot's middleware handles all the API Gateway features, as shown in the following image:\\n\\na\", \"dJ appsettings.json aL Dockerfile\\n\\nD a C* Program.cs\\n\\nD G C# Startup.cs\\n\\nFigure 6 -32. The OcelotApi\", \"Gw base project in eShopOnContainers\\n\\n<!-- image -->\\n\\nThis ASP.NET Core WebHost project is built wit\", \"h two simple files: Program.cs and Startup.cs.\\n\\nThe Program.cs just needs to create and configure th\", \"e typical ASP.NET Core BuildWebHost.\\n\\n```\\nnamespace OcelotApiGw { public class Program { public stat\", \"ic void Main(string[] args) { BuildWebHost(args).Run(); } public static IWebHost BuildWebHost(string\", \"[] args) { var builder = WebHost . CreateDefaultBuilder(args); builder . ConfigureServices(s => s . \", \"AddSingleton(builder)) . ConfigureAppConfiguration( ic => ic . AddJsonFile(Path . Combine(\\\"configura\", \"tion\\\" , \\\"configuration.json\\\"))) . UseStartup<Startup>(); var host = builder . Build(); return host; \", \"} } }\\n```\\n\\nThe important point here for Ocelot is the configuration.json file that you must provide \", \"to the builder through the AddJsonFile() method. That configuration.json is where you specify all th\", \"e API Gateway ReRoutes, meaning the external endpoints with specific ports and the correlated intern\", \"al endpoints, usually using different ports.\\n\\n```\\n{ \\\"ReRoutes\\\": [] , \\\"GlobalConfiguration\\\": {} }\\n```\", \"\\n\\nThere are two sections to the configuration. An array of ReRoutes and a GlobalConfiguration. The R\", \"eRoutes are the objects that tell Ocelot how to treat an upstream request. The Global configuration \", \"allows overrides of ReRoute specific settings. It's useful if you don't want to manage lots of ReRou\", \"te specific settings.\\n\\nHere's a simplified example of ReRoute configuration file from one of the API\", \" Gateways from eShopOnContainers.\\n\\n```\\n{ \\\"ReRoutes\\\": [ { \\\"DownstreamPathTemplate\\\": \\\"/api/{version}/{\", \"everything}\\\" , \\\"DownstreamScheme\\\": \\\"http\\\" , \\\"DownstreamHostAndPorts\\\": [ { \\\"Host\\\": \\\"catalog-api\\\" , \\\"P\", \"ort\\\": 80 } ] , \\\"UpstreamPathTemplate\\\": \\\"/api/{version}/c/{everything}\\\" , \\\"UpstreamHttpMethod\\\": [ \\\"PO\", \"ST\\\" , \\\"PUT\\\" , \\\"GET\\\" ] } , { \\\"DownstreamPathTemplate\\\": \\\"/api/{version}/{everything}\\\" , \\\"DownstreamSch\", \"eme\\\": \\\"http\\\" , \\\"DownstreamHostAndPorts\\\": [ { \\\"Host\\\": \\\"basket -api\\\" , \\\"Port\\\": 80 } ] , \\\"UpstreamPathT\", \"emplate\\\": \\\"/api/{version}/b/{everything}\\\" , \\\"UpstreamHttpMethod\\\": [ \\\"POST\\\" , \\\"PUT\\\" , \\\"GET\\\" ] , \\\"Auth\", \"enticationOptions\\\": { \\\"AuthenticationProviderKey\\\": \\\"IdentityApiKey\\\" , \\\"AllowedScopes\\\": [] } } ] , \\\"G\", \"lobalConfiguration\\\": { \\\"RequestIdKey\\\": \\\"OcRequestId\\\" , \\\"AdministrationPath\\\": \\\"/administration\\\" } }\\n`\", \"``\\n\\nThe main functionality of an Ocelot API Gateway is to take incoming HTTP requests and forward th\", \"em on to a downstream service, currently as another HTTP request. Ocelot's describes the routing of \", \"one request to another as a ReRoute.\\n\\nFor instance, let's focus on one of the ReRoutes in the config\", \"uration.json from above, the configuration for the Basket microservice.\\n\\n```\\n{ \\\"DownstreamPathTempla\", \"te\\\": \\\"/api/{version}/{everything}\\\" , \\\"DownstreamScheme\\\": \\\"http\\\" , \\\"DownstreamHostAndPorts\\\": [ { \\\"Hos\", \"t\\\": \\\"basket -api\\\" , \\\"Port\\\": 80 } ] , \\\"UpstreamPathTemplate\\\": \\\"/api/{version}/b/{everything}\\\" , \\\"Upst\", \"reamHttpMethod\\\": [ \\\"POST\\\" , \\\"PUT\\\" , \\\"GET\\\" ] , \\\"AuthenticationOptions\\\": { \\\"AuthenticationProviderKey\\\"\", \": \\\"IdentityApiKey\\\" , \\\"AllowedScopes\\\": [] } }\\n```\\n\\nThe DownstreamPathTemplate, Scheme, and Downstream\", \"HostAndPorts make the internal microservice URL that this request will be forwarded to.\\n\\nThe port is\", \" the internal port used by the service. When using containers, the port specified at its dockerfile.\", \"\\n\\nThe Host is a service name that depends on the service name resolution you are using. When using d\", \"ocker -compose, the services names are provided by the Docker Host, which is using the service names\", \" provided in the docker-compose files. If using an orchestrator like Kubernetes or Service Fabric, t\", \"hat name should be resolved by the DNS or name resolution provided by each orchestrator.\\n\\nDownstream\", \"HostAndPorts is an array that contains the host and port of any downstream services that you wish to\", \" forward requests to. Usually this configuration will just contain one entry but sometimes you might\", \" want to load balance requests to your downstream services and Ocelot lets you add more than one ent\", \"ry and then select a load balancer. But if using Azure and any orchestrator it is probably a better \", \"idea to load balance with the cloud and orchestrator infrastructure.\\n\\nThe UpstreamPathTemplate is th\", \"e URL that Ocelot will use to identify which DownstreamPathTemplate to use for a given request from \", \"the client. Finally, the UpstreamHttpMethod is used so Ocelot can distinguish between different requ\", \"ests (GET, POST, PUT) to the same URL.\\n\\nAt this point, you could have a single Ocelot API Gateway (A\", \"SP.NET Core WebHost) using one or multiple merged configuration.json files or you can also store the\", \" configuration in a Consul KV store .\\n\\nBut as introduced in the architecture and design sections, if\", \" you really want to have autonomous microservices, it might be better to split that single monolithi\", \"c API Gateway into multiple API Gateways and/or BFF (Backend for Frontend). For that purpose, let's \", \"see how to implement that approach with Docker containers.\\n\\nContainers\\n\\nAPI Gateways / BFF\\n\\n## Using\", \" a single Docker container image to run multiple different API Gateway / BFF container types\\n\\nIn eSh\", \"opOnContainers, we're using a single Docker container image with the Ocelot API Gateway but then, at\", \" run time, we create different services/containers for each type of API-Gateway/BFF by providing a d\", \"ifferent configuration.json file, using a docker volume to access a different PC folder for each ser\", \"vice.\\n\\nImage\\n\\nOCELOT\\n\\nGeneric\\n\\nOcelot API Gateway\\n\\nFigure 6 -33. Reusing a single Ocelot Docker imag\", \"e across multiple API Gateway types\\n\\n<!-- image -->\\n\\nIn eShopOnContainers, the \\\"Generic Ocelot API G\", \"ateway Docker Image\\\" is created with the project named 'OcelotApiGw' and the image name \\\"eshop/ocelo\", \"tapigw\\\" that is specified in the dockercompose.yml file. Then, when deploying to Docker, there will \", \"be four API-Gateway containers created from that same Docker image, as shown in the following extrac\", \"t from the docker-compose.yml file.\\n\\nmobileshoppingapigw:\\n\\nimage: eshop/ocelotapigw:${TAG:-latest}\\n\\n\", \"```\\nbuild: context: . dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile mobilemarketingapigw: image:\", \" eshop/ocelotapigw:${TAG:-latest} build: context: . dockerfile: src/ApiGateways/ApiGw-Base/Dockerfil\", \"e webshoppingapigw: image: eshop/ocelotapigw:${TAG:-latest} build: context: . dockerfile: src/ApiGat\", \"eways/ApiGw-Base/Dockerfile webmarketingapigw: image: eshop/ocelotapigw:${TAG:-latest} build: contex\", \"t: . dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\\n```\\n\\nAdditionally, as you can see in the foll\", \"owing docker-compose.override.yml file, the only difference between those API Gateway containers is \", \"the Ocelot configuration file, which is different for each service container and it's specified at r\", \"un time through a Docker volume.\\n\\n```\\nmobileshoppingapigw: environment: -ASPNETCORE_ENVIRONMENT=Deve\", \"lopment -IdentityUrl=http://identity-api ports: -\\\"5200:80\\\" volumes: -./src/ApiGateways/Mobile.Bff.Sh\", \"opping/apigw:/app/configuration mobilemarketingapigw: environment: -ASPNETCORE_ENVIRONMENT=Developme\", \"nt -IdentityUrl=http://identity-api ports: -\\\"5201:80\\\" volumes: -./src/ApiGateways/Mobile.Bff.Marketi\", \"ng/apigw:/app/configuration webshoppingapigw: environment: -ASPNETCORE_ENVIRONMENT=Development -Iden\", \"tityUrl=http://identity-api ports: -\\\"5202:80\\\" volumes: -./src/ApiGateways/Web.Bff.Shopping/apigw:/ap\", \"p/configuration webmarketingapigw: environment: -ASPNETCORE_ENVIRONMENT=Development -IdentityUrl=htt\", \"p://identity-api ports: -\\\"5203:80\\\"\\n```\\n\\nlocalhost\\n\\nApiGateways x + v\\n\\n&lt; &gt; 0\\n\\nApiGw-Base\\n\\n\\u2022 loc\", \"alhost:5202/api/v1/c/catalog/items/2/\\n\\n(\\\"id\\\": 2, \\\"name\\\": \\\".NET Black &amp; White Mug\\\", \\\"description\\\"\", \": \\\".NET Black &amp; White\\n\\nD a@] OcelotApiGw\\n\\nId\\\": 2, \\\"catalogBrand\\\": nul1, \\\"availableStock\\\":99, \\\"re\", \"stockThreshold\\\": 0, \\\"maxStockThreshold\\\": 0, \\\"onReorder\\\": false)\\n\\n- \\u2022 \\u00d7\\n\\n* 2 &lt; ...\\n\\nMug\\\", \\\"price\\\":\", \" 8.50, \\\"pictureFileName\\\": \\\"2.png\\\", \\\"pictureUri\\\":\\\"http://localhost:5202/api/v1/c/catalog/items/2/pic/\", \"\\\",\\\"catalogTypeId\\\":1, \\\"catalogType\\\": null, \\\"catalogBrand\\n\\nMobile. Bff.Marketing\\n\\n<!-- image -->\\n\\nBeca\", \"use of that previous code, and as shown in the Visual Studio Explorer below, the only file needed to\", \" define each specific business/BFF API Gateway is just a configuration.json file, because the four A\", \"PI Gateways are based on the same Docker image.\\n\\n\\u2022 Web.Bff.Shopping\\n\\nD a\\u00a9] Web.Shopping.HttpAggregat\", \"or foJ configuration.json\\n\\nFigure 6 -34. The only file needed to define each API Gateway / BFF with \", \"Ocelot is a configuration file\\n\\n<!-- image -->\\n\\nBy splitting the API Gateway into multiple API Gatew\", \"ays, different development teams focusing on different subsets of microservices can manage their own\", \" API Gateways by using independent Ocelot configuration files. Plus, at the same time they can reuse\", \" the same Ocelot Docker image.\\n\\nNow, if you run eShopOnContainers with the API Gateways (included by\", \" default in VS when opening eShopOnContainers-ServicesAndWebApps.sln solution or if running \\\"docker-\", \"compose up\\\"), the following sample routes will be performed.\\n\\nFor instance, when visiting the upstre\", \"am URL\\n\\nhttp://host.docker.internal:5202/api/v1/c/catalog/items/2/ served by the webshoppingapigw AP\", \"I Gateway, you get the same result from the internal Downstream URL http://catalog-api/api/v1/2 with\", \"in the Docker host, as in the following browser.\\n\\nFigure 6 -35. Accessing a microservice through a U\", \"RL provided by the API Gateway\\n\\n<!-- image -->\\n\\nBecause of testing or debugging reasons, if you want\", \"ed to directly access to the Catalog Docker container (only at the development environment) without \", \"passing through the API Gateway, since 'catalog-api' is a DNS resolution internal to the Docker host\", \" (service discovery handled by dockercompose service names), the only way to directly access the con\", \"tainer is through the external port published in the docker-compose.override.yml, which is provided \", \"only for development tests, such as http://host.docker.internal:5101/api/v1/Catalog/items/1 in the f\", \"ollowing browser.\\n\\n|\\n\\nlocalhost\\n\\n&lt; &gt; 0\\n\\nx + v eShopOnContainers\\n\\n\\u2022 localhost:5101/api/v1/Catal\", \"og/items/2|\\n\\n(API Gateways / BFF and Aggregator-services details)\\n\\n(\\\"id\\\":2,\\\"name\\\":\\\" NET Black &amp; \", \"White Mug\\\", \\\"description\\\":\\\".NET Black &amp; White\\n\\nClient apps eShop mobile app\\n\\nXamarin Forms\\n\\nC#\\n\\n\", \"xPlat. OS:\\n\\niOS\\n\\nAndroid\\n\\nWindows eShop SPA Web app\\n\\nTypeScript/Angular\\n\\nId\\\": 2, \\\"catalogBrand\\\": nul\", \"1, \\\"availableStock\\\": 99, \\\"restockThreshold\\\": 0, \\\"maxStockThreshold\\\": 0, \\\"onReorder\\\": false)\\n\\n- \\u2022 X\\n\\n\", \"# R E\\n\\nMug\\\", \\\"price\\\":8.50, \\\"pictureFileName\\\": \\\"2.png\\\",\\\"pictureUri\\\":\\\"http://localhost:5202/api/v1/c/c\", \"atalog/items/2/pic/\\\",\\\"catalogTypeId\\\":1,\\\"catalogType\\\":nu1l, \\\"catalogBrand\\n\\nIdentity microservice (STS\", \"+users)\\n\\nSQL Server database\\n\\nFigure 6 -36. Direct access to a microservice for testing purposes\\n\\n<!\", \"-- image -->\\n\\nEvent Bus\\n\\nAzure\\n\\nService Bus\\n\\nBut the application is configured so it accesses all th\", \"e microservices through the API Gateways, not through the direct port \\\"shortcuts\\\". Marketing microse\", \"rvice MongoDB\\n\\n\\u2022 CosmosDB\\n\\n## The Gateway aggregation pattern in eShopOnContainers SQL Server\\n\\nWeb-M\", \"arketing\\n\\nMongoDB,\\n\\nAs introduced previously, a flexible way to implement requests aggregation is wi\", \"th custom services, by code. You could also implement request aggregation with the Request Aggregati\", \"on feature in Ocelot , but it might not be as flexible as you need. Therefore, the selected way to i\", \"mplement aggregation in eShopOnContainers is with an explicit ASP.NET Core Web API service for each \", \"aggregator.\\n\\nAccording to that approach, the API Gateway composition diagram is in reality a bit mor\", \"e extended when considering the aggregator services that are not shown in the simplified global arch\", \"itecture diagram shown previously.\\n\\nIn the following diagram, you can also see how the aggregator se\", \"rvices work with their related API Gateways.\\n\\nFigure 6 -37. eShopOnContainers architecture with aggr\", \"egator services\\n\\n<!-- image -->\\n\\nZooming in further, on the \\\"Shopping\\\" business area in the followin\", \"g image, you can see that chattiness between the client apps and the microservices is reduced when u\", \"sing the aggregator services in the API Gateways.\\n\\neShopOnContainers\\n\\n(API Gateways / BFF and Aggreg\", \"ator-services zoom-in)\\n\\nAPI Gateways / BFF\\n\\nMobile-Shopping\\n\\nAggregator\\n\\nMobile-Marketing\\n\\nWeb-Shopp\", \"ing\\n\\nAggregator\\n\\nWeb-Marketing\\n\\nFigure 6 -38. Zoom in vision of the Aggregator services\\n\\n<!-- image \", \"-->\\n\\nYou can notice how when the diagram shows the possible requests coming from the API Gateways it\", \" can get complex. On the other hand, when you use the aggregator pattern, you can see how the arrows\", \" in blue would simplify the communication from a client app perspective. This pattern not only helps\", \" to reduce the chattiness and latency in the communication, it also improves the user experience sig\", \"nificantly for the remote apps (mobile and SPA apps).\\n\\nIn the case of the \\\"Marketing\\\" business area \", \"and microservices, it is a simple use case so there was no need to use aggregators, but it could als\", \"o be possible, if needed.\\n\\n## Authentication and authorization in Ocelot API Gateways\\n\\nIn an Ocelot \", \"API Gateway, you can sit the authentication service, such as an ASP.NET Core Web API service using I\", \"dentityServer providing the auth token, either out or inside the API Gateway.\\n\\nSince eShopOnContaine\", \"rs is using multiple API Gateways with boundaries based on BFF and business areas, the Identity/Auth\", \" service is left out of the API Gateways, as highlighted in yellow in the following diagram.\\n\\n## \\\"Sh\", \"opping\\\"\\n\\nmicroservices\\n\\nClient apps eShop mobile app\\n\\nAuthentication in Ocelot API Gateway\\n\\nXamarin.\", \"Forms\\n\\nC#\\n\\nxPlat. OS:\\n\\niOS\\n\\nAndroid\\n\\nWindows eShop traditional Web app\\n\\nClient apps\\n\\neShop SPA Web a\", \"pp\\n\\nTypeScript/Angular\\n\\nIdentity microservice (STS+users)\\n\\nSQL Server database\\n\\nCatalog microservice\", \"\\n\\n\\u2022 0-\\n\\nSQL Server database\\n\\nFigure 6 -39. Position of the Identity service in eShopOnContainers\\n\\n<!\", \"-- image -->\\n\\nHowever, Ocelot also supports sitting the Identity/Auth microservice within the API Ga\", \"teway boundary, as in this other diagram.\\n\\nFigure 6 -40. Authentication in Ocelot\\n\\n<!-- image -->\\n\\nA\", \"s the previous diagram shows, when the Identity microservice is beneath the API gateway (AG): 1) AG \", \"requests an auth token from identity microservice, 2) The identity microservice returns token to AG,\", \" 34) AG requests from microservices using the auth token. Because eShopOnContainers application has \", \"split the API Gateway into multiple BFF (Backend for Frontend) and business areas API Gateways, anot\", \"her option would have been to create an additional API Gateway for cross-cutting concerns. That choi\", \"ce would be fair in a more complex microservice based architecture with multiple cross-cutting conce\", \"rns microservices. Since there's only one cross-cutting concern in eShopOnContainers, it was decided\", \" to just handle the security service out of the API Gateway realm, for simplicity's sake.\\n\\nAPI Gatew\", \"ays / BFF\\n\\n\\u2022 0\\n\\nInternal microservices\\n\\nDocker Host\\n\\nRabbitMO\\n\\nIn any case, if the app is secured at\", \" the API Gateway level, the authentication module of the Ocelot API Gateway is visited at first when\", \" trying to use any secured microservice. That redirects the HTTP request to visit the Identity or au\", \"th microservice to get the access token so you can visit the protected services with the access\\\\_tok\", \"en.\\n\\nThe way you secure with authentication any service at the API Gateway level is by setting the A\", \"uthenticationProviderKey in its related settings at the configuration.json.\\n\\n```\\n{ \\\"DownstreamPathTe\", \"mplate\\\": \\\"/api/{version}/{everything}\\\" , \\\"DownstreamScheme\\\": \\\"http\\\" , \\\"DownstreamHostAndPorts\\\": [ { \", \"\\\"Host\\\": \\\"basket -api\\\" , \\\"Port\\\": 80 } ] , \\\"UpstreamPathTemplate\\\": \\\"/api/{version}/b/{everything}\\\" , \\\"\", \"UpstreamHttpMethod\\\": [] , \\\"AuthenticationOptions\\\": { \\\"AuthenticationProviderKey\\\": \\\"IdentityApiKey\\\" ,\", \" \\\"AllowedScopes\\\": [] } }\\n```\\n\\nWhen Ocelot runs, it will look at the ReRoutes AuthenticationOptions.A\", \"uthenticationProviderKey and check that there is an Authentication Provider registered with the give\", \"n key. If there isn't, then Ocelot will not start up. If there is, then the ReRoute will use that pr\", \"ovider when it executes.\\n\\nBecause the Ocelot WebHost is configured with the authenticationProviderKe\", \"y = \\\"IdentityApiKey\\\", that will require authentication whenever that service has any requests withou\", \"t any auth token.\\n\\n```\\nnamespace OcelotApiGw { public class Startup { private readonly IConfiguratio\", \"n _cfg; public Startup(IConfiguration configuration) => _cfg = configuration; public void ConfigureS\", \"ervices(IServiceCollection services) { var identityUrl = _cfg . GetValue<string>(\\\"IdentityUrl\\\"); var\", \" authenticationProviderKey = \\\"IdentityApiKey\\\"; //\\u2026 services . AddAuthentication() . AddJwtBearer(aut\", \"henticationProviderKey , x => { x . Authority = identityUrl; x . RequireHttpsMetadata = false; x . T\", \"okenValidationParameters = new Microsoft . IdentityModel . Tokens . TokenValidationParameters() { Va\", \"lidAudiences = new[] { \\\"orders\\\" , \\\"basket\\\" , \\\"locations\\\" , \\\"marketing\\\" , \\\"mobileshoppingagg\\\" , \\\"webs\", \"hoppingagg\\\" } }; }); //...\\n```\\n\\n```\\n} } }\\n```\\n\\nThen, you also need to set authorization with the [Au\", \"thorize] attribute on any resource to be accessed like the microservices, such as in the following B\", \"asket microservice controller.\\n\\n```\\nnamespace Microsoft . eShopOnContainers . Services . Basket . AP\", \"I . Controllers { [Route(\\\"api/v1/[controller]\\\")] [Authorize] public class BasketController : Control\", \"ler { //... } }\\n```\\n\\nThe ValidAudiences such as \\\"basket\\\" are correlated with the audience defined in\", \" each microservice with AddJwtBearer() at the ConfigureServices() of the Startup class, such as in t\", \"he code below.\\n\\n```\\n// prevent from mapping \\\"sub\\\" claim to nameidentifier. JwtSecurityTokenHandler .\", \" DefaultInboundClaimTypeMap . Clear(); var identityUrl = Configuration . GetValue<string>(\\\"IdentityU\", \"rl\\\"); services . AddAuthentication(options => { options . DefaultAuthenticateScheme = JwtBearerDefau\", \"lts . AuthenticationScheme; options . DefaultChallengeScheme = JwtBearerDefaults . AuthenticationSch\", \"eme; }).AddJwtBearer(options => { options . Authority = identityUrl; options . RequireHttpsMetadata \", \"= false; options . Audience = \\\"basket\\\"; });\\n```\\n\\nIf you try to access any secured microservice, like\", \" the Basket microservice with a ReRoute URL based on the API Gateway like http://host.docker.interna\", \"l:5202/api/v1/b/basket/1, then you'll get a 401 Unauthorized unless you provide a valid token. On th\", \"e other hand, if a ReRoute URL is authenticated, Ocelot will invoke whatever downstream scheme is as\", \"sociated with it (the internal microservice URL).\\n\\nAuthorization at Ocelot's ReRoutes tier. Ocelot s\", \"upports claims-based authorization evaluated after the authentication. You set the authorization at \", \"a route level by adding the following lines to the ReRoute configuration.\\n\\n```\\n\\\"RouteClaimsRequireme\", \"nt\\\": { \\\"UserType\\\": \\\"employee\\\" }\\n```\\n\\nIn that example, when the authorization middleware is called, O\", \"celot will find if the user has the claim type 'UserType' in the token and if the value of that clai\", \"m is 'employee'. If it isn't, then the user will not be authorized and the response will be 403 forb\", \"idden.\\n\\nClient apps eShopOnContainers\\n\\n(Deployment into Kubernetes environment)\\n\\nAzure Kubernetes Se\", \"rvice (AKS)\\n\\nIdentity microservice (STS+ users)\\n\\nSQL Server database\\n\\n## Using Kubernetes Ingress pl\", \"us Ocelot API Gateways\\n\\nRabbitMQl\\n\\nWhen using Kubernetes (like in an Azure Kubernetes Service cluste\", \"r), you usually unify all the HTTP requests through the Kubernetes Ingress tier based on Nginx . Ord\", \"ering API\\n\\nBasket microservice\\n\\nIn Kubernetes, if you don't use any ingress approach, then your serv\", \"ices and pods have IPs only routable by the cluster network. Web-Shopping Aggregator Marketing micro\", \"service Azure\\n\\n\\u2022 CosmosDB\\n\\nBut if you use an ingress approach, you'll have a middle tier between the\", \" Internet and your services (including your API Gateways), acting as a reverse proxy. Locations micr\", \"oservice MongoDB CosmosDB\\n\\nAs a definition, an Ingress is a collection of rules that allow inbound c\", \"onnections to reach the cluster services. An ingress is configured to provide services externally re\", \"achable URLs, load balance traffic, SSL termination and more. Users request ingress by POSTing the I\", \"ngress resource to the API server.\\n\\nIn eShopOnContainers, when developing locally and using just you\", \"r development machine as the Docker host, you are not using any ingress but only the multiple API Ga\", \"teways.\\n\\nHowever, when targeting a \\\"production\\\" environment based on Kubernetes, eShopOnContainers i\", \"s using an ingress in front of the API gateways. That way, the clients still call the same base URL \", \"but the requests are routed to multiple API Gateways or BFF.\\n\\nAPI Gateways are front-ends or fa\\u00e7ades\", \" surfacing only the services but not the web applications that are usually out of their scope. In ad\", \"dition, the API Gateways might hide certain internal microservices.\\n\\nThe ingress, however, is just r\", \"edirecting HTTP requests but not trying to hide any microservice or web app.\\n\\nHaving an ingress Ngin\", \"x tier in Kubernetes in front of the web applications plus the several Ocelot API Gateways / BFF is \", \"the ideal architecture, as shown in the following diagram.\\n\\nFigure 6 -41. The ingress tier in eShopO\", \"nContainers when deployed into Kubernetes\\n\\n<!-- image -->\\n\\nA Kubernetes Ingress acts as a reverse pr\", \"oxy for all traffic to the app, including the web applications, that are out of the Api gateway scop\", \"e. When you deploy eShopOnContainers into Kubernetes, it\\n\\nexposes just a few services or endpoints v\", \"ia ingress, basically the following list of postfixes on the URLs:\\n\\n- / for the client SPA web appli\", \"cation\\n- /webmvc for the client MVC web application\\n- /webstatus for the client web app showing the \", \"status/healthchecks\\n- /webshoppingapigw for the web BFF and shopping business processes\\n- /webmarket\", \"ingapigw for the web BFF and marketing business processes\\n- /mobileshoppingapigw for the mobile BFF \", \"and shopping business processes\\n- /mobilemarketingapigw for the mobile BFF and marketing business pr\", \"ocesses\\n\\nWhen deploying to Kubernetes, each Ocelot API Gateway is using a different \\\"configuration.j\", \"son\\\" file for each pod running the API Gateways. Those \\\"configuration.json\\\" files are provided by mo\", \"unting (originally with the deploy.ps1 script) a volume created based on a Kubernetes config map nam\", \"ed 'ocelot'. Each container mounts its related configuration file in the container's folder named /a\", \"pp/configuration.\\n\\nIn the source code files of eShopOnContainers, the original \\\"configuration.json\\\" \", \"files can be found within the k8s/ocelot/ folder. There's one file for each BFF/APIGateway.\\n\\n## Addi\", \"tional cross -cutting features in an Ocelot API Gateway\\n\\nThere are other important features to resea\", \"rch and use, when using an Ocelot API Gateway, described in the following links.\\n\\n- Service discover\", \"y in the client side integrating Ocelot with Consul or Eureka https://ocelot.readthedocs.io/en/lates\", \"t/features/servicediscovery.html\\n- Caching at the API Gateway tier https://ocelot.readthedocs.io/en/\", \"latest/features/caching.html\\n- Logging at the API Gateway tier https://ocelot.readthedocs.io/en/late\", \"st/features/logging.html\\n- Quality of Service (Retries and Circuit breakers) at the API Gateway tier\", \" https://ocelot.readthedocs.io/en/latest/features/qualityofservice.html\\n- Rate limiting https://ocel\", \"ot.readthedocs.io/en/latest/features/ratelimiting.html\\n- Swagger for Ocelot https://github.com/Burgy\", \"n/MMLib.SwaggerForOcelot\\n\\n## Tackle Business Complexity in a Microservice with DDD and CQRS Patterns\", \"\\n\\nDesign a domain model for each microservice or Bounded Context that reflects understanding of the \", \"business domain.\\n\\nThis section focuses on more advanced microservices that you implement when you ne\", \"ed to tackle complex subsystems, or microservices derived from the knowledge of domain experts with \", \"everchanging business rules. The architecture patterns used in this section are based on domain-driv\", \"en design (DDD) and Command and Query Responsibility Segregation (CQRS) approaches, as illustrated i\", \"n Figure 7-1.\\n\\nExternal architecture per application\\n\\nBack end\\n\\n## Microservice 1\\n\\nMicroservice 2]\\n\\n\", \"API Gateway\\n\\nInternal architecture per microservice\\n\\nDDD\\n\\nCQRS and\\n\\nDomain Events\\n\\nFigure 7 -1. Exte\", \"rnal microservice architecture versus internal architecture patterns for each microservice\\n\\n<!-- ima\", \"ge -->\\n\\nHowever, most of the techniques for data driven microservices, such as how to implement an A\", \"SP.NET Core Web API service or how to expose Swagger metadata with Swashbuckle or NSwag, are also ap\", \"plicable to the more advanced microservices implemented internally with DDD patterns. This section i\", \"s an extension of the previous sections, because most of the practices explained earlier also apply \", \"here or for any kind of microservice.\\n\\nThis section first provides details on the simplified CQRS pa\", \"tterns used in the eShopOnContainers reference application. Later, you will get an overview of the D\", \"DD techniques that enable you to find common patterns that you can reuse in your applications.\\n\\nDDD \", \"is a large topic with a rich set of resources for learning. You can start with books like DomainDriv\", \"en Design by Eric Evans and additional materials from Vaughn Vernon, Jimmy Nilsson, Greg Young, Udi \", \"Dahan, Jimmy Bogard, and many other DDD/CQRS experts. But most of all you need to try to learn how t\", \"o apply DDD techniques from the conversations, whiteboarding, and domain modeling sessions with the \", \"experts in your concrete business domain.\\n\\n## Additional resources\\n\\n## DDD (Domain-Driven Design)\\n\\n-\", \" Eric Evans. Domain Language https://domainlanguage.com/\\n- Martin Fowler. Domain -Driven Design http\", \"s://martinfowler.com/tags/domain%20driven%20design.html\\n- Jimmy Bogard. Strengthening your domain: a\", \" primer https://lostechies.com/jimmybogard/2010/02/04/strengthening-your-domain-a-primer/\\n\\nClient ap\", \"ps\\n\\nMobile app\\n\\nSPA\\n\\nWeb app\\n\\n## DDD books\\n\\n- Eric Evans. Domain -Driven Design: Tackling Complexity\", \" in the Heart of Software https://www.amazon.com/Domain-Driven-Design-Tackling-ComplexitySoftware/dp\", \"/0321125215/\\n- Eric Evans. Domain -Driven Design Reference: Definitions and Pattern Summaries https:\", \"//www.amazon.com/Domain-Driven-Design-Reference-Definitions-2014-0922/dp/B01N8YB4ZO/\\n- Vaughn Vernon\", \". Implementing Domain-Driven Design https://www.amazon.com/Implementing-Domain-Driven-Design-VaughnV\", \"ernon/dp/0321834577/\\n- Vaughn Vernon. Domain-Driven Design Distilled https://www.amazon.com/Domain-D\", \"riven-Design-Distilled-Vaughn-Vernon/dp/0134434420/\\n- Jimmy Nilsson. Applying Domain-Driven Design a\", \"nd Patterns https://www.amazon.com/Applying-Domain-Driven-Design-PatternsExamples/dp/0321268202/\\n- C\", \"esar de la Torre. N -Layered Domain-Oriented Architecture Guide with .NET https://www.amazon.com/N-L\", \"ayered-Domain-Oriented-Architecture-GuideNET/dp/8493903612/\\n- Abel Avram and Floyd Marinescu. Domain\", \"-Driven Design Quickly https://www.amazon.com/Domain-Driven-Design-Quickly-Abel-Avram/dp/1411609255/\", \"\\n- Scott Millett, Nick Tune - Patterns, Principles, and Practices of Domain-Driven Design https://ww\", \"w.wiley.com/Patterns%2C+Principles%2C+and+Practices+of+Domain+Driven+Des ign-p-9781118714706\\n\\n## DDD\", \" training\\n\\n- Julie Lerman and Steve Smith. Domain -Driven Design Fundamentals https://www.pluralsigh\", \"t.com/courses/fundamentals-domain-driven-design\\n\\n## Apply simplified CQRS and DDD patterns in a micr\", \"oservice\\n\\nCQRS is an architectural pattern that separates the models for reading and writing data. T\", \"he related term Command Query Separation (CQS) was originally defined by Bertrand Meyer in his book \", \"ObjectOriented Software Construction. The basic idea is that you can divide a system's operations in\", \"to two sharply separated categories:\\n\\n- Queries. These queries return a result and don't change the \", \"state of the system, and they're free of side effects.\\n- Commands. These commands change the state o\", \"f a system.\\n\\nCQS is a simple concept: it is about methods within the same object being either querie\", \"s or commands. Each method either returns state or mutates state, but not both. Even a single reposi\", \"tory pattern object can comply with CQS. CQS can be considered a foundational principle for CQRS.\\n\\nC\", \"ommand and Query Responsibility Segregation (CQRS) was introduced by Greg Young and strongly promote\", \"d by Udi Dahan and others. It's based on the CQS principle, although it's more detailed. It can be c\", \"onsidered a pattern based on commands and events plus optionally on asynchronous messages. In many c\", \"ases, CQRS is related to more advanced scenarios, like having a different physical database for read\", \"s (queries) than for writes (updates). Moreover, a more evolved CQRS system might implement Event-So\", \"urcing (ES) for your updates database, so you would only store events in the domain model instead of\", \" storing the current-state data. However, this approach is not used in this guide. This guide uses t\", \"he simplest CQRS approach, which consists of just separating the queries from the commands.\\n\\nThe sep\", \"aration aspect of CQRS is achieved by grouping query operations in one layer and commands in another\", \" layer. Each layer has its own data model (note that we say model, not necessarily a different datab\", \"ase) and is built using its own combination of patterns and technologies. More importantly, the two \", \"layers can be within the same tier or microservice, as in the example (ordering microservice) used f\", \"or this guide. Or they could be implemented on different microservices or processes so they can be o\", \"ptimized and scaled out separately without affecting one another.\\n\\nCQRS means having two objects for\", \" a read/write operation where in other contexts there's one. There are reasons to have a denormalize\", \"d reads database, which you can learn about in more advanced CQRS literature. But we aren't using th\", \"at approach here, where the goal is to have more flexibility in the queries instead of limiting the \", \"queries with constraints from DDD patterns like aggregates.\\n\\nAn example of this kind of service is t\", \"he ordering microservice from the eShopOnContainers reference application. This service implements a\", \" microservice based on a simplified CQRS approach. It uses a single data source or database, but two\", \" logical models plus DDD patterns for the transactional domain, as shown in Figure 7-2.\\n\\n\\u0413\\n\\nExternal\", \" IP\\n\\nand Port |\\n\\nSimplified CQRS and DDD microservice\\n\\nHigh level design\\n\\n-\\n\\n- - -\\n\\nDocker Host\\n\\nLog\", \"ical \\\"Ordering\\\" Microservice\\n\\n<!-- image -->\\n\\n-\\n\\nFigure 7 -2. Simplified CQRS- and DDD-based microse\", \"rvice\\n\\nThe Logical \\\"Ordering\\\" Microservice includes its Ordering database, which can be, but doesn't\", \" have to be, the same Docker host. Having the database in the same Docker host is good for developme\", \"nt, but not for production.\\n\\nThe application layer can be the Web API itself. The important design a\", \"spect here is that the microservice has split the queries and ViewModels (data models especially cre\", \"ated for the client applications) from the commands, domain model, and transactions following the CQ\", \"RS pattern. This approach keeps the queries independent from restrictions and constraints coming fro\", \"m DDD patterns that only make sense for transactions and updates, as explained in later sections.\\n\\n#\", \"# Additional resources\\n\\n- Greg Young. Versioning in an Event Sourced System (Free to read online e-b\", \"ook) https://leanpub.com/esversioning/read\\n\\n## Apply CQRS and CQS approaches in a DDD microservice i\", \"n eShopOnContainers\\n\\nThe design of the ordering microservice at the eShopOnContainers reference appl\", \"ication is based on CQRS principles. However, it uses the simplest approach, which is just separatin\", \"g the queries from the commands and using the same database for both actions.\\n\\nThe essence of those \", \"patterns, and the important point here, is that queries are idempotent: no matter how many times you\", \" query a system, the state of that system won't change. In other words, queries are side -effect fre\", \"e.\\n\\nTherefore, you could use a different \\\"reads\\\" data model than the transactional logic \\\"writes\\\" do\", \"main model, even though the ordering microservices are using the same database. Hence, this is a sim\", \"plified CQRS approach.\\n\\nOn the other hand, commands, which trigger transactions and data updates, ch\", \"ange state in the system. With commands, you need to be careful when dealing with complexity and eve\", \"r-changing business rules. This is where you want to apply DDD techniques to have a better modeled s\", \"ystem.\\n\\nThe DDD patterns presented in this guide should not be applied universally. They introduce c\", \"onstraints on your design. Those constraints provide benefits such as higher quality over time, espe\", \"cially in commands and other code that modifies system state. However, those constraints add complex\", \"ity with fewer benefits for reading and querying data.\\n\\nOne such pattern is the Aggregate pattern, w\", \"hich we examine more in later sections. Briefly, in the Aggregate pattern, you treat many domain obj\", \"ects as a single unit as a result of their relationship in the domain. You might not always gain adv\", \"antages from this pattern in queries; it can increase the complexity of query logic. For read-only q\", \"ueries, you do not get the advantages of treating multiple objects as a single Aggregate. You only g\", \"et the complexity.\\n\\nAs shown in Figure 7-2 in the previous section, this guide suggests using DDD pa\", \"tterns only in the transactional/updates area of your microservice (that is, as triggered by command\", \"s). Queries can follow a simpler approach and should be separated from commands, following a CQRS ap\", \"proach.\\n\\nFor implementing the \\\"queries side\\\", you can choose between many approaches, from your full\", \"-blown ORM like EF Core, AutoMapper projections, stored procedures, views, materialized views or a m\", \"icro ORM.\\n\\nIn this guide and in eShopOnContainers (specifically the ordering microservice) we chose \", \"to implement straight queries using a micro ORM like Dapper. This guide lets you implement any query\", \" based on SQL statements to get the best performance, thanks to a light framework with little overhe\", \"ad.\\n\\nWhen you use this approach, any updates to your model that impact how entities are persisted to\", \" a SQL database also need separate updates to SQL queries used by Dapper or any other separate (nonE\", \"F) approaches to querying.\\n\\n## CQRS and DDD patterns are not top-level architectures\\n\\nIt's important\", \" to understand that CQRS and most DDD patterns (like DDD layers or a domain model with aggregates) a\", \"re not architectural styles, but only architecture patterns. Microservices, SOA, and event -driven a\", \"rchitecture (EDA) are examples of architectural styles. They describe a system of many components, s\", \"uch as many microservices. CQRS and DDD patterns describe something inside a single system or compon\", \"ent; in this case, something inside a microservice.\\n\\nDifferent Bounded Contexts (BCs) will employ di\", \"fferent patterns. They have different responsibilities, and that leads to different solutions. It is\", \" worth emphasizing that forcing the same pattern everywhere leads to failure. Do not use CQRS and DD\", \"D patterns everywhere. Many subsystems, BCs, or microservices are simpler and can be implemented mor\", \"e easily using simple CRUD services or using another approach.\\n\\nHigh level \\\"Queries-side\\\" in a simpl\", \"ified CQRS\\n\\nUl app\\n\\nThere is only one application architecture: the architecture of the system or en\", \"d-to-end application you are designing (for example, the microservices architecture). However, the d\", \"esign of each Bounded Context or microservice within that application reflects its own tradeoffs and\", \" internal design decisions at an architecture patterns level. Do not try to apply the same architect\", \"ural patterns as CQRS or DDD everywhere.\\n\\n## Additional resources\\n\\nInfrastructure\\n\\n(MicroORM or\\n\\nEnt\", \"ity Framework)\\n\\n- Martin Fowler. CQRS https://martinfowler.com/bliki/CQRS.html\\n- Greg Young. CQRS Do\", \"cuments https://cqrs.files.wordpress.com/2010/11/cqrs\\\\_documents.pdf\\n- Udi Dahan. Clarified CQRS htt\", \"ps://udidahan.com/2009/12/09/clarified-cqrs/\\n\\n## Implement reads/queries in a CQRS microservice\\n\\nFor\", \" reads/queries, the ordering microservice from the eShopOnContainers reference application implement\", \"s the queries independently from the DDD model and transactional area. This implementation was done \", \"primarily because the demands for queries and for transactions are drastically different. Writes exe\", \"cute transactions that must be compliant with the domain logic. Queries, on the other hand, are idem\", \"potent and can be segregated from the domain rules.\\n\\nThe approach is simple, as shown in Figure 7-3.\", \" The API interface is implemented by the Web API controllers using any infrastructure, such as a mic\", \"ro Object Relational Mapper (ORM) like Dapper, and returning dynamic ViewModels depending on the nee\", \"ds of the UI applications.\\n\\nFigure 7 -3. The simplest approach for queries in a CQRS microservice\\n\\n<\", \"!-- image -->\\n\\nThe simplest approach for the queries-side in a simplified CQRS approach can be imple\", \"mented by querying the database with a Micro-ORM like Dapper, returning dynamic ViewModels. The quer\", \"y\\n\\nDatabase\\n\\ndefinitions query the database and return a dynamic ViewModel built on the fly for each\", \" query. Since the queries are idempotent, they won't change the data no matter how many times you ru\", \"n a query. Therefore, you don't need to be restricted by any DDD pattern used in the transactional s\", \"ide, like aggregates and other patterns, and that is why queries are separated from the transactiona\", \"l area. You query the database for the data that the UI needs and return a dynamic ViewModel that do\", \"es not need to be statically defined anywhere (no classes for the ViewModels) except in the SQL stat\", \"ements themselves.\\n\\nSince this approach is simple, the code required for the queries side (such as c\", \"ode using a micro ORM like Dapper) can be implemented within the same Web API project. Figure 7-4 sh\", \"ows this approach. The queries are defined in the Ordering.API microservice project within the eShop\", \"OnContainers solution.\\n\\nFigure 7 -4. Queries in the Ordering microservice in eShopOnContainers\\n\\n<!--\", \" image -->\\n\\n## Use ViewModels specifically made for client apps, independent from domain model const\", \"raints\\n\\nSince the queries are performed to obtain the data needed by the client applications, the re\", \"turned type can be specifically made for the clients, based on the data returned by the queries. The\", \"se models, or Data Transfer Objects (DTOs), are called ViewModels.\\n\\nThe returned data (ViewModel) ca\", \"n be the result of joining data from multiple entities or tables in the database, or even across mul\", \"tiple aggregates defined in the domain model for the transactional area. In this case, because you a\", \"re creating queries independent of the domain model, the aggregates boundaries and constraints are i\", \"gnored and you're free to query any table and column you might need. This approach provides great fl\", \"exibility and productivity for the developers creating or updating the queries.\\n\\nThe ViewModels can \", \"be static types defined in classes (as is implemented in the ordering microservice). Or they can be \", \"created dynamically based on the queries performed, which is agile for developers.\\n\\n## Use Dapper as\", \" a micro ORM to perform queries\\n\\nYou can use any micro ORM, Entity Framework Core, or even plain ADO\", \".NET for querying. In the sample application, Dapper was selected for the ordering microservice in e\", \"ShopOnContainers as a good example of a popular micro ORM. It can run plain SQL queries with great p\", \"erformance, because it's a light framework. Using Dapper, you can write a SQL query that can access \", \"and join multiple tables.\\n\\n\\u2022\\n\\nDapper by Sam Saffron, Marc Gravell, Nick Craver\\n\\nA high performance M\", \"icro-ORM supporting SQL Server, MySQL, Sqlite, SqICE, Firebird etc...\\n\\nDapper is an open-source proj\", \"ect (original created by Sam Saffron), and is part of the building blocks used in Stack Overflow. To\", \" use Dapper, you just need to install it through the Dapper NuGet package , as shown in the followin\", \"g figure:\\n\\n<!-- image -->\\n\\nYou also need to add a using directive so your code has access to the Dap\", \"per extension methods.\\n\\nWhen you use Dapper in your code, you directly use the SqlConnection class a\", \"vailable in the Microsoft.Data.SqlClient namespace. Through the QueryAsync method and other extensio\", \"n methods that extend the SqlConnection class, you can run queries in a straightforward and performa\", \"nt way.\\n\\n## Dynamic versus static ViewModels\\n\\nWhen returning ViewModels from the server-side to clie\", \"nt apps, you can think about those ViewModels as DTOs (Data Transfer Objects) that can be different \", \"to the internal domain entities of your entity model because the ViewModels hold the data the way th\", \"e client app needs. Therefore, in many cases, you can aggregate data coming from multiple domain ent\", \"ities and compose the ViewModels precisely according to how the client app needs that data.\\n\\nThose V\", \"iewModels or DTOs can be defined explicitly (as data holder classes), like the OrderSummary class sh\", \"own in a later code snippet. Or, you could just return dynamic ViewModels or dynamic DTOs based on t\", \"he attributes returned by your queries as a dynamic type.\\n\\n## ViewModel as dynamic type\\n\\nAs shown in\", \" the following code, a ViewModel can be directly returned by the queries by just returning a dynamic\", \" type that internally is based on the attributes returned by a query. That means that the subset of \", \"attributes to be returned is based on the query itself. Therefore, if you add a new column to the qu\", \"ery or join, that data is dynamically added to the returned ViewModel.\\n\\n```\\nusing Dapper; using Micr\", \"osoft . Extensions . Configuration; using System . Data . SqlClient; using System . Threading . Task\", \"s; using System . Dynamic; using System . Collections . Generic; public class OrderQueries : IOrderQ\", \"ueries { public async Task<IEnumerable<dynamic>> GetOrdersAsync() { using (var connection = new SqlC\", \"onnection(_connectionString)) { connection . Open(); return await connection . QueryAsync<dynamic>( \", \"@\\\"SELECT o.[Id] as ordernumber, o.[OrderDate] as [date],os.[Name] as [status], SUM(oi . units*oi . u\", \"nitprice) as total FROM [ordering].[Orders] o LEFT JOIN[ordering].[orderitems] oi ON o . Id = oi . o\", \"rderid LEFT JOIN[ordering].[orderstatus] os on o . OrderStatusId = os . Id\\n```\\n\\n\\u2022 v1.50.5\\n\\n```\\nGROUP\", \" BY o.[Id], o.[OrderDate], os.[Name]\\\"); } } }\\n```\\n\\nThe important point is that by using a dynamic ty\", \"pe, the returned collection of data is dynamically assembled as the ViewModel.\\n\\nPros: This approach \", \"reduces the need to modify static ViewModel classes whenever you update the SQL sentence of a query,\", \" making this design approach agile when coding, straightforward, and quick to evolve in regard to fu\", \"ture changes.\\n\\nCons: In the long term, dynamic types can negatively impact the clarity and the compa\", \"tibility of a service with client apps. In addition, middleware software like Swashbuckle cannot pro\", \"vide the same level of documentation on returned types if using dynamic types.\\n\\n## ViewModel as pred\", \"efined DTO classes\\n\\nPros: Having static, predefined ViewModel classes, like \\\"contracts\\\" based on exp\", \"licit DTO classes, is definitely better for public APIs but also for long-term microservices, even i\", \"f they are only used by the same application.\\n\\nIf you want to specify response types for Swagger, yo\", \"u need to use explicit DTO classes as the return type. Therefore, predefined DTO classes allow you t\", \"o offer richer information from Swagger. That improves the API documentation and compatibility when \", \"consuming an API.\\n\\nCons: As mentioned earlier, when updating the code, it takes some more steps to u\", \"pdate the DTO classes.\\n\\nTip based on our experience: In the queries implemented at the Ordering micr\", \"oservice in eShopOnContainers, we started developing by using dynamic ViewModels as it was straightf\", \"orward and agile on the early development stages. But, once the development was stabilized, we chose\", \" to refactor the APIs and use static or pre-defined DTOs for the ViewModels, because it is clearer f\", \"or the microservice's consumers to know explicit DTO types, used as \\\"contracts\\\".\\n\\nIn the following e\", \"xample, you can see how the query is returning data by using an explicit ViewModel DTO class: the Or\", \"derSummary class.\\n\\n```\\nusing Dapper; using Microsoft . Extensions . Configuration; using System . Da\", \"ta . SqlClient; using System . Threading . Tasks; using System . Dynamic; using System . Collections\", \" . Generic; public class OrderQueries : IOrderQueries { public async Task<IEnumerable<OrderSummary>>\", \" GetOrdersAsync() { using (var connection = new SqlConnection(_connectionString)) { connection . Ope\", \"n(); return await connection . QueryAsync<OrderSummary>( @\\\"SELECT o.[Id] as ordernumber, o.[OrderDat\", \"e] as [date],os.[Name] as [status],\\n```\\n\\n```\\nSUM(oi . units*oi . unitprice) as total FROM [ordering]\", \".[Orders] o LEFT JOIN[ordering].[orderitems] oi ON o . Id = oi . orderid LEFT JOIN[ordering].[orders\", \"tatus] os on o . OrderStatusId = os . Id GROUP BY o.[Id], o.[OrderDate], os.[Name] ORDER BY o.[Id]\\\")\", \"; } } }\\n```\\n\\n## Describe response types of Web APIs\\n\\nDevelopers consuming web APIs and microservices\", \" are most concerned with what is returned\\u2014 specifically response types and error codes (if not stand\", \"ard). The response types are handled in the XML comments and data annotations.\\n\\nWithout proper docum\", \"entation in the Swagger UI, the consumer lacks knowledge of what types are being returned or what HT\", \"TP codes can be returned. That problem is fixed by adding the Microsoft.AspNetCore.Mvc.ProducesRespo\", \"nseTypeAttribute, so Swashbuckle can generate richer information about the API return model and valu\", \"es, as shown in the following code:\\n\\n```\\nnamespace Microsoft . eShopOnContainers . Services . Orderi\", \"ng . API . Controllers { [Route(\\\"api/v1/[controller]\\\")] [Authorize] public class OrdersController : \", \"Controller { //Additional code... [Route( \\\"\\\" )] [HttpGet] [ProducesResponseType(typeof(IEnumerable<O\", \"rderSummary>), (int)HttpStatusCode . OK)] public async Task<IActionResult> GetOrders() { var userid \", \"= _identityService . GetUserIdentity(); var orders = await _orderQueries . GetOrdersFromUserAsync(Gu\", \"id . Parse(userid)); return Ok(orders); } } }\\n```\\n\\nHowever, the ProducesResponseType attribute canno\", \"t use dynamic as a type but requires to use explicit types, like the OrderSummary ViewModel DTO, sho\", \"wn in the following example:\\n\\n```\\npublic class OrderSummary { public int ordernumber { get; set; } p\", \"ublic DateTime date { get; set; } public string status { get; set; } public double total { get; set;\", \" } } // or using C# 8 record types: public record OrderSummary(int ordernumber , DateTime date , str\", \"ing status , double total);\\n```\\n\\n1- swagger\\n\\nOrdering HTTP API\\u00ae\\n\\nswasger/v1/swagser.ison\\n\\nThe Orderi\", \"ng Service HTTP API\\n\\nTerms of service\\n\\nThis is another reason why explicit returned types are better\", \" than dynamic types, in the long term. When using the ProducesResponseType attribute, you can also s\", \"pecify what is the expected outcome regarding possible HTTP errors/codes, like 200, 400, etc. Author\", \"ize |\\n\\nIn the following image, you can see how Swagger UI shows the ResponseType information.\\n\\nV\\n\\nFi\", \"gure 7 -5. Swagger UI showing response types and possible HTTP status codes from a Web API\\n\\n<!-- ima\", \"ge -->\\n\\nThe image shows some example values based on the ViewModel types and the possible HTTP statu\", \"s codes that can be returned.\\n\\n## Additional resources\\n\\n- Dapper\\n\\nhttps://github.com/StackExchange/d\", \"apper-dot-net\\n\\n- Julie Lerman. Data Points -Dapper, Entity Framework and Hybrid Apps (MSDN magazine \", \"article)\\n\\nOrders\\n\\nPUT\\n\\nPUT\\n\\nGET\\n\\nGET\\n\\n/api/v1/Orders/cancel\\n\\n/api/v1/Orders/ship\\n\\n/api/v1/Orders/{or\", \"derId}\\n\\n/api/v1/Orders\\n\\nParameters\\n\\nNo parameters\\n\\nResponses\\n\\nCode\\n\\n200\\n\\n401\\n\\n403\\n\\nSelect a spec\\n\\nOr\", \"dering.API V1\\n\\nhttps://learn.microsoft.com/archive/msdn-magazine/2016/may/data-points-dapper-entityf\", \"ramework -and -hybrid-apps\\n\\n- ASP.NET Core Web API Help Pages using Swagger https://learn.microsoft.\", \"com/aspnet/core/tutorials/web-api-help-pages-usingswagger?tabs=visual-studio\\n- Create record types h\", \"ttps://learn.microsoft.com/dotnet/csharp/whats-new/tutorials/records\\n\\n## Design a DDD -oriented micr\", \"oservice\\n\\nDomain -driven design (DDD) advocates modeling based on the reality of business as relevan\", \"t to your use cases. In the context of building applications, DDD talks about problems as domains. I\", \"t describes independent problem areas as Bounded Contexts (each Bounded Context correlates to a micr\", \"oservice), and emphasizes a common language to talk about these problems. It also suggests many tech\", \"nical concepts and patterns, like domain entities with rich models (no anemic-domain model), value o\", \"bjects, aggregates, and aggregate root (or root entity) rules to support the internal implementation\", \". This section introduces the design and implementation of those internal patterns.\\n\\nSometimes these\", \" DDD technical rules and patterns are perceived as obstacles that have a steep learning curve for im\", \"plementing DDD approaches. But the important part is not the patterns themselves, but organizing the\", \" code so it is aligned to the business problems, and using the same business terms (ubiquitous langu\", \"age). In addition, DDD approaches should be applied only if you are implementing complex microservic\", \"es with significant business rules. Simpler responsibilities, like a CRUD service, can be managed wi\", \"th simpler approaches.\\n\\nWhere to draw the boundaries is the key task when designing and defining a m\", \"icroservice. DDD patterns help you understand the complexity in the domain. For the domain model for\", \" each Bounded Context, you identify and define the entities, value objects, and aggregates that mode\", \"l your domain. You build and refine a domain model that is contained within a boundary that defines \", \"your context. And that is explicit in the form of a microservice. The components within those bounda\", \"ries end up being your microservices, although in some cases a BC or business microservices can be c\", \"omposed of several physical services. DDD is about boundaries and so are microservices.\\n\\n## Keep the\", \" microservice context boundaries relatively small\\n\\nDetermining where to place boundaries between Bou\", \"nded Contexts balances two competing goals. First, you want to initially create the smallest possibl\", \"e microservices, although that should not be the main driver; you should create a boundary around th\", \"ings that need cohesion. Second, you want to avoid chatty communications between microservices. Thes\", \"e goals can contradict one another. You should balance them by decomposing the system into as many s\", \"mall microservices as you can until you see communication boundaries growing quickly with each addit\", \"ional attempt to separate a new Bounded Context. Cohesion is key within a single bounded context.\\n\\nI\", \"t is similar to the Inappropriate Intimacy code smell when implementing classes. If two microservice\", \"s need to collaborate a lot with each other, they should probably be the same microservice.\\n\\nAnother\", \" way to look at this aspect is autonomy. If a microservice must rely on another service to directly \", \"service a request, it is not truly autonomous.\\n\\n## Layers in DDD microservices\\n\\nMost enterprise appl\", \"ications with significant business and technical complexity are defined by multiple layers. The laye\", \"rs are a logical artifact, and are not related to the deployment of the service. They exist to help \", \"developers manage the complexity in the code. Different layers (like the domain model layer versus t\", \"he presentation layer, etc.) might have different types, which mandate translations between those ty\", \"pes.\\n\\nFor example, an entity could be loaded from the database. Then part of that information, or an\", \" aggregation of information including additional data from other entities, can be sent to the client\", \" UI through a REST Web API. The point here is that the domain entity is contained within the domain \", \"model layer and should not be propagated to other areas that it does not belong to, like to the pres\", \"entation layer.\\n\\nAdditionally, you need to have always-valid entities (see the Designing validations\", \" in the domain model layer section) controlled by aggregate roots (root entities). Therefore, entiti\", \"es should not be bound to client views, because at the UI level some data might still not be validat\", \"ed. This reason is what the ViewModel is for. The ViewModel is a data model exclusively for presenta\", \"tion layer needs. The domain entities do not belong directly to the ViewModel. Instead, you need to \", \"translate between ViewModels and domain entities and vice versa.\\n\\nWhen tackling complexity, it is im\", \"portant to have a domain model controlled by aggregate roots that make sure that all the invariants \", \"and rules related to that group of entities (aggregate) are performed through a single entry-point o\", \"r gate, the aggregate root.\\n\\nFigure 7-5 shows how a layered design is implemented in the eShopOnCont\", \"ainers application.\\n\\nOrdering.Domain\\n\\n\\u2022 i Dependencies\\n\\nLayers in a Domain-Driven Design Microservic\", \"e\\n\\n\\u2022 '\\u00ae NuGet\\n\\n\\u2022 SDK\\n\\nNETStandard.Library\\n\\nOrdering microservice\\n\\nOrdering\\n\\n\\u2022 aQ] Ordering.API\\n\\n\\u2022 a|\", \"c# Ordering.Domain +\\n\\nD all Ordering Infrastructure\\n\\nASP.NET Web API\\n\\nNetwork access to microservice\", \"\\n\\n## Application layer API contracts/implementation Commands and command handlers Queries (when usin\", \"g an CQS approach)\\n\\nMicro ORMs like Dapper\\n\\nFigure 7 -5. DDD layers in the ordering microservice in \", \"eShopOnContainers\\n\\n<!-- image -->\\n\\nThe three layers in a DDD microservice like Ordering. Each layer \", \"is a VS project: Application layer is Ordering.API, Domain layer is Ordering.Domain and the Infrastr\", \"ucture layer is Ordering.Infrastructure. You want to design the system so that each layer communicat\", \"es only with certain other layers. That approach may be easier to enforce if layers are implemented \", \"as different class libraries, because you can clearly identify what dependencies are set between lib\", \"raries. For instance, the domain model layer should not take a dependency on any other layer (the do\", \"main model classes should be Plain Old Class Objects, or POCO, classes). As shown in Figure 7-6, the\", \" Ordering.Domain layer library has dependencies only on the .NET libraries or NuGet packages, but no\", \"t on any other custom library, such as data library or persistence library.\\n\\nFigure 7 -6. Layers imp\", \"lemented as libraries allow better control of dependencies between layers\\n\\n<!-- image -->\\n\\n## The do\", \"main model layer\\n\\nEric Evans's excellent book Domain Driven Design says the following about the doma\", \"in model layer and the application layer.\\n\\nDomain Model Layer: Responsible for representing concepts\", \" of the business, information about the business situation, and business rules. State that reflects \", \"the business situation is controlled and used here, even though the technical details of storing it \", \"are delegated to the infrastructure. This layer is the heart of business software.\\n\\n\\u2022\\n\\nThe domain mo\", \"del layer is where the business is expressed. When you implement a microservice domain model layer i\", \"n .NET, that layer is coded as a class library with the domain entities that capture data plus behav\", \"ior (methods with logic).\\n\\nFollowing the Persistence Ignorance and the Infrastructure Ignorance prin\", \"ciples, this layer must completely ignore data persistence details. These persistence tasks should b\", \"e performed by the infrastructure layer. Therefore, this layer should not take direct dependencies o\", \"n the infrastructure, which means that an important rule is that your domain model entity classes sh\", \"ould be POCOs.\\n\\nDomain entities should not have any direct dependency (like deriving from a base cla\", \"ss) on any data access infrastructure framework like Entity Framework or NHibernate. Ideally, your d\", \"omain entities should not derive from or implement any type defined in any infrastructure framework.\", \"\\n\\nMost modern ORM frameworks like Entity Framework Core allow this approach, so that your domain mod\", \"el classes are not coupled to the infrastructure. However, having POCO entities is not always possib\", \"le when using certain NoSQL databases and frameworks, like Actors and Reliable Collections in Azure \", \"Service Fabric.\\n\\nEven when it is important to follow the Persistence Ignorance principle for your Do\", \"main model, you should not ignore persistence concerns. It is still important to understand the phys\", \"ical data model and how it maps to your entity object model. Otherwise you can create impossible des\", \"igns.\\n\\nAlso, this aspect does not mean you can take a model designed for a relational database and d\", \"irectly move it to a NoSQL or document -oriented database. In some entity models, the model might fi\", \"t, but usually it does not. There are still constraints that your entity model must adhere to, based\", \" both on the storage technology and ORM technology.\\n\\n## The application layer\\n\\nMoving on to the appl\", \"ication layer, we can again cite Eric Evans\\u2019s book Domain Driven Design:\\n\\nApplication Layer: Defines\", \" the jobs the software is supposed to do and directs the expressive domain objects to work out probl\", \"ems. The tasks this layer is responsible for are meaningful to the business or necessary for interac\", \"tion with the application layers of other systems. This layer is kept thin. It does not contain busi\", \"ness rules or knowledge, but only coordinates tasks and delegates work to collaborations of domain o\", \"bjects in the next layer down. It does not have state reflecting the business situation, but it can \", \"have state that reflects the progress of a task for the user or the program.\\n\\nA microservice's appli\", \"cation layer in .NET is commonly coded as an ASP.NET Core Web API project. The project implements th\", \"e microservice's interaction, remote network access, and the external Web APIs used from the UI or c\", \"lient apps. It includes queries if using a CQRS approach, commands accepted by the microservice, and\", \" even the event-driven communication between microservices (integration events). The ASP.NET Core We\", \"b API that represents the application layer must not contain business rules or domain knowledge (esp\", \"ecially domain rules for transactions or updates); these should be owned by the domain model class l\", \"ibrary. The application layer must only coordinate tasks and must not hold or define any domain stat\", \"e (domain model). It delegates the execution of business rules to the domain model classes themselve\", \"s (aggregate roots and domain entities), which will ultimately update the data within those domain e\", \"ntities.\\n\\nDependencies between Layers in a Domain-Driven Design service\\n\\nMicroservice\\n\\nApplication\\n\\n\", \"Layer\\n\\nDepends on the Domain-Model Layer so it can:\\n\\n\\u2022 Use entity objects\\n\\n\\u2022 Use Repository Interfac\", \"es/Contracts\\n\\nBasically, the application logic is where you implement all use cases that depend on a\", \" given front end. For example, the implementation related to a Web API service. Depends on the Infra\", \"structure Layer (thru DI) so it can:\\n\\nIdeally, it must NOT take dependency on any other layer\\n\\nThe g\", \"oal is that the domain logic in the domain model layer, its invariants, the data model, and related \", \"business rules must be completely independent from the presentation and application layers. Most of \", \"all, the domain model layer must not directly depend on any infrastructure framework.\\n\\nInfrastructur\", \"e\\n\\nLayer\\n\\n## The infrastructure layer \\u00b7 Use entity objects.\\n\\nThe infrastructure layer is how the dat\", \"a that is initially held in domain entities (in memory) is persisted in databases or another persist\", \"ent store. An example is using Entity Framework Core code to implement the Repository pattern classe\", \"s that use a DBContext to persist data in a relational database.\\n\\nIn accordance with the previously \", \"mentioned Persistence Ignorance and Infrastructure Ignorance principles, the infrastructure layer mu\", \"st not \\\"contaminate\\\" the domain model layer. You must keep the domain model entity classes agnostic \", \"from the infrastructure that you use to persist data (EF or any other framework) by not taking hard \", \"dependencies on frameworks. Your domain model layer class library should have only your domain code,\", \" just POCO entity classes implementing the heart of your software and completely decoupled from infr\", \"astructure technologies.\\n\\nThus, your layers or class libraries and projects should ultimately depend\", \" on your domain model layer (library), not vice versa, as shown in Figure 7-7.\\n\\nFigure 7 -7. Depende\", \"ncies between layers in DDD\\n\\n<!-- image -->\\n\\nDependencies in a DDD Service, the Application layer de\", \"pends on Domain and Infrastructure, and Infrastructure depends on Domain, but Domain doesn't depend \", \"on any layer. This layer design should be independent for each microservice. As noted earlier, you c\", \"an implement the most complex microservices following DDD patterns, while implementing simpler data-\", \"driven microservices (simple CRUD in a single layer) in a simpler way.\\n\\n\\u2022 Depends on the Domain-Mode\", \"l Layer so it can:\\n\\n\\u2022\\n\\n## Additional resources\\n\\n- DevIQ. Persistence Ignorance principle https://dev\", \"iq.com/persistence-ignorance/\\n- Oren Eini. Infrastructure Ignorance https://ayende.com/blog/3137/inf\", \"rastructure-ignorance\\n- Angel Lopez. Layered Architecture In Domain-Driven Design https://ajlopez.wo\", \"rdpress.com/2008/09/12/layered-architecture-in-domain-driven-design/\\n\\n## Design a microservice domai\", \"n model\\n\\nDefine one rich domain model for each business microservice or Bounded Context.\\n\\nYour goal \", \"is to create a single cohesive domain model for each business microservice or Bounded Context (BC). \", \"Keep in mind, however, that a BC or business microservice could sometimes be composed of several phy\", \"sical services that share a single domain model. The domain model must capture the rules, behavior, \", \"business language, and constraints of the single Bounded Context or business microservice that it re\", \"presents.\\n\\n## The Domain Entity pattern\\n\\nEntities represent domain objects and are primarily defined\", \" by their identity, continuity, and persistence over time, and not only by the attributes that compr\", \"ise them. As Eric Evans says, \\\"an object primarily defined by its identity is called an Entity.\\\" Ent\", \"ities are very important in the domain model, since they are the base for a model. Therefore, you sh\", \"ould identify and design them carefully.\\n\\nAn entity\\u2019s identity can cross multiple microservices or B\", \"ounded Contexts.\\n\\nThe same identity (that is, the same Id value, although perhaps not the same domai\", \"n entity) can be modeled across multiple Bounded Contexts or microservices. However, that does not i\", \"mply that the same entity, with the same attributes and logic would be implemented in multiple Bound\", \"ed Contexts. Instead, entities in each Bounded Context limit their attributes and behaviors to those\", \" required in that Bounded Context's domain.\\n\\nFor instance, the buyer entity might have most of a per\", \"son's attributes that are defined in the user entity in the profile or identity microservice, includ\", \"ing the identity. But the buyer entity in the ordering microservice might have fewer attributes, bec\", \"ause only certain buyer data is related to the order process. The context of each microservice or Bo\", \"unded Context impacts its domain model.\\n\\nDomain entities must implement behavior in addition to impl\", \"ementing data attributes.\\n\\nA domain entity in DDD must implement the domain logic or behavior relate\", \"d to the entity data (the object accessed in memory). For example, as part of an order entity class \", \"you must have business logic and operations implemented as methods for tasks such as adding an order\", \" item, data validation, and total calculation. The entity's methods take care of the invariants and \", \"rules of the entity instead of having those rules spread across the application layer.\\n\\nDomain Entit\", \"y pattern\\n\\nOrder entity class\\n\\nAttributes\\n\\nID\\n\\nFirstName\\n\\nLastName\\n\\nAddress\\n\\nOrderltems (List)\\n\\nFigu\", \"re 7-8 shows a domain entity that implements not only data attributes but operations or methods with\", \" related domain logic.\\n\\nMethods\\n\\nOrder constructor\\n\\nAddOrderltem(item)\\n\\nSetAddress(address)\\n\\n## Enti\", \"ty's\\n\\nBehavior\\n\\nFigure 7 -8. Example of a domain entity design implementing data plus behavior\\n\\n<!--\", \" image -->\\n\\nA domain model entity implements behaviors through methods, that is, it's not an \\\"anemic\", \"\\\" model. Of course, sometimes you can have entities that do not implement any logic as part of the e\", \"ntity class. This can happen in child entities within an aggregate if the child entity does not have\", \" any special logic because most of the logic is defined in the aggregate root. If you have a complex\", \" microservice that has logic implemented in the service classes instead of in the domain entities, y\", \"ou could be falling into the anemic domain model, explained in the following section.\\n\\n## Rich domai\", \"n model versus anemic domain model\\n\\nIn his post AnemicDomainModel, Martin Fowler describes an anemic\", \" domain model this way:\\n\\nThe basic symptom of an Anemic Domain Model is that at first blush it looks\", \" like the real thing. There are objects, many named after the nouns in the domain space, and these o\", \"bjects are connected with the rich relationships and structure that true domain models have. The cat\", \"ch comes when you look at the behavior, and you realize that there is hardly any behavior on these o\", \"bjects, making them little more than bags of getters and setters.\\n\\nOf course, when you use an anemic\", \" domain model, those data models will be used from a set of service objects (traditionally named the\", \" business layer) which capture all the domain or business logic. The business layer sits on top of t\", \"he data model and uses the data model just as data.\\n\\nThe anemic domain model is just a procedural st\", \"yle design. Anemic entity objects are not real objects because they lack behavior (methods). They on\", \"ly hold data properties and thus it is not objectoriented design. By putting all the behavior out in\", \"to service objects (the business layer), you essentially end up with spaghetti code or transaction s\", \"cripts, and therefore you lose the advantages that a domain model provides.\\n\\nRegardless, if your mic\", \"roservice or Bounded Context is very simple (a CRUD service), the anemic domain model in the form of\", \" entity objects with just data properties might be good enough, and it might not be worth implementi\", \"ng more complex DDD patterns. In that case, it will be simply a persistence model, because you have \", \"intentionally created an entity with only data for CRUD purposes.\\n\\nThat is why microservices archite\", \"ctures are perfect for a multi-architectural approach depending on each Bounded Context. For instanc\", \"e, in eShopOnContainers, the ordering microservice implements DDD patterns, but the catalog microser\", \"vice, which is a simple CRUD service, does not.\\n\\nSome people say that the anemic domain model is an \", \"anti-pattern. It really depends on what you are implementing. If the microservice you are creating i\", \"s simple enough (for example, a CRUD service), following the anemic domain model it is not an anti-p\", \"attern. However, if you need to tackle the complexity of a microservice's domain that has a lot of e\", \"ver-changing business rules, the anemic domain model might be an anti-pattern for that microservice \", \"or Bounded Context. In that case, designing it as a rich model with entities containing data plus be\", \"havior as well as implementing additional DDD patterns (aggregates, value objects, etc.) might have \", \"huge benefits for the long-term success of such a microservice.\\n\\n## Additional resources\\n\\n- DevIQ. D\", \"omain Entity https://deviq.com/entity/\\n- Martin Fowler. The Domain Model https://martinfowler.com/ea\", \"aCatalog/domainModel.html\\n- Martin Fowler. The Anemic Domain Model https://martinfowler.com/bliki/An\", \"emicDomainModel.html\\n\\n## The Value Object pattern\\n\\nAs Eric Evans has noted, \\\"Many objects do not hav\", \"e conceptual identity. These objects describe certain characteristics of a thing.\\\"\\n\\nAn entity requir\", \"es an identity, but there are many objects in a system that do not, like the Value Object pattern. A\", \" value object is an object with no conceptual identity that describes a domain aspect. These are obj\", \"ects that you instantiate to represent design elements that only concern you temporarily. You care a\", \"bout what they are, not who they are. Examples include numbers and strings, but can also be higher-l\", \"evel concepts like groups of attributes.\\n\\nSomething that is an entity in a microservice might not be\", \" an entity in another microservice, because in the second case, the Bounded Context might have a dif\", \"ferent meaning. For example, an address in an e-commerce application might not have an identity at a\", \"ll, since it might only represent a group of attributes of the customer's profile for a person or co\", \"mpany. In this case, the address should be classified as a value object. However, in an application \", \"for an electric power utility company, the customer address could be important for the business doma\", \"in. Therefore, the address must have an identity so the billing system can be directly linked to the\", \" address. In that case, an address should be classified as a domain entity.\\n\\nA person with a name an\", \"d surname is usually an entity because a person has identity, even if the name and surname coincide \", \"with another set of values, such as if those names also refer to a different person.\\n\\nValue objects \", \"are hard to manage in relational databases and ORMs like Entity Framework (EF), whereas in document \", \"-oriented databases they are easier to implement and use.\\n\\nEF Core 2.0 and later versions include th\", \"e Owned Entities feature that makes it easier to handle value objects, as we'll see in detail later \", \"on.\\n\\n## Additional resources\\n\\n- Martin Fowler. Value Object pattern https://martinfowler.com/bliki/V\", \"alueObject.html\\n- Value Object https://deviq.com/value-object/\\n- Value Objects in Test-Driven Develo\", \"pment https://leanpub.com/tdd-ebook/read#leanpub-auto-value-objects\\n- Eric Evans. Domain -Driven Des\", \"ign: Tackling Complexity in the Heart of Software. (Book; includes a discussion of value objects) ht\", \"tps://www.amazon.com/Domain-Driven-Design-Tackling-ComplexitySoftware/dp/0321125215/\\n\\n## The Aggrega\", \"te pattern\\n\\nA domain model contains clusters of different data entities and processes that can contr\", \"ol a significant area of functionality, such as order fulfillment or inventory. A more fine-grained \", \"DDD unit is the aggregate, which describes a cluster or group of entities and behaviors that can be \", \"treated as a cohesive unit.\\n\\nYou usually define an aggregate based on the transactions that you need\", \". A classic example is an order that also contains a list of order items. An order item will usually\", \" be an entity. But it will be a child entity within the order aggregate, which will also contain the\", \" order entity as its root entity, typically called an aggregate root.\\n\\nIdentifying aggregates can be\", \" hard. An aggregate is a group of objects that must be consistent together, but you cannot just pick\", \" a group of objects and label them an aggregate. You must start with a domain concept and think abou\", \"t the entities that are used in the most common transactions related to that concept. Those entities\", \" that need to be transactionally consistent are what forms an aggregate. Thinking about transaction \", \"operations is probably the best way to identify aggregates.\\n\\n## The Aggregate Root or Root Entity pa\", \"ttern\\n\\nAn aggregate is composed of at least one entity: the aggregate root, also called root entity \", \"or primary entity. Additionally, it can have multiple child entities and value objects, with all ent\", \"ities and objects working together to implement required behavior and transactions.\\n\\nThe purpose of \", \"an aggregate root is to ensure the consistency of the aggregate; it should be the only entry point f\", \"or updates to the aggregate through methods or operations in the aggregate root class. You should ma\", \"ke changes to entities within the aggregate only via the aggregate root. It is the aggregate's consi\", \"stency guardian, considering all the invariants and consistency rules you might need to comply with \", \"in your aggregate. If you change a child entity or value object independently, the\\n\\nAggregate patter\", \"n\\n\\nBuyer Aggregate (One entity)\\n\\nOrder Aggregate (Multiple entities and Value-Object)\\n\\naggregate roo\", \"t cannot ensure that the aggregate is in a valid state. It would be like a table with a loose leg. M\", \"aintaining consistency is the main purpose of the aggregate root.\\n\\nAttributes\\n\\nID\\n\\nIn Figure 7-9, yo\", \"u can see sample aggregates like the buyer aggregate, which contains a single entity (the aggregate \", \"root Buyer). The order aggregate contains multiple entities and a value object.\\n\\nFullName\\n\\n[PaymentM\", \"ethods]\\n\\nMethods\\n\\nBuyer (params) constructor\\n\\n## [Orderitems] 1 Orderltem (Child Entity)\\n\\nMethods\\n\\nA\", \"ttributes\\n\\nID\\n\\nFigure 7 -9. Example of aggregates with multiple or single entities\\n\\n<!-- image -->\\n\\n\", \"A DDD domain model is composed from aggregates, an aggregate can have just one entity or more, and c\", \"an include value objects as well. Note that the Buyer aggregate could have additional child entities\", \", depending on your domain, as it does in the ordering microservice in the eShopOnContainers referen\", \"ce application. Figure 7-9 just illustrates a case in which the buyer has a single entity, as an exa\", \"mple of an aggregate that contains only an aggregate root.\\n\\nIn order to maintain separation of aggre\", \"gates and keep clear boundaries between them, it is a good practice in a DDD domain model to disallo\", \"w direct navigation between aggregates and only having the foreign key (FK) field, as implemented in\", \" the Ordering microservice domain model in eShopOnContainers. The Order entity only has a foreign ke\", \"y field for the buyer, but not an EF Core navigation property, as shown in the following code:\\n\\n```\\n\", \"public class Order : Entity , IAggregateRoot { private DateTime _orderDate; public Address Address {\", \" get; private set; } private int? _buyerId; // FK pointing to a different aggregate root public Orde\", \"rStatus OrderStatus { get; private set; } private readonly List<OrderItem> _orderItems; public IRead\", \"OnlyCollection<OrderItem> OrderItems => _orderItems; // ... Additional code }\\n```\\n\\n1\\n\\n&lt;-1\\n\\nIdenti\", \"fying and working with aggregates requires research and experience. For more information, see the fo\", \"llowing Additional resources list.\\n\\n## Additional resources\\n\\n- Vaughn Vernon. Effective Aggregate De\", \"sign - Part I: Modeling a Single Aggregate (from https://dddcommunity.org/) https://dddcommunity.org\", \"/wp-content/uploads/files/pdf\\\\_articles/Vernon\\\\_2011\\\\_1.pdf\\n- Vaughn Vernon. Effective Aggregate Des\", \"ign - Part II: Making Aggregates Work Together (from https://dddcommunity.org/) https://dddcommunity\", \".org/wp-content/uploads/files/pdf\\\\_articles/Vernon\\\\_2011\\\\_2.pdf\\n- Vaughn Vernon. Effective Aggregate\", \" Design - Part III: Gaining Insight Through Discovery (from https://dddcommunity.org/) https://dddco\", \"mmunity.org/wp-content/uploads/files/pdf\\\\_articles/Vernon\\\\_2011\\\\_3.pdf\\n- Sergey Grybniak. DDD Tactic\", \"al Design Patterns https://www.codeproject.com/Articles/1164363/Domain-Driven-Design-Tactical-Design\", \"Patterns -Part\\n- Chris Richardson. Developing Transactional Microservices Using Aggregates https://w\", \"ww.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson\\n- DevIQ. The Aggregate \", \"pattern https://deviq.com/aggregate-pattern/\\n\\n## Implement a microservice domain model with .NET\\n\\nIn\", \" the previous section, the fundamental design principles and patterns for designing a domain model w\", \"ere explained. Now it's time to explore possible ways to implement the domain model by using .NET (p\", \"lain C# code) and EF Core. Your domain model will be composed simply of your code. It will have just\", \" the EF Core model requirements, but not real dependencies on EF. You shouldn't have hard dependenci\", \"es or references to EF Core or any other ORM in your domain model.\\n\\n## Domain model structure in a c\", \"ustom .NET Standard Library\\n\\nThe folder organization used for the eShopOnContainers reference applic\", \"ation demonstrates the DDD model for the application. You might find that a different folder organiz\", \"ation more clearly communicates the design choices made for your application. As you can see in Figu\", \"re 7-10, in the ordering domain model there are two aggregates, the order aggregate and the buyer ag\", \"gregate. Each aggregate is a group of domain entities and value objects, although you could have an \", \"aggregate composed of a single domain entity (the aggregate root or root entity) as well.\\n\\nWeb API\\n\\n\", \"application layer project/library\\n\\nInfrastructure layer project/library\\n\\nrepos &amp; EF code\\n\\nOrderi\", \"ng Microservice/Container\\n\\nOrdering\\n\\n\\u2192 \\u2022 6\\u0424] Ordering.API\\n\\n4 a C# Ordering. Domain\\n\\n<!-- image -->\\n\\n\", \"-=----\\n\\nOrdering microservice\\n\\nFigure 7 -10. Domain model structure for the ordering microservice in\", \" eShopOnContainers\\n\\nAdditionally, the domain model layer includes the repository contracts (interfac\", \"es) that are the infrastructure requirements of your domain model. In other words, these interfaces \", \"express what repositories and the methods the infrastructure layer must implement. It's critical tha\", \"t the implementation of the repositories be placed outside of the domain model layer, in the infrast\", \"ructure layer library, so the domain model layer isn't \\\"contaminated\\\" by API or classes from infrast\", \"ructure technologies, like Entity Framework.\\n\\nYou can also see a SeedWork folder that contains custo\", \"m base classes that you can use as a base for your domain entities and value objects, so you don't h\", \"ave redundant code in each domain's object class.\\n\\n## Structure aggregates in a custom .NET Standard\", \" library\\n\\nAn aggregate refers to a cluster of domain objects grouped together to match transactional\", \" consistency. Those objects could be instances of entities (one of which is the aggregate root or ro\", \"ot entity) plus any additional value objects.\\n\\nTransactional consistency means that an aggregate is \", \"guaranteed to be consistent and up to date at the end of a business action. For example, the order a\", \"ggregate from the eShopOnContainers ordering microservice domain model is composed as shown in Figur\", \"e 7-11.\\n\\nOrder aggregate\\n\\n4 a\\u2264 OrderAggregate\\n\\n\\u2022 a C# Address.cs\\n\\n\\u2022 a C# Order.cs\\n\\n\\u2022 a C# Orderltem.\", \"cs\\n\\n\\u2022 a C# OrderStatus.cs\\n\\nFigure 7 -11. The order aggregate in Visual Studio solution\\n\\n<!-- image -\", \"->\\n\\nIf you open any of the files in an aggregate folder, you can see how it's marked as either a cus\", \"tom base class or interface, like entity or value object, as implemented in the SeedWork folder.\\n\\n##\", \" Implement domain entities as POCO classes\\n\\nYou implement a domain model in .NET by creating POCO cl\", \"asses that implement your domain entities. In the following example, the Order class is defined as a\", \"n entity and also as an aggregate root. Because the Order class derives from the Entity base class, \", \"it can reuse common code related to entities. Bear in mind that these base classes and interfaces ar\", \"e defined by you in the domain model project, so it is your code, not infrastructure code from an OR\", \"M like EF.\\n\\n```\\n// COMPATIBLE WITH ENTITY FRAMEWORK CORE 5.0 // Entity is a custom base class with t\", \"he ID public class Order : Entity , IAggregateRoot { private DateTime _orderDate; public Address Add\", \"ress { get; private set; } private int? _buyerId; public OrderStatus OrderStatus { get; private set;\", \" } private int _orderStatusId; private string _description; private int? _paymentMethodId; private r\", \"eadonly List<OrderItem> _orderItems; public IReadOnlyCollection<OrderItem> OrderItems => _orderItems\", \"; public Order(string userId , Address address , int cardTypeId , string cardNumber , string cardSec\", \"urityNumber , string cardHolderName , DateTime cardExpiration , int? buyerId = null , int? paymentMe\", \"thodId = null) { _orderItems = new List<OrderItem>(); _buyerId = buyerId; _paymentMethodId = payment\", \"MethodId; _orderStatusId = OrderStatus . Submitted . Id; _orderDate = DateTime . UtcNow;\\n```\\n\\n```\\nAd\", \"dress = address; // ...Additional code ... } public void AddOrderItem(int productId , string product\", \"Name , decimal unitPrice , decimal discount , string pictureUrl , int units = 1) { //... // Domain r\", \"ules/logic for adding the OrderItem to the order // ... var orderItem = new OrderItem(productId , pr\", \"oductName , unitPrice , discount , pictureUrl , units); _orderItems . Add(orderItem); } // ... // Ad\", \"ditional methods with domain rules/logic related to the Order aggregate // ... }\\n```\\n\\nIt's important\", \" to note that this is a domain entity implemented as a POCO class. It doesn't have any direct depend\", \"ency on Entity Framework Core or any other infrastructure framework. This implementation is as it sh\", \"ould be in DDD, just C# code implementing a domain model.\\n\\nIn addition, the class is decorated with \", \"an interface named IAggregateRoot. That interface is an empty interface, sometimes called a marker i\", \"nterface, that's used just to indicate that this entity class is also an aggregate root.\\n\\nA marker i\", \"nterface is sometimes considered as an anti -pattern; however, it's also a clean way to mark a class\", \", especially when that interface might be evolving. An attribute could be the other choice for the m\", \"arker, but it's quicker to see the base class (Entity) next to the IAggregate interface instead of p\", \"utting an Aggregate attribute marker above the class. It's a matter of preferences, in any case.\\n\\nHa\", \"ving an aggregate root means that most of the code related to consistency and business rules of the \", \"aggregate's entities should be implemented as methods in the Order aggregate root class (for example\", \", AddOrderItem when adding an OrderItem object to the aggregate). You should not create or update Or\", \"derItems objects independently or directly; the AggregateRoot class must keep control and consistenc\", \"y of any update operation against its child entities.\\n\\n## Encapsulate data in the Domain Entities\\n\\nA\", \" common problem in entity models is that they expose collection navigation properties as publicly ac\", \"cessible list types. This allows any collaborator developer to manipulate the contents of these coll\", \"ection types, which may bypass important business rules related to the collection, possibly leaving \", \"the object in an invalid state. The solution to this is to expose read-only access to related collec\", \"tions and explicitly provide methods that define ways in which clients can manipulate them.\\n\\nIn the \", \"previous code, note that many attributes are read-only or private and are only updatable by the clas\", \"s methods, so any update considers business domain invariants and logic specified within the class m\", \"ethods.\\n\\nFor example, following DDD patterns, you should not do the following from any command handl\", \"er method or application layer class (actually, it should be impossible for you to do so):\\n\\n```\\n// W\", \"RONG ACCORDING TO DDD PATTERNS \\u2013 CODE AT THE APPLICATION LAYER OR // COMMAND HANDLERS // Code in com\", \"mand handler methods or Web API controllers //... (WRONG) Some code with business logic out of the d\", \"omain classes ... OrderItem myNewOrderItem = new OrderItem(orderId , productId , productName , pictu\", \"reUrl , unitPrice , discount , units); //... (WRONG) Accessing the OrderItems collection directly fr\", \"om the application layer // or command handlers myOrder . OrderItems . Add(myNewOrderItem); //...\\n``\", \"`\\n\\nIn this case, the Add method is purely an operation to add data, with direct access to the OrderI\", \"tems collection. Therefore, most of the domain logic, rules, or validations related to that operatio\", \"n with the child entities will be spread across the application layer (command handlers and Web API \", \"controllers).\\n\\nIf you go around the aggregate root, the aggregate root cannot guarantee its invarian\", \"ts, its validity, or its consistency. Eventually you'll have spaghetti code or transactional script \", \"code.\\n\\nTo follow DDD patterns, entities must not have public setters in any entity property. Changes\", \" in an entity should be driven by explicit methods with explicit ubiquitous language about the chang\", \"e they're performing in the entity.\\n\\nFurthermore, collections within the entity (like the order item\", \"s) should be read-only properties (the AsReadOnly method explained later). You should be able to upd\", \"ate it only from within the aggregate root class methods or the child entity methods.\\n\\nAs you can se\", \"e in the code for the Order aggregate root, all setters should be private or at least readonly exter\", \"nally, so that any operation against the entity's data or its child entities has to be performed thr\", \"ough methods in the entity class. This maintains consistency in a controlled and object-oriented way\", \" instead of implementing transactional script code.\\n\\nThe following code snippet shows the proper way\", \" to code the task of adding an OrderItem object to the Order aggregate.\\n\\n```\\n// RIGHT ACCORDING TO D\", \"DD--CODE AT THE APPLICATION LAYER OR COMMAND HANDLERS // The code in command handlers or WebAPI cont\", \"rollers, related only to application stuff // There is NO code here related to OrderItem object's bu\", \"siness logic myOrder . AddOrderItem(productId , productName , pictureUrl , unitPrice , discount , un\", \"its); // The code related to OrderItem params validations or domain rules should // be WITHIN the Ad\", \"dOrderItem method. //...\\n```\\n\\nIn this snippet, most of the validations or logic related to the creat\", \"ion of an OrderItem object will be under the control of the Order aggregate root\\u2014in the AddOrderItem\", \" method\\u2014especially validations and logic related to other elements in the aggregate. For instance, y\", \"ou might get the same product item as the result of multiple calls to AddOrderItem. In that method, \", \"you could examine the product items and consolidate the same product items into a single OrderItem o\", \"bject with several units.\\n\\nAdditionally, if there are different discount amounts but the product ID \", \"is the same, you would likely apply the higher discount. This principle applies to any other domain \", \"logic for the OrderItem object.\\n\\nIn addition, the new OrderItem(params) operation will also be contr\", \"olled and performed by the AddOrderItem method from the Order aggregate root. Therefore, most of the\", \" logic or validations related to that operation (especially anything that impacts the consistency be\", \"tween other child entities) will be in a single place within the aggregate root. That is the ultimat\", \"e purpose of the aggregate root pattern.\\n\\nWhen you use Entity Framework Core 1.1 or later, a DDD ent\", \"ity can be better expressed because it allows mapping to fields in addition to properties. This is u\", \"seful when protecting collections of child entities or value objects. With this enhancement, you can\", \" use simple private fields instead of properties and you can implement any update to the field colle\", \"ction in public methods and provide read -only access through the AsReadOnly method.\\n\\nIn DDD, you wa\", \"nt to update the entity only through methods in the entity (or the constructor) in order to control \", \"any invariant and the consistency of the data, so properties are defined only with a get accessor. T\", \"he properties are backed by private fields. Private members can only be accessed from within the cla\", \"ss. However, there is one exception: EF Core needs to set these fields as well (so it can return the\", \" object with the proper values).\\n\\n## Map properties with only get accessors to the fields in the dat\", \"abase table\\n\\nMapping properties to database table columns is not a domain responsibility but part of\", \" the infrastructure and persistence layer. We mention this here just so you're aware of the new capa\", \"bilities in EF Core 1.1 or later related to how you can model entities. Additional details on this t\", \"opic are explained in the infrastructure and persistence section.\\n\\nWhen you use EF Core 1.0 or later\", \", within the DbContext you need to map the properties that are defined only with getters to the actu\", \"al fields in the database table. This is done with the HasField method of the PropertyBuilder class.\", \"\\n\\n## Map fields without properties\\n\\nWith the feature in EF Core 1.1 or later to map columns to field\", \"s, it's also possible to not use properties. Instead, you can just map columns from a table to field\", \"s. A common use case for this is private fields for an internal state that doesn't need to be access\", \"ed from outside the entity.\\n\\nFor example, in the preceding OrderAggregate code example, there are se\", \"veral private fields, like the \\\\_paymentMethodId field, that have no related property for either a s\", \"etter or getter. That field could also be calculated within the order's business logic and used from\", \" the order's methods, but it needs to be persisted in the database as well. So in EF Core (since v1.\", \"1), there's a way to map a field without a related property to a column in the database. This is als\", \"o explained in the Infrastructure layer section of this guide.\\n\\n## Additional resources\\n\\n- Vaughn Ve\", \"rnon. Modeling Aggregates with DDD and Entity Framework. Note that this is not Entity Framework Core\", \". https://kalele.io/blog-posts/modeling-aggregates-with-ddd-and-entity-framework/\\n- Julie Lerman. Da\", \"ta Points -Coding for Domain-Driven Design: Tips for Data-Focused Devs\\n\\nhttps://learn.microsoft.com/\", \"archive/msdn-magazine/2013/august/data-points-coding-fordomain -driven -design-tips-for-data-focused\", \"-devs\\n\\n- Udi Dahan. How to create fully encapsulated Domain Models https://udidahan.com/2008/02/29/h\", \"ow-to-create-fully-encapsulated-domain-models/\\n- Steve Smith. What is the difference between a DTO a\", \"nd a POCO? https://ardalis.com/dtoor-poco/\\n\\n## Seedwork (reusable base classes and interfaces for yo\", \"ur domain model)\\n\\nThe solution folder contains a SeedWork folder. This folder contains custom base c\", \"lasses that you can use as a base for your domain entities and value objects. Use these base classes\", \" so you don't have redundant code in each domain's object class. The folder for these types of class\", \"es is called SeedWork and not something like Framework. It's called SeedWork because the folder cont\", \"ains just a small subset of reusable classes that cannot really be considered a framework. Seedwork \", \"is a term introduced by Michael Feathers and popularized by Martin Fowler but you could also name th\", \"at folder Common, SharedKernel, or similar.\\n\\nFigure 7-12 shows the classes that form the seedwork of\", \" the domain model in the ordering microservice. It has a few custom base classes like Entity, ValueO\", \"bject, and Enumeration, plus a few interfaces. These interfaces (IRepository and IUnitOfWork) inform\", \" the infrastructure layer about what needs to be implemented. Those interfaces are also used through\", \" Dependency Injection from the application layer.\\n\\nFigure 7 -12. A sample set of domain model \\\"seedw\", \"ork\\\" base classes and interfaces\\n\\n<!-- image -->\\n\\nThis is the type of copy and paste reuse that many\", \" developers share between projects, not a formal framework. You can have seedworks in any layer or l\", \"ibrary. However, if the set of classes and interfaces gets large enough, you might want to create a \", \"single class library.\\n\\n## The custom Entity base class\\n\\nThe following code is an example of an Entit\", \"y base class where you can place code that can be used the same way by any domain entity, such as th\", \"e entity ID, equality operators, a domain event list per entity, etc.\\n\\n```\\n// COMPATIBLE WITH ENTITY\", \" FRAMEWORK CORE (1.1 and later) public abstract class Entity { int? _requestedHashCode; int _Id; pri\", \"vate List<INotification> _domainEvents; public virtual int Id { get { return _Id; } protected set { \", \"_Id = value; } } public List<INotification> DomainEvents => _domainEvents; public void AddDomainEven\", \"t(INotification eventItem) { _domainEvents = _domainEvents ?? new List<INotification>(); _domainEven\", \"ts . Add(eventItem); } public void RemoveDomainEvent(INotification eventItem) { if (_domainEvents is\", \" null) return; _domainEvents . Remove(eventItem); } public bool IsTransient() { return this . Id == \", \"default(Int32); } public override bool Equals(object obj) { if (obj == null || !(obj is Entity)) ret\", \"urn false; if (Object . ReferenceEquals(this , obj)) return true; if (this . GetType() != obj . GetT\", \"ype()) return false; Entity item = (Entity)obj; if (item . IsTransient() || this . IsTransient()) re\", \"turn false; else return item . Id == this . Id; } public override int GetHashCode() { if (!IsTransie\", \"nt())\\n```\\n\\n```\\n{ if (!_requestedHashCode . HasValue) _requestedHashCode = this . Id . GetHashCode() \", \"^ 31; // XOR for random distribution. See: // https://learn.microsoft.com/archive/blogs/ericlippert/\", \"guidelines-and-rulesfor -gethashcode return _requestedHashCode . Value; } else return base . GetHash\", \"Code(); } public static bool operator ==(Entity left , Entity right) { if (Object . Equals(left , nu\", \"ll)) return (Object . Equals(right , null)); else return left . Equals(right); } public static bool \", \"operator !=(Entity left , Entity right) { return !(left == right); } }\\n```\\n\\nThe previous code using \", \"a domain event list per entity will be explained in the next sections when focusing on domain events\", \".\\n\\n## Repository contracts (interfaces) in the domain model layer\\n\\nRepository contracts are simply .\", \"NET interfaces that express the contract requirements of the repositories to be used for each aggreg\", \"ate.\\n\\nThe repositories themselves, with EF Core code or any other infrastructure dependencies and co\", \"de (Linq, SQL, etc.), must not be implemented within the domain model; the repositories should only \", \"implement the interfaces you define in the domain model.\\n\\nA pattern related to this practice (placin\", \"g the repository interfaces in the domain model layer) is the Separated Interface pattern. As explai\", \"ned by Martin Fowler, \\\"Use Separated Interface to define an interface in one package but implement i\", \"t in another. This way a client that needs the dependency to the interface can be completely unaware\", \" of the implementation.\\\"\\n\\nFollowing the Separated Interface pattern enables the application layer (i\", \"n this case, the Web API project for the microservice) to have a dependency on the requirements defi\", \"ned in the domain model, but not a direct dependency to the infrastructure/persistence layer. In add\", \"ition, you can use Dependency Injection to isolate the implementation, which is implemented in the i\", \"nfrastructure/ persistence layer using repositories.\\n\\nFor example, the following example with the IO\", \"rderRepository interface defines what operations the OrderRepository class will need to implement at\", \" the infrastructure layer. In the current implementation of the application, the code just needs to \", \"add or update orders to the database, since queries are split following the simplified CQRS approach\", \".\\n\\n```\\n// Defined at IOrderRepository.cs public interface IOrderRepository : IRepository<Order> {\\n``\", \"`\\n\\n```\\nOrder Add(Order order); void Update(Order order); Task<Order> GetAsync(int orderId); } // Def\", \"ined at IRepository.cs (Part of the Domain Seedwork) public interface IRepository<T> where T : IAggr\", \"egateRoot { IUnitOfWork UnitOfWork { get; } }\\n```\\n\\n## Additional resources\\n\\n- Martin Fowler. Separat\", \"ed Interface.\\n\\nhttps://www.martinfowler.com/eaaCatalog/separatedInterface.html\\n\\n## Implement value o\", \"bjects\\n\\nAs discussed in earlier sections about entities and aggregates, identity is fundamental for \", \"entities. However, there are many objects and data items in a system that do not require an identity\", \" and identity tracking, such as value objects.\\n\\nA value object can reference other entities. For exa\", \"mple, in an application that generates a route that describes how to get from one point to another, \", \"that route would be a value object. It would be a snapshot of points on a specific route, but this s\", \"uggested route would not have an identity, even though internally it might refer to entities like Ci\", \"ty, Road, etc.\\n\\nFigure 7-13 shows the Address value object within the Order aggregate.\\n\\nValue Object\", \" within Aggregate\\n\\n## Order Aggregate (Multiple entities and Value-Object)\\n\\nOrder (Aggregate Root)\\n\\n\", \"Attributes\\n\\nID\\n\\nOrderDate\\n\\n[BuyerID]\\n\\n[Address]\\n\\n[Orderitems]\\n\\n\\u2022.\\u2022\\n\\nMethods\\n\\nOrder (params) construc\", \"tor\\n\\nAddOrderltem(item)\\n\\nSetAddress(address)\\n\\nCalculate TotalO\\n\\nAddress (Value-Object)\\n\\n## 1 Attribu\", \"tes\\n\\nFigure 7 -13. Address value object within the Order aggregate\\n\\n<!-- image -->\\n\\nAs shown in Figu\", \"re 7-13, an entity is usually composed of multiple attributes. For example, the Order entity can be \", \"modeled as an entity with an identity and composed internally of a set of attributes such as OrderId\", \", OrderDate, OrderItems, etc. But the address, which is simply a complex-value composed of country/r\", \"egion, street, city, etc., and has no identity in this domain, must be modeled and treated as a valu\", \"e object.\\n\\n## Important characteristics of value objects\\n\\nThere are two main characteristics for val\", \"ue objects:\\n\\n- They have no identity.\\n- They are immutable.\\n\\nThe first characteristic was already di\", \"scussed. Immutability is an important requirement. The values of a value object must be immutable on\", \"ce the object is created. Therefore, when the object is\\n\\n1\\n\\nconstructed, you must provide the requir\", \"ed values, but you must not allow them to change during the object's lifetime.\\n\\nValue objects allow \", \"you to perform certain tricks for performance, thanks to their immutable nature. This is especially \", \"true in systems where there may be thousands of value object instances, many of which have the same \", \"values. Their immutable nature allows them to be reused; they can be interchangeable objects, since \", \"their values are the same and they have no identity. This type of optimization can sometimes make a \", \"difference between software that runs slowly and software with good performance. Of course, all thes\", \"e cases depend on the application environment and deployment context.\\n\\n## Value object implementatio\", \"n in C#\\n\\nIn terms of implementation, you can have a value object base class that has basic utility m\", \"ethods like equality based on the comparison between all the attributes (since a value object must n\", \"ot be based on identity) and other fundamental characteristics. The following example shows a value \", \"object base class used in the ordering microservice from eShopOnContainers.\\n\\n```\\npublic abstract cla\", \"ss ValueObject { protected static bool EqualOperator(ValueObject left , ValueObject right) { if (Ref\", \"erenceEquals(left , null) ^ ReferenceEquals(right , null)) { return false; } return ReferenceEquals(\", \"left , right) || left . Equals(right); } protected static bool NotEqualOperator(ValueObject left , V\", \"alueObject right) { return !(EqualOperator(left , right)); } protected abstract IEnumerable<object> \", \"GetEqualityComponents(); public override bool Equals(object obj) { if (obj == null || obj . GetType(\", \") != GetType()) { return false; } var other = (ValueObject)obj; return this . GetEqualityComponents(\", \").SequenceEqual(other . GetEqualityComponents()); } public override int GetHashCode() { return GetEq\", \"ualityComponents() . Select(x => x != null ? x . GetHashCode() : 0) . Aggregate((x , y) => x ^ y); }\", \" // Other utility methods }\\n```\\n\\nThe ValueObject is an abstract class type, but in this example, it \", \"doesn't overload the == and != operators. You could choose to do so, making comparisons delegate to \", \"the Equals override. For example, consider the following operator overloads to the ValueObject type:\", \"\\n\\n```\\npublic static bool operator ==(ValueObject one , ValueObject two) { return EqualOperator(one ,\", \" two); } public static bool operator !=(ValueObject one , ValueObject two) { return NotEqualOperator\", \"(one , two); } You can use this class when implementing your actual value object, as with the Addres\", \"s value object shown in the following example: public class Address : ValueObject { public String St\", \"reet { get; private set; } public String City { get; private set; } public String State { get; priva\", \"te set; } public String Country { get; private set; } public String ZipCode { get; private set; } pu\", \"blic Address() { } public Address(string street , string city , string state , string country , stri\", \"ng zipcode) { Street = street; City = city; State = state; Country = country; ZipCode = zipcode; } p\", \"rotected override IEnumerable<object> GetEqualityComponents() { // Using a yield return statement to\", \" return each element one at a time yield return Street; yield return City; yield return State; yield\", \" return Country; yield return ZipCode; } }\\n```\\n\\nThis value object implementation of Address has no i\", \"dentity, and therefore no ID field is defined for it, either in the Address class definition or the \", \"ValueObject class definition.\\n\\nHaving no ID field in a class to be used by Entity Framework (EF) was\", \" not possible until EF Core 2.0, which greatly helps to implement better value objects with no ID. T\", \"hat is precisely the explanation of the next section.\\n\\nIt could be argued that value objects, being \", \"immutable, should be read-only (that is, have get-only properties), and that's indeed true. However,\", \" value objects are usually serialized and deserialized to go\\n\\nthrough message queues, and being read\", \"-only stops the deserializer from assigning values, so you just leave them as private set, which is \", \"read-only enough to be practical.\\n\\n## Value object comparison semantics\\n\\nTwo instances of the Addres\", \"s type can be compared using all the following methods:\\n\\n```\\nvar one = new Address(\\\"1 Microsoft Way\\\"\", \" , \\\"Redmond\\\" , \\\"WA\\\" , \\\"US\\\" , \\\"98052\\\"); var two = new Address(\\\"1 Microsoft Way\\\" , \\\"Redmond\\\" , \\\"WA\\\" , \", \"\\\"US\\\" , \\\"98052\\\"); Console . WriteLine(EqualityComparer<Address>.Default . Equals(one , two)); // True\", \" Console . WriteLine(object . Equals(one , two)); // True Console . WriteLine(one . Equals(two)); //\", \" True Console . WriteLine(one == two); // True\\n```\\n\\nWhen all the values are the same, the comparison\", \"s are correctly evaluated as true. If you didn't choose to overload the == and != operators, then th\", \"e last comparison of one == two would evaluate as false. For more information, see Overload ValueObj\", \"ect equality operators .\\n\\n## How to persist value objects in the database with EF Core 2.0 and later\", \"\\n\\nYou just saw how to define a value object in your domain model. But how can you actually persist i\", \"t into the database using Entity Framework Core since it usually targets entities with identity?\\n\\n##\", \" Background and older approaches using EF Core 1.1\\n\\nAs background, a limitation when using EF Core 1\", \".0 and 1.1 was that you could not use complex types as defined in EF 6.x in the traditional .NET Fra\", \"mework. Therefore, if using EF Core 1.0 or 1.1, you needed to store your value object as an EF entit\", \"y with an ID field. Then, so it looked more like a value object with no identity, you could hide its\", \" ID so you make clear that the identity of a value object is not important in the domain model. You \", \"could hide that ID by using the ID as a shadow property . Since that configuration for hiding the ID\", \" in the model is set up in the EF infrastructure level, it would be kind of transparent for your dom\", \"ain model.\\n\\nIn the initial version of eShopOnContainers (.NET Core 1.1), the hidden ID needed by EF \", \"Core infrastructure was implemented in the following way in the DbContext level, using Fluent API at\", \" the infrastructure project. Therefore, the ID was hidden from the domain model point of view, but s\", \"till present in the infrastructure.\\n\\n```\\n// Old approach with EF Core 1.1 // Fluent API within the O\", \"rderingContext:DbContext in the Infrastructure project void ConfigureAddress(EntityTypeBuilder<Addre\", \"ss> addressConfiguration) { addressConfiguration . ToTable(\\\"address\\\" , DEFAULT_SCHEMA); addressConfi\", \"guration . Property<int>(\\\"Id\\\") // Id is a shadow property . IsRequired(); addressConfiguration . Has\", \"Key(\\\"Id\\\"); // Id is a shadow property }\\n```\\n\\nHowever, the persistence of that value object into the \", \"database was performed like a regular entity in a different table.\\n\\nWith EF Core 2.0 and later, ther\", \"e are new and better ways to persist value objects.\\n\\n## Persist value objects as owned entity types \", \"in EF Core 2.0 and later\\n\\nEven with some gaps between the canonical value object pattern in DDD and \", \"the owned entity type in EF Core, it's currently the best way to persist value objects with EF Core \", \"2.0 and later. You can see limitations at the end of this section.\\n\\nThe owned entity type feature wa\", \"s added to EF Core since version 2.0.\\n\\nAn owned entity type allows you to map types that do not have\", \" their own identity explicitly defined in the domain model and are used as properties, such as a val\", \"ue object, within any of your entities. An owned entity type shares the same CLR type with another e\", \"ntity type (that is, it's just a regular class). The entity containing the defining navigation is th\", \"e owner entity. When querying the owner, the owned types are included by default.\\n\\nJust by looking a\", \"t the domain model, an owned type looks like it doesn't have any identity. However, under the covers\", \", owned types do have the identity, but the owner navigation property is part of this identity.\\n\\nThe\", \" identity of instances of owned types is not completely their own. It consists of three components:\\n\", \"\\n- The identity of the owner\\n- The navigation property pointing to them\\n- In the case of collections\", \" of owned types, an independent component (supported in EF Core 2.2 and later).\\n\\nFor example, in the\", \" Ordering domain model at eShopOnContainers, as part of the Order entity, the Address value object i\", \"s implemented as an owned entity type within the owner entity, which is the Order entity. Address is\", \" a type with no identity property defined in the domain model. It is used as a property of the Order\", \" type to specify the shipping address for a particular order.\\n\\nBy convention, a shadow primary key i\", \"s created for the owned type and it will be mapped to the same table as the owner by using table spl\", \"itting. This allows to use owned types similarly to how complex types are used in EF6 in the traditi\", \"onal .NET Framework .\\n\\nIt is important to note that owned types are never discovered by convention i\", \"n EF Core, so you have to declare them explicitly.\\n\\nIn eShopOnContainers, in the OrderingContext.cs \", \"file, within the OnModelCreating() method, multiple infrastructure configurations are applied. One o\", \"f them is related to the Order entity.\\n\\n```\\n// Part of the OrderingContext.cs class at the Ordering.\", \"Infrastructure project // protected override void OnModelCreating(ModelBuilder modelBuilder) { model\", \"Builder . ApplyConfiguration(new ClientRequestEntityTypeConfiguration()); modelBuilder . ApplyConfig\", \"uration(new PaymentMethodEntityTypeConfiguration()); modelBuilder . ApplyConfiguration(new OrderEnti\", \"tyTypeConfiguration()); modelBuilder . ApplyConfiguration(new OrderItemEntityTypeConfiguration()); /\", \"/...Additional type configurations }\\n```\\n\\nIn the following code, the persistence infrastructure is d\", \"efined for the Order entity:\\n\\n```\\n// Part of the OrderEntityTypeConfiguration.cs class // public voi\", \"d Configure(EntityTypeBuilder<Order> orderConfiguration) { orderConfiguration . ToTable(\\\"orders\\\" , O\", \"rderingContext . DEFAULT_SCHEMA); orderConfiguration . HasKey(o => o . Id); orderConfiguration . Ign\", \"ore(b => b . DomainEvents); orderConfiguration . Property(o => o . Id) . ForSqlServerUseSequenceHiLo\", \"(\\\"orderseq\\\" , OrderingContext . DEFAULT_SCHEMA); //Address value object persisted as owned entity in\", \" EF Core 2.0 orderConfiguration . OwnsOne(o => o . Address); orderConfiguration . Property<DateTime>\", \"(\\\"OrderDate\\\").IsRequired(); //...Additional validations, constraints and code... //... }\\n```\\n\\nIn the\", \" previous code, the orderConfiguration.OwnsOne(o =&gt; o.Address) method specifies that the Address \", \"property is an owned entity of the Order type.\\n\\nBy default, EF Core conventions name the database co\", \"lumns for the properties of the owned entity type as EntityProperty\\\\_OwnedEntityProperty. Therefore,\", \" the internal properties of Address will appear in the Orders table with the names Address\\\\_Street, \", \"Address\\\\_City (and so on for State, Country, and ZipCode).\\n\\nYou can append the Property().HasColumnN\", \"ame() fluent method to rename those columns. In the case where Address is a public property, the map\", \"pings would be like the following:\\n\\n```\\norderConfiguration . OwnsOne(p => p . Address) . Property(p=\", \">p . Street).HasColumnName(\\\"ShippingStreet\\\"); orderConfiguration . OwnsOne(p => p . Address) . Prope\", \"rty(p=>p . City).HasColumnName(\\\"ShippingCity\\\");\\n```\\n\\nIt's possible to chain the OwnsOne method in a \", \"fluent mapping. In the following hypothetical example, OrderDetails owns BillingAddress and Shipping\", \"Address, which are both Address types. Then OrderDetails is owned by the Order type.\\n\\n```\\norderConfi\", \"guration . OwnsOne(p => p . OrderDetails , cb => { cb . OwnsOne(c => c . BillingAddress); cb . OwnsO\", \"ne(c => c . ShippingAddress); }); //... //... public class Order { public int Id { get; set; } publi\", \"c OrderDetails OrderDetails { get; set; } } public class OrderDetails { public Address BillingAddres\", \"s { get; set; }\\n```\\n\\n```\\npublic Address ShippingAddress { get; set; } } public class Address { publi\", \"c string Street { get; set; } public string City { get; set; } }\\n```\\n\\n## Additional details on owned\", \" entity types\\n\\n- Owned types are defined when you configure a navigation property to a particular ty\", \"pe using the OwnsOne fluent API.\\n- The definition of an owned type in our metadata model is a compos\", \"ite of: the owner type, the navigation property, and the CLR type of the owned type.\\n- The identity \", \"(key) of an owned type instance in our stack is a composite of the identity of the owner type and th\", \"e definition of the owned type.\\n\\n## Owned entities capabilities\\n\\n- Owned types can reference other e\", \"ntities, either owned (nested owned types) or non-owned (regular reference navigation properties to \", \"other entities).\\n- You can map the same CLR type as different owned types in the same owner entity t\", \"hrough separate navigation properties.\\n- Table splitting is set up by convention, but you can opt ou\", \"t by mapping the owned type to a different table using ToTable.\\n- Eager loading is performed automat\", \"ically on owned types, that is, there's no need to call .Include() on the query.\\n- Can be configured\", \" with attribute [Owned], using EF Core 2.1 and later.\\n- Can handle collections of owned types (using\", \" version 2.2 and later).\\n\\n## Owned entities limitations\\n\\n- You can't create a DbSet&lt;T&gt; of an o\", \"wned type (by design).\\n- You can't call ModelBuilder.Entity&lt;T&gt;() on owned types (currently by \", \"design).\\n- No support for optional (that is, nullable) owned types that are mapped with the owner in\", \" the same table (that is, using table splitting). This is because mapping is done for each property,\", \" there is no separate sentinel for the null complex value as a whole.\\n- No inheritance -mapping supp\", \"ort for owned types, but you should be able to map two leaf types of the same inheritance hierarchie\", \"s as different owned types. EF Core will not reason about the fact that they are part of the same hi\", \"erarchy.\\n\\n## Main differences with EF6\\u2019s complex types\\n\\n- Table splitting is optional, that is, they\", \" can optionally be mapped to a separate table and still be owned types.\\n\\n## Additional resources\\n\\n- \", \"Martin Fowler. ValueObject pattern https://martinfowler.com/bliki/ValueObject.html\\n- Eric Evans. Dom\", \"ain -Driven Design: Tackling Complexity in the Heart of Software. (Book; includes a discussion of va\", \"lue objects) https://www.amazon.com/Domain-Driven-Design-Tackling-ComplexitySoftware/dp/0321125215/\\n\", \"- Vaughn Vernon. Implementing Domain-Driven Design. (Book; includes a discussion of value objects)\\n-\", \" https://www.amazon.com/Implementing-Domain-Driven-Design-VaughnVernon/dp/0321834577/\\n- Owned Entity\", \" Types https://learn.microsoft.com/ef/core/modeling/owned-entities\\n- Shadow Properties https://learn\", \".microsoft.com/ef/core/modeling/shadow-properties\\n- Complex types and/or value objects. Discussion i\", \"n the EF Core GitHub repo (Issues tab) https://github.com/dotnet/efcore/issues/246\\n- ValueObject.cs.\", \" Base value object class in eShopOnContainers. https://github.com/dotnetarchitecture/eShopOnContaine\", \"rs/blob/dev/src/Services/Ordering/Ordering.Domain/SeedWor k/ValueObject.cs\\n- ValueObject.cs. Base va\", \"lue object class in CSharpFunctionalExtensions. https://github.com/vkhorikov/CSharpFunctionalExtensi\", \"ons/blob/master/CSharpFunctionalExte nsions/ValueObject/ValueObject.cs\\n- Address class. Sample value\", \" object class in eShopOnContainers. https://github.com/dotnetarchitecture/eShopOnContainers/blob/dev\", \"/src/Services/Ordering/Ordering.Domain/Aggregat esModel/OrderAggregate/Address.cs\\n\\nUse enumeration c\", \"lasses instead of enum types\\n\\nEnumerations (or enum types for short) are a thin language wrapper aro\", \"und an integral type. You might want to limit their use to when you are storing one value from a clo\", \"sed set of values. Classification based on sizes (small, medium, large) is a good example. Using enu\", \"ms for control flow or more robust abstractions can be a code smell. This type of usage leads to fra\", \"gile code with many control flow statements checking values of the enum.\\n\\nInstead, you can create En\", \"umeration classes that enable all the rich features of an object-oriented language.\\n\\nHowever, this i\", \"sn't a critical topic and in many cases, for simplicity, you can still use regular enum types if tha\", \"t's your preference. The use of enumeration classes is more related to business-related concepts.\\n\\n#\", \"# Implement an Enumeration base class\\n\\nThe ordering microservice in eShopOnContainers provides a sam\", \"ple Enumeration base class implementation, as shown in the following example:\\n\\n```\\npublic abstract c\", \"lass Enumeration : IComparable { public string Name { get; private set; } public int Id { get; priva\", \"te set; } protected Enumeration(int id , string name) => (Id , Name) = (id , name); public override \", \"string ToString() => Name; public static IEnumerable<T> GetAll<T>() where T : Enumeration => typeof(\", \"T).GetFields(BindingFlags . Public | BindingFlags . Static | BindingFlags . DeclaredOnly) . Select(f\", \" => f . GetValue(null)) . Cast<T>(); public override bool Equals(object obj) { if (obj is not Enumer\", \"ation otherValue) { return false; } var typeMatches = GetType().Equals(obj . GetType()); var valueMa\", \"tches = Id . Equals(otherValue . Id); return typeMatches && valueMatches; } public int CompareTo(obj\", \"ect other) => Id . CompareTo(((Enumeration)other).Id); // Other utility methods ... } You can use th\", \"is class as a type in any entity or value object, as for the following CardType : Enumeration class:\", \" public class CardType : Enumeration { public static CardType Amex = new(1 , nameof(Amex)); public s\", \"tatic CardType Visa = new(2 , nameof(Visa)); public static CardType MasterCard = new(3 , nameof(Mast\", \"erCard)); public CardType(int id , string name) : base(id , name) {\\n```\\n\\n}\\n\\n## Additional resources\\n\", \"\\n- Jimmy Bogard. Enumeration classes https://lostechies.com/jimmybogard/2008/08/12/enumeration-class\", \"es/\\n- Steve Smith. Enum Alternatives in C# https://ardalis.com/enum-alternatives-in-c\\n- Enumeration.\", \"cs. Base Enumeration class in eShopOnContainers https://github.com/dotnetarchitecture/eShopOnContain\", \"ers/blob/dev/src/Services/Ordering/Ordering.Domain/SeedWor k/Enumeration.cs\\n- CardType.cs. Sample En\", \"umeration class in eShopOnContainers. https://github.com/dotnetarchitecture/eShopOnContainers/blob/d\", \"ev/src/Services/Ordering/Ordering.Domain/Aggregat esModel/BuyerAggregate/CardType.cs\\n- SmartEnum. Ar\", \"dalis -Classes to help produce strongly typed smarter enums in .NET. https://www.nuget.org/packages/\", \"Ardalis.SmartEnum/\\n\\n## Design validations in the domain model layer\\n\\nIn DDD, validation rules can be\", \" thought as invariants. The main responsibility of an aggregate is to enforce invariants across stat\", \"e changes for all the entities within that aggregate.\\n\\nDomain entities should always be valid entiti\", \"es. There are a certain number of invariants for an object that should always be true. For example, \", \"an order item object always has to have a quantity that must be a positive integer, plus an article \", \"name and price. Therefore, invariants enforcement is the responsibility of the domain entities (espe\", \"cially of the aggregate root) and an entity object should not be able to exist without being valid. \", \"Invariant rules are simply expressed as contracts, and exceptions or notifications are raised when t\", \"hey are violated.\\n\\nThe reasoning behind this is that many bugs occur because objects are in a state \", \"they should never have been in.\\n\\nLet's propose we now have a SendUserCreationEmailService that takes\", \" a UserProfile \\u2026 how can we rationalize in that service that Name is not null? Do we check it again?\", \" Or more likely \\u2026 you just don't bother to check and \\\"hope for the best\\\"\\u2014you hope that someone bothe\", \"red to validate it before sending it to you. Of course, using TDD one of the first tests we should b\", \"e writing is that if I send a customer with a null name that it should raise an error. But once we s\", \"tart writing these kinds of tests over and over again we realize \\u2026 \\\"what if we never allowed name to\", \" become null? we wouldn't have all of these tests!\\\".\\n\\n}\\n\\n## Implement validations in the domain mode\", \"l layer\\n\\nValidations are usually implemented in domain entity constructors or in methods that can up\", \"date the entity. There are multiple ways to implement validations, such as verifying data and raisin\", \"g exceptions if the validation fails. There are also more advanced patterns such as using the Specif\", \"ication pattern for validations, and the Notification pattern to return a collection of errors inste\", \"ad of returning an exception for each validation as it occurs.\\n\\n## Validate conditions and throw exc\", \"eptions\\n\\nThe following code example shows the simplest approach to validation in a domain entity by \", \"raising an exception. In the references table at the end of this section you can see links to more a\", \"dvanced implementations based on the patterns we have discussed previously.\\n\\n```\\npublic void SetAddr\", \"ess(Address address) { _shippingAddress = address?? throw new ArgumentNullException(nameof(address))\", \"; } A better example would demonstrate the need to ensure that either the internal state did not cha\", \"nge, or that all the mutations for a method occurred. For example, the following implementation woul\", \"d leave the object in an invalid state: public void SetAddress(string line1 , string line2 , string \", \"city , string state , int zip) { _shippingAddress . line1 = line1 ?? throw new ... _shippingAddress \", \". line2 = line2; _shippingAddress . city = city ?? throw new ... _shippingAddress . state = (IsValid\", \"(state) ? state : throw new \\u2026 ); }\\n```\\n\\nIf the value of the state is invalid, the first address line\", \" and the city have already been changed. That might make the address invalid.\\n\\nA similar approach ca\", \"n be used in the entity's constructor, raising an exception to make sure that the entity is valid on\", \"ce it is created.\\n\\n## Use validation attributes in the model based on data annotations\\n\\nData annotat\", \"ions, like the Required or MaxLength attributes, can be used to configure EF Core database field pro\", \"perties, as explained in detail in the Table mapping section, but they no longer work for entity val\", \"idation in EF Core (neither does the IValidatableObject.Validate method), as they have done since EF\", \" 4.x in .NET Framework.\\n\\nData annotations and the IValidatableObject interface can still be used for\", \" model validation during model binding, prior to the controller's actions invocation as usual, but t\", \"hat model is meant to be a ViewModel or DTO and that's an MVC or API concern not a domain model conc\", \"ern.\\n\\nHaving made the conceptual difference clear, you can still use data annotations and IValidatab\", \"leObject in the entity class for validation, if your actions receive an entity class object paramete\", \"r, which is not recommended. In that case, validation will occur upon model binding, just before inv\", \"oking the action and you can check the controller's ModelState.IsValid property to check\\n\\nthe result\", \", but then again, it happens in the controller, not before persisting the entity object in the DbCon\", \"text, as it had done since EF 4.x.\\n\\nYou can still implement custom validation in the entity class us\", \"ing data annotations and the IValidatableObject.Validate method, by overriding the DbContext's SaveC\", \"hanges method.\\n\\nYou can see a sample implementation for validating IValidatableObject entities in th\", \"is comment on GitHub. That sample doesn't do attribute-based validations, but they should be easy to\", \" implement using reflection in the same override.\\n\\nHowever, from a DDD point of view, the domain mod\", \"el is best kept lean with the use of exceptions in your entity's behavior methods, or by implementin\", \"g the Specification and Notification patterns to enforce validation rules.\\n\\nIt can make sense to use\", \" data annotations at the application layer in ViewModel classes (instead of domain entities) that wi\", \"ll accept input, to allow for model validation within the UI layer. However, this should not be done\", \" at the exclusion of validation within the domain model.\\n\\n## Validate entities by implementing the S\", \"pecification pattern and the Notification pattern\\n\\nFinally, a more elaborate approach to implementin\", \"g validations in the domain model is by implementing the Specification pattern in conjunction with t\", \"he Notification pattern, as explained in some of the additional resources listed later.\\n\\nIt is worth\", \" mentioning that you can also use just one of those patterns\\u2014for example, validating manually with c\", \"ontrol statements, but using the Notification pattern to stack and return a list of validation error\", \"s.\\n\\n## Use deferred validation in the domain\\n\\nThere are various approaches to deal with deferred val\", \"idations in the domain. In his book Implementing Domain-Driven Design, Vaughn Vernon discusses these\", \" in the section on validation.\\n\\n## Two -step validation\\n\\nAlso consider two -step validation. Use fie\", \"ld-level validation on your command Data Transfer Objects (DTOs) and domain-level validation inside \", \"your entities. You can do this by returning a result object instead of exceptions in order to make i\", \"t easier to deal with the validation errors.\\n\\nUsing field validation with data annotations, for exam\", \"ple, you do not duplicate the validation definition. The execution, though, can be both server-side \", \"and client-side in the case of DTOs (commands and ViewModels, for instance).\\n\\n## Additional resource\", \"s\\n\\n- Rachel Appel. Introduction to model validation in ASP.NET Core MVC https://learn.microsoft.com/\", \"aspnet/core/mvc/models/validation\\n- Rick Anderson. Adding validation https://learn.microsoft.com/asp\", \"net/core/tutorials/first-mvc-app/validation\\n\\n- Martin Fowler. Replacing Throwing Exceptions with Not\", \"ification in Validations https://martinfowler.com/articles/replaceThrowWithNotification.html\\n- Speci\", \"fication and Notification Patterns https://www.codeproject.com/Tips/790758/Specification-and-Notific\", \"ation-Patterns\\n- Lev Gorodinski. Validation in Domain -Driven Design (DDD) http://gorodinski.com/blo\", \"g/2012/05/19/validation-in-domain-driven-design-ddd/\\n- Colin Jack. Domain Model Validation https://c\", \"olinjack.blogspot.com/2008/03/domain-model-validation.html\\n- Jimmy Bogard. Validation in a DDD world\", \" https://lostechies.com/jimmybogard/2009/02/15/validation-in-a-ddd-world/\\n\\n## Client -side validatio\", \"n (validation in the presentation layers)\\n\\nEven when the source of truth is the domain model and ult\", \"imately you must have validation at the domain model level, validation can still be handled at both \", \"the domain model level (server side) and the UI (client side).\\n\\nClient -side validation is a great c\", \"onvenience for users. It saves time they would otherwise spend waiting for a round trip to the serve\", \"r that might return validation errors. In business terms, even a few fractions of seconds multiplied\", \" hundreds of times each day adds up to a lot of time, expense, and frustration. Straightforward and \", \"immediate validation enables users to work more efficiently and produce better quality input and out\", \"put.\\n\\nJust as the view model and the domain model are different, view model validation and domain mo\", \"del validation might be similar but serve a different purpose. If you are concerned about DRY (the D\", \"on't Repeat Yourself principle), consider that in this case code reuse might also mean coupling, and\", \" in enterprise applications it is more important not to couple the server side to the client side th\", \"an to follow the DRY principle.\\n\\nEven when using client-side validation, you should always validate \", \"your commands or input DTOs in server code, because the server APIs are a possible attack vector. Us\", \"ually, doing both is your best bet because if you have a client application, from a UX perspective, \", \"it is best to be proactive and not allow the user to enter invalid information.\\n\\nTherefore, in clien\", \"t -side code you typically validate the ViewModels. You could also validate the client output DTOs o\", \"r commands before you send them to the services.\\n\\nThe implementation of client-side validation depen\", \"ds on what kind of client application you are building. It will be different if you are validating d\", \"ata in a web MVC web application with most of the code in .NET, a SPA web application with that vali\", \"dation being coded in JavaScript or TypeScript, or a mobile app coded with Xamarin and C#.\\n\\n## Addit\", \"ional resources\\n\\n## Validation in Xamarin mobile apps\\n\\n- Validate Text Input and Show Errors\\n\\nhttps:\", \"//developer.xamarin.com/recipes/ios/standard\\\\_controls/text\\\\_field/validate\\\\_input/\\n\\n- Validation Ca\", \"llback https://developer.xamarin.com/samples/xamarin-forms/XAML/ValidationCallback/\\n\\n## Validation i\", \"n ASP.NET Core apps\\n\\n- Rick Anderson. Adding validation https://learn.microsoft.com/aspnet/core/tuto\", \"rials/first-mvc-app/validation\\n\\n## Validation in SPA Web apps (Angular 2, TypeScript, JavaScript, Bl\", \"azor WebAssembly)\\n\\n- Form Validation https://angular.io/guide/form-validation\\n- Validation. Breeze d\", \"ocumentation. https://breeze.github.io/doc-js/validation.html\\n- ASP.NET Core Blazor forms and input \", \"components\\n\\nIn summary, these are the most important concepts in regards to validation:\\n\\n- Entities \", \"and aggregates should enforce their own consistency and be \\\"always valid\\\". Aggregate roots are respo\", \"nsible for multi-entity consistency within the same aggregate.\\n- If you think that an entity needs t\", \"o enter an invalid state, consider using a different object model \\u2014 for example, using a temporary D\", \"TO until you create the final domain entity.\\n- If you need to create several related objects, such a\", \"s an aggregate, and they are only valid once all of them have been created, consider using the Facto\", \"ry pattern.\\n- In most of the cases, having redundant validation in the client side is good, because \", \"the application can be proactive.\\n\\n## Domain events: Design and implementation\\n\\nUse domain events to\", \" explicitly implement side effects of changes within your domain. In other words, and using DDD term\", \"inology, use domain events to explicitly implement side effects across multiple aggregates. Optional\", \"ly, for better scalability and less impact in database locks, use eventual consistency between aggre\", \"gates within the same domain.\\n\\n## What is a domain event?\\n\\nAn event is something that has happened i\", \"n the past. A domain event is, something that happened in the domain that you want other parts of th\", \"e same domain (in-process) to be aware of. The notified parts usually react somehow to the events.\\n\\n\", \"An important benefit of domain events is that side effects can be expressed explicitly.\\n\\nFor example\", \", if you're just using Entity Framework and there has to be a reaction to some event, you would prob\", \"ably code whatever you need close to what triggers the event. So the rule gets coupled, implicitly, \", \"to the code, and you have to look into the code to, hopefully, realize the rule is implemented there\", \".\\n\\nOn the other hand, using domain events makes the concept explicit, because there's a DomainEvent \", \"and at least one DomainEventHandler involved.\\n\\nFor example, in the eShopOnContainers application, wh\", \"en an order is created, the user becomes a buyer, so an OrderStartedDomainEvent is raised and handle\", \"d in the\\n\\nValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler, so the underlying concept i\", \"s evident.\\n\\nIn short, domain events help you to express, explicitly, the domain rules, based in the \", \"ubiquitous language provided by the domain experts. Domain events also enable a better separation of\", \" concerns among classes within the same domain.\\n\\nIt's important to ensure that, just like a database\", \" transaction, either all the operations related to a domain event finish successfully or none of the\", \"m do.\\n\\nDomain events are similar to messaging-style events, with one important difference. With real\", \" messaging, message queuing, message brokers, or a service bus using AMQP, a message is always sent \", \"asynchronously and communicated across processes and machines. This is useful for integrating multip\", \"le Bounded Contexts, microservices, or even different applications. However, with domain events, you\", \" want to raise an event from the domain operation you're currently running, but you want any side ef\", \"fects to occur within the same domain.\\n\\nThe domain events and their side effects (the actions trigge\", \"red afterwards that are managed by event handlers) should occur almost immediately, usually in-proce\", \"ss, and within the same domain. Thus, domain events could be synchronous or asynchronous. Integratio\", \"n events, however, should always be asynchronous.\\n\\n## Domain events versus integration events\\n\\nSeman\", \"tically, domain and integration events are the same thing: notifications about something that just h\", \"appened. However, their implementation must be different. Domain events are just messages pushed to \", \"a domain event dispatcher, which could be implemented as an in-memory mediator based on an IoC conta\", \"iner or any other method.\\n\\nOn the other hand, the purpose of integration events is to propagate comm\", \"itted transactions and updates to additional subsystems, whether they are other microservices, Bound\", \"ed Contexts or even\\n\\nData\\n\\nBehavior\\n\\nOrder Aggregate external applications. Hence, they should occur\", \" only if the entity is successfully persisted, otherwise it's as if the entire operation never happe\", \"ned. Buyer Aggregate --\\n\\nAs mentioned before, integration events must be based on asynchronous commu\", \"nication between multiple microservices (other Bounded Contexts) or even external systems/applicatio\", \"ns.\\n\\nThus, the event bus interface needs some infrastructure that allows inter -process and distribu\", \"ted communication between potentially remote services. It can be based on a commercial service bus, \", \"queues, a shared database used as a mailbox, or any other distributed and ideally push based messagi\", \"ng system.\\n\\n## Domain events as a preferred way to trigger side effects across multiple aggregates w\", \"ithin the same domain\\n\\nIf executing a command related to one aggregate instance requires additional \", \"domain rules to be run on one or more additional aggregates, you should design and implement those s\", \"ide effects to be triggered by domain events. As shown in Figure 7-14, and as one of the most import\", \"ant use cases, a domain event should be used to propagate state changes across multiple aggregates w\", \"ithin the same domain model.\\n\\nFigure 7 -14. Domain events to enforce consistency between multiple ag\", \"gregates within the same domain\\n\\n<!-- image -->\\n\\nFigure 7-14 shows how consistency between aggregate\", \"s is achieved by domain events. When the user initiates an order, the Order Aggregate sends an Order\", \"Started domain event. The OrderStarted domain event is handled by the Buyer Aggregate to create a Bu\", \"yer object in the ordering microservice, based on the original user info from the identity microserv\", \"ice (with information provided in the CreateOrder command).\\n\\nAlternately, you can have the aggregate\", \" root subscribed for events raised by members of its aggregates (child entities). For instance, each\", \" OrderItem child entity can raise an event when the item price is higher than a specific amount, or \", \"when the product item amount is too high. The aggregate root can then receive those events and perfo\", \"rm a global calculation or aggregation.\\n\\nDomain Model\\n\\n(Ordering microservice)\\n\\nIt's important to un\", \"derstand that this event-based communication is not implemented directly within the aggregates; you \", \"need to implement domain event handlers.\\n\\nHandling the domain events is an application concern. The \", \"domain model layer should only focus on the domain logic\\u2014things that a domain expert would understan\", \"d, not application infrastructure like handlers and side -effect persistence actions using repositor\", \"ies. Therefore, the application layer level is where you should have domain event handlers triggerin\", \"g actions when a domain event is raised.\\n\\nDomain events can also be used to trigger any number of ap\", \"plication actions, and what is more important, must be open to increase that number in the future in\", \" a decoupled way. For instance, when the order is started, you might want to publish a domain event \", \"to propagate that info to other aggregates or even to raise application actions like notifications.\\n\", \"\\nThe key point is the open number of actions to be executed when a domain event occurs. Eventually, \", \"the actions and rules in the domain and application will grow. The complexity or number of sideeffec\", \"t actions when something happens will grow, but if your code were coupled with \\\"glue\\\" (that is, crea\", \"ting specific objects with new), then every time you needed to add a new action you would also need \", \"to change working and tested code.\\n\\nThis change could result in new bugs and this approach also goes\", \" against the Open/Closed principle from SOLID. Not only that, the original class that was orchestrat\", \"ing the operations would grow and grow, which goes against the Single Responsibility Principle (SRP)\", \" .\\n\\nOn the other hand, if you use domain events, you can create a fine-grained and decoupled impleme\", \"ntation by segregating responsibilities using this approach:\\n\\n1. Send a command (for example, Create\", \"Order).\\n2. Receive the command in a command handler.\\n3. \\u2013 Execute a single aggregate's transaction.\\n\", \"4. \\u2013 (Optional) Raise domain events for side effects (for example, OrderStartedDomainEvent).\\n3. Hand\", \"le domain events (within the current process) that will execute an open number of side effects in mu\", \"ltiple aggregates or application actions. For example:\\n6. \\u2013 Verify or create buyer and payment metho\", \"d.\\n7. \\u2013 Create and send a related integration event to the event bus to propagate states across micr\", \"oservices or trigger external actions like sending an email to the buyer.\\n8. \\u2013 Handle other side eff\", \"ects.\\n\\nAs shown in Figure 7-15, starting from the same domain event, you can handle multiple actions\", \" related to other aggregates in the domain or additional application actions you need to perform acr\", \"oss microservices connecting with integration events and the event bus.\\n\\nDomain\\n\\nAggregate (Order)\\n\\n\", \"Order (Aggregate Root)\\n\\nData\\n\\nBehavior\\n\\nApplication\\n\\nEvent Handler 1\\n\\nFigure 7 -15. Handling multipl\", \"e actions per domain\\n\\n<!-- image -->\\n\\nThere can be several handlers for the same domain event in the\", \" Application Layer, one handler can solve consistency between aggregates and another handler can pub\", \"lish an integration event, so other microservices can do something with it. The event handlers are t\", \"ypically in the application layer, because you'll use infrastructure objects like repositories or an\", \" application API for the microservice's behavior. In that sense, event handlers are similar to comma\", \"nd handlers, so both are part of the application layer. The important difference is that a command s\", \"hould be processed only once. A domain event could be processed zero or n times, because it can be r\", \"eceived by multiple receivers or event handlers with a different purpose for each handler.\\n\\nHaving a\", \"n open number of handlers per domain event allows you to add as many domain rules as needed, without\", \" affecting current code. For instance, implementing the following business rule might be as easy as \", \"adding a few event handlers (or even just one):\\n\\nWhen the total amount purchased by a customer in th\", \"e store, across any number of orders, exceeds $6,000, apply a 10% off discount to every new order an\", \"d notify the customer with an email about that discount for future orders.\\n\\n## Implement domain even\", \"ts\\n\\nIn C#, a domain event is simply a data-holding structure or class, like a DTO, with all the info\", \"rmation related to what just happened in the domain, as shown in the following example:\\n\\n```\\npublic \", \"class OrderStartedDomainEvent : INotification { public string UserId { get; } public string UserName\", \" { get; } public int CardTypeId { get; } public string CardNumber { get; } public string CardSecurit\", \"yNumber { get; }\\n```\\n\\n```\\npublic string CardHolderName { get; } public DateTime CardExpiration { get\", \"; } public Order Order { get; } public OrderStartedDomainEvent(Order order , string userId , string \", \"userName , int cardTypeId , string cardNumber , string cardSecurityNumber , string cardHolderName , \", \"DateTime cardExpiration) { Order = order; UserId = userId; UserName = userName; CardTypeId = cardTyp\", \"eId; CardNumber = cardNumber; CardSecurityNumber = cardSecurityNumber; CardHolderName = cardHolderNa\", \"me; CardExpiration = cardExpiration; } }\\n```\\n\\nThis is essentially a class that holds all the data re\", \"lated to the OrderStarted event.\\n\\nIn terms of the ubiquitous language of the domain, since an event \", \"is something that happened in the past, the class name of the event should be represented as a past-\", \"tense verb, like OrderStartedDomainEvent or OrderShippedDomainEvent. That's how the domain event is \", \"implemented in the ordering microservice in eShopOnContainers.\\n\\nAs noted earlier, an important chara\", \"cteristic of events is that since an event is something that happened in the past, it shouldn't chan\", \"ge. Therefore, it must be an immutable class. You can see in the previous code that the properties a\", \"re read-only. There's no way to update the object, you can only set values when you create it.\\n\\nIt's\", \" important to highlight here that if domain events were to be handled asynchronously, using a queue \", \"that required serializing and deserializing the event objects, the properties would have to be \\\"priv\", \"ate set\\\" instead of read-only, so the deserializer would be able to assign the values upon dequeuing\", \". This is not an issue in the Ordering microservice, as the domain event pub/sub is implemented sync\", \"hronously using MediatR.\\n\\n## Raise domain events\\n\\nThe next question is how to raise a domain event s\", \"o it reaches its related event handlers. You can use multiple approaches.\\n\\nUdi Dahan originally prop\", \"osed (for example, in several related posts, such as Domain Events \\u2013 Take 2) using a static class fo\", \"r managing and raising the events. This might include a static class named DomainEvents that would r\", \"aise domain events immediately when it's called, using syntax like DomainEvents.Raise(Event myEvent)\", \". Jimmy Bogard wrote a blog post (Strengthening your domain: Domain Events) that recommends a simila\", \"r approach.\\n\\nHowever, when the domain events class is static, it also dispatches to handlers immedia\", \"tely. This makes testing and debugging more difficult, because the event handlers with side-effects \", \"logic are executed immediately after the event is raised. When you're testing and debugging, you jus\", \"t want to focus on what is happening in the current aggregate classes; you don't want to suddenly be\", \"\\n\\nredirected to other event handlers for side effects related to other aggregates or application log\", \"ic. This is why other approaches have evolved, as explained in the next section.\\n\\n## The deferred ap\", \"proach to raise and dispatch events\\n\\nInstead of dispatching to a domain event handler immediately, a\", \" better approach is to add the domain events to a collection and then to dispatch those domain event\", \"s right before or right after committing the transaction (as with SaveChanges in EF). (This approach\", \" was described by Jimmy Bogard in this post A better domain events pattern.)\\n\\nDeciding if you send t\", \"he domain events right before or right after committing the transaction is important, since it deter\", \"mines whether you will include the side effects as part of the same transaction or in different tran\", \"sactions. In the latter case, you need to deal with eventual consistency across multiple aggregates.\", \" This topic is discussed in the next section.\\n\\nThe deferred approach is what eShopOnContainers uses.\", \" First, you add the events happening in your entities into a collection or list of events per entity\", \". That list should be part of the entity object, or even better, part of your base entity class, as \", \"shown in the following example of the Entity base class:\\n\\n```\\npublic abstract class Entity { //... p\", \"rivate List<INotification> _domainEvents; public List<INotification> DomainEvents => _domainEvents; \", \"public void AddDomainEvent(INotification eventItem) { _domainEvents = _domainEvents ?? new List<INot\", \"ification>(); _domainEvents . Add(eventItem); } public void RemoveDomainEvent(INotification eventIte\", \"m) { _domainEvents?.Remove(eventItem); } //... Additional code }\\n```\\n\\nWhen you want to raise an even\", \"t, you just add it to the event collection from code at any method of the aggregate-root entity.\\n\\nTh\", \"e following code, part of the Order aggregate-root at eShopOnContainers, shows an example:\\n\\n```\\nvar \", \"orderStartedDomainEvent = new OrderStartedDomainEvent(this , //Order object cardTypeId , cardNumber \", \", cardSecurityNumber , cardHolderName , cardExpiration); this . AddDomainEvent(orderStartedDomainEve\", \"nt);\\n```\\n\\nNotice that the only thing that the AddDomainEvent method is doing is adding an event to t\", \"he list. No event is dispatched yet, and no event handler is invoked yet.\\n\\nYou actually want to disp\", \"atch the events later on, when you commit the transaction to the database. If you are using Entity F\", \"ramework Core, that means in the SaveChanges method of your EF DbContext, as in the following code:\\n\", \"\\n```\\n// EF Core DbContext public class OrderingContext : DbContext , IUnitOfWork { // ... public asy\", \"nc Task<bool> SaveEntitiesAsync(CancellationToken cancellationToken = default(CancellationToken)) { \", \"// Dispatch Domain Events collection. // Choices: // A) Right BEFORE committing data (EF SaveChanges\", \") into the DB. This makes // a single transaction including side effects from the domain event // ha\", \"ndlers that are using the same DbContext with Scope lifetime // B) Right AFTER committing data (EF S\", \"aveChanges) into the DB. This makes // multiple transactions. You will need to handle eventual consi\", \"stency and // compensatory actions in case of failures. await _mediator . DispatchDomainEventsAsync(\", \"this); // After this line runs, all the changes (from the Command Handler and Domain // event handle\", \"rs) performed through the DbContext will be committed var result = await base . SaveChangesAsync(); \", \"} }\\n```\\n\\nWith this code, you dispatch the entity events to their respective event handlers.\\n\\nThe ove\", \"rall result is that you've decoupled the raising of a domain event (a simple add into a list in memo\", \"ry) from dispatching it to an event handler. In addition, depending on what kind of dispatcher you a\", \"re using, you could dispatch the events synchronously or asynchronously.\\n\\nBe aware that transactiona\", \"l boundaries come into significant play here. If your unit of work and transaction can span more tha\", \"n one aggregate (as when using EF Core and a relational database), this can work well. But if the tr\", \"ansaction cannot span aggregates, you have to implement additional steps to achieve consistency. Thi\", \"s is another reason why persistence ignorance is not universal; it depends on the storage system you\", \" use.\\n\\n## Single transaction across aggregates versus eventual consistency across aggregates\\n\\nThe qu\", \"estion of whether to perform a single transaction across aggregates versus relying on eventual consi\", \"stency across those aggregates is a controversial one. Many DDD authors like Eric Evans and Vaughn V\", \"ernon advocate the rule that one transaction = one aggregate and therefore argue for eventual consis\", \"tency across aggregates. For example, in his book Domain-Driven Design, Eric Evans says this:\\n\\nAny r\", \"ule that spans Aggregates will not be expected to be up-to-date at all times. Through event processi\", \"ng, batch processing, or other update mechanisms, other dependencies can be resolved within some spe\", \"cific time. (page 128)\\n\\nVaughn Vernon says the following in Effective Aggregate Design. Part II: Mak\", \"ing Aggregates Work Together:\\n\\nThus, if executing a command on one aggregate instance requires that \", \"additional business rules execute on one or more aggregates, use eventual consistency [\\u2026] There is a\", \" practical way to support eventual consistency in a DDD model. An aggregate method publishes a domai\", \"n event that is in time delivered to one or more asynchronous subscribers.\\n\\nThis rationale is based \", \"on embracing fine-grained transactions instead of transactions spanning many aggregates or entities.\", \" The idea is that in the second case, the number of database locks will be substantial in large-scal\", \"e applications with high scalability needs. Embracing the fact that highly scalable applications nee\", \"d not have instant transactional consistency between multiple aggregates helps with accepting the co\", \"ncept of eventual consistency. Atomic changes are often not needed by the business, and it is in any\", \" case the responsibility of the domain experts to say whether particular operations need atomic tran\", \"sactions or not. If an operation always needs an atomic transaction between multiple aggregates, you\", \" might ask whether your aggregate should be larger or wasn't correctly designed.\\n\\nHowever, other dev\", \"elopers and architects like Jimmy Bogard are okay with spanning a single transaction across several \", \"aggregates\\u2014but only when those additional aggregates are related to side effects for the same origin\", \"al command. For instance, in A better domain events pattern, Bogard says this:\\n\\nTypically, I want th\", \"e side effects of a domain event to occur within the same logical transaction, but not necessarily i\", \"n the same scope of raising the domain event [\\u2026] Just before we commit our transaction, we dispatch \", \"our events to their respective handlers.\\n\\nIf you dispatch the domain events right before committing \", \"the original transaction, it is because you want the side effects of those events to be included in \", \"the same transaction. For example, if the EF DbContext SaveChanges method fails, the transaction wil\", \"l roll back all changes, including the result of any side effect operations implemented by the relat\", \"ed domain event handlers. This is because the DbContext life scope is by default defined as \\\"scoped.\", \"\\\" Therefore, the DbContext object is shared across multiple repository objects being instantiated wi\", \"thin the same scope or object graph. This coincides with the HttpRequest scope when developing Web A\", \"PI or MVC apps.\\n\\nActually, both approaches (single atomic transaction and eventual consistency) can \", \"be right. It really depends on your domain or business requirements and what the domain experts tell\", \" you. It also depends on how scalable you need the service to be (more granular transactions have le\", \"ss impact with regard to database locks). And it depends on how much investment you're willing to ma\", \"ke in your code, since eventual consistency requires more complex code in order to detect possible i\", \"nconsistencies across aggregates and the need to implement compensatory actions. Consider that if yo\", \"u commit changes to the original aggregate and afterwards, when the events are being dispatched, if \", \"there's an issue and the event handlers cannot commit their side effects, you'll have inconsistencie\", \"s between aggregates.\\n\\nA way to allow compensatory actions would be to store the domain events in ad\", \"ditional database tables so they can be part of the original transaction. Afterwards, you could have\", \" a batch process that detects inconsistencies and runs compensatory actions by comparing the list of\", \" events with the current state of the aggregates. The compensatory actions are part of a complex top\", \"ic that will require deep analysis from your side, which includes discussing it with the business us\", \"er and domain experts.\\n\\nIn any case, you can choose the approach you need. But the initial deferred \", \"approach\\u2014raising the events before committing, so you use a single transaction\\u2014is the simplest appro\", \"ach when using EF Core and a relational database. It's easier to implement and valid in many busines\", \"s cases. It's also the approach used in the ordering microservice in eShopOnContainers.\\n\\nBut how do \", \"you actually dispatch those events to their respective event handlers? What's the \\\\_mediator object \", \"you see in the previous example? It has to do with the techniques and artifacts you use to map betwe\", \"en events and their event handlers.\\n\\n## The domain event dispatcher: mapping from events to event ha\", \"ndlers\\n\\nOnce you're able to dispatch or publish the events, you need some kind of artifact that will\", \" publish the event, so that every related handler can get it and process side effects based on that \", \"event.\\n\\nOne approach is a real messaging system or even an event bus, possibly based on a service bu\", \"s as opposed to in-memory events. However, for the first case, real messaging would be overkill for \", \"processing domain events, since you just need to process those events within the same process (that \", \"is, within the same domain and application layer).\\n\\n## How to subscribe to domain events\\n\\nWhen you u\", \"se MediatR, each event handler must use an event type that is provided on the generic parameter of t\", \"he INotificationHandler interface, as you can see in the following code:\\n\\n```\\npublic class ValidateO\", \"rAddBuyerAggregateWhenOrderStartedDomainEventHandler : INotificationHandler<OrderStartedDomainEvent>\", \"\\n```\\n\\nBased on the relationship between event and event handler, which can be considered the subscri\", \"ption, the MediatR artifact can discover all the event handlers for each event and trigger each one \", \"of those event handlers.\\n\\n## How to handle domain events\\n\\nFinally, the event handler usually impleme\", \"nts application layer code that uses infrastructure repositories to obtain the required additional a\", \"ggregates and to execute side-effect domain logic. The following domain event handler code at eShopO\", \"nContainers, shows an implementation example.\\n\\n```\\npublic class ValidateOrAddBuyerAggregateWhenOrder\", \"StartedDomainEventHandler : INotificationHandler<OrderStartedDomainEvent> { private readonly ILogger\", \" _logger; private readonly IBuyerRepository _buyerRepository; private readonly IOrderingIntegrationE\", \"ventService _orderingIntegrationEventService; public ValidateOrAddBuyerAggregateWhenOrderStartedDoma\", \"inEventHandler( ILogger<ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler> logger , IBuy\", \"erRepository buyerRepository , IOrderingIntegrationEventService orderingIntegrationEventService) { _\", \"buyerRepository = buyerRepository ?? throw new ArgumentNullException(nameof(buyerRepository)); _orde\", \"ringIntegrationEventService = orderingIntegrationEventService ?? throw new ArgumentNullException(nam\", \"eof(orderingIntegrationEventService));\\n```\\n\\n```\\n_logger = logger ?? throw new ArgumentNullException(\", \"nameof(logger)); } public async Task Handle( OrderStartedDomainEvent domainEvent , CancellationToken\", \" cancellationToken) { var cardTypeId = domainEvent . CardTypeId != 0 ? domainEvent . CardTypeId : 1;\", \" var buyer = await _buyerRepository . FindAsync(domainEvent . UserId); var buyerExisted = buyer is n\", \"ot null; if (!buyerExisted) { buyer = new Buyer(domainEvent . UserId , domainEvent . UserName); } bu\", \"yer . VerifyOrAddPaymentMethod( cardTypeId , $\\\"Payment Method on {DateTime.UtcNow}\\\" , domainEvent . \", \"CardNumber , domainEvent . CardSecurityNumber , domainEvent . CardHolderName , domainEvent . CardExp\", \"iration , domainEvent . Order . Id); var buyerUpdated = buyerExisted ? _buyerRepository . Update(buy\", \"er) : _buyerRepository . Add(buyer); await _buyerRepository . UnitOfWork . SaveEntitiesAsync(cancell\", \"ationToken); var integrationEvent = new OrderStatusChangedToSubmittedIntegrationEvent( domainEvent .\", \" Order . Id , domainEvent . Order . OrderStatus . Name , buyer . Name); await _orderingIntegrationEv\", \"entService . AddAndSaveEventAsync(integrationEvent); OrderingApiTrace . LogOrderBuyerAndPaymentValid\", \"atedOrUpdated( _logger , buyerUpdated . Id , domainEvent . Order . Id); } }\\n```\\n\\nThe previous domain\", \" event handler code is considered application layer code because it uses infrastructure repositories\", \", as explained in the next section on the infrastructure-persistence layer. Event handlers could als\", \"o use other infrastructure components.\\n\\n## Domain events can generate integration events to be publi\", \"shed outside of the microservice boundaries\\n\\nFinally, it's important to mention that you might somet\", \"imes want to propagate events across multiple microservices. That propagation is an integration even\", \"t, and it could be published through an event bus from any specific domain event handler.\\n\\n## Conclu\", \"sions on domain events\\n\\nAs stated, use domain events to explicitly implement side effects of changes\", \" within your domain. To use DDD terminology, use domain events to explicitly implement side effects \", \"across one or multiple aggregates. Additionally, and for better scalability and less impact on datab\", \"ase locks, use eventual consistency between aggregates within the same domain.\\n\\nThe reference app us\", \"es MediatR to propagate domain events synchronously across aggregates, within a single transaction. \", \"However, you could also use some AMQP implementation like RabbitMQ or Azure Service Bus to propagate\", \" domain events asynchronously, using eventual consistency but, as mentioned above, you have to consi\", \"der the need for compensatory actions in case of failures.\\n\\n## Additional resources\\n\\n- Greg Young. W\", \"hat is a Domain Event? https://cqrs.files.wordpress.com/2010/11/cqrs\\\\_documents.pdf#page=25\\n- Jan St\", \"enberg. Domain Events and Eventual Consistency https://www.infoq.com/news/2015/09/domain-events-cons\", \"istency\\n- Jimmy Bogard. A better domain events pattern https://lostechies.com/jimmybogard/2014/05/13\", \"/a-better-domain-events-pattern/\\n- Vaughn Vernon. Effective Aggregate Design Part II: Making Aggrega\", \"tes Work Together https://dddcommunity.org/wp-content/uploads/files/pdf\\\\_articles/Vernon\\\\_2011\\\\_2.pd\", \"f\\n- Jimmy Bogard. Strengthening your domain: Domain Events https://lostechies.com/jimmybogard/2010/0\", \"4/08/strengthening-your-domain-domainevents/\\n- Udi Dahan. How to create fully encapsulated Domain Mo\", \"dels https://udidahan.com/2008/02/29/how-to-create-fully-encapsulated-domain-models/\\n- Udi Dahan. Do\", \"main Events \\u2013 Take 2 https://udidahan.com/2008/08/25/domain-events-take-2/\\n- Udi Dahan. Domain Event\", \"s \\u2013 Salvation https://udidahan.com/2009/06/14/domain-events-salvation/\\n- Cesar de la Torre. Domain E\", \"vents vs. Integration Events in DDD and microservices architectures\\n\\nhttps://devblogs.microsoft.com/\", \"cesardelatorre/domain-events-vs-integration-events-indomain -driven -design-and-microservices-archit\", \"ectures/\\n\\n## Design the infrastructure persistence layer\\n\\nData persistence components provide access\", \" to the data hosted within the boundaries of a microservice (that is, a microservice's database). Th\", \"ey contain the actual implementation of components such as repositories and Unit of Work classes, li\", \"ke custom Entity Framework (EF) DbContext objects. EF DbContext implements both the Repository and t\", \"he Unit of Work patterns.\\n\\n## The Repository pattern\\n\\nThe Repository pattern is a Domain-Driven Desi\", \"gn pattern intended to keep persistence concerns outside of the system's domain model. One or more p\", \"ersistence abstractions - interfaces - are defined\\n\\nin the domain model, and these abstractions have\", \" implementations in the form of persistence-specific adapters defined elsewhere in the application.\\n\", \"\\nRepository implementations are classes that encapsulate the logic required to access data sources. \", \"They centralize common data access functionality, providing better maintainability and decoupling th\", \"e infrastructure or technology used to access databases from the domain model. If you use an Object-\", \"Relational Mapper (ORM) like Entity Framework, the code that must be implemented is simplified, than\", \"ks to LINQ and strong typing. This lets you focus on the data persistence logic rather than on data \", \"access plumbing.\\n\\nThe Repository pattern is a well-documented way of working with a data source. In \", \"the book Patterns of Enterprise Application Architecture, Martin Fowler describes a repository as fo\", \"llows:\\n\\nA repository performs the tasks of an intermediary between the domain model layers and data \", \"mapping, acting in a similar way to a set of domain objects in memory. Client objects declaratively \", \"build queries and send them to the repositories for answers. Conceptually, a repository encapsulates\", \" a set of objects stored in the database and operations that can be performed on them, providing a w\", \"ay that is closer to the persistence layer. Repositories, also, support the purpose of separating, c\", \"learly and in one direction, the dependency between the work domain and the data allocation or mappi\", \"ng.\\n\\n## Define one repository per aggregate\\n\\nFor each aggregate or aggregate root, you should create\", \" one repository class. You may be able to leverage C# Generics to reduce the total number concrete c\", \"lasses you need to maintain (as demonstrated later in this chapter). In a microservice based on Doma\", \"in-Driven Design (DDD) patterns, the only channel you should use to update the database should be th\", \"e repositories. This is because they have a one-to-one relationship with the aggregate root, which c\", \"ontrols the aggregate's invariants and transactional consistency. It's okay to query the database th\", \"rough other channels (as you can do following a CQRS approach), because queries don't change the sta\", \"te of the database. However, the transactional area (that is, the updates) must always be controlled\", \" by the repositories and the aggregate roots.\\n\\nBasically, a repository allows you to populate data i\", \"n memory that comes from the database in the form of the domain entities. Once the entities are in m\", \"emory, they can be changed and then persisted back to the database through transactions.\\n\\nAs noted e\", \"arlier, if you're using the CQS/CQRS architectural pattern, the initial queries are performed by sid\", \"e queries out of the domain model, performed by simple SQL statements using Dapper. This approach is\", \" much more flexible than repositories because you can query and join any tables you need, and these \", \"queries aren't restricted by rules from the aggregates. That data goes to the presentation layer or \", \"client app.\\n\\nIf the user makes changes, the data to be updated comes from the client app or presenta\", \"tion layer to the application layer (such as a Web API service). When you receive a command in a com\", \"mand handler, you use repositories to get the data you want to update from the database. You update \", \"it in memory with the data passed with the commands, and you then add or update the data (domain ent\", \"ities) in the database through a transaction.\\n\\nLayer\\n\\nOrder (Aggregate Root)\\n\\nData\\n\\nIt's important t\", \"o emphasize again that you should only define one repository for each aggregate root, as shown in Fi\", \"gure 7-17. To achieve the goal of the aggregate root to maintain transactional consistency between a\", \"ll the objects within the aggregate, you should never create a repository for each table in the data\", \"base.\\n\\nInfrastructure-\\n\\nPersistence\\n\\nLayer\\n\\nData\\n\\nTier\\n\\nFigure 7 -17. The relationship between repos\", \"itories, aggregates, and database tables\\n\\n<!-- image -->\\n\\nThe above diagram shows the relationships \", \"between Domain and Infrastructure layers: Buyer Aggregate depends on the IBuyerRepository and Order \", \"Aggregate depends on the IOrderRepository interfaces, these interfaces are implemented in the Infras\", \"tructure layer by the corresponding repositories that depend on UnitOfWork, also implemented there, \", \"that accesses the tables in the Data tier.\\n\\n## Enforce one aggregate root per repository\\n\\nIt can be \", \"valuable to implement your repository design in such a way that it enforces the rule that only aggre\", \"gate roots should have repositories. You can create a generic or base repository type that constrain\", \"s the type of entities it works with to ensure they have the IAggregateRoot marker interface.\\n\\nThus,\", \" each repository class implemented at the infrastructure layer implements its own contract or interf\", \"ace, as shown in the following code:\\n\\n```\\nnamespace Microsoft . eShopOnContainers . Services . Order\", \"ing . Infrastructure . Repositories { public class OrderRepository : IOrderRepository { // ...\\n```\\n\\n\", \"Buyer Aggregate\\n\\nBuyer (Aggregate Root)\\n\\nOrder Aggregate\\n\\nAddress (Value-Object)\\n\\nData\\n\\nBehavior\\n\\n``\", \"`\\n} }\\n```\\n\\nEach specific repository interface implements the generic IRepository interface:\\n\\n```\\npub\", \"lic interface IOrderRepository : IRepository<Order> { Order Add(Order order); // ... }\\n```\\n\\nHowever,\", \" a better way to have the code enforce the convention that each repository is related to a single ag\", \"gregate is to implement a generic repository type. That way, it's explicit that you're using a repos\", \"itory to target a specific aggregate. That can be easily done by implementing a generic IRepository \", \"base interface, as in the following code:\\n\\n```\\npublic interface IRepository<T> where T : IAggregateR\", \"oot { //.... }\\n```\\n\\n## The Repository pattern makes it easier to test your application logic\\n\\nThe Re\", \"pository pattern allows you to easily test your application with unit tests. Remember that unit test\", \"s only test your code, not infrastructure, so the repository abstractions make it easier to achieve \", \"that goal.\\n\\nAs noted in an earlier section, it's recommended that you define and place the repositor\", \"y interfaces in the domain model layer so the application layer, such as your Web API microservice, \", \"doesn't depend directly on the infrastructure layer where you've implemented the actual repository c\", \"lasses. By doing this and using Dependency Injection in the controllers of your Web API, you can imp\", \"lement mock repositories that return fake data instead of data from the database. This decoupled app\", \"roach allows you to create and run unit tests that focus the logic of your application without requi\", \"ring connectivity to the database.\\n\\nConnections to databases can fail and, more importantly, running\", \" hundreds of tests against a database is bad for two reasons. First, it can take a long time because\", \" of the large number of tests. Second, the database records might change and impact the results of y\", \"our tests, especially if your tests are running in parallel, so that they might not be consistent. U\", \"nit tests typically can run in parallel; integration tests may not support parallel execution depend\", \"ing on their implementation. Testing against the database isn't a unit test but an integration test.\", \" You should have many unit tests running fast, but fewer integration tests against the databases.\\n\\nI\", \"n terms of separation of concerns for unit tests, your logic operates on domain entities in memory. \", \"It assumes the repository class has delivered those. Once your logic modifies the domain entities, i\", \"t assumes the repository class will store them correctly. The important point here is to create unit\", \" tests against your domain model and its domain logic. Aggregate roots are the main consistency boun\", \"daries in DDD.\\n\\nThe repositories implemented in eShopOnContainers rely on EF Core's DbContext implem\", \"entation of the Repository and Unit of Work patterns using its change tracker, so they don't duplica\", \"te this functionality.\\n\\n## The difference between the Repository pattern and the legacy Data Access \", \"class (DAL class) pattern\\n\\nA typical DAL object directly performs data access and persistence operat\", \"ions against storage, often at the level of a single table and row. Simple CRUD operations implement\", \"ed with a set of DAL classes frequently do not support transactions (though this is not always the c\", \"ase). Most DAL class approaches make minimal use of abstractions, resulting in tight coupling betwee\", \"n application or Business Logic Layer (BLL) classes that call the DAL objects.\\n\\nWhen using repositor\", \"y, the implementation details of persistence are encapsulated away from the domain model. The use of\", \" an abstraction provides ease of extending behavior through patterns like Decorators or Proxies. For\", \" instance, cross-cutting concerns like caching, logging, and error handling can all be applied using\", \" these patterns rather than hard-coded in the data access code itself. It's also trivial to support \", \"multiple repository adapters which may be used in different environments, from local development to \", \"shared staging environments to production.\\n\\n## Implementing Unit of Work\\n\\nA unit of work refers to a\", \" single transaction that involves multiple insert, update, or delete operations. In simple terms, it\", \" means that for a specific user action, such as a registration on a website, all the insert, update,\", \" and delete operations are handled in a single transaction. This is more efficient than handling mul\", \"tiple database operations in a chattier way.\\n\\nThese multiple persistence operations are performed la\", \"ter in a single action when your code from the application layer commands it. The decision about app\", \"lying the in-memory changes to the actual database storage is typically based on the Unit of Work pa\", \"ttern. In EF, the Unit of Work pattern is implemented by a DbContext and is executed when a call is \", \"made to SaveChanges.\\n\\nIn many cases, this pattern or way of applying operations against the storage \", \"can increase application performance and reduce the possibility of inconsistencies. It also reduces \", \"transaction blocking in the database tables, because all the intended operations are committed as pa\", \"rt of one transaction. This is more efficient in comparison to executing many isolated operations ag\", \"ainst the database. Therefore, the selected ORM can optimize the execution against the database by g\", \"rouping several update actions within the same transaction, as opposed to many small and separate tr\", \"ansaction executions.\\n\\nThe Unit of Work pattern can be implemented with or without using the Reposit\", \"ory pattern.\\n\\n## Repositories shouldn\\u2019t be mandatory\\n\\nCustom repositories are useful for the reasons\", \" cited earlier, and that is the approach for the ordering microservice in eShopOnContainers. However\", \", it isn't an essential pattern to implement in a DDD design or even in general .NET development.\\n\\nF\", \"or instance, Jimmy Bogard, when providing direct feedback for this guide, said the following:\\n\\nThis'\", \"ll probably be my biggest feedback. I'm really not a fan of repositories, mainly because they hide t\", \"he important details of the underlying persistence mechanism. It's why I go for MediatR for commands\", \", too. I can use the full power of the persistence layer, and push all that domain behavior into my \", \"aggregate roots. I don't usually want to mock my repositories \\u2013 I still need to have that\\n\\nintegrati\", \"on test with the real thing. Going CQRS meant that we didn't really have a need for repositories any\", \" more.\\n\\nRepositories might be useful, but they are not critical for your DDD design in the way that \", \"the Aggregate pattern and a rich domain model are. Therefore, use the Repository pattern or not, as \", \"you see fit.\\n\\n## Additional resources\\n\\n## Repository pattern\\n\\n- Edward Hieatt and Rob Mee. Repositor\", \"y pattern. https://martinfowler.com/eaaCatalog/repository.html\\n- The Repository pattern\\n\\nhttps://lea\", \"rn.microsoft.com/previous-versions/msp-n-p/ff649690(v=pandp.10)\\n\\n- Eric Evans. Domain -Driven Design\", \": Tackling Complexity in the Heart of Software. (Book; includes a discussion of the Repository patte\", \"rn) https://www.amazon.com/Domain-Driven-Design-Tackling-ComplexitySoftware/dp/0321125215/\\n\\n## Unit \", \"of Work pattern\\n\\n- Martin Fowler. Unit of Work pattern. https://martinfowler.com/eaaCatalog/unitOfWo\", \"rk.html\\n- Implementing the Repository and Unit of Work Patterns in an ASP.NET MVC Application\\n\\nhttps\", \"://learn.microsoft.com/aspnet/mvc/overview/older-versions/getting-started-with-ef-5using-mvc-4/imple\", \"menting-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvcapplication\\n\\n## Implement the infr\", \"astructure persistence layer with Entity Framework Core\\n\\nWhen you use relational databases such as S\", \"QL Server, Oracle, or PostgreSQL, a recommended approach is to implement the persistence layer based\", \" on Entity Framework (EF). EF supports LINQ and provides strongly typed objects for your model, as w\", \"ell as simplified persistence into your database.\\n\\nEntity Framework has a long history as part of th\", \"e .NET Framework. When you use .NET, you should also use Entity Framework Core, which runs on Window\", \"s or Linux in the same way as .NET. EF Core is a complete rewrite of Entity Framework that's impleme\", \"nted with a much smaller footprint and important improvements in performance.\\n\\n## Introduction to En\", \"tity Framework Core\\n\\nEntity Framework (EF) Core is a lightweight, extensible, and cross-platform ver\", \"sion of the popular Entity Framework data access technology. It was introduced with .NET Core in mid\", \"-2016.\\n\\nSince an introduction to EF Core is already available in Microsoft documentation, here we si\", \"mply provide links to that information.\\n\\n## Additional resources\\n\\n- Entity Framework Core https://le\", \"arn.microsoft.com/ef/core/\\n- Getting started with ASP.NET Core and Entity Framework Core using Visua\", \"l Studio https://learn.microsoft.com/aspnet/core/data/ef-mvc/\\n- DbContext Class https://learn.micros\", \"oft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext\\n- Compare EF Core &amp; EF6.x https://lea\", \"rn.microsoft.com/ef/efcore-and-ef6/index\\n\\n## Infrastructure in Entity Framework Core from a DDD pers\", \"pective\\n\\nFrom a DDD point of view, an important capability of EF is the ability to use POCO domain e\", \"ntities, also known in EF terminology as POCO code-first entities. If you use POCO domain entities, \", \"your domain model classes are persistence-ignorant, following the Persistence Ignorance and the Infr\", \"astructure Ignorance principles.\\n\\nPer DDD patterns, you should encapsulate domain behavior and rules\", \" within the entity class itself, so it can control invariants, validations, and rules when accessing\", \" any collection. Therefore, it is not a good practice in DDD to allow public access to collections o\", \"f child entities or value objects. Instead, you want to expose methods that control how and when you\", \"r fields and property collections can be updated, and what behavior and actions should occur when th\", \"at happens.\\n\\nSince EF Core 1.1, to satisfy those DDD requirements, you can have plain fields in your\", \" entities instead of public properties. If you do not want an entity field to be externally accessib\", \"le, you can just create the attribute or field instead of a property. You can also use private prope\", \"rty setters.\\n\\nIn a similar way, you can now have read-only access to collections by using a public p\", \"roperty typed as IReadOnlyCollection&lt;T&gt;, which is backed by a private field member for the col\", \"lection (like a List&lt;T&gt;) in your entity that relies on EF for persistence. Previous versions o\", \"f Entity Framework required collection properties to support ICollection&lt;T&gt;, which meant that \", \"any developer using the parent entity class could add or remove items through its property collectio\", \"ns. That possibility would be against the recommended patterns in DDD.\\n\\nYou can use a private collec\", \"tion while exposing a read-only IReadOnlyCollection&lt;T&gt; object, as shown in the following code \", \"example:\\n\\n```\\npublic class Order : Entity {\\n```\\n\\n```\\n// Using private fields, allowed since EF Core \", \"1.1 private DateTime _orderDate; // Other fields ... private readonly List<OrderItem> _orderItems; p\", \"ublic IReadOnlyCollection<OrderItem> OrderItems => _orderItems; protected Order() { } public Order(i\", \"nt buyerId , int paymentMethodId , Address address) { // Initializations ... } public void AddOrderI\", \"tem(int productId , string productName , decimal unitPrice , decimal discount , string pictureUrl , \", \"int units = 1) { // Validation logic... var orderItem = new OrderItem(productId , productName , unit\", \"Price , discount , pictureUrl , units); _orderItems . Add(orderItem); } }\\n```\\n\\nThe OrderItems proper\", \"ty can only be accessed as read-only using IReadOnlyCollection&lt;OrderItem&gt;. This type is read-o\", \"nly so it is protected against regular external updates.\\n\\nEF Core provides a way to map the domain m\", \"odel to the physical database without \\\"contaminating\\\" the domain model. It is pure .NET POCO code, b\", \"ecause the mapping action is implemented in the persistence layer. In that mapping action, you need \", \"to configure the fields-to-database mapping. In the following example of the OnModelCreating method \", \"from OrderingContext and the OrderEntityTypeConfiguration class, the call to SetPropertyAccessMode t\", \"ells EF Core to access the OrderItems property through its field.\\n\\n```\\n// At OrderingContext.cs from\", \" eShopOnContainers protected override void OnModelCreating(ModelBuilder modelBuilder) { // ... model\", \"Builder . ApplyConfiguration(new OrderEntityTypeConfiguration()); // Other entities' configuration .\", \".. } // At OrderEntityTypeConfiguration.cs from eShopOnContainers class OrderEntityTypeConfiguration\", \" : IEntityTypeConfiguration<Order> { public void Configure(EntityTypeBuilder<Order> orderConfigurati\", \"on) { orderConfiguration . ToTable(\\\"orders\\\" , OrderingContext . DEFAULT_SCHEMA); // Other configurat\", \"ion var navigation = orderConfiguration . Metadata . FindNavigation(nameof(Order . OrderItems)); //E\", \"F access the OrderItem collection property through its backing field navigation . SetPropertyAccessM\", \"ode(PropertyAccessMode . Field);\\n```\\n\\n```\\n// Other configuration } }\\n```\\n\\nWhen you use fields instea\", \"d of properties, the OrderItem entity is persisted as if it had a List&lt;OrderItem&gt; property. Ho\", \"wever, it exposes a single accessor, the AddOrderItem method, for adding new items to the order. As \", \"a result, behavior and data are tied together and will be consistent throughout any application code\", \" that uses the domain model.\\n\\n## Implement custom repositories with Entity Framework Core\\n\\nAt the im\", \"plementation level, a repository is simply a class with data persistence code coordinated by a unit \", \"of work (DBContext in EF Core) when performing updates, as shown in the following class:\\n\\n```\\n// usi\", \"ng directives... namespace Microsoft . eShopOnContainers . Services . Ordering . Infrastructure . Re\", \"positories { public class BuyerRepository : IBuyerRepository { private readonly OrderingContext _con\", \"text; public IUnitOfWork UnitOfWork { get { return _context; } } public BuyerRepository(OrderingCont\", \"ext context) { _context = context ?? throw new ArgumentNullException(nameof(context)); } public Buye\", \"r Add(Buyer buyer) { return _context . Buyers . Add(buyer).Entity; } public async Task<Buyer> FindAs\", \"ync(string buyerIdentityGuid) { var buyer = await _context . Buyers . Include(b => b . Payments) . W\", \"here(b => b . FullName == buyerIdentityGuid) . SingleOrDefaultAsync(); return buyer; } } }\\n```\\n\\nThe \", \"IBuyerRepository interface comes from the domain model layer as a contract. However, the repository \", \"implementation is done at the persistence and infrastructure layer.\\n\\nThe EF DbContext comes through \", \"the constructor through Dependency Injection. It is shared between multiple repositories within the \", \"same HTTP request scope, thanks to its default lifetime (ServiceLifetime.Scoped) in the IoC containe\", \"r (which can also be explicitly set with services.AddDbContext&lt;&gt;).\\n\\n## Methods to implement in\", \" a repository (updates or transactions versus queries)\\n\\nWithin each repository class, you should put\", \" the persistence methods that update the state of entities contained by its related aggregate. Remem\", \"ber there is one-to-one relationship between an aggregate and its related repository. Consider that \", \"an aggregate root entity object might have embedded child entities within its EF graph. For example,\", \" a buyer might have multiple payment methods as related child entities.\\n\\nSince the approach for the \", \"ordering microservice in eShopOnContainers is also based on CQS/CQRS, most of the queries are not im\", \"plemented in custom repositories. Developers have the freedom to create the queries and joins they n\", \"eed for the presentation layer without the restrictions imposed by aggregates, custom repositories p\", \"er aggregate, and DDD in general. Most of the custom repositories suggested by this guide have sever\", \"al update or transactional methods but just the query methods needed to get data to be updated. For \", \"example, the BuyerRepository repository implements a FindAsync method, because the application needs\", \" to know whether a particular buyer exists before creating a new buyer related to the order.\\n\\nHoweve\", \"r, the real query methods to get data to send to the presentation layer or client apps are implement\", \"ed, as mentioned, in the CQRS queries based on flexible queries using Dapper.\\n\\n## Using a custom rep\", \"ository versus using EF DbContext directly\\n\\nThe Entity Framework DbContext class is based on the Uni\", \"t of Work and Repository patterns and can be used directly from your code, such as from an ASP.NET C\", \"ore MVC controller. The Unit of Work and Repository patterns result in the simplest code, as in the \", \"CRUD catalog microservice in eShopOnContainers. In cases where you want the simplest code possible, \", \"you might want to directly use the DbContext class, as many developers do.\\n\\nHowever, implementing cu\", \"stom repositories provides several benefits when implementing more complex microservices or applicat\", \"ions. The Unit of Work and Repository patterns are intended to encapsulate the infrastructure persis\", \"tence layer so it is decoupled from the application and domainmodel layers. Implementing these patte\", \"rns can facilitate the use of mock repositories simulating access to the database.\\n\\nIn Figure 7-18, \", \"you can see the differences between not using repositories (directly using the EF DbContext) versus \", \"using repositories, which makes it easier to mock those repositories.\\n\\nNo Repository\\n\\nDirect access \", \"to database from controller\\n\\nWeb Server\\n\\n(Kestrel, IIS, etc.)\\n\\nController or\\n\\nApplication Layer\\n\\nDbC\", \"ontext\\n\\nEntity Framework\\n\\nDatabase\\n\\nAbstraction layer between controller and database context.\\n\\nWith\", \" Repository\\n\\nUnit tests can mock data to facilitate testing\\n\\nWeb Server\\n\\n(Kestrel, IIS, etc.)\\n\\nWeb S\", \"erver\\n\\n(Kestrel, IIS, etc.)\\n\\nFigure 7 -18. Using custom repositories versus a plain DbContext\\n\\n<!-- \", \"image -->\\n\\nFigure 7-18 shows that using a custom repository adds an abstraction layer that can be us\", \"ed to ease testing by mocking the repository. There are multiple alternatives when mocking. You coul\", \"d mock just repositories or you could mock a whole unit of work. Usually mocking just the repositori\", \"es is enough, and the complexity to abstract and mock a whole unit of work is usually not needed.\\n\\nL\", \"ater, when we focus on the application layer, you will see how Dependency Injection works in ASP.NET\", \" Core and how it is implemented when using repositories.\\n\\nIn short, custom repositories allow you to\", \" test code more easily with unit tests that are not impacted by the data tier state. If you run test\", \"s that also access the actual database through the Entity Framework, they are not unit tests but int\", \"egration tests, which are a lot slower.\\n\\nIf you were using DbContext directly, you would have to moc\", \"k it or to run unit tests by using an inmemory SQL Server with predictable data for unit tests. But \", \"mocking the DbContext or controlling fake data requires more work than mocking at the repository lev\", \"el. Of course, you could always test the MVC controllers.\\n\\n## EF DbContext and IUnitOfWork instance \", \"lifetime in your IoC container\\n\\nThe DbContext object (exposed as an IUnitOfWork object) should be sh\", \"ared among multiple repositories within the same HTTP request scope. For example, this is true when \", \"the operation being executed must deal with multiple aggregates, or simply because you are using mul\", \"tiple repository instances. It is also important to mention that the IUnitOfWork interface is part o\", \"f your domain layer, not an EF Core type.\\n\\nIn order to do that, the instance of the DbContext object\", \" has to have its service lifetime set to ServiceLifetime.Scoped. This is the default lifetime when r\", \"egistering a DbContext with builder.Services.AddDbContext in your IoC container from the Program.cs \", \"file in your ASP.NET Core Web API project. The following code illustrates this.\\n\\n```\\n// Add framewor\", \"k services. builder . Services . AddMvc(options => { options . Filters . Add(typeof(HttpGlobalExcept\", \"ionFilter)); }).AddControllersAsServices(); builder . Services . AddEntityFrameworkSqlServer() . Add\", \"DbContext<OrderingContext>(options => { options . UseSqlServer(Configuration[\\\"ConnectionString\\\"], sq\", \"lOptions => sqlOptions . MigrationsAssembly(typeof(Startup).GetTypeInfo(). Assembly . GetName().Name\", \")); }, ServiceLifetime . Scoped // Note that Scoped is the default choice // in AddDbContext. It is \", \"shown here only for // pedagogic purposes. );\\n```\\n\\nThe DbContext instantiation mode should not be co\", \"nfigured as ServiceLifetime.Transient or ServiceLifetime.Singleton.\\n\\n## The repository instance life\", \"time in your IoC container\\n\\nIn a similar way, repository's lifetime should usually be set as scoped \", \"(InstancePerLifetimeScope in Autofac). It could also be transient (InstancePerDependency in Autofac)\", \", but your service will be more efficient in regards to memory when using the scoped lifetime.\\n\\n```\\n\", \"// Registering a Repository in Autofac IoC container builder . RegisterType<OrderRepository>() . As<\", \"IOrderRepository>() . InstancePerLifetimeScope();\\n```\\n\\nUsing the singleton lifetime for the reposito\", \"ry could cause you serious concurrency problems when your DbContext is set to scoped (InstancePerLif\", \"etimeScope) lifetime (the default lifetimes for a DBContext). As long as your service lifetimes for \", \"your repositories and your DbContext are both Scoped, you'll avoid these issues.\\n\\n## Additional reso\", \"urces\\n\\n- Implementing the Repository and Unit of Work Patterns in an ASP.NET MVC Application\\n\\nhttps:\", \"//www.asp.net/mvc/overview/older-versions/getting-started-with-ef-5-using-mvc4/implementing-the-repo\", \"sitory-and-unit-of-work-patterns-in-an-asp-net-mvc-application\\n\\n- Jonathan Allen. Implementation Str\", \"ategies for the Repository Pattern with Entity Framework, Dapper, and Chain https://www.infoq.com/ar\", \"ticles/repository-implementation-strategies\\n- Cesar de la Torre. Comparing ASP.NET Core IoC containe\", \"r service lifetimes with Autofac IoC container instance scopes\\n\\nhttps://devblogs.microsoft.com/cesar\", \"delatorre/comparing-asp-net-core-ioc-service-lifetimes -and -autofac -ioc -instance -scopes/\\n\\n## Tab\", \"le mapping\\n\\nTable mapping identifies the table data to be queried from and saved to the database. Pr\", \"eviously you saw how domain entities (for example, a product or order domain) can be used to generat\", \"e a related database schema. EF is strongly designed around the concept of conventions. Conventions \", \"address questions like \\\"What will the name of a table be?\\\" or \\\"What property is the primary key?\\\" Co\", \"nventions are typically based on conventional names. For example, it is typical for the primary key \", \"to be a property that ends with Id.\\n\\nBy convention, each entity will be set up to map to a table wit\", \"h the same name as the DbSet&lt;TEntity&gt; property that exposes the entity on the derived context.\", \" If no DbSet&lt;TEntity&gt; value is provided for the given entity, the class name is used.\\n\\n## Data\", \" Annotations versus Fluent API\\n\\nThere are many additional EF Core conventions, and most of them can \", \"be changed by using either data annotations or Fluent API, implemented within the OnModelCreating me\", \"thod.\\n\\nData annotations must be used on the entity model classes themselves, which is a more intrusi\", \"ve way from a DDD point of view. This is because you are contaminating your model with data annotati\", \"ons related to the infrastructure database. On the other hand, Fluent API is a convenient way to cha\", \"nge most conventions and mappings within your data persistence infrastructure layer, so the entity m\", \"odel will be clean and decoupled from the persistence infrastructure.\\n\\n## Fluent API and the OnModel\", \"Creating method\\n\\nAs mentioned, in order to change conventions and mappings, you can use the OnModelC\", \"reating method in the DbContext class.\\n\\nThe ordering microservice in eShopOnContainers implements ex\", \"plicit mapping and configuration, when needed, as shown in the following code.\\n\\n```\\n// At OrderingCo\", \"ntext.cs from eShopOnContainers protected override void OnModelCreating(ModelBuilder modelBuilder) {\", \" // ... modelBuilder . ApplyConfiguration(new OrderEntityTypeConfiguration()); // Other entities' co\", \"nfiguration ... } // At OrderEntityTypeConfiguration.cs from eShopOnContainers class OrderEntityType\", \"Configuration : IEntityTypeConfiguration<Order> { public void Configure(EntityTypeBuilder<Order> ord\", \"erConfiguration) { orderConfiguration . ToTable(\\\"orders\\\" , OrderingContext . DEFAULT_SCHEMA); orderC\", \"onfiguration . HasKey(o => o . Id); orderConfiguration . Ignore(b => b . DomainEvents); orderConfigu\", \"ration . Property(o => o . Id) . UseHiLo(\\\"orderseq\\\" , OrderingContext . DEFAULT_SCHEMA);\\n```\\n\\n```\\n//\", \"Address value object persisted as owned entity type supported since EF Core 2.0 orderConfiguration .\", \" OwnsOne(o => o . Address , a => { a . WithOwner(); }); orderConfiguration . Property<int?>(\\\"_buyerI\", \"d\\\") . UsePropertyAccessMode(PropertyAccessMode . Field) . HasColumnName(\\\"BuyerId\\\") . IsRequired(fals\", \"e); orderConfiguration . Property<DateTime>(\\\"_orderDate\\\") . UsePropertyAccessMode(PropertyAccessMode\", \" . Field) . HasColumnName(\\\"OrderDate\\\") . IsRequired(); orderConfiguration . Property<int>(\\\"_orderSta\", \"tusId\\\") . UsePropertyAccessMode(PropertyAccessMode . Field) . HasColumnName(\\\"OrderStatusId\\\") . IsReq\", \"uired(); orderConfiguration . Property<int?>(\\\"_paymentMethodId\\\") . UsePropertyAccessMode(PropertyAcc\", \"essMode . Field) . HasColumnName(\\\"PaymentMethodId\\\") . IsRequired(false); orderConfiguration . Proper\", \"ty<string>(\\\"Description\\\").IsRequired(false); var navigation = orderConfiguration . Metadata . FindNa\", \"vigation(nameof(Order . OrderItems)); // DDD Patterns comment: //Set as field (New since EF 1.1) to \", \"access the OrderItem collection property through its field navigation . SetPropertyAccessMode(Proper\", \"tyAccessMode . Field); orderConfiguration . HasOne<PaymentMethod>() . WithMany() . HasForeignKey(\\\"_p\", \"aymentMethodId\\\") . IsRequired(false) . OnDelete(DeleteBehavior . Restrict); orderConfiguration . Has\", \"One<Buyer>() . WithMany() . IsRequired(false) . HasForeignKey(\\\"_buyerId\\\"); orderConfiguration . HasO\", \"ne(o => o . OrderStatus) . WithMany() . HasForeignKey(\\\"_orderStatusId\\\"); } }\\n```\\n\\nYou could set all \", \"the Fluent API mappings within the same OnModelCreating method, but it's advisable to partition that\", \" code and have multiple configuration classes, one per entity, as shown in\\n\\nthe example. Especially \", \"for large models, it is advisable to have separate configuration classes for configuring different e\", \"ntity types.\\n\\nThe code in the example shows a few explicit declarations and mapping. However, EF Cor\", \"e conventions do many of those mappings automatically, so the actual code you would need in your cas\", \"e might be smaller.\\n\\n## The Hi/Lo algorithm in EF Core\\n\\nAn interesting aspect of code in the precedi\", \"ng example is that it uses the Hi/Lo algorithm as the key generation strategy.\\n\\nThe Hi/Lo algorithm \", \"is useful when you need unique keys before committing changes. As a summary, the Hi -Lo algorithm as\", \"signs unique identifiers to table rows while not depending on storing the row in the database immedi\", \"ately. This lets you start using the identifiers right away, as happens with regular sequential data\", \"base IDs.\\n\\nThe Hi/Lo algorithm describes a mechanism for getting a batch of unique IDs from a relate\", \"d database sequence. These IDs are safe to use because the database guarantees the uniqueness, so th\", \"ere will be no collisions between users. This algorithm is interesting for these reasons:\\n\\n- It does\", \" not break the Unit of Work pattern.\\n- It gets sequence IDs in batches, to minimize round trips to t\", \"he database.\\n- It generates a human readable identifier, unlike techniques that use GUIDs.\\n\\nEF Core \", \"supports HiLo with the UseHiLo method, as shown in the preceding example.\\n\\n## Map fields instead of \", \"properties\\n\\nWith this feature, available since EF Core 1.1, you can directly map columns to fields. \", \"It is possible to not use properties in the entity class, and just to map columns from a table to fi\", \"elds. A common use for that would be private fields for any internal state that do not need to be ac\", \"cessed from outside the entity.\\n\\nYou can do this with single fields or also with collections, like a\", \" List&lt;&gt; field. This point was mentioned earlier when we discussed modeling the domain model cl\", \"asses, but here you can see how that mapping is performed with the PropertyAccessMode.Field configur\", \"ation highlighted in the previous code.\\n\\n## Use shadow properties in EF Core, hidden at the infrastr\", \"ucture level\\n\\nShadow properties in EF Core are properties that do not exist in your entity class mod\", \"el. The values and states of these properties are maintained purely in the ChangeTracker class at th\", \"e infrastructure level.\\n\\n## Implement the Query Specification pattern\\n\\nAs introduced earlier in the \", \"design section, the Query Specification pattern is a Domain-Driven Design pattern designed as the pl\", \"ace where you can put the definition of a query with optional sorting and paging logic.\\n\\nThe Query S\", \"pecification pattern defines a query in an object. For example, in order to encapsulate a paged quer\", \"y that searches for some products you can create a PagedProduct specification that takes the necessa\", \"ry input parameters (pageNumber, pageSize, filter, etc.). Then, within any Repository method (usuall\", \"y a List() overload) it would accept an IQuerySpecification and run the expected query based on that\", \" specification.\\n\\nAn example of a generic Specification interface is the following code, which is sim\", \"ilar to code used in the eShopOnWeb reference application.\\n\\n```\\n// GENERIC SPECIFICATION INTERFACE /\", \"/ https://github.com/dotnet-architecture/eShopOnWeb public interface ISpecification<T> { Expression<\", \"Func<T , bool>> Criteria { get; } List<Expression<Func<T , object>>> Includes { get; } List<string> \", \"IncludeStrings { get; } }\\n```\\n\\nThen, the implementation of a generic specification base class is the\", \" following.\\n\\n```\\n// GENERIC SPECIFICATION IMPLEMENTATION (BASE CLASS) // https://github.com/dotnet-a\", \"rchitecture/eShopOnWeb public abstract class BaseSpecification<T> : ISpecification<T> { public BaseS\", \"pecification(Expression<Func<T , bool>> criteria) { Criteria = criteria; } public Expression<Func<T \", \", bool>> Criteria { get; } public List<Expression<Func<T , object>>> Includes { get; } = new List<Ex\", \"pression<Func<T , object>>>(); public List<string> IncludeStrings { get; } = new List<string>(); pro\", \"tected virtual void AddInclude(Expression<Func<T , object>> includeExpression) { Includes . Add(incl\", \"udeExpression); } // string-based includes allow for including children of children // e.g. Basket.I\", \"tems.Product protected virtual void AddInclude(string includeString) { IncludeStrings . Add(includeS\", \"tring); } }\\n```\\n\\nThe following specification loads a single basket entity given either the basket's \", \"ID or the ID of the buyer to whom the basket belongs. It will eagerly load the basket's Items collec\", \"tion.\\n\\n```\\n// SAMPLE QUERY SPECIFICATION IMPLEMENTATION public class BasketWithItemsSpecification : \", \"BaseSpecification<Basket> { public BasketWithItemsSpecification(int basketId) : base(b => b . Id == \", \"basketId) { AddInclude(b => b . Items); } public BasketWithItemsSpecification(string buyerId) : base\", \"(b => b . BuyerId == buyerId) { AddInclude(b => b . Items); } }\\n```\\n\\nAnd finally, you can see below \", \"how a generic EF Repository can use such a specification to filter and eager -load data related to a\", \" given entity type T.\\n\\n```\\n// GENERIC EF REPOSITORY WITH SPECIFICATION // https://github.com/dotnet-\", \"architecture/eShopOnWeb public IEnumerable<T> List(ISpecification<T> spec) { // fetch a Queryable th\", \"at includes all expression-based includes var queryableResultWithIncludes = spec . Includes . Aggreg\", \"ate(_dbContext . Set<T>().AsQueryable(), (current , include) => current . Include(include)); // modi\", \"fy the IQueryable to include any string-based include statements var secondaryResult = spec . Includ\", \"eStrings . Aggregate(queryableResultWithIncludes , (current , include) => current . Include(include)\", \"); // return the result of the query using the specification's criteria expression return secondaryR\", \"esult . Where(spec . Criteria) . AsEnumerable(); }\\n```\\n\\nIn addition to encapsulating filtering logic\", \", the specification can specify the shape of the data to be returned, including which properties to \", \"populate.\\n\\nAlthough we don't recommend returning IQueryable from a repository, it's perfectly fine t\", \"o use them within the repository to build up a set of results. You can see this approach used in the\", \" List method above, which uses intermediate IQueryable expressions to build up the query's list of i\", \"ncludes before executing the query with the specification's criteria on the last line.\\n\\nLearn how th\", \"e specification pattern is applied in the eShopOnWeb sample .\\n\\n## Additional resources\\n\\n- Table Mapp\", \"ing https://learn.microsoft.com/ef/core/modeling/relational/tables\\n- Use HiLo to generate keys with \", \"Entity Framework Core https://www.talkingdotnet.com/use-hilo-to-generate-keys-with-entity-framework-\", \"core/\\n- Backing Fields https://learn.microsoft.com/ef/core/modeling/backing-field\\n- Steve Smith. Enc\", \"apsulated Collections in Entity Framework Core https://ardalis.com/encapsulated-collections-in-entit\", \"y-framework-core\\n- Shadow Properties https://learn.microsoft.com/ef/core/modeling/shadow-properties\\n\", \"- The Specification pattern https://deviq.com/specification-pattern/\\n\\nArdalis.Specification NuGet Pa\", \"ckage Used by eShopOnWeb. https://www.nuget.org/packages/Ardalis.Specification\\n\\n## Use NoSQL databas\", \"es as a persistence infrastructure\\n\\nWhen you use NoSQL databases for your infrastructure data tier, \", \"you typically do not use an ORM like Entity Framework Core. Instead you use the API provided by the \", \"NoSQL engine, such as Azure Cosmos DB, MongoDB, Cassandra, RavenDB, CouchDB, or Azure Storage Tables\", \".\\n\\nHowever, when you use a NoSQL database, especially a document-oriented database like Azure Cosmos\", \" DB, CouchDB, or RavenDB, the way you design your model with DDD aggregates is partially similar to \", \"how you can do it in EF Core, in regards to the identification of aggregate roots, child entity clas\", \"ses, and value object classes. But, ultimately, the database selection will impact in your design.\\n\\n\", \"When you use a document-oriented database, you implement an aggregate as a single document, serializ\", \"ed in JSON or another format. However, the use of the database is transparent from a domain model co\", \"de point of view. When using a NoSQL database, you still are using entity classes and aggregate root\", \" classes, but with more flexibility than when using EF Core because the persistence is not relationa\", \"l.\\n\\nThe difference is in how you persist that model. If you implemented your domain model based on P\", \"OCO entity classes, agnostic to the infrastructure persistence, it might look like you could move to\", \" a different persistence infrastructure, even from relational to NoSQL. However, that should not be \", \"your goal. There are always constraints and trade-offs in the different database technologies, so yo\", \"u will not be able to have the same model for relational or NoSQL databases. Changing persistence mo\", \"dels is not a trivial task, because transactions and persistence operations will be very different.\\n\", \"\\nFor example, in a document-oriented database, it is okay for an aggregate root to have multiple chi\", \"ld collection properties. In a relational database, querying multiple child collection properties is\", \" not\\n\\neasily optimized, because you get a UNION ALL SQL statement back from EF. Having the same doma\", \"in model for relational databases or NoSQL databases is not simple, and you should not try to do it.\", \" You really have to design your model with an understanding of how the data is going to be used in e\", \"ach particular database.\\n\\nA benefit when using NoSQL databases is that the entities are more denorma\", \"lized, so you do not set a table mapping. Your domain model can be more flexible than when using a r\", \"elational database.\\n\\nWhen you design your domain model based on aggregates, moving to NoSQL and docu\", \"mentoriented databases might be even easier than using a relational database, because the aggregates\", \" you design are similar to serialized documents in a document-oriented database. Then you can includ\", \"e in those \\\"bags\\\" all the information you might need for that aggregate.\\n\\nFor instance, the followin\", \"g JSON code is a sample implementation of an order aggregate when using a document -oriented databas\", \"e. It is similar to the order aggregate we implemented in the eShopOnContainers sample, but without \", \"using EF Core underneath.\\n\\n```\\n{ \\\"id\\\": \\\"2024001\\\" , \\\"orderDate\\\": \\\"2/25/2024\\\" , \\\"buyerId\\\": \\\"1234567\\\" ,\", \" \\\"address\\\": [ { \\\"street\\\": \\\"100 One Microsoft Way\\\" , \\\"city\\\": \\\"Redmond\\\" , \\\"state\\\": \\\"WA\\\" , \\\"zip\\\": \\\"9805\", \"2\\\" , \\\"country\\\": \\\"U.S.\\\" } ] , \\\"orderItems\\\": [ {\\\"id\\\": 20240011 , \\\"productId\\\": \\\"123456\\\" , \\\"productName\\\"\", \": \\\".NET T-Shirt\\\" , \\\"unitPrice\\\": 25 , \\\"units\\\": 2 , \\\"discount\\\": 0} , {\\\"id\\\": 20240012 , \\\"productId\\\": \\\"1\", \"23457\\\" , \\\"productName\\\": \\\".NET Mug\\\" , \\\"unitPrice\\\": 15 , \\\"units\\\": 1 , \\\"discount\\\": 0} ] }\\n```\\n\\n## Intro\", \"duction to Azure Cosmos DB and the native Cosmos DB API\\n\\nAzure Cosmos DB is Microsoft's globally dis\", \"tributed database service for mission-critical applications. Azure Cosmos DB provides turn-key globa\", \"l distribution , elastic scaling of throughput and storage worldwide, single-digit millisecond laten\", \"cies at the 99th percentile, five well-defined consistency levels, and guaranteed high availability,\", \" all backed by industry-leading SLAs. Azure Cosmos DB automatically indexes data without requiring y\", \"ou to deal with schema and index management. It is multi -model and supports document, key-value, gr\", \"aph, and columnar data models.\\n\\nGlobal distribution\\n\\nAzure Cosmos DB\\n\\nKey-Value\\n\\n:\\n\\nElastic scale ou\", \"t\\n\\n<!-- image -->\\n\\nComprehensive SLAs\\n\\nFigure 7 -19. Azure Cosmos DB global distribution\\n\\n<!-- image\", \" -->\\n\\nWhen you use a C# model to implement the aggregate to be used by the Azure Cosmos DB API, the \", \"aggregate can be similar to the C# POCO classes used with EF Core. The difference is in the way to u\", \"se them from the application and infrastructure layers, as in the following code:\\n\\n```\\n// C# EXAMPLE\", \" OF AN ORDER AGGREGATE BEING PERSISTED WITH AZURE COSMOS DB API // *** Domain Model Code *** // Aggr\", \"egate: Create an Order object with its child entities and/or value objects. // Then, use AggregateRo\", \"ot's methods to add the nested objects so invariants and // logic is consistent across the nested pr\", \"operties (value objects and entities). Order orderAggregate = new Order { Id = \\\"2024001\\\" , OrderDate\", \" = new DateTime(2005 , 7 , 1), BuyerId = \\\"1234567\\\" , PurchaseOrderNumber = \\\"PO18009186470\\\" } Address\", \" address = new Address { Street = \\\"100 One Microsoft Way\\\" , City = \\\"Redmond\\\" , State = \\\"WA\\\" , Zip = \", \"\\\"98052\\\" , Country = \\\"U.S.\\\" } orderAggregate . UpdateAddress(address); OrderItem orderItem1 = new Ord\", \"erItem {\\n```\\n\\n```\\nId = 20240011 , ProductId = \\\"123456\\\" , ProductName = \\\".NET T -Shirt\\\" , UnitPrice =\", \" 25 , Units = 2 , Discount = 0; }; //Using methods with domain logic within the entity. No anemic-do\", \"main model orderAggregate . AddOrderItem(orderItem1); // *** End of Domain Model Code *** // *** Inf\", \"rastructure Code using Cosmos DB Client API *** Uri collectionUri = UriFactory . CreateDocumentColle\", \"ctionUri(databaseName , collectionName); await client . CreateDocumentAsync(collectionUri , orderAgg\", \"regate); // As your app evolves, let's say your object has a new schema. You can insert // OrderV2 o\", \"bjects without any changes to the database tier. Order2 newOrder = GetOrderV2Sample(\\\"IdForSalesOrder\", \"2\\\"); await client . CreateDocumentAsync(collectionUri , newOrder);\\n```\\n\\nYou can see that the way you\", \" work with your domain model can be similar to the way you use it in your domain model layer when th\", \"e infrastructure is EF. You still use the same aggregate root methods to ensure consistency, invaria\", \"nts, and validations within the aggregate.\\n\\nHowever, when you persist your model into the NoSQL data\", \"base, the code and API change dramatically compared to EF Core code or any other code related to rel\", \"ational databases.\\n\\n## Implement .NET code targeting MongoDB and Azure Cosmos DB\\n\\n## Use Azure Cosmo\", \"s DB from .NET containers\\n\\nYou can access Azure Cosmos DB databases from .NET code running in contai\", \"ners, like from any other .NET application. For instance, the Locations.API and Marketing.API micros\", \"ervices in eShopOnContainers are implemented so they can consume Azure Cosmos DB databases.\\n\\nHowever\", \", there's a limitation in Azure Cosmos DB from a Docker development environment point of view. Even \", \"though there's an on-premises Azure Cosmos DB Emulator that can run in a local development machine, \", \"it only supports Windows. Linux and macOS aren't supported.\\n\\nThere's also the possibility to run thi\", \"s emulator on Docker, but just on Windows Containers, not with Linux Containers. That's an initial h\", \"andicap for the development environment if your application is deployed as Linux containers, since, \", \"currently, you can't deploy Linux and Windows Containers on Docker for Windows at the same time. Eit\", \"her all containers being deployed have to be for Linux or for Windows.\\n\\nThe ideal and more straightf\", \"orward deployment for a dev/test solution is to be able to deploy your database systems as container\", \"s along with your custom containers so your dev/test environments are always consistent.\\n\\nnodeo\\n\\nMic\", \"rosoft\\n\\n~ python\\\"\\n\\n## Use MongoDB API for local dev/test Linux/Windows containers plus Azure Cosmos \", \"DB Azure Cosmos DB:\\n\\nNET\\n\\nRuby\\n\\nCosmos DB databases support MongoDB API for .NET as well as the nati\", \"ve MongoDB wire protocol. This means that by using existing drivers, your application written for Mo\", \"ngoDB can now communicate with Cosmos DB and use Cosmos DB databases instead of MongoDB databases, a\", \"s shown in Figure 7-20.\\n\\nFigure 7 -20. Using MongoDB API and protocol to access Azure Cosmos DB\\n\\n<!-\", \"- image -->\\n\\nThis is a very convenient approach for proof of concepts in Docker environments with Li\", \"nux containers because the MongoDB Docker image is a multi-arch image that supports Docker Linux con\", \"tainers and Docker Windows containers.\\n\\nAs shown in the following image, by using the MongoDB API, e\", \"ShopOnContainers supports MongoDB Linux and Windows containers for the local development environment\", \" but then, you can move to a scalable, PaaS cloud solution as Azure Cosmos DB by simply changing the\", \" MongoDB connection string to point to Azure Cosmos DB .\\n\\n| Docker Host\\n\\nMicrosott\\n\\nAzure\\n\\nFigure 7 \", \"-21. eShopOnContainers using MongoDB containers for dev -env or Azure Cosmos DB for production\\n\\n<!--\", \" image -->\\n\\nThe production Azure Cosmos DB would be running in Azure\\u2019s cloud as a PaaS and scalable \", \"service.\\n\\nYour custom .NET containers can run on a local development Docker host (that is using Dock\", \"er for Windows in a Windows 10 machine) or be deployed into a production environment, like Kubernete\", \"s in Azure AKS or Azure Service Fabric. In this second environment, you would deploy only the .NET c\", \"ustom containers but not the MongoDB container since you'd be using Azure Cosmos DB in the cloud for\", \" handling the data in production.\\n\\nA clear benefit of using the MongoDB API is that your solution co\", \"uld run in both database engines, MongoDB or Azure Cosmos DB, so migrations to different environment\", \"s should be easy. However, sometimes it is worthwhile to use a native API (that is the native Cosmos\", \" DB API) in order to take full advantage of the capabilities of a specific database engine.\\n\\nFor fur\", \"ther comparison between simply using MongoDB versus Cosmos DB in the cloud, see the Benefits of usin\", \"g Azure Cosmos DB in this page .\\n\\n## Analyze your approach for production applications: MongoDB API \", \"vs. Cosmos DB API\\n\\nIn eShopOnContainers, we're using MongoDB API because our priority was fundamenta\", \"lly to have a consistent dev/test environment using a NoSQL database that could also work with Azure\", \" Cosmos DB.\\n\\nHowever, if you are planning to use MongoDB API to access Azure Cosmos DB in Azure for \", \"production applications, you should analyze the differences in capabilities and performance when usi\", \"ng MongoDB API to access Azure Cosmos DB databases compared to using the native Azure Cosmos DB API.\", \" If it is similar you can use MongoDB API and you get the benefit of supporting two NoSQL database e\", \"ngines at the same time.\\n\\nYou could also use MongoDB clusters as the production database in Azure's \", \"cloud, too, with MongoDB Azure Service. But that is not a PaaS service provided by Microsoft. In thi\", \"s case, Azure is just hosting that solution coming from MongoDB.\\n\\nBasically, this is just a disclaim\", \"er stating that you shouldn't always use MongoDB API against Azure Cosmos DB, as we did in eShopOnCo\", \"ntainers because it was a convenient choice for Linux containers. The decision should be based on th\", \"e specific needs and tests you need to do for your production application.\\n\\n## The code: Use MongoDB\", \" API in .NET applications\\n\\nMongoDB API for .NET is based on NuGet packages that you need to add to y\", \"our projects, like in the Locations.API project shown in the following figure.\\n\\nLocations.API\\n\\nConne\", \"cted Services\\n\\n#\\u2022 Dependencies\\n\\nF Analyzers\\n\\nFrameworks\\n\\n*.\\n\\n*.\\n\\n<!-- image -->\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\\n\\nP\", \"\\n\\nP\\n\\n\\u2022.\\n\\n\\u2022 Projects\\n\\nProperties\\n\\nFigure 7 -22. MongoDB API NuGet packages references in a .NET proje\", \"ct\\n\\nLet\\u2019s investigate the code in the following sections.\\n\\n## A Model used by MongoDB API\\n\\nFirst, yo\", \"u need to define a model that will hold the data coming from the database in your application's memo\", \"ry space. Here's an example of the model used for Locations at eShopOnContainers.\\n\\n```\\nusing MongoDB\", \" . Bson; using MongoDB . Bson . Serialization . Attributes; using MongoDB . Driver . GeoJsonObjectMo\", \"del; using System . Collections . Generic; public class Locations {\\n```\\n\\n```\\n[BsonId] [BsonRepresent\", \"ation(BsonType . ObjectId)] public string Id { get; set; } public int LocationId { get; set; } publi\", \"c string Code { get; set; } [BsonRepresentation(BsonType . ObjectId)] public string Parent_Id { get;\", \" set; } public string Description { get; set; } public double Latitude { get; set; } public double L\", \"ongitude { get; set; } public GeoJsonPoint<GeoJson2DGeographicCoordinates> Location { get; private s\", \"et; } public GeoJsonPolygon<GeoJson2DGeographicCoordinates> Polygon { get; private set; } public voi\", \"d SetLocation(double lon , double lat) => SetPosition(lon , lat); public void SetArea(List<GeoJson2D\", \"GeographicCoordinates> coordinatesList) => SetPolygon(coordinatesList); private void SetPosition(dou\", \"ble lon , double lat) { Latitude = lat; Longitude = lon; Location = new GeoJsonPoint<GeoJson2DGeogra\", \"phicCoordinates>( new GeoJson2DGeographicCoordinates(lon , lat)); } private void SetPolygon(List<Geo\", \"Json2DGeographicCoordinates> coordinatesList) { Polygon = new GeoJsonPolygon<GeoJson2DGeographicCoor\", \"dinates>( new GeoJsonPolygonCoordinates<GeoJson2DGeographicCoordinates>( new GeoJsonLinearRingCoordi\", \"nates<GeoJson2DGeographicCoordinates>( coordinatesList))); } }\\n```\\n\\nYou can see there are a few attr\", \"ibutes and types coming from the MongoDB NuGet packages.\\n\\nNoSQL databases are usually very well suit\", \"ed for working with non-relational hierarchical data. In this example, we are using MongoDB types es\", \"pecially made for geo-locations, like GeoJson2DGeographicCoordinates.\\n\\n## Retrieve the database and \", \"the collection\\n\\nIn eShopOnContainers, we have created a custom database context where we implement t\", \"he code to retrieve the database and the MongoCollections, as in the following code.\\n\\n```\\npublic cla\", \"ss LocationsContext { private readonly IMongoDatabase _database = null; public LocationsContext(IOpt\", \"ions<LocationSettings> settings) { var client = new MongoClient(settings . Value . ConnectionString)\", \"; if (client != null) _database = client . GetDatabase(settings . Value . Database); } public IMongo\", \"Collection<Locations> Locations {\\n```\\n\\n```\\nget { return _database . GetCollection<Locations>(\\\"Locati\", \"ons\\\"); } } }\\n```\\n\\n## Retrieve the data\\n\\nIn C# code, like Web API controllers or custom Repositories \", \"implementation, you can write similar code to the following when querying through the MongoDB API. N\", \"ote that the \\\\_context object is an instance of the previous LocationsContext class.\\n\\n```\\npublic asy\", \"nc Task<Locations> GetAsync(int locationId) { var filter = Builders<Locations>.Filter . Eq(\\\"Location\", \"Id\\\" , locationId); return await _context . Locations . Find(filter) . FirstOrDefaultAsync(); }\\n```\\n\\n\", \"## Use an env -var in the docker -compose.override.yml file for the MongoDB connection string\\n\\nWhen \", \"creating a MongoClient object, it needs a fundamental parameter which is precisely the ConnectionStr\", \"ing parameter pointing to the right database. In the case of eShopOnContainers, the connection strin\", \"g can point to a local MongoDB Docker container or to a \\\"production\\\" Azure Cosmos DB database. That \", \"connection string comes from the environment variables defined in the dockercompose.override.yml fil\", \"es used when deploying with docker-compose or Visual Studio, as in the following yml code.\\n\\n```\\n# do\", \"cker -compose.override.yml version: '3.4' services: # Other services locations -api: environment: # \", \"Other settings -ConnectionString=${ESHOP_AZURE_COSMOSDB:-mongodb://nosqldata}\\n```\\n\\nThe ConnectionStr\", \"ing environment variable is resolved this way: If the ESHOP\\\\_AZURE\\\\_COSMOSDB global variable is defi\", \"ned in the .env file with the Azure Cosmos DB connection string, it will use it to access the Azure \", \"Cosmos DB database in the cloud. If it's not defined, it will take the mongodb://nosqldata value and\", \" use the development MongoDB container.\\n\\nThe following code shows the .env file with the Azure Cosmo\", \"s DB connection string global environment variable, as implemented in eShopOnContainers:\\n\\n```\\n# .env\", \" file, in eShopOnContainers root folder # Other Docker environment variables ESHOP_EXTERNAL_DNS_NAME\", \"_OR_IP=host.docker.internal ESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP=<YourDockerHostIP> #ESHOP_AZURE_COSMO\", \"SDB=<YourAzureCosmosDBConnData>\\n```\\n\\n```\\n#Other environment variables for additional Azure infrastru\", \"cture assets #ESHOP_AZURE_REDIS_BASKET_DB=<YourAzureRedisBasketInfo> #ESHOP_AZURE_STORAGE_CATALOG_UR\", \"L=<YourAzureStorage_Catalog_BLOB_URL> #ESHOP_AZURE_SERVICE_BUS=<YourAzureServiceBusInfo>\\n```\\n\\nUncomm\", \"ent the ESHOP\\\\_AZURE\\\\_COSMOSDB line and update it with your Azure Cosmos DB connection string obtain\", \"ed from the Azure portal as explained in Connect a MongoDB application to Azure Cosmos DB .\\n\\nIf the \", \"ESHOP\\\\_AZURE\\\\_COSMOSDB global variable is empty, meaning it's commented out in the .env file, then t\", \"he container uses a default MongoDB connection string. This connection string points to the local Mo\", \"ngoDB container deployed in eShopOnContainers that is named nosqldata and was defined at the docker \", \"-compose file, as shown in the following .yml code:\\n\\n```\\n# docker -compose.yml version: '3.4' servic\", \"es: # ...Other services... nosqldata: image: mongo\\n```\\n\\n## Additional resources\\n\\n- Modeling document\", \" data for NoSQL databases https://learn.microsoft.com/azure/cosmos-db/modeling-data\\n- Vaughn Vernon.\", \" The Ideal Domain-Driven Design Aggregate Store? https://kalele.io/blog-posts/the-ideal-domain-drive\", \"n-design-aggregate-store/\\n- Introduction to Azure Cosmos DB: API for MongoDB https://learn.microsoft\", \".com/azure/cosmos-db/mongodb-introduction\\n- Azure Cosmos DB: Build a MongoDB API web app with .NET a\", \"nd the Azure portal https://learn.microsoft.com/azure/cosmos-db/create-mongodb-dotnet\\n- Use the Azur\", \"e Cosmos DB Emulator for local development and testing https://learn.microsoft.com/azure/cosmos-db/l\", \"ocal-emulator\\n- Connect a MongoDB application to Azure Cosmos DB https://learn.microsoft.com/azure/c\", \"osmos-db/connect-mongodb-account\\n- The Cosmos DB Emulator Docker image (Windows Container) https://h\", \"ub.docker.com/r/microsoft/azure-cosmosdb-emulator/\\n- The MongoDB Docker image (Linux and Windows Con\", \"tainer) https://hub.docker.com/\\\\_/mongo/\\n- Use MongoChef (Studio 3T) with an Azure Cosmos DB: API fo\", \"r MongoDB account https://learn.microsoft.com/azure/cosmos-db/mongodb-mongochef\\n\\n## Design the micro\", \"service application layer and Web API\\n\\n## Use SOLID principles and Dependency Injection\\n\\nSOLID princ\", \"iples are critical techniques to be used in any modern and mission-critical application, such as dev\", \"eloping a microservice with DDD patterns. SOLID is an acronym that groups five fundamental principle\", \"s:\\n\\n- Single Responsibility principle\\n- Open/closed principle\\n- Liskov substitution principle\\n- Inte\", \"rface Segregation principle\\n- Dependency Inversion principle\\n\\nSOLID is more about how you design you\", \"r application or microservice internal layers and about decoupling dependencies between them. It is \", \"not related to the domain, but to the application's technical design. The final principle, the Depen\", \"dency Inversion principle, allows you to decouple the infrastructure layer from the rest of the laye\", \"rs, which allows a better decoupled implementation of the DDD layers.\\n\\nDependency Injection (DI) is \", \"one way to implement the Dependency Inversion principle. It is a technique for achieving loose coupl\", \"ing between objects and their dependencies. Rather than directly instantiating collaborators, or usi\", \"ng static references (that is, using new\\u2026), the objects that a class needs in order to perform its a\", \"ctions are provided to (or \\\"injected into\\\") the class. Most often, classes will declare their depend\", \"encies via their constructor, allowing them to follow the Explicit Dependencies principle. Dependenc\", \"y Injection is usually based on specific Inversion of Control (IoC) containers. ASP.NET Core provide\", \"s a simple built-in IoC container, but you can also use your favorite IoC container, like Autofac or\", \" Ninject.\\n\\nBy following the SOLID principles, your classes will tend naturally to be small, well-fac\", \"tored, and easily tested. But how can you know if too many dependencies are being injected into your\", \" classes? If you use DI through the constructor, it will be easy to detect that by just looking at t\", \"he number of parameters for your constructor. If there are too many dependencies, this is generally \", \"a sign (a code smell) that your class is trying to do too much, and is probably violating the Single\", \" Responsibility principle.\\n\\nIt would take another guide to cover SOLID in detail. Therefore, this gu\", \"ide requires you to have only a minimum knowledge of these topics.\\n\\n## Additional resources\\n\\n- SOLID\", \": Fundamental OOP Principles\\n\\nhttps://deviq.com/solid/\\n\\n4 &amp;O] Ordering.API\\n\\n$ Connected Services\", \"\\n\\n\\u2022 .: Dependencies\\n\\n\\u2022 &amp;\\n\\nProperties\\n\\n\\u2022 &amp;\\n\\n\\u2022 &amp;|\\n\\n- Inversion of Control Containers and t\", \"he Dependency Injection pattern https://martinfowler.com/articles/injection.html \\u00b7 Behaviors\\n\\n\\u2022 \\u00e2|\\n\\n\", \"\\u2022 6[\\n\\n- Steve Smith. New is Glue https://ardalis.com/new-is-glue\\n\\nModels\\n\\n\\u2022 a 1 Queries\\n\\n\\u2022 a\\n\\n\\u2022 &amp\", \";\\n\\n\\u2022 a\\n\\n## Implement the microservice application layer using the Web API Infrastructure\\n\\n\\u2022 a\\n\\n## Us\", \"e Dependency Injection to inject infrastructure objects into your application layer\\n\\nAs mentioned pr\", \"eviously, the application layer can be implemented as part of the artifact (assembly) you are buildi\", \"ng, such as within a Web API project or an MVC web app project. In the case of a microservice built \", \"with ASP.NET Core, the application layer will usually be your Web API library. If you want to separa\", \"te what is coming from ASP.NET Core (its infrastructure plus your controllers) from your custom appl\", \"ication layer code, you could also place your application layer in a separate class library, but tha\", \"t is optional.\\n\\nFor instance, the application layer code of the ordering microservice is directly im\", \"plemented as part of the Ordering.API project (an ASP.NET Core Web API project), as shown in Figure \", \"7-23.\\n\\nFigure 7 -23. The application layer in the Ordering.API ASP.NET Core Web API project\\n\\n<!-- im\", \"age -->\\n\\nASP.NET Core includes a simple built-in IoC container (represented by the IServiceProvider \", \"interface) that supports constructor injection by default, and ASP.NET makes certain services availa\", \"ble through DI. ASP.NET Core uses the term service for any of the types you register that will be in\", \"jected through DI. You configure the built-in container's services in your application's Program.cs \", \"file. Your\\n\\ndependencies are implemented in the services that a type needs and that you register in \", \"the IoC container.\\n\\nTypically, you want to inject dependencies that implement infrastructure objects\", \". A typical dependency to inject is a repository. But you could inject any other infrastructure depe\", \"ndency that you may have. For simpler implementations, you could directly inject your Unit of Work p\", \"attern object (the EF DbContext object), because the DBContext is also the implementation of your in\", \"frastructure persistence objects.\\n\\nIn the following example, you can see how .NET is injecting the r\", \"equired repository objects through the constructor. The class is a command handler, which will get c\", \"overed in the next section.\\n\\n```\\npublic class CreateOrderCommandHandler : IRequestHandler<CreateOrde\", \"rCommand , bool> { private readonly IOrderRepository _orderRepository; private readonly IIdentitySer\", \"vice _identityService; private readonly IMediator _mediator; private readonly IOrderingIntegrationEv\", \"entService _orderingIntegrationEventService; private readonly ILogger<CreateOrderCommandHandler> _lo\", \"gger; // Using DI to inject infrastructure persistence Repositories public CreateOrderCommandHandler\", \"(IMediator mediator , IOrderingIntegrationEventService orderingIntegrationEventService , IOrderRepos\", \"itory orderRepository , IIdentityService identityService , ILogger<CreateOrderCommandHandler> logger\", \") { _orderRepository = orderRepository ?? throw new ArgumentNullException(nameof(orderRepository)); \", \"_identityService = identityService ?? throw new ArgumentNullException(nameof(identityService)); _med\", \"iator = mediator ?? throw new ArgumentNullException(nameof(mediator)); _orderingIntegrationEventServ\", \"ice = orderingIntegrationEventService ?? throw new ArgumentNullException(nameof(orderingIntegrationE\", \"ventService)); _logger = logger ?? throw new ArgumentNullException(nameof(logger)); } public async T\", \"ask<bool> Handle(CreateOrderCommand message , CancellationToken cancellationToken) { // Add Integrat\", \"ion event to clean the basket var orderStartedIntegrationEvent = new OrderStartedIntegrationEvent(me\", \"ssage . UserId); await _orderingIntegrationEventService . AddAndSaveEventAsync(orderStartedIntegrati\", \"onEvent); // Add/Update the Buyer AggregateRoot // DDD patterns comment: Add child entities and valu\", \"e-objects through the Order Aggregate -Root // methods and constructor so validations, invariants an\", \"d business logic // make sure that consistency is preserved across the whole aggregate var address =\", \" new Address(message . Street , message.City , message.State , message.Country , message.ZipCode); v\", \"ar order = new Order(message . UserId , message.UserName , address , message.CardTypeId , message.Ca\", \"rdNumber , message.CardSecurityNumber , message.CardHolderName , message.CardExpiration); foreach (v\", \"ar item in message . OrderItems)\\n```\\n\\n```\\n{ order . AddOrderItem(item . ProductId , item . ProductNa\", \"me , item . UnitPrice , item . Discount , item . PictureUrl , item . Units); } _logger . LogInformat\", \"ion( \\\" -----Creating Order -Order: {@Order}\\\" , order); _orderRepository . Add(order); return await _\", \"orderRepository . UnitOfWork . SaveEntitiesAsync(cancellationToken); } }\\n```\\n\\nThe class uses the inj\", \"ected repositories to execute the transaction and persist the state changes. It does not matter whet\", \"her that class is a command handler, an ASP.NET Core Web API controller method, or a DDD Application\", \" Service. It is ultimately a simple class that uses repositories, domain entities, and other applica\", \"tion coordination in a fashion similar to a command handler. Dependency Injection works the same way\", \" for all the mentioned classes, as in the example using DI based on the constructor.\\n\\n## Register th\", \"e dependency implementation types and interfaces or abstractions\\n\\nBefore you use the objects injecte\", \"d through constructors, you need to know where to register the interfaces and classes that produce t\", \"he objects injected into your application classes through DI. (Like DI based on the constructor, as \", \"shown previously.)\\n\\n## Use the built -in IoC container provided by ASP.NET Core\\n\\nWhen you use the bu\", \"ilt-in IoC container provided by ASP.NET Core, you register the types you want to inject in the Prog\", \"ram.cs file, as in the following code:\\n\\n```\\n// Register out-of-the-box framework services. builder .\", \" Services . AddDbContext<CatalogContext>(c => c . UseSqlServer(Configuration[\\\"ConnectionString\\\"]), S\", \"erviceLifetime . Scoped); builder . Services . AddMvc(); // Register custom application dependencies\", \". builder . Services . AddScoped<IMyCustomRepository , MyCustomSQLRepository>();\\n```\\n\\nThe most commo\", \"n pattern when registering types in an IoC container is to register a pair of types\\u2014an interface and\", \" its related implementation class. Then when you request an object from the IoC container through an\", \"y constructor, you request an object of a certain type of interface. For instance, in the previous e\", \"xample, the last line states that when any of your constructors have a dependency on IMyCustomReposi\", \"tory (interface or abstraction), the IoC container will inject an instance of the MyCustomSQLServerR\", \"epository implementation class.\\n\\n## Use the Scrutor library for automatic types registration\\n\\nWhen u\", \"sing DI in .NET, you might want to be able to scan an assembly and automatically register its types \", \"by convention. This feature is not currently available in ASP.NET Core. However, you can use the\\n\\nSc\", \"rutor library for that. This approach is convenient when you have dozens of types that need to be re\", \"gistered in your IoC container.\\n\\n## Additional resources\\n\\n- Matthew King. Registering services with \", \"Scrutor https://www.mking.net/blog/registering-services-with-scrutor\\n- Kristian Hellang. Scrutor. Gi\", \"tHub repo. https://github.com/khellang/Scrutor\\n\\n## Use Autofac as an IoC container\\n\\nYou can also use\", \" additional IoC containers and plug them into the ASP.NET Core pipeline, as in the ordering microser\", \"vice in eShopOnContainers, which uses Autofac. When using Autofac you typically register the types v\", \"ia modules, which allow you to split the registration types between multiple files depending on wher\", \"e your types are, just as you could have the application types distributed across multiple class lib\", \"raries.\\n\\nFor example, the following is the Autofac application module for the Ordering.API Web API p\", \"roject with the types you will want to inject.\\n\\n```\\npublic class ApplicationModule : Autofac . Modul\", \"e { public string QueriesConnectionString { get; } public ApplicationModule(string qconstr) { Querie\", \"sConnectionString = qconstr; } protected override void Load(ContainerBuilder builder) { builder . Re\", \"gister(c => new OrderQueries(QueriesConnectionString)) . As<IOrderQueries>() . InstancePerLifetimeSc\", \"ope(); builder . RegisterType<BuyerRepository>() . As<IBuyerRepository>() . InstancePerLifetimeScope\", \"(); builder . RegisterType<OrderRepository>() . As<IOrderRepository>() . InstancePerLifetimeScope();\", \" builder . RegisterType<RequestManager>() . As<IRequestManager>() . InstancePerLifetimeScope(); } }\\n\", \"```\\n\\nAutofac also has a feature to scan assemblies and register types by name conventions .\\n\\nThe reg\", \"istration process and concepts are very similar to the way you can register types with the builtin A\", \"SP.NET Core IoC container, but the syntax when using Autofac is a bit different.\\n\\nIn the example cod\", \"e, the abstraction IOrderRepository is registered along with the implementation class OrderRepositor\", \"y. This means that whenever a constructor is declaring a dependency through the\\n\\nIOrderRepository ab\", \"straction or interface, the IoC container will inject an instance of the OrderRepository class.\\n\\nThe\", \" instance scope type determines how an instance is shared between requests for the same service or d\", \"ependency. When a request is made for a dependency, the IoC container can return the following:\\n\\n- A\", \" single instance per lifetime scope (referred to in the ASP.NET Core IoC container as scoped).\\n- A n\", \"ew instance per dependency (referred to in the ASP.NET Core IoC container as transient).\\n- A single \", \"instance shared across all objects using the IoC container (referred to in the ASP.NET Core IoC cont\", \"ainer as singleton).\\n\\n## Additional resources\\n\\n- Introduction to Dependency Injection in ASP.NET Cor\", \"e https://learn.microsoft.com/aspnet/core/fundamentals/dependency-injection\\n- Autofac. Official docu\", \"mentation. https://docs.autofac.org/en/latest/\\n- Comparing ASP.NET Core IoC container service lifeti\", \"mes with Autofac IoC container instance scopes - Cesar de la Torre. https://devblogs.microsoft.com/c\", \"esardelatorre/comparing-asp-net-core-ioc-service-lifetimes -and -autofac -ioc -instance -scopes/\\n\\n##\", \" Implement the Command and Command Handler patterns\\n\\nIn the DI -through-constructor example shown in\", \" the previous section, the IoC container was injecting repositories through a constructor in a class\", \". But exactly where were they injected? In a simple Web API (for example, the catalog microservice i\", \"n eShopOnContainers), you inject them at the MVC controllers' level, in a controller constructor, as\", \" part of the request pipeline of ASP.NET Core. However, in the initial code of this section (the Cre\", \"ateOrderCommandHandler class from the Ordering.API service in eShopOnContainers), the injection of d\", \"ependencies is done through the constructor of a particular command handler. Let us explain what a c\", \"ommand handler is and why you would want to use it.\\n\\nThe Command pattern is intrinsically related to\", \" the CQRS pattern that was introduced earlier in this guide. CQRS has two sides. The first area is q\", \"ueries, using simplified queries with the Dapper micro ORM, which was explained previously. The seco\", \"nd area is commands, which are the starting point for transactions, and the input channel from outsi\", \"de the service.\\n\\nAs shown in Figure 7-24, the pattern is based on accepting commands from the client\", \"-side, processing them based on the domain model rules, and finally persisting the states with trans\", \"actions.\\n\\nUl app\\n\\nCommand\\n\\nHigh level \\\"Writes-side\\\" in CQRS\\n\\nDomain\\n\\n## Command Model\\n\\nAPI\\n\\ninterfac\", \"e\\n\\nCommand\\n\\nHandler\\n\\nFigure 7 -24. High -level view of the commands or \\\"transactional side\\\" in a CQR\", \"S pattern\\n\\n<!-- image -->\\n\\nFigure 7-24 shows that the UI app sends a command through the API that ge\", \"ts to a CommandHandler, that depends on the Domain model and the Infrastructure, to update the datab\", \"ase.\\n\\n## The command class\\n\\nA command is a request for the system to perform an action that changes \", \"the state of the system. Commands are imperative, and should be processed just once.\\n\\nSince commands\", \" are imperatives, they are typically named with a verb in the imperative mood (for example, \\\"create\\\"\", \" or \\\"update\\\"), and they might include the aggregate type, such as CreateOrderCommand. Unlike an even\", \"t, a command is not a fact from the past; it is only a request, and thus may be refused.\\n\\nCommands c\", \"an originate from the UI as a result of a user initiating a request, or from a process manager when \", \"the process manager is directing an aggregate to perform an action.\\n\\nAn important characteristic of \", \"a command is that it should be processed just once by a single receiver. This is because a command i\", \"s a single action or transaction you want to perform in the application. For example, the same order\", \" creation command should not be processed more than once. This is an important difference between co\", \"mmands and events. Events may be processed multiple times, because many systems or microservices mig\", \"ht be interested in the event.\\n\\nIn addition, it is important that a command be processed only once i\", \"n case the command is not idempotent. A command is idempotent if it can be executed multiple times w\", \"ithout changing the result, either because of the nature of the command, or because of the way the s\", \"ystem handles the command.\\n\\nIt is a good practice to make your commands and updates idempotent when \", \"it makes sense under your domain's business rules and invariants. For instance, to use the same exam\", \"ple, if for any reason (retry logic, hacking, etc.) the same CreateOrder command reaches your system\", \" multiple times, you should be able to identify it and ensure that you do not create multiple orders\", \". To do so, you need to\\n\\nattach some kind of identity in the operations and identify whether the com\", \"mand or update was already processed.\\n\\nYou send a command to a single receiver; you do not publish a\", \" command. Publishing is for events that state a fact \\u2014 that something has happened and might be inte\", \"resting for event receivers. In the case of events, the publisher has no concerns about which receiv\", \"ers get the event or what they do it. But domain or integration events are a different story already\", \" introduced in previous sections.\\n\\nA command is implemented with a class that contains data fields o\", \"r collections with all the information that is needed in order to execute that command. A command is\", \" a special kind of Data Transfer Object (DTO), one that is specifically used to request changes or t\", \"ransactions. The command itself is based on exactly the information that is needed for processing th\", \"e command, and nothing more.\\n\\nThe following example shows the simplified CreateOrderCommand class. T\", \"his is an immutable command that is used in the ordering microservice in eShopOnContainers.\\n\\n```\\n// \", \"DDD and CQRS patterns comment: Note that it is recommended to implement immutable Commands // In thi\", \"s case, its immutability is achieved by having all the setters as private // plus only being able to\", \" update the data just once, when creating the object through its constructor. // References on Immut\", \"able Commands: // http://cqrs.nu/Faq // https://docs.spine3.org/motivation/immutability.html // http\", \"://blog.gauffin.org/2012/06/griffin-container-introducing-command-support/ // https://learn.microsof\", \"t.com/dotnet/csharp/programming-guide/classes-and-structs/how-toimplement-a-lightweight-class-with-a\", \"uto-implemented-properties [DataContract] public class CreateOrderCommand : IRequest<bool> { [DataMe\", \"mber] private readonly List<OrderItemDTO> _orderItems; [DataMember] public string UserId { get; priv\", \"ate set; } [DataMember] public string UserName { get; private set; } [DataMember] public string City\", \" { get; private set; } [DataMember] public string Street { get; private set; } [DataMember] public s\", \"tring State { get; private set; } [DataMember] public string Country { get; private set; } [DataMemb\", \"er] public string ZipCode { get; private set; } [DataMember]\\n```\\n\\n```\\npublic string CardNumber { get\", \"; private set; } [DataMember] public string CardHolderName { get; private set; } [DataMember] public\", \" DateTime CardExpiration { get; private set; } [DataMember] public string CardSecurityNumber { get; \", \"private set; } [DataMember] public int CardTypeId { get; private set; } [DataMember] public IEnumera\", \"ble<OrderItemDTO> OrderItems => _orderItems; public CreateOrderCommand() { _orderItems = new List<Or\", \"derItemDTO>(); } public CreateOrderCommand(List<BasketItem> basketItems , string userId , string use\", \"rName , string city , string street , string state , string country , string zipcode , string cardNu\", \"mber , string cardHolderName , DateTime cardExpiration , string cardSecurityNumber , int cardTypeId)\", \" : this() { _orderItems = basketItems . ToOrderItemsDTO().ToList(); UserId = userId; UserName = user\", \"Name; City = city; Street = street; State = state; Country = country; ZipCode = zipcode; CardNumber \", \"= cardNumber; CardHolderName = cardHolderName; CardExpiration = cardExpiration; CardSecurityNumber =\", \" cardSecurityNumber; CardTypeId = cardTypeId; CardExpiration = cardExpiration; } public class OrderI\", \"temDTO { public int ProductId { get; set; } public string ProductName { get; set; } public decimal U\", \"nitPrice { get; set; } public decimal Discount { get; set; } public int Units { get; set; } public s\", \"tring PictureUrl { get; set; } } }\\n```\\n\\nBasically, the command class contains all the data you need \", \"for performing a business transaction by using the domain model objects. Thus, commands are simply d\", \"ata structures that contain read-only data, and no behavior. The command's name indicates its purpos\", \"e. In many languages like C#, commands are represented as classes, but they are not true classes in \", \"the real object-oriented sense.\\n\\nAs an additional characteristic, commands are immutable, because th\", \"e expected usage is that they are processed directly by the domain model. They do not need to change\", \" during their projected lifetime. In a C# class, immutability can be achieved by not having any sett\", \"ers or other methods that change the internal state.\\n\\nKeep in mind that if you intend or expect comm\", \"ands to go through a serializing/deserializing process, the properties must have a private setter, a\", \"nd the [DataMember] (or [JsonProperty]) attribute. Otherwise, the deserializer won't be able to reco\", \"nstruct the object at the destination with the required values. You can also use truly read-only pro\", \"perties if the class has a constructor with parameters for all properties, with the usual camelCase \", \"naming convention, and annotate the constructor as [JsonConstructor]. However, this option requires \", \"more code.\\n\\nFor example, the command class for creating an order is probably similar in terms of dat\", \"a to the order you want to create, but you probably do not need the same attributes. For instance, C\", \"reateOrderCommand does not have an order ID, because the order has not been created yet.\\n\\nMany comma\", \"nd classes can be simple, requiring only a few fields about some state that needs to be changed. Tha\", \"t would be the case if you are just changing the status of an order from \\\"in process\\\" to \\\"paid\\\" or \\\"\", \"shipped\\\" by using a command similar to the following:\\n\\n```\\n[DataContract] public class UpdateOrderSt\", \"atusCommand :IRequest<bool> { [DataMember] public string Status { get; private set; } [DataMember] p\", \"ublic string OrderId { get; private set; } [DataMember] public string BuyerIdentityGuid { get; priva\", \"te set; } }\\n```\\n\\nSome developers make their UI request objects separate from their command DTOs, but\", \" that is just a matter of preference. It is a tedious separation with not much additional value, and\", \" the objects are almost exactly the same shape. For instance, in eShopOnContainers, some commands co\", \"me directly from the client -side.\\n\\n## The Command handler class\\n\\nYou should implement a specific co\", \"mmand handler class for each command. That is how the pattern works, and it's where you'll use the c\", \"ommand object, the domain objects, and the infrastructure repository objects. The command handler is\", \" in fact the heart of the application layer in terms of CQRS and DDD. However, all the domain logic \", \"should be contained in the domain classes\\u2014within the aggregate roots (root entities), child entities\", \", or domain services, but not within the command handler, which is a class from the application laye\", \"r.\\n\\nThe command handler class offers a strong stepping stone in the way to achieve the Single Respon\", \"sibility Principle (SRP) mentioned in a previous section.\\n\\nA command handler receives a command and \", \"obtains a result from the aggregate that is used. The result should be either successful execution o\", \"f the command, or an exception. In the case of an exception, the system state should be unchanged.\\n\\n\", \"The command handler usually takes the following steps:\\n\\n- It receives the command object, like a DTO\", \" (from the mediator or other infrastructure object).\\n- It validates that the command is valid (if no\", \"t validated by the mediator).\\n- It instantiates the aggregate root instance that is the target of th\", \"e current command.\\n- It executes the method on the aggregate root instance, getting the required dat\", \"a from the command.\\n- It persists the new state of the aggregate to its related database. This last \", \"operation is the actual transaction.\\n\\nTypically, a command handler deals with a single aggregate dri\", \"ven by its aggregate root (root entity). If multiple aggregates should be impacted by the reception \", \"of a single command, you could use domain events to propagate states or actions across multiple aggr\", \"egates.\\n\\nThe important point here is that when a command is being processed, all the domain logic sh\", \"ould be inside the domain model (the aggregates), fully encapsulated and ready for unit testing. The\", \" command handler just acts as a way to get the domain model from the database, and as the final step\", \", to tell the infrastructure layer (repositories) to persist the changes when the model is changed. \", \"The advantage of this approach is that you can refactor the domain logic in an isolated, fully encap\", \"sulated, rich, behavioral domain model without changing code in the application or infrastructure la\", \"yers, which are the plumbing level (command handlers, Web API, repositories, etc.).\\n\\nWhen command ha\", \"ndlers get complex, with too much logic, that can be a code smell. Review them, and if you find doma\", \"in logic, refactor the code to move that domain behavior to the methods of the domain objects (the a\", \"ggregate root and child entity).\\n\\nAs an example of a command handler class, the following code shows\", \" the same CreateOrderCommandHandler class that you saw at the beginning of this chapter. In this cas\", \"e, it also highlights the Handle method and the operations with the domain model objects/aggregates.\", \"\\n\\n```\\npublic class CreateOrderCommandHandler : IRequestHandler<CreateOrderCommand , bool> { private \", \"readonly IOrderRepository _orderRepository; private readonly IIdentityService _identityService; priv\", \"ate readonly IMediator _mediator; private readonly IOrderingIntegrationEventService _orderingIntegra\", \"tionEventService; private readonly ILogger<CreateOrderCommandHandler> _logger; // Using DI to inject\", \" infrastructure persistence Repositories public CreateOrderCommandHandler(IMediator mediator , IOrde\", \"ringIntegrationEventService orderingIntegrationEventService , IOrderRepository orderRepository ,\\n```\", \"\\n\\n```\\nIIdentityService identityService , ILogger<CreateOrderCommandHandler> logger) { _orderReposito\", \"ry = orderRepository ?? throw new ArgumentNullException(nameof(orderRepository)); _identityService =\", \" identityService ?? throw new ArgumentNullException(nameof(identityService)); _mediator = mediator ?\", \"? throw new ArgumentNullException(nameof(mediator)); _orderingIntegrationEventService = orderingInte\", \"grationEventService ?? throw new ArgumentNullException(nameof(orderingIntegrationEventService)); _lo\", \"gger = logger ?? throw new ArgumentNullException(nameof(logger)); } public async Task<bool> Handle(C\", \"reateOrderCommand message , CancellationToken cancellationToken) { // Add Integration event to clean\", \" the basket var orderStartedIntegrationEvent = new OrderStartedIntegrationEvent(message . UserId); a\", \"wait _orderingIntegrationEventService . AddAndSaveEventAsync(orderStartedIntegrationEvent); // Add/U\", \"pdate the Buyer AggregateRoot // DDD patterns comment: Add child entities and value-objects through \", \"the Order Aggregate -Root // methods and constructor so validations, invariants and business logic /\", \"/ make sure that consistency is preserved across the whole aggregate var address = new Address(messa\", \"ge . Street , message.City , message.State , message.Country , message.ZipCode); var order = new Ord\", \"er(message . UserId , message.UserName , address , message.CardTypeId , message.CardNumber , message\", \".CardSecurityNumber , message.CardHolderName , message.CardExpiration); foreach (var item in message\", \" . OrderItems) { order . AddOrderItem(item . ProductId , item . ProductName , item . UnitPrice , ite\", \"m . Discount , item . PictureUrl , item . Units); } _logger . LogInformation( \\\" -----Creating Order \", \"-Order: {@Order}\\\" , order); _orderRepository . Add(order); return await _orderRepository . UnitOfWor\", \"k . SaveEntitiesAsync(cancellationToken); } }\\n```\\n\\nThese are additional steps a command handler shou\", \"ld take:\\n\\n- Use the command's data to operate with the aggregate root's methods and behavior.\\n- Inte\", \"rnally within the domain objects, raise domain events while the transaction is executed, but that is\", \" transparent from a command handler point of view.\\n- If the aggregate's operation result is successf\", \"ul and after the transaction is finished, raise integration events. (These might also be raised by i\", \"nfrastructure classes like repositories.)\\n\\n## Additional resources\\n\\n- Mark Seemann. At the Boundarie\", \"s, Applications are Not Object-Oriented https://blog.ploeh.dk/2011/05/31/AttheBoundaries,Application\", \"sareNotObject-Oriented/\\n- Commands and events https://cqrs.nu/faq/Command%20and%20Events\\n- What does\", \" a command handler do? https://cqrs.nu/faq/Command%20Handlers\\n- Jimmy Bogard. Domain Command Pattern\", \"s \\u2013 Handlers https://jimmybogard.com/domain-command-patterns-handlers/\\n- Jimmy Bogard. Domain Comman\", \"d Patterns \\u2013 Validation https://jimmybogard.com/domain-command-patterns-validation/\\n\\n## The Command \", \"process pipeline: how to trigger a command handler\\n\\nThe next question is how to invoke a command han\", \"dler. You could manually call it from each related ASP.NET Core controller. However, that approach w\", \"ould be too coupled and is not ideal.\\n\\nThe other two main options, which are the recommended options\", \", are:\\n\\n- Through an in-memory Mediator pattern artifact.\\n- With an asynchronous message queue, in b\", \"etween controllers and handlers.\\n\\n## Use the Mediator pattern (in-memory) in the command pipeline\\n\\nA\", \"s shown in Figure 7-25, in a CQRS approach you use an intelligent mediator, similar to an in-memory \", \"bus, which is smart enough to redirect to the right command handler based on the type of the command\", \" or DTO being received. The single black arrows between components represent the dependencies betwee\", \"n objects (in many cases, injected through DI) with their related interactions.\\n\\nRunning as a contai\", \"ner\\n\\nASP.NET Core\\n\\nController\\n\\n(Application\\n\\nLayer)\\n\\nAggregates\\n\\nFigure 7 -25. Using the Mediator pa\", \"ttern in process in a single CQRS microservice\\n\\n<!-- image -->\\n\\nThe above diagram shows a zoom-in fr\", \"om image 7-24: the ASP.NET Core controller sends the command to MediatR's command pipeline, so they \", \"get to the appropriate handler.\\n\\nThe reason that using the Mediator pattern makes sense is that in e\", \"nterprise applications, the processing requests can get complicated. You want to be able to add an o\", \"pen number of crosscutting concerns like logging, validations, audit, and security. In these cases, \", \"you can rely on a mediator pipeline (see Mediator pattern) to provide a means for these extra behavi\", \"ors or crosscutting concerns.\\n\\nA mediator is an object that encapsulates the \\\"how\\\" of this process: \", \"it coordinates execution based on state, the way a command handler is invoked, or the payload you pr\", \"ovide to the handler. With a mediator component, you can apply cross-cutting concerns in a centraliz\", \"ed and transparent way by applying decorators (or pipeline behaviors since MediatR 3). For more info\", \"rmation, see the Decorator pattern .\\n\\nDecorators and behaviors are similar to Aspect Oriented Progra\", \"mming (AOP), only applied to a specific process pipeline managed by the mediator component. Aspects \", \"in AOP that implement crosscutting concerns are applied based on aspect weavers injected at compilat\", \"ion time or based on object call interception. Both typical AOP approaches are sometimes said to wor\", \"k \\\"like magic,\\\" because it is not easy to see how AOP does its work. When dealing with serious issue\", \"s or bugs, AOP can be difficult to debug. On the other hand, these decorators/behaviors are explicit\", \" and applied only in the context of the mediator, so debugging is much more predictable and easy.\\n\\nF\", \"or example, in the eShopOnContainers ordering microservice, has an implementation of two sample beha\", \"viors, a LogBehavior class and a ValidatorBehavior class. The implementation of the behaviors is exp\", \"lained in the next section by showing how eShopOnContainers uses MediatR behaviors .\\n\\nMicroservice\\n\\n\", \"(Writes-side of a simplified CQRS Architecture pattern)\\n\\nCommand\\n\\nCommand\\n\\n(Domain Model\\n\\nRunning as\", \" a container I\\n\\nWrites-side of a CQRS Architecture pattern using messaging\\n\\nWeb API\\n\\nmicroservice\\n\\nC\", \"ommand-Handlers backend microservice\\n\\nRunning as a container\\n\\n## Use message queues (out-of-proc) in\", \" the command\\u2019s pipeline\\n\\nAnother choice is to use asynchronous messages based on brokers or message \", \"queues, as shown in Figure 7-26. That option could also be combined with the mediator component righ\", \"t before the command handler. Handler\\n\\n(Application\\n\\nLayer)\\n\\n-\\n\\n(Application\\n\\n## Layer) HA Message Q\", \"ueue (External to the Repository (Infrastructure\\n\\nmicroservices)\\n\\nLayer)\\n\\nDatabase\\n\\nFigure 7 -26. Us\", \"ing message queues (out of the process and inter -process communication) with CQRS commands\\n\\n<!-- im\", \"age -->\\n\\nCommand's pipeline can also be handled by a high availability message queue to deliver the \", \"commands to the appropriate handler. Using message queues to accept the commands can further complic\", \"ate your command's pipeline, because you will probably need to split the pipeline into two processes\", \" connected through the external message queue. Still, it should be used if you need to have improved\", \" scalability and performance based on asynchronous messaging. Consider that in the case of Figure 7-\", \"26, the controller just posts the command message into the queue and returns. Then the command handl\", \"ers process the messages at their own pace. That is a great benefit of queues: the message queue can\", \" act as a buffer in cases when hyper scalability is needed, such as for stocks or any other scenario\", \" with a high volume of ingress data.\\n\\nHowever, because of the asynchronous nature of message queues,\", \" you need to figure out how to communicate with the client application about the success or failure \", \"of the command's process. As a rule, you should never use \\\"fire and forget\\\" commands. Every business\", \" application needs to know if a command was processed successfully, or at least validated and accept\", \"ed.\\n\\nThus, being able to respond to the client after validating a command message that was submitted\", \" to an asynchronous queue adds complexity to your system, as compared to an in-process command proce\", \"ss that returns the operation's result after running the transaction. Using queues, you might need t\", \"o return the result of the command process through other operation result messages, which will requi\", \"re additional components and custom communication in your system.\\n\\nAdditionally, async commands are \", \"one-way commands, which in many cases might not be needed, as is explained in the following interest\", \"ing exchange between Burtsev Alexey and Greg Young in an online conversation:\\n\\n[Burtsev Alexey] I fi\", \"nd lots of code where people use async command handling or one-way command messaging without any rea\", \"son to do so (they are not doing some long operation, they are not executing external async code, th\", \"ey do not even cross-application boundary to be using message bus). Why do they introduce this unnec\", \"essary complexity? And actually, I haven't seen a CQRS code example with blocking command handlers s\", \"o far, though it will work just fine in most cases.\\n\\n[Greg Young] [\\u2026] an asynchronous command doesn'\", \"t exist; it's actually another event. If I must accept what you send me and raise an event if I disa\", \"gree, it's no longer you telling me to do something [that is, it's not a command]. It's you telling \", \"me something has been done. This seems like a slight difference at first, but it has many implicatio\", \"ns.\\n\\nAsynchronous commands greatly increase the complexity of a system, because there is no simple w\", \"ay to indicate failures. Therefore, asynchronous commands are not recommended other than when scalin\", \"g requirements are needed or in special cases when communicating the internal microservices through \", \"messaging. In those cases, you must design a separate reporting and recovery system for failures.\\n\\nI\", \"n the initial version of eShopOnContainers, it was decided to use synchronous command processing, st\", \"arted from HTTP requests and driven by the Mediator pattern. That easily allows you to return the su\", \"ccess or failure of the process, as in the CreateOrderCommandHandler implementation.\\n\\nIn any case, t\", \"his should be a decision based on your application's or microservice's business requirements.\\n\\n## Im\", \"plement the command process pipeline with a mediator pattern (MediatR)\\n\\nAs a sample implementation, \", \"this guide proposes using the in-process pipeline based on the Mediator pattern to drive command ing\", \"estion and route commands, in memory, to the right command handlers. The guide also proposes applyin\", \"g behaviors in order to separate cross-cutting concerns.\\n\\nFor implementation in .NET, there are mult\", \"iple open-source libraries available that implement the Mediator pattern. The library used in this g\", \"uide is the MediatR open-source library (created by Jimmy Bogard), but you could use another approac\", \"h. MediatR is a small and simple library that allows you to process in-memory messages like a comman\", \"d, while applying decorators or behaviors.\\n\\nUsing the Mediator pattern helps you to reduce coupling \", \"and to isolate the concerns of the requested work, while automatically connecting to the handler tha\", \"t performs that work\\u2014in this case, to command handlers.\\n\\nAnother good reason to use the Mediator pat\", \"tern was explained by Jimmy Bogard when reviewing this guide:\\n\\nI think it might be worth mentioning \", \"testing here \\u2013 it provides a nice consistent window into the behavior of your system. Request-in, re\", \"sponse-out. We've found that aspect quite valuable in building consistently behaving tests.\\n\\nFirst, \", \"let's look at a sample WebAPI controller where you actually would use the mediator object. If you we\", \"ren't using the mediator object, you'd need to inject all the dependencies for that controller, thin\", \"gs like a logger object and others. Therefore, the constructor would be complicated. On the other ha\", \"nd, if you use the mediator object, the constructor of your controller can be a lot simpler, with ju\", \"st a few dependencies instead of many dependencies if you had one per cross-cutting operation, as in\", \" the following example:\\n\\n```\\npublic class MyMicroserviceController : Controller { public MyMicroserv\", \"iceController(IMediator mediator , IMyMicroserviceQueries microserviceQueries) { // ... } }\\n```\\n\\nYou\", \" can see that the mediator provides a clean and lean Web API controller constructor. In addition, wi\", \"thin the controller methods, the code to send a command to the mediator object is almost one line:\\n\\n\", \"```\\n[Route(\\\"new\\\")] [HttpPost] public async Task<IActionResult> ExecuteBusinessOperation([FromBody]Ru\", \"nOpCommand runOperationCommand) { var commandResult = await _mediator . SendAsync(runOperationComman\", \"d); return commandResult ? (IActionResult)Ok() : (IActionResult)BadRequest(); }\\n```\\n\\n## Implement id\", \"empotent Commands\\n\\nIn eShopOnContainers, a more advanced example than the above is submitting a Crea\", \"teOrderCommand object from the Ordering microservice. But since the Ordering business process is a b\", \"it more complex and, in our case, it actually starts in the Basket microservice, this action of subm\", \"itting the CreateOrderCommand object is performed from an integration-event handler named UserChecko\", \"utAcceptedIntegrationEventHandler instead of a simple WebAPI controller called from the client App a\", \"s in the previous simpler example.\\n\\nNevertheless, the action of submitting the Command to MediatR is\", \" pretty similar, as shown in the following code.\\n\\n```\\nvar createOrderCommand = new CreateOrderComman\", \"d(eventMsg . Basket . Items , eventMsg . UserId , eventMsg . City , eventMsg . Street , eventMsg . S\", \"tate , eventMsg . Country , eventMsg . ZipCode , eventMsg . CardNumber , eventMsg . CardHolderName ,\", \" eventMsg . CardExpiration , eventMsg . CardSecurityNumber , eventMsg . CardTypeId);\\n```\\n\\n```\\nvar re\", \"questCreateOrder = new IdentifiedCommand<CreateOrderCommand , bool>(createOrderCommand , eventMsg . \", \"RequestId); result = await _mediator . Send(requestCreateOrder);\\n```\\n\\nHowever, this case is also sli\", \"ghtly more advanced because we're also implementing idempotent commands. The CreateOrderCommand proc\", \"ess should be idempotent, so if the same message comes duplicated through the network, because of an\", \"y reason, like retries, the same business order will be processed just once.\\n\\nThis is implemented by\", \" wrapping the business command (in this case CreateOrderCommand) and embedding it into a generic Ide\", \"ntifiedCommand, which is tracked by an ID of every message coming through the network that has to be\", \" idempotent.\\n\\nIn the code below, you can see that the IdentifiedCommand is nothing more than a DTO w\", \"ith and ID plus the wrapped business command object.\\n\\n```\\npublic class IdentifiedCommand<T , R> : IR\", \"equest<R> where T : IRequest<R> { public T Command { get; } public Guid Id { get; } public Identifie\", \"dCommand(T command , Guid id) { Command = command; Id = id; } }\\n```\\n\\nThen the CommandHandler for the\", \" IdentifiedCommand named IdentifiedCommandHandler.cs will basically check if the ID coming as part o\", \"f the message already exists in a table. If it already exists, that command won't be processed again\", \", so it behaves as an idempotent command. That infrastructure code is performed by the \\\\_requestMana\", \"ger.ExistAsync method call below.\\n\\n```\\n// IdentifiedCommandHandler.cs public class IdentifiedCommand\", \"Handler<T , R> : IRequestHandler<IdentifiedCommand<T , R>, R> where T : IRequest<R> { private readon\", \"ly IMediator _mediator; private readonly IRequestManager _requestManager; private readonly ILogger<I\", \"dentifiedCommandHandler<T , R>> _logger; public IdentifiedCommandHandler( IMediator mediator , IRequ\", \"estManager requestManager , ILogger<IdentifiedCommandHandler<T , R>> logger) { _mediator = mediator;\", \" _requestManager = requestManager; _logger = logger ?? throw new System . ArgumentNullException(name\", \"of(logger)); } /// <summary> /// Creates the result value to return if a previous request was found \", \"/// </summary> /// <returns></returns>\\n```\\n\\n```\\nprotected virtual R CreateResultForDuplicateRequest(\", \") { return default(R); } /// <summary> /// This method handles the command. It just ensures that no \", \"other request exists with the same ID, and if this is the case /// just enqueues the original inner \", \"command. /// </summary> /// <param name=\\\"message\\\">IdentifiedCommand which contains both original com\", \"mand & request ID</param> /// <returns>Return value of inner command or default value if request sam\", \"e ID was found</returns> public async Task<R> Handle(IdentifiedCommand<T , R> message , Cancellation\", \"Token cancellationToken) { var alreadyExists = await _requestManager . ExistAsync(message . Id); if \", \"(alreadyExists) { return CreateResultForDuplicateRequest(); } else { await _requestManager . CreateR\", \"equestForCommandAsync<T>(message . Id); try { var command = message . Command; var commandName = com\", \"mand . GetGenericTypeName(); var idProperty = string . Empty; var commandId = string . Empty; switch\", \" (command) { case CreateOrderCommand createOrderCommand: idProperty = nameof(createOrderCommand . Us\", \"erId); commandId = createOrderCommand . UserId; break; case CancelOrderCommand cancelOrderCommand: i\", \"dProperty = nameof(cancelOrderCommand . OrderNumber); commandId = $\\\"{cancelOrderCommand.OrderNumber}\", \"\\\"; break; case ShipOrderCommand shipOrderCommand: idProperty = nameof(shipOrderCommand . OrderNumber\", \"); commandId = $\\\"{shipOrderCommand.OrderNumber}\\\"; break; default: idProperty = \\\"Id?\\\"; commandId = \\\"n\", \"/a\\\"; break; } _logger . LogInformation( \\\" -----Sending command: {CommandName} -{IdProperty}: {Comman\", \"dId} ({@Command})\\\" , commandName , idProperty , commandId ,\\n```\\n\\n```\\ncommand); // Send the embedded \", \"business command to mediator so it runs its related CommandHandler var result = await _mediator . Se\", \"nd(command , cancellationToken); _logger . LogInformation( \\\" -----Command result: {@Result} -{Comman\", \"dName} -{IdProperty}: {CommandId} ({@Command})\\\" , result , commandName , idProperty , commandId , co\", \"mmand); return result; } catch { return default(R); } } } }\\n```\\n\\nSince the IdentifiedCommand acts li\", \"ke a business command's envelope, when the business command needs to be processed because it is not \", \"a repeated ID, then it takes that inner business command and resubmits it to Mediator, as in the las\", \"t part of the code shown above when running \\\\_mediator.Send(message.Command), from the IdentifiedCom\", \"mandHandler.cs .\\n\\nWhen doing that, it will link and run the business command handler, in this case, \", \"the CreateOrderCommandHandler, which is running transactions against the Ordering database, as shown\", \" in the following code.\\n\\n```\\n// CreateOrderCommandHandler.cs public class CreateOrderCommandHandler \", \": IRequestHandler<CreateOrderCommand , bool> { private readonly IOrderRepository _orderRepository; p\", \"rivate readonly IIdentityService _identityService; private readonly IMediator _mediator; private rea\", \"donly IOrderingIntegrationEventService _orderingIntegrationEventService; private readonly ILogger<Cr\", \"eateOrderCommandHandler> _logger; // Using DI to inject infrastructure persistence Repositories publ\", \"ic CreateOrderCommandHandler(IMediator mediator , IOrderingIntegrationEventService orderingIntegrati\", \"onEventService , IOrderRepository orderRepository , IIdentityService identityService , ILogger<Creat\", \"eOrderCommandHandler> logger) { _orderRepository = orderRepository ?? throw new ArgumentNullExceptio\", \"n(nameof(orderRepository)); _identityService = identityService ?? throw new ArgumentNullException(na\", \"meof(identityService)); _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator)); _\", \"orderingIntegrationEventService = orderingIntegrationEventService ?? throw new ArgumentNullException\", \"(nameof(orderingIntegrationEventService)); _logger = logger ?? throw new ArgumentNullException(nameo\", \"f(logger));\\n```\\n\\n```\\n} public async Task<bool> Handle(CreateOrderCommand message , CancellationToken\", \" cancellationToken) { // Add Integration event to clean the basket var orderStartedIntegrationEvent \", \"= new OrderStartedIntegrationEvent(message . UserId); await _orderingIntegrationEventService . AddAn\", \"dSaveEventAsync(orderStartedIntegrationEvent); // Add/Update the Buyer AggregateRoot // DDD patterns\", \" comment: Add child entities and value-objects through the Order Aggregate -Root // methods and cons\", \"tructor so validations, invariants and business logic // make sure that consistency is preserved acr\", \"oss the whole aggregate var address = new Address(message . Street , message.City , message.State , \", \"message.Country , message.ZipCode); var order = new Order(message . UserId , message.UserName , addr\", \"ess , message.CardTypeId , message.CardNumber , message.CardSecurityNumber , message.CardHolderName \", \", message.CardExpiration); foreach (var item in message . OrderItems) { order . AddOrderItem(item . \", \"ProductId , item . ProductName , item . UnitPrice , item . Discount , item . PictureUrl , item . Uni\", \"ts); } _logger . LogInformation( \\\" -----Creating Order -Order: {@Order}\\\" , order); _orderRepository \", \". Add(order); return await _orderRepository . UnitOfWork . SaveEntitiesAsync(cancellationToken); } }\", \"\\n```\\n\\n## Register the types used by MediatR\\n\\nIn order for MediatR to be aware of your command handle\", \"r classes, you need to register the mediator classes and the command handler classes in your IoC con\", \"tainer. By default, MediatR uses Autofac as the IoC container, but you can also use the built-in ASP\", \".NET Core IoC container or any other container supported by MediatR.\\n\\nThe following code shows how t\", \"o register Mediator's types and commands when using Autofac modules.\\n\\n```\\npublic class MediatorModul\", \"e : Autofac . Module { protected override void Load(ContainerBuilder builder) { builder . RegisterAs\", \"semblyTypes(typeof(IMediator).GetTypeInfo().Assembly) . AsImplementedInterfaces(); // Register all t\", \"he Command classes (they implement IRequestHandler) // in assembly holding the Commands builder . Re\", \"gisterAssemblyTypes(typeof(CreateOrderCommand).GetTypeInfo().Assembly) . AsClosedTypesOf(typeof(IReq\", \"uestHandler<,>)); // Other types registration\\n```\\n\\n```\\n//... } }\\n```\\n\\nThis is where \\u201cthe magic happe\", \"ns\\u201d with MediatR.\\n\\nAs each command handler implements the generic IRequestHandler&lt;T&gt; interface\", \", when you register the assemblies using RegisteredAssemblyTypes method all the types marked as IReq\", \"uestHandler also gets registered with their Commands. For example:\\n\\n```\\npublic class CreateOrderComm\", \"andHandler : IRequestHandler<CreateOrderCommand , bool> {\\n```\\n\\nThat is the code that correlates comm\", \"ands with command handlers. The handler is just a simple class, but it inherits from RequestHandler&\", \"lt;T&gt;, where T is the command type, and MediatR makes sure it is invoked with the correct payload\", \" (the command).\\n\\n## Apply cross-cutting concerns when processing commands with the Behaviors in Medi\", \"atR\\n\\nThere is one more thing: being able to apply cross-cutting concerns to the mediator pipeline. Y\", \"ou can also see at the end of the Autofac registration module code how it registers a behavior type,\", \" specifically, a custom LoggingBehavior class and a ValidatorBehavior class. But you could add other\", \" custom behaviors, too.\\n\\n```\\npublic class MediatorModule : Autofac . Module { protected override voi\", \"d Load(ContainerBuilder builder) { builder . RegisterAssemblyTypes(typeof(IMediator).GetTypeInfo().A\", \"ssembly) . AsImplementedInterfaces(); // Register all the Command classes (they implement IRequestHa\", \"ndler) // in assembly holding the Commands builder . RegisterAssemblyTypes( typeof(CreateOrderComman\", \"d).GetTypeInfo().Assembly). AsClosedTypesOf(typeof(IRequestHandler<,>)); // Other types registration\", \" //... builder . RegisterGeneric(typeof(LoggingBehavior<,>)). As(typeof(IPipelineBehavior<,>)); buil\", \"der . RegisterGeneric(typeof(ValidatorBehavior<,>)). As(typeof(IPipelineBehavior<,>)); } }\\n```\\n\\nThat\", \" LoggingBehavior class can be implemented as the following code, which logs information about the co\", \"mmand handler being executed and whether it was successful or not.\\n\\n```\\npublic class LoggingBehavior\", \"<TRequest , TResponse> : IPipelineBehavior<TRequest , TResponse> { private readonly ILogger<LoggingB\", \"ehavior<TRequest , TResponse>> _logger; public LoggingBehavior(ILogger<LoggingBehavior<TRequest , TR\", \"esponse>> logger) => _logger = logger;\\n```\\n\\n```\\npublic async Task<TResponse> Handle(TRequest request\", \" , RequestHandlerDelegate<TResponse> next) { _logger . LogInformation($\\\"Handling {typeof(TRequest).N\", \"ame}\\\"); var response = await next(); _logger . LogInformation($\\\"Handled {typeof(TResponse).Name}\\\"); \", \"return response; } }\\n```\\n\\nJust by implementing this behavior class and by registering it in the pipe\", \"line (in the MediatorModule above), all the commands processed through MediatR will be logging infor\", \"mation about the execution.\\n\\nThe eShopOnContainers ordering microservice also applies a second behav\", \"ior for basic validations, the ValidatorBehavior class that relies on the FluentValidation library, \", \"as shown in the following code:\\n\\n```\\npublic class ValidatorBehavior<TRequest , TResponse> : IPipelin\", \"eBehavior<TRequest , TResponse> { private readonly IValidator<TRequest>[] _validators; public Valida\", \"torBehavior(IValidator<TRequest>[] validators) => _validators = validators; public async Task<TRespo\", \"nse> Handle(TRequest request , RequestHandlerDelegate<TResponse> next) { var failures = _validators \", \". Select(v => v . Validate(request)) . SelectMany(result => result . Errors) . Where(error => error \", \"!= null) . ToList(); if (failures . Any()) { throw new OrderingDomainException( $\\\"Command Validation\", \" Errors for type {typeof(TRequest).Name}\\\" , new ValidationException(\\\"Validation exception\\\" , failure\", \"s)); } var response = await next(); return response; } }\\n```\\n\\nHere the behavior is raising an except\", \"ion if validation fails, but you could also return a result object, containing the command result if\", \" it succeeded or the validation messages in case it didn't. This would probably make it easier to di\", \"splay validation results to the user.\\n\\nThen, based on the FluentValidation library, you would create\", \" validation for the data passed with CreateOrderCommand, as in the following code:\\n\\n```\\npublic class\", \" CreateOrderCommandValidator : AbstractValidator<CreateOrderCommand> { public CreateOrderCommandVali\", \"dator() { RuleFor(command => command . City).NotEmpty(); RuleFor(command => command . Street).NotEmp\", \"ty();\\n```\\n\\n```\\nRuleFor(command => command . State).NotEmpty(); RuleFor(command => command . Country)\", \".NotEmpty(); RuleFor(command => command . ZipCode).NotEmpty(); RuleFor(command => command . CardNumb\", \"er).NotEmpty().Length(12 , 19); RuleFor(command => command . CardHolderName).NotEmpty(); RuleFor(com\", \"mand => command . CardExpiration).NotEmpty().Must(BeValidExpirationDate).WithMessage(\\\"Please specify\", \" a valid card expiration date\\\"); RuleFor(command => command . CardSecurityNumber).NotEmpty().Length(\", \"3); RuleFor(command => command . CardTypeId).NotEmpty(); RuleFor(command => command . OrderItems).Mu\", \"st(ContainOrderItems).WithMessage(\\\"No order items found\\\"); } private bool BeValidExpirationDate(Date\", \"Time dateTime) { return dateTime >= DateTime . UtcNow; } private bool ContainOrderItems(IEnumerable<\", \"OrderItemDTO> orderItems) { return orderItems . Any(); } }\\n```\\n\\nYou could create additional validati\", \"ons. This is a very clean and elegant way to implement your command validations.\\n\\nIn a similar way, \", \"you could implement other behaviors for additional aspects or cross-cutting concerns that you want t\", \"o apply to commands when handling them.\\n\\n## Additional resources\\n\\n## The mediator pattern\\n\\n- \\u2022\\n- Med\", \"iator pattern\\n\\nhttps://en.wikipedia.org/wiki/Mediator\\\\_pattern\\n\\n## The decorator pattern\\n\\n- \\u2022\\n- Deco\", \"rator pattern https://en.wikipedia.org/wiki/Decorator\\\\_pattern\\n\\n## MediatR (Jimmy Bogard)\\n\\n- \\u2022\\n- \\u2022\\n-\", \" MediatR. GitHub repo.\\n\\nhttps://github.com/jbogard/MediatR\\n\\n- CQRS with MediatR and AutoMapper\\n\\nhttp\", \"s://lostechies.com/jimmybogard/2015/05/05/cqrs-with-mediatr-and-automapper/\\n\\n- Put your controllers \", \"on a diet: POSTs and commands.\\n\\nhttps://lostechies.com/jimmybogard/2013/12/19/put-your-controllers-o\", \"n-a-diet-posts-andcommands/\\n\\n- Tackling cross-cutting concerns with a mediator pipeline https://lost\", \"echies.com/jimmybogard/2014/09/09/tackling-cross-cutting-concerns-with-amediator -pipeline/\\n- CQRS a\", \"nd REST: the perfect match https://lostechies.com/jimmybogard/2016/06/01/cqrs-and-rest-the-perfect-m\", \"atch/\\n- MediatR Pipeline Examples https://lostechies.com/jimmybogard/2016/10/13/mediatr-pipeline-exa\", \"mples/\\n- Vertical Slice Test Fixtures for MediatR and ASP.NET Core https://lostechies.com/jimmybogar\", \"d/2016/10/24/vertical-slice-test-fixtures-for-mediatr-andasp -net -core/\\n- MediatR Extensions for Mi\", \"crosoft Dependency Injection Released https://lostechies.com/jimmybogard/2016/07/19/mediatr-extensio\", \"ns-for-microsoftdependency-injection-released/\\n\\n## Fluent validation\\n\\n- Jeremy Skinner. FluentValida\", \"tion. GitHub repo. https://github.com/JeremySkinner/FluentValidation\\n\\n## Implement resilient applica\", \"tions\\n\\nYour microservice and cloud -based applications must embrace the partial failures that will c\", \"ertainly occur eventually. You must design your application to be resilient to those partial failure\", \"s.\\n\\nResiliency is the ability to recover from failures and continue to function. It isn't about avoi\", \"ding failures but accepting the fact that failures will happen and responding to them in a way that \", \"avoids downtime or data loss. The goal of resiliency is to return the application to a fully functio\", \"ning state after a failure.\\n\\nIt's challenging enough to design and deploy a microservices-based appl\", \"ication. But you also need to keep your application running in an environment where some sort of fai\", \"lure is certain. Therefore, your application should be resilient. It should be designed to cope with\", \" partial failures, like network outages or nodes or VMs crashing in the cloud. Even microservices (c\", \"ontainers) being moved to a different node within a cluster can cause intermittent short failures wi\", \"thin the application.\\n\\nThe many individual components of your application should also incorporate he\", \"alth monitoring features. By following the guidelines in this chapter, you can create an application\", \" that can work smoothly in spite of transient downtime or the normal hiccups that occur in complex a\", \"nd cloud-based deployments.\\n\\n## Important\\n\\neShopOnContainer had been using the Polly library to impl\", \"ement resiliency using Typed Clients up until the release 3.0.0.\\n\\nStarting with release 3.0.0, the H\", \"TTP calls resiliency is implemented using a Linkerd mesh, that handles retries in a transparent and \", \"configurable fashion, within a Kubernetes cluster, without having to handle those concerns in the co\", \"de.\\n\\nThe Polly library is still used to add resilience to database connections, specially while star\", \"ting up the services.\\n\\n## Warning\\n\\nAll code samples and images in this section were valid before usi\", \"ng Linkerd and are not updated to reflect the current actual code. So they make sense in the context\", \" of this section.\\n\\nMultiple\\n\\nCustomers\\n\\nBrowser\\n\\nSubmit Order page r - -\\n\\n| Backend\\n\\n## Handle parti\", \"al failure\\n\\nASP.NET Core\\n\\nRunning as\\n\\nIn distributed systems like microservices-based applications, \", \"there's an ever-present risk of partial failure. For instance, a single microservice/container can f\", \"ail or might not be available to respond for a short time, or a single VM or server can crash. Since\", \" clients and services are separate processes, a service might not be able to respond in a timely way\", \" to a client's request. The service might be overloaded and responding very slowly to requests or mi\", \"ght simply not be accessible for a short time because of network issues. ad n Container\\n\\nThread pool\", \"\\n\\nFor example, consider the Order details page from the eShopOnContainers sample application. If the\", \" ordering microservice is unresponsive when the user tries to submit an order, a bad implementation \", \"of the client process (the MVC web application)\\u2014for example, if the client code were to use synchron\", \"ous RPCs with no timeout\\u2014would block threads indefinitely waiting for a response. Besides creating a\", \" bad user experience, every unresponsive wait consumes or blocks a thread, and threads are extremely\", \" valuable in highly scalable applications. If there are many blocked threads, eventually the applica\", \"tion's runtime can run out of threads. In that case, the application can become globally unresponsiv\", \"e instead of just partially unresponsive, as shown in Figure 8-1.\\n\\nFigure 8 -1. Partial failures bec\", \"ause of dependencies that impact service thread availability\\n\\n<!-- image -->\\n\\nIn a large microservic\", \"es-based application, any partial failure can be amplified, especially if most of the internal micro\", \"services interaction is based on synchronous HTTP calls (which is considered an antipattern). Think \", \"about a system that receives millions of incoming calls per day. If your system has a bad design tha\", \"t's based on long chains of synchronous HTTP calls, these incoming calls might result in many more m\", \"illions of outgoing calls (let's suppose a ratio of 1:4) to dozens of internal microservices as sync\", \"hronous dependencies. This situation is shown in Figure 8-2, especially dependency #3, that starts a\", \" chain, calling dependency #4, which then calls #5.\\n\\nPartial failures\\n\\nRunning as\\n\\nContainer\\n\\nMultip\", \"le\\n\\nCustomers\\n\\nMultiple\\n\\nCustomers\\n\\nBrowser\\n\\nBrowser\\n\\nSubmit Order page\\n\\nSubmit Order page\\n\\nMultiple\", \" distributed dependencies\\n\\nPartial Failure Amplified in Microservices\\n\\n- -\\n\\n| Backend / Orchestrator\", \"\\n\\n| Backend / Orchestrator\\n\\nWeb App\\n\\n<!-- image -->\\n\\nDependency\\n\\nDependency\\n\\nFigure 8 -2. The impact\", \" of having an incorrect design featuring long chains of HTTP requests\\n\\nIntermittent failure is guara\", \"nteed in a distributed and cloud-based system, even if every dependency itself has excellent availab\", \"ility. It's a fact you need to consider.\\n\\nIf you do not design and implement techniques to ensure fa\", \"ult tolerance, even small downtimes can be amplified. As an example, 50 dependencies each with 99.99\", \"% of availability would result in several hours of downtime each month because of this ripple effect\", \". When a microservice dependency fails while handling a high volume of requests, that failure can qu\", \"ickly saturate all available request threads in each service and crash the whole application.\\n\\nFigur\", \"e 8 -3. Partial failure amplified by microservices with long chains of synchronous HTTP calls\\n\\n<!-- \", \"image -->\\n\\nTo minimize this problem, in the section Asynchronous microservice integration enforce mi\", \"croservice's autonomy, this guide encourages you to use asynchronous communication across the intern\", \"al microservices.\\n\\nIn addition, it's essential that you design your microservices and client applica\", \"tions to handle partial failures \\u2014 that is, to build resilient microservices and client applications\", \".\\n\\n## Strategies to handle partial failure\\n\\nTo deal with partial failures, use one of the strategies\", \" described here.\\n\\nUse asynchronous communication (for example, message-based communication) across i\", \"nternal microservices. It's highly advisable not to create long chains of synchronous HTTP calls acr\", \"oss the internal microservices because that incorrect design will eventually become the main cause o\", \"f bad outages. On the contrary, except for the front-end communications between the client applicati\", \"ons and the first level of microservices or fine-grained API Gateways, it's recommended to use only \", \"asynchronous (message-based) communication once past the initial request/response cycle, across the \", \"internal microservices. Eventual consistency and event-driven architectures will help to minimize ri\", \"pple effects. These approaches enforce a higher level of microservice autonomy and therefore prevent\", \" against the problem noted here.\\n\\nUse retries with exponential backoff. This technique helps to avoi\", \"d short and intermittent failures by performing call retries a certain number of times, in case the \", \"service was not available only for a short time. This might occur due to intermittent network issues\", \" or when a microservice/container is moved to a different node in a cluster. However, if these retri\", \"es are not designed properly with circuit breakers, it can aggravate the ripple effects, ultimately \", \"even causing a Denial of Service (DoS) .\\n\\nWork around network timeouts. In general, clients should b\", \"e designed not to block indefinitely and to always use timeouts when waiting for a response. Using t\", \"imeouts ensures that resources are never tied up indefinitely.\\n\\nUse the Circuit Breaker pattern. In \", \"this approach, the client process tracks the number of failed requests. If the error rate exceeds a \", \"configured limit, a \\\"circuit breaker\\\" trips so that further attempts fail immediately. (If a large n\", \"umber of requests are failing, that suggests the service is unavailable and that sending requests is\", \" pointless.) After a timeout period, the client should try again and, if the new requests are succes\", \"sful, close the circuit breaker.\\n\\nProvide fallbacks. In this approach, the client process performs f\", \"allback logic when a request fails, such as returning cached data or a default value. This is an app\", \"roach suitable for queries, and is more complex for updates or commands.\\n\\nLimit the number of queued\", \" requests. Clients should also impose an upper bound on the number of outstanding requests that a cl\", \"ient microservice can send to a particular service. If the limit has been reached, it's probably poi\", \"ntless to make additional requests, and those attempts should fail immediately. In terms of implemen\", \"tation, the Polly Bulkhead Isolation policy can be used to fulfill this requirement. This approach i\", \"s essentially a parallelization throttle with SemaphoreSlim as the implementation. It also permits a\", \" \\\"queue\\\" outside the bulkhead. You can proactively shed excess load even before execution (for examp\", \"le, because capacity is deemed full). This makes its response to\\n\\ncertain failure scenarios faster t\", \"han a circuit breaker would be, since the circuit breaker waits for the failures. The BulkheadPolicy\", \" object in Polly exposes how full the bulkhead and queue are, and offers events on overflow so can a\", \"lso be used to drive automated horizontal scaling.\\n\\n## Additional resources\\n\\n- Resiliency patterns\\n\\n\", \"https://learn.microsoft.com/azure/architecture/framework/resiliency/reliability-patterns\\n\\n- Adding R\", \"esilience and Optimizing Performance\\n\\nhttps://learn.microsoft.com/previous-versions/msp-n-p/jj591574\", \"(v=pandp.10)\\n\\n- Bulkhead. GitHub repo. Implementation with Polly policy. https://github.com/App-vNex\", \"t/Polly/wiki/Bulkhead\\n- Designing resilient applications for Azure\\n\\nhttps://learn.microsoft.com/azur\", \"e/architecture/framework/resiliency/app-design\\n\\n- Transient fault handling https://learn.microsoft.c\", \"om/azure/architecture/best-practices/transient-faults\\n\\n## Implement retries with exponential backoff\", \"\\n\\nRetries with exponential backoff is a technique that retries an operation, with an exponentially i\", \"ncreasing wait time, up to a maximum retry count has been reached (the exponential backoff). This te\", \"chnique embraces the fact that cloud resources might intermittently be unavailable for more than a f\", \"ew seconds for any reason. For example, an orchestrator might be moving a container to another node \", \"in a cluster for load balancing. During that time, some requests might fail. Another example could b\", \"e a database like SQL Azure, where a database can be moved to another server for load balancing, cau\", \"sing the database to be unavailable for a few seconds.\\n\\nThere are many approaches to implement retri\", \"es logic with exponential backoff.\\n\\n## Implement resilient Entity Framework Core SQL connections\\n\\nFo\", \"r Azure SQL DB, Entity Framework (EF) Core already provides internal database connection resiliency \", \"and retry logic. But you need to enable the Entity Framework execution strategy for each DbContext c\", \"onnection if you want to have resilient EF Core connections .\\n\\nFor instance, the following code at t\", \"he EF Core connection level enables resilient SQL connections that are retried if the connection fai\", \"ls.\\n\\n```\\n// Program.cs from any ASP.NET Core Web API // Other code ... builder . Services . AddDbCon\", \"text<CatalogContext>(options => {\\n```\\n\\n```\\noptions . UseSqlServer(builder . Configuration[\\\"Connectio\", \"nString\\\"], sqlServerOptionsAction: sqlOptions => { sqlOptions . EnableRetryOnFailure( maxRetryCount:\", \" 10 , maxRetryDelay: TimeSpan . FromSeconds(30), errorNumbersToAdd: null); }); });\\n```\\n\\n## Execution\", \" strategies and explicit transactions using BeginTransaction and multiple DbContexts\\n\\nWhen retries a\", \"re enabled in EF Core connections, each operation you perform using EF Core becomes its own retryabl\", \"e operation. Each query and each call to SaveChanges will be retried as a unit if a transient failur\", \"e occurs.\\n\\nHowever, if your code initiates a transaction using BeginTransaction, you're defining you\", \"r own group of operations that need to be treated as a unit. Everything inside the transaction has t\", \"o be rolled back if a failure occurs.\\n\\nIf you try to execute that transaction when using an EF execu\", \"tion strategy (retry policy) and you call SaveChanges from multiple DbContexts, you'll get an except\", \"ion like this one:\\n\\nSystem.InvalidOperationException: The configured execution strategy\\n\\n'SqlServerR\", \"etryingExecutionStrategy' does not support user initiated transactions. Use the execution strategy r\", \"eturned by 'DbContext.Database.CreateExecutionStrategy()' to execute all the operations in the trans\", \"action as a retriable unit.\\n\\nThe solution is to manually invoke the EF execution strategy with a del\", \"egate representing everything that needs to be executed. If a transient failure occurs, the executio\", \"n strategy will invoke the delegate again. For example, the following code shows how it's implemente\", \"d in eShopOnContainers with two multiple DbContexts (\\\\_catalogContext and the IntegrationEventLogCon\", \"text) when updating a product and then saving the ProductPriceChangedIntegrationEvent object, which \", \"needs to use a different DbContext.\\n\\n```\\npublic async Task<IActionResult> UpdateProduct( [FromBody]C\", \"atalogItem productToUpdate) { // Other code ... var oldPrice = catalogItem . Price; var raiseProduct\", \"PriceChangedEvent = oldPrice != productToUpdate . Price; // Update current product catalogItem = pro\", \"ductToUpdate; // Save product's data and publish integration event through the Event Bus // if price\", \" has changed if (raiseProductPriceChangedEvent) { //Create Integration Event to be published through\", \" the Event Bus var priceChangedEvent = new ProductPriceChangedIntegrationEvent( catalogItem . Id , p\", \"roductToUpdate . Price , oldPrice);\\n```\\n\\n```\\n// Achieving atomicity between original Catalog databas\", \"e operation and the // IntegrationEventLog thanks to a local transaction await _catalogIntegrationEv\", \"entService . SaveEventAndCatalogContextChangesAsync( priceChangedEvent); // Publish through the Even\", \"t Bus and mark the saved event as published await _catalogIntegrationEventService . PublishThroughEv\", \"entBusAsync( priceChangedEvent); } // Just save the updated product because the Product's Price hasn\", \"'t changed. else { await _catalogContext . SaveChangesAsync(); } }\\n```\\n\\nThe first DbContext is \\\\_cat\", \"alogContext and the second DbContext is within the \\\\_catalogIntegrationEventService object. The Comm\", \"it action is performed across all DbContext objects using an EF execution strategy.\\n\\nTo achieve this\", \" multiple DbContext commit, the SaveEventAndCatalogContextChangesAsync uses a ResilientTransaction c\", \"lass, as shown in the following code:\\n\\n```\\npublic class CatalogIntegrationEventService : ICatalogInt\", \"egrationEventService { //\\u2026 public async Task SaveEventAndCatalogContextChangesAsync( IntegrationEven\", \"t evt) { // Use of an EF Core resiliency strategy when using multiple DbContexts // within an explic\", \"it BeginTransaction(): // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency aw\", \"ait ResilientTransaction . New(_catalogContext).ExecuteAsync(async () => { // Achieving atomicity be\", \"tween original catalog database // operation and the IntegrationEventLog thanks to a local transacti\", \"on await _catalogContext . SaveChangesAsync(); await _eventLogService . SaveEventAsync(evt , _catalo\", \"gContext . Database . CurrentTransaction . GetDbTransaction()); }); } }\\n```\\n\\nThe ResilientTransactio\", \"n.ExecuteAsync method basically begins a transaction from the passed DbContext (\\\\_catalogContext) an\", \"d then makes the EventLogService use that transaction to save changes from the IntegrationEventLogCo\", \"ntext and then commits the whole transaction.\\n\\n```\\npublic class ResilientTransaction { private DbCon\", \"text _context; private ResilientTransaction(DbContext context) => _context = context ?? throw new Ar\", \"gumentNullException(nameof(context)); public static ResilientTransaction New (DbContext context) => \", \"new ResilientTransaction(context); public async Task ExecuteAsync(Func<Task> action) {\\n```\\n\\n```\\n// U\", \"se of an EF Core resiliency strategy when using multiple DbContexts // within an explicit BeginTrans\", \"action(): // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency var strategy = \", \"_context . Database . CreateExecutionStrategy(); await strategy . ExecuteAsync(async () => { await u\", \"sing var transaction = await _context . Database . BeginTransactionAsync(); await action(); await tr\", \"ansaction . CommitAsync(); }); } }\\n```\\n\\n## Additional resources\\n\\n- Connection Resiliency and Command\", \" Interception with EF in an ASP.NET MVC Application\\n\\nhttps://learn.microsoft.com/aspnet/mvc/overview\", \"/getting-started/getting-started-with-efusing-mvc/connection-resiliency-and-command-interception-wit\", \"h-the-entity-framework-inan-asp -net -mvc-application\\n\\n- Cesar de la Torre. Using Resilient Entity F\", \"ramework Core SQL Connections and Transactions\\n\\nhttps://devblogs.microsoft.com/cesardelatorre/using-\", \"resilient-entity-framework-core-sqlconnections -and -transactions -retries -with -exponential-backof\", \"f/\\n\\n## Use IHttpClientFactory to implement resilient HTTP requests\\n\\nIHttpClientFactory is a contract\", \" implemented by DefaultHttpClientFactory, an opinionated factory, available since .NET Core 2.1, for\", \" creating HttpClient instances to be used in your applications.\\n\\n## Issues with the original HttpCli\", \"ent class available in .NET\\n\\nThe original and well-known HttpClient class can be easily used, but in\", \" some cases, it isn't being properly used by many developers.\\n\\nThough this class implements IDisposa\", \"ble, declaring and instantiating it within a using statement is not preferred because when the HttpC\", \"lient object gets disposed of, the underlying socket is not immediately released, which can lead to \", \"a socket exhaustion problem. For more information about this issue, see the blog post You're using H\", \"ttpClient wrong and it's destabilizing your software .\\n\\nTherefore, HttpClient is intended to be inst\", \"antiated once and reused throughout the life of an application. Instantiating an HttpClient class fo\", \"r every request will exhaust the number of sockets available under heavy loads. That issue will resu\", \"lt in SocketException errors. Possible approaches to solve that problem are based on the creation of\", \" the HttpClient object as singleton or static, as explained in this Microsoft article on HttpClient \", \"usage. This can be a good solution for short-lived console apps or similar, that run a few times a d\", \"ay.\\n\\nAnother issue that developers run into is when using a shared instance of HttpClient in long-ru\", \"nning processes. In a situation where the HttpClient is instantiated as a singleton or a static obje\", \"ct, it fails to handle the DNS changes as described in this issue of the dotnet/runtime GitHub repos\", \"itory.\\n\\nHowever, the issue isn't really with HttpClient per se, but with the default constructor for\", \" HttpClient , because it creates a new concrete instance of HttpMessageHandler, which is the one tha\", \"t has sockets exhaustion and DNS changes issues mentioned above.\\n\\nTo address the issues mentioned ab\", \"ove and to make HttpClient instances manageable, .NET Core 2.1 introduced two approaches, one of the\", \"m being IHttpClientFactory. It's an interface that's used to configure and create HttpClient instanc\", \"es in an app through Dependency Injection (DI). It also provides extensions for Polly-based middlewa\", \"re to take advantage of delegating handlers in HttpClient.\\n\\nThe alternative is to use SocketsHttpHan\", \"dler with configured PooledConnectionLifetime. This approach is applied to long-lived, static or sin\", \"gleton HttpClient instances. To learn more about different strategies, see HttpClient guidelines for\", \" .NET .\\n\\nPolly is a transient-fault-handling library that helps developers add resiliency to their a\", \"pplications, by using some pre-defined policies in a fluent and thread-safe manner.\\n\\n## Benefits of \", \"using IHttpClientFactory\\n\\nThe current implementation of IHttpClientFactory, that also implements IHt\", \"tpMessageHandlerFactory , offers the following benefits:\\n\\n- Provides a central location for naming a\", \"nd configuring logical HttpClient objects. For example, you may configure a client (Service Agent) t\", \"hat's pre-configured to access a specific microservice.\\n- Codify the concept of outgoing middleware \", \"via delegating handlers in HttpClient and implementing Polly-based middleware to take advantage of P\", \"olly's policies for resiliency.\\n- HttpClient already has the concept of delegating handlers that cou\", \"ld be linked together for outgoing HTTP requests. You can register HTTP clients into the factory and\", \" you can use a Polly handler to use Polly policies for Retry, CircuitBreakers, and so on.\\n- Manage t\", \"he lifetime of HttpMessageHandler to avoid the mentioned problems/issues that can occur when managin\", \"g HttpClient lifetimes yourself.\\n\\n## Tip\\n\\nThe HttpClient instances injected by DI can be disposed of\", \" safely, because the associated HttpMessageHandler is managed by the factory. Injected HttpClient in\", \"stances are Transient from a DI perspective, while HttpMessageHandler instances can be regarded as S\", \"coped. HttpMessageHandler instances have their own DI scopes, separate from the application scopes (\", \"for example, ASP.NET incoming request scopes). For more information, see Using HttpClientFactory in \", \".NET .\\n\\n## Note\\n\\nThe implementation of IHttpClientFactory (DefaultHttpClientFactory) is tightly tied\", \" to the DI implementation in the Microsoft.Extensions.DependencyInjection NuGet package. If you need\", \" to use HttpClient without DI or with other DI implementations, consider using a static or singleton\", \" HttpClient with PooledConnectionLifetime set up. For more information, see HttpClient guidelines fo\", \"r .NET .\\n\\n## Multiple ways to use IHttpClientFactory\\n\\nThere are several ways that you can use IHttpC\", \"lientFactory in your application:\\n\\n- Basic usage\\n- Use Named Clients\\n- Use Typed Clients\\n- Use Gener\", \"ated Clients\\n\\nFor the sake of brevity, this guidance shows the most structured way to use IHttpClien\", \"tFactory, which is to use Typed Clients (Service Agent pattern). However, all options are documented\", \" and are currently listed in this article covering the IHttpClientFactory usage .\\n\\n## Note\\n\\nIf your \", \"app requires cookies, it might be better to avoid using IHttpClientFactory in your app. For alternat\", \"ive ways of managing clients, see Guidelines for using HTTP clients\\n\\n## How to use Typed Clients wit\", \"h IHttpClientFactory\\n\\nSo, what's a \\\"Typed Client\\\"? It's just an HttpClient that's pre-configured for\", \" some specific use. This configuration can include specific values such as the base server, HTTP hea\", \"ders or time outs.\\n\\nThe following diagram shows how Typed Clients are used with IHttpClientFactory:\\n\", \"\\nClient application/code\\n\\nDependency Injection\\n\\nU\\n\\nController or client cod\\n\\nFigure 8 -4. Using IHtt\", \"pClientFactory with Typed Client classes.\\n\\n<!-- image -->\\n\\nIn the above image, a ClientService (used\", \" by a controller or client code) uses an HttpClient created by the registered IHttpClientFactory. Th\", \"is factory assigns an HttpMessageHandler from a pool to the HttpClient. The HttpClient can be config\", \"ured with Polly's policies when registering the IHttpClientFactory in the DI container with the exte\", \"nsion method AddHttpClient .\\n\\nTo configure the above structure, add IHttpClientFactory in your appli\", \"cation by installing the Microsoft.Extensions.Http NuGet package that includes the AddHttpClient ext\", \"ension method for IServiceCollection. This extension method registers the internal DefaultHttpClient\", \"Factory class to be used as a singleton for the interface IHttpClientFactory. It defines a transient\", \" configuration for the HttpMessageHandlerBuilder. This message handler (HttpMessageHandler object), \", \"taken from a pool, is used by the HttpClient returned from the factory.\\n\\nIn the next snippet, you ca\", \"n see how AddHttpClient() can be used to register Typed Clients (Service Agents) that need to use Ht\", \"tpClient.\\n\\n```\\n// Program.cs //Add http client services at ConfigureServices(IServiceCollection serv\", \"ices) builder . Services . AddHttpClient<ICatalogService , CatalogService>(); builder . Services . A\", \"ddHttpClient<IBasketService , BasketService>(); builder . Services . AddHttpClient<IOrderingService \", \", OrderingService>();\\n```\\n\\nRegistering the client services as shown in the previous snippet, makes t\", \"he DefaultClientFactory create a standard HttpClient for each service. The typed client is registere\", \"d as transient with DI container. In the preceding code, AddHttpClient() registers CatalogService , \", \"BasketService , OrderingService as transient services so they can be injected and consumed directly \", \"without any need for additional registrations.\\n\\nYou could also add instance -specific configuration \", \"in the registration to, for example, configure the base address, and add some resiliency policies, a\", \"s shown in the following:\\n\\n```\\nbuilder . Services . AddHttpClient<ICatalogService , CatalogService>(\", \"client => { client . BaseAddress = new Uri(builder . Configuration[\\\"BaseUrl\\\"]); }) . AddPolicyHandle\", \"r(GetRetryPolicy()) . AddPolicyHandler(GetCircuitBreakerPolicy());\\n```\\n\\nIn this next example, you ca\", \"n see the configuration of one of the above policies:\\n\\n```\\nstatic IAsyncPolicy<HttpResponseMessage> \", \"GetRetryPolicy() { return HttpPolicyExtensions . HandleTransientHttpError() . OrResult(msg => msg . \", \"StatusCode == System . Net . HttpStatusCode . NotFound) . WaitAndRetryAsync(6 , retryAttempt => Time\", \"Span . FromSeconds(Math . Pow(2 , retryAttempt))); }\\n```\\n\\nYou can find more details about using Poll\", \"y in the Next article .\\n\\n## HttpClient lifetimes\\n\\nEach time you get an HttpClient object from the IH\", \"ttpClientFactory, a new instance is returned. But each HttpClient uses an HttpMessageHandler that's \", \"pooled and reused by the IHttpClientFactory to reduce resource consumption, as long as the HttpMessa\", \"geHandler's lifetime hasn't expired.\\n\\nPooling of handlers is desirable as each handler typically man\", \"ages its own underlying HTTP connections; creating more handlers than necessary can result in connec\", \"tion delays. Some handlers also keep connections open indefinitely, which can prevent the handler fr\", \"om reacting to DNS changes.\\n\\nThe HttpMessageHandler objects in the pool have a lifetime that's the l\", \"ength of time that an HttpMessageHandler instance in the pool can be reused. The default value is tw\", \"o minutes, but it can be overridden per Typed Client. To override it, call SetHandlerLifetime() on t\", \"he IHttpClientBuilder that's returned when creating the client, as shown in the following code:\\n\\n```\", \"\\n//Set 5 min as the lifetime for the HttpMessageHandler objects in the pool used for the Catalog Typ\", \"ed Client builder . Services . AddHttpClient<ICatalogService , CatalogService>() . SetHandlerLifetim\", \"e(TimeSpan . FromMinutes(5));\\n```\\n\\nEach Typed Client can have its own configured handler lifetime va\", \"lue. Set the lifetime to InfiniteTimeSpan to disable handler expiry.\\n\\n## Implement your Typed Client\", \" classes that use the injected and configured HttpClient\\n\\nAs a previous step, you need to have your \", \"Typed Client classes defined, such as the classes in the sample code, like 'BasketService', 'Catalog\", \"Service', 'OrderingService', etc. \\u2013 A Typed Client is a class that accepts an HttpClient object (inj\", \"ected through its constructor) and uses it to call some remote HTTP service. For example:\\n\\n```\\npubli\", \"c class CatalogService : ICatalogService { private readonly HttpClient _httpClient; private readonly\", \" string _remoteServiceBaseUrl; public CatalogService(HttpClient httpClient) { _httpClient = httpClie\", \"nt; } public async Task<Catalog> GetCatalogItems(int page , int take , int? brand , int? type) { var\", \" uri = API . Catalog . GetAllCatalogItems(_remoteServiceBaseUrl , page, take , brand , type); var re\", \"sponseString = await _httpClient . GetStringAsync(uri); var catalog = JsonConvert . DeserializeObjec\", \"t<Catalog>(responseString); return catalog; } }\\n```\\n\\nThe Typed Client (CatalogService in the example\", \") is activated by DI (Dependency Injection), which means it can accept any registered service in its\", \" constructor, in addition to HttpClient.\\n\\nA Typed Client is effectively a transient object, that mea\", \"ns a new instance is created each time one is needed. It receives a new HttpClient instance each tim\", \"e it's constructed. However, the HttpMessageHandler objects in the pool are the objects that are reu\", \"sed by multiple HttpClient instances.\\n\\n## Use your Typed Client classes\\n\\nFinally, once you have your\", \" typed classes implemented, you can have them registered and configured with AddHttpClient(). After \", \"that you can use them wherever services are injected by DI, such as in Razor page code or an MVC web\", \" app controller, shown in the below code from eShopOnContainers:\\n\\n```\\nnamespace Microsoft . eShopOnC\", \"ontainers . WebMVC . Controllers { public class CatalogController : Controller { private ICatalogSer\", \"vice _catalogSvc; public CatalogController(ICatalogService catalogSvc) => _catalogSvc = catalogSvc; \", \"public async Task<IActionResult> Index(int? BrandFilterApplied , int? TypesFilterApplied ,\\n```\\n\\n```\\n\", \"int? page , [FromQuery]string errorMsg) { var itemsPage = 10; var catalog = await _catalogSvc . GetC\", \"atalogItems(page ?? 0 , itemsPage , BrandFilterApplied , TypesFilterApplied); //\\u2026 Additional code } \", \"} }\\n```\\n\\nUp to this point, the above code snippet only shows the example of performing regular HTTP \", \"requests. But the 'magic' comes in the following sections where it shows how all the HTTP requests m\", \"ade by HttpClient can have resilient policies such as retries with exponential backoff, circuit brea\", \"kers, security features using auth tokens, or even any other custom feature. And all of these can be\", \" done just by adding policies and delegating handlers to your registered Typed Clients.\\n\\n## Addition\", \"al resources\\n\\n- HttpClient guidelines for .NET\\n- https://learn.microsoft.com/en-us/dotnet/fundamenta\", \"ls/networking/http/httpclientguidelines\\n- Using HttpClientFactory in .NET https://learn.microsoft.co\", \"m/en-us/dotnet/core/extensions/httpclient-factory\\n- Using HttpClientFactory in ASP.NET Core https://\", \"learn.microsoft.com/aspnet/core/fundamentals/http-requests\\n- HttpClientFactory source code in the do\", \"tnet/runtime GitHub repository\\n\\nhttps://github.com/dotnet/runtime/tree/release/7.0/src/libraries/Mic\", \"rosoft.Extensions.Http/\\n\\n- Polly (.NET resilience and transient-fault-handling library) https://thep\", \"ollyproject.azurewebsites.net/\\n\\n## Implement HTTP call retries with exponential backoff with IHttpCl\", \"ientFactory and Polly policies\\n\\nThe recommended approach for retries with exponential backoff is to \", \"take advantage of more advanced .NET libraries like the open-source Polly library .\\n\\nPolly is a .NET\", \" library that provides resilience and transient-fault handling capabilities. You can implement those\", \" capabilities by applying Polly policies such as Retry, Circuit Breaker, Bulkhead Isolation, Timeout\", \", and Fallback. Polly targets .NET Framework 4.x and .NET Standard 1.0, 1.1, and 2.0 (which supports\", \" .NET Core and later).\\n\\nThe following steps show how you can use Http retries with Polly integrated \", \"into IHttpClientFactory, which is explained in the previous section.\\n\\n## Install .NET packages\\n\\nFirs\", \"t, you will need to install the Microsoft.Extensions.Http.Polly package.\\n\\n- Install with Visual Stud\", \"io\\n- Install with dotnet CLI\\n- Install with nuget.exe CLI\\n- Install with Package Manager Console (Po\", \"werShell)\\n\\n## Reference the .NET 7 packages\\n\\nIHttpClientFactory is available since .NET Core 2.1, ho\", \"wever, we recommend you use the latest .NET 7 packages from NuGet in your project. You typically als\", \"o need to reference the extension package Microsoft.Extensions.Http.Polly.\\n\\n## Configure a client wi\", \"th Polly\\u2019s Retry policy, in app startup\\n\\nThe AddPolicyHandler() method is what adds policies to the \", \"HttpClient objects you'll use. In this case, it's adding a Polly's policy for Http Retries with expo\", \"nential backoff.\\n\\nTo have a more modular approach, the Http Retry Policy can be defined in a separat\", \"e method within the Program.cs file, as shown in the following code:\\n\\n```\\nstatic IAsyncPolicy<HttpRe\", \"sponseMessage> GetRetryPolicy() { return HttpPolicyExtensions . HandleTransientHttpError() . OrResul\", \"t(msg => msg . StatusCode == System . Net . HttpStatusCode . NotFound) . WaitAndRetryAsync(6 , retry\", \"Attempt => TimeSpan . FromSeconds(Math . Pow(2 , retryAttempt))); }\\n```\\n\\nAs shown in previous sectio\", \"ns, you need to define a named or typed client HttpClient configuration in your standard Program.cs \", \"app configuration. Now you add incremental code specifying the policy for the Http retries with expo\", \"nential backoff, as follows:\\n\\n```\\n// Program.cs builder . Services . AddHttpClient<IBasketService , \", \"BasketService>() . SetHandlerLifetime(TimeSpan . FromMinutes(5)) //Set lifetime to five minutes . Ad\", \"dPolicyHandler(GetRetryPolicy());\\n```\\n\\nWith Polly, you can define a Retry policy with the number of \", \"retries, the exponential backoff configuration, and the actions to take when there's an HTTP excepti\", \"on, such as logging the error. In this case, the policy is configured to try six times with an expon\", \"ential retry, starting at two seconds.\\n\\n## Add a jitter strategy to the retry policy\\n\\nA regular Retr\", \"y policy can affect your system in cases of high concurrency and scalability and under high contenti\", \"on. To overcome peaks of similar retries coming from many clients in partial outages, a good workaro\", \"und is to add a jitter strategy to the retry algorithm/policy. This strategy can improve the overall\", \" performance of the end-to-end system. As recommended in Polly: Retry with Jitter, a good\\n\\njitter st\", \"rategy can be implemented by smooth and evenly distributed retry intervals applied with a well -cont\", \"rolled median initial retry delay on an exponential backoff. This approach helps to spread out the s\", \"pikes when the issue arises. The principle is illustrated by the following example:\\n\\n```\\nvar delay =\", \" Backoff . DecorrelatedJitterBackoffV2(medianFirstRetryDelay: TimeSpan . FromSeconds(1), retryCount:\", \" 5); var retryPolicy = Policy . Handle<FooException>() . WaitAndRetryAsync(delay);\\n```\\n\\n## Additiona\", \"l resources\\n\\n- Retry pattern https://learn.microsoft.com/azure/architecture/patterns/retry\\n- Polly a\", \"nd IHttpClientFactory https://github.com/App-vNext/Polly/wiki/Polly-andHttpClientFactory\\n- Polly (.N\", \"ET resilience and transient-fault-handling library) https://github.com/AppvNext/Polly\\n- Polly: Retry\", \" with Jitter https://github.com/App-vNext/Polly/wiki/Retry-with-jitter\\n- Marc Brooker. Jitter: Makin\", \"g Things Better With Randomness https://brooker.co.za/blog/2015/03/21/backoff.html\\n\\n## Implement the\", \" Circuit Breaker pattern\\n\\nAs noted earlier, you should handle faults that might take a variable amou\", \"nt of time to recover from, as might happen when you try to connect to a remote service or resource.\", \" Handling this type of fault can improve the stability and resiliency of an application.\\n\\nIn a distr\", \"ibuted environment, calls to remote resources and services can fail due to transient faults, such as\", \" slow network connections and timeouts, or if resources are responding slowly or are temporarily una\", \"vailable. These faults typically correct themselves after a short time, and a robust cloud applicati\", \"on should be prepared to handle them by using a strategy like the \\\"Retry pattern\\\".\\n\\nHowever, there c\", \"an also be situations where faults are due to unanticipated events that might take much longer to fi\", \"x. These faults can range in severity from a partial loss of connectivity to the complete failure of\", \" a service. In these situations, it might be pointless for an application to continually retry an op\", \"eration that's unlikely to succeed.\\n\\nInstead, the application should be coded to accept that the ope\", \"ration has failed and handle the failure accordingly.\\n\\nUsing Http retries carelessly could result in\", \" creating a Denial of Service (DoS) attack within your own software. As a microservice fails or perf\", \"orms slowly, multiple clients might repeatedly retry failed requests. That creates a dangerous risk \", \"of exponentially increasing traffic targeted at the failing service.\\n\\nTherefore, you need some kind \", \"of defense barrier so that excessive requests stop when it isn't worth to keep trying. That defense \", \"barrier is precisely the circuit breaker.\\n\\nThe Circuit Breaker pattern has a different purpose than \", \"the \\\"Retry pattern\\\". The \\\"Retry pattern\\\" enables an application to retry an operation in the expecta\", \"tion that the operation will eventually succeed. The Circuit Breaker pattern prevents an application\", \" from performing an operation that's likely to fail. An application can combine these two patterns. \", \"However, the retry logic should be sensitive to any exception returned by the circuit breaker, and i\", \"t should abandon retry attempts if the circuit breaker indicates that a fault is not transient.\\n\\n## \", \"Implement Circuit Breaker pattern with IHttpClientFactory and Polly\\n\\nAs when implementing retries, t\", \"he recommended approach for circuit breakers is to take advantage of proven .NET libraries like Poll\", \"y and its native integration with IHttpClientFactory.\\n\\nAdding a circuit breaker policy into your IHt\", \"tpClientFactory outgoing middleware pipeline is as simple as adding a single incremental piece of co\", \"de to what you already have when using IHttpClientFactory.\\n\\nThe only addition here to the code used \", \"for HTTP call retries is the code where you add the Circuit Breaker policy to the list of policies t\", \"o use, as shown in the following incremental code.\\n\\n```\\n// Program.cs var retryPolicy = GetRetryPoli\", \"cy(); var circuitBreakerPolicy = GetCircuitBreakerPolicy(); builder . Services . AddHttpClient<IBask\", \"etService , BasketService>() . SetHandlerLifetime(TimeSpan . FromMinutes(5)) // Sample: default life\", \"time is 2 minutes . AddHttpMessageHandler<HttpClientAuthorizationDelegatingHandler>() . AddPolicyHan\", \"dler(retryPolicy) . AddPolicyHandler(circuitBreakerPolicy);\\n```\\n\\nThe AddPolicyHandler() method is wh\", \"at adds policies to the HttpClient objects you'll use. In this case, it's adding a Polly policy for \", \"a circuit breaker.\\n\\nTo have a more modular approach, the Circuit Breaker Policy is defined in a sepa\", \"rate method called GetCircuitBreakerPolicy(), as shown in the following code:\\n\\n```\\n// also in Progra\", \"m.cs static IAsyncPolicy<HttpResponseMessage> GetCircuitBreakerPolicy() { return HttpPolicyExtension\", \"s . HandleTransientHttpError() . CircuitBreakerAsync(5 , TimeSpan . FromSeconds(30)); }\\n```\\n\\nIn the \", \"code example above, the circuit breaker policy is configured so it breaks or opens the circuit when \", \"there have been five consecutive faults when retrying the Http requests. When that happens, the circ\", \"uit will break for 30 seconds: in that period, calls will be failed immediately by the circuitbreake\", \"r rather than actually be placed. The policy automatically interprets relevant exceptions and HTTP s\", \"tatus codes as faults.\\n\\nCircuit breakers should also be used to redirect requests to a fallback infr\", \"astructure if you had issues in a particular resource that's deployed in a different environment tha\", \"n the client application or\\n\\nservice that's performing the HTTP call. That way, if there's an outage\", \" in the datacenter that impacts only your backend microservices but not your client applications, th\", \"e client applications can redirect to the fallback services. Polly is planning a new policy to autom\", \"ate this failover policy scenario.\\n\\nAll those features are for cases where you're managing the failo\", \"ver from within the .NET code, as opposed to having it managed automatically for you by Azure, with \", \"location transparency.\\n\\nFrom a usage point of view, when using HttpClient, there's no need to add an\", \"ything new here because the code is the same than when using HttpClient with IHttpClientFactory, as \", \"shown in previous sections.\\n\\n## Test Http retries and circuit breakers in eShopOnContainers\\n\\nWheneve\", \"r you start the eShopOnContainers solution in a Docker host, it needs to start multiple containers. \", \"Some of the containers are slower to start and initialize, like the SQL Server container. This is es\", \"pecially true the first time you deploy the eShopOnContainers application into Docker because it nee\", \"ds to set up the images and the database. The fact that some containers start slower than others can\", \" cause the rest of the services to initially throw HTTP exceptions, even if you set dependencies bet\", \"ween containers at the docker -compose level, as explained in previous sections. Those dockercompose\", \" dependencies between containers are just at the process level. The container's entry point process \", \"might be started, but SQL Server might not be ready for queries. The result can be a cascade of erro\", \"rs, and the application can get an exception when trying to consume that particular container.\\n\\nYou \", \"might also see this type of error on startup when the application is deploying to the cloud. In that\", \" case, orchestrators might be moving containers from one node or VM to another (that is, starting ne\", \"w instances) when balancing the number of containers across the cluster's nodes.\\n\\nThe way 'eShopOnCo\", \"ntainers' solves those issues when starting all the containers is by using the Retry pattern illustr\", \"ated earlier.\\n\\n## Test the circuit breaker in eShopOnContainers\\n\\nThere are a few ways you can break/\", \"open the circuit and test it with eShopOnContainers.\\n\\nOne option is to lower the allowed number of r\", \"etries to 1 in the circuit breaker policy and redeploy the whole solution into Docker. With a single\", \" retry, there's a good chance that an HTTP request will fail during deployment, the circuit breaker \", \"will open, and you get an error.\\n\\nAnother option is to use custom middleware that's implemented in t\", \"he Basket microservice. When this middleware is enabled, it catches all HTTP requests and returns st\", \"atus code 500. You can enable the middleware by making a GET request to the failing URI, like the fo\", \"llowing:\\n\\n- GET http://localhost:5103/failing This request returns the current state of the middlewa\", \"re. If the middleware is enabled, the request return status code 500. If the middleware is disabled,\", \" there's no response.\\n- GET http://localhost:5103/failing?enable This request enables the middleware\", \".\\n\\n- GET http://localhost:5103/failing?disable This request disables the middleware.\\n\\nFor instance, \", \"once the application is running, you can enable the middleware by making a request using the followi\", \"ng URI in any browser. Note that the ordering microservice uses port 5103.\\n\\nhttp://localhost:5103/fa\", \"iling?enable\\n\\nYou can then check the status using the URI http://localhost:5103/failing, as shown in\", \" Figure 8-5.\\n\\nFigure 8 -5. Checking the state of the \\\"Failing\\\" ASP.NET middleware \\u2013 In this case, di\", \"sabled.\\n\\n<!-- image -->\\n\\nAt this point, the Basket microservice responds with status code 500 whenev\", \"er you call invoke it.\\n\\nOnce the middleware is running, you can try making an order from the MVC web\", \" application. Because the requests fail, the circuit will open.\\n\\nIn the following example, you can s\", \"ee that the MVC web application has a catch block in the logic for placing an order. If the code cat\", \"ches an open-circuit exception, it shows the user a friendly message telling them to wait.\\n\\n```\\npubl\", \"ic class CartController : Controller { //\\u2026 public async Task<IActionResult> Index() { try { var user\", \" = _appUserParser . Parse(HttpContext . User); //Http requests using the Typed Client (Service Agent\", \") var vm = await _basketSvc . GetBasket(user); return View(vm); } catch (BrokenCircuitException) { /\", \"/ Catches error when Basket.api is in circuit-opened mode HandleBrokenCircuitException(); } return V\", \"iew(); } private void HandleBrokenCircuitException() { TempData[\\\"BasketInoperativeMsg\\\"] = \\\"Basket Se\", \"rvice is inoperative, please try later on. (Business message due to Circuit-Breaker)\\\"; } }\\n```\\n\\nHere\", \"'s a summary. The Retry policy tries several times to make the HTTP request and gets HTTP errors. Wh\", \"en the number of retries reaches the maximum number set for the Circuit Breaker policy (in this case\", \", 5), the application throws a BrokenCircuitException. The result is a friendly message, as shown in\", \" Figure 8-6.\\n\\nMy Cart - Microsoft.eSh X\\n\\n+ v\\n\\n\\u2022 localhost:5100/Cart\\n\\ne\\n\\neSHOP\\n\\nonCONTAINERS\\n\\nBACK TO\", \" CATALOG\\n\\nBasket Service is inoperative, please try later on. (Business Msg Due to Circuit-Breaker)\\n\", \"\\n\\u2022 *\\n\\ndemouser@microsoft.com sa\\n\\nFigure 8 -6. Circuit breaker returning an error to the UI\\n\\n<!-- ima\", \"ge -->\\n\\nYou can implement different logic for when to open/break the circuit. Or you can try an HTTP\", \" request against a different back-end microservice if there's a fallback datacenter or redundant bac\", \"k-end system.\\n\\nFinally, another possibility for the CircuitBreakerPolicy is to use Isolate (which fo\", \"rces open and holds open the circuit) and Reset (which closes it again). These could be used to buil\", \"d a utility HTTP endpoint that invokes Isolate and Reset directly on the policy. Such an HTTP endpoi\", \"nt could also be used, suitably secured, in production for temporarily isolating a downstream system\", \", such as when you want to upgrade it. Or it could trip the circuit manually to protect a downstream\", \" system you suspect to be faulting.\\n\\n## Additional resources\\n\\n- \\u2022\\n- Circuit Breaker pattern https://\", \"learn.microsoft.com/azure/architecture/patterns/circuit-breaker\\n\\n## Health monitoring\\n\\nHealth monito\", \"ring can allow near-real-time information about the state of your containers and microservices. Heal\", \"th monitoring is critical to multiple aspects of operating microservices and is especially important\", \" when orchestrators perform partial application upgrades in phases, as explained later.\\n\\nMicroservic\", \"es -based applications often use heartbeats or health checks to enable their performance monitors, s\", \"chedulers, and orchestrators to keep track of the multitude of services. If services cannot send som\", \"e sort of \\\"I'm alive\\\" signal, either on demand or on a schedule, your application might face risks w\", \"hen you deploy updates, or it might just detect failures too late and not be able to stop cascading \", \"failures that can end up in major outages.\\n\\nIn the typical model, services send reports about their \", \"status, and that information is aggregated to provide an overall view of the state of health of your\", \" application. If you're using an orchestrator, you can provide health information to your orchestrat\", \"or's cluster, so that the cluster can act accordingly. If you invest in high-quality health reportin\", \"g that's customized for your application, you can detect and fix issues for your running application\", \" much more easily.\\n\\n-\\n\\n## Implement health checks in ASP.NET Core services\\n\\nWhen developing an ASP.N\", \"ET Core microservice or web application, you can use the built-in health checks feature that was rel\", \"eased in ASP .NET Core 2.2\\n\\n(Microsoft.Extensions.Diagnostics.HealthChecks). Like many ASP.NET Core \", \"features, health checks come with a set of services and a middleware.\\n\\nHealth check services and mid\", \"dleware are easy to use and provide capabilities that let you validate if any external resource need\", \"ed for your application (like a SQL Server database or a remote API) is working properly. When you u\", \"se this feature, you can also decide what it means that the resource is healthy, as we explain later\", \".\\n\\nTo use this feature effectively, you need to first configure services in your microservices. Seco\", \"nd, you need a front -end application that queries for the health reports. That front-end applicatio\", \"n could be a custom reporting application, or it could be an orchestrator itself that can react acco\", \"rdingly to the health states.\\n\\n## Use the HealthChecks feature in your back-end ASP.NET microservice\", \"s\\n\\nIn this section, you'll learn how to implement the HealthChecks feature in a sample ASP.NET Core \", \"7.0 Web API application when using the Microsoft.Extensions.Diagnostics.HealthChecks package. The Im\", \"plementation of this feature in a large-scale microservices like the eShopOnContainers is explained \", \"in the next section.\\n\\nTo begin, you need to define what constitutes a healthy status for each micros\", \"ervice. In the sample application, we define the microservice is healthy if its API is accessible vi\", \"a HTTP and its related SQL Server database is also available.\\n\\nIn .NET 7, with the built-in APIs, yo\", \"u can configure the services, add a Health Check for the microservice and its dependent SQL Server d\", \"atabase in this way:\\n\\n```\\n// Program.cs from .NET 7 Web API sample //... // Registers required servi\", \"ces for health checks builder . Services . AddHealthChecks() // Add a health check for a SQL Server \", \"database . AddCheck( \\\"OrderingDB-check\\\" , new SqlConnectionHealthCheck(builder . Configuration[\\\"Conn\", \"ectionString\\\"]), HealthStatus . Unhealthy , new string[] { \\\"orderingdb\\\" });\\n```\\n\\nIn the previous cod\", \"e, the services.AddHealthChecks() method configures a basic HTTP check that returns a status code 20\", \"0 with \\\"Healthy\\\". Further, the AddCheck() extension method configures a custom SqlConnectionHealthCh\", \"eck that checks the related SQL Database's health.\\n\\nThe AddCheck() method adds a new health check wi\", \"th a specified name and the implementation of type IHealthCheck. You can add multiple Health Checks \", \"using AddCheck method, so a microservice won't provide a \\\"healthy\\\" status until all its checks are h\", \"ealthy.\\n\\nSqlConnectionHealthCheck is a custom class that implements IHealthCheck, which takes a conn\", \"ection string as a constructor parameter and executes a simple query to check if the connection to t\", \"he SQL database is successful. It returns HealthCheckResult.Healthy() if the query was executed succ\", \"essfully and a FailureStatus with the actual exception when it fails.\\n\\n```\\n// Sample SQL Connection \", \"Health Check public class SqlConnectionHealthCheck : IHealthCheck { private const string DefaultTest\", \"Query = \\\"Select 1\\\"; public string ConnectionString { get; } public string TestQuery { get; } public \", \"SqlConnectionHealthCheck(string connectionString) : this(connectionString , testQuery: DefaultTestQu\", \"ery) { } public SqlConnectionHealthCheck(string connectionString , string testQuery) { ConnectionStr\", \"ing = connectionString ?? throw new ArgumentNullException(nameof(connectionString)); TestQuery = tes\", \"tQuery; } public async Task<HealthCheckResult> CheckHealthAsync(HealthCheckContext context , Cancell\", \"ationToken cancellationToken = default(CancellationToken)) { using (var connection = new SqlConnecti\", \"on(ConnectionString)) { try { await connection . OpenAsync(cancellationToken); if (TestQuery != null\", \") { var command = connection . CreateCommand(); command . CommandText = TestQuery; await command . E\", \"xecuteNonQueryAsync(cancellationToken); } } catch (DbException ex) { return new HealthCheckResult(st\", \"atus: context . Registration . FailureStatus , exception: ex); } } return HealthCheckResult . Health\", \"y(); } }\\n```\\n\\nNote that in the previous code, Select 1 is the query used to check the Health of the \", \"database. To monitor the availability of your microservices, orchestrators like Kubernetes periodica\", \"lly perform health checks by sending requests to test the microservices. It's important to keep your\", \" database queries efficient so that these operations are quick and don't result in a higher utilizat\", \"ion of resources.\\n\\nCatalog.API\\n\\n\\u2022 Connected Services\\n\\n** Dependencies\\n\\ni\\n\\nAnalyzers\\n\\nFinally, add a \", \"middleware that responds to the url path /hc:\\n\\n```\\n// Program.cs from .NET 7 Web Api sample app.MapH\", \"ealthChecks(\\\"/hc\\\");\\n```\\n\\nWhen the endpoint &lt;yourmicroservice&gt;/hc is invoked, it runs all the h\", \"ealth checks that are configured in the AddHealthChecks() method in the Startup class and shows the \", \"result.\\n\\nAutofac.Extensions.Dependencylnjection (5.0.1)\\n\\n## HealthChecks implementation in eShopOnCo\", \"ntainers\\n\\nMicroservices in eShopOnContainers rely on multiple services to perform its task. For exam\", \"ple, the Catalog.API microservice from eShopOnContainers depends on many services, such as Azure Blo\", \"b Storage, SQL Server, and RabbitMQ. Therefore, it has several health checks added using the AddChec\", \"k() method. For every dependent service, a custom IHealthCheck implementation that defines its respe\", \"ctive health status would need to be added.\\n\\nThe open-source project AspNetCore.Diagnostics.HealthCh\", \"ecks solves this problem by providing custom health check implementations for each of these enterpri\", \"se services, that are built on top of .NET 7. Each health check is available as an individual NuGet \", \"package that can be easily added to the project. eShopOnContainers uses them extensively in all its \", \"microservices.\\n\\nFor instance, in the Catalog.API microservice, the following NuGet packages were add\", \"ed:\\n\\nFigure 8 -7. Custom Health Checks implemented in Catalog.API using AspNetCore.Diagnostics.Healt\", \"hChecks\\n\\n<!-- image -->\\n\\nIn the following code, the health check implementations are added for each \", \"dependent service and then the middleware is configured:\\n\\n```\\n// Extension method from Catalog.api m\", \"icroservice // public static IServiceCollection AddCustomHealthCheck(this IServiceCollection service\", \"s , IConfiguration configuration) { var accountName = configuration . GetValue<string>(\\\"AzureStorage\", \"AccountName\\\"); var accountKey = configuration . GetValue<string>(\\\"AzureStorageAccountKey\\\"); var hcBu\", \"ilder = services . AddHealthChecks();\\n```\\n\\nP\\n\\n```\\nhcBuilder . AddSqlServer( configuration[\\\"Connectio\", \"nString\\\"], name: \\\"CatalogDB-check\\\" , tags: new string[] { \\\"catalogdb\\\" }); if (!string . IsNullOrEmpt\", \"y(accountName) && !string . IsNullOrEmpty(accountKey)) { hcBuilder . AddAzureBlobStorage( $\\\"DefaultE\", \"ndpointsProtocol=https;AccountName={accountName};AccountKey={accountKey};Endpoint Suffix=core.window\", \"s.net\\\" , name: \\\"catalog-storage-check\\\" , tags: new string[] { \\\"catalogstorage\\\" }); } if (configurati\", \"on . GetValue<bool>(\\\"AzureServiceBusEnabled\\\")) { hcBuilder . AddAzureServiceBusTopic( configuration[\", \"\\\"EventBusConnection\\\"], topicName: \\\"eshop_event_bus\\\" , name: \\\"catalog-servicebus-check\\\" , tags: new s\", \"tring[] { \\\"servicebus\\\" }); } else { hcBuilder . AddRabbitMQ( $\\\"amqp://{configuration[\\\"EventBusConnec\", \"tion\\\"]}\\\" , name: \\\"catalog-rabbitmqbus-check\\\" , tags: new string[] { \\\"rabbitmqbus\\\" }); } return servi\", \"ces; }\\n```\\n\\nFinally, add the HealthCheck middleware to listen to \\u201c/hc\\u201d endpoint:\\n\\n```\\n// HealthCheck\", \" middleware app.UseHealthChecks(\\\"/hc\\\" , new HealthCheckOptions() { Predicate = _ => true , ResponseW\", \"riter = UIResponseWriter . WriteHealthCheckUIResponse });\\n```\\n\\n## Query your microservices to report\", \" about their health status\\n\\nWhen you've configured health checks as described in this article and yo\", \"u have the microservice running in Docker, you can directly check from a browser if it's healthy. Yo\", \"u have to publish the container port in the Docker host, so you can access the container through the\", \" external Docker host IP or through host.docker.internal, as shown in figure 8-8.\\n\\n&lt; \\u2192\\n\\nL host.do\", \"cker.internal:5101/hc\\n\\nA Notsecure | host.docker.internal:5101/hc\\n\\n\\u2022 (\\\"status\\\": \\\"Healthy\\\", \\\"totalDur\", \"ation\\\": \\\"00:00:00.0035390\\\", \\\"entries\\\": (\\\"self\\\": (\\\"data\\\":\\n\\n{), \\\"duration\\\": \\\"00:00:00.0000017\\\"\\n\\n0), \\\"d\", \"uration\\\": \\\"00:00:00.0034585\\\".\\n\\n\\\"duration\\\" 0: 09:90. 0024642 status Healthy tags\\\": *load-Cacklegrabbi\", \"embus-sheck\\\": (\\\"data\\\":\\n\\nFigure 8 -8. Checking health status of a single service from a browser\\n\\n<!--\", \" image -->\\n\\nIn that test, you can see that the Catalog.API microservice (running on port 5101) is he\", \"althy, returning HTTP status 200 and status information in JSON. The service also checked the health\", \" of its SQL Server database dependency and RabbitMQ, so the health check reported itself as healthy.\", \"\\n\\n## Use watchdogs\\n\\nA watchdog is a separate service that can watch health and load across services,\", \" and report health about the microservices by querying with the HealthChecks library introduced earl\", \"ier. This can help prevent errors that would not be detected based on the view of a single service. \", \"Watchdogs also are a good place to host code that can perform remediation actions for known conditio\", \"ns without user interaction.\\n\\nThe eShopOnContainers sample contains a web page that displays sample \", \"health check reports, as shown in Figure 8-9. This is the simplest watchdog you could have since it \", \"only shows the state of the microservices and web applications in eShopOnContainers. Usually a watch\", \"dog also takes actions when it detects unhealthy states.\\n\\nFortunately, AspNetCore.Diagnostics.Health\", \"Checks also provides AspNetCore.HealthChecks.UI NuGet package that can be used to display the health\", \" check results from the configured URIs.\\n\\n|- Health Checks UI\\n\\n+\\n\\n&lt; lcalhost:5107/hc-ui#/healthch\", \"ecks\\n\\nE\\n\\n&amp; Health Checks\\n\\n\\u2022 Webhooks\\n\\nHealth Checks status\\n\\n(=)\\n\\nRefresh every\\n\\nm\\n\\n10\\n\\n-\\n\\na\\n\\nsec\", \"onds\\n\\nChange\\n\\nFigure 8 -9. Sample health check report in eShopOnContainers\\n\\n<!-- image -->\\n\\nIn summa\", \"ry, this watchdog service queries each microservice's \\\"/hc\\\" endpoint. This will execute all the heal\", \"th checks defined within it and return an overall health state depending on all those checks. The He\", \"althChecksUI is easy to consume with a few configuration entries and two lines of code that needs to\", \" be added into the Startup.cs of the watchdog service.\\n\\nSample configuration file for health check U\", \"I:\\n\\n```\\n// Configuration { \\\"HealthChecksUI\\\": { \\\"HealthChecks\\\": [ { \\\"Name\\\": \\\"Ordering HTTP Check\\\" , \\\"\", \"Uri\\\": \\\"http://host.docker.internal:5102/hc\\\" } , { \\\"Name\\\": \\\"Ordering HTTP Background Check\\\" , \\\"Uri\\\": \", \"\\\"http://host.docker.internal:5111/hc\\\" } , //... ]} }\\n```\\n\\nProgram.cs file that adds HealthChecksUI:\\n\", \"\\n```\\n// Program.cs from WebStatus(Watch Dog) service // // Registers required services for health ch\", \"ecks builder . Services . AddHealthChecksUI(); // build the app, register other middleware app.UseHe\", \"althChecksUI(config => config . UIPath = \\\"/hc-ui\\\");\\n```\\n\\n## Health checks when using orchestrators\\n\\n\", \"To monitor the availability of your microservices, orchestrators like Kubernetes and Service Fabric \", \"periodically perform health checks by sending requests to test the microservices. When an orchestrat\", \"or determines that a service/container is unhealthy, it stops routing requests to that instance. It \", \"also usually creates a new instance of that container.\\n\\nFor instance, most orchestrators can use hea\", \"lth checks to manage zero-downtime deployments. Only when the status of a service/container changes \", \"to healthy will the orchestrator start routing traffic to service/container instances.\\n\\nHealth monit\", \"oring is especially important when an orchestrator performs an application upgrade. Some orchestrato\", \"rs (like Azure Service Fabric) update services in phases\\u2014for example, they might update one-fifth of\", \" the cluster surface for each application upgrade. The set of nodes that's upgraded at the same time\", \" is referred to as an upgrade domain. After each upgrade domain has been upgraded and is available t\", \"o users, that upgrade domain must pass health checks before the deployment moves to the next upgrade\", \" domain.\\n\\nAnother aspect of service health is reporting metrics from the service. This is an advance\", \"d capability of the health model of some orchestrators, like Service Fabric. Metrics are important w\", \"hen using an orchestrator because they are used to balance resource usage. Metrics also can be an in\", \"dicator of system health. For example, you might have an application that has many microservices, an\", \"d each instance reports a requests-per-second (RPS) metric. If one service is using more resources (\", \"memory, processor, etc.) than another service, the orchestrator could move service instances around \", \"in the cluster to try to maintain even resource utilization.\\n\\nNote that Azure Service Fabric provide\", \"s its own Health Monitoring model, which is more advanced than simple health checks.\\n\\n## Advanced mo\", \"nitoring: visualization, analysis, and alerts\\n\\nThe final part of monitoring is visualizing the event\", \" stream, reporting on service performance, and alerting when an issue is detected. You can use diffe\", \"rent solutions for this aspect of monitoring.\\n\\nYou can use simple custom applications showing the st\", \"ate of your services, like the custom page shown when explaining the AspNetCore.Diagnostics.HealthCh\", \"ecks. Or you could use more advanced tools like Azure Monitor to raise alerts based on the stream of\", \" events.\\n\\nFinally, if you're storing all the event streams, you can use Microsoft Power BI or other \", \"solutions like Kibana or Splunk to visualize the data.\\n\\n## Additional resources\\n\\n- HealthChecks and \", \"HealthChecks UI for ASP.NET Core https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks\\n- In\", \"troduction to Service Fabric health monitoring https://learn.microsoft.com/azure/service-fabric/serv\", \"ice-fabric-health-introduction\\n- Azure Monitor https://azure.microsoft.com/services/monitor/\\n\\nClient\", \" Mobile App\\n\\n| Backend\\n\\nJ HTTP Request\\n\\nSign-in\\n\\nAPI Gateway\\n\\nASPNET Core\\n\\nWeb API\\n\\nRequest with use\", \"r information\\n\\nBasket Microservice\\n\\n101\\u2022\\n\\nSQL Server conta ner\\n\\nRedis cache\\n\\n## Make secure .NET Mic\", \"roservices and Web Applications\\n\\nThere are so many aspects about security in microservices and web a\", \"pplications that the topic could easily take several books like this one. So, in this section, we'll\", \" focus on authentication, authorization, and application secrets.\\n\\n## Implement authentication in .N\", \"ET microservices and web applications\\n\\nIt's often necessary for resources and APIs published by a se\", \"rvice to be limited to certain trusted users or clients. The first step to making these sorts of API\", \"-level trust decisions is authentication. Authentication is the process of reliably verifying a user\", \"'s identity.\\n\\nIn microservice scenarios, authentication is typically handled centrally. If you're us\", \"ing an API Gateway, the gateway is a good place to authenticate, as shown in Figure 9-1. If you use \", \"this approach, make sure that the individual microservices cannot be reached directly (without the A\", \"PI Gateway) unless additional security is in place to authenticate messages whether they come from t\", \"he gateway or not.\\n\\nFigure 9 -1. Centralized authentication with an API Gateway\\n\\n<!-- image -->\\n\\nWhe\", \"n the API Gateway centralizes authentication, it adds user information when forwarding requests to t\", \"he microservices. If services can be accessed directly, an authentication service like Azure Active\\n\", \"\\nCatalog Microservice\\n\\nClient Apps\\n\\nMobile\\n\\nApp\\n\\nWeb\\n\\nApp\\n\\nBackend Microservices\\n\\nSign-in\\n\\nSecurity \", \"token\\n\\nIdentity Microservice (STS + Users)\\n\\nSQL Server database\\n\\nDirectory or a dedicated authentica\", \"tion microservice acting as a security token service (STS) can be used to authenticate users. Trust \", \"decisions are shared between services with security tokens or cookies. (These tokens can be shared b\", \"etween ASP.NET Core applications, if needed, by implementing cookie sharing.) This pattern is illust\", \"rated in Figure 9-2. database\\n\\nFigure 9 -2. Authentication by identity microservice; trust is shared\", \" using an authorization token\\n\\n<!-- image -->\\n\\nWhen microservices are accessed directly, trust, that\", \" includes authentication and authorization, is handled by a security token issued by a dedicated mic\", \"roservice, shared between microservices.\\n\\n## Authenticate with ASP.NET Core Identity\\n\\nThe primary me\", \"chanism in ASP.NET Core for identifying an application's users is the ASP.NET Core Identity membersh\", \"ip system. ASP.NET Core Identity stores user information (including sign-in information, roles, and \", \"claims) in a data store configured by the developer. Typically, the ASP.NET Core Identity data store\", \" is an Entity Framework store provided in the\\n\\nMicrosoft.AspNetCore.Identity.EntityFrameworkCore pac\", \"kage. However, custom stores or other thirdparty packages can be used to store identity information \", \"in Azure Table Storage, CosmosDB, or other locations.\\n\\n## Tip\\n\\nASP.NET Core 2.1 and later provides A\", \"SP.NET Core Identity as a Razor Class Library, so you won't see much of the necessary code in your p\", \"roject, as was the case for previous versions. For details on how to customize the Identity code to \", \"suit your needs, see Scaffold Identity in ASP.NET Core projects .\\n\\nThe following code is taken from \", \"the ASP.NET Core Web Application MVC 3.1 project template with individual user account authenticatio\", \"n selected. It shows how to configure ASP.NET Core Identity using Entity Framework Core in the Progr\", \"am.cs file.\\n\\n```\\n//... builder . Services . AddDbContext<ApplicationDbContext>(options => options . \", \"UseSqlServer( builder . Configuration . GetConnectionString(\\\"DefaultConnection\\\")));\\n```\\n\\n```\\nbuilder\", \" . Services . AddDefaultIdentity<IdentityUser>(options => options . SignIn . RequireConfirmedAccount\", \" = true) . AddEntityFrameworkStores<ApplicationDbContext>(); builder . Services . AddRazorPages(); /\", \"/... Once ASP.NET Core Identity is configured, you enable it by adding the app.UseAuthentication() a\", \"nd endpoints.MapRazorPages() as shown in the following code in the service's Program.cs file: //... \", \"app.UseRouting(); app.UseAuthentication(); app.UseAuthorization(); app.UseEndpoints(endpoints => { e\", \"ndpoints . MapRazorPages(); }); //...\\n```\\n\\n## Important\\n\\nThe lines in the preceding code MUST BE IN \", \"THE ORDER SHOWN for Identity to work correctly.\\n\\nUsing ASP.NET Core Identity enables several scenari\", \"os:\\n\\n- Create new user information using the UserManager type (userManager.CreateAsync).\\n- Authentic\", \"ate users using the SignInManager type. You can use signInManager.SignInAsync to sign in directly, o\", \"r signInManager.PasswordSignInAsync to confirm the user's password is correct and then sign them in.\", \"\\n- Identify a user based on information stored in a cookie (which is read by ASP.NET Core Identity m\", \"iddleware) so that subsequent requests from a browser will include a signed-in user's identity and c\", \"laims.\\n\\nASP.NET Core Identity also supports two-factor authentication .\\n\\nFor authentication scenario\", \"s that make use of a local user data store and that persist identity between requests using cookies \", \"(as is typical for MVC web applications), ASP.NET Core Identity is a recommended solution.\\n\\n## Authe\", \"nticate with external providers\\n\\nASP.NET Core also supports using external authentication providers \", \"to let users sign in via OAuth 2.0 flows. This means that users can sign in using existing authentic\", \"ation processes from providers like Microsoft, Google, Facebook, or Twitter and associate those iden\", \"tities with an ASP.NET Core identity in your application.\\n\\nTo use external authentication, besides i\", \"ncluding the authentication middleware as mentioned before, using the app.UseAuthentication() method\", \", you also have to register the external provider in Program.cs as shown in the following example:\\n\\n\", \"```\\n//... services . AddDefaultIdentity<IdentityUser>(options => options . SignIn . RequireConfirmed\", \"Account = true) . AddEntityFrameworkStores<ApplicationDbContext>(); services . AddAuthentication() .\", \" AddMicrosoftAccount(microsoftOptions => { microsoftOptions . ClientId = builder . Configuration[\\\"Au\", \"thentication:Microsoft:ClientId\\\"]; microsoftOptions . ClientSecret = builder . Configuration[\\\"Authen\", \"tication:Microsoft:ClientSecret\\\"]; }) . AddGoogle(googleOptions => { ... }) . AddTwitter(twitterOpti\", \"ons => { ... }) . AddFacebook(facebookOptions => { ... }); //...\\n```\\n\\nPopular external authenticatio\", \"n providers and their associated NuGet packages are shown in the following table:\\n\\n| Provider   | Pa\", \"ckage                                              |\\n|------------|---------------------------------\", \"---------------------|\\n| Microsoft  | Microsoft.AspNetCore.Authentication.MicrosoftAccount |\\n| Googl\", \"e     | Microsoft.AspNetCore.Authentication.Google           |\\n| Facebook   | Microsoft.AspNetCore.A\", \"uthentication.Facebook         |\\n| Twitter    | Microsoft.AspNetCore.Authentication.Twitter         \", \" |\\n\\nIn all cases, you must complete an application registration procedure that is vendor dependent a\", \"nd that usually involves:\\n\\n1. Getting a Client Application ID.\\n2. Getting a Client Application Secre\", \"t.\\n3. Configuring a redirection URL, that's handled by the authorization middleware and the register\", \"ed provider\\n4. Optionally, configuring a sign-out URL to properly handle sign out in a Single Sign O\", \"n (SSO) scenario.\\n\\nFor details on configuring your app for an external provider, see the External pr\", \"ovider authentication in the ASP.NET Core documentation).\\n\\n## Tip\\n\\nAll details are handled by the au\", \"thorization middleware and services previously mentioned. So, you just have to choose the Individual\", \" User Account authentication option when you create the ASP.NET Core web application project in Visu\", \"al Studio, as shown in Figure 9-3, besides registering the authentication providers previously menti\", \"oned.\\n\\nCreate a new ASP.NET Core web application\\n\\n.NET Core\\n\\nASP.NET Core Empty it.\\n\\nASP.NET Core We\", \"b API\\n\\nASP.NET Core Web App\\n\\nASP.NET Core with Angular\\n\\nD\\n\\nASP.NET Core with React.js\\n\\nGet additiona\", \"l project templates\\n\\nFigure 9 -3. Selecting the Individual User Accounts option, for using external \", \"authentication, when creating a web application project in Visual Studio 2019.\\n\\n<!-- image -->\\n\\nIn a\", \"ddition to the external authentication providers listed previously, third-party packages are availab\", \"le that provide middleware for using many more external authentication providers. For a list, see th\", \"e AspNet.Security.OAuth.Providers repository on GitHub.\\n\\nYou can also create your own external authe\", \"ntication middleware to solve some special need.\\n\\n## Authenticate with bearer tokens\\n\\nAuthenticating\", \" with ASP.NET Core Identity (or Identity plus external authentication providers) works well for many\", \" web application scenarios in which storing user information in a cookie is appropriate. In other sc\", \"enarios, though, cookies are not a natural means of persisting and transmitting data.\\n\\nFor example, \", \"in an ASP.NET Core Web API that exposes RESTful endpoints that might be accessed by Single Page Appl\", \"ications (SPAs), by native clients, or even by other Web APIs, you typically want to use bearer toke\", \"n authentication instead. These types of applications do not work with cookies, but can easily retri\", \"eve a bearer token and include it in the authorization header of subsequent requests. To enable toke\", \"n authentication, ASP.NET Core supports several options for using OAuth 2.0 and OpenID Connect .\\n\\n##\", \" Authenticate with an OpenID Connect or OAuth 2.0 Identity provider\\n\\nIf user information is stored i\", \"n Azure Active Directory or another identity solution that supports OpenID Connect or OAuth 2.0, you\", \" can use the\\n\\nMicrosoft.AspNetCore.Authentication.OpenIdConnect package to authenticate using the Op\", \"enID Connect workflow. For example, to authenticate to the Identity.Api microservice in eShopOnConta\", \"iners, an ASP.NET Core web application can use middleware from that package as shown in the followin\", \"g simplified example in Program.cs:\\n\\n```\\n// Program.cs var identityUrl = builder . Configuration . G\", \"etValue<string>(\\\"IdentityUrl\\\"); var callBackUrl = builder . Configuration . GetValue<string>(\\\"CallBa\", \"ckUrl\\\"); var sessionCookieLifetime = builder . Configuration . GetValue(\\\"SessionCookieLifetimeMinute\", \"s\\\" , 60); // Add Authentication services services . AddAuthentication(options => { options . Default\", \"Scheme = CookieAuthenticationDefaults . AuthenticationScheme; options . DefaultChallengeScheme = Jwt\", \"BearerDefaults . AuthenticationScheme; }) . AddCookie(setup => setup . ExpireTimeSpan = TimeSpan . F\", \"romMinutes(sessionCookieLifetime)) . AddOpenIdConnect(options => { options . SignInScheme = CookieAu\", \"thenticationDefaults . AuthenticationScheme; options . Authority = identityUrl . ToString(); options\", \" . SignedOutRedirectUri = callBackUrl . ToString(); options . ClientId = useLoadTest ? \\\"mvctest\\\" : \\\"\", \"mvc\\\"; options . ClientSecret = \\\"secret\\\"; options . ResponseType = useLoadTest ? \\\"code id_token token\", \"\\\" : \\\"code id_token\\\"; options . SaveTokens = true; options . GetClaimsFromUserInfoEndpoint = true; op\", \"tions . RequireHttpsMetadata = false; options . Scope . Add(\\\"openid\\\"); options . Scope . Add(\\\"profil\", \"e\\\"); options . Scope . Add(\\\"orders\\\"); options . Scope . Add(\\\"basket\\\"); options . Scope . Add(\\\"market\", \"ing\\\"); options . Scope . Add(\\\"locations\\\"); options . Scope . Add(\\\"webshoppingagg\\\"); options . Scope \", \". Add(\\\"orders.signalrhub\\\"); }); // Build the app //\\u2026 app.UseAuthentication(); //\\u2026 app.UseEndpoints(e\", \"ndpoints => { //... });\\n```\\n\\nWhen you use this workflow, the ASP.NET Core Identity middleware is not\", \" needed, because all user information storage and authentication is handled by the Identity service.\", \"\\n\\n## Issue security tokens from an ASP.NET Core service\\n\\nIf you prefer to issue security tokens for \", \"local ASP.NET Core Identity users rather than using an external identity provider, you can take adva\", \"ntage of some good third-party libraries.\\n\\nIdentityServer4 and OpenIddict are OpenID Connect provide\", \"rs that integrate easily with ASP.NET Core Identity to let you issue security tokens from an ASP.NET\", \" Core service. The IdentityServer4 documentation has in -depth instructions for using the library. H\", \"owever, the basic steps to using IdentityServer4 to issue tokens are as follows.\\n\\n1. You configure I\", \"dentityServer4 in Program.cs by making a call to builder.Services.AddIdentityServer.\\n2. You call app\", \".UseIdentityServer in Program.cs to add IdentityServer4 to the application's HTTP request processing\", \" pipeline. This lets the library serve requests to OpenID Connect and OAuth2 endpoints like /connect\", \"/token.\\n3. You configure identity server by setting the following data:\\n4. \\u2013 The credentials to use \", \"for signing.\\n5. \\u2013 The Identity and API resources that users might request access to:\\n- API resources\", \" represent protected data or functionality that a user can access with an access token. An example o\", \"f an API resource would be a web API (or set of APIs) that requires authorization.\\n- Identity resour\", \"ces represent information (claims) that are given to a client to identify a user. The claims might i\", \"nclude the user name, email address, and so on.\\n8. \\u2013 The clients that will be connecting in order to\", \" request tokens.\\n9. \\u2013 The storage mechanism for user information, such as ASP.NET Core Identity or a\", \"n alternative.\\n\\nWhen you specify clients and resources for IdentityServer4 to use, you can pass an I\", \"Enumerable collection of the appropriate type to methods that take in-memory client or resource stor\", \"es. Or for more complex scenarios, you can provide client or resource provider types via Dependency \", \"Injection.\\n\\nA sample configuration for IdentityServer4 to use in-memory resources and clients provid\", \"ed by a custom IClientStore type might look like the following example:\\n\\n```\\n// Program.cs builder .\", \" Services . AddSingleton<IClientStore , CustomClientStore>(); builder . Services . AddIdentityServer\", \"() . AddSigningCredential(\\\"CN=sts\\\") . AddInMemoryApiResources(MyApiResourceProvider . GetAllResource\", \"s()) . AddAspNetIdentity<ApplicationUser>(); //...\\n```\\n\\n## Consume security tokens\\n\\nAuthenticating a\", \"gainst an OpenID Connect endpoint or issuing your own security tokens covers some scenarios. But wha\", \"t about a service that simply needs to limit access to those users who have valid security tokens th\", \"at were provided by a different service?\\n\\nFor that scenario, authentication middleware that handles \", \"JWT tokens is available in the Microsoft.AspNetCore.Authentication.JwtBearer package. JWT stands for\", \" \\\"JSON Web Token\\\" and is a common security token format (defined by RFC 7519) for communicating secu\", \"rity claims. A simplified example of how to use middleware to consume such tokens might look like th\", \"is code fragment, taken from the Ordering.Api microservice of eShopOnContainers.\\n\\n```\\n// Program.cs \", \"var identityUrl = builder . Configuration . GetValue<string>(\\\"IdentityUrl\\\"); // Add Authentication s\", \"ervices builder . Services . AddAuthentication(options => { options . DefaultAuthenticateScheme = As\", \"pNetCore . Authentication . JwtBearer . JwtBearerDefaults . AuthenticationScheme; options . DefaultC\", \"hallengeScheme = AspNetCore . Authentication . JwtBearer . JwtBearerDefaults . AuthenticationScheme;\", \" }).AddJwtBearer(options => { options . Authority = identityUrl; options . RequireHttpsMetadata = fa\", \"lse; options . Audience = \\\"orders\\\"; }); // Build the app app.UseAuthentication(); //\\u2026 app.UseEndpoin\", \"ts(endpoints => { //... });\\n```\\n\\nThe parameters in this usage are:\\n\\n- Audience represents the receiv\", \"er of the incoming token or the resource that the token grants access to. If the value specified in \", \"this parameter does not match the parameter in the token, the token will be rejected.\\n- Authority is\", \" the address of the token-issuing authentication server. The JWT bearer authentication middleware us\", \"es this URI to get the public key that can be used to validate the token's signature. The middleware\", \" also confirms that the iss parameter in the token matches this URI.\\n\\nAnother parameter, RequireHttp\", \"sMetadata, is useful for testing purposes; you set this parameter to false so you can test in enviro\", \"nments where you don't have certificates. In real-world deployments, JWT bearer tokens should always\", \" be passed only over HTTPS .\\n\\nWith this middleware in place, JWT tokens are automatically extracted \", \"from authorization headers. They are then deserialized, validated (using the values in the Audience \", \"and Authority parameters), and stored as user information to be referenced later by MVC actions or a\", \"uthorization filters.\\n\\nThe JWT bearer authentication middleware can also support more advanced scena\", \"rios, such as using a local certificate to validate a token if the authority is not available. For t\", \"his scenario, you can specify a TokenValidationParameters object in the JwtBearerOptions object.\\n\\n##\", \" Additional resources\\n\\n- Sharing cookies between applications https://learn.microsoft.com/aspnet/cor\", \"e/security/cookie-sharing\\n- Introduction to Identity https://learn.microsoft.com/aspnet/core/securit\", \"y/authentication/identity\\n- Rick Anderson. Two -factor authentication with SMS https://learn.microso\", \"ft.com/aspnet/core/security/authentication/2fa\\n- Enabling authentication using Facebook, Google and \", \"other external providers https://learn.microsoft.com/aspnet/core/security/authentication/social/\\n- M\", \"ichell Anicas. An Introduction to OAuth 2 https://www.digitalocean.com/community/tutorials/an-introd\", \"uction-to-oauth-2\\n- AspNet.Security.OAuth.Providers (GitHub repo for ASP.NET OAuth providers) https:\", \"//github.com/aspnet-contrib/AspNet.Security.OAuth.Providers/tree/dev/src\\n- IdentityServer4. Official\", \" documentation https://identityserver4.readthedocs.io/en/latest/\\n\\n## About authorization in .NET mic\", \"roservices and web applications\\n\\nAfter authentication, ASP.NET Core Web APIs need to authorize acces\", \"s. This process allows a service to make APIs available to some authenticated users, but not to all.\", \" Authorization can be done based on users' roles or based on custom policy, which might include insp\", \"ecting claims or other heuristics.\\n\\nRestricting access to an ASP.NET Core MVC route is as easy as ap\", \"plying an Authorize attribute to the action method (or to the controller's class if all the controll\", \"er's actions require authorization), as shown in following example:\\n\\n```\\npublic class AccountControl\", \"ler : Controller { public ActionResult Login() { } [Authorize]\\n```\\n\\n```\\npublic ActionResult Logout()\", \" { } }\\n```\\n\\nBy default, adding an Authorize attribute without parameters will limit access to authen\", \"ticated users for that controller or action. To further restrict an API to be available for only spe\", \"cific users, the attribute can be expanded to specify required roles or policies that users must sat\", \"isfy.\\n\\n## Implement role-based authorization\\n\\nASP.NET Core Identity has a built-in concept of roles.\", \" In addition to users, ASP.NET Core Identity stores information about different roles used by the ap\", \"plication and keeps track of which users are assigned to which roles. These assignments can be chang\", \"ed programmatically with the RoleManager type that updates roles in persisted storage, and the UserM\", \"anager type that can grant or revoke roles from users.\\n\\nIf you're authenticating with JWT bearer tok\", \"ens, the ASP.NET Core JWT bearer authentication middleware will populate a user's roles based on rol\", \"e claims found in the token. To limit access to an MVC action or controller to users in specific rol\", \"es, you can include a Roles parameter in the Authorize annotation (attribute), as shown in the follo\", \"wing code fragment:\\n\\n```\\n[Authorize(Roles = \\\"Administrator, PowerUser\\\")] public class ControlPanelCo\", \"ntroller : Controller { public ActionResult SetTime() { } [Authorize(Roles = \\\"Administrator\\\")] publi\", \"c ActionResult ShutDown() { } }\\n```\\n\\nIn this example, only users in the Administrator or PowerUser r\", \"oles can access APIs in the ControlPanel controller (such as executing the SetTime action). The Shut\", \"Down API is further restricted to allow access only to users in the Administrator role.\\n\\nTo require \", \"a user be in multiple roles, you use multiple Authorize attributes, as shown in the following exampl\", \"e:\\n\\n```\\n[Authorize(Roles = \\\"Administrator, PowerUser\\\")] [Authorize(Roles = \\\"RemoteEmployee \\\")] [Auth\", \"orize(Policy = \\\"CustomPolicy\\\")] public ActionResult API1 () { }\\n```\\n\\nIn this example, to call API1, \", \"a user must:\\n\\n- Be in the Administrator or PowerUser role, and\\n- Be in the RemoteEmployee role, and\\n\", \"\\n- Satisfy a custom handler for CustomPolicy authorization.\\n\\n## Implement policy-based authorization\", \"\\n\\nCustom authorization rules can also be written using authorization policies. This section provides\", \" an overview. For more information, see the ASP.NET Authorization Workshop .\\n\\nCustom authorization p\", \"olicies are registered in the Startup.ConfigureServices method using the service.AddAuthorization me\", \"thod. This method takes a delegate that configures an AuthorizationOptions argument.\\n\\n```\\nservices .\", \" AddAuthorization(options => { options . AddPolicy(\\\"AdministratorsOnly\\\" , policy => policy . Require\", \"Role(\\\"Administrator\\\")); options . AddPolicy(\\\"EmployeesOnly\\\" , policy => policy . RequireClaim(\\\"Emplo\", \"yeeNumber\\\")); options . AddPolicy(\\\"Over21\\\" , policy => policy . Requirements . Add(new MinimumAgeReq\", \"uirement(21))); });\\n```\\n\\nAs shown in the example, policies can be associated with different types of\", \" requirements. After the policies are registered, they can be applied to an action or controller by \", \"passing the policy's name as the Policy argument of the Authorize attribute (for example, [Authorize\", \"(Policy=\\\"EmployeesOnly\\\")]) Policies can have multiple requirements, not just one (as shown in these \", \"examples).\\n\\nIn the previous example, the first AddPolicy call is just an alternative way of authoriz\", \"ing by role. If [Authorize(Policy=\\\"AdministratorsOnly\\\")] is applied to an API, only users in the Adm\", \"inistrator role will be able to access it.\\n\\nThe second AddPolicy call demonstrates an easy way to re\", \"quire that a particular claim should be present for the user. The RequireClaim method also optionall\", \"y takes expected values for the claim. If values are specified, the requirement is met only if the u\", \"ser has both a claim of the correct type and one of the specified values. If you're using the JWT be\", \"arer authentication middleware, all JWT properties will be available as user claims.\\n\\nThe most inter\", \"esting policy shown here is in the third AddPolicy method, because it uses a custom authorization re\", \"quirement. By using custom authorization requirements, you can have a great deal of control over how\", \" authorization is performed. For this to work, you must implement these types:\\n\\n- A Requirements typ\", \"e that derives from IAuthorizationRequirement and that contains fields specifying the details of the\", \" requirement. In the example, this is an age field for the sample MinimumAgeRequirement type.\\n- A ha\", \"ndler that implements AuthorizationHandler, where T is the type of IAuthorizationRequirement that th\", \"e handler can satisfy. The handler must implement the HandleRequirementAsync method, which checks wh\", \"ether a specified context that contains information about the user satisfies the requirement.\\n\\nIf th\", \"e user meets the requirement, a call to context.Succeed will indicate that the user is authorized. I\", \"f there are multiple ways that a user might satisfy an authorization requirement, multiple handlers \", \"can be created.\\n\\nIn addition to registering custom policy requirements with AddPolicy calls, you als\", \"o need to register custom requirement handlers via Dependency Injection (services.AddTransient&lt;IA\", \"uthorizationHandler, MinimumAgeHandler&gt;()).\\n\\nAn example of a custom authorization requirement and\", \" handler for checking a user's age (based on a DateOfBirth claim) is available in the ASP.NET Core a\", \"uthorization documentation .\\n\\n## Authorization and minimal apis\\n\\nASP.NET supports minimal APIs as an\", \" alternative to controller-based APIs. Authorization policies are the recommended way to configure a\", \"uthorization for minimal APIs, as this example demonstrates:\\n\\n```\\n// Program.cs builder . Services .\", \" AddAuthorizationBuilder() . AddPolicy(\\\"admin_greetings\\\" , policy => policy . RequireRole(\\\"admin\\\") .\", \" RequireScope(\\\"greetings_api\\\")); // build the app app.MapGet(\\\"/hello\\\" , () => \\\"Hello world!\\\") . Requ\", \"ireAuthorization(\\\"admin_greetings\\\");\\n```\\n\\n## Additional resources\\n\\n- ASP.NET Core Authentication htt\", \"ps://learn.microsoft.com/aspnet/core/security/authentication/identity\\n- ASP.NET Core Authorization\\n\\n\", \"https://learn.microsoft.com/aspnet/core/security/authorization/introduction\\n\\n- Role -based Authoriza\", \"tion https://learn.microsoft.com/aspnet/core/security/authorization/roles\\n- Custom Policy-Based Auth\", \"orization https://learn.microsoft.com/aspnet/core/security/authorization/policies\\n- Authentication a\", \"nd authorization in minimal APIs https://learn.microsoft.com/aspnet/core/fundamentals/minimal-apis/s\", \"ecurity\\n\\n## Store application secrets safely during development\\n\\nTo connect with protected resources\", \" and other services, ASP.NET Core applications typically need to use connection strings, passwords, \", \"or other credentials that contain sensitive information. These sensitive pieces of information are c\", \"alled secrets. It's a best practice to not include secrets in source\\n\\ncode and making sure not to st\", \"ore secrets in source control. Instead, you should use the ASP.NET Core configuration model to read \", \"the secrets from more secure locations.\\n\\nYou must separate the secrets for accessing development and\", \" staging resources from the ones used for accessing production resources, because different individu\", \"als will need access to those different sets of secrets. To store secrets used during development, c\", \"ommon approaches are to either store secrets in environment variables or by using the ASP.NET Core S\", \"ecret Manager tool. For more secure storage in production environments, microservices can store secr\", \"ets in an Azure Key Vault.\\n\\n## Store secrets in environment variables\\n\\nOne way to keep secrets out o\", \"f source code is for developers to set string-based secrets as environment variables on their develo\", \"pment machines. When you use environment variables to store secrets with hierarchical names, such as\", \" the ones nested in configuration sections, you must name the variables to include the complete hier\", \"archy of its sections, delimited with colons (:).\\n\\nFor example, setting an environment variable Logg\", \"ing:LogLevel:Default to Debug value would be equivalent to a configuration value from the following \", \"JSON file:\\n\\n```\\n{ \\\"Logging\\\": { \\\"LogLevel\\\": { \\\"Default\\\": \\\"Debug\\\" } } }\\n```\\n\\nTo access these values fr\", \"om environment variables, the application just needs to call AddEnvironmentVariables on its Configur\", \"ationBuilder when constructing an IConfigurationRoot object.\\n\\n## Note\\n\\nEnvironment variables are com\", \"monly stored as plain text, so if the machine or process with the environment variables is compromis\", \"ed, the environment variable values will be visible.\\n\\n## Store secrets with the ASP.NET Core Secret \", \"Manager\\n\\nThe ASP.NET Core Secret Manager tool provides another method of keeping secrets out of sour\", \"ce code during development. To use the Secret Manager tool, install the package Microsoft.Extensions\", \".Configuration.SecretManager in your project file. Once that dependency is present and has been rest\", \"ored, the dotnet user-secrets command can be used to set the value of secrets from the command line.\", \" These secrets will be stored in a JSON file in the user's profile directory (details vary by OS), a\", \"way from source code.\\n\\nSecrets set by the Secret Manager tool are organized by the UserSecretsId pro\", \"perty of the project that's using the secrets. Therefore, you must be sure to set the UserSecretsId \", \"property in your project file, as shown in the snippet below. The default value is a GUID assigned b\", \"y Visual Studio, but the actual string is not important as long as it's unique in your computer.\\n\\n&l\", \"t;PropertyGroup&gt; &lt;UserSecretsId&gt;UniqueIdentifyingString&lt;/UserSecretsId&gt; &lt;/Property\", \"Group&gt;\\n\\nUsing secrets stored with Secret Manager in an application is accomplished by calling Add\", \"UserSecrets&lt;T&gt; on the ConfigurationBuilder instance to include secrets for the application in \", \"its configuration. The generic parameter T should be a type from the assembly that the UserSecretId \", \"was applied to. Usually, using AddUserSecrets&lt;Startup&gt; is fine.\\n\\nThe AddUserSecrets&lt;Startup\", \"&gt;() is included in the default options for the Development environment when using the CreateDefau\", \"ltBuilder method in Program.cs .\\n\\n## Use Azure Key Vault to protect secrets at production time\\n\\nSecr\", \"ets stored as environment variables or stored by the Secret Manager tool are still stored locally an\", \"d unencrypted on the machine. A more secure option for storing secrets is Azure Key Vault, which pro\", \"vides a secure, central location for storing keys and secrets.\\n\\nThe Azure.Extensions.AspNetCore.Conf\", \"iguration.Secrets package allows an ASP.NET Core application to read configuration information from \", \"Azure Key Vault. To start using secrets from an Azure Key Vault, you follow these steps:\\n\\n1. Registe\", \"r your application as an Azure AD application. (Access to key vaults is managed by Azure AD.) This c\", \"an be done through the Azure management portal.\\n\\nAlternatively, if you want your application to auth\", \"enticate using a certificate instead of a password or client secret, you can use the New-AzADApplica\", \"tion PowerShell cmdlet. The certificate that you register with Azure Key Vault needs only your publi\", \"c key. Your application will use the private key.\\n\\n2. Give the registered application access to the \", \"key vault by creating a new service principal. You can do this using the following PowerShell comman\", \"ds:\\n\\n$sp = New-AzADServicePrincipal -ApplicationId \\\"&lt;Application ID guid&gt;\\\" Set -AzKeyVaultAcce\", \"ssPolicy -VaultName \\\"&lt;VaultName&gt;\\\" -ServicePrincipalName $sp . ServicePrincipalNames[0] -Permis\", \"sionsToSecrets all -ResourceGroupName \\\"&lt;KeyVault Resource Group&gt;\\\"\\n\\n3. Include the key vault as\", \" a configuration source in your application by calling the AzureKeyVaultConfigurationExtensions.AddA\", \"zureKeyVault extension method when you create an IConfigurationRoot instance.\\n\\nNote that calling Add\", \"AzureKeyVault requires the application ID that was registered and given access to the key vault in t\", \"he previous steps. Or you can firstly running the Azure CLI command: az login, then using an overloa\", \"d of AddAzureKeyVault that takes a DefaultAzureCredential in place of the client.\\n\\n## Important\\n\\nWe \", \"recommend that you register Azure Key Vault as the last configuration provider, so it can override c\", \"onfiguration values from previous providers.\\n\\n## Additional resources\\n\\n- Using Azure Key Vault to pr\", \"otect application secrets https://learn.microsoft.com/azure/architecture/multitenant-identity\\n- Safe\", \" storage of app secrets during development https://learn.microsoft.com/aspnet/core/security/app-secr\", \"ets\\n- Configuring data protection https://learn.microsoft.com/aspnet/core/security/data-protection/c\", \"onfiguration/overview\\n- Data Protection key management and lifetime in ASP.NET Core https://learn.mi\", \"crosoft.com/aspnet/core/security/data-protection/configuration/defaultsettings\\n\\n## .NET Microservice\", \"s Architecture key takeaways\\n\\nAs a summary and key takeaways, the following are the most important c\", \"onclusions from this guide.\\n\\nBenefits of using containers. Container-based solutions provide importa\", \"nt cost savings because they help reduce deployment problems caused by failing dependencies in produ\", \"ction environments. Containers significantly improve DevOps and production operations.\\n\\nContainers w\", \"ill be ubiquitous. Docker-based containers are becoming the de facto standard in the industry, suppo\", \"rted by key vendors in the Windows and Linux ecosystems, such as Microsoft, Amazon AWS, Google, and \", \"IBM. Docker will probably soon be ubiquitous in both the cloud and on-premises datacenters.\\n\\nContain\", \"ers as a unit of deployment. A Docker container is becoming the standard unit of deployment for any \", \"server-based application or service.\\n\\nMicroservices. The microservices architecture is becoming the \", \"preferred approach for distributed and large or complex mission-critical applications based on many \", \"independent subsystems in the form of autonomous services. In a microservice -based architecture, th\", \"e application is built as a collection of services that are developed, tested, versioned, deployed, \", \"and scaled independently. Each service can include any related autonomous database.\\n\\nDomain -driven \", \"design and SOA. The microservices architecture patterns derive from serviceoriented architecture (SO\", \"A) and domain-driven design (DDD). When you design and develop microservices for environments with e\", \"volving business needs and rules, it's important to consider DDD approaches and patterns.\\n\\nMicroserv\", \"ices challenges. Microservices offer many powerful capabilities, like independent deployment, strong\", \" subsystem boundaries, and technology diversity. However, they also raise many new challenges relate\", \"d to distributed application development, such as fragmented and independent data models, resilient \", \"communication between microservices, eventual consistency, and operational complexity that results f\", \"rom aggregating logging and monitoring information from multiple microservices. These aspects introd\", \"uce a much higher complexity level than a traditional monolithic application. As a result, only spec\", \"ific scenarios are suitable for microservice-based applications. These include large and complex app\", \"lications with multiple evolving subsystems. In these cases, it's worth investing in a more complex \", \"software architecture, because it will provide better long-term agility and application maintenance.\", \"\\n\\nContainers for any application. Containers are convenient for microservices, but can also be usefu\", \"l for monolithic applications based on the traditional .NET Framework, when using Windows Containers\", \". The benefits of using Docker, such as solving many deployment-to-production issues and providing s\", \"tate-of-the-art Dev and Test environments, apply to many different types of applications.\\n\\nCLI versu\", \"s IDE. With Microsoft tools, you can develop containerized .NET applications using your preferred ap\", \"proach. You can develop with a CLI and an editor-based environment by using the Docker CLI and Visua\", \"l Studio Code. Or you can use an IDE-focused approach with Visual Studio and its unique features for\", \" Docker, such as multi -container debugging.\\n\\nResilient cloud applications. In cloud-based systems a\", \"nd distributed systems in general, there is always the risk of partial failure. Since clients and se\", \"rvices are separate processes (containers), a service might not be able to respond in a timely way t\", \"o a client's request. For example, a service might be down because of a partial failure or for maint\", \"enance; the service might be overloaded and responding slowly to requests; or it might not be access\", \"ible for a short time because of network issues. Therefore, a cloud -based application must embrace \", \"those failures and have a strategy in place to respond to those failures. These strategies can inclu\", \"de retry policies (resending messages or retrying requests) and implementing circuit-breaker pattern\", \"s to avoid exponential load of repeated requests. Basically, cloud-based applications must have resi\", \"lient mechanisms\\u2014either based on cloud infrastructure or custom, as the high-level ones provided by \", \"orchestrators or service buses.\\n\\nSecurity. Our modern world of containers and microservices can expo\", \"se new vulnerabilities. There are several ways to implement basic application security, based on aut\", \"hentication and authorization. However, container security must consider additional key components t\", \"hat result in inherently safer applications. A critical element of building safer apps is having a s\", \"ecure way of communicating with other apps and systems, something that often requires credentials, t\", \"okens, passwords, and the like, commonly referred to as application secrets. Any secure solution mus\", \"t follow security best practices, such as encrypting secrets while in transit and at rest, and preve\", \"nting secrets from leaking when consumed by the final application. Those secrets need to be stored a\", \"nd kept safely, as when using Azure Key Vault.\\n\\nOrchestrators. Container -based orchestrators, such \", \"as Azure Kubernetes Service and Azure Service Fabric are key part of any significant microservice an\", \"d container-based application. These applications carry with them high complexity, scalability needs\", \", and go through constant evolution. This guide has introduced orchestrators and their role in micro\", \"service -based and container -based solutions. If your application needs are moving you toward compl\", \"ex containerized apps, you will find it useful to seek out additional resources for learning more ab\", \"out orchestrators.\"]"