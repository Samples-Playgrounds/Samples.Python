"[\"![](_page_0_Picture_0.jpeg)\\n\\n# NET Microservices: Architecture for Containerized .NET Applications\\n\\n\", \"![](_page_0_Picture_2.jpeg)\\n\\nCesar de la Torre Bill Wagner Mike Rousos\\n\\nMicrosoft Corporation\\n\\n![](_\", \"page_1_Picture_0.jpeg)\\n\\n### **EDITION v7.0** - Updated to ASP.NET Core 7.0\\n\\nRefer [changelog](https:\", \"//aka.ms/MicroservicesEbookChangelog) for the book updates and community contributions.\\n\\nThis guide \", \"is an introduction to developing microservices-based applications and managing them using containers\", \". It discusses architectural design and implementation approaches using .NET and Docker containers.\\n\", \"\\nTo make it easier to get started, the guide focuses on a reference containerized and microserviceba\", \"sed application that you can explore. The reference application is available at the [eShopOnContaine\", \"rs](https://github.com/dotnet-architecture/eShopOnContainers) GitHub repo.\\n\\n## Action links\\n\\n- This \", \"e-book is also available in a PDF format (English version only) [Download](https://aka.ms/microservi\", \"cesebook)\\n- Clone/Fork the reference application [eShopOnContainers on GitHub](https://github.com/do\", \"tnet-architecture/eShopOnContainers)\\n- Watch the [introductory video](https://aka.ms/microservices-v\", \"ideo)\\n- Get to know the [Microservices Architecture](https://aka.ms/MicroservicesArchitecture) right\", \" away\\n\\n## Introduction\\n\\nEnterprises are increasingly realizing cost savings, solving deployment prob\", \"lems, and improving DevOps and production operations by using containers. Microsoft has been releasi\", \"ng container innovations for Windows and Linux by creating products like Azure Kubernetes Service an\", \"d Azure Service Fabric, and by partnering with industry leaders like Docker, Mesosphere, and Kuberne\", \"tes. These products deliver container solutions that help companies build and deploy applications at\", \" cloud speed and scale, whatever their choice of platform or tools.\\n\\nDocker is becoming the de facto\", \" standard in the container industry, supported by the most significant vendors in the Windows and Li\", \"nux ecosystems. (Microsoft is one of the main cloud vendors supporting Docker). In the future, Docke\", \"r will probably be ubiquitous in any datacenter in the cloud or on-premises.\\n\\nIn addition, the [micr\", \"oservices](https://martinfowler.com/articles/microservices.html) architecture is emerging as an impo\", \"rtant approach for distributed mission-critical applications. In a microservice-based architecture, \", \"the application is built on a collection of services that can be developed, tested, deployed, and ve\", \"rsioned independently.\\n\\n## About this guide\\n\\nThis guide is an introduction to developing microservic\", \"es-based applications and managing them using containers. It discusses architectural design and impl\", \"ementation approaches using .NET and Docker containers. To make it easier to get started with contai\", \"ners and microservices, the guide focuses on a reference containerized and microservice-based applic\", \"ation that you can explore. The sample application is available at the [eShopOnContainers](https://g\", \"ithub.com/dotnet-architecture/eShopOnContainers) GitHub repo.\\n\\n![](_page_2_Picture_0.jpeg)\\n\\nThis gui\", \"de provides foundational development and architectural guidance primarily at a development environme\", \"nt level with a focus on two technologies: Docker and .NET. Our intention is that you read this guid\", \"e when thinking about your application design without focusing on the infrastructure (cloud or on-pr\", \"emises) of your production environment. You will make decisions about your infrastructure later, whe\", \"n you create your production-ready applications. Therefore, this guide is intended to be infrastruct\", \"ure agnostic and more development-environment-centric.\\n\\nAfter you have studied this guide, your next\", \" step would be to learn about production-ready microservices on Microsoft Azure.\\n\\n## Version\\n\\nThis g\", \"uide has been revised to cover **.NET 7** version along with many additional updates related to the \", \"same \\\"wave\\\" of technologies (that is, Azure and additional third-party technologies) coinciding in t\", \"ime with the .NET 7 release. That's why the book version has also been updated to version **7.0**.\\n\\n\", \"## What this guide does not cover\\n\\nThis guide does not focus on the application lifecycle, DevOps, C\", \"I/CD pipelines, or team work. The complementary guide [Containerized Docker Application Lifecycle wi\", \"th Microsoft Platform and Tools](https://aka.ms/dockerlifecycleebook) focuses on that subject. The c\", \"urrent guide also does not provide implementation details on Azure infrastructure, such as informati\", \"on on specific orchestrators.\\n\\n### **Additional resources**\\n\\n\\u2022 Containerized Docker Application Life\", \"cycle with Microsoft Platform and Tools (downloadable e-book) <https://aka.ms/dockerlifecycleebook>\\n\", \"\\n## Who should use this guide\\n\\nWe wrote this guide for developers and solution architects who are ne\", \"w to Docker-based application development and to microservices-based architecture. This guide is for\", \" you if you want to learn how to architect, design, and implement proof-of-concept applications with\", \" Microsoft development technologies (with special focus on .NET) and with Docker containers.\\n\\nYou wi\", \"ll also find this guide useful if you are a technical decision maker, such as an enterprise architec\", \"t, who wants an architecture and technology overview before you decide on what approach to select fo\", \"r new and modern distributed applications.\\n\\n### **How to use this guide**\\n\\nThe first part of this gu\", \"ide introduces Docker containers, discusses how to choose between .NET 7 and the .NET Framework as a\", \" development framework, and provides an overview of microservices. This content is for architects an\", \"d technical decision makers who want an overview but don't need to focus on code implementation deta\", \"ils.\\n\\n![](_page_3_Picture_0.jpeg)\\n\\nThe second part of the guide starts with the Development process \", \"for Docker based applications section. It focuses on the development and microservice patterns for i\", \"mplementing applications using .NET and Docker. This section will be of most interest to developers \", \"and architects who want to focus on code and on patterns and implementation details.\\n\\n## Related mic\", \"roservice and container-based reference application: eShopOnContainers\\n\\nThe eShopOnContainers applic\", \"ation is an open-source reference app for .NET and microservices that is designed to be deployed usi\", \"ng Docker containers. The application consists of multiple subsystems, including several e-store UI \", \"front-ends (a Web MVC app, a Web SPA, and a native mobile app). It also includes the back-end micros\", \"ervices and containers for all required server-side operations.\\n\\nThe purpose of the application is t\", \"o showcase architectural patterns. **IT IS NOT A PRODUCTION-READY TEMPLATE** to start real-world app\", \"lications. In fact, the application is in a permanent beta state, as it's also used to test new pote\", \"ntially interesting technologies as they show up.\\n\\n## Credits\\n\\nCo-Authors:\\n\\n**Cesar de la Torre**, S\", \"r. PM, .NET product team, Microsoft Corp.\\n\\n**Bill Wagner**, Sr. Content Developer, C+E, Microsoft Co\", \"rp.\\n\\n**Mike Rousos**, Principal Software Engineer, DevDiv CAT team, Microsoft\\n\\nEditors:\\n\\n**Mike Pope\", \"**\\n\\n**Steve Hoag**\\n\\nParticipants and reviewers:\\n\\n**Jeffrey Richter**, Partner Software Eng, Azure te\", \"am, Microsoft\\n\\n**Jimmy Bogard**, Chief Architect at Headspring\\n\\n**Udi Dahan**, Founder & CEO, Partic\", \"ular Software\\n\\n**Jimmy Nilsson**, Co-founder and CEO of Factor10\\n\\n**Glenn Condron**, Sr. Program Man\", \"ager, ASP.NET team\\n\\n**Mark Fussell**, Principal PM Lead, Azure Service Fabric team, Microsoft\\n\\n**Die\", \"go Vega**, PM Lead, Entity Framework team, Microsoft\\n\\n**Barry Dorrans**, Sr. Security Program Manage\", \"r\\n\\n**Rowan Miller**, Sr. Program Manager, Microsoft\\n\\n![](_page_4_Picture_0.jpeg)\\n\\n**Ankit Asthana**,\", \" Principal PM Manager, .NET team, Microsoft\\n\\n**Scott Hunter**, Partner Director PM, .NET team, Micro\", \"soft\\n\\n**Nish Anil**, Sr. Program Manager, .NET team, Microsoft\\n\\n**Dylan Reisenberger**, Architect an\", \"d Dev Lead at Polly\\n\\n**Steve \\\"ardalis\\\" Smith** - Software Architect and Trainer - [Ardalis.com](http\", \"s://ardalis.com/)\\n\\n**Ian Cooper**, Coding Architect at Brighter\\n\\n**Unai Zorrilla**, Architect and De\", \"v Lead at Plain Concepts\\n\\n**Eduard Tomas**, Dev Lead at Plain Concepts\\n\\n**Ramon Tomas**, Developer a\", \"t Plain Concepts\\n\\n**David Sanz**, Developer at Plain Concepts\\n\\n**Javier Valero**, Chief Operating Of\", \"ficer at Grupo Solutio\\n\\n**Pierre Millet**, Sr. Consultant, Microsoft\\n\\n**Michael Friis**, Product Man\", \"ager, Docker Inc\\n\\n**Charles Lowell**, Software Engineer, VS CAT team, Microsoft\\n\\n**Miguel Veloso**, \", \"Software Development Engineer at Plain Concepts\\n\\n**Sumit Ghosh**, Principal Consultant at Neudesic\\n\\n\", \"## Copyright\\n\\nPUBLISHED BY\\n\\nMicrosoft Developer Division, .NET and Visual Studio product teams\\n\\nA di\", \"vision of Microsoft Corporation\\n\\nOne Microsoft Way\\n\\nRedmond, Washington 98052-6399\\n\\nCopyright \\u00a9 2023\", \" by Microsoft Corporation\\n\\nAll rights reserved. No part of the contents of this book may be reproduc\", \"ed or transmitted in any form or by any means without the written permission of the publisher.\\n\\nThis\", \" book is provided \\\"as-is\\\" and expresses the author's views and opinions. The views, opinions and inf\", \"ormation expressed in this book, including URL and other Internet website references, may change wit\", \"hout notice.\\n\\nSome examples depicted herein are provided for illustration only and are fictitious. N\", \"o real association or connection is intended or should be inferred.\\n\\n![](_page_5_Picture_0.jpeg)\\n\\nMi\", \"crosoft and the trademarks listed at [https://www.microsoft.com](https://www.microsoft.com/) on the \", \"\\\"Trademarks\\\" webpage are trademarks of the Microsoft group of companies.\\n\\nMac and macOS are trademar\", \"ks of Apple Inc.\\n\\nThe Docker whale logo is a registered trademark of Docker, Inc. Used by permission\", \".\\n\\nAll other marks and logos are property of their respective owners.\\n\\n# Contents\\n\\n| Introduction to\", \" Containers and Docker                                       | 1  |\\n|-------------------------------\", \"----------------------------------------------|----|\\n| What is Docker?                              \", \"                               | 2  |\\n| Comparing Docker containers with virtual machines           \", \"                | 3  |\\n| A simple analogy                                                           \", \" | 4  |\\n| Docker terminology                                                          | 5  |\\n| Docke\", \"r containers, images, and registries                                   | 7  |\\n| Choosing Between .NE\", \"T and .NET Framework for Docker Containers              | 9  |\\n| General guidance                   \", \"                                         | 9  |\\n| When to choose .NET for Docker containers         \", \"                          | 10 |\\n| Developing and deploying cross platform                          \", \"           | 10 |\\n| Using containers for new (\\\"green-field\\\") projects                           | 11\", \" |\\n| Create and deploy microservices on containers                               | 11 |\\n| Deploying \", \"high density in scalable systems                                  | 11 |\\n| When to choose .NET Frame\", \"work for Docker containers                         | 12 |\\n| Migrating existing applications directly\", \" to a Windows Server container      | 12 |\\n| Using third-party .NET libraries or NuGet packages not \", \"available for .NET 7 | 12 |\\n| Using .NET technologies not available for .NET 7                      \", \"      | 12 |\\n| Using a platform or API that doesn't support .NET 7                         | 13 |\\n| \", \"Porting existing ASP.NET application to .NET 7                              | 13 |\\n| Decision table:\", \" .NET implementations to use for Docker                      | 13 |\\n| What OS to target with .NET co\", \"ntainers                                      | 14 |\\n| Official .NET Docker images                  \", \"                               | 16 |\\n| .NET and Docker image optimizations for development versus p\", \"roduction       | 16 |\\n| Architecting container and microservice-based applications                 \", \" | 18 |\\n| Container design principles                                                 | 18 |\\n| Conta\", \"inerizing monolithic applications                                      | 19 |\\n| Deploying a monolith\", \"ic application as a container                           | 21 |\\n| Publishing a single-container-based\", \" application to Azure App Service        | 21 |\\n\\n| Manage state and data in Docker applications<br> \", \"                                 | 22     |\\n|-------------------------------------------------------\", \"----------------------------|--------|\\n| Service-oriented architecture                              \", \"                       | 25     |\\n| Microservices architecture                                      \", \"                  | 25     |\\n| Additional resources<br>                                             \", \"             | 27     |\\n| Data sovereignty per microservice<br>                                     \", \"        | 27     |\\n| The relationship between microservices and the Bounded Context pattern         \", \"   | 29     |\\n| Logical architecture versus physical architecture                                 | \", \"30     |\\n| Challenges and solutions for distributed data management                          | 31   \", \"  |\\n| Challenge #1: How to define the boundaries of each microservice                   | 31     |\\n|\", \" Challenge #2: How to create queries that retrieve data from several microservices | 32     |\\n| Chal\", \"lenge #3: How to achieve consistency across multiple microservices<br>        | 33     |\\n| Challenge\", \" #4: How to design communication across microservice boundaries<br>      | 35     |\\n| Additional res\", \"ources<br>                                                          | 36     |\\n| Identify domain-mod\", \"el boundaries for each microservice                            | 36     |\\n| The API gateway pattern \", \"versus the Direct client-to-microservice communication    | 40     |\\n| Direct client-to-microservice\", \" communication<br>                                   | 40     |\\n| Why consider API Gateways instead \", \"of direct client-to-microservice communication  | <br>41 |\\n| What is the API Gateway pattern?       \", \"                                           | 42     |\\n| Main features in the API Gateway pattern<br>\", \"                                      | 44     |\\n| Using products with API Gateway features         \", \"                                 | 45     |\\n| Drawbacks of the API Gateway pattern                  \", \"                            | 47     |\\n| Additional resources<br>                                   \", \"                       | 48     |\\n| Communication in a microservice architecture<br>                \", \"                  | 48     |\\n| Communication types<br>                                              \", \"             | 49     |\\n| Asynchronous microservice integration enforces microservice's autonomy<br>\", \"        | 50     |\\n| Communication styles<br>                                                       \", \"   | 52     |\\n| Asynchronous message-based communication                                          | \", \"54     |\\n| Single-receiver message-based communication<br>                                   | 55   \", \"  |\\n| Multiple-receivers message-based communication<br>                                | 56     |\\n|\", \" Asynchronous event-driven communication                                           | 56     |\\n| A no\", \"te about messaging technologies for production systems<br>                    | 57     |\\n| Resilient\", \"ly publishing to the event bus<br>                                       | 58     |\\n\\n| Additional re\", \"sources<br>                                                                                     | 58\", \"     |\\n|--------------------------------------------------------------------------------------------\", \"------------------|--------|\\n| Creating, evolving, and versioning microservice APIs and contracts<br\", \">                                       | 59     |\\n| Additional resources<br>                       \", \"                                                              | 59     |\\n| Microservices addressabil\", \"ity and the service registry<br>                                                    | 60     |\\n| Add\", \"itional resources<br>                                                                               \", \"      | 60     |\\n| Creating composite UI based on microservices<br>                                 \", \"                            | 60     |\\n| Additional resources<br>                                   \", \"                                                  | 62     |\\n| Resiliency and high availability in m\", \"icroservices<br>                                                        | 63     |\\n| Health manageme\", \"nt and diagnostics in microservices<br>                                                       | 63  \", \"   |\\n| Additional resources<br>                                                                     \", \"                | 65     |\\n| Orchestrate microservices and multi-container applications for high sca\", \"lability and availability             | <br>66 |\\n| Software platforms for container clustering, orch\", \"estration, and scheduling                                   | 68     |\\n| Using container-based orche\", \"strators in Microsoft Azure                                                       | 68     |\\n| Using\", \" Azure Kubernetes Service<br>                                                                       \", \"    | 69     |\\n| Development environment for Kubernetes                                             \", \"                          | 70     |\\n| Getting started with Azure Kubernetes Service (AKS)<br>      \", \"                                                | 70     |\\n| Deploy with Helm charts into Kubernetes\", \" clusters                                                             | 71     |\\n| Additional resour\", \"ces<br>                                                                                     | 71    \", \" |\\n| Development process for Docker-based applications                                              \", \"              | 72     |\\n| Development environment for Docker apps<br>                              \", \"                                    | 72     |\\n| Development tool choices: IDE or editor            \", \"                                                          | 72     |\\n| Additional resources<br>     \", \"                                                                                | 73     |\\n| .NET la\", \"nguages and frameworks for Docker containers<br>                                                    \", \"  | 73     |\\n| Development workflow for Docker apps                                                 \", \"                        | 73     |\\n| Workflow for developing Docker container-based applications<br>\", \"                                              | 73     |\\n| Step 1. Start coding and create your init\", \"ial application or service baseline                                 | 75     |\\n| Step 2. Create a Do\", \"ckerfile related to an existing .NET base image                                           | 76     |\", \"\\n| Step 3. Create your custom Docker images and embed your application or service in them           \", \"            | 83     |\\n| Step 4. Define your services in docker-compose.yml when building a multi-co\", \"ntainer Docker<br>application<br> | 84     |\\n| Step 5. Build and run your Docker application<br>    \", \"                                                        | 87     |\\n| Step 6. Test your Docker applic\", \"ation using your local Docker host                                            | 89     |\\n\\niii Conten\", \"ts\\n\\n| Simplified workflow when developing containers with Visual Studio<br>              | 90  |\\n|--\", \"----------------------------------------------------------------------------------|-----|\\n| Using Po\", \"werShell commands in a Dockerfile to set up Windows Containers             | 91  |\\n| Designing and D\", \"eveloping Multi-Container and Microservice-Based .NET Applications  |     |\\n|                       \", \"                                                             | 93  |\\n| Design a microservice-oriente\", \"d application<br>                                     | 93  |\\n| Application specifications<br>      \", \"                                               | 93  |\\n| Development team context<br>               \", \"                                        | 94  |\\n| Choosing an architecture                          \", \"                                 | 94  |\\n| Benefits of a microservice-based solution<br>            \", \"                          | 97  |\\n| Downsides of a microservice-based solution<br>                  \", \"                   | 98  |\\n| External versus internal architecture and design patterns              \", \"            | 99  |\\n| The new world: multiple architectural patterns and polyglot microservices100  \", \"     |     |\\n| Creating a simple data-driven CRUD microservice<br>102                             | \", \"    |\\n| Designing a simple CRUD microservice<br>102                                        |     |\\n|\", \" Implementing a simple CRUD microservice with ASP.NET Core103                       |     |\\n| The DB\", \" connection string and environment variables used by Docker containers       | 109 |\\n| Generating Sw\", \"agger description metadata from your ASP.NET Core Web API<br>111      |     |\\n| Defining your multi-\", \"container application with docker-compose.yml<br>116           |     |\\n| Use a database server runni\", \"ng as a container<br>127                                |     |\\n| SQL Server running as a container \", \"with a microservice-related database128          |     |\\n| Seeding with test data on Web application\", \" startup129                               |     |\\n| EF Core InMemory database versus SQL Server runn\", \"ing as a container132              |     |\\n| Using a Redis cache service running in a container132  \", \"                            |     |\\n| Implementing event-based communication between microservices (\", \"integration events)  | 133 |\\n| Using message brokers and service buses for production systems<br>134\", \"              |     |\\n| Integration events135                                                       \", \"       |     |\\n| The event bus<br>136                                                               \", \"|     |\\n| Additional resources<br>138                                                        |     |\", \"\\n| Implementing an event bus with RabbitMQ for the development or test environment138 |     |\\n| Impl\", \"ementing a simple publish method with RabbitMQ139                              |     |\\n| Implementin\", \"g the subscription code with the RabbitMQ API140                        |     |\\n| Additional resourc\", \"es<br>141                                                        |     |\\n\\niv Contents\\n\\n| Subscribing\", \" to events                                                                                         |\", \" 141               |\\n|------------------------------------------------------------------------------\", \"---------------------------------|-------------------|\\n| Publishing events through the event bus    \", \"                                                                   | 142               |\\n| Idempoten\", \"cy in update message events                                                                         \", \" | 149               |\\n| Deduplicating integration event messages                                   \", \"                                   | 150               |\\n| Testing ASP.NET Core services and web app\", \"s                                                                    | 152               |\\n| Testing\", \" in eShopOnContainers                                                                               \", \"   | 155               |\\n| Implement background tasks in microservices with IHostedService and the B\", \"ackground                            | oundService class |\\n|                                        \", \"                                                                       | 157               |\\n| Regis\", \"tering hosted services in your WebHost or Host                                                      \", \"     | 159               |\\n| The IHostedService interface                                           \", \"                                       | 159               |\\n| Implementing IHostedService with a cu\", \"stom hosted service class deriving from the BackgroundService base class |                   |\\n| Add\", \"itional resources                                                                                   \", \"       | 163               |\\n| Implement API Gateways with Ocelot                                   \", \"                                         | 163               |\\n| Architect and design your API Gatew\", \"ays                                                                        | 163               |\\n| I\", \"mplementing your API Gateways with Ocelot                                                           \", \"         | 168               |\\n| Using Kubernetes Ingress plus Ocelot API Gateways                  \", \"                                           | 180               |\\n| Additional cross-cutting features\", \" in an Ocelot API Gateway                                                    | 181               |\\n|\", \" Tackle Business Complexity in a Microservice with DDD and CQRS Patter                              \", \"           | ns 182            |\\n| Apply simplified CQRS and DDD patterns in a microservice         \", \"                                             | 184               |\\n| Additional resources           \", \"                                                                               | 186               |\", \"\\n| Apply CQRS and CQS approaches in a DDD microservice in eShopOnContainers                         \", \"             | 186               |\\n| CQRS and DDD patterns are not top-level architectures          \", \"                                               | 187               |\\n| Implement reads/queries in a \", \"CQRS microservice                                                                |                  \", \" |\\n| Use ViewModels specifically made for client apps, independent from domain mo                   \", \"               | odel constraints  |\\n| Use Dapper as a micro ORM to perform queries                 \", \"                                                 |                   |\\n| Dynamic versus static ViewM\", \"odels                                                                              |                \", \"   |\\n| Additional resources                                                                         \", \"                 |                   |\\n| Design a DDD-oriented microservice                         \", \"                                                   |                   |\\n| Keep the microservice con\", \"text boundaries relatively small                                                     |              \", \"     |\\n| Lavers in DDD microservices                                                                \", \"                   |                   |\\n| Lavels III DDD IIII DSELVICES                            \", \"                                                     | 197               |\\n\\nv Contents\\n\\n| Design a m\", \"icroservice domain model                                                                            \", \"                                               | 199 |\\n|--------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"--------------|-----|\\n| The Domain Entity pattern                                                   \", \"                                                                                 | 199 |\\n| Implement\", \" a microservice domain model with .NET                                                              \", \"                                                | 204 |\\n| Domain model structure in a custom .NET St\", \"andard Library                                                                                      \", \"               | 204 |\\n| Structure aggregates in a custom .NET Standard library                     \", \"                                                                                  | 205 |\\n| Implemen\", \"t domain entities as POCO classes                                                                   \", \"                                                 | 206 |\\n| Encapsulate data in the Domain Entities  \", \"                                                                                                    \", \"                | 207 |\\n| Seedwork (reusable base classes and interfaces for your domain model)     \", \"                                                                                   | 210 |\\n| The cus\", \"tom Entity base class                                                                               \", \"                                                  | 211 |\\n| Repository contracts (interfaces) in the\", \" domain model layer                                                                                 \", \"                 | 212 |\\n| Additional resources                                                     \", \"                                                                                    | 213 |\\n| Implem\", \"ent value objects                                                                                   \", \"                                                   | 213 |\\n| Important characteristics of value obje\", \"cts                                                                                                 \", \"                  | 214 |\\n| Value object implementation in C#                                       \", \"                                                                                     | 215 |\\n| How t\", \"o persist value objects in the database with EF Core 2.0 and later                                  \", \"                                                    | 217 |\\n| Persist value objects as owned entity \", \"types in EF Core 2.0 and later                                                                      \", \"                   | 218 |\\n| Additional resources                                                   \", \"                                                                                      | 221 |\\n| Use \", \"enumeration classes instead of enum types                                                           \", \"                                                     | 221 |\\n| Implement an Enumeration base class  \", \"                                                                                                    \", \"                    | 222 |\\n| Additional resources                                                  \", \"                                                                                       | 223 |\\n| Des\", \"ign validations in the domain model layer                                                           \", \"                                                      | 223 |\\n| Implement validations in the domain \", \"model layer                                                                                         \", \"                     | 224 |\\n| Additional resources                                                 \", \"                                                                                        | 225 |\\n| Cl\", \"ient-side validation (validation in the presentation layers)                                        \", \"                                                       | 226 |\\n| Additional resources               \", \"                                                                                                    \", \"                      | 227 |\\n| Domain events: Design and implementation                            \", \"                                                                                         | 227 |\\n| W\", \"hat is a domain event?                                                                              \", \"                                                        | 228 |\\n| Domain events versus integration e\", \"vents                                                                                               \", \"                       | 228 |\\n| Domain events as a preferred way to trigger side effects across mul\", \"tiple aggregations as a preferred way to trigger side effects across multiple aggregation |     |\\n| \", \"Implement domain events                                                                             \", \"                                                         | 231 |\\n| Conclusions on domain events     \", \"                                                                                                    \", \"                        | 237 |\\n\\nvi\\n\\n| Additional resources<br>238                                  \", \"                                    |     |\\n|-------------------------------------------------------\", \"-------------------------------------------|-----|\\n| Design the infrastructure persistence layer238 \", \"                                                  |     |\\n| The Repository pattern<br>238           \", \"                                                         |     |\\n| Additional resources<br>243      \", \"                                                                |     |\\n| Implement the infrastructu\", \"re persistence layer with Entity Framework Core<br>243                 |     |\\n| Introduction to Ent\", \"ity Framework Core244                                                         |     |\\n| Infrastructu\", \"re in Entity Framework Core from a DDD perspective244                                |     |\\n| Imple\", \"ment custom repositories with Entity Framework Core246                                      |     |\\n\", \"| EF DbContext and IUnitOfWork instance lifetime in your IoC container248                          |\", \"     |\\n| The repository instance lifetime in your IoC container249                                  \", \"      |     |\\n| Table mapping<br>250                                                                \", \"             |     |\\n| Implement the Query Specification pattern253                                 \", \"                    |     |\\n| Use NoSQL databases as a persistence infrastructure255                \", \"                           |     |\\n| Introduction to Azure Cosmos DB and the native Cosmos DB API<br\", \">256                              |     |\\n| Implement .NET code targeting MongoDB and Azure Cosmos D\", \"B<br>258                                 |     |\\n| Design the microservice application layer and Web\", \" API<br>266                                     |     |\\n| Use SOLID principles and Dependency Inject\", \"ion266                                                 |     |\\n| Implement the microservice applicat\", \"ion layer using the Web API<br>267                            |     |\\n| Use Dependency Injection to \", \"inject infrastructure objects into your application layer267         |     |\\n| Implement the Command\", \" and Command Handler patterns<br>271                                        |     |\\n| The Command pr\", \"ocess pipeline: how to trigger a command handler278                                |     |\\n| Impleme\", \"nt the command process pipeline with a mediator pattern (MediatR)<br>281                  |     |\\n| \", \"Apply cross-cutting concerns when processing commands with the Behaviors in MediatR              | 2\", \"87 |\\n| Implement resilient applications<br>                                                         \", \"    | 291 |\\n| Handle partial failure292                                                             \", \"           |     |\\n| Strategies to handle partial failure294                                        \", \"                  |     |\\n| Additional resources<br>295                                             \", \"                         |     |\\n| Implement retries with exponential backoff<br>295                \", \"                                |     |\\n| Implement resilient Entity Framework Core SQL connections2\", \"95                                     |     |\\n| Execution strategies and explicit transactions usin\", \"g BeginTransaction and multiple DbContexts296 |     |\\n| Additional resources<br>298                 \", \"                                                     |     |\\n| Use IHttpClientFactory to implement r\", \"esilient HTTP requests<br>298                               |     |\\n\\nvii Contents\\n\\n| issues with the\", \" original HttpClient class available in .NE1                                   | 298   |\\n|----------\", \"-------------------------------------------------------------------------------------|-------|\\n| Ben\", \"efits of using IHttpClientFactory                                                          | 299   |\", \"\\n| Multiple ways to use IHttpClientFactory                                                       | 3\", \"00   |\\n| How to use Typed Clients with IHttpClientFactory                                           \", \"   | 300   |\\n| Additional resources                                                                 \", \"         | 304   |\\n| Implement HTTP call retries with exponential backoff with IHttpClientFactory an\", \"d Polly polici | es304 |\\n| Add a jitter strategy to the retry policy                                \", \"                     | 305   |\\n| Additional resources                                               \", \"                           | 306   |\\n| Implement the Circuit Breaker pattern                        \", \"                                 | 306   |\\n| Implement Circuit Breaker pattern with IHttpClientFacto\", \"ry and Polly                           | 307   |\\n| Test Http retries and circuit breakers in eShopOn\", \"Containers                                   | 308   |\\n| Additional resources                       \", \"                                                   | 310   |\\n| Health monitoring                    \", \"                                                         | 310   |\\n| Implement health checks in ASP.\", \"NET Core services                                              | 311   |\\n| Use watchdogs            \", \"                                                                     | 315   |\\n| Health checks when \", \"using orchestrators                                                        | 317   |\\n| Advanced moni\", \"toring: visualization, analysis, and alerts                                      | 317   |\\n| Additio\", \"nal resources                                                                          | 318   |\\n| M\", \"ake secure .NET Microservices and Web Applications                                           | 319  \", \" |\\n| Implement authentication in .NET microservices and web applications                           |\", \" 319   |\\n| Authenticate with ASP.NET Core Identity                                                  \", \"     | 320   |\\n| Authenticate with external providers                                               \", \"           | 321   |\\n| Authenticate with bearer tokens                                              \", \"                 | 323   |\\n| Authenticate with an OpenID Connect or OAuth 2.0 Identity provider     \", \"                       | 324   |\\n| Issue security tokens from an ASP.NET Core service               \", \"                             | 325   |\\n| Consume security tokens                                    \", \"                                   |       |\\n| Additional resources                                 \", \"                                         | 327   |\\n| About authorization in .NET microservices and w\", \"eb applications                                | 327   |\\n| Implement role-based authorization       \", \"                                                     |       |\\n| Implement policy-based authorizatio\", \"n                                                          |       |\\n| Authorization and minimal api\", \"s                                                                |       |\\n| Additional resources   \", \"                                                                       | 330   |\\n\\n| .NET Microservic\", \"es Architecture key takeaways<br>                | 334 |\\n|------------------------------------------\", \"------------------------|-----|\\n| Additional resources<br>333                                      |\", \"     |\\n| Use Azure Key Vault to protect secrets at production time<br>332 |     |\\n| Store secrets wi\", \"th the ASP.NET Core Secret Manager<br>331        |     |\\n| Store secrets in environment variables<br\", \">331                    |     |\\n| Store application secrets safely during development330           |\", \"     |\\n\\nix Contents\\n\\n**CHAPTER** 1\\n\\n# <span id=\\\"page-15-0\\\"></span>Introduction to Containers and Doc\", \"ker\\n\\nContainerization is an approach to software development in which an application or service, its\", \" dependencies, and its configuration (abstracted as deployment manifest files) are packaged together\", \" as a container image. The containerized application can be tested as a unit and deployed as a conta\", \"iner image instance to the host operating system (OS).\\n\\nJust as shipping containers allow goods to b\", \"e transported by ship, train, or truck regardless of the cargo inside, software containers act as a \", \"standard unit of software deployment that can contain different code and dependencies. Containerizin\", \"g software this way enables developers and IT professionals to deploy them across environments with \", \"little or no modification.\\n\\nContainers also isolate applications from each other on a shared OS. Con\", \"tainerized applications run on top of a container host that in turn runs on the OS (Linux or Windows\", \"). Containers therefore have a significantly smaller footprint than virtual machine (VM) images.\\n\\nEa\", \"ch container can run a whole web application or a service, as shown in Figure 2-1. In this example, \", \"Docker host is a container host, and App1, App2, Svc 1, and Svc 2 are containerized applications or \", \"services.\\n\\n![](_page_15_Picture_7.jpeg)\\n\\n*Figure 2-1. Multiple containers running on a container hos\", \"t*\\n\\nAnother benefit of containerization is scalability. You can scale out quickly by creating new co\", \"ntainers for short-term tasks. From an application point of view, instantiating an image (creating a\", \" container) is similar to instantiating a process like a service or a web app. For reliability, howe\", \"ver, when you run multiple instances of the same image across multiple host servers, you typically w\", \"ant each container (image instance) to run in a different host server or VM in different fault domai\", \"ns.\\n\\nIn short, containers offer the benefits of isolation, portability, agility, scalability, and co\", \"ntrol across the whole application lifecycle workflow. The most important benefit is the environment\", \"'s isolation provided between Dev and Ops.\\n\\n## <span id=\\\"page-16-0\\\"></span>What is Docker?\\n\\n[Docker]\", \"(https://www.docker.com/) is an [open-source project](https://github.com/docker/docker) for automati\", \"ng the deployment of applications as portable, selfsufficient containers that can run on the cloud o\", \"r on-premises. Docker is also a [company](https://www.docker.com/) that promotes and evolves this te\", \"chnology, working in collaboration with cloud, Linux, and Windows vendors, including Microsoft.\\n\\n![]\", \"(_page_16_Picture_4.jpeg)\\n\\n*Figure 2-2. Docker deploys containers at all layers of the hybrid cloud.\", \"*\\n\\nDocker containers can run anywhere, on-premises in the customer datacenter, in an external servic\", \"e provider or in the cloud, on Azure. Docker image containers can run natively on Linux and Windows.\", \" However, Windows images can run only on Windows hosts and Linux images can run on Linux hosts and W\", \"indows hosts (using a Hyper-V Linux VM, so far), where host means a server or a VM.\\n\\nDevelopers can \", \"use development environments on Windows, Linux, or macOS. On the development computer, the developer\", \" runs a Docker host where Docker images are deployed, including the app and its dependencies. Develo\", \"pers who work on Linux or on macOS use a Docker host that is Linux based, and they can create images\", \" only for Linux containers. (Developers working on macOS can edit code or run the Docker CLI from ma\", \"cOS, but as of the time of this writing, containers don't run\\n\\ndirectly on macOS.) Developers who wo\", \"rk on Windows can create images for either Linux or Windows Containers.\\n\\nTo host containers in devel\", \"opment environments and provide additional developer tools, Docker ships Docker Desktop for [Windows\", \"](https://hub.docker.com/editions/community/docker-ce-desktop-windows) or for [macOS.](https://hub.d\", \"ocker.com/editions/community/docker-ce-desktop-mac) These products install the necessary VM (the Doc\", \"ker host) to host the containers.\\n\\nTo run [Windows Containers,](https://docs.microsoft.com/virtualiz\", \"ation/windowscontainers/about/) there are two types of runtimes:\\n\\n- Windows Server Containers provid\", \"e application isolation through process and namespace isolation technology. A Windows Server Contain\", \"er shares a kernel with the container host and with all containers running on the host.\\n- Hyper-V Co\", \"ntainers expand on the isolation provided by Windows Server Containers by running each container in \", \"a highly optimized virtual machine. In this configuration, the kernel of the container host isn't sh\", \"ared with the Hyper-V Containers, providing better isolation.\\n\\nThe images for these containers are c\", \"reated the same way and function the same. The difference is in how the container is created from th\", \"e image running a Hyper-V Container requires an extra parameter. For details, see [Hyper-V Container\", \"s.](https://docs.microsoft.com/virtualization/windowscontainers/manage-containers/hyperv-container)\\n\", \"\\n### <span id=\\\"page-17-0\\\"></span>**Comparing Docker containers with virtual machines**\\n\\nFigure 2-3 s\", \"hows a comparison between VMs and Docker containers.\\n\\n![](_page_17_Figure_8.jpeg)\\n\\n| Virtual Machine\", \"s                                                                                                   \", \"                                                                          | Docker Containers       \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                               |\\n|--\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------|-----------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"---------|\\n| Virtual machines include the application, the<br>required libraries or binaries, and a \", \"full guest<br>operating system. Full virtualization requires<br>more resources than containerization\", \". | Containers include the application and all its<br>dependencies. However, they share the OS kerne\", \"l<br>with other containers, running as isolated<br>processes in user space on the host operating<br>\", \"system. (Except in Hyper-V containers, where<br>each container runs inside of a special virtual<br>m\", \"achine per container.) |\\n\\n*Figure 2-3. Comparison of traditional virtual machines to Docker containe\", \"rs*\\n\\nFor VMs, there are three base layers in the host server, from the bottom-up: infrastructure, Ho\", \"st Operating System and a Hypervisor and on top of all that each VM has its own OS and all necessary\", \" libraries. For Docker, the host server only has the infrastructure and the OS and on top of that, t\", \"he container engine, that keeps container isolated but sharing the base OS services.\\n\\nBecause contai\", \"ners require far fewer resources (for example, they don't need a full OS), they're easy to deploy an\", \"d they start fast. This allows you to have higher density, meaning that it allows you to run more se\", \"rvices on the same hardware unit, thereby reducing costs.\\n\\nAs a side effect of running on the same k\", \"ernel, you get less isolation than VMs.\\n\\nThe main goal of an image is that it makes the environment \", \"(dependencies) the same across different deployments. This means that you can debug it on your machi\", \"ne and then deploy it to another machine with the same environment guaranteed.\\n\\nA container image is\", \" a way to package an app or service and deploy it in a reliable and reproducible way. You could say \", \"that Docker isn't only a technology but also a philosophy and a process.\\n\\nWhen using Docker, you won\", \"'t hear developers say, \\\"It works on my machine, why not in production?\\\" They can simply say, \\\"It ru\", \"ns on Docker\\\", because the packaged Docker application can be executed on any supported Docker envir\", \"onment, and it runs the way it was intended to on all deployment targets (such as Dev, QA, staging, \", \"and production).\\n\\n### <span id=\\\"page-18-0\\\"></span>**A simple analogy**\\n\\nPerhaps a simple analogy can\", \" help getting the grasp of the core concept of Docker.\\n\\nLet's go back in time to the 1950s for a mom\", \"ent. There were no word processors, and the photocopiers were used everywhere (kind of).\\n\\nImagine yo\", \"u're responsible for quickly issuing batches of letters as required, to mail them to customers, usin\", \"g real paper and envelopes, to be delivered physically to each customer's address (there was no emai\", \"l back then).\\n\\nAt some point, you realize the letters are just a composition of a large set of parag\", \"raphs, which are picked and arranged as needed, according to the purpose of the letter, so you devis\", \"e a system to issue letters quickly, expecting to get a hefty raise.\\n\\nThe system is simple:\\n\\n1. You \", \"begin with a deck of transparent sheets containing one paragraph each.\\n\\n- 2. To issue a set of lette\", \"rs, you pick the sheets with the paragraphs you need, then you stack and align them so they look and\", \" read fine.\\n- 3. Finally, you place the set in the photocopier and press start to produce as many le\", \"tters as required.\\n\\nSo, simplifying, that's the core idea of Docker.\\n\\nIn Docker, each layer is the r\", \"esulting set of changes that happen to the filesystem after executing a command, such as, installing\", \" a program.\\n\\nSo, when you \\\"look\\\" at the filesystem after the layer has been copied, you see all the \", \"files, included in the layer when the program was installed.\\n\\nYou can think of an image as an auxili\", \"ary read-only hard disk ready to be installed in a \\\"computer\\\" where the operating system is already \", \"installed.\\n\\nSimilarly, you can think of a container as the \\\"computer\\\" with the image hard disk insta\", \"lled. The container, just like a computer, can be powered on or off.\\n\\n## <span id=\\\"page-19-0\\\"></span\", \">Docker terminology\\n\\nThis section lists terms and definitions you should be familiar with before get\", \"ting deeper into Docker. For further definitions, see the extensive [glossary](https://docs.docker.c\", \"om/glossary/) provided by Docker.\\n\\n**Container image**: A package with all the dependencies and info\", \"rmation needed to create a container. An image includes all the dependencies (such as frameworks) pl\", \"us deployment and execution configuration to be used by a container runtime. Usually, an image deriv\", \"es from multiple base images that are layers stacked on top of each other to form the container's fi\", \"lesystem. An image is immutable once it has been created.\\n\\n**Dockerfile**: A text file that contains\", \" instructions for building a Docker image. It's like a batch script, the first line states the base \", \"image to begin with and then follow the instructions to install required programs, copy files, and s\", \"o on, until you get the working environment you need.\\n\\n**Build**: The action of building a container\", \" image based on the information and context provided by its Dockerfile, plus additional files in the\", \" folder where the image is built. You can build images with the following Docker command:\\n\\ndocker bu\", \"ild\\n\\n**Container**: An instance of a Docker image. A container represents the execution of a single \", \"application, process, or service. It consists of the contents of a Docker image, an execution enviro\", \"nment, and a standard set of instructions. When scaling a service, you create multiple instances of \", \"a container from the same image. Or a batch job can create multiple containers from the same image, \", \"passing different parameters to each instance.\\n\\n**Volumes**: Offer a writable filesystem that the co\", \"ntainer can use. Since images are read-only but most programs need to write to the filesystem, volum\", \"es add a writable layer, on top of the container image, so the programs have access to a writable fi\", \"lesystem. The program doesn't know it's accessing a\\n\\nlayered filesystem, it's just the filesystem as\", \" usual. Volumes live in the host system and are managed by Docker.\\n\\n**Tag**: A mark or label you can\", \" apply to images so that different images or versions of the same image (depending on the version nu\", \"mber or the target environment) can be identified.\\n\\n**Multi-stage Build**: Is a feature, since Docke\", \"r 17.05 or higher, that helps to reduce the size of the final images. For example, a large base imag\", \"e, containing the SDK can be used for compiling and publishing and then a small runtime-only base im\", \"age can be used to host the application.\\n\\n**Repository (repo)**: A collection of related Docker imag\", \"es, labeled with a tag that indicates the image version. Some repos contain multiple variants of a s\", \"pecific image, such as an image containing SDKs (heavier), an image containing only runtimes (lighte\", \"r), etc. Those variants can be marked with tags. A single repo can contain platform variants, such a\", \"s a Linux image and a Windows image.\\n\\n**Registry**: A service that provides access to repositories. \", \"The default registry for most public images is [Docker Hub](https://hub.docker.com/) (owned by Docke\", \"r as an organization). A registry usually contains repositories from multiple teams. Companies often\", \" have private registries to store and manage images they've created. Azure Container Registry is ano\", \"ther example.\\n\\n**Multi-arch image**: For multi-architecture (or [multi-platform](https://docs.docker\", \".com/build/building/multi-platform/)), it's a Docker feature that simplifies the selection of the ap\", \"propriate image, according to the platform where Docker is running. For example, when a Dockerfile r\", \"equests a base image **FROM mcr.microsoft.com/dotnet/sdk:7.0** from the registry, it actually gets *\", \"*7.0-nanoserver-ltsc2022**, **7.0-nanoserver-1809** or **7.0-bullseye-slim**, depending on the opera\", \"ting system and version where Docker is running.\\n\\n**Docker Hub**: A public registry to upload images\", \" and work with them. Docker Hub provides Docker image hosting, public or private registries, build t\", \"riggers and web hooks, and integration with GitHub and Bitbucket.\\n\\n**Azure Container Registry**: A p\", \"ublic resource for working with Docker images and its components in Azure. This provides a registry \", \"that's close to your deployments in Azure and that gives you control over access, making it possible\", \" to use your Azure Active Directory groups and permissions.\\n\\n**Docker Trusted Registry (DTR)**: A Do\", \"cker registry service (from Docker) that can be installed onpremises so it lives within the organiza\", \"tion's datacenter and network. It's convenient for private images that should be managed within the \", \"enterprise. Docker Trusted Registry is included as part of the Docker Datacenter product.\\n\\n**Docker \", \"Desktop**: Development tools for Windows and macOS for building, running, and testing containers loc\", \"ally. Docker Desktop for Windows provides development environments for both Linux and Windows Contai\", \"ners. The Linux Docker host on Windows is based on a [Hyper-V](https://www.microsoft.com/cloud-platf\", \"orm/server-virtualization) virtual machine. The host for Windows Containers is directly based on Win\", \"dows. Docker Desktop for Mac is based on the Apple Hypervisor framework and the [xhyve hypervisor,](\", \"https://github.com/mist64/xhyve) which provides a Linux Docker host virtual machine on macOS. Docker\", \" Desktop for Windows and for Mac replaces Docker Toolbox, which was based on Oracle VirtualBox.\\n\\n**C\", \"ompose**: A command-line tool and YAML file format with metadata for defining and running multiconta\", \"iner applications. You define a single application based on multiple images with one or more .yml fi\", \"les that can override values depending on the environment. After you've created the definitions, you\", \" can deploy the whole multi-container application with a single command (docker-compose up) that cre\", \"ates a container per image on the Docker host.\\n\\n**Cluster**: A collection of Docker hosts exposed as\", \" if it were a single virtual Docker host, so that the application can scale to multiple instances of\", \" the services spread across multiple hosts within the cluster. Docker clusters can be created with K\", \"ubernetes, Azure Service Fabric, Docker Swarm and Mesosphere DC/OS.\\n\\n**Orchestrator**: A tool that s\", \"implifies the management of clusters and Docker hosts. Orchestrators enable you to manage their imag\", \"es, containers, and hosts through a command-line interface (CLI) or a graphical UI. You can manage c\", \"ontainer networking, configurations, load balancing, service discovery, high availability, Docker ho\", \"st configuration, and more. An orchestrator is responsible for running, distributing, scaling, and h\", \"ealing workloads across a collection of nodes. Typically, orchestrator products are the same product\", \"s that provide cluster infrastructure, like Kubernetes and Azure Service Fabric, among other offerin\", \"gs in the market.\\n\\n## <span id=\\\"page-21-0\\\"></span>Docker containers, images, and registries\\n\\nWhen us\", \"ing Docker, a developer creates an app or service and packages it and its dependencies into a contai\", \"ner image. An image is a static representation of the app or service and its configuration and depen\", \"dencies.\\n\\nTo run the app or service, the app's image is instantiated to create a container, which wi\", \"ll be running on the Docker host. Containers are initially tested in a development environment or PC\", \".\\n\\nDevelopers should store images in a registry, which acts as a library of images and is needed whe\", \"n deploying to production orchestrators. Docker maintains a public registry via [Docker Hub;](https:\", \"//hub.docker.com/) other vendors provide registries for different collections of images, including [\", \"Azure Container Registry.](https://azure.microsoft.com/services/container-registry/)  Alternatively,\", \" enterprises can have a private registry on-premises for their own Docker images.\\n\\nFigure 2-4 shows \", \"how images and registries in Docker relate to other components. It also shows the multiple registry \", \"offerings from vendors.\\n\\n![](_page_22_Figure_0.jpeg)\\n\\n*Figure 2-4. Taxonomy of Docker terms and conc\", \"epts*\\n\\nThe registry is like a bookshelf where images are stored and available to be pulled for build\", \"ing containers to run services or web apps. There are private Docker registries on-premises and on t\", \"he public cloud. Docker Hub is a public registry maintained by Docker, along the Docker Trusted Regi\", \"stry an enterprise-grade solution, Azure offers the Azure Container Registry. AWS, Google, and other\", \"s also have container registries.\\n\\nPutting images in a registry lets you store static and immutable \", \"application bits, including all their dependencies at a framework level. Those images can then be ve\", \"rsioned and deployed in multiple environments and therefore provide a consistent deployment unit.\\n\\nP\", \"rivate image registries, either hosted on-premises or in the cloud, are recommended when:\\n\\n- Your im\", \"ages must not be shared publicly due to confidentiality.\\n- You want to have minimum network latency \", \"between your images and your chosen deployment environment. For example, if your production environm\", \"ent is Azure cloud, you probably want to store your images in [Azure Container Registry](https://azu\", \"re.microsoft.com/services/container-registry/) so that network latency will be minimal. In a similar\", \" way, if your production environment is on-premises, you might want to have an on-premises Docker Tr\", \"usted Registry available within the same local network.\\n\\n# <span id=\\\"page-23-0\\\"></span>Choosing Betw\", \"een .NET and .NET Framework for Docker Containers\\n\\nThere are two supported frameworks for building s\", \"erver-side containerized Docker applications with .NET: [.NET Framework and .NET 7.](https://dotnet.\", \"microsoft.com/download) They share many .NET platform components, and you can share code across the \", \"two. However, there are fundamental differences between them, and which framework you use will depen\", \"d on what you want to accomplish. This section provides guidance on when to choose each framework.\\n\\n\", \"## <span id=\\\"page-23-1\\\"></span>General guidance\\n\\nThis section provides a summary of when to choose .\", \"NET 7 or .NET Framework. We provide more details about these choices in the sections that follow.\\n\\nU\", \"se .NET 7, with Linux or Windows Containers, for your containerized Docker server application when:\\n\", \"\\n- You have cross-platform needs. For example, you want to use both Linux and Windows Containers.\\n- \", \"Your application architecture is based on microservices.\\n- You need to start containers fast and wan\", \"t a small footprint per container to achieve better density or more containers per hardware unit in \", \"order to lower your costs.\\n\\nIn short, when you create new containerized .NET applications, you shoul\", \"d consider .NET 7 as the default choice. It has many benefits and fits best with the containers phil\", \"osophy and style of working.\\n\\nAn extra benefit of using .NET 7 is that you can run side-by-side .NET\", \" versions for applications within the same machine. This benefit is more important for servers or VM\", \"s that do not use containers, because containers isolate the versions of .NET that the app needs. (A\", \"s long as they are compatible with the underlying OS.)\\n\\nUse .NET Framework for your containerized Do\", \"cker server application when:\\n\\n\\u2022 Your application currently uses .NET Framework and has strong depen\", \"dencies on Windows.\\n\\n- You need to use Windows APIs that are not supported by .NET 7.\\n- You need to \", \"use third-party .NET libraries or NuGet packages that are not available for .NET 7.\\n\\nUsing .NET Fram\", \"ework on Docker can improve your deployment experiences by minimizing deployment issues. This [\\\"lift\", \" and shift\\\" scenario](https://aka.ms/liftandshiftwithcontainersebook) is important for containerizin\", \"g legacy applications that were originally developed with the traditional .NET Framework, like ASP.N\", \"ET WebForms, MVC web apps, or WCF (Windows Communication Foundation) services.\\n\\n### <span id=\\\"page-2\", \"4-2\\\"></span>**Additional resources**\\n\\n- **E-book: Modernize existing .NET Framework applications wit\", \"h Azure and Windows Containers** <https://aka.ms/liftandshiftwithcontainersebook>\\n- **Sample apps: M\", \"odernization of legacy ASP.NET web apps by using Windows Containers** <https://aka.ms/eshopmodernizi\", \"ng>\\n\\n## <span id=\\\"page-24-0\\\"></span>When to choose .NET for Docker containers\\n\\nThe modularity and li\", \"ghtweight nature of .NET 7 makes it perfect for containers. When you deploy and start a container, i\", \"ts image is far smaller with .NET 7 than with .NET Framework. In contrast, to use .NET Framework for\", \" a container, you must base your image on the Windows Server Core image, which is a lot heavier than\", \" the Windows Nano Server or Linux images that you use for .NET 7.\\n\\nAdditionally, .NET 7 is cross-pla\", \"tform, so you can deploy server apps with Linux or Windows container images. However, if you are usi\", \"ng the traditional .NET Framework, you can only deploy images based on Windows Server Core.\\n\\nThe fol\", \"lowing is a more detailed explanation of why to choose .NET 7.\\n\\n### <span id=\\\"page-24-1\\\"></span>**De\", \"veloping and deploying cross platform**\\n\\nClearly, if your goal is to have an application (web app or\", \" service) that can run on multiple platforms supported by Docker (Linux and Windows), the right choi\", \"ce is .NET 7, because .NET Framework only supports Windows.\\n\\n.NET 7 also supports macOS as a develop\", \"ment platform. However, when you deploy containers to a Docker host, that host must (currently) be b\", \"ased on Linux or Windows. For example, in a development environment, you could use a Linux VM runnin\", \"g on a Mac.\\n\\n[Visual Studio](https://www.visualstudio.com/vs/) provides an integrated development en\", \"vironment (IDE) for Windows and supports Docker development.\\n\\n[Visual Studio for Mac](https://www.vi\", \"sualstudio.com/vs/visual-studio-mac/) is an IDE, evolution of Xamarin Studio, that runs on macOS and\", \" supports Docker-based application development. This tool should be the preferred choice for develop\", \"ers working in Mac machines who also want to use a powerful IDE.\\n\\nYou can also use [Visual Studio Co\", \"de](https://code.visualstudio.com/) on macOS, Linux, and Windows. Visual Studio Code fully supports \", \".NET 7, including IntelliSense and debugging. Because VS Code is a lightweight editor, you can use i\", \"t to develop containerized apps on the machine in conjunction with the Docker CLI and the [.NET CLI.\", \"](https://docs.microsoft.com/dotnet/core/tools/) You can also target .NET 7 with most third-party ed\", \"itors like Sublime, Emacs, vi, and the open-source OmniSharp project, which also provides IntelliSen\", \"se support.\\n\\nIn addition to the IDEs and editors, you can use the [.NET CLI](https://docs.microsoft.\", \"com/dotnet/core/tools/) for all supported platforms.\\n\\n### <span id=\\\"page-25-0\\\"></span>**Using contai\", \"ners for new (\\\"green-field\\\") projects**\\n\\nContainers are commonly used in conjunction with a microser\", \"vices architecture, although they can also be used to containerize web apps or services that follow \", \"any architectural pattern. You can use .NET Framework on Windows Containers, but the modularity and \", \"lightweight nature of .NET 7 makes it perfect for containers and microservices architectures. When y\", \"ou create and deploy a container, its image is far smaller with .NET 7 than with .NET Framework.\\n\\n##\", \"# <span id=\\\"page-25-1\\\"></span>**Create and deploy microservices on containers**\\n\\nYou could use the t\", \"raditional .NET Framework for building microservices-based applications (without containers) by usin\", \"g plain processes. That way, because the .NET Framework is already installed and shared across proce\", \"sses, processes are light and fast to start. However, if you are using containers, the image for the\", \" traditional .NET Framework is also based on Windows Server Core and that makes it too heavy for a m\", \"icroservices-on-containers approach. However, teams have been looking for opportunities to improve t\", \"he experience for .NET Framework users as well. Recently, size of the [Windows Server Core container\", \" images have been reduced to >40% smaller.](https://devblogs.microsoft.com/dotnet/we-made-windows-se\", \"rver-core-container-images-40-smaller)\\n\\nOn the other hand, .NET 7 is the best candidate if you're em\", \"bracing a microservices-oriented system that is based on containers because .NET 7 is lightweight. I\", \"n addition, its related container images, for either Linux or Windows Nano Server, are lean and smal\", \"l, making containers light and fast to start.\\n\\nA microservice is meant to be as small as possible: t\", \"o be light when spinning up, to have a small footprint, to have a small Bounded Context (check DDD, \", \"[Domain-Driven Design\\\\)](https://en.wikipedia.org/wiki/Domain-driven_design), to represent a small a\", \"rea of concerns, and to be able to start and stop fast. For those requirements, you will want to use\", \" small and fast-to-instantiate container images like the .NET 7 container image.\\n\\nA microservices ar\", \"chitecture also allows you to mix technologies across a service boundary. This approach enables a gr\", \"adual migration to .NET 7 for new microservices that work in conjunction with other microservices or\", \" with services developed with Node.js, Python, Java, GoLang, or other technologies.\\n\\n### <span id=\\\"p\", \"age-25-2\\\"></span>**Deploying high density in scalable systems**\\n\\nWhen your container-based system ne\", \"eds the best possible density, granularity, and performance, .NET and ASP.NET Core are your best opt\", \"ions. ASP.NET Core is up to 10 times faster than ASP.NET in the traditional .NET Framework, and it l\", \"eads to other popular industry technologies for microservices, such as Java servlets, Go, and Node.j\", \"s.\\n\\nThis approach is especially relevant for microservices architectures, where you could have hundr\", \"eds of microservices (containers) running. With ASP.NET Core images (based on the .NET runtime) on L\", \"inux or Windows Nano, you can run your system with a much lower number of servers or VMs, ultimately\", \" saving costs in infrastructure and hosting.\\n\\n## <span id=\\\"page-26-0\\\"></span>When to choose .NET Fra\", \"mework for Docker containers\\n\\nWhile .NET 7 offers significant benefits for new applications and appl\", \"ication patterns, .NET Framework will continue to be a good choice for many existing scenarios.\\n\\n###\", \" <span id=\\\"page-26-1\\\"></span>**Migrating existing applications directly to a Windows Server containe\", \"r**\\n\\nYou might want to use Docker containers just to simplify deployment, even if you are not creati\", \"ng microservices. For example, perhaps you want to improve your DevOps workflow with Docker containe\", \"rs can give you better isolated test environments and can also eliminate deployment issues caused by\", \" missing dependencies when you move to a production environment. In cases like these, even if you ar\", \"e deploying a monolithic application, it makes sense to use Docker and Windows Containers for your c\", \"urrent .NET Framework applications.\\n\\nIn most cases for this scenario, you will not need to migrate y\", \"our existing applications to .NET 7; you can use Docker containers that include the traditional .NET\", \" Framework. However, a recommended approach is to use .NET 7 as you extend an existing application, \", \"such as writing a new service in ASP.NET Core.\\n\\n## <span id=\\\"page-26-2\\\"></span>**Using third-party .\", \"NET libraries or NuGet packages not available for .NET 7**\\n\\nThird-party libraries are quickly embrac\", \"ing [.NET Standard,](https://docs.microsoft.com/dotnet/standard/net-standard) which enables code sha\", \"ring across all .NET flavors, including .NET 7. With .NET Standard 2.0 and later, the API surface co\", \"mpatibility across different frameworks has become significantly larger. Even more, .NET Core 2.x an\", \"d newer applications can also directly reference existing .NET Framework libraries (see [.NET Framew\", \"ork 4.6.1 supporting](https://github.com/dotnet/standard/blob/v2.1.0/docs/planning/netstandard-2.0/R\", \"EADME.md#net-framework-461-supporting-net-standard-20)  [.NET Standard 2.0\\\\)](https://github.com/dot\", \"net/standard/blob/v2.1.0/docs/planning/netstandard-2.0/README.md#net-framework-461-supporting-net-st\", \"andard-20).\\n\\nIn addition, the [Windows Compatibility Pack](https://docs.microsoft.com/dotnet/core/po\", \"rting/windows-compat-pack) extends the API surface available for .NET Standard 2.0 on Windows. This \", \"pack allows recompiling most existing code to .NET Standard 2.x with little or no modification, to r\", \"un on Windows.\\n\\nHowever, even with that exceptional progression since .NET Standard 2.0 and .NET Cor\", \"e 2.1 or later, there might be cases where certain NuGet packages need Windows to run and might not \", \"support .NET Core or later. If those packages are critical for your application, then you will need \", \"to use .NET Framework on Windows Containers.\\n\\n### <span id=\\\"page-26-3\\\"></span>**Using .NET technolog\", \"ies not available for .NET 7**\\n\\nSome .NET Framework technologies aren't available in .NET 7. Some of\", \" them might become available in later releases, but others don't fit the new application patterns ta\", \"rgeted by .NET Core and might never be available.\\n\\nThe following list shows most of the technologies\", \" that aren't available in .NET 7:\\n\\n- ASP.NET Web Forms. This technology is only available on .NET Fr\", \"amework. Currently there are no plans to bring ASP.NET Web Forms to .NET or later.\\n- Workflow-relate\", \"d services. Windows Workflow Foundation (WF), Workflow Services (WCF + WF in a single service), and \", \"WCF Data Services (formerly known as ADO.NET Data Services) are only available on .NET Framework. Th\", \"ere are currently no plans to bring them to .NET 7.\\n\\nIn addition to the technologies listed in the o\", \"fficial [.NET roadmap,](https://github.com/dotnet/core/blob/main/roadmap.md) other features might be\", \" ported to the new [unified .NET platform.](https://devblogs.microsoft.com/dotnet/introducing-net-5/\", \") You might consider participating in the discussions on GitHub so that your voice can be heard. And\", \" if you think something is missing, file a new issue in the [dotnet/runtime](https://github.com/dotn\", \"et/runtime/issues/new) GitHub repository.\\n\\n### <span id=\\\"page-27-0\\\"></span>**Using a platform or API\", \" that doesn't support .NET 7**\\n\\nSome Microsoft and third-party platforms don't support .NET 7. For e\", \"xample, some Azure services provide an SDK that isn't yet available for consumption on .NET 7 yet. M\", \"ost Azure SDK should eventually be ported to .NET 7/.NET Standard, but some might not for several re\", \"asons. You can see the available Azure SDKs in the [Azure SDK Latest Releases](https://azure.github.\", \"io/azure-sdk/releases/latest/index.html) page.\\n\\nIn the meantime, if any platform or service in Azure\", \" still doesn't support .NET 7 with its client API, you can use the equivalent REST API from the Azur\", \"e service or the client SDK on .NET Framework.\\n\\n### <span id=\\\"page-27-1\\\"></span>**Porting existing A\", \"SP.NET application to .NET 7**\\n\\n.NET Core is a revolutionary step forward from .NET Framework. It of\", \"fers a host of advantages over .NET Framework across the board from productivity to performance, and\", \" from cross-platform support to developer satisfaction. If you are using .NET Framework and planning\", \" to migrate your application to .NET Core or .NET 5+, see [Porting Existing ASP.NET Apps to .NET Cor\", \"e.](https://docs.microsoft.com/dotnet/architecture/porting-existing-aspnet-apps/)\\n\\n### **Additional \", \"resources**\\n\\n- **.NET fundamentals** [https://learn.microsoft.com/dotnet/fundamentals](https://docs.\", \"microsoft.com/dotnet/fundamentals/index.yml)\\n- **Porting Projects to .NET 5** [https://learn.microso\", \"ft.com/events/dotnetconf-2020/porting-projects-to-net-5](https://docs.microsoft.com/Events/dotnetCon\", \"f/2020/Porting-Projects-to-NET-5)\\n- **.NET on Docker Guide** [https://learn.microsoft.com/dotnet/cor\", \"e/docker/introduction](https://docs.microsoft.com/dotnet/core/docker/introduction)\\n\\n## <span id=\\\"pag\", \"e-27-2\\\"></span>Decision table: .NET implementations to use for Docker\\n\\nThe following decision table \", \"summarizes whether to use .NET Framework or .NET 7. Remember that for Linux containers, you need Lin\", \"ux-based Docker hosts (VMs or servers), and that for Windows Containers, you need Windows Server-bas\", \"ed Docker hosts (VMs or servers).\\n\\n### **Important**\\n\\nYour development machines will run one Docker \", \"host, either Linux or Windows. Related microservices that you want to run and test together in one s\", \"olution will all need to run on the same container platform.\\n\\n| Architecture / App Type             \", \"                                 | Linux containers                                                 \", \"                    | Windows Containers                                                            \", \"                         |\\n|----------------------------------------------------------------------|-\", \"-------------------------------------------------------------------------------------|--------------\", \"------------------------------------------------------------------------------------------|\\n| Micros\", \"ervices on containers                                          | .NET 7                             \", \"                                                  | .NET 7                                          \", \"                                                       |\\n| Monolithic app                           \", \"                            | .NET 7                                                                \", \"               | .NET Framework<br>.NET 7                                                           \", \"                    |\\n| Best-in-class performance and<br>scalability                         | .NET \", \"7                                                                               | .NET 7            \", \"                                                                                     |\\n| Windows Ser\", \"ver legacy app (\\\"brown<br>field\\\") migration to containers | \\u2013                                       \", \"                                             | .NET Framework                                       \", \"                                                  |\\n| New container-based development<br>(\\\"green-fie\", \"ld\\\")                   | .NET 7                                                                     \", \"          | .NET 7                                                                                  \", \"               |\\n| ASP.NET Core                                                         | .NET 7    \", \"                                                                           | .NET 7 (recommended)<br\", \">.NET Framework                                                                 |\\n| ASP.NET 4 (MVC 5\", \", Web API 2, and<br>Web Forms)                       | \\u2013                                            \", \"                                        | .NET Framework                                            \", \"                                             |\\n| SignalR services                                   \", \"                  | .NET Core 2.1 or higher<br>version                                              \", \"     | .NET Framework<br>.NET Core 2.1 or higher<br>version                                         \", \"          |\\n| WCF, WF, and other legacy<br>frameworks                              | WCF in .NET Cor\", \"e (client<br>library only) or CoreWCF                                 | .NET Framework<br>WCF in .NE\", \"T 7 (client library<br>only) or CoreWCF                                    |\\n| Consumption of Azure \", \"services                                        | .NET 7<br>(eventually most Azure<br>services will \", \"provide client<br>SDKs for .NET 7) | .NET Framework<br>.NET 7<br>(eventually most Azure<br>services \", \"will provide client<br>SDKs for .NET 7) |\\n\\n## <span id=\\\"page-28-0\\\"></span>What OS to target with .NE\", \"T containers\\n\\nGiven the diversity of operating systems supported by Docker and the differences betwe\", \"en .NET Framework and .NET 7, you should target a specific OS and specific versions depending on the\", \" framework you are using.\\n\\nFor Windows, you can use Windows Server Core or Windows Nano Server. Thes\", \"e Windows versions provide different characteristics (IIS in Windows Server Core versus a self-hoste\", \"d web server like Kestrel in Nano Server) that might be needed by .NET Framework or .NET 7, respecti\", \"vely.\\n\\nFor Linux, multiple distros are available and supported in official .NET Docker images (like \", \"Debian).\\n\\n![](_page_29_Figure_1.jpeg)\\n\\n*Figure 3-1. Operating systems to target depending on version\", \"s of the .NET framework*\\n\\nWhen deploying legacy .NET Framework applications you have to target Windo\", \"ws Server Core, compatible with legacy apps and IIS, but it has a larger image. When deploying .NET \", \"7 applications, you can target Windows Nano Server, which is cloud optimized, uses Kestrel and is sm\", \"aller and starts faster. You can also target Linux, supporting Debian, Alpine, and others.\\n\\nYou can \", \"also create your own Docker image in cases where you want to use a different Linux distro or where y\", \"ou want an image with versions not provided by Microsoft. For example, you might create an image wit\", \"h ASP.NET Core running on the traditional .NET Framework and Windows Server Core, which is a not-so-\", \"common scenario for Docker.\\n\\nWhen you add the image name to your Dockerfile file, you can select the\", \" operating system and version depending on the tag you use, as in the following examples:\\n\\n| Image  \", \"                                                 | Comments                                         \", \"                                                                                                    \", \"                             |\\n|---------------------------------------------------------|----------\", \"----------------------------------------------------------------------------------------------------\", \"---------------------------------------------------------------------|\\n| mcr.microsoft.com/dotnet/ru\", \"ntime:7.0                    | .NET 7 multi-architecture: Supports Linux and Windows<br>Nano Server \", \"depending on the Docker host.                                                                       \", \"         |\\n| mcr.microsoft.com/dotnet/aspnet:7.0                     | ASP.NET Core 7.0 multi-archit\", \"ecture: Supports Linux and<br>Windows Nano Server depending on the Docker host.<br>The aspnetcore im\", \"age has a few optimizations for<br>ASP.NET Core. |\\n| mcr.microsoft.com/dotnet/aspnet:7.0-<br>bullsey\", \"e-slim   | .NET 7 runtime-only on Linux Debian distro                                               \", \"                                                                                         |\\n| mcr.mic\", \"rosoft.com/dotnet/aspnet:7.0-<br>nanoserver-1809 | .NET 7 runtime-only on Windows Nano Server (Windo\", \"ws<br>Server version 1809)                                                                          \", \"                             |\\n\\n## <span id=\\\"page-30-0\\\"></span>Official .NET Docker images\\n\\nThe Offi\", \"cial .NET Docker images are Docker images created and optimized by Microsoft. They're publicly avail\", \"able on [Microsoft Artifact Registry.](https://mcr.microsoft.com/) You can search over the catalog t\", \"o find all .NET image repositories, for example [.NET SDK](https://mcr.microsoft.com/product/dotnet/\", \"sdk/about) repository.\\n\\nEach repository can contain multiple images, depending on .NET versions, and\", \" depending on the OS and versions (Linux Debian, Linux Alpine, Windows Nano Server, Windows Server C\", \"ore, and so on). Image repositories provide extensive tagging to help you select not just a specific\", \" framework version, but also to choose an OS (Linux distribution or Windows version).\\n\\n## <span id=\\\"\", \"page-30-1\\\"></span>**.NET and Docker image optimizations for development versus production**\\n\\nWhen bu\", \"ilding Docker images for developers, Microsoft focused on the following main scenarios:\\n\\n- Images us\", \"ed to *develop* and build .NET apps.\\n- Images used to *run* .NET apps.\\n\\nWhy multiple images? When de\", \"veloping, building, and running containerized applications, you usually have different priorities. B\", \"y providing different images for these separate tasks, Microsoft helps optimize the separate process\", \"es of developing, building, and deploying apps.\\n\\n### **During development and build**\\n\\nDuring develo\", \"pment, what is important is how fast you can iterate changes, and the ability to debug the changes. \", \"The size of the image isn't as important as the ability to make changes to your code and see the cha\", \"nges quickly. Some tools and \\\"build-agent containers\\\", use the development .NET image (*mcr.microsof\", \"t.com/dotnet/sdk:7.0*) during development and build process. When building inside a Docker container\", \", the important aspects are the elements that are needed to compile your app. This includes the comp\", \"iler and any other .NET dependencies.\\n\\nWhy is this type of build image important? You don't deploy t\", \"his image to production. Instead, it's an image that you use to build the content you place into a p\", \"roduction image. This image would be used in your continuous integration (CI) environment or build e\", \"nvironment when using Docker multi-stage builds.\\n\\n### **In production**\\n\\nWhat is important in produc\", \"tion is how fast you can deploy and start your containers based on a production .NET image. Therefor\", \"e, the runtime-only image based on *mcr.microsoft.com/dotnet/aspnet:7.0* is small so that it can tra\", \"vel quickly across the network from your Docker registry to your Docker hosts. The contents are read\", \"y to run, enabling the fastest time from starting the container to processing results. In the Docker\", \" model, there is no need for compilation from C# code, as there's when you run dotnet build or dotne\", \"t publish when using the build container.\\n\\nIn this optimized image, you put only the binaries and ot\", \"her content needed to run the application. For example, the content created by dotnet publish contai\", \"ns only the compiled .NET binaries, images, .js, and .css files. Over time, you'll see images that c\", \"ontain pre-jitted (the compilation from IL to native that occurs at run time) packages.\\n\\nAlthough th\", \"ere are multiple versions of the .NET and ASP.NET Core images, they all share one or more layers, in\", \"cluding the base layer. Therefore, the amount of disk space needed to store an image is small; it co\", \"nsists only of the delta between your custom image and its base image. The result is that it's quick\", \" to pull the image from your registry.\\n\\nWhen you explore the .NET image repositories at Microsoft Ar\", \"tifact Registry, you'll find multiple image versions classified or marked with tags. These tags help\", \" to decide which one to use, depending on the version you need, like those in the following table:\\n\\n\", \"| Image                               | Comments                                                    \", \"                                         |\\n|-------------------------------------|------------------\", \"------------------------------------------------------------------------------------|\\n| mcr.microsof\", \"t.com/dotnet/aspnet:7.0 | ASP.NET Core, with runtime only and ASP.NET Core<br>optimizations, on Linu\", \"x and Windows (multi-arch) |\\n| mcr.microsoft.com/dotnet/sdk:7.0    | .NET 7, with SDKs included, on \", \"Linux and Windows<br>(multi-arch)                                     |\\n\\nYou can find all the availa\", \"ble docker images in [dotnet-docker](https://github.com/dotnet/dotnet-docker) and also refer to the \", \"latest preview releases by using nightly build mcr.microsoft.com/dotnet/nightly/\\\\*\\n\\n**CHAPTER** 3\\n\\n#\", \" <span id=\\\"page-32-0\\\"></span>Architecting container and microservice-based applications\\n\\n*Microservi\", \"ces offer great benefits but also raise huge new challenges. Microservice architecture patterns are \", \"fundamental pillars when creating a microservice-based application.*\\n\\nEarlier in this guide, you lea\", \"rned basic concepts about containers and Docker. That information was the minimum you needed to get \", \"started with containers. Even though containers are enablers of, and a great fit for microservices, \", \"they aren't mandatory for a microservice architecture. Many architectural concepts in this architect\", \"ure section could be applied without containers. However, this guide focuses on the intersection of \", \"both due to the already introduced importance of containers.\\n\\nEnterprise applications can be complex\", \" and are often composed of multiple services instead of a single service-based application. For thos\", \"e cases, you need to understand other architectural approaches, such as the microservices and certai\", \"n Domain-Driven Design (DDD) patterns plus container orchestration concepts. Note that this chapter \", \"describes not just microservices on containers, but any containerized application, as well.\\n\\n## <spa\", \"n id=\\\"page-32-1\\\"></span>Container design principles\\n\\nIn the container model, a container image insta\", \"nce represents a single process. By defining a container image as a process boundary, you can create\", \" primitives that can be used to scale or batch the process.\\n\\nWhen you design a container image, you'\", \"ll see an [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint) definition in t\", \"he Dockerfile. This definition defines the process whose lifetime controls the lifetime of the conta\", \"iner. When the process completes, the container lifecycle ends. Containers might represent long-runn\", \"ing processes like web servers, but can also represent short-lived processes like batch jobs, which \", \"formerly might have been implemented as Azure [WebJobs.](https://github.com/Azure/azure-webjobs-sdk/\", \"wiki)\\n\\nIf the process fails, the container ends, and the orchestrator takes over. If the orchestrato\", \"r was configured to keep five instances running and one fails, the orchestrator will create another \", \"container instance to replace the failed process. In a batch job, the process is started with parame\", \"ters. When the process completes, the work is complete. This guidance drills-down on orchestrators, \", \"later on.\\n\\nYou might find a scenario where you want multiple processes running in a single container\", \". For that scenario, since there can be only one entry point per container, you could run a script w\", \"ithin the container that launches as many programs as needed. For example, you can use [Supervisor](\", \"http://supervisord.org/) or a similar tool to take care of launching multiple processes inside a sin\", \"gle container. However, even though you can find architectures that hold multiple processes per cont\", \"ainer, that approach isn't very common.\\n\\n## <span id=\\\"page-33-0\\\"></span>Containerizing monolithic ap\", \"plications\\n\\nYou might want to build a single, monolithically deployed web application or service and\", \" deploy it as a container. The application itself might not be internally monolithic, but structured\", \" as several libraries, components, or even layers (application layer, domain layer, data-access laye\", \"r, etc.). Externally, however, it's a single container\\u2014a single process, a single web application, o\", \"r a single service.\\n\\nTo manage this model, you deploy a single container to represent the applicatio\", \"n. To increase capacity, you scale out, that is, just add more copies with a load balancer in front.\", \" The simplicity comes from managing a single deployment in a single container or VM.\\n\\n![](_page_33_F\", \"igure_5.jpeg)\\n\\n*Figure 4-1. Example of the architecture of a containerized monolithic application*\\n\\n\", \"You can include multiple components, libraries, or internal layers in each container, as illustrated\", \" in Figure 4-1. A monolithic containerized application has most of its functionality within a single\", \" container, with internal layers or libraries, and scales out by cloning the container on multiple s\", \"ervers/VMs. However, this monolithic pattern might conflict with the container principle \\\"a containe\", \"r does one thing, and does it in one process\\\", but might be ok for some cases.\\n\\nThe downside of this\", \" approach becomes evident if the application grows, requiring it to scale. If the entire application\", \" can scale, it isn't really a problem. However, in most cases, just a few parts of the application a\", \"re the choke points that require scaling, while other components are used less.\\n\\nFor example, in a t\", \"ypical e-commerce application, you likely need to scale the product information subsystem, because m\", \"any more customers browse products than purchase them. More customers use their basket than use the \", \"payment pipeline. Fewer customers add comments or view their purchase history. And you might have on\", \"ly a handful of employees that need to manage the content and marketing campaigns. If you scale the \", \"monolithic design, all the code for these different tasks is deployed multiple times and scaled at t\", \"he same grade.\\n\\nThere are multiple ways to scale an application-horizontal duplication, splitting di\", \"fferent areas of the application, and partitioning similar business concepts or data. But, in additi\", \"on to the problem of scaling all components, changes to a single component require complete retestin\", \"g of the entire application, and a complete redeployment of all the instances.\\n\\nHowever, the monolit\", \"hic approach is common, because the development of the application is initially easier than for micr\", \"oservices approaches. Thus, many organizations develop using this architectural approach. While some\", \" organizations have had good enough results, others are hitting limits. Many organizations designed \", \"their applications using this model because tools and infrastructure made it too difficult to build \", \"service-oriented architectures (SOA) years ago, and they did not see the needuntil the application g\", \"rew.\\n\\nFrom an infrastructure perspective, each server can run many applications within the same host\", \" and have an acceptable ratio of efficiency in resources usage, as shown in Figure 4-2.\\n\\n![](_page_3\", \"4_Picture_5.jpeg)\\n\\n*Figure 4-2. Monolithic approach: Host running multiple apps, each app running as\", \" a container*\\n\\nMonolithic applications in Microsoft Azure can be deployed using dedicated VMs for ea\", \"ch instance. Additionally, using [Azure virtual machine scale sets,](https://azure.microsoft.com/doc\", \"umentation/services/virtual-machine-scale-sets/) you can easily scale the VMs. [Azure App Service](h\", \"ttps://azure.microsoft.com/services/app-service/) can also run monolithic applications and easily sc\", \"ale instances without requiring you to manage the VMs. Since 2016, Azure App Services can run single\", \" instances of Docker containers as well, simplifying deployment.\\n\\nAs a QA environment or a limited p\", \"roduction environment, you can deploy multiple Docker host VMs and balance them using the Azure bala\", \"ncer, as shown in Figure 4-3. This lets you manage scaling with a coarse-grain approach, because the\", \" whole application lives within a single container.\\n\\n![](_page_35_Figure_1.jpeg)\\n\\n*Figure 4-3. Examp\", \"le of multiple hosts scaling up a single container application*\\n\\nDeployment to the various hosts can\", \" be managed with traditional deployment techniques. Docker hosts can be managed with commands like d\", \"ocker run or docker-compose performed manually, or through automation such as continuous delivery (C\", \"D) pipelines.\\n\\n### <span id=\\\"page-35-0\\\"></span>**Deploying a monolithic application as a container**\", \"\\n\\nThere are benefits to using containers to manage monolithic application deployments. Scaling conta\", \"iner instances is far faster and easier than deploying additional VMs. Even if you use virtual machi\", \"ne scale sets, VMs take time to start. When deployed as traditional application instances instead of\", \" containers, the configuration of the application is managed as part of the VM, which isn't ideal.\\n\\n\", \"Deploying updates as Docker images is far faster and network efficient. Docker images typically star\", \"t in seconds, which speeds rollouts. Tearing down a Docker image instance is as easy as issuing a do\", \"cker stop command, and typically completes in less than a second.\\n\\nBecause containers are immutable \", \"by design, you never need to worry about corrupted VMs. In contrast, update scripts for a VM might f\", \"orget to account for some specific configuration or file left on disk.\\n\\nWhile monolithic application\", \"s can benefit from Docker, we're touching only on the benefits. Additional benefits of managing cont\", \"ainers come from deploying with container orchestrators, which manage the various instances and life\", \"cycle of each container instance. Breaking up the monolithic application into subsystems that can be\", \" scaled, developed, and deployed individually is your entry point into the realm of microservices.\\n\\n\", \"### <span id=\\\"page-35-1\\\"></span>**Publishing a single-container-based application to Azure App Servi\", \"ce**\\n\\nWhether you want to get validation of a container deployed to Azure or when an application is \", \"simply a single-container application, Azure App Service provides a great way to provide scalable si\", \"nglecontainer-based services. Using Azure App Service is simple. It provides great integration with \", \"Git to make it easy to take your code, build it in Visual Studio, and deploy it directly to Azure.\\n\\n\", \"![](_page_36_Picture_0.jpeg)\\n\\n*Figure 4-4. Publishing a single-container application to Azure App Se\", \"rvice from Visual Studio 2022*\\n\\nWithout Docker, if you needed other capabilities, frameworks, or dep\", \"endencies that aren't supported in Azure App Service, you had to wait until the Azure team updated t\", \"hose dependencies in App Service. Or you had to switch to other services like Azure Cloud Services o\", \"r VMs, where you had further control and you could install a required component or framework for you\", \"r application.\\n\\nContainer support in Visual Studio 2017 and later gives you the ability to include w\", \"hatever you want in your application environment, as shown in Figure 4-4. Since you're running it in\", \" a container, if you add a dependency to your application, you can include the dependency in your Do\", \"ckerfile or Docker image.\\n\\nAs also shown in Figure 4-4, the publish flow pushes an image through a c\", \"ontainer registry. This can be the Azure Container Registry (a registry close to your deployments in\", \" Azure and secured by Azure Active Directory groups and accounts), or any other Docker registry, lik\", \"e Docker Hub or an onpremises registry.\\n\\n## <span id=\\\"page-36-0\\\"></span>Manage state and data in Doc\", \"ker applications\\n\\nIn most cases, you can think of a container as an instance of a process. A process\", \" doesn't maintain persistent state. While a container can write to its local storage, assuming that \", \"an instance will be around indefinitely would be like assuming that a single location in memory will\", \" be durable. You\\n\\nshould assume that container images, like processes, have multiple instances or wi\", \"ll eventually be killed. If they're managed with a container orchestrator, you should assume that th\", \"ey might get moved from one node or VM to another.\\n\\nThe following solutions are used to manage data \", \"in Docker applications:\\n\\nFrom the Docker host, as [Docker Volumes:](https://docs.docker.com/engine/a\", \"dmin/volumes/)\\n\\n- **Volumes** are stored in an area of the host filesystem that's managed by Docker.\", \"\\n- **Bind mounts** can map to any folder in the host filesystem, so access can't be controlled from \", \"Docker process and can pose a security risk as a container could access sensitive OS folders.\\n- **tm\", \"pfs mounts** are like virtual folders that only exist in the host's memory and are never written to \", \"the filesystem.\\n\\n### From remote storage:\\n\\n- [Azure Storage,](https://azure.microsoft.com/documentat\", \"ion/services/storage/) which provides geo-distributable storage, providing a good long-term persiste\", \"nce solution for containers.\\n- Remote relational databases like [Azure SQL Database](https://azure.m\", \"icrosoft.com/services/sql-database/) or NoSQL databases like [Azure Cosmos](https://docs.microsoft.c\", \"om/azure/cosmos-db/introduction)  [DB,](https://docs.microsoft.com/azure/cosmos-db/introduction) or \", \"cache services like [Redis.](https://redis.io/)\\n\\n### From the Docker container:\\n\\n\\u2022 **Overlay File Sy\", \"stem**. This Docker feature implements a copy-on-write task that stores updated information to the r\", \"oot file system of the container. That information is \\\"on top\\\" of the original image on which the co\", \"ntainer is based. If the container is deleted from the system, those changes are lost. Therefore, wh\", \"ile it's possible to save the state of a container within its local storage, designing a system arou\", \"nd this would conflict with the premise of container design, which by default is stateless.\\n\\nHowever\", \", using Docker Volumes is now the preferred way to handle local data in Docker. If you need more inf\", \"ormation about storage in containers check on [Docker storage drivers](https://docs.docker.com/stora\", \"ge/storagedriver/select-storage-driver/) and [About storage](https://docs.docker.com/storage/storage\", \"driver/)  [drivers.](https://docs.docker.com/storage/storagedriver/)\\n\\nThe following provides more de\", \"tail about these options:\\n\\n**Volumes** are directories mapped from the host OS to directories in con\", \"tainers. When code in the container has access to the directory, that access is actually to a direct\", \"ory on the host OS. This directory is not tied to the lifetime of the container itself, and the dire\", \"ctory is managed by Docker and isolated from the core functionality of the host machine. Thus, data \", \"volumes are designed to persist data independently of the life of the container. If you delete a con\", \"tainer or an image from the Docker host, the data persisted in the data volume isn't deleted.\\n\\nVolum\", \"es can be named or anonymous (the default). Named volumes are the evolution of **Data Volume Contain\", \"ers** and make it easy to share data between containers. Volumes also support volume drivers that al\", \"low you to store data on remote hosts, among other options.\\n\\n**Bind mounts** are available since a l\", \"ong time ago and allow the mapping of any folder to a mount point in a container. Bind mounts have m\", \"ore limitations than volumes and some important security issues, so volumes are the recommended opti\", \"on.\\n\\n**tmpfs mounts** are basically virtual folders that live only in the host's memory and are neve\", \"r written to the filesystem. They are fast and secure but use memory and are only meant for temporar\", \"y, nonpersistent data.\\n\\nAs shown in Figure 4-5, regular Docker volumes can be stored outside of the \", \"containers themselves but within the physical boundaries of the host server or VM. However, Docker c\", \"ontainers can't access a volume from one host server or VM to another. In other words, with these vo\", \"lumes, it isn't possible to manage data shared between containers that run on different Docker hosts\", \", although it could be achieved with a volume driver that supports remote hosts.\\n\\n*Figure 4-5. Volum\", \"es and external data sources for container-based applications*\\n\\nVolumes can be shared between contai\", \"ners, but only in the same host, unless you use a remote driver that supports remote hosts. In addit\", \"ion, when Docker containers are managed by an orchestrator, containers might \\\"move\\\" between hosts, d\", \"epending on the optimizations performed by the cluster. Therefore, it isn't recommended that you use\", \" data volumes for business data. But they're a good mechanism to work with trace files, temporal fil\", \"es, or similar that will not impact business data consistency.\\n\\n**Remote data sources and cache** to\", \"ols like Azure SQL Database, Azure Cosmos DB, or a remote cache like Redis can be used in containeri\", \"zed applications the same way they are used when developing without containers. This is a proven way\", \" to store business application data.\\n\\n**Azure Storage.** Business data usually will need to be place\", \"d in external resources or databases, like Azure Storage. Azure Storage, in concrete, provides the f\", \"ollowing services in the cloud:\\n\\n\\u2022 Blob storage stores unstructured object data. A blob can be any t\", \"ype of text or binary data, such as document or media files (images, audio, and video files). Blob s\", \"torage is also referred to as Object storage.\\n\\n- File storage offers shared storage for legacy appli\", \"cations using standard SMB protocol. Azure virtual machines and cloud services can share file data a\", \"cross application components via mounted shares. On-premises applications can access file data in a \", \"share via the File service REST API.\\n- Table storage stores structured datasets. Table storage is a \", \"NoSQL key-attribute data store, which allows rapid development and fast access to large quantities o\", \"f data.\\n\\n**Relational databases and NoSQL databases.** There are many choices for external databases\", \", from relational databases like SQL Server, PostgreSQL, Oracle, or NoSQL databases like Azure Cosmo\", \"s DB, MongoDB, etc. These databases are not going to be explained as part of this guide since they a\", \"re in a completely different subject.\\n\\n## <span id=\\\"page-39-0\\\"></span>Service-oriented architecture\\n\", \"\\nService-oriented architecture (SOA) was an overused term and has meant different things to differen\", \"t people. But as a common denominator, SOA means that you structure your application by decomposing \", \"it into multiple services (most commonly as HTTP services) that can be classified as different types\", \" like subsystems or tiers.\\n\\nThose services can now be deployed as Docker containers, which solves de\", \"ployment issues, because all the dependencies are included in the container image. However, when you\", \" need to scale up SOA applications, you might have scalability and availability challenges if you're\", \" deploying based on single Docker hosts. This is where Docker clustering software or an orchestrator\", \" can help you, as explained in later sections where deployment approaches for microservices are desc\", \"ribed.\\n\\nDocker containers are useful (but not required) for both traditional service-oriented archit\", \"ectures and the more advanced microservices architectures.\\n\\nMicroservices derive from SOA, but SOA i\", \"s different from microservices architecture. Features like large central brokers, central orchestrat\", \"ors at the organization level, and the [Enterprise Service Bus](https://en.wikipedia.org/wiki/Enterp\", \"rise_service_bus)  [\\\\(ESB\\\\)](https://en.wikipedia.org/wiki/Enterprise_service_bus) are typical in SO\", \"A. But in most cases, these are anti-patterns in the microservice community. In fact, some people ar\", \"gue that \\\"The microservice architecture is SOA done right.\\\"\\n\\nThis guide focuses on microservices, be\", \"cause a SOA approach is less prescriptive than the requirements and techniques used in a microservic\", \"e architecture. If you know how to build a microservice-based application, you also know how to buil\", \"d a simpler service-oriented application.\\n\\n## <span id=\\\"page-39-1\\\"></span>Microservices architecture\", \"\\n\\nAs the name implies, a microservices architecture is an approach to building a server application \", \"as a set of small services. That means a microservices architecture is mainly oriented to the back-e\", \"nd, although the approach is also being used for the front end. Each service runs in its own process\", \" and communicates with other processes using protocols such as HTTP/HTTPS, WebSockets, or [AMQP.](ht\", \"tps://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol) Each microservice implements a specif\", \"ic end-to-end domain or business capability within a certain context boundary, and each must be deve\", \"loped autonomously and be deployable independently. Finally, each microservice should own its relate\", \"d domain data model and domain logic (sovereignty\\n\\nand decentralized data management) and could be b\", \"ased on different data storage technologies (SQL, NoSQL) and different programming languages.\\n\\nWhat \", \"size should a microservice be? When developing a microservice, size shouldn't be the important point\", \". Instead, the important point should be to create loosely coupled services so you have autonomy of \", \"development, deployment, and scale, for each service. Of course, when identifying and designing micr\", \"oservices, you should try to make them as small as possible as long as you don't have too many direc\", \"t dependencies with other microservices. More important than the size of the microservice is the int\", \"ernal cohesion it must have and its independence from other services.\\n\\nWhy a microservices architect\", \"ure? In short, it provides long-term agility. Microservices enable better maintainability in complex\", \", large, and highly-scalable systems by letting you create applications based on many independently \", \"deployable services that each have granular and autonomous lifecycles.\\n\\nAs an additional benefit, mi\", \"croservices can scale out independently. Instead of having a single monolithic application that you \", \"must scale out as a unit, you can instead scale out specific microservices. That way, you can scale \", \"just the functional area that needs more processing power or network bandwidth to support demand, ra\", \"ther than scaling out other areas of the application that don't need to be scaled. That means cost s\", \"avings because you need less hardware.\\n\\n*Figure 4-6. Monolithic deployment versus the microservices \", \"approach*\\n\\nAs Figure 4-6 shows, in the traditional monolithic approach, the application scales by cl\", \"oning the whole app in several servers/VM. In the microservices approach, functionality is segregate\", \"d in smaller services, so each service can scale independently. The microservices approach allows ag\", \"ile changes and rapid iteration of each microservice, because you can change specific, small areas o\", \"f complex, large, and scalable applications.\\n\\nArchitecting fine-grained microservices-based applicat\", \"ions enables continuous integration and continuous delivery practices. It also accelerates delivery \", \"of new functions into the application. Finegrained composition of applications also allows you to ru\", \"n and test microservices in isolation, and to evolve them autonomously while maintaining clear contr\", \"acts between them. As long as you don't change the interfaces or contracts, you can change the inter\", \"nal implementation of any microservice or add new functionality without breaking other microservices\", \".\\n\\nThe following are important aspects to enable success in going into production with a microservic\", \"esbased system:\\n\\n- Monitoring and health checks of the services and infrastructure.\\n- Scalable infra\", \"structure for the services (that is, cloud and orchestrators).\\n- Security design and implementation \", \"at multiple levels: authentication, authorization, secrets management, secure communication, etc.\\n- \", \"Rapid application delivery, usually with different teams focusing on different microservices.\\n- DevO\", \"ps and CI/CD practices and infrastructure.\\n\\nOf these, only the first three are covered or introduced\", \" in this guide. The last two points, which are related to application lifecycle, are covered in the \", \"additional [Containerized Docker Application](https://aka.ms/dockerlifecycleebook)  [Lifecycle with \", \"Microsoft Platform and Tools](https://aka.ms/dockerlifecycleebook) e-book.\\n\\n### <span id=\\\"page-41-0\\\"\", \"></span>**Additional resources**\\n\\n- **Mark Russinovich. Microservices: An application revolution pow\", \"ered by the cloud** [https://azure.microsoft.com/blog/microservices-an-application-revolution-powere\", \"d-by-the](https://azure.microsoft.com/blog/microservices-an-application-revolution-powered-by-the-cl\", \"oud/)[cloud/](https://azure.microsoft.com/blog/microservices-an-application-revolution-powered-by-th\", \"e-cloud/)\\n- **Martin Fowler. Microservices** <https://www.martinfowler.com/articles/microservices.ht\", \"ml>\\n- **Martin Fowler. Microservice Prerequisites** <https://martinfowler.com/bliki/MicroservicePrer\", \"equisites.html>\\n- **Jimmy Nilsson. Chunk Cloud Computing** <https://www.infoq.com/articles/CCC-Jimmy\", \"-Nilsson>\\n- **Cesar de la Torre. Containerized Docker Application Lifecycle with Microsoft Platform \", \"and Tools** (downloadable e-book) <https://aka.ms/dockerlifecycleebook>\\n\\n## <span id=\\\"page-41-1\\\"></s\", \"pan>Data sovereignty per microservice\\n\\nAn important rule for microservices architecture is that each\", \" microservice must own its domain data and logic. Just as a full application owns its logic and data\", \", so must each microservice own its logic and data under an autonomous lifecycle, with independent d\", \"eployment per microservice.\\n\\nThis means that the conceptual model of the domain will differ between \", \"subsystems or microservices. Consider enterprise applications, where customer relationship managemen\", \"t (CRM) applications,\\n\\ntransactional purchase subsystems, and customer support subsystems each call \", \"on unique customer entity attributes and data, and where each employs a different Bounded Context (B\", \"C).\\n\\nThis principle is similar in [Domain-driven design \\\\(DDD\\\\),](https://en.wikipedia.org/wiki/Doma\", \"in-driven_design) where each [Bounded Context](https://martinfowler.com/bliki/BoundedContext.html) o\", \"r autonomous subsystem or service must own its domain model (data plus logic and behavior). Each DDD\", \" Bounded Context correlates to one business microservice (one or several services). This point about\", \" the Bounded Context pattern is expanded in the next section.\\n\\nOn the other hand, the traditional (m\", \"onolithic data) approach used in many applications is to have a single centralized database or just \", \"a few databases. This is often a normalized SQL database that's used for the whole application and a\", \"ll its internal subsystems, as shown in Figure 4-7.\\n\\n![](_page_42_Figure_4.jpeg)\\n\\n![](_page_42_Figur\", \"e_9.jpeg)\\n\\n*Figure 4-7. Data sovereignty comparison: monolithic database versus microservices*\\n\\nIn t\", \"he traditional approach, there's a single database shared across all services, typically in a tiered\", \" architecture. In the microservices approach, each microservice owns its model/data. The centralized\", \" database approach initially looks simpler and seems to enable reuse of entities in different subsys\", \"tems to make everything consistent. But the reality is you end up with huge tables that serve many d\", \"ifferent subsystems, and that include attributes and columns that aren't needed in most cases. It's \", \"like trying to use the same physical map for hiking a short trail, taking a day-long car trip, and l\", \"earning geography.\\n\\nA monolithic application with typically a single relational database has two imp\", \"ortant benefits: [ACID](https://en.wikipedia.org/wiki/ACID)  [transactions](https://en.wikipedia.org\", \"/wiki/ACID) and the SQL language, both working across all the tables and data related to your applic\", \"ation. This approach provides a way to easily write a query that combines data from multiple tables.\", \"\\n\\nHowever, data access becomes much more complicated when you move to a microservices architecture. \", \"Even when using ACID transactions within a microservice or Bounded Context, it is crucial to conside\", \"r that the data owned by each microservice is private to that microservice and should only\\n\\nbe acces\", \"sed either synchronously through its API endpoints(REST, gRPC, SOAP, etc) or asynchronously via mess\", \"aging(AMQP or similar).\\n\\nEncapsulating the data ensures that the microservices are loosely coupled a\", \"nd can evolve independently of one another. If multiple services were accessing the same data, schem\", \"a updates would require coordinated updates to all the services. This would break the microservice l\", \"ifecycle autonomy. But distributed data structures mean that you can't make a single ACID transactio\", \"n across microservices. This in turn means you must use eventual consistency when a business process\", \" spans multiple microservices. This is much harder to implement than simple SQL joins, because you c\", \"an't create integrity constraints or use distributed transactions between separate databases, as we'\", \"ll explain later on. Similarly, many other relational database features aren't available across mult\", \"iple microservices.\\n\\nGoing even further, different microservices often use different *kinds* of data\", \"bases. Modern applications store and process diverse kinds of data, and a relational database isn't \", \"always the best choice. For some use cases, a NoSQL database such as Azure CosmosDB or MongoDB might\", \" have a more convenient data model and offer better performance and scalability than a SQL database \", \"like SQL Server or Azure SQL Database. In other cases, a relational database is still the best appro\", \"ach. Therefore, microservices-based applications often use a mixture of SQL and NoSQL databases, whi\", \"ch is sometimes called the [polyglot persistence](https://martinfowler.com/bliki/PolyglotPersistence\", \".html) approach.\\n\\nA partitioned, polyglot-persistent architecture for data storage has many benefits\", \". These include loosely coupled services and better performance, scalability, costs, and manageabili\", \"ty. However, it can introduce some distributed data management challenges, as explained in \\\"Identify\", \"ing domain-model boundaries\\\" later in this chapter.\\n\\n## <span id=\\\"page-43-0\\\"></span>**The relationsh\", \"ip between microservices and the Bounded Context pattern**\\n\\nThe concept of microservice derives from\", \" the [Bounded Context \\\\(BC\\\\) pattern](https://martinfowler.com/bliki/BoundedContext.html) in [domain\", \"-driven design](https://en.wikipedia.org/wiki/Domain-driven_design)  [\\\\(DDD\\\\).](https://en.wikipedia\", \".org/wiki/Domain-driven_design) DDD deals with large models by dividing them into multiple BCs and b\", \"eing explicit about their boundaries. Each BC must have its own model and database; likewise, each m\", \"icroservice owns its related data. In addition, each BC usually has its own [ubiquitous language](ht\", \"tps://martinfowler.com/bliki/UbiquitousLanguage.html) to help communication between software develop\", \"ers and domain experts.\\n\\nThose terms (mainly domain entities) in the ubiquitous language can have di\", \"fferent names in different Bounded Contexts, even when different domain entities share the same iden\", \"tity (that is, the unique ID that's used to read the entity from storage). For instance, in a user-p\", \"rofile Bounded Context, the User domain entity might share identity with the Buyer domain entity in \", \"the ordering Bounded Context.\\n\\nA microservice is therefore like a Bounded Context, but it also speci\", \"fies that it's a distributed service. It's built as a separate process for each Bounded Context, and\", \" it must use the distributed protocols noted earlier, like HTTP/HTTPS, WebSockets, or [AMQP.](https:\", \"//en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol) The Bounded Context pattern, however, doe\", \"sn't specify whether the Bounded Context is a distributed service or if it's simply a logical bounda\", \"ry (such as a generic subsystem) within a monolithic-deployment application.\\n\\nIt's important to high\", \"light that defining a service for each Bounded Context is a good place to start. But you don't have \", \"to constrain your design to it. Sometimes you must design a Bounded Context or business microservice\", \" composed of several physical services. But ultimately, both patterns -Bounded Context and microserv\", \"ice- are closely related.\\n\\nDDD benefits from microservices by getting real boundaries in the form of\", \" distributed microservices. But ideas like not sharing the model between microservices are what you \", \"also want in a Bounded Context.\\n\\n## **Additional resources**\\n\\n- **Chris Richardson. Pattern: Databas\", \"e per service** <https://microservices.io/patterns/data/database-per-service.html>\\n- **Martin Fowler\", \". BoundedContext** <https://martinfowler.com/bliki/BoundedContext.html>\\n- **Martin Fowler. PolyglotP\", \"ersistence** <https://martinfowler.com/bliki/PolyglotPersistence.html>\\n- **Alberto Brandolini. Strat\", \"egic Domain Driven Design with Context Mapping** <https://www.infoq.com/articles/ddd-contextmapping>\", \"\\n\\n## <span id=\\\"page-44-0\\\"></span>Logical architecture versus physical architecture\\n\\nIt's useful at t\", \"his point to stop and discuss the distinction between logical architecture and physical architecture\", \", and how this applies to the design of microservice-based applications.\\n\\nTo begin, building microse\", \"rvices doesn't require the use of any specific technology. For instance, Docker containers aren't ma\", \"ndatory to create a microservice-based architecture. Those microservices could also be run as plain \", \"processes. Microservices is a logical architecture.\\n\\nMoreover, even when a microservice could be phy\", \"sically implemented as a single service, process, or container (for simplicity's sake, that's the ap\", \"proach taken in the initial version of [eShopOnContainers\\\\)](https://aka.ms/MicroservicesArchitectur\", \"e), this parity between business microservice and physical service or container isn't necessarily re\", \"quired in all cases when you build a large and complex application composed of many dozens or even h\", \"undreds of services.\\n\\nThis is where there's a difference between an application's logical architectu\", \"re and physical architecture. The logical architecture and logical boundaries of a system do not nec\", \"essarily map oneto-one to the physical or deployment architecture. It can happen, but it often doesn\", \"'t.\\n\\nAlthough you might have identified certain business microservices or Bounded Contexts, it doesn\", \"'t mean that the best way to implement them is always by creating a single service (such as an ASP.N\", \"ET Web API) or single Docker container for each business microservice. Having a rule saying each bus\", \"iness microservice has to be implemented using a single service or container is too rigid.\\n\\nTherefor\", \"e, a business microservice or Bounded Context is a logical architecture that might coincide (or not)\", \" with physical architecture. The important point is that a business microservice or Bounded Context \", \"must be autonomous by allowing code and state to be independently versioned, deployed, and scaled.\\n\\n\", \"As Figure 4-8 shows, the catalog business microservice could be composed of several services or proc\", \"esses. These could be multiple ASP.NET Web API services or any other kind of services using HTTP or \", \"any other protocol. More importantly, the services could share the same data, as long as these servi\", \"ces are cohesive with respect to the same business domain.\\n\\n![](_page_45_Picture_1.jpeg)\\n\\n*Figure 4-\", \"8. Business microservice with several physical services*\\n\\nThe services in the example share the same\", \" data model because the Web API service targets the same data as the Search service. So, in the phys\", \"ical implementation of the business microservice, you're splitting that functionality so you can sca\", \"le each of those internal services up or down as needed. Maybe the Web API service usually needs mor\", \"e instances than the Search service, or vice versa.\\n\\nIn short, the logical architecture of microserv\", \"ices doesn't always have to coincide with the physical deployment architecture. In this guide, whene\", \"ver we mention a microservice, we mean a business or logical microservice that could map to one or m\", \"ore (physical) services. In most cases, this will be a single service, but it might be more.\\n\\n## <sp\", \"an id=\\\"page-45-0\\\"></span>Challenges and solutions for distributed data management\\n\\n## <span id=\\\"page\", \"-45-1\\\"></span>**Challenge #1: How to define the boundaries of each microservice**\\n\\nDefining microser\", \"vice boundaries is probably the first challenge anyone encounters. Each microservice has to be a pie\", \"ce of your application and each microservice should be autonomous with all the benefits and challeng\", \"es that it conveys. But how do you identify those boundaries?\\n\\nFirst, you need to focus on the appli\", \"cation's logical domain models and related data. Try to identify decoupled islands of data and diffe\", \"rent contexts within the same application. Each context could have a different business language (di\", \"fferent business terms). The contexts should be defined and managed independently. The terms and ent\", \"ities that are used in those different contexts might sound similar, but you might discover that in \", \"a particular context, a business concept with one is used for a different\\n\\npurpose in another contex\", \"t, and might even have a different name. For instance, a user can be referred as a user in the ident\", \"ity or membership context, as a customer in a CRM context, as a buyer in an ordering context, and so\", \" forth.\\n\\nThe way you identify boundaries between multiple application contexts with a different doma\", \"in for each context is exactly how you can identify the boundaries for each business microservice an\", \"d its related domain model and data. You always attempt to minimize the coupling between those micro\", \"services. This guide goes into more detail about this identification and domain model design in the \", \"section Identifying domain-model boundaries for each microservice later.\\n\\n## <span id=\\\"page-46-0\\\"></\", \"span>**Challenge #2: How to create queries that retrieve data from several microservices**\\n\\nA second\", \" challenge is how to implement queries that retrieve data from several microservices, while avoiding\", \" chatty communication to the microservices from remote client apps. An example could be a single scr\", \"een from a mobile app that needs to show user information that's owned by the basket, catalog, and u\", \"ser identity microservices. Another example would be a complex report involving many tables located \", \"in multiple microservices. The right solution depends on the complexity of the queries. But in any c\", \"ase, you'll need a way to aggregate information if you want to improve the efficiency in the communi\", \"cations of your system. The most popular solutions are the following.\\n\\n**API Gateway.** For simple d\", \"ata aggregation from multiple microservices that own different databases, the recommended approach i\", \"s an aggregation microservice referred to as an API Gateway. However, you need to be careful about i\", \"mplementing this pattern, because it can be a choke point in your system, and it can violate the pri\", \"nciple of microservice autonomy. To mitigate this possibility, you can have multiple fined-grained A\", \"PI Gateways each one focusing on a vertical \\\"slice\\\" or business area of the system. The API Gateway \", \"pattern is explained in more detail in the API Gateway section later.\\n\\n**GraphQL Federation** One op\", \"tion to consider if your microservices are already using GraphQL is [GraphQL Federation](https://www\", \".apollographql.com/docs/federation/). Federation allows you to define \\\"subgraphs\\\" from other service\", \"s and compose them into an aggregate \\\"supergraph\\\" that acts as a standalone schema.\\n\\n**CQRS with que\", \"ry/reads tables.** Another solution for aggregating data from multiple microservices is the [Materia\", \"lized View pattern.](https://docs.microsoft.com/azure/architecture/patterns/materialized-view) In th\", \"is approach, you generate, in advance (prepare denormalized data before the actual queries happen), \", \"a read-only table with the data that's owned by multiple microservices. The table has a format suite\", \"d to the client app's needs.\\n\\nConsider something like the screen for a mobile app. If you have a sin\", \"gle database, you might pull together the data for that screen using a SQL query that performs a com\", \"plex join involving multiple tables. However, when you have multiple databases, and each database is\", \" owned by a different microservice, you cannot query those databases and create a SQL join. Your com\", \"plex query becomes a challenge. You can address the requirement using a CQRS approach\\u2014you create a d\", \"enormalized table in a different database that's used just for queries. The table can be designed sp\", \"ecifically for the data you need for the complex query, with a one-to-one relationship between field\", \"s needed by your application's screen and the columns in the query table. It could also serve for re\", \"porting purposes.\\n\\nThis approach not only solves the original problem (how to query and join across \", \"microservices), but it also improves performance considerably when compared with a complex join, bec\", \"ause you already\\n\\nhave the data that the application needs in the query table. Of course, using Comm\", \"and and Query Responsibility Segregation (CQRS) with query/reads tables means additional development\", \" work, and you'll need to embrace eventual consistency. Nonetheless, requirements on performance and\", \" high scalability in [collaborative scenarios](http://udidahan.com/2011/10/02/why-you-should-be-usin\", \"g-cqrs-almost-everywhere/) (or competitive scenarios, depending on the point of view) are where you \", \"should apply CQRS with multiple databases.\\n\\n**\\\"Cold data\\\" in central databases.** For complex report\", \"s and queries that might not require real-time data, a common approach is to export your \\\"hot data\\\" \", \"(transactional data from the microservices) as \\\"cold data\\\" into large databases that are used only f\", \"or reporting. That central database system can be a Big Data-based system, like Hadoop; a data wareh\", \"ouse like one based on Azure SQL Data Warehouse; or even a single SQL database that's used just for \", \"reports (if size won't be an issue).\\n\\nKeep in mind that this centralized database would be used only\", \" for queries and reports that do not need real-time data. The original updates and transactions, as \", \"your source of truth, have to be in your microservices data. The way you would synchronize data woul\", \"d be either by using event-driven communication (covered in the next sections) or by using other dat\", \"abase infrastructure import/export tools. If you use event-driven communication, that integration pr\", \"ocess would be similar to the way you propagate data as described earlier for CQRS query tables.\\n\\nHo\", \"wever, if your application design involves constantly aggregating information from multiple microser\", \"vices for complex queries, it might be a symptom of a bad design -a microservice should be as isolat\", \"ed as possible from other microservices. (This excludes reports/analytics that always should use col\", \"d-data central databases.) Having this problem often might be a reason to merge microservices. You n\", \"eed to balance the autonomy of evolution and deployment of each microservice with strong dependencie\", \"s, cohesion, and data aggregation.\\n\\n## <span id=\\\"page-47-0\\\"></span>**Challenge #3: How to achieve co\", \"nsistency across multiple microservices**\\n\\nAs stated previously, the data owned by each microservice\", \" is private to that microservice and can only be accessed using its microservice API. Therefore, a c\", \"hallenge presented is how to implement end-toend business processes while keeping consistency across\", \" multiple microservices.\\n\\nTo analyze this problem, let's look at an example from the [eShopOnContain\", \"ers reference application.](https://aka.ms/eshoponcontainers) The Catalog microservice maintains inf\", \"ormation about all the products, including the product price. The Basket microservice manages tempor\", \"al data about product items that users are adding to their shopping baskets, which includes the pric\", \"e of the items at the time they were added to the basket. When a product's price is updated in the c\", \"atalog, that price should also be updated in the active baskets that hold that same product, plus th\", \"e system should probably warn the user saying that a particular item's price has changed since they \", \"added it to their basket.\\n\\nIn a hypothetical monolithic version of this application, when the price \", \"changes in the products table, the catalog subsystem could simply use an ACID transaction to update \", \"the current price in the Basket table.\\n\\nHowever, in a microservices-based application, the Product a\", \"nd Basket tables are owned by their respective microservices. No microservice should ever include ta\", \"bles/storage owned by another microservice in its own transactions, not even in direct queries, as s\", \"hown in Figure 4-9.\\n\\n![](_page_48_Figure_0.jpeg)\\n\\n*Figure 4-9. A microservice can't directly access \", \"a table in another microservice*\\n\\nThe Catalog microservice shouldn't update the Basket table directl\", \"y, because the Basket table is owned by the Basket microservice. To make an update to the Basket mic\", \"roservice, the Catalog microservice should use eventual consistency probably based on asynchronous c\", \"ommunication such as integration events (message and event-based communication). This is how the [eS\", \"hopOnContainers](https://aka.ms/eshoponcontainers) reference application performs this type of consi\", \"stency across microservices.\\n\\nAs stated by the [CAP theorem,](https://en.wikipedia.org/wiki/CAP_theo\", \"rem) you need to choose between availability and ACID strong consistency. Most microservice-based sc\", \"enarios demand availability and high scalability as opposed to strong consistency. Mission-critical \", \"applications must remain up and running, and developers can work around strong consistency by using \", \"techniques for working with weak or eventual consistency. This is the approach taken by most microse\", \"rvice-based architectures.\\n\\nMoreover, ACID-style or two-phase commit transactions are not just again\", \"st microservices principles; most NoSQL databases (like Azure Cosmos DB, MongoDB, etc.) do not suppo\", \"rt two-phase commit transactions, typical in distributed databases scenarios. However, maintaining d\", \"ata consistency across services and databases is essential. This challenge is also related to the qu\", \"estion of how to propagate changes across multiple microservices when certain data needs to be redun\", \"dant\\u2014for example, when you need to have the product's name or description in the Catalog microservic\", \"e and the Basket microservice.\\n\\nA good solution for this problem is to use eventual consistency betw\", \"een microservices articulated through event-driven communication and a publish-and-subscribe system.\", \" These topics are covered in the section [Asynchronous event-driven communication](#page-70-1) later\", \" in this guide.\\n\\n## <span id=\\\"page-49-0\\\"></span>**Challenge #4: How to design communication across m\", \"icroservice boundaries**\\n\\nCommunicating across microservice boundaries is a real challenge. In this \", \"context, communication doesn't refer to what protocol you should use (HTTP and REST, AMQP, messaging\", \", and so on). Instead, it addresses what communication style you should use, and especially how coup\", \"led your microservices should be. Depending on the level of coupling, when failure occurs, the impac\", \"t of that failure on your system will vary significantly.\\n\\nIn a distributed system like a microservi\", \"ces-based application, with so many artifacts moving around and with distributed services across man\", \"y servers or hosts, components will eventually fail. Partial failure and even larger outages will oc\", \"cur, so you need to design your microservices and the communication across them considering the comm\", \"on risks in this type of distributed system.\\n\\nA popular approach is to implement HTTP (REST)-based m\", \"icroservices, due to their simplicity. An HTTP-based approach is perfectly acceptable; the issue her\", \"e is related to how you use it. If you use HTTP requests and responses just to interact with your mi\", \"croservices from client applications or from API Gateways, that's fine. But if you create long chain\", \"s of synchronous HTTP calls across microservices, communicating across their boundaries as if the mi\", \"croservices were objects in a monolithic application, your application will eventually run into prob\", \"lems.\\n\\nFor instance, imagine that your client application makes an HTTP API call to an individual mi\", \"croservice like the Ordering microservice. If the Ordering microservice in turn calls additional mic\", \"roservices using HTTP within the same request/response cycle, you're creating a chain of HTTP calls.\", \" It might sound reasonable initially. However, there are important points to consider when going dow\", \"n this path:\\n\\n- Blocking and low performance. Due to the synchronous nature of HTTP, the original re\", \"quest doesn't get a response until all the internal HTTP calls are finished. Imagine if the number o\", \"f these calls increases significantly and at the same time one of the intermediate HTTP calls to a m\", \"icroservice is blocked. The result is that performance is impacted, and the overall scalability will\", \" be exponentially affected as additional HTTP requests increase.\\n- Coupling microservices with HTTP.\", \" Business microservices shouldn't be coupled with other business microservices. Ideally, they should\", \"n't \\\"know\\\" about the existence of other microservices. If your application relies on coupling micros\", \"ervices as in the example, achieving autonomy per microservice will be almost impossible.\\n- Failure \", \"in any one microservice. If you implemented a chain of microservices linked by HTTP calls, when any \", \"of the microservices fails (and eventually they will fail) the whole chain of microservices will fai\", \"l. A microservice-based system should be designed to continue to work as well as possible during par\", \"tial failures. Even if you implement client logic that uses retries with exponential backoff or circ\", \"uit breaker mechanisms, the more complex the HTTP call chains are, the more complex it is to impleme\", \"nt a failure strategy based on HTTP.\\n\\nIn fact, if your internal microservices are communicating by c\", \"reating chains of HTTP requests as described, it could be argued that you have a monolithic applicat\", \"ion, but one based on HTTP between processes instead of intra-process communication mechanisms.\\n\\nThe\", \"refore, in order to enforce microservice autonomy and have better resiliency, you should minimize th\", \"e use of chains of request/response communication across microservices. It's recommended that you us\", \"e only asynchronous interaction for inter-microservice communication, either by using asynchronous m\", \"essage- and event-based communication, or by using (asynchronous) HTTP polling independently of the \", \"original HTTP request/response cycle.\\n\\nThe use of asynchronous communication is explained with addit\", \"ional details later in this guide in the sections Asynchronous microservice integration enforces mic\", \"roservice's autonomy and [Asynchronous](#page-68-0)  [message-based communication.](#page-68-0)\\n\\n## \", \"<span id=\\\"page-50-0\\\"></span>**Additional resources**\\n\\n- **CAP theorem** [https://en.wikipedia.org/wi\", \"ki/CAP\\\\\\\\_theorem](https://en.wikipedia.org/wiki/CAP_theorem)\\n- **Eventual consistency** [https://en.\", \"wikipedia.org/wiki/Eventual\\\\\\\\_consistency](https://en.wikipedia.org/wiki/Eventual_consistency)\\n- **D\", \"ata Consistency Primer** [https://learn.microsoft.com/previous-versions/msp-n-p/dn589800\\\\(v=pandp.10\", \"\\\\)](https://docs.microsoft.com/previous-versions/msp-n-p/dn589800(v=pandp.10))\\n- **Martin Fowler. CQ\", \"RS (Command and Query Responsibility Segregation)** <https://martinfowler.com/bliki/CQRS.html>\\n- **M\", \"aterialized View** [https://learn.microsoft.com/azure/architecture/patterns/materialized-view](https\", \"://docs.microsoft.com/azure/architecture/patterns/materialized-view)\\n- **Charles Row. ACID vs. BASE:\", \" The Shifting pH of Database Transaction Processing** [https://www.dataversity.net/acid-vs-base-the-\", \"shifting-ph-of-database-transaction](https://www.dataversity.net/acid-vs-base-the-shifting-ph-of-dat\", \"abase-transaction-processing/)[processing/](https://www.dataversity.net/acid-vs-base-the-shifting-ph\", \"-of-database-transaction-processing/)\\n- **Compensating Transaction** [https://learn.microsoft.com/az\", \"ure/architecture/patterns/compensating-transaction](https://docs.microsoft.com/azure/architecture/pa\", \"tterns/compensating-transaction)\\n- **Udi Dahan. Service Oriented Composition** <https://udidahan.com\", \"/2014/07/30/service-oriented-composition-with-video/>\\n\\n## <span id=\\\"page-50-1\\\"></span>Identify domai\", \"n-model boundaries for each microservice\\n\\nThe goal when identifying model boundaries and size for ea\", \"ch microservice isn't to get to the most granular separation possible, although you should tend towa\", \"rd small microservices if possible. Instead, your goal should be to get to the most meaningful separ\", \"ation guided by your domain knowledge. The emphasis isn't on the size, but instead on business capab\", \"ilities. In addition, if there's clear cohesion needed for a certain area of the application based o\", \"n a high number of dependencies, that indicates the need for a single microservice, too. Cohesion is\", \" a way to identify how to break apart or group together microservices. Ultimately, while you gain mo\", \"re knowledge about the domain, you should adapt the size of your microservice, iteratively. Finding \", \"the right size isn't a one-shot process.\\n\\n[Sam Newman,](https://samnewman.io/) a recognized promoter\", \" of microservices and author of the book [Building Microservices,](https://samnewman.io/books/buildi\", \"ng_microservices/) highlights that you should design your microservices based on the Bounded Context\", \" (BC) pattern (part of domain-driven design), as introduced earlier. Sometimes, a BC could be compos\", \"ed of several physical services, but not vice versa.\\n\\nA domain model with specific domain entities a\", \"pplies within a concrete BC or microservice. A BC delimits the applicability of a domain model and g\", \"ives developer team members a clear and shared understanding of what must be cohesive and what can b\", \"e developed independently. These are the same goals for microservices.\\n\\nAnother tool that informs yo\", \"ur design choice is [Conway's law](https://en.wikipedia.org/wiki/Conway%27s_law), which states that \", \"an application will reflect the social boundaries of the organization that produced it. But sometime\", \"s the opposite is true the company's organization is formed by the software. You might need to rever\", \"se Conway's law and build the boundaries the way you want the company to be organized, leaning towar\", \"d business process consulting.\\n\\nTo identify bounded contexts, you can use a DDD pattern called the [\", \"Context Mapping pattern.](https://www.infoq.com/articles/ddd-contextmapping) With Context Mapping, y\", \"ou identify the various contexts in the application and their boundaries. It's common to have a diff\", \"erent context and boundary for each small subsystem, for instance. The Context Map is a way to defin\", \"e and make explicit those boundaries between domains. A BC is autonomous and includes the details of\", \" a single domain -details like the domain entities- and defines integration contracts with other BCs\", \". This is similar to the definition of a microservice: it's autonomous, it implements certain domain\", \" capability, and it must provide interfaces. This is why Context Mapping and the Bounded Context pat\", \"tern are good approaches for identifying the domain model boundaries of your microservices.\\n\\nWhen de\", \"signing a large application, you'll see how its domain model can be fragmented - a domain expert fro\", \"m the catalog domain will name entities differently in the catalog and inventory domains than a ship\", \"ping domain expert, for instance. Or the user domain entity might be different in size and number of\", \" attributes when dealing with a CRM expert who wants to store every detail about the customer than f\", \"or an ordering domain expert who just needs partial data about the customer. It's very hard to disam\", \"biguate all domain terms across all the domains related to a large application. But the most importa\", \"nt thing is that you shouldn't try to unify the terms. Instead, accept the differences and richness \", \"provided by each domain. If you try to have a unified database for the whole application, attempts a\", \"t a unified vocabulary will be awkward and won't sound right to any of the multiple domain experts. \", \"Therefore, BCs (implemented as microservices) will help you to clarify where you can use certain dom\", \"ain terms and where you'll need to split the system and create additional BCs with different domains\", \".\\n\\nYou'll know that you got the right boundaries and sizes of each BC and domain model if you have f\", \"ew strong relationships between domain models, and you do not usually need to merge information from\", \" multiple domain models when performing typical application operations.\\n\\nPerhaps the best answer to \", \"the question of how large a domain model for each microservice should be is the following: it should\", \" have an autonomous BC, as isolated as possible, that enables you to work without having to constant\", \"ly switch to other contexts (other microservice's models). In Figure 410, you can see how multiple m\", \"icroservices (multiple BCs) each has their own model and how their entities can be defined, dependin\", \"g on the specific requirements for each of the identified domains in your application.\\n\\n![](_page_52\", \"_Figure_2.jpeg)\\n\\n*Figure 4-10. Identifying entities and microservice model boundaries*\\n\\nFigure 4-10 \", \"illustrates a sample scenario related to an online conference management system. The same entity app\", \"ears as \\\"Users\\\", \\\"Buyers\\\", \\\"Payers\\\", and \\\"Customers\\\" depending on the bounded context. You've identi\", \"fied several BCs that could be implemented as microservices, based on domains that domain experts de\", \"fined for you. As you can see, there are entities that are present just in a single microservice mod\", \"el, like Payments in the Payment microservice. Those will be easy to implement.\\n\\nHowever, you might \", \"also have entities that have a different shape but share the same identity across the multiple domai\", \"n models from the multiple microservices. For example, the User entity is identified in the Conferen\", \"ces Management microservice. That same user, with the same identity, is the one named Buyers in the \", \"Ordering microservice, or the one named Payer in the Payment microservice, and even the one named Cu\", \"stomer in the Customer Service microservice. This is because, depending on the [ubiquitous language]\", \"(https://martinfowler.com/bliki/UbiquitousLanguage.html) that each domain expert is using, a user mi\", \"ght have a different perspective even with different attributes. The user entity in the microservice\", \" model named Conferences Management might have most of its personal data attributes. However, that s\", \"ame user in the shape of Payer in the microservice Payment or in the shape of Customer in the micros\", \"ervice Customer Service might not need the same list of attributes.\\n\\nA similar approach is illustrat\", \"ed in Figure 4-11.\\n\\n![](_page_53_Figure_0.jpeg)\\n\\n*Figure 4-11. Decomposing traditional data models i\", \"nto multiple domain models*\\n\\nWhen decomposing a traditional data model between bounded contexts, you\", \" can have different entities that share the same identity (a buyer is also a user) with different at\", \"tributes in each bounded context. You can see how the user is present in the Conferences Management \", \"microservice model as the User entity and is also present in the form of the Buyer entity in the Pri\", \"cing microservice, with alternate attributes or details about the user when it's actually a buyer. E\", \"ach microservice or BC might not need all the data related to a User entity, just part of it, depend\", \"ing on the problem to solve or the context. For instance, in the Pricing microservice model, you do \", \"not need the address or the name of the user, just the ID (as identity) and Status, which will have \", \"an impact on discounts when pricing the seats per buyer.\\n\\nThe Seat entity has the same name but diff\", \"erent attributes in each domain model. However, Seat shares identity based on the same ID, as happen\", \"s with User and Buyer.\\n\\nBasically, there's a shared concept of a user that exists in multiple servic\", \"es (domains), which all share the identity of that user. But in each domain model there might be add\", \"itional or different details about the user entity. Therefore, there needs to be a way to map a user\", \" entity from one domain (microservice) to another.\\n\\nThere are several benefits to not sharing the sa\", \"me user entity with the same number of attributes across domains. One benefit is to reduce duplicati\", \"on, so that microservice models do not have any data that they do not need. Another benefit is havin\", \"g a primary microservice that owns a certain type of data per entity so that updates and queries for\", \" that type of data are driven only by that microservice.\\n\\n## <span id=\\\"page-54-0\\\"></span>The API gat\", \"eway pattern versus the Direct client-tomicroservice communication\\n\\nIn a microservices architecture,\", \" each microservice exposes a set of (typically) fine-grained endpoints. This fact can impact the cli\", \"ent-to-microservice communication, as explained in this section.\\n\\n## <span id=\\\"page-54-1\\\"></span>**D\", \"irect client-to-microservice communication**\\n\\nA possible approach is to use a direct client-to-micro\", \"service communication architecture. In this approach, a client app can make requests directly to som\", \"e of the microservices, as shown in Figure 4- 12.\\n\\n![](_page_54_Figure_6.jpeg)\\n\\n*Figure 4-12. Using \", \"a direct client-to-microservice communication architecture*\\n\\nIn this approach, each microservice has\", \" a public endpoint, sometimes with a different TCP port for each microservice. An example of a URL f\", \"or a particular service could be the following URL in Azure:\\n\\nhttp://eshoponcontainers.westus.clouda\", \"pp.azure.com:88/\\n\\nIn a production environment based on a cluster, that URL would map to the load bal\", \"ancer used in the cluster, which in turn distributes the requests across the microservices. In produ\", \"ction environments, you could have an Application Delivery Controller (ADC) like [Azure Application \", \"Gateway](https://docs.microsoft.com/azure/application-gateway/application-gateway-introduction) betw\", \"een your microservices and the Internet. This layer acts as a transparent tier that not only perform\", \"s load balancing, but secures your services by offering SSL termination. This approach improves the \", \"load of your hosts by offloading CPU-intensive SSL termination and other routing duties to the Azure\", \" Application Gateway. In any case, a load balancer and ADC are transparent from a logical applicatio\", \"n architecture point of view.\\n\\nA direct client-to-microservice communication architecture could be g\", \"ood enough for a small microservice-based application, especially if the client app is a server-side\", \" web application like an ASP.NET MVC app. However, when you build large and complex microservice-bas\", \"ed applications (for example, when handling dozens of microservice types), and especially when the c\", \"lient apps are remote mobile apps or SPA web applications, that approach faces a few issues.\\n\\nConsid\", \"er the following questions when developing a large application based on microservices:\\n\\n\\u2022 *How can c\", \"lient apps minimize the number of requests to the back end and reduce chatty communication to multip\", \"le microservices?*\\n\\nInteracting with multiple microservices to build a single UI screen increases th\", \"e number of round trips across the Internet. This approach increases latency and complexity on the U\", \"I side. Ideally, responses should be efficiently aggregated in the server side. This approach reduce\", \"s latency, since multiple pieces of data come back in parallel and some UI can show data as soon as \", \"it's ready.\\n\\n\\u2022 *How can you handle cross-cutting concerns such as authorization, data transformation\", \"s, and dynamic request dispatching?*\\n\\nImplementing security and cross-cutting concerns like security\", \" and authorization on every microservice can require significant development effort. A possible appr\", \"oach is to have those services within the Docker host or internal cluster to restrict direct access \", \"to them from the outside, and to implement those cross-cutting concerns in a centralized place, like\", \" an API Gateway.\\n\\n\\u2022 *How can client apps communicate with services that use non-Internet-friendly pr\", \"otocols?*\\n\\nProtocols used on the server side (like AMQP or binary protocols) are not supported in cl\", \"ient apps. Therefore, requests must be performed through protocols like HTTP/HTTPS and translated to\", \" the other protocols afterwards. A *man-in-the-middle* approach can help in this situation.\\n\\n\\u2022 *How \", \"can you shape a facade especially made for mobile apps?*\\n\\nThe API of multiple microservices might no\", \"t be well designed for the needs of different client applications. For instance, the needs of a mobi\", \"le app might be different than the needs of a web app. For mobile apps, you might need to optimize e\", \"ven further so that data responses can be more efficient. You might do this functionality by aggrega\", \"ting data from multiple microservices and returning a single set of data, and sometimes eliminating \", \"any data in the response that isn't needed by the mobile app. And, of course, you might compress tha\", \"t data. Again, a facade or API in between the mobile app and the microservices can be convenient for\", \" this scenario.\\n\\n## <span id=\\\"page-55-0\\\"></span>**Why consider API Gateways instead of direct client\", \"-to-microservice communication**\\n\\nIn a microservices architecture, the client apps usually need to c\", \"onsume functionality from more than one microservice. If that consumption is performed directly, the\", \" client needs to handle multiple calls to microservice endpoints. What happens when the application \", \"evolves and new microservices are introduced or existing microservices are updated? If your applicat\", \"ion has many microservices, handling so many endpoints from the client apps can be a nightmare. Sinc\", \"e the client app would be coupled to those internal endpoints, evolving the microservices in the fut\", \"ure can cause high impact for the client apps.\\n\\nTherefore, having an intermediate level or tier of i\", \"ndirection (Gateway) can be convenient for microservice-based applications. If you don't have API Ga\", \"teways, the client apps must send requests directly to the microservices and that raises problems, s\", \"uch as the following issues:\\n\\n- **Coupling**: Without the API Gateway pattern, the client apps are c\", \"oupled to the internal microservices. The client apps need to know how the multiple areas of the app\", \"lication are decomposed in microservices. When evolving and refactoring the internal microservices, \", \"those actions impact maintenance because they cause breaking changes to the client apps due to the d\", \"irect reference to the internal microservices from the client apps. Client apps need to be updated f\", \"requently, making the solution harder to evolve.\\n- **Too many round trips**: A single page/screen in\", \" the client app might require several calls to multiple services. That approach can result in multip\", \"le network round trips between the client and the server, adding significant latency. Aggregation ha\", \"ndled in an intermediate level could improve the performance and user experience for the client app.\", \"\\n- **Security issues**: Without a gateway, all the microservices must be exposed to the \\\"external wo\", \"rld\\\", making the attack surface larger than if you hide internal microservices that aren't directly \", \"used by the client apps. The smaller the attack surface is, the more secure your application can be.\", \"\\n- **Cross-cutting concerns**: Each publicly published microservice must handle concerns such as aut\", \"horization and SSL. In many situations, those concerns could be handled in a single tier so the inte\", \"rnal microservices are simplified.\\n\\n## <span id=\\\"page-56-0\\\"></span>**What is the API Gateway pattern\", \"?**\\n\\nWhen you design and build large or complex microservice-based applications with multiple client\", \" apps, a good approach to consider can be an [API Gateway.](https://microservices.io/patterns/apigat\", \"eway.html) This pattern is a service that provides a single-entry point for certain groups of micros\", \"ervices. It's similar to the [Facade pattern](https://en.wikipedia.org/wiki/Facade_pattern) from obj\", \"ectoriented design, but in this case, it's part of a distributed system. The API Gateway pattern is \", \"also sometimes known as the \\\"backend for frontend\\\" ([BFF\\\\)](https://samnewman.io/patterns/architectu\", \"ral/bff/) because you build it while thinking about the needs of the client app.\\n\\nTherefore, the API\", \" gateway sits between the client apps and the microservices. It acts as a reverse proxy, routing req\", \"uests from clients to services. It can also provide other cross-cutting features such as authenticat\", \"ion, SSL termination, and cache.\\n\\nFigure 4-13 shows how a custom API Gateway can fit into a simplifi\", \"ed microservice-based architecture with just a few microservices.\\n\\n![](_page_57_Figure_1.jpeg)\\n\\n*Fig\", \"ure 4-13. Using an API Gateway implemented as a custom service*\\n\\nApps connect to a single endpoint, \", \"the API Gateway, that's configured to forward requests to individual microservices. In this example,\", \" the API Gateway would be implemented as a custom ASP.NET Core WebHost service running as a containe\", \"r.\\n\\nIt's important to highlight that in that diagram, you would be using a single custom API Gateway\", \" service facing multiple and different client apps. That fact can be an important risk because your \", \"API Gateway service will be growing and evolving based on many different requirements from the clien\", \"t apps. Eventually, it will be bloated because of those different needs and effectively it could be \", \"similar to a monolithic application or monolithic service. That's why it's very much recommended to \", \"split the API Gateway in multiple services or multiple smaller API Gateways, one per client app form\", \"-factor type, for instance.\\n\\nYou need to be careful when implementing the API Gateway pattern. Usual\", \"ly it isn't a good idea to have a single API Gateway aggregating all the internal microservices of y\", \"our application. If it does, it acts as a monolithic aggregator or orchestrator and violates microse\", \"rvice autonomy by coupling all the microservices.\\n\\nTherefore, the API Gateways should be segregated \", \"based on business boundaries and the client apps and not act as a single aggregator for all the inte\", \"rnal microservices.\\n\\nWhen splitting the API Gateway tier into multiple API Gateways, if your applica\", \"tion has multiple client apps, that can be a primary pivot when identifying the multiple API Gateway\", \"s types, so that you can have a different facade for the needs of each client app. This case is a pa\", \"ttern named \\\"Backend for Frontend\\\" ([BFF\\\\)](https://samnewman.io/patterns/architectural/bff/) where \", \"each API Gateway can provide a different API tailored for each client app type, possibly even based \", \"on the client form factor by implementing specific adapter code which underneath calls multiple inte\", \"rnal microservices, as shown in the following image:\\n\\n![](_page_58_Figure_1.jpeg)\\n\\n*Figure 4-13.1. U\", \"sing multiple custom API Gateways*\\n\\nFigure 4-13.1 shows API Gateways that are segregated by client t\", \"ype; one for mobile clients and one for web clients. A traditional web app connects to an MVC micros\", \"ervice that uses the web API Gateway. The example depicts a simplified architecture with multiple fi\", \"ne-grained API Gateways. In this case, the boundaries identified for each API Gateway are based pure\", \"ly on the \\\"Backend for Frontend\\\" ([BFF\\\\)](https://samnewman.io/patterns/architectural/bff/) pattern,\", \" hence based just on the API needed per client app. But in larger applications you should also go fu\", \"rther and create other API Gateways based on business boundaries as a second design pivot.\\n\\n## <span\", \" id=\\\"page-58-0\\\"></span>**Main features in the API Gateway pattern**\\n\\nAn API Gateway can offer multip\", \"le features. Depending on the product it might offer richer or simpler features, however, the most i\", \"mportant and foundational features for any API Gateway are the following design patterns:\\n\\n**Reverse\", \" proxy or gateway routing.** The API Gateway offers a reverse proxy to redirect or route requests (l\", \"ayer 7 routing, usually HTTP requests) to the endpoints of the internal microservices. The gateway p\", \"rovides a single endpoint or URL for the client apps and then internally maps the requests to a grou\", \"p of internal microservices. This routing feature helps to decouple the client apps from the microse\", \"rvices but it's also convenient when modernizing a monolithic API by sitting the API Gateway in betw\", \"een the monolithic API and the client apps, then you can add new APIs as new microservices while sti\", \"ll using the legacy monolithic API until it's split into many microservices in the future. Because o\", \"f the API Gateway, the client apps won't notice if the APIs being used are implemented as internal m\", \"icroservices or a monolithic API and more importantly, when evolving and refactoring the monolithic \", \"API into microservices, thanks to the API Gateway routing, client apps won't be impacted with any UR\", \"I change.\\n\\nFor more information, see [Gateway routing pattern.](https://docs.microsoft.com/azure/arc\", \"hitecture/patterns/gateway-routing)\\n\\n**Requests aggregation.** As part of the gateway pattern you ca\", \"n aggregate multiple client requests (usually HTTP requests) targeting multiple internal microservic\", \"es into a single client request. This pattern is especially convenient when a client page/screen nee\", \"ds information from several microservices. With this approach, the client app sends a single request\", \" to the API Gateway that dispatches several requests to the internal microservices and then aggregat\", \"es the results and sends everything back to the client app. The main benefit and goal of this design\", \" pattern is to reduce chattiness between the client apps and the backend API, which is especially im\", \"portant for remote apps out of the datacenter where the microservices live, like mobile apps or requ\", \"ests coming from SPA apps that come from JavaScript in client remote browsers. For regular web apps \", \"performing the requests in the server environment (like an ASP.NET Core MVC web app), this pattern i\", \"s not so important as the latency is very much smaller than for remote client apps.\\n\\nDepending on th\", \"e API Gateway product you use, it might be able to perform this aggregation. However, in many cases \", \"it's more flexible to create aggregation microservices under the scope of the API Gateway, so you de\", \"fine the aggregation in code (that is, C# code):\\n\\nFor more information, see [Gateway aggregation pat\", \"tern.](https://docs.microsoft.com/azure/architecture/patterns/gateway-aggregation)\\n\\n**Cross-cutting \", \"concerns or gateway offloading.** Depending on the features offered by each API Gateway product, you\", \" can offload functionality from individual microservices to the gateway, which simplifies the implem\", \"entation of each microservice by consolidating cross-cutting concerns into one tier. This approach i\", \"s especially convenient for specialized features that can be complex to implement properly in every \", \"internal microservice, such as the following functionality:\\n\\n- Authentication and authorization\\n- Se\", \"rvice discovery integration\\n- Response caching\\n- Retry policies, circuit breaker, and QoS\\n- Rate lim\", \"iting and throttling\\n- Load balancing\\n- Logging, tracing, correlation\\n- Headers, query strings, and \", \"claims transformation\\n- IP allowlisting\\n\\nFor more information, see [Gateway offloading pattern.](htt\", \"ps://docs.microsoft.com/azure/architecture/patterns/gateway-offloading)\\n\\n### <span id=\\\"page-59-0\\\"></\", \"span>**Using products with API Gateway features**\\n\\nThere can be many more cross-cutting concerns off\", \"ered by the API Gateways products depending on each implementation. We'll explore here:\\n\\n- [Azure AP\", \"I Management](https://azure.microsoft.com/services/api-management/)\\n- [Ocelot](https://github.com/Th\", \"reeMammals/Ocelot)\\n\\n## **Azure API Management**\\n\\n[Azure API Management](https://azure.microsoft.com/\", \"services/api-management/) (as shown in Figure 4-14) not only solves your API Gateway needs but provi\", \"des features like gathering insights from your APIs. If you're using an API management solution, an \", \"API Gateway is only a component within that full API management solution.\\n\\n![](_page_60_Figure_3.jpe\", \"g)\\n\\n*Figure 4-14. Using Azure API Management for your API Gateway*\\n\\nAzure API Management solves both\", \" your API Gateway and Management needs like logging, security, metering, etc. In this case, when usi\", \"ng a product like Azure API Management, the fact that you might have a single API Gateway is not so \", \"risky because these kinds of API Gateways are \\\"thinner\\\", meaning that you don't implement custom C# \", \"code that could evolve towards a monolithic component.\\n\\nThe API Gateway products usually act like a \", \"reverse proxy for ingress communication, where you can also filter the APIs from the internal micros\", \"ervices plus apply authorization to the published APIs in this single tier.\\n\\nThe insights available \", \"from an API Management system help you get an understanding of how your APIs are being used and how \", \"they are performing. They do this activity by letting you view near realtime analytics reports and i\", \"dentifying trends that might impact your business. Plus, you can have logs about request and respons\", \"e activity for further online and offline analysis.\\n\\nWith Azure API Management, you can secure your \", \"APIs using a key, a token, and IP filtering. These features let you enforce flexible and fine-graine\", \"d quotas and rate limits, modify the shape and behavior of your APIs using policies, and improve per\", \"formance with response caching.\\n\\nIn this guide and the reference sample application (eShopOnContaine\", \"rs), the architecture is limited to a simpler and custom-made containerized architecture in order to\", \" focus on plain containers without\\n\\nusing PaaS products like Azure API Management. But for large mic\", \"roservice-based applications that are deployed into Microsoft Azure, we encourage you to evaluate Az\", \"ure API Management as the base for your API Gateways in production.\\n\\n## **Ocelot**\\n\\n[Ocelot](https:/\", \"/github.com/ThreeMammals/Ocelot) is a lightweight API Gateway, recommended for simpler approaches. O\", \"celot is an Open Source .NET Core-based API Gateway especially made for microservices architectures \", \"that need unified points of entry into their systems. It's lightweight, fast, and scalable and provi\", \"des routing and authentication among many other features.\\n\\nThe main reason to choose Ocelot for the \", \"[eShopOnContainers reference application 2.0](https://github.com/dotnet-architecture/eShopOnContaine\", \"rs/releases/tag/2.0) is because Ocelot is a .NET Core lightweight API Gateway that you can deploy in\", \"to the same application deployment environment where you're deploying your microservices/containers,\", \" such as a Docker Host, Kubernetes, etc. And since it's based on .NET Core, it's cross-platform allo\", \"wing you to deploy on Linux or Windows.\\n\\nThe previous diagrams showing custom API Gateways running i\", \"n containers are precisely how you can also run Ocelot in a container and microservice-based applica\", \"tion.\\n\\nIn addition, there are many other products in the market offering API Gateways features, such\", \" as Apigee, Kong, MuleSoft, WSO2, and other products like Linkerd and Istio for service mesh ingress\", \" controller features.\\n\\nAfter the initial architecture and patterns explanation sections, the next se\", \"ctions explain how to implement API Gateways with [Ocelot.](https://github.com/ThreeMammals/Ocelot)\\n\", \"\\n### <span id=\\\"page-61-0\\\"></span>**Drawbacks of the API Gateway pattern**\\n\\n- The most important draw\", \"back is that when you implement an API Gateway, you're coupling that tier with the internal microser\", \"vices. Coupling like this might introduce serious difficulties for your application. Clemens Vaster,\", \" architect at the Azure Service Bus team, refers to this potential difficulty as \\\"the new ESB\\\" in th\", \"e \\\"[Messaging and Microservices](https://www.youtube.com/watch?v=rXi5CLjIQ9k)\\\" session at GOTO 2016.\", \"\\n- Using a microservices API Gateway creates an additional possible single point of failure.\\n- An AP\", \"I Gateway can introduce increased response time due to the additional network call. However, this ex\", \"tra call usually has less impact than having a client interface that's too chatty directly calling t\", \"he internal microservices.\\n- If not scaled out properly, the API Gateway can become a bottleneck.\\n- \", \"An API Gateway requires additional development cost and future maintenance if it includes custom log\", \"ic and data aggregation. Developers must update the API Gateway in order to expose each microservice\", \"'s endpoints. Moreover, implementation changes in the internal microservices might cause code change\", \"s at the API Gateway level. However, if the API Gateway is just applying security, logging, and vers\", \"ioning (as when using Azure API Management), this additional development cost might not apply.\\n\\n\\u2022 If\", \" the API Gateway is developed by a single team, there can be a development bottleneck. This aspect i\", \"s another reason why a better approach is to have several fined-grained API Gateways that respond to\", \" different client needs. You could also segregate the API Gateway internally into multiple areas or \", \"layers that are owned by the different teams working on the internal microservices.\\n\\n## <span id=\\\"pa\", \"ge-62-0\\\"></span>**Additional resources**\\n\\n- **Chris Richardson. Pattern: API Gateway / Backend for F\", \"ront-End** <https://microservices.io/patterns/apigateway.html>\\n- **API Gateway pattern** [https://le\", \"arn.microsoft.com/azure/architecture/microservices/gateway](https://docs.microsoft.com/azure/archite\", \"cture/microservices/gateway)\\n- **Aggregation and composition pattern** <https://microservices.io/pat\", \"terns/data/api-composition.html>\\n- **Azure API Management** <https://azure.microsoft.com/services/ap\", \"i-management/>\\n- **Udi Dahan. Service Oriented Composition** <https://udidahan.com/2014/07/30/servic\", \"e-oriented-composition-with-video/>\\n- **Clemens Vasters. Messaging and Microservices at GOTO 2016 (v\", \"ideo)** <https://www.youtube.com/watch?v=rXi5CLjIQ9k>\\n- **API Gateway in a Nutshell** (ASP.NET Core \", \"API Gateway Tutorial Series) <https://www.pogsdotnet.com/2018/08/api-gateway-in-nutshell.html>\\n\\n## <\", \"span id=\\\"page-62-1\\\"></span>Communication in a microservice architecture\\n\\nIn a monolithic application\", \" running on a single process, components invoke one another using language-level method or function \", \"calls. These can be strongly coupled if you're creating objects with code (for example, new ClassNam\", \"e()), or can be invoked in a decoupled way if you're using Dependency Injection by referencing abstr\", \"actions rather than concrete object instances. Either way, the objects are running within the same p\", \"rocess. The biggest challenge when changing from a monolithic application to a microservices-based a\", \"pplication lies in changing the communication mechanism. A direct conversion from in-process method \", \"calls into RPC calls to services will cause a chatty and not efficient communication that won't perf\", \"orm well in distributed environments. The challenges of designing distributed system properly are we\", \"ll enough known that there's even a canon known as the [Fallacies of distributed computing](https://\", \"en.wikipedia.org/wiki/Fallacies_of_distributed_computing) that lists assumptions that developers oft\", \"en make when moving from monolithic to distributed designs.\\n\\nThere isn't one solution, but several. \", \"One solution involves isolating the business microservices as much as possible. You then use asynchr\", \"onous communication between the internal microservices and replace fine-grained communication that's\", \" typical in intra-process communication between objects with coarser-grained communication. You can \", \"do this by grouping calls, and by returning data that aggregates the results of multiple internal ca\", \"lls, to the client.\\n\\nA microservices-based application is a distributed system running on multiple p\", \"rocesses or services, usually even across multiple servers or hosts. Each service instance is typica\", \"lly a process. Therefore, services must interact using an inter-process communication protocol such \", \"as HTTP, AMQP, or a binary protocol like TCP, depending on the nature of each service.\\n\\nThe microser\", \"vice community promotes the philosophy of \\\"[smart endpoints and dumb pipes](https://simplicable.com/\", \"new/smart-endpoints-and-dumb-pipes)\\\". This slogan encourages a design that's as decoupled as possibl\", \"e between microservices, and as cohesive as possible within a single microservice. As explained earl\", \"ier, each microservice owns its own data and its own domain logic. But the microservices composing a\", \"n end-to-end application are usually simply choreographed by using REST communications rather than c\", \"omplex protocols such as WS-\\\\* and flexible event-driven communications instead of centralized busin\", \"ess-process-orchestrators.\\n\\nThe two commonly used protocols are HTTP request/response with resource \", \"APIs (when querying most of all), and lightweight asynchronous messaging when communicating updates \", \"across multiple microservices. These are explained in more detail in the following sections.\\n\\n## <sp\", \"an id=\\\"page-63-0\\\"></span>**Communication types**\\n\\nClient and services can communicate through many d\", \"ifferent types of communication, each one targeting a different scenario and goals. Initially, those\", \" types of communications can be classified in two axes.\\n\\nThe first axis defines if the protocol is s\", \"ynchronous or asynchronous:\\n\\n- Synchronous protocol. HTTP is a synchronous protocol. The client send\", \"s a request and waits for a response from the service. That's independent of the client code executi\", \"on that could be synchronous (thread is blocked) or asynchronous (thread isn't blocked, and the resp\", \"onse will reach a callback eventually). The important point here is that the protocol (HTTP/HTTPS) i\", \"s synchronous and the client code can only continue its task when it receives the HTTP server respon\", \"se.\\n- Asynchronous protocol. Other protocols like AMQP (a protocol supported by many operating syste\", \"ms and cloud environments) use asynchronous messages. The client code or message sender usually does\", \"n't wait for a response. It just sends the message as when sending a message to a RabbitMQ queue or \", \"any other message broker.\\n\\nThe second axis defines if the communication has a single receiver or mul\", \"tiple receivers:\\n\\n- Single receiver. Each request must be processed by exactly one receiver or servi\", \"ce. An example of this communication is the [Command pattern.](https://en.wikipedia.org/wiki/Command\", \"_pattern)\\n- Multiple receivers. Each request can be processed by zero to multiple receivers. This ty\", \"pe of communication must be asynchronous. An example is the [publish/subscribe](https://en.wikipedia\", \".org/wiki/Publish%E2%80%93subscribe_pattern) mechanism used in patterns like [Event-driven architect\", \"ure.](https://microservices.io/patterns/data/event-driven-architecture.html) This is based on an eve\", \"nt-bus interface or message broker when propagating data updates between multiple microservices thro\", \"ugh events; it's usually implemented through a service bus or similar artifact like [Azure Service B\", \"us](https://azure.microsoft.com/services/service-bus/) by using [topics and subscriptions.](https://\", \"docs.microsoft.com/azure/service-bus-messaging/service-bus-dotnet-how-to-use-topics-subscriptions)\\n\\n\", \"A microservice-based application will often use a combination of these communication styles. The mos\", \"t common type is single-receiver communication with a synchronous protocol like HTTP/HTTPS when invo\", \"king a regular Web API HTTP service. Microservices also typically use messaging protocols for asynch\", \"ronous communication between microservices.\\n\\nThese axes are good to know so you have clarity on the \", \"possible communication mechanisms, but they're not the important concerns when building microservice\", \"s. Neither the asynchronous nature of client thread execution nor the asynchronous nature of the sel\", \"ected protocol are the important points when integrating microservices. What *is* important is being\", \" able to integrate your microservices asynchronously while maintaining the independence of microserv\", \"ices, as explained in the following section.\\n\\n## <span id=\\\"page-64-0\\\"></span>**Asynchronous microser\", \"vice integration enforces microservice's autonomy**\\n\\nAs mentioned, the important point when building\", \" a microservices-based application is the way you integrate your microservices. Ideally, you should \", \"try to minimize the communication between the internal microservices. The fewer communications betwe\", \"en microservices, the better. But in many cases, you'll have to somehow integrate the microservices.\", \" When you need to do that, the critical rule here is that the communication between the microservice\", \"s should be asynchronous. That doesn't mean that you have to use a specific protocol (for example, a\", \"synchronous messaging versus synchronous HTTP). It just means that the communication between microse\", \"rvices should be done only by propagating data asynchronously, but try not to depend on other intern\", \"al microservices as part of the initial service's HTTP request/response operation.\\n\\nIf possible, nev\", \"er depend on synchronous communication (request/response) between multiple microservices, not even f\", \"or queries. The goal of each microservice is to be autonomous and available to the client consumer, \", \"even if the other services that are part of the end-to-end application are down or unhealthy. If you\", \" think you need to make a call from one microservice to other microservices (like performing an HTTP\", \" request for a data query) to be able to provide a response to a client application, you have an arc\", \"hitecture that won't be resilient when some microservices fail.\\n\\nMoreover, having HTTP dependencies \", \"between microservices, like when creating long request/response cycles with HTTP request chains, as \", \"shown in the first part of the Figure 4-15, not only makes your microservices not autonomous but als\", \"o their performance is impacted as soon as one of the services in that chain isn't performing well.\\n\", \"\\nThe more you add synchronous dependencies between microservices, such as query requests, the worse \", \"the overall response time gets for the client apps.\\n\\n![](_page_65_Figure_1.jpeg)\\n\\n*Figure 4-15. Anti\", \"-patterns and patterns in communication between microservices*\\n\\nAs shown in the above diagram, in sy\", \"nchronous communication a \\\"chain\\\" of requests is created between microservices while serving the cli\", \"ent request. This is an anti-pattern. In asynchronous communication microservices use asynchronous m\", \"essages or http polling to communicate with other microservices, but the client request is served ri\", \"ght away.\\n\\nIf your microservice needs to raise an additional action in another microservice, if poss\", \"ible, do not perform that action synchronously and as part of the original microservice request and \", \"reply operation. Instead, do it asynchronously (using asynchronous messaging or integration events, \", \"queues, etc.). But, as much as possible, do not invoke the action synchronously as part of the origi\", \"nal synchronous request and reply operation.\\n\\nAnd finally (and this is where most of the issues aris\", \"e when building microservices), if your initial microservice needs data that's originally owned by o\", \"ther microservices, do not rely on making synchronous requests for that data. Instead, replicate or \", \"propagate that data (only the attributes you need) into the initial service's database by using even\", \"tual consistency (typically by using integration events, as explained in upcoming sections).\\n\\nAs not\", \"ed earlier in the Identifying domain-model boundaries for each microservice section, duplicating som\", \"e data across several microservices isn't an incorrect design\\u2014on the contrary, when doing that you c\", \"an translate the data into the specific language or terms of that additional domain or Bounded Conte\", \"xt. For instance, in the [eShopOnContainers application](https://github.com/dotnet-architecture/eSho\", \"pOnContainers) you have a microservice named identity-api that's in charge of most of the user's dat\", \"a with an entity named User. However, when you need to store data about the user within the Ordering\", \" microservice, you store it as a different entity named Buyer. The Buyer entity shares the same iden\", \"tity with the original User entity, but it might have only the few attributes needed by the Ordering\", \" domain, and not the whole user profile.\\n\\nYou might use any protocol to communicate and propagate da\", \"ta asynchronously across microservices in order to have eventual consistency. As mentioned, you coul\", \"d use integration events using an event bus or message broker or you could even use HTTP by polling \", \"the other services instead. It doesn't matter. The important rule is to not create synchronous depen\", \"dencies between your microservices.\\n\\nThe following sections explain the multiple communication style\", \"s you can consider using in a microservice-based application.\\n\\n## <span id=\\\"page-66-0\\\"></span>**Comm\", \"unication styles**\\n\\nThere are many protocols and choices you can use for communication, depending on\", \" the communication type you want to use. If you're using a synchronous request/response-based commun\", \"ication mechanism, protocols such as HTTP and REST approaches are the most common, especially if you\", \"'re publishing your services outside the Docker host or microservice cluster. If you're communicatin\", \"g between services internally (within your Docker host or microservices cluster), you might also wan\", \"t to use binary format communication mechanisms (like WCF using TCP and binary format). Alternativel\", \"y, you can use asynchronous, message-based communication mechanisms such as AMQP.\\n\\nThere are also mu\", \"ltiple message formats like JSON or XML, or even binary formats, which can be more efficient. If you\", \"r chosen binary format isn't a standard, it's probably not a good idea to publicly publish your serv\", \"ices using that format. You could use a non-standard format for internal communication between your \", \"microservices. You might do this when communicating between microservices within your Docker host or\", \" microservice cluster (for example, Docker orchestrators), or for proprietary client applications th\", \"at talk to the microservices.\\n\\n### **Request/response communication with HTTP and REST**\\n\\nWhen a cli\", \"ent uses request/response communication, it sends a request to a service, then the service processes\", \" the request and sends back a response. Request/response communication is especially well suited for\", \" querying data for a real-time UI (a live user interface) from client apps. Therefore, in a microser\", \"vice architecture you'll probably use this communication mechanism for most queries, as shown in Fig\", \"ure 4-16.\\n\\n![](_page_66_Figure_9.jpeg)\\n\\n*Figure 4-16. Using HTTP request/response communication (syn\", \"chronous or asynchronous)*\\n\\nWhen a client uses request/response communication, it assumes that the r\", \"esponse will arrive in a short time, typically less than a second, or a few seconds at most. For del\", \"ayed responses, you need to implement asynchronous communication based on [messaging patterns](https\", \"://docs.microsoft.com/azure/architecture/patterns/category/messaging) and [messaging technologies,](\", \"https://en.wikipedia.org/wiki/Message-oriented_middleware) which is a different approach that we exp\", \"lain in the next section.\\n\\nA popular architectural style for request/response communication is [REST\", \".](https://en.wikipedia.org/wiki/Representational_state_transfer) This approach is based on, and tig\", \"htly coupled to, the [HTTP](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol) protocol, emb\", \"racing HTTP verbs like GET, POST, and PUT. REST is the most commonly used architectural communicatio\", \"n approach when creating services. You can implement REST services when you develop ASP.NET Core Web\", \" API services.\\n\\nThere's additional value when using HTTP REST services as your interface definition \", \"language. For instance, if you use [Swagger metadata](https://swagger.io/) to describe your service \", \"API, you can use tools that generate client stubs that can directly discover and consume your servic\", \"es.\\n\\n### **Additional resources**\\n\\n- **Martin Fowler. Richardson Maturity Model** A description of t\", \"he REST model. <https://martinfowler.com/articles/richardsonMaturityModel.html>\\n- **Swagger** The of\", \"ficial site. <https://swagger.io/>\\n\\n### **Push and real-time communication based on HTTP**\\n\\nAnother \", \"possibility (usually for different purposes than REST) is a real-time and one-to-many communication \", \"with higher-level frameworks such as [ASP.NET SignalR](https://www.asp.net/signalr) and protocols su\", \"ch as [WebSockets.](https://en.wikipedia.org/wiki/WebSocket)\\n\\nAs Figure 4-17 shows, real-time HTTP c\", \"ommunication means that you can have server code pushing content to connected clients as the data be\", \"comes available, rather than having the server wait for a client to request new data.\\n\\n![](_page_68_\", \"Figure_2.jpeg)\\n\\n*Figure 4-17. One-to-many real-time asynchronous message communication*\\n\\nSignalR is \", \"a good way to achieve real-time communication for pushing content to the clients from a back-end ser\", \"ver. Since communication is in real time, client apps show the changes almost instantly. This is usu\", \"ally handled by a protocol such as WebSockets, using many WebSockets connections (one per client). A\", \" typical example is when a service communicates a change in the score of a sports game to many clien\", \"t web apps simultaneously.\\n\\n## <span id=\\\"page-68-0\\\"></span>Asynchronous message-based communication\\n\", \"\\nAsynchronous messaging and event-driven communication are critical when propagating changes across \", \"multiple microservices and their related domain models. As mentioned earlier in the discussion micro\", \"services and Bounded Contexts (BCs), models (User, Customer, Product, Account, etc.) can mean differ\", \"ent things to different microservices or BCs. That means that when changes occur, you need some way \", \"to reconcile changes across the different models. A solution is eventual consistency and event-drive\", \"n communication based on asynchronous messaging.\\n\\nWhen using messaging, processes communicate by exc\", \"hanging messages asynchronously. A client makes a command or a request to a service by sending it a \", \"message. If the service needs to reply, it sends a different message back to the client. Since it's \", \"a message-based communication, the client assumes that the reply won't be received immediately, and \", \"that there might be no response at all.\\n\\nA message is composed by a header (metadata such as identif\", \"ication or security information) and a body. Messages are usually sent through asynchronous protocol\", \"s like AMQP.\\n\\nThe preferred infrastructure for this type of communication in the microservices commu\", \"nity is a lightweight message broker, which is different than the large brokers and orchestrators us\", \"ed in SOA. In a lightweight message broker, the infrastructure is typically \\\"dumb,\\\" acting only as a\", \" message broker, with simple implementations such as RabbitMQ or a scalable service bus in the cloud\", \" like\\n\\nAzure Service Bus. In this scenario, most of the \\\"smart\\\" thinking still lives in the endpoint\", \"s that are producing and consuming messages-that is, in the microservices.\\n\\nAnother rule you should \", \"try to follow, as much as possible, is to use only asynchronous messaging between the internal servi\", \"ces, and to use synchronous communication (such as HTTP) only from the client apps to the front-end \", \"services (API Gateways plus the first level of microservices).\\n\\nThere are two kinds of asynchronous \", \"messaging communication: single receiver message-based communication, and multiple receivers message\", \"-based communication. The following sections provide details about them.\\n\\n## <span id=\\\"page-69-0\\\"></\", \"span>**Single-receiver message-based communication**\\n\\nMessage-based asynchronous communication with \", \"a single receiver means there's point-to-point communication that delivers a message to exactly one \", \"of the consumers that's reading from the channel, and that the message is processed just once. Howev\", \"er, there are special situations. For instance, in a cloud system that tries to automatically recove\", \"r from failures, the same message could be sent multiple times. Due to network or other failures, th\", \"e client has to be able to retry sending messages, and the server has to implement an operation to b\", \"e idempotent in order to process a particular message just once.\\n\\nSingle-receiver message-based comm\", \"unication is especially well suited for sending asynchronous commands from one microservice to anoth\", \"er as shown in Figure 4-18 that illustrates this approach.\\n\\nOnce you start sending message-based com\", \"munication (either with commands or events), you should avoid mixing message-based communication wit\", \"h synchronous HTTP communication.\\n\\n![](_page_69_Figure_9.jpeg)\\n\\n*Figure 4-18. A single microservice \", \"receiving an asynchronous message*\\n\\nWhen the commands come from client applications, they can be imp\", \"lemented as HTTP synchronous commands. Use message-based commands when you need higher scalability o\", \"r when you're already in a message-based business process.\\n\\n## <span id=\\\"page-70-0\\\"></span>**Multipl\", \"e-receivers message-based communication**\\n\\nAs a more flexible approach, you might also want to use a\", \" publish/subscribe mechanism so that your communication from the sender will be available to additio\", \"nal subscriber microservices or to external applications. Thus, it helps you to follow the [open/clo\", \"sed principle](https://en.wikipedia.org/wiki/Open/closed_principle) in the sending service. That way\", \", additional subscribers can be added in the future without the need to modify the sender service.\\n\\n\", \"When you use a publish/subscribe communication, you might be using an event bus interface to publish\", \" events to any subscriber.\\n\\n## <span id=\\\"page-70-1\\\"></span>**Asynchronous event-driven communication\", \"**\\n\\nWhen using asynchronous event-driven communication, a microservice publishes an integration even\", \"t when something happens within its domain and another microservice needs to be aware of it, like a \", \"price change in a product catalog microservice. Additional microservices subscribe to the events so \", \"they can receive them asynchronously. When that happens, the receivers might update their own domain\", \" entities, which can cause more integration events to be published. This publish/subscribe system is\", \" performed by using an implementation of an event bus. The event bus can be designed as an abstracti\", \"on or interface, with the API that's needed to subscribe or unsubscribe to events and to publish eve\", \"nts. The event bus can also have one or more implementations based on any inter-process and messagin\", \"g broker, like a messaging queue or service bus that supports asynchronous communication and a publi\", \"sh/subscribe model.\\n\\nIf a system uses eventual consistency driven by integration events, it's recomm\", \"ended that this approach is made clear to the end user. The system shouldn't use an approach that mi\", \"mics integration events, like SignalR or polling systems from the client. The end user and the busin\", \"ess owner have to explicitly embrace eventual consistency in the system and realize that in many cas\", \"es the business doesn't have any problem with this approach, as long as it's explicit. This approach\", \" is important because users might expect to see some results immediately and this aspect might not h\", \"appen with eventual consistency.\\n\\nAs noted earlier in the Challenges and solutions for distributed d\", \"ata management section, you can use integration events to implement business tasks that span multipl\", \"e microservices. Thus, you'll have eventual consistency between those services. An eventually consis\", \"tent transaction is made up of a collection of distributed actions. At each action, the related micr\", \"oservice updates a domain entity and publishes another integration event that raises the next action\", \" within the same end-to-end business task.\\n\\nAn important point is that you might want to communicate\", \" to multiple microservices that are subscribed to the same event. To do so, you can use publish/subs\", \"cribe messaging based on eventdriven communication, as shown in Figure 4-19. This publish/subscribe \", \"mechanism isn't exclusive to the microservice architecture. It's similar to the way [Bounded Context\", \"s](https://martinfowler.com/bliki/BoundedContext.html) in DDD should communicate, or to the way you \", \"propagate updates from the write database to the read database in the [Command](https://martinfowler\", \".com/bliki/CQRS.html)  [and Query Responsibility Segregation \\\\(CQRS\\\\)](https://martinfowler.com/blik\", \"i/CQRS.html) architecture pattern. The goal is to have eventual consistency between multiple data so\", \"urces across your distributed system.\\n\\n![](_page_71_Figure_3.jpeg)\\n\\n*Figure 4-19. Asynchronous event\", \"-driven message communication*\\n\\nIn asynchronous event-driven communication, one microservice publish\", \"es events to an event bus and many microservices can subscribe to it, to get notified and act on it.\", \" Your implementation will determine what protocol to use for event-driven, message-based communicati\", \"ons. [AMQP](https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol) can help achieve reliab\", \"le queued communication.\\n\\nWhen you use an event bus, you might want to use an abstraction level (lik\", \"e an event bus interface) based on a related implementation in classes with code using the API from \", \"a message broker like [RabbitMQ](https://www.rabbitmq.com/) or a service bus like [Azure Service Bus\", \" with Topics.](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-dotnet-how-to-use-\", \"topics-subscriptions) Alternatively, you might want to use a higher-level service bus like [NService\", \"Bus,](https://particular.net/nservicebus) [MassTransit,](https://masstransit.io/) or [Brighter](http\", \"s://www.goparamore.io/) to articulate your event bus and publish/subscribe system.\\n\\n## <span id=\\\"pag\", \"e-71-0\\\"></span>**A note about messaging technologies for production systems**\\n\\nThe messaging technol\", \"ogies available for implementing your abstract event bus are at different levels. For instance, prod\", \"ucts like RabbitMQ (a messaging broker transport) and Azure Service Bus sit at a lower level than ot\", \"her products like [NServiceBus,](https://particular.net/nservicebus) [MassTransit,](https://masstran\", \"sit.io/) or [Brighter,](https://www.goparamore.io/) which can work on top of RabbitMQ and Azure Serv\", \"ice Bus. Your choice depends on how many rich features at the application level and out-of-the-box s\", \"calability you need for your application. For implementing just a proof-ofconcept event bus for your\", \" development environment, as it was done in the eShopOnContainers sample, a simple implementation on\", \" top of RabbitMQ running on a Docker container might be enough.\\n\\nHowever, for mission-critical and p\", \"roduction systems that need hyper-scalability, you might want to evaluate Azure Service Bus. For hig\", \"h-level abstractions and features that make the development of distributed applications easier, we r\", \"ecommend that you evaluate other commercial and open-source service buses, such as [NServiceBus,](ht\", \"tps://particular.net/nservicebus) [MassTransit,](https://masstransit.io/) and [Brighter.](https://ww\", \"w.goparamore.io/) Of course, you can build your own\\n\\nservice-bus features on top of lower-level tech\", \"nologies like RabbitMQ and Docker. But that plumbing work might cost too much for a custom enterpris\", \"e application.\\n\\n## <span id=\\\"page-72-0\\\"></span>**Resiliently publishing to the event bus**\\n\\nA challe\", \"nge when implementing an event-driven architecture across multiple microservices is how to atomicall\", \"y update state in the original microservice while resiliently publishing its related integration eve\", \"nt into the event bus, somehow based on transactions. The following are a few ways to accomplish thi\", \"s functionality, although there could be additional approaches as well.\\n\\n- Using a transactional (DT\", \"C-based) queue like MSMQ. (However, this is a legacy approach.)\\n- Using transaction log mining.\\n- Us\", \"ing full [Event Sourcing](https://docs.microsoft.com/azure/architecture/patterns/event-sourcing) pat\", \"tern.\\n- Using the [Outbox pattern:](https://www.kamilgrzybek.com/design/the-outbox-pattern/) a trans\", \"actional database table as a message queue that will be the base for an event-creator component that\", \" would create the event and publish it.\\n\\nFor a more complete description of the challenges in this s\", \"pace, including how messages with potentially incorrect data can end up being published, see [Data p\", \"latform for mission-critical](https://docs.microsoft.com/azure/architecture/reference-architectures/\", \"containers/aks-mission-critical/mission-critical-data-platform#every-message-must-be-processed)  [wo\", \"rkloads on Azure: Every message must be processed.](https://docs.microsoft.com/azure/architecture/re\", \"ference-architectures/containers/aks-mission-critical/mission-critical-data-platform#every-message-m\", \"ust-be-processed)\\n\\nAdditional topics to consider when using asynchronous communication are message i\", \"dempotence and message deduplication. These topics are covered in the section Implementing event-bas\", \"ed communication between microservices (integration events) later in this guide.\\n\\n### <span id=\\\"page\", \"-72-1\\\"></span>**Additional resources**\\n\\n- **Event Driven Messaging** [https://patterns.arcitura.com/\", \"soa-patterns/design\\\\\\\\_patterns/event\\\\\\\\_driven\\\\\\\\_messaging](https://patterns.arcitura.com/soa-pattern\", \"s/design_patterns/event_driven_messaging)\\n- **Publish/Subscribe Channel** [https://www.enterpriseint\", \"egrationpatterns.com/patterns/messaging/PublishSubscribeChannel.](https://www.enterpriseintegrationp\", \"atterns.com/patterns/messaging/PublishSubscribeChannel.html) [html](https://www.enterpriseintegratio\", \"npatterns.com/patterns/messaging/PublishSubscribeChannel.html)\\n- **Udi Dahan. Clarified CQRS** <http\", \"s://udidahan.com/2009/12/09/clarified-cqrs/>\\n- **Command and Query Responsibility Segregation (CQRS)\", \"** [https://learn.microsoft.com/azure/architecture/patterns/cqrs](https://docs.microsoft.com/azure/a\", \"rchitecture/patterns/cqrs)\\n- **Communicating Between Bounded Contexts** [https://learn.microsoft.com\", \"/previous-versions/msp-n-p/jj591572\\\\(v=pandp.10\\\\)](https://docs.microsoft.com/previous-versions/msp-\", \"n-p/jj591572(v=pandp.10))\\n- **Eventual consistency** [https://en.wikipedia.org/wiki/Eventual\\\\\\\\_consi\", \"stency](https://en.wikipedia.org/wiki/Eventual_consistency)\\n- **Jimmy Bogard. Refactoring Towards Re\", \"silience: Evaluating Coupling** <https://jimmybogard.com/refactoring-towards-resilience-evaluating-c\", \"oupling/>\\n\\n## <span id=\\\"page-73-0\\\"></span>Creating, evolving, and versioning microservice APIs and c\", \"ontracts\\n\\nA microservice API is a contract between the service and its clients. You'll be able to ev\", \"olve a microservice independently only if you do not break its API contract, which is why the contra\", \"ct is so important. If you change the contract, it will impact your client applications or your API \", \"Gateway.\\n\\nThe nature of the API definition depends on which protocol you're using. For instance, if \", \"you're using messaging, like AMQP, the API consists of the message types. If you're using HTTP and R\", \"ESTful services, the API consists of the URLs and the request and response JSON formats.\\n\\nHowever, e\", \"ven if you're thoughtful about your initial contract, a service API will need to change over time. W\", \"hen that happens\\u2014and especially if your API is a public API consumed by multiple client applications\", \" \\u2014 you typically can't force all clients to upgrade to your new API contract. You usually need to in\", \"crementally deploy new versions of a service in a way that both old and new versions of a service co\", \"ntract are running simultaneously. Therefore, it's important to have a strategy for your service ver\", \"sioning.\\n\\nWhen the API changes are small, like if you add attributes or parameters to your API, clie\", \"nts that use an older API should switch and work with the new version of the service. You might be a\", \"ble to provide default values for any missing attributes that are required, and the clients might be\", \" able to ignore any extra response attributes.\\n\\nHowever, sometimes you need to make major and incomp\", \"atible changes to a service API. Because you might not be able to force client applications or servi\", \"ces to upgrade immediately to the new version, a service must support older versions of the API for \", \"some period. If you're using an HTTPbased mechanism such as REST, one approach is to embed the API v\", \"ersion number in the URL or into an HTTP header. Then you can decide between implementing both versi\", \"ons of the service simultaneously within the same service instance, or deploying different instances\", \" that each handle a version of the API. A good approach for this functionality is the [Mediator patt\", \"ern](https://en.wikipedia.org/wiki/Mediator_pattern) (for example, [MediatR library\\\\)](https://githu\", \"b.com/jbogard/MediatR) to decouple the different implementation versions into independent handlers.\\n\", \"\\nFinally, if you're using a REST architecture, [Hypermedia](https://www.infoq.com/articles/mark-bake\", \"r-hypermedia) is the best solution for versioning your services and allowing evolvable APIs.\\n\\n### <s\", \"pan id=\\\"page-73-1\\\"></span>**Additional resources**\\n\\n- **Scott Hanselman. ASP.NET Core RESTful Web AP\", \"I versioning made easy** <https://www.hanselman.com/blog/ASPNETCoreRESTfulWebAPIVersioningMadeEasy.a\", \"spx>\\n- **Versioning a RESTful web API** [https://learn.microsoft.com/azure/architecture/best-practic\", \"es/api-design#versioning-a](https://docs.microsoft.com/azure/architecture/best-practices/api-design#\", \"versioning-a-restful-web-api)[restful-web-api](https://docs.microsoft.com/azure/architecture/best-pr\", \"actices/api-design#versioning-a-restful-web-api)\\n- **Roy Fielding. Versioning, Hypermedia, and REST*\", \"* <https://www.infoq.com/articles/roy-fielding-on-versioning>\\n\\n## <span id=\\\"page-74-0\\\"></span>Micros\", \"ervices addressability and the service registry\\n\\nEach microservice has a unique name (URL) that's us\", \"ed to resolve its location. Your microservice needs to be addressable wherever it's running. If you \", \"have to think about which computer is running a particular microservice, things can go bad quickly. \", \"In the same way that DNS resolves a URL to a particular computer, your microservice needs to have a \", \"unique name so that its current location is discoverable. Microservices need addressable names that \", \"make them independent from the infrastructure that they're running on. This approach implies that th\", \"ere's an interaction between how your service is deployed and how it's discovered, because there nee\", \"ds to be a [service registry.](https://microservices.io/patterns/service-registry.html) In the same \", \"vein, when a computer fails, the registry service must be able to indicate where the service is now \", \"running.\\n\\nThe [service registry pattern](https://microservices.io/patterns/service-registry.html) is\", \" a key part of service discovery. The registry is a database containing the network locations of ser\", \"vice instances. A service registry needs to be highly available and up-to-date. Clients could cache \", \"network locations obtained from the service registry. However, that information eventually goes out \", \"of date and clients can no longer discover service instances. So, a service registry consists of a c\", \"luster of servers that use a replication protocol to maintain consistency.\\n\\nIn some microservice dep\", \"loyment environments (called clusters, to be covered in a later section), service discovery is built\", \" in. For example, an Azure Kubernetes Service (AKS) environment can handle service instance registra\", \"tion and deregistration. It also runs a proxy on each cluster host that plays the role of server-sid\", \"e discovery router.\\n\\n### <span id=\\\"page-74-1\\\"></span>**Additional resources**\\n\\n- **Chris Richardson.\", \" Pattern: Service registry** <https://microservices.io/patterns/service-registry.html>\\n- **Auth0. Th\", \"e Service Registry** <https://auth0.com/blog/an-introduction-to-microservices-part-3-the-service-reg\", \"istry/>\\n- **Gabriel Schenker. Service discovery** <https://lostechies.com/gabrielschenker/2016/01/27\", \"/service-discovery/>\\n\\n## <span id=\\\"page-74-2\\\"></span>Creating composite UI based on microservices\\n\\nM\", \"icroservices architecture often starts with the server-side handling data and logic, but, in many ca\", \"ses, the UI is still handled as a monolith. However, a more advanced approach, called [micro](https:\", \"//martinfowler.com/articles/micro-frontends.html)  [frontends,](https://martinfowler.com/articles/mi\", \"cro-frontends.html) is to design your application UI based on microservices as well. That means havi\", \"ng a composite UI produced by the microservices, instead of having microservices on the server and j\", \"ust a monolithic client app consuming the microservices. With this approach, the microservices you b\", \"uild can be complete with both logic and visual representation.\\n\\nFigure 4-20 shows the simpler appro\", \"ach of just consuming microservices from a monolithic client application. Of course, you could have \", \"an ASP.NET MVC service in between producing the HTML and JavaScript. The figure is a simplification \", \"that highlights that you have a single (monolithic) client UI\\n\\nconsuming the microservices, which ju\", \"st focus on logic and data and not on the UI shape (HTML and JavaScript).\\n\\n![](_page_75_Figure_2.jpe\", \"g)\\n\\n*Figure 4-20. A monolithic UI application consuming back-end microservices*\\n\\nIn contrast, a comp\", \"osite UI is precisely generated and composed by the microservices themselves. Some of the microservi\", \"ces drive the visual shape of specific areas of the UI. The key difference is that you have client U\", \"I components (TypeScript classes, for example) based on templates, and the datashaping-UI ViewModel \", \"for those templates comes from each microservice.\\n\\nAt client application start-up time, each of the \", \"client UI components (TypeScript classes, for example) registers itself with an infrastructure micro\", \"service capable of providing ViewModels for a given scenario. If the microservice changes the shape,\", \" the UI changes also.\\n\\nFigure 4-21 shows a version of this composite UI approach. This approach is s\", \"implified because you might have other microservices that are aggregating granular parts that are ba\", \"sed on different techniques. It depends on whether you're building a traditional web approach (ASP.N\", \"ET MVC) or an SPA (Single Page Application).\\n\\n![](_page_76_Figure_1.jpeg)\\n\\nEach of those UI composit\", \"ion microservices would be similar to a small API Gateway. But in this case, each one is responsible\", \" for a small UI area.\\n\\nA composite UI approach that's driven by microservices can be more challengin\", \"g or less so, depending on what UI technologies you're using. For instance, you won't use the same t\", \"echniques for building a traditional web application that you use for building an SPA or for native \", \"mobile app (as when developing Xamarin apps, which can be more challenging for this approach).\\n\\nThe \", \"[eShopOnContainers](https://aka.ms/MicroservicesArchitecture) sample application uses the monolithic\", \" UI approach for multiple reasons. First, it's an introduction to microservices and containers. A co\", \"mposite UI is more advanced but also requires further complexity when designing and developing the U\", \"I. Second, eShopOnContainers also provides a native mobile app based on Xamarin, which would make it\", \" more complex on the client C# side.\\n\\nHowever, we encourage you to use the following references to l\", \"earn more about composite UI based on microservices.\\n\\n### <span id=\\\"page-76-0\\\"></span>**Additional r\", \"esources**\\n\\n- **Micro Frontends (Martin Fowler's blog)** <https://martinfowler.com/articles/micro-fr\", \"ontends.html>\\n- **Micro Frontends (Michael Geers site)** <https://micro-frontends.org/>\\n- **Composit\", \"e UI using ASP.NET (Particular's Workshop)** <https://github.com/Particular/Workshop/tree/master/dem\", \"os/asp-net-core>\\n- **Ruben Oostinga. The Monolithic Frontend in the Microservices Architecture** <ht\", \"tps://xebia.com/blog/the-monolithic-frontend-in-the-microservices-architecture/>\\n\\n- **Mauro Servient\", \"i. The secret of better UI composition** <https://particular.net/blog/secret-of-better-ui-compositio\", \"n>\\n- **Viktor Farcic. Including Front-End Web Components Into Microservices** [https://technologycon\", \"versations.com/2015/08/09/including-front-end-web-components](https://technologyconversations.com/20\", \"15/08/09/including-front-end-web-components-into-microservices/)[into-microservices/](https://techno\", \"logyconversations.com/2015/08/09/including-front-end-web-components-into-microservices/)\\n- **Managin\", \"g Frontend in the Microservices Architecture** <https://allegro.tech/2016/03/Managing-Frontend-in-th\", \"e-microservices-architecture.html>\\n\\n## <span id=\\\"page-77-0\\\"></span>Resiliency and high availability \", \"in microservices\\n\\nDealing with unexpected failures is one of the hardest problems to solve, especial\", \"ly in a distributed system. Much of the code that developers write involves handling exceptions, and\", \" this is also where the most time is spent in testing. The problem is more involved than writing cod\", \"e to handle failures. What happens when the machine where the microservice is running fails? Not onl\", \"y do you need to detect this microservice failure (a hard problem on its own), but you also need som\", \"ething to restart your microservice.\\n\\nA microservice needs to be resilient to failures and to be abl\", \"e to restart often on another machine for availability. This resiliency also comes down to the state\", \" that was saved on behalf of the microservice, where the microservice can recover this state from, a\", \"nd whether the microservice can restart successfully. In other words, there needs to be resiliency i\", \"n the compute capability (the process can restart at any time) as well as resilience in the state or\", \" data (no data loss, and the data remains consistent).\\n\\nThe problems of resiliency are compounded du\", \"ring other scenarios, such as when failures occur during an application upgrade. The microservice, w\", \"orking with the deployment system, needs to determine whether it can continue to move forward to the\", \" newer version or instead roll back to a previous version to maintain a consistent state. Questions \", \"such as whether enough machines are available to keep moving forward and how to recover previous ver\", \"sions of the microservice need to be considered. This approach requires the microservice to emit hea\", \"lth information so that the overall application and orchestrator can make these decisions.\\n\\nIn addit\", \"ion, resiliency is related to how cloud-based systems must behave. As mentioned, a cloudbased system\", \" must embrace failures and must try to automatically recover from them. For instance, in case of net\", \"work or container failures, client apps or client services must have a strategy to retry sending mes\", \"sages or to retry requests, since in many cases failures in the cloud are partial. The [Implementing\", \" Resilient Applications](#page-305-0) section in this guide addresses how to handle partial failure.\", \" It describes techniques like retries with exponential backoff or the Circuit Breaker pattern in .NE\", \"T by using libraries like [Polly,](https://github.com/App-vNext/Polly) which offers a large variety \", \"of policies to handle this subject.\\n\\n## <span id=\\\"page-77-1\\\"></span>**Health management and diagnost\", \"ics in microservices**\\n\\nIt may seem obvious, and it's often overlooked, but a microservice must repo\", \"rt its health and diagnostics. Otherwise, there's little insight from an operations perspective. Cor\", \"relating diagnostic events across a set of independent services and dealing with machine clock skews\", \" to make sense of the event order is challenging. In the same way that you interact with a microserv\", \"ice over agreedupon protocols and data formats, there's a need for standardization in how to log hea\", \"lth and diagnostic events that ultimately end up in an event store for querying and viewing. In a mi\", \"croservices approach, it's key that different teams agree on a single logging format. There needs to\", \" be a consistent approach to viewing diagnostic events in the application.\\n\\n## **Health checks**\\n\\nHe\", \"alth is different from diagnostics. Health is about the microservice reporting its current state to \", \"take appropriate actions. A good example is working with upgrade and deployment mechanisms to mainta\", \"in availability. Although a service might currently be unhealthy due to a process crash or machine r\", \"eboot, the service might still be operational. The last thing you need is to make this worse by perf\", \"orming an upgrade. The best approach is to do an investigation first or allow time for the microserv\", \"ice to recover. Health events from a microservice help us make informed decisions and, in effect, he\", \"lp create self-healing services.\\n\\nIn the Implementing health checks in ASP.NET Core services section\", \" of this guide, we explain how to use a new ASP.NET HealthChecks library in your microservices so th\", \"ey can report their state to a monitoring service to take appropriate actions.\\n\\nYou also have the op\", \"tion of using an excellent open-source library called AspNetCore.Diagnostics.HealthChecks, available\", \" on [GitHub](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks) and as [a NuGet package\", \".](https://www.nuget.org/packages/Microsoft.AspNetCore.Diagnostics.HealthChecks/) This library also \", \"does health checks, with a twist, it handles two types of checks:\\n\\n- **Liveness**: Checks if the mic\", \"roservice is alive, that is, if it's able to accept requests and respond.\\n- **Readiness**: Checks if\", \" the microservice's dependencies (Database, queue services, etc.) are themselves ready, so the micro\", \"service can do what it's supposed to do.\\n\\n### **Using diagnostics and logs event streams**\\n\\nLogs pro\", \"vide information about how an application or service is running, including exceptions, warnings, and\", \" simple informational messages. Usually, each log is in a text format with one line per event, altho\", \"ugh exceptions also often show the stack trace across multiple lines.\\n\\nIn monolithic server-based ap\", \"plications, you can write logs to a file on disk (a logfile) and then analyze it with any tool. Sinc\", \"e application execution is limited to a fixed server or VM, it generally isn't too complex to analyz\", \"e the flow of events. However, in a distributed application where multiple services are executed acr\", \"oss many nodes in an orchestrator cluster, being able to correlate distributed events is a challenge\", \".\\n\\nA microservice-based application should not try to store the output stream of events or logfiles \", \"by itself, and not even try to manage the routing of the events to a central place. It should be tra\", \"nsparent, meaning that each process should just write its event stream to a standard output that und\", \"erneath will be collected by the execution environment infrastructure where it's running. An example\", \" of these event stream routers is [Microsoft.Diagnostic.EventFlow,](https://github.com/Azure/diagnos\", \"tics-eventflow) which collects event streams from multiple sources and publishes it to output system\", \"s. These can include simple standard output for a development environment or cloud systems like [Azu\", \"re Monitor](https://azure.microsoft.com/services/monitor/) and [Azure Diagnostics.](https://docs.mic\", \"rosoft.com/azure/azure-monitor/platform/diagnostics-extension-overview) There are also good third-pa\", \"rty log analysis platforms and tools that can search, alert, report, and monitor logs, even in real \", \"time, like [Splunk.](https://www.splunk.com/goto/Splunk_Log_Management?ac=ga_usa_log_analysis_phrase\", \"_Mar17&_kk=logs%20analysis&gclid=CNzkzIrex9MCFYGHfgodW5YOtA)\\n\\n## **Orchestrators managing health and\", \" diagnostics information**\\n\\nWhen you create a microservice-based application, you need to deal with \", \"complexity. Of course, a single microservice is simple to deal with, but dozens or hundreds of types\", \" and thousands of instances of microservices is a complex problem. It isn't just about building your\", \" microservice architecture\\u2014you also need high availability, addressability, resiliency, health, and \", \"diagnostics if you intend to have a stable and cohesive system.\\n\\n![](_page_79_Picture_2.jpeg)\\n\\n*Figu\", \"re 4-22. A Microservice Platform is fundamental for an application's health management*\\n\\nThe complex\", \" problems shown in Figure 4-22 are hard to solve by yourself. Development teams should focus on solv\", \"ing business problems and building custom applications with microservice-based approaches. They shou\", \"ld not focus on solving complex infrastructure problems; if they did, the cost of any microservice-b\", \"ased application would be huge. Therefore, there are microservice-oriented platforms, referred to as\", \" orchestrators or microservice clusters, that try to solve the hard problems of building and running\", \" a service and using infrastructure resources efficiently. This approach reduces the complexities of\", \" building applications that use a microservices approach.\\n\\nDifferent orchestrators might sound simil\", \"ar, but the diagnostics and health checks offered by each of them differ in features and state of ma\", \"turity, sometimes depending on the OS platform, as explained in the next section.\\n\\n### <span id=\\\"pag\", \"e-79-0\\\"></span>**Additional resources**\\n\\n- **The Twelve-Factor App. XI. Logs: Treat logs as event st\", \"reams** <https://12factor.net/logs>\\n- **Microsoft Diagnostic EventFlow Library** GitHub repo. <https\", \"://github.com/Azure/diagnostics-eventflow>\\n- **What is Azure Diagnostics** [https://learn.microsoft.\", \"com/azure/azure-diagnostics](https://docs.microsoft.com/azure/azure-diagnostics)\\n\\n- **Connect Window\", \"s computers to the Azure Monitor service** [https://learn.microsoft.com/azure/azure-monitor/platform\", \"/agent-windows](https://docs.microsoft.com/azure/azure-monitor/platform/agent-windows)\\n- **Logging W\", \"hat You Mean: Using the Semantic Logging Application Block** [https://learn.microsoft.com/previous-v\", \"ersions/msp-n-p/dn440729\\\\(v=pandp.60\\\\)](https://docs.microsoft.com/previous-versions/msp-n-p/dn44072\", \"9(v=pandp.60))\\n- **Splunk** Official site. <https://www.splunk.com/>\\n- **EventSource Class** API for\", \" events tracing for Windows (ETW) [https://learn.microsoft.com/dotnet/api/system.diagnostics.tracing\", \".eventsource](https://docs.microsoft.com/dotnet/api/system.diagnostics.tracing.eventsource)\\n\\n## <spa\", \"n id=\\\"page-80-0\\\"></span>Orchestrate microservices and multi-container applications for high scalabil\", \"ity and availability\\n\\nUsing orchestrators for production-ready applications is essential if your app\", \"lication is based on microservices or simply split across multiple containers. As introduced previou\", \"sly, in a microservicebased approach, each microservice owns its model and data so that it will be a\", \"utonomous from a development and deployment point of view. But even if you have a more traditional a\", \"pplication that's composed of multiple services (like SOA), you'll also have multiple containers or \", \"services comprising a single business application that need to be deployed as a distributed system. \", \"These kinds of systems are complex to scale out and manage; therefore, you absolutely need an orches\", \"trator if you want to have a production-ready and scalable multi-container application.\\n\\nFigure 4-23\", \" illustrates deployment into a cluster of an application composed of multiple microservices (contain\", \"ers).\\n\\n![](_page_81_Figure_1.jpeg)\\n\\n*Figure 4-23. A cluster of containers*\\n\\nYou use one container fo\", \"r each service instance. Docker containers are \\\"units of deployment\\\" and a container is an instance \", \"of a Docker. A host handles many containers. It looks like a logical approach. But how are you handl\", \"ing load-balancing, routing, and orchestrating these composed applications?\\n\\nThe plain Docker Engine\", \" in single Docker hosts meets the needs of managing single image instances on one host, but it falls\", \" short when it comes to managing multiple containers deployed on multiple hosts for more complex dis\", \"tributed applications. In most cases, you need a management platform that will automatically start c\", \"ontainers, scale out containers with multiple instances per image, suspend them or shut them down wh\", \"en needed, and ideally also control how they access resources like the network and data storage.\\n\\nTo\", \" go beyond the management of individual containers or simple composed apps and move toward larger en\", \"terprise applications with microservices, you must turn to orchestration and clustering platforms.\\n\\n\", \"From an architecture and development point of view, if you're building large enterprise composed of \", \"microservices-based applications, it's important to understand the following platforms and products \", \"that support advanced scenarios:\\n\\n**Clusters and orchestrators.** When you need to scale out applica\", \"tions across many Docker hosts, as when a large microservice-based application, it's critical to be \", \"able to manage all those hosts as a single cluster by abstracting the complexity of the underlying p\", \"latform. That's what the container clusters and orchestrators provide. Kubernetes is an example of a\", \"n orchestrator, and is available in Azure through Azure Kubernetes Service.\\n\\n**Schedulers.** *Schedu\", \"ling* means to have the capability for an administrator to launch containers in a cluster so they al\", \"so provide a UI. A cluster scheduler has several responsibilities: to use the cluster's resources ef\", \"ficiently, to set the constraints provided by the user, to efficiently load-balance containers acros\", \"s nodes or hosts, and to be robust against errors while providing high availability.\\n\\nThe concepts o\", \"f a cluster and a scheduler are closely related, so the products provided by different vendors often\", \" provide both sets of capabilities. The following list shows the most important platform and softwar\", \"e choices you have for clusters and schedulers. These orchestrators are generally offered in public \", \"clouds like Azure.\\n\\n## <span id=\\\"page-82-0\\\"></span>**Software platforms for container clustering, or\", \"chestration, and scheduling**\\n\\n| Platform                       | Description                       \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"                         |\\n|--------------------------------|---------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"----------------------------------------------------------------------------------------------------\", \"---------------------|\\n| Kubernetes                     | Kubernetes<br>is an open-source<br>product\", \" that provides functionality<br>that ranges from cluster<br>infrastructure and container<br>scheduli\", \"ng to orchestrating<br>capabilities. It lets you automate<br>deployment, scaling, and<br>operations \", \"of application<br>containers across clusters of hosts.<br>Kubernetes<br>provides a container<br>cent\", \"ric infrastructure that groups<br>application containers into logical<br>units for easy management a\", \"nd<br>discovery. |\\n|                                | Kubernetes<br>is mature in Linux, less<br>matu\", \"re in Windows.                                                                                      \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"             |\\n| Azure Kubernetes Service (AKS) | AKS<br>is a managed Kubernetes<br>container orches\", \"tration service in<br>Azure that simplifies Kubernetes<br>cluster's management, deployment,<br>and o\", \"perations.                                                                                          \", \"                                                                                                    \", \"                                                                                                    \", \"         |\\n| Azure Container Apps           | Azure Container Apps<br>is a managed<br>serverless con\", \"tainer service for<br>building and deploying modern<br>apps at scale.                               \", \"                                                                                                    \", \"                                                                                                    \", \"                                                                                                    \", \"     |\\n\\n## <span id=\\\"page-82-1\\\"></span>**Using container-based orchestrators in Microsoft Azure**\\n\\nS\", \"everal cloud vendors offer Docker containers support plus Docker clusters and orchestration support,\", \" including Microsoft Azure, Amazon EC2 Container Service, and Google Container Engine. Microsoft Azu\", \"re provides Docker cluster and orchestrator support through Azure Kubernetes Service (AKS).\\n\\n## <spa\", \"n id=\\\"page-83-0\\\"></span>**Using Azure Kubernetes Service**\\n\\nA Kubernetes cluster pools multiple Dock\", \"er hosts and exposes them as a single virtual Docker host, so you can deploy multiple containers int\", \"o the cluster and scale-out with any number of container instances. The cluster will handle all the \", \"complex management plumbing, like scalability, health, and so forth.\\n\\nAKS provides a way to simplify\", \" the creation, configuration, and management of a cluster of virtual machines in Azure that are prec\", \"onfigured to run containerized applications. Using an optimized configuration of popular open-source\", \" scheduling and orchestration tools, AKS enables you to use your existing skills or draw on a large \", \"and growing body of community expertise to deploy and manage container-based applications on Microso\", \"ft Azure.\\n\\nAzure Kubernetes Service optimizes the configuration of popular Docker clustering open-so\", \"urce tools and technologies specifically for Azure. You get an open solution that offers portability\", \" for both your containers and your application configuration. You select the size, the number of hos\", \"ts, and the orchestrator tools, and AKS handles everything else.\\n\\n![](_page_83_Figure_4.jpeg)\\n\\n*Figu\", \"re 4-24. Kubernetes cluster's simplified structure and topology*\\n\\nIn figure 4-24, you can see the st\", \"ructure of a Kubernetes cluster where a master node (VM) controls most of the coordination of the cl\", \"uster and you can deploy containers to the rest of the nodes, which are managed as a single pool fro\", \"m an application point of view and allows you to scale to thousands or even tens of thousands of con\", \"tainers.\\n\\n## <span id=\\\"page-84-0\\\"></span>**Development environment for Kubernetes**\\n\\nIn the developm\", \"ent environment, Docker announced in July 2018 that Kubernetes can also run in a single development \", \"machine (Windows 10 or macOS) by installing [Docker Desktop.](https://docs.docker.com/install/) You \", \"can later deploy to the cloud (AKS) for further integration tests, as shown in figure 4-25.\\n\\n![](_pa\", \"ge_84_Figure_3.jpeg)\\n\\n*Figure 4-25. Running Kubernetes in dev machine and the cloud*\\n\\n### <span id=\\\"\", \"page-84-1\\\"></span>**Getting started with Azure Kubernetes Service (AKS)**\\n\\nTo begin using AKS, you d\", \"eploy an AKS cluster from the Azure portal or by using the CLI. For more information on deploying a \", \"Kubernetes cluster in Azure, see [Deploy an Azure Kubernetes Service](https://docs.microsoft.com/azu\", \"re/aks/kubernetes-walkthrough-portal)  [\\\\(AKS\\\\) cluster.](https://docs.microsoft.com/azure/aks/kuber\", \"netes-walkthrough-portal)\\n\\nThere are no fees for any of the software installed by default as part of\", \" AKS. All default options are implemented with open-source software. AKS is available for multiple v\", \"irtual machines in Azure. You're charged only for the compute instances you choose, and the other un\", \"derlying infrastructure resources consumed, such as storage and networking. There are no incremental\", \" charges for AKS itself.\\n\\nThe default production deployment option for Kubernetes is to use Helm cha\", \"rts, which are introduced in the next section.\\n\\n## <span id=\\\"page-85-0\\\"></span>**Deploy with Helm ch\", \"arts into Kubernetes clusters**\\n\\nWhen deploying an application to a Kubernetes cluster, you can use \", \"the original kubectl.exe CLI tool using deployment files based on the native format (.yaml files), a\", \"s already mentioned in the previous section. However, for more complex Kubernetes applications such \", \"as when deploying complex microservice-based applications, it's recommended to use [Helm.](https://h\", \"elm.sh/)\\n\\nHelm Charts helps you define, version, install, share, upgrade, or rollback even the most \", \"complex Kubernetes application.\\n\\nGoing further, Helm usage is also recommended because other Kuberne\", \"tes environments in Azure, such as [Azure Dev Spaces](https://docs.microsoft.com/azure/dev-spaces/az\", \"ure-dev-spaces) are also based on Helm charts.\\n\\nHelm is maintained by the [Cloud Native Computing Fo\", \"undation \\\\(CNCF\\\\)](https://www.cncf.io/) - in collaboration with Microsoft, Google, Bitnami, and the\", \" Helm contributor community.\\n\\nFor more implementation information on Helm charts and Kubernetes, see\", \" the [Using Helm Charts to](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Deploy-to-\", \"Azure-Kubernetes-Service-(AKS))  [deploy eShopOnContainers to AKS](https://github.com/dotnet-archite\", \"cture/eShopOnContainers/wiki/Deploy-to-Azure-Kubernetes-Service-(AKS)) post.\\n\\n### <span id=\\\"page-85-\", \"1\\\"></span>**Additional resources**\\n\\n- **Getting started with Azure Kubernetes Service (AKS)** [https\", \"://learn.microsoft.com/azure/aks/kubernetes-walkthrough-portal](https://docs.microsoft.com/azure/aks\", \"/kubernetes-walkthrough-portal)\\n- **Azure Dev Spaces** [https://learn.microsoft.com/azure/dev-spaces\", \"/azure-dev-spaces](https://docs.microsoft.com/azure/dev-spaces/azure-dev-spaces)\\n- **Kubernetes** Th\", \"e official site. <https://kubernetes.io/>\\n\\n**CHAPTER** 4\\n\\n# <span id=\\\"page-86-0\\\"></span>Development \", \"process for Docker-based applications\\n\\n*Develop containerized .NET applications the way you like, ei\", \"ther Integrated Development Environment (IDE) focused with Visual Studio and Visual Studio tools for\", \" Docker or CLI/Editor focused with Docker CLI and Visual Studio Code.*\\n\\n## <span id=\\\"page-86-1\\\"></sp\", \"an>Development environment for Docker apps\\n\\n## <span id=\\\"page-86-2\\\"></span>**Development tool choice\", \"s: IDE or editor**\\n\\nWhether you prefer a full and powerful IDE or a lightweight and agile editor, Mi\", \"crosoft has tools that you can use for developing Docker applications.\\n\\n**Visual Studio (for Windows\", \").** Docker-based .NET 7 application development with Visual Studio requires Visual Studio 2022 vers\", \"ion 17.0 or later. Visual Studio 2022 comes with tools for Docker already built in. The tools for Do\", \"cker let you develop, run, and validate your applications directly in the target Docker environment.\", \" You can press F5 to run and debug your application (single container or multiple containers) direct\", \"ly into a Docker host, or press CTRL + F5 to edit and refresh your application without having to reb\", \"uild the container. This IDE is the most powerful development choice for Docker-based apps.\\n\\n**Visua\", \"l Studio for Mac.** It's an IDE, evolution of Xamarin Studio, running in macOS. This tool should be \", \"the preferred choice for developers working in macOS machines who also want to use a powerful IDE.\\n\\n\", \"**Visual Studio Code and Docker CLI**. If you prefer a lightweight and cross-platform editor that su\", \"pports any development language, you can use Visual Studio Code and the Docker CLI. This IDE is a cr\", \"oss-platform development approach for macOS, Linux, and Windows. Additionally, Visual Studio Code su\", \"pports extensions for Docker such as IntelliSense for Dockerfiles and shortcut tasks to run Docker c\", \"ommands from the editor.\\n\\nBy installing [Docker Desktop,](https://hub.docker.com/search/?type=editio\", \"n&offering=community) you can use a single Docker CLI to build apps for both Windows and Linux.\\n\\n## \", \"<span id=\\\"page-87-0\\\"></span>**Additional resources**\\n\\n- **Visual Studio**. Official site. [https://v\", \"isualstudio.microsoft.com/vs/](https://visualstudio.microsoft.com/vs/?utm_medium=microsoft&utm_sourc\", \"e=learn.microsoft.com&utm_campaign=inline+link)\\n- **Visual Studio Code**. Official site. <https://co\", \"de.visualstudio.com/download>\\n- **Docker Desktop for Windows** <https://hub.docker.com/editions/comm\", \"unity/docker-ce-desktop-windows>\\n- **Docker Desktop for Mac** <https://hub.docker.com/editions/commu\", \"nity/docker-ce-desktop-mac>\\n\\n## <span id=\\\"page-87-1\\\"></span>.NET languages and frameworks for Docker\", \" containers\\n\\nAs mentioned in earlier sections of this guide, you can use .NET Framework, .NET 7, or \", \"the opensource Mono project when developing Docker containerized .NET applications. You can develop \", \"in C#, F#, or Visual Basic when targeting Linux or Windows Containers, depending on which .NET frame\", \"work is in use. For more details about.NET languages, see the blog post [The .NET Language](https://\", \"devblogs.microsoft.com/dotnet/the-net-language-strategy/)  [Strategy.](https://devblogs.microsoft.co\", \"m/dotnet/the-net-language-strategy/)\\n\\n## <span id=\\\"page-87-2\\\"></span>Development workflow for Docker\", \" apps\\n\\nThe application development life cycle starts at your computer, as a developer, where you cod\", \"e the application using your preferred language and test it locally. With this workflow, no matter w\", \"hich language, framework, and platform you choose, you're always developing and testing Docker conta\", \"iners, but doing so locally.\\n\\nEach container (an instance of a Docker image) includes the following \", \"components:\\n\\n- An operating system selection, for example, a Linux distribution, Windows Nano Server\", \", or Windows Server Core.\\n- Files added during development, for example, source code and application\", \" binaries.\\n- Configuration information, such as environment settings and dependencies.\\n\\n## <span id=\", \"\\\"page-87-3\\\"></span>**Workflow for developing Docker container-based applications**\\n\\nThis section des\", \"cribes the *inner-loop* development workflow for Docker container-based applications. The inner-loop\", \" workflow means it's not considering the broader DevOps workflow, which can include up to production\", \" deployment, and just focuses on the development work done on the developer's computer. The initial \", \"steps to set up the environment aren't included, since those steps are done only once.\\n\\nAn applicati\", \"on is composed of your own services plus additional libraries (dependencies). The following are the \", \"basic steps you usually take when building a Docker application, as illustrated in Figure 5-1.\\n\\n![](\", \"_page_88_Figure_2.jpeg)\\n\\n*Figure 5-1. Step-by-step workflow for developing Docker containerized apps\", \"*\\n\\nIn this section, this whole process is detailed and every major step is explained by focusing on \", \"a Visual Studio environment.\\n\\nWhen you're using an editor/CLI development approach (for example, Vis\", \"ual Studio Code plus Docker CLI on macOS or Windows), you need to know every step, generally in more\", \" detail than if you're using Visual Studio. For more information about working in a CLI environment,\", \" see the e-book [Containerized Docker Application lifecycle with Microsoft Platforms and Tools.](htt\", \"ps://aka.ms/dockerlifecycleebook/)\\n\\nWhen you're using Visual Studio 2022, many of those steps are ha\", \"ndled for you, which dramatically improves your productivity. This is especially true when you're us\", \"ing Visual Studio 2022 and targeting multi-container applications. For instance, with just one mouse\", \" click, Visual Studio adds the Dockerfile and docker-compose.yml file to your projects with the conf\", \"iguration for your application. When you run the application in Visual Studio, it builds the Docker \", \"image and runs the multi-container application directly in Docker; it even allows you to debug sever\", \"al containers at once. These features will boost your development speed.\\n\\nHowever, just because Visu\", \"al Studio makes those steps automatic doesn't mean that you don't need to know what's going on under\", \"neath with Docker. Therefore, the following guidance details every step.\\n\\n![](_page_89_Picture_0.jpe\", \"g)\\n\\n## <span id=\\\"page-89-0\\\"></span>**Step 1. Start coding and create your initial application or ser\", \"vice baseline**\\n\\nDeveloping a Docker application is similar to the way you develop an application wi\", \"thout Docker. The difference is that while developing for Docker, you're deploying and testing your \", \"application or services running within Docker containers in your local environment (either a Linux V\", \"M setup by Docker or directly Windows if using Windows Containers).\\n\\n## **Set up your local environm\", \"ent with Visual Studio**\\n\\nTo begin, make sure you have [Docker Desktop for Windows](https://docs.doc\", \"ker.com/docker-for-windows/) for Windows installed, as explained in the following instructions:\\n\\n###\", \" [Get started with Docker Desktop for Windows](https://docs.docker.com/docker-for-windows/)\\n\\nIn addi\", \"tion, you need Visual Studio 2022 version 17.0, with the **.ASP.NET and web development** workload i\", \"nstalled, as shown in Figure 5-2.\\n\\n![](_page_89_Picture_7.jpeg)\\n\\n*Figure 5-2. Selecting the ASP.NET \", \"and web development workload during Visual Studio 2022 setup*\\n\\nYou can start coding your application\", \" in plain .NET (usually in .NET Core or later if you're planning to use containers) even before enab\", \"ling Docker in your application and deploying and testing in Docker. However, it is recommended that\", \" you start working on Docker as soon as possible, because that will be the real environment and any \", \"issues can be discovered as soon as possible. This is encouraged\\n\\nbecause Visual Studio makes it so \", \"easy to work with Docker that it almost feels transparent\\u2014the best example when debugging multi-cont\", \"ainer applications from Visual Studio.\\n\\n## **Additional resources**\\n\\n- **Get started with Docker Des\", \"ktop for Windows** <https://docs.docker.com/docker-for-windows/>\\n- **Visual Studio 2022** <https://v\", \"isualstudio.microsoft.com/downloads/>\\n\\n![](_page_90_Picture_4.jpeg)\\n\\n## <span id=\\\"page-90-0\\\"></span>\", \"**Step 2. Create a Dockerfile related to an existing .NET base image**\\n\\nYou need a Dockerfile for ea\", \"ch custom image you want to build; you also need a Dockerfile for each container to be deployed, whe\", \"ther you deploy automatically from Visual Studio or manually using the Docker CLI (docker run and do\", \"cker-compose commands). If your application contains a single custom service, you need a single Dock\", \"erfile. If your application contains multiple services (as in a microservices architecture), you nee\", \"d one Dockerfile for each service.\\n\\nThe Dockerfile is placed in the root folder of your application \", \"or service. It contains the commands that tell Docker how to set up and run your application or serv\", \"ice in a container. You can manually create a Dockerfile in code and add it to your project along wi\", \"th your .NET dependencies.\\n\\nWith Visual Studio and its tools for Docker, this task requires only a f\", \"ew mouse clicks. When you create a new project in Visual Studio 2022, there's an option named **Enab\", \"le Docker Support**, as shown in Figure 5-3.\\n\\n![](_page_91_Picture_0.jpeg)\\n\\n*Figure 5-3. Enabling Do\", \"cker Support when creating a new ASP.NET Core project in Visual Studio 2022*\\n\\nYou can also enable Do\", \"cker support on an existing ASP.NET Core web app project by right-clicking the project in **Solution\", \" Explorer** and selecting **Add** > **Docker Support\\u2026**, as shown in Figure 5-4.\\n\\n![](_page_91_Pictu\", \"re_3.jpeg)\\n\\n*Figure 5-4. Enabling Docker support in an existing Visual Studio 2022 project*\\n\\nThis ac\", \"tion adds a *Dockerfile* to the project with the required configuration, and is only available on AS\", \"P.NET Core projects.\\n\\nIn a similar fashion, Visual Studio can also add a docker-compose.yml file for\", \" the whole solution with the option **Add > Container Orchestrator Support\\u2026**. In step 4, we'll expl\", \"ore this option in greater detail.\\n\\n## **Using an existing official .NET Docker image**\\n\\nYou usually\", \" build a custom image for your container on top of a base image you get from an official repository \", \"like the [Docker Hub](https://hub.docker.com/) registry. That is precisely what happens under the co\", \"vers when you enable Docker support in Visual Studio. Your Dockerfile will use an existing dotnet/co\", \"re/aspnet image.\\n\\nEarlier we explained which Docker images and repos you can use, depending on the f\", \"ramework and OS you have chosen. For instance, if you want to use ASP.NET Core (Linux or Windows), t\", \"he image to use is mcr.microsoft.com/dotnet/aspnet:7.0. Therefore, you just need to specify what bas\", \"e Docker image you will use for your container. You do that by adding FROM mcr.microsoft.com/dotnet/\", \"aspnet:7.0 to your Dockerfile. This will be automatically performed by Visual Studio, but if you wer\", \"e to update the version, you update this value.\\n\\nUsing an official .NET image repository from Docker\", \" Hub with a version number ensures that the same language features are available on all machines (in\", \"cluding development, testing, and production).\\n\\nThe following example shows a sample Dockerfile for \", \"an ASP.NET Core container.\\n\\n```\\nFROM mcr.microsoft.com/dotnet/aspnet:7.0\\nARG source\\nWORKDIR /app\\nEXP\", \"OSE 80\\nCOPY ${source:-obj/Docker/publish} .\\nENTRYPOINT [\\\"dotnet\\\", \\\" MySingleContainerWebApp.dll \\\"]\\n`\", \"``\\n\\nIn this case, the image is based on version 7.0 of the official ASP.NET Core Docker image (multi\", \"-arch for Linux and Windows). This is the setting FROM mcr.microsoft.com/dotnet/aspnet:7.0. (For mor\", \"e information about this base image, see the [ASP.NET Core Docker Image](https://hub.docker.com/_/mi\", \"crosoft-dotnet-aspnet/) page.) In the Dockerfile, you also need to instruct Docker to listen on the \", \"TCP port you will use at runtime (in this case, port 80, as configured with the EXPOSE setting).\\n\\nYo\", \"u can specify additional configuration settings in the Dockerfile, depending on the language and fra\", \"mework you're using. For instance, the ENTRYPOINT line with [\\\"dotnet\\\",\\n\\n\\\"MySingleContainerWebApp.dll\", \"\\\"] tells Docker to run a .NET application. If you're using the SDK and the .NET CLI (dotnet CLI) to \", \"build and run the .NET application, this setting would be different. The bottom line is that the ENT\", \"RYPOINT line and other settings will be different depending on the language and platform you choose \", \"for your application.\\n\\n### **Additional resources**\\n\\n\\u2022 **Building Docker Images for ASP.NET Core App\", \"lications** [https://learn.microsoft.com/dotnet/core/docker/building-net-docker-images](https://docs\", \".microsoft.com/aspnet/core/host-and-deploy/docker/building-net-docker-images)\\n\\n- **Build your own im\", \"age**. In the official Docker documentation. <https://docs.docker.com/engine/tutorials/dockerimages/\", \">\\n- **Staying up-to-date with .NET Container Images** <https://devblogs.microsoft.com/dotnet/staying\", \"-up-to-date-with-net-container-images/>\\n- **Using .NET and Docker Together - DockerCon 2018 Update**\", \" [https://devblogs.microsoft.com/dotnet/using-net-and-docker-together-dockercon-2018](https://devblo\", \"gs.microsoft.com/dotnet/using-net-and-docker-together-dockercon-2018-update/) [update/](https://devb\", \"logs.microsoft.com/dotnet/using-net-and-docker-together-dockercon-2018-update/)\\n\\n## **Using multi-ar\", \"ch image repositories**\\n\\nA single repo can contain platform variants, such as a Linux image and a Wi\", \"ndows image. This feature allows vendors like Microsoft (base image creators) to create a single rep\", \"o to cover multiple platforms (that is Linux and Windows). For example, the [.NET](https://hub.docke\", \"r.com/_/microsoft-dotnet/) repository available in the Docker Hub registry provides support for Linu\", \"x and Windows Nano Server by using the same repo name.\\n\\nIf you specify a tag, targeting a platform t\", \"hat is explicit like in the following cases:\\n\\n- mcr.microsoft.com/dotnet/aspnet:7.0-bullseye-slim Ta\", \"rgets: .NET 7 runtime-only on Linux\\n- mcr.microsoft.com/dotnet/aspnet:7.0-nanoserver-ltsc2022 Target\", \"s: .NET 7 runtime-only on Windows Nano Server\\n\\nBut, if you specify the same image name, even with th\", \"e same tag, the multi-arch images (like the aspnet image) will use the Linux or Windows version depe\", \"nding on the Docker host OS you're deploying, as shown in the following example:\\n\\n\\u2022 mcr.microsoft.co\", \"m/dotnet/aspnet:7.0 Multi-arch: .NET 7 runtime-only on Linux or Windows Nano Server depending on the\", \" Docker host OS\\n\\nThis way, when you pull an image from a Windows host, it will pull the Windows vari\", \"ant, and pulling the same image name from a Linux host will pull the Linux variant.\\n\\n### **Multi-sta\", \"ge builds in Dockerfile**\\n\\nThe Dockerfile is similar to a batch script. Similar to what you would do\", \" if you had to set up the machine from the command line.\\n\\nIt starts with a base image that sets up t\", \"he initial context, it's like the startup filesystem, that sits on top of the host OS. It's not an O\", \"S, but you can think of it like \\\"the\\\" OS inside the container.\\n\\nThe execution of every command line \", \"creates a new layer on the filesystem with the changes from the previous one, so that, when combined\", \", produce the resulting filesystem.\\n\\nSince every new layer \\\"rests\\\" on top of the previous one and th\", \"e resulting image size increases with every command, images can get very large if they have to inclu\", \"de, for example, the SDK needed to build and publish an application.\\n\\nThis is where multi-stage buil\", \"ds get into the plot (from Docker 17.05 and higher) to do their magic.\\n\\nThe core idea is that you ca\", \"n separate the Dockerfile execution process in stages, where a stage is an initial image followed by\", \" one or more commands, and the last stage determines the final image size.\\n\\nIn short, multi-stage bu\", \"ilds allow splitting the creation in different \\\"phases\\\" and then assemble the final image taking onl\", \"y the relevant directories from the intermediate stages. The general strategy to use this feature is\", \":\\n\\n- 1. Use a base SDK image (doesn't matter how large), with everything needed to build and publish\", \" the application to a folder and then\\n- 2. Use a base, small, runtime-only image and copy the publis\", \"hing folder from the previous stage to produce a small final image.\\n\\nProbably the best way to unders\", \"tand multi-stage is going through a Dockerfile in detail, line by line, so let's begin with the init\", \"ial Dockerfile created by Visual Studio when adding Docker support to a project and will get into so\", \"me optimizations later.\\n\\nThe initial Dockerfile might look something like this:\\n\\n```\\n1 FROM mcr.micr\", \"osoft.com/dotnet/aspnet:7.0 AS base\\n2 WORKDIR /app\\n3 EXPOSE 80\\n4\\n5 FROM mcr.microsoft.com/dotnet/sdk\", \":7.0 AS build\\n6 WORKDIR /src\\n7 COPY src/Services/Catalog/Catalog.API/Catalog.API.csproj \\u2026\\n8 COPY src\", \"/BuildingBlocks/HealthChecks/src/Microsoft.AspNetCore.HealthChecks \\u2026\\n9 COPY src/BuildingBlocks/Healt\", \"hChecks/src/Microsoft.Extensions.HealthChecks \\u2026\\n10 COPY src/BuildingBlocks/EventBus/IntegrationEvent\", \"LogEF/ \\u2026\\n11 COPY src/BuildingBlocks/EventBus/EventBus/EventBus.csproj \\u2026\\n12 COPY src/BuildingBlocks/E\", \"ventBus/EventBusRabbitMQ/EventBusRabbitMQ.csproj \\u2026\\n13 COPY src/BuildingBlocks/EventBus/EventBusServi\", \"ceBus/EventBusServiceBus.csproj \\u2026\\n14 COPY src/BuildingBlocks/WebHostCustomization/WebHost.Customizat\", \"ion \\u2026\\n15 COPY src/BuildingBlocks/HealthChecks/src/Microsoft.Extensions \\u2026\\n16 COPY src/BuildingBlocks/\", \"HealthChecks/src/Microsoft.Extensions \\u2026\\n17 RUN dotnet restore src/Services/Catalog/Catalog.API/Catal\", \"og.API.csproj\\n18 COPY . .\\n19 WORKDIR /src/src/Services/Catalog/Catalog.API\\n20 RUN dotnet build Catal\", \"og.API.csproj -c Release -o /app\\n21\\n22 FROM build AS publish\\n23 RUN dotnet publish Catalog.API.cspro\", \"j -c Release -o /app\\n24\\n25 FROM base AS final\\n26 WORKDIR /app\\n27 COPY --from=publish /app .\\n28 ENTRY\", \"POINT [\\\"dotnet\\\", \\\"Catalog.API.dll\\\"]\\n```\\n\\nAnd these are the details, line by line:\\n\\n- **Line #1:** Be\", \"gin a stage with a \\\"small\\\" runtime-only base image, call it **base** for reference.\\n- **Line #2:** C\", \"reate the **/app** directory in the image.\\n- **Line #3:** Expose port **80**.\\n\\n- **Line #5:** Begin \", \"a new stage with the \\\"large\\\" image for building/publishing. Call it **build** for reference.\\n- **Lin\", \"e #6:** Create directory **/src** in the image.\\n- **Line #7:** Up to line 16, copy referenced **.csp\", \"roj** project files to be able to restore packages later.\\n- **Line #17:** Restore packages for the *\", \"*Catalog.API** project and the referenced projects.\\n- **Line #18:** Copy **all directory tree for th\", \"e solution** (except the files/directories included in the **.dockerignore** file) to the **/src** d\", \"irectory in the image.\\n- **Line #19:** Change the current folder to the **Catalog.API** project.\\n- *\", \"*Line #20:** Build the project (and other project dependencies) and output to the **/app** directory\", \" in the image.\\n- **Line #22:** Begin a new stage continuing from the build. Call it **publish** for \", \"reference.\\n- **Line #23:** Publish the project (and dependencies) and output to the **/app** directo\", \"ry in the image.\\n- **Line #25:** Begin a new stage continuing from **base** and call it **final**.\\n-\", \" **Line #26:** Change the current directory to **/app**.\\n- **Line #27:** Copy the **/app** directory\", \" from stage **publish** to the current directory.\\n- **Line #28:** Define the command to run when the\", \" container is started.\\n\\nNow let's explore some optimizations to improve the whole process performanc\", \"e that, in the case of eShopOnContainers, means about 22 minutes or more to build the complete solut\", \"ion in Linux containers.\\n\\nYou'll take advantage of Docker's layer cache feature, which is quite simp\", \"le: if the base image and the commands are the same as some previously executed, it can just use the\", \" resulting layer without the need to execute the commands, thus saving some time.\\n\\nSo, let's focus o\", \"n the **build** stage, lines 5-6 are mostly the same, but lines 7-17 are different for every service\", \" from eShopOnContainers, so they have to execute every single time, however if you changed lines 7-1\", \"6 to:\\n\\n### **COPY** . .\\n\\nThen it would be just the same for every service, it would copy the whole s\", \"olution and would create a larger layer but:\\n\\n- 1. The copy process would only be executed the first\", \" time (and when rebuilding if a file is changed) and would use the cache for all other services and\\n\", \"- 2. Since the larger image occurs in an intermediate stage, it doesn't affect the final image size.\", \"\\n\\nThe next significant optimization involves the restore command executed in line 17, which is also \", \"different for every service of eShopOnContainers. If you change that line to just:\\n\\n### **RUN** dotn\", \"et restore\\n\\nIt would restore the packages for the whole solution, but then again, it would do it jus\", \"t once, instead of the 15 times with the current strategy.\\n\\nHowever, dotnet restore only runs if the\", \"re's a single project or solution file in the folder, so achieving this is a bit more complicated an\", \"d the way to solve it, without getting into too many details, is this:\\n\\n- 1. Add the following lines\", \" to **.dockerignore**:\\n  - \\\\*.sln, to ignore all solution files in the main folder tree\\n  - !eShopOn\", \"Containers-ServicesAndWebApps.sln, to include only this solution file.\\n- 2. Include the /ignoreproje\", \"ctextensions:.dcproj argument to dotnet restore, so it also ignores the docker-compose project and o\", \"nly restores the packages for the eShopOnContainers-ServicesAndWebApps solution.\\n\\nFor the final opti\", \"mization, it just happens that line 20 is redundant, as line 23 also builds the application and come\", \"s, in essence, right after line 20, so there goes another time-consuming command.\\n\\nThe resulting fil\", \"e is then:\\n\\n```\\n1 FROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base\\n2 WORKDIR /app\\n3 EXPOSE 80\\n4\\n5 FR\", \"OM mcr.microsoft.com/dotnet/sdk:7.0 AS publish\\n6 WORKDIR /src\\n7 COPY . .\\n8 RUN dotnet restore /ignor\", \"eprojectextensions:.dcproj\\n9 WORKDIR /src/src/Services/Catalog/Catalog.API\\n10 RUN dotnet publish Cat\", \"alog.API.csproj -c Release -o /app\\n11\\n12 FROM base AS final\\n13 WORKDIR /app\\n14 COPY --from=publish /\", \"app .\\n15 ENTRYPOINT [\\\"dotnet\\\", \\\"Catalog.API.dll\\\"]\\n```\\n\\n### **Creating your base image from scratch**\", \"\\n\\nYou can create your own Docker base image from scratch. This scenario is not recommended for someo\", \"ne who is starting with Docker, but if you want to set the specific bits of your own base image, you\", \" can do so.\\n\\n### **Additional resources**\\n\\n\\u2022 **Multi-arch .NET Core images**. <https://github.com/do\", \"tnet/announcements/issues/14> \\u2022 **Create a base image**. Official Docker documentation. <https://doc\", \"s.docker.com/develop/develop-images/baseimages/>\\n\\n![](_page_97_Picture_1.jpeg)\\n\\n## <span id=\\\"page-97\", \"-0\\\"></span>**Step 3. Create your custom Docker images and embed your application or service in them*\", \"*\\n\\nFor each service in your application, you need to create a related image. If your application is \", \"made up of a single service or web application, you just need a single image.\\n\\nNote that the Docker \", \"images are built automatically for you in Visual Studio. The following steps are only needed for the\", \" editor/CLI workflow and explained for clarity about what happens underneath.\\n\\nYou, as a developer, \", \"need to develop and test locally until you push a completed feature or change to your source control\", \" system (for example, to GitHub). This means that you need to create the Docker images and deploy co\", \"ntainers to a local Docker host (Windows or Linux VM) and run, test, and debug against those local c\", \"ontainers.\\n\\nTo create a custom image in your local environment by using Docker CLI and your Dockerfi\", \"le, you can use the docker build command, as in Figure 5-5.\\n\\n*Figure 5-5. Creating a custom Docker i\", \"mage*\\n\\nOptionally, instead of directly running docker build from the project folder, you can first g\", \"enerate a deployable folder with the required .NET libraries and binaries by running dotnet publish,\", \" and then use the docker build command.\\n\\nThis will create a Docker image with the name cesardl/netco\", \"re-webapi-microservice-docker:first. In this case, :first is a tag that represents a specific versio\", \"n. You can repeat this step for each custom image you need to create for your composed Docker applic\", \"ation.\\n\\nWhen an application is made of multiple containers (that is, it is a multi-container applica\", \"tion), you can also use the docker-compose up --build command to build all the related images with a\", \" single command by using the metadata exposed in the related docker-compose.yml files.\\n\\nYou can find\", \" the existing images in your local repository by using the docker images command, as shown in Figure\", \" 5-6.\\n\\n*Figure 5-6. Viewing existing images using the docker images command*\\n\\n## **Creating Docker i\", \"mages with Visual Studio**\\n\\nWhen you use Visual Studio to create a project with Docker support, you \", \"don't explicitly create an image. Instead, the image is created for you when you press F5 (or Ctrl+F\", \"5) to run the dockerized application or service. This step is automatic in Visual Studio and you won\", \"'t see it happen, but it's important that you know what's going on underneath.\\n\\n![](_page_98_Picture\", \"_4.jpeg)\\n\\n## <span id=\\\"page-98-0\\\"></span>**Step 4. Define your services in docker-compose.yml when b\", \"uilding a multi-container Docker application**\\n\\nThe [docker-compose.yml](https://docs.docker.com/com\", \"pose/compose-file/) file lets you define a set of related services to be deployed as a composed appl\", \"ication with deployment commands. It also configures its dependency relations and runtime configurat\", \"ion.\\n\\nTo use a docker-compose.yml file, you need to create the file in your main or root solution fo\", \"lder, with content similar to that in the following example:\\n\\n```\\nversion: '3.4'\\nservices:\\n webmvc:\\n\", \" image: eshop/web\\n environment:\\n - CatalogUrl=http://catalog-api\\n - OrderingUrl=http://ordering-api\\n\", \" ports:\\n - \\\"80:80\\\"\\n depends_on:\\n - catalog-api\\n - ordering-api\\n catalog-api:\\n image: eshop/catalog-a\", \"pi\\n environment:\\n - ConnectionString=Server=sqldata;Port=1433;Database=CatalogDB;\\u2026\\n ports:\\n - \\\"81:80\", \"\\\"\\n depends_on:\\n - sqldata\\n ordering-api:\\n image: eshop/ordering-api\\n```\\n\\n```\\n environment:\\n - Connec\", \"tionString=Server=sqldata;Database=OrderingDb;\\u2026\\n ports:\\n - \\\"82:80\\\"\\n extra_hosts:\\n - \\\"CESARDLBOOKVHD:\", \"10.0.75.1\\\"\\n depends_on:\\n - sqldata\\n sqldata:\\n image: mcr.microsoft.com/mssql/server:latest\\n environm\", \"ent:\\n - SA_PASSWORD=Pass@word\\n - ACCEPT_EULA=Y\\n ports:\\n - \\\"5433:1433\\\"\\n```\\n\\nThis docker-compose.yml f\", \"ile is a simplified and merged version. It contains static configuration data for each container (li\", \"ke the name of the custom image), which is always required, and configuration information that might\", \" depend on the deployment environment, like the connection string. In later sections, you will learn\", \" how to split the docker-compose.yml configuration into multiple dockercompose files and override va\", \"lues depending on the environment and execution type (debug or release).\\n\\nThe docker-compose.yml fil\", \"e example defines four services: the webmvc service (a web application), two microservices (ordering\", \"-api and basket-api), and one data source container, sqldata, based on SQL Server for Linux running \", \"as a container. Each service will be deployed as a container, so a Docker image is required for each\", \".\\n\\nThe docker-compose.yml file specifies not only what containers are being used, but how they are i\", \"ndividually configured. For instance, the webmvc container definition in the .yml file:\\n\\n- Uses a pr\", \"e-built eshop/web:latest image. However, you could also configure the image to be built as part of t\", \"he docker-compose execution with an additional configuration based on a build: section in the docker\", \"-compose file.\\n- Initializes two environment variables (CatalogUrl and OrderingUrl).\\n- Forwards the \", \"exposed port 80 on the container to the external port 80 on the host machine.\\n- Links the web app to\", \" the catalog and ordering service with the depends\\\\_on setting. This causes the service to wait unti\", \"l those services are started.\\n\\nWe will revisit the docker-compose.yml file in a later section when w\", \"e cover how to implement microservices and multi-container apps.\\n\\n### **Working with docker-compose.\", \"yml in Visual Studio 2022**\\n\\nBesides adding a Dockerfile to a project, as we mentioned before, Visua\", \"l Studio 2017 (from version 15.8 on) can add orchestrator support for Docker Compose to a solution.\\n\", \"\\nWhen you add container orchestrator support, as shown in Figure 5-7, for the first time, Visual Stu\", \"dio creates the Dockerfile for the project and creates a new (service section) project in your solut\", \"ion with\\n\\nseveral global docker-compose\\\\*.yml files, and then adds the project to those files. You c\", \"an then open the docker-compose.yml files and update them with additional features.\\n\\nRepeat this ope\", \"ration for every project you want to include in the docker-compose.yml file.\\n\\nAt the time of this wr\", \"iting, Visual Studio supports **Docker Compose** orchestrators.\\n\\n![](_page_100_Picture_3.jpeg)\\n\\n*Fig\", \"ure 5-7. Adding Docker support in Visual Studio 2022 by right-clicking an ASP.NET Core project*\\n\\nAft\", \"er you add orchestrator support to your solution in Visual Studio, you will also see a new node (in \", \"the docker-compose.dcproj project file) in Solution Explorer that contains the added dockercompose.y\", \"ml files, as shown in Figure 5-8.\\n\\n![](_page_100_Picture_6.jpeg)\\n\\n*Figure 5-8. The docker-compose tr\", \"ee node added in Visual Studio 2022 Solution Explorer*\\n\\nYou could deploy a multi-container applicati\", \"on with a single docker-compose.yml file by using the docker-compose up command. However, Visual Stu\", \"dio adds a group of them so you can override values depending on the environment (development or pro\", \"duction) and execution type (release or debug). This capability will be explained in later sections.\", \"\\n\\n![](_page_101_Picture_0.jpeg)\\n\\n## <span id=\\\"page-101-0\\\"></span>**Step 5. Build and run your Docker\", \" application**\\n\\nIf your application only has a single container, you can run it by deploying it to y\", \"our Docker host (VM or physical server). However, if your application contains multiple services, yo\", \"u can deploy it as a composed application, either using a single CLI command (docker-compose up), or\", \" with Visual Studio, which will use that command under the covers. Let's look at the different optio\", \"ns.\\n\\n## **Option A: Running a single-container application**\\n\\n## **Using Docker CLI**\\n\\nYou can run a\", \" Docker container using the docker run command, as shown in Figure 5-9:\\n\\ndocker run -t -d -p 80:5000\", \" cesardl/netcore-webapi-microservice-docker:first\\n\\nThe above command will create a new container ins\", \"tance from the specified image, every time it's run. You can use the --name parameter to give a name\", \" to the container and then use docker start {name} (or use the container ID or automatic name) to ru\", \"n an existing container instance.\\n\\n*Figure 5-9. Running a Docker container using the docker run comm\", \"and*\\n\\nIn this case, the command binds the internal port 5000 of the container to port 80 of the host\", \" machine. This means that the host is listening on port 80 and forwarding to port 5000 on the contai\", \"ner.\\n\\nThe hash shown is the container ID and it's also assigned a random readable name if the --name\", \" option is not used.\\n\\n### **Using Visual Studio**\\n\\nIf you haven't added container orchestrator suppo\", \"rt, you can also run a single container app in Visual Studio by pressing Ctrl+F5 and you can also us\", \"e F5 to debug the application within the container. The container runs locally using docker run.\\n\\n##\", \"# **Option B: Running a multi-container application**\\n\\nIn most enterprise scenarios, a Docker applic\", \"ation will be composed of multiple services, which means you need to run a multi-container applicati\", \"on, as shown in Figure 5-10.\\n\\n![](_page_102_Picture_0.jpeg)\\n\\n*Figure 5-10. VM with Docker containers\", \" deployed*\\n\\n### **Using Docker CLI**\\n\\nTo run a multi-container application with the Docker CLI, you \", \"use the docker-compose up command. This command uses the **docker-compose.yml** file that you have a\", \"t the solution level to deploy a multi-container application. Figure 5-11 shows the results when run\", \"ning the command from your main solution directory, which contains the docker-compose.yml file.\\n\\n*Fi\", \"gure 5-11. Example results when running the docker-compose up command*\\n\\nAfter the docker-compose up \", \"command runs, the application and its related containers are deployed into your Docker host, as depi\", \"cted in Figure 5-10.\\n\\n### **Using Visual Studio**\\n\\nRunning a multi-container application using Visua\", \"l Studio 2019 can't get any simpler. You just press Ctrl+F5 to run or F5 to debug, as usual, setting\", \" up the **docker-compose** project as the startup project. Visual Studio handles all needed setup, s\", \"o you can create breakpoints as usual and debug what finally become independent processes running in\", \" \\\"remote servers\\\", with the debugger already attached, just like that.\\n\\nAs mentioned before, each ti\", \"me you add Docker solution support to a project within a solution, that project is configured in the\", \" global (solution-level) docker-compose.yml file, which lets you run or debug the whole solution at \", \"once. Visual Studio will start one container for each project that has Docker solution support enabl\", \"ed, and perform all the internal steps for you (dotnet publish, docker build, etc.).\\n\\nIf you want to\", \" take a peek at all the drudgery, take a look at the file:\\n\\n{root solution folder}\\\\obj\\\\Docker\\\\docker\", \"-compose.vs.debug.g.yml\\n\\nThe important point here is that, as shown in Figure 5-12, in Visual Studio\", \" 2019 there is an additional **Docker** command for the F5 key action. This option lets you run or d\", \"ebug a multi-container application by running all the containers that are defined in the docker-comp\", \"ose.yml files at the solution level. The ability to debug multiple-container solutions means that yo\", \"u can set several breakpoints, each breakpoint in a different project (container), and while debuggi\", \"ng from Visual Studio you will stop at breakpoints defined in different projects and running on diff\", \"erent containers.\\n\\n![](_page_103_Picture_1.jpeg)\\n\\n*Figure 5-12. Running multi-container apps in Visu\", \"al Studio 2022*\\n\\n### **Additional resources**\\n\\n\\u2022 **Deploy an ASP.NET container to a remote Docker ho\", \"st** [https://learn.microsoft.com/visualstudio/containers/hosting-web-apps-in-docker](https://docs.m\", \"icrosoft.com/visualstudio/containers/hosting-web-apps-in-docker)\\n\\n## **A note about testing and depl\", \"oying with orchestrators**\\n\\nThe docker-compose up and docker run commands (or running and debugging \", \"the containers in Visual Studio) are adequate for testing containers in your development environment\", \". But you should not use this approach for production deployments, where you should target orchestra\", \"tors like [Kubernetes](https://kubernetes.io/) or [Service Fabric](https://azure.microsoft.com/servi\", \"ces/service-fabric/). If you're using Kubernetes, you have to use [pods](https://kubernetes.io/docs/\", \"concepts/workloads/pods/pod/) to organize containers and [services](https://kubernetes.io/docs/conce\", \"pts/services-networking/service/) to network them. You also use [deployments](https://kubernetes.io/\", \"docs/concepts/workloads/controllers/deployment/) to organize pod creation and modification.\\n\\n![](_pa\", \"ge_103_Picture_7.jpeg)\\n\\n### <span id=\\\"page-103-0\\\"></span>**Step 6. Test your Docker application usin\", \"g your local Docker host**\\n\\nThis step will vary depending on what your application is doing. In a si\", \"mple .NET Web application that is deployed as a single container or service, you can access the serv\", \"ice by opening a browser on the Docker host and navigating to that site, as shown in Figure 5-13. (I\", \"f the configuration in the Dockerfile maps the container to a port on the host that is anything othe\", \"r than 80, include the host port in the URL.)\\n\\n![](_page_103_Picture_10.jpeg)\\n\\n*Figure 5-13. Example\", \" of testing your Docker application locally using localhost*\\n\\nIf localhost is not pointing to the Do\", \"cker host IP (by default, when using Docker CE, it should), to navigate to your service, use the IP \", \"address of your machine's network card.\\n\\nThis URL in the browser uses port 80 for the particular con\", \"tainer example being discussed. However, internally the requests are being redirected to port 5000, \", \"because that was how it was deployed with the docker run command, as explained in a previous step.\\n\\n\", \"You can also test the application using curl from the terminal, as shown in Figure 5-14. In a Docker\", \" installation on Windows, the default Docker Host IP is always 10.0.75.1 in addition to your machine\", \"'s actual IP address.\\n\\n*Figure 5-14. Example of testing your Docker application locally using curl*\\n\", \"\\n### **Testing and debugging containers with Visual Studio 2022**\\n\\nWhen running and debugging the co\", \"ntainers with Visual Studio 2022, you can debug the .NET application in much the same way as you wou\", \"ld when running without containers.\\n\\n### **Testing and debugging without Visual Studio**\\n\\nIf you're \", \"developing using the editor/CLI approach, debugging containers is more difficult and you'll probably\", \" want to debug by generating traces.\\n\\n### **Additional resources**\\n\\n- **Quickstart: Docker in Visual\", \" Studio.** [https://learn.microsoft.com/visualstudio/containers/container-tools](https://docs.micros\", \"oft.com/visualstudio/containers/container-tools)\\n- **Debugging apps in a local Docker container** [h\", \"ttps://learn.microsoft.com/visualstudio/containers/edit-and-refresh](https://docs.microsoft.com/visu\", \"alstudio/containers/edit-and-refresh)\\n\\n### <span id=\\\"page-104-0\\\"></span>**Simplified workflow when d\", \"eveloping containers with Visual Studio**\\n\\nEffectively, the workflow when using Visual Studio is a l\", \"ot simpler than if you use the editor/CLI approach. Most of the steps required by Docker related to \", \"the Dockerfile and docker-compose.yml files are hidden or simplified by Visual Studio, as shown in F\", \"igure 5-15.\\n\\n![](_page_105_Figure_1.jpeg)\\n\\n*Figure 5-15. Simplified workflow when developing with Vi\", \"sual Studio*\\n\\nIn addition, you need to perform step 2 (adding Docker support to your projects) just \", \"once. Therefore, the workflow is similar to your usual development tasks when using .NET for any oth\", \"er development. You need to know what is going on under the covers (the image build process, what ba\", \"se images you're using, deployment of containers, etc.) and sometimes you will also need to edit the\", \" Dockerfile or docker-compose.yml file to customize behaviors. But most of the work is greatly simpl\", \"ified by using Visual Studio, making you a lot more productive.\\n\\n## <span id=\\\"page-105-0\\\"></span>**U\", \"sing PowerShell commands in a Dockerfile to set up Windows Containers**\\n\\n[Windows Containers](https:\", \"//docs.microsoft.com/virtualization/windowscontainers/about/index) allow you to convert your existin\", \"g Windows applications into Docker images and deploy them with the same tools as the rest of the Doc\", \"ker ecosystem. To use Windows Containers, you run PowerShell commands in the Dockerfile, as shown in\", \" the following example:\\n\\n```\\nFROM mcr.microsoft.com/windows/servercore\\nLABEL Description=\\\"IIS\\\" Vendo\", \"r=\\\"Microsoft\\\" Version=\\\"10\\\"\\nRUN powershell -Command Add-WindowsFeature Web-Server\\nCMD [ \\\"ping\\\", \\\"loca\", \"lhost\\\", \\\"-t\\\" ]\\n```\\n\\nIn this case, we are using a Windows Server Core base image (the FROM setting) a\", \"nd installing IIS with a PowerShell command (the RUN setting). In a similar way, you could also use \", \"PowerShell commands to set up additional components like ASP.NET 4.x, .NET Framework 4.6, or any oth\", \"er Windows software. For example, the following command in a Dockerfile sets up ASP.NET 4.5:\\n\\n**RUN*\", \"* powershell add-windowsfeature web-asp-net45\\n\\n### **Additional resources**\\n\\n\\u2022 **aspnet-docker/Docke\", \"rfile.** Example PowerShell commands to run from dockerfiles to include Windows features.\\n\\n[https://\", \"github.com/Microsoft/aspnet-docker/blob/master/4.7.1-windowsservercore](https://github.com/Microsoft\", \"/aspnet-docker/blob/master/4.7.1-windowsservercore-ltsc2016/runtime/Dockerfile)[ltsc2016/runtime/Doc\", \"kerfile](https://github.com/Microsoft/aspnet-docker/blob/master/4.7.1-windowsservercore-ltsc2016/run\", \"time/Dockerfile)\\n\\n**CHAPTER** 5\\n\\n# <span id=\\\"page-107-0\\\"></span>Designing and Developing Multi-Conta\", \"iner and Microservice-Based .NET Applications\\n\\n*Developing containerized microservice applications m\", \"eans you are building multi-container applications. However, a multi-container application could als\", \"o be simpler\\u2014for example, a three-tier application\\u2014and might not be built using a microservice archi\", \"tecture.*\\n\\nEarlier we raised the question \\\"Is Docker necessary when building a microservice architec\", \"ture?\\\" The answer is a clear no. Docker is an enabler and can provide significant benefits, but cont\", \"ainers and Docker are not a hard requirement for microservices. As an example, you could create a mi\", \"croservices-based application with or without Docker when using Azure Service Fabric, which supports\", \" microservices running as simple processes or as Docker containers.\\n\\nHowever, if you know how to des\", \"ign and develop a microservices-based application that is also based on Docker containers, you will \", \"be able to design and develop any other, simpler application model. For example, you might design a \", \"three-tier application that also requires a multi-container approach. Because of that, and because m\", \"icroservice architectures are an important trend within the container world, this section focuses on\", \" a microservice architecture implementation using Docker containers.\\n\\n## <span id=\\\"page-107-1\\\"></spa\", \"n>Design a microservice-oriented application\\n\\nThis section focuses on developing a hypothetical serv\", \"er-side enterprise application.\\n\\n### <span id=\\\"page-107-2\\\"></span>**Application specifications**\\n\\nTh\", \"e hypothetical application handles requests by executing business logic, accessing databases, and th\", \"en returning HTML, JSON, or XML responses. We will say that the application must support various cli\", \"ents, including desktop browsers running Single Page Applications (SPAs), traditional web apps, mobi\", \"le web apps, and native mobile apps. The application might also expose an API for third parties\\n\\nto \", \"consume. It should also be able to integrate its microservices or external applications asynchronous\", \"ly, so that approach will help resiliency of the microservices in the case of partial failures.\\n\\nThe\", \" application will consist of these types of components:\\n\\n- Presentation components. These components\", \" are responsible for handling the UI and consuming remote services.\\n- Domain or business logic. This\", \" component is the application's domain logic.\\n- Database access logic. This component consists of da\", \"ta access components responsible for accessing databases (SQL or NoSQL).\\n- Application integration l\", \"ogic. This component includes a messaging channel, based on message brokers.\\n\\nThe application will r\", \"equire high scalability, while allowing its vertical subsystems to scale out autonomously, because c\", \"ertain subsystems will require more scalability than others.\\n\\nThe application must be able to be dep\", \"loyed in multiple infrastructure environments (multiple public clouds and on-premises) and ideally s\", \"hould be cross-platform, able to move from Linux to Windows (or vice versa) easily.\\n\\n### <span id=\\\"p\", \"age-108-0\\\"></span>**Development team context**\\n\\nWe also assume the following about the development p\", \"rocess for the application:\\n\\n- You have multiple dev teams focusing on different business areas of t\", \"he application.\\n- New team members must become productive quickly, and the application must be easy \", \"to understand and modify.\\n- The application will have a long-term evolution and ever-changing busine\", \"ss rules.\\n- You need good long-term maintainability, which means having agility when implementing ne\", \"w changes in the future while being able to update multiple subsystems with minimum impact on the ot\", \"her subsystems.\\n- You want to practice continuous integration and continuous deployment of the appli\", \"cation.\\n- You want to take advantage of emerging technologies (frameworks, programming languages, et\", \"c.) while evolving the application. You do not want to make full migrations of the application when \", \"moving to new technologies, because that would result in high costs and impact the predictability an\", \"d stability of the application.\\n\\n### <span id=\\\"page-108-1\\\"></span>**Choosing an architecture**\\n\\nWhat\", \" should the application deployment architecture be? The specifications for the application, along wi\", \"th the development context, strongly suggest that you should architect the application by decomposin\", \"g it into autonomous subsystems in the form of collaborating microservices and containers, where a m\", \"icroservice is a container.\\n\\nIn this approach, each service (container) implements a set of cohesive\", \" and narrowly related functions. For example, an application might consist of services such as the c\", \"atalog service, ordering service, basket service, user profile service, etc.\\n\\nMicroservices communic\", \"ate using protocols such as HTTP (REST), but also asynchronously (for example, using AMQP) whenever \", \"possible, especially when propagating updates with integration events.\\n\\nMicroservices are developed \", \"and deployed as containers independently of one another. This approach means that a development team\", \" can be developing and deploying a certain microservice without impacting other subsystems.\\n\\nEach mi\", \"croservice has its own database, allowing it to be fully decoupled from other microservices. When ne\", \"cessary, consistency between databases from different microservices is achieved using application-le\", \"vel integration events (through a logical event bus), as handled in Command and Query Responsibility\", \" Segregation (CQRS). Because of that, the business constraints must embrace eventual consistency bet\", \"ween the multiple microservices and related databases.\\n\\n## **eShopOnContainers: A reference applicat\", \"ion for .NET and microservices deployed using containers**\\n\\nSo that you can focus on the architectur\", \"e and technologies instead of thinking about a hypothetical business domain that you might not know,\", \" we have selected a well-known business domain\\u2014namely, a simplified e-commerce (e-shop) application \", \"that presents a catalog of products, takes orders from customers, verifies inventory, and performs o\", \"ther business functions. This container-based application source code is available in the [eShopOnCo\", \"ntainers](https://aka.ms/MicroservicesArchitecture) GitHub repo.\\n\\nThe application consists of multip\", \"le subsystems, including several store UI front ends (a Web application and a native mobile app), al\", \"ong with the back-end microservices and containers for all the required server-side operations with \", \"several API Gateways as consolidated entry points to the internal microservices. Figure 6-1 shows th\", \"e architecture of the reference application.\\n\\n![](_page_110_Figure_2.jpeg)\\n\\n*Figure 6-1. The eShopOn\", \"Containers reference application architecture for development environment*\\n\\nThe above diagram shows \", \"that Mobile and SPA clients communicate to single API gateway endpoints, that then communicate to mi\", \"croservices. Traditional web clients communicate to MVC microservice, that communicates to microserv\", \"ices through the API gateway.\\n\\n**Hosting environment**. In Figure 6-1, you see several containers de\", \"ployed within a single Docker host. That would be the case when deploying to a single Docker host wi\", \"th the docker-compose up command. However, if you are using an orchestrator or container cluster, ea\", \"ch container could be running in a different host (node), and any node could be running any number o\", \"f containers, as we explained earlier in the architecture section.\\n\\n**Communication architecture**. \", \"The eShopOnContainers application uses two communication types, depending on the kind of the functio\", \"nal action (queries versus updates and transactions):\\n\\n- Http client-to-microservice communication t\", \"hrough API Gateways. This approach is used for queries and when accepting update or transactional co\", \"mmands from the client apps. The approach using API Gateways is explained in detail in later section\", \"s.\\n- Asynchronous event-based communication. This communication occurs through an event bus to propa\", \"gate updates across microservices or to integrate with external applications. The event bus can be i\", \"mplemented with any messaging-broker infrastructure technology like RabbitMQ, or using higher-level \", \"(abstraction-level) service buses like Azure Service Bus, NServiceBus, MassTransit, or Brighter.\\n\\nTh\", \"e application is deployed as a set of microservices in the form of containers. Client apps can commu\", \"nicate with those microservices running as containers through the public URLs published by the API G\", \"ateways.\\n\\n## **Data sovereignty per microservice**\\n\\nIn the sample application, each microservice own\", \"s its own database or data source, although all SQL Server databases are deployed as a single contai\", \"ner. This design decision was made only to make it easy for a developer to get the code from GitHub,\", \" clone it, and open it in Visual Studio or Visual Studio Code. Or alternatively, it makes it easy to\", \" compile the custom Docker images using the .NET CLI and the Docker CLI, and then deploy and run the\", \"m in a Docker development environment. Either way, using containers for data sources lets developers\", \" build and deploy in a matter of minutes without having to provision an external database or any oth\", \"er data source with hard dependencies on infrastructure (cloud or on-premises).\\n\\nIn a real productio\", \"n environment, for high availability and for scalability, the databases should be based on database \", \"servers in the cloud or on-premises, but not in containers.\\n\\nTherefore, the units of deployment for \", \"microservices (and even for databases in this application) are Docker containers, and the reference \", \"application is a multi-container application that embraces microservices principles.\\n\\n### **Addition\", \"al resources**\\n\\n\\u2022 **eShopOnContainers GitHub repo. Source code for the reference application** <http\", \"s://aka.ms/eShopOnContainers/>\\n\\n### <span id=\\\"page-111-0\\\"></span>**Benefits of a microservice-based \", \"solution**\\n\\nA microservice-based solution like this has many benefits:\\n\\n**Each microservice is relat\", \"ively small\\u2014easy to manage and evolve**. Specifically:\\n\\n- It is easy for a developer to understand a\", \"nd get started quickly with good productivity.\\n- Containers start fast, which makes developers more \", \"productive.\\n- An IDE like Visual Studio can load smaller projects fast, making developers productive\", \".\\n- Each microservice can be designed, developed, and deployed independently of other microservices,\", \" which provide agility because it is easier to deploy new versions of microservices frequently.\\n\\n**I\", \"t is possible to scale out individual areas of the application**. For instance, the catalog service \", \"or the basket service might need to be scaled out, but not the ordering process. A microservices inf\", \"rastructure will be much more efficient with regard to the resources used when scaling out than a mo\", \"nolithic architecture would be.\\n\\n**You can divide the development work between multiple teams**. Eac\", \"h service can be owned by a single development team. Each team can manage, develop, deploy, and scal\", \"e their service independently of the rest of the teams.\\n\\n**Issues are more isolated**. If there is a\", \"n issue in one service, only that service is initially impacted (except when the wrong design is use\", \"d, with direct dependencies between microservices), and other services can continue to handle reques\", \"ts. In contrast, one malfunctioning component in a monolithic deployment architecture can bring down\", \" the entire system, especially when it involves resources, such as a memory leak. Additionally, when\", \" an issue in a microservice is resolved, you can deploy just the affected microservice without impac\", \"ting the rest of the application.\\n\\n**You can use the latest technologies**. Because you can start de\", \"veloping services independently and run them side by side (thanks to containers and .NET), you can s\", \"tart using the latest technologies and frameworks expediently instead of being stuck on an older sta\", \"ck or framework for the whole application.\\n\\n## <span id=\\\"page-112-0\\\"></span>**Downsides of a microse\", \"rvice-based solution**\\n\\nA microservice-based solution like this also has some drawbacks:\\n\\n**Distribu\", \"ted application**. Distributing the application adds complexity for developers when they are designi\", \"ng and building the services. For example, developers must implement inter-service communication usi\", \"ng protocols like HTTP or AMQP, which adds complexity for testing and exception handling. It also ad\", \"ds latency to the system.\\n\\n**Deployment complexity**. An application that has dozens of microservice\", \"s types and needs high scalability (it needs to be able to create many instances per service and bal\", \"ance those services across many hosts) means a high degree of deployment complexity for IT operation\", \"s and management. If you are not using a microservice-oriented infrastructure (like an orchestrator \", \"and scheduler), that additional complexity can require far more development efforts than the busines\", \"s application itself.\\n\\n**Atomic transactions**. Atomic transactions between multiple microservices u\", \"sually are not possible. The business requirements have to embrace eventual consistency between mult\", \"iple microservices.\\n\\n**Increased global resource needs** (total memory, drives, and network resource\", \"s for all the servers or hosts). In many cases, when you replace a monolithic application with a mic\", \"roservices approach, the amount of initial global resources needed by the new microservice-based app\", \"lication will be larger than the infrastructure needs of the original monolithic application. This a\", \"pproach is because the higher degree of granularity and distributed services requires more global re\", \"sources. However, given the low cost of resources in general and the benefit of being able to scale \", \"out certain areas of the application compared to long-term costs when evolving monolithic applicatio\", \"ns, the increased use of resources is usually a good tradeoff for large, long-term applications.\\n\\n**\", \"Issues with direct client-to-microservice communication**. When the application is large, with dozen\", \"s of microservices, there are challenges and limitations if the application requires direct clientto\", \"-microservice communications. One problem is a potential mismatch between the needs of the client an\", \"d the APIs exposed by each of the microservices. In certain cases, the client application might need\", \" to make many separate requests to compose the UI, which can be inefficient over the Internet and wo\", \"uld be impractical over a mobile network. Therefore, requests from the client application to the bac\", \"k-end system should be minimized.\\n\\nAnother problem with direct client-to-microservice communications\", \" is that some microservices might be using protocols that are not Web-friendly. One service might us\", \"e a binary protocol, while another service might use AMQP messaging. Those protocols are not firewal\", \"l-friendly and are best used internally. Usually, an application should use protocols such as HTTP a\", \"nd WebSockets for communication outside of the firewall.\\n\\nYet another drawback with this direct clie\", \"nt-to-service approach is that it makes it difficult to refactor the contracts for those microservic\", \"es. Over time developers might want to change how the system is partitioned into services. For examp\", \"le, they might merge two services or split a service into two or more services. However, if clients \", \"communicate directly with the services, performing this kind of refactoring can break compatibility \", \"with client apps.\\n\\nAs mentioned in the architecture section, when designing and building a complex a\", \"pplication based on microservices, you might consider the use of multiple fine-grained API Gateways \", \"instead of the simpler direct client-to-microservice communication approach.\\n\\n**Partitioning the mic\", \"roservices**. Finally, no matter, which approach you take for your microservice architecture, anothe\", \"r challenge is deciding how to partition an end-to-end application into multiple microservices. As n\", \"oted in the architecture section of the guide, there are several techniques and approaches you can t\", \"ake. Basically, you need to identify areas of the application that are decoupled from the other area\", \"s and that have a low number of hard dependencies. In many cases, this approach is aligned to partit\", \"ioning services by use case. For example, in our e-shop application, we have an ordering service tha\", \"t is responsible for all the business logic related to the order process. We also have the catalog s\", \"ervice and the basket service that implement other capabilities. Ideally, each service should have o\", \"nly a small set of responsibilities. This approach is similar to the single responsibility principle\", \" (SRP) applied to classes, which states that a class should only have one reason to change. But in t\", \"his case, it is about microservices, so the scope will be larger than a single class. Most of all, a\", \" microservice has to be autonomous, end to end, including responsibility for its own data sources.\\n\\n\", \"### <span id=\\\"page-113-0\\\"></span>**External versus internal architecture and design patterns**\\n\\nThe \", \"external architecture is the microservice architecture composed by multiple services, following the \", \"principles described in the architecture section of this guide. However, depending on the nature of \", \"each microservice, and independently of high-level microservice architecture you choose, it is commo\", \"n and sometimes advisable to have different internal architectures, each based on different patterns\", \", for different microservices. The microservices can even use different technologies and programming\", \" languages. Figure 6-2 illustrates this diversity.\\n\\n![](_page_114_Figure_2.jpeg)\\n\\n*Figure 6-2. Exter\", \"nal versus internal architecture and design*\\n\\nFor instance, in our *eShopOnContainers* sample, the c\", \"atalog, basket, and user profile microservices are simple (basically, CRUD subsystems). Therefore, t\", \"heir internal architecture and design is straightforward. However, you might have other microservice\", \"s, such as the ordering microservice, which is more complex and represents ever-changing business ru\", \"les with a high degree of domain complexity. In cases like these, you might want to implement more a\", \"dvanced patterns within a particular microservice, like the ones defined with domain-driven design (\", \"DDD) approaches, as we are doing in the *eShopOnContainers* ordering microservice. (We will review t\", \"hese DDD patterns in the section later that explains the implementation of the *eShopOnContainers* o\", \"rdering microservice.)\\n\\nAnother reason for a different technology per microservice might be the natu\", \"re of each microservice. For example, it might be better to use a functional programming language li\", \"ke F#, or even a language like R if you are targeting AI and machine learning domains, instead of a \", \"more object-oriented programming language like C#.\\n\\nThe bottom line is that each microservice can ha\", \"ve a different internal architecture based on different design patterns. Not all microservices shoul\", \"d be implemented using advanced DDD patterns, because that would be over-engineering them. Similarly\", \", complex microservices with ever-changing business logic should not be implemented as CRUD componen\", \"ts, or you can end up with low-quality code.\\n\\n## <span id=\\\"page-114-0\\\"></span>**The new world: multi\", \"ple architectural patterns and polyglot microservices**\\n\\nThere are many architectural patterns used \", \"by software architects and developers. The following are a few (mixing architecture styles and archi\", \"tecture patterns):\\n\\n\\u2022 Simple CRUD, single-tier, single-layer.\\n\\n- [Traditional N-Layered.](https://do\", \"cs.microsoft.com/previous-versions/msp-n-p/ee658109(v=pandp.10))\\n- [Domain-Driven Design N-layered.]\", \"(https://devblogs.microsoft.com/cesardelatorre/published-first-alpha-version-of-domain-oriented-n-la\", \"yered-architecture-v2-0/)\\n- [Clean Architecture](https://docs.microsoft.com/dotnet/architecture/mode\", \"rn-web-apps-azure/common-web-application-architectures#clean-architecture) (as used with [eShopOnWeb\", \"\\\\)](https://aka.ms/WebAppArchitecture)\\n- [Command and Query Responsibility Segregation](https://mart\", \"infowler.com/bliki/CQRS.html) (CQRS).\\n- [Event-Driven Architecture](https://en.wikipedia.org/wiki/Ev\", \"ent-driven_architecture) (EDA).\\n\\nYou can also build microservices with many technologies and languag\", \"es, such as ASP.NET Core Web APIs, NancyFx, ASP.NET Core SignalR (available with .NET Core 2 or late\", \"r), F#, Node.js, Python, Java, C++, GoLang, and more.\\n\\nThe important point is that no particular arc\", \"hitecture pattern or style, nor any particular technology, is right for all situations. Figure 6-3 s\", \"hows some approaches and technologies (although not in any particular order) that could be used in d\", \"ifferent microservices.\\n\\n![](_page_115_Figure_8.jpeg)\\n\\n*Figure 6-3. Multi-architectural patterns and\", \" the polyglot microservices world*\\n\\nMulti-architectural pattern and polyglot microservices means you\", \" can mix and match languages and technologies to the needs of each microservice and still have them \", \"talking to each other. As shown in Figure 6-3, in applications composed of many microservices (Bound\", \"ed Contexts in domain-driven design terminology, or simply \\\"subsystems\\\" as autonomous microservices)\", \", you might implement each microservice in a different way. Each might have a different architecture\", \" pattern and use different languages and databases depending on the application's nature, business r\", \"equirements, and priorities. In some cases, the microservices might be similar. But that is not usua\", \"lly the case, because each subsystem's context boundary and requirements are usually different.\\n\\nFor\", \" instance, for a simple CRUD maintenance application, it might not make sense to design and implemen\", \"t DDD patterns. But for your core domain or core business, you might need to apply more advanced pat\", \"terns to tackle business complexity with ever-changing business rules.\\n\\nEspecially when you deal wit\", \"h large applications composed by multiple subsystems, you should not apply a single top-level archit\", \"ecture based on a single architecture pattern. For instance, CQRS should not be applied as a top-lev\", \"el architecture for a whole application, but might be useful for a specific set of services.\\n\\nThere \", \"is no silver bullet or a right architecture pattern for every given case. You cannot have \\\"one archi\", \"tecture pattern to rule them all.\\\" Depending on the priorities of each microservice, you must choose\", \" a different approach for each, as explained in the following sections.\\n\\n## <span id=\\\"page-116-0\\\"></\", \"span>Creating a simple data-driven CRUD microservice\\n\\nThis section outlines how to create a simple m\", \"icroservice that performs create, read, update, and delete (CRUD) operations on a data source.\\n\\n## <\", \"span id=\\\"page-116-1\\\"></span>**Designing a simple CRUD microservice**\\n\\nFrom a design point of view, t\", \"his type of containerized microservice is very simple. Perhaps the problem to solve is simple, or pe\", \"rhaps the implementation is only a proof of concept.\\n\\n![](_page_116_Figure_7.jpeg)\\n\\n*Figure 6-4. Int\", \"ernal design for simple CRUD microservices*\\n\\nAn example of this kind of simple data-drive service is\", \" the catalog microservice from the eShopOnContainers sample application. This type of service implem\", \"ents all its functionality in a single ASP.NET Core Web API project that includes classes for its da\", \"ta model, its business logic, and its data access code. It also stores its related data in a databas\", \"e running in SQL Server (as another container for dev/test purposes), but could also be any regular \", \"SQL Server host, as shown in Figure 6-5.\\n\\n![](_page_117_Figure_1.jpeg)\\n\\nThe previous diagram shows t\", \"he logical Catalog microservice, that includes its Catalog database, which can be or not in the same\", \" Docker host. Having the database in the same Docker host might be good for development, but not for\", \" production. When you are developing this kind of service, you only need [ASP.NET Core](https://docs\", \".microsoft.com/aspnet/core/) and a data-access API or ORM like [Entity Framework Core.](https://docs\", \".microsoft.com/ef/core/index) You could also generate [Swagger](https://swagger.io/) metadata automa\", \"tically through [Swashbuckle](https://github.com/domaindrivendev/Swashbuckle.AspNetCore) to provide \", \"a description of what your service offers, as explained in the next section.\\n\\nNote that running a da\", \"tabase server like SQL Server within a Docker container is great for development environments, becau\", \"se you can have all your dependencies up and running without needing to provision a database in the \", \"cloud or on-premises. This approach is convenient when running integration tests. However, for produ\", \"ction environments, running a database server in a container is not recommended, because you usually\", \" do not get high availability with that approach. For a production environment in Azure, it is recom\", \"mended that you use Azure SQL DB or any other database technology that can provide high availability\", \" and high scalability. For example, for a NoSQL approach, you might choose CosmosDB.\\n\\nFinally, by ed\", \"iting the Dockerfile and docker-compose.yml metadata files, you can configure how the image of this \", \"container will be created\\u2014what base image it will use, plus design settings such as internal and ext\", \"ernal names and TCP ports.\\n\\n### <span id=\\\"page-117-0\\\"></span>**Implementing a simple CRUD microservi\", \"ce with ASP.NET Core**\\n\\nTo implement a simple CRUD microservice using .NET and Visual Studio, you st\", \"art by creating a simple ASP.NET Core Web API project (running on .NET so it can run on a Linux Dock\", \"er host), as shown in Figure 6-6.\\n\\n![](_page_118_Figure_0.jpeg)\\n\\n*Figure 6-6. Creating an ASP.NET Co\", \"re Web API project in Visual Studio 2019*\\n\\nTo create an ASP.NET Core Web API Project, first select a\", \"n ASP.NET Core Web Application and then select the API type. After creating the project, you can imp\", \"lement your MVC controllers as you would in any other Web API project, using the Entity Framework AP\", \"I or other API. In a new Web API project, you can see that the only dependency you have in that micr\", \"oservice is on ASP.NET Core itself. Internally, within the *Microsoft.AspNetCore.All* dependency, it\", \" is referencing Entity Framework and many other .NET NuGet packages, as shown in Figure 6-7.\\n\\n![](_p\", \"age_119_Figure_0.jpeg)\\n\\n*Figure 6-7. Dependencies in a simple CRUD Web API microservice*\\n\\nThe API pr\", \"oject includes references to Microsoft.AspNetCore.App NuGet package, that includes references to all\", \" essential packages. It could include some other packages as well.\\n\\n## **Implementing CRUD Web API s\", \"ervices with Entity Framework Core**\\n\\nEntity Framework (EF) Core is a lightweight, extensible, and c\", \"ross-platform version of the popular Entity Framework data access technology. EF Core is an object-r\", \"elational mapper (ORM) that enables .NET developers to work with a database using .NET objects.\\n\\nThe\", \" catalog microservice uses EF and the SQL Server provider because its database is running in a conta\", \"iner with the SQL Server for Linux Docker image. However, the database could be deployed into any SQ\", \"L Server, such as Windows on-premises or Azure SQL DB. The only thing you would need to change is th\", \"e connection string in the ASP.NET Web API microservice.\\n\\n### **The data model**\\n\\nWith EF Core, data\", \" access is performed by using a model. A model is made up of (domain model) entity classes and a der\", \"ived context (DbContext) that represents a session with the database, allowing you to query and save\", \" data. You can generate a model from an existing database, manually code a model to match your datab\", \"ase, or use EF migrations technique to create a database from your model, using the code-first appro\", \"ach (that makes it easy to evolve the database as your model changes over time). For the catalog mic\", \"roservice, the last approach has been used. You can see an example of the CatalogItem entity class i\", \"n the following code example, which is a simple Plain Old Class Object [\\\\(POCO\\\\)](https://docs.micro\", \"soft.com/dotnet/standard/glossary#poco) entity class.\\n\\n```\\npublic class CatalogItem\\n{\\n public int Id\", \" { get; set; }\\n public string Name { get; set; }\\n public string Description { get; set; }\\n public de\", \"cimal Price { get; set; }\\n public string PictureFileName { get; set; }\\n public string PictureUri { g\", \"et; set; }\\n public int CatalogTypeId { get; set; }\\n public CatalogType CatalogType { get; set; }\\n pu\", \"blic int CatalogBrandId { get; set; }\\n public CatalogBrand CatalogBrand { get; set; }\\n public int Av\", \"ailableStock { get; set; }\\n public int RestockThreshold { get; set; }\\n public int MaxStockThreshold \", \"{ get; set; }\\n public bool OnReorder { get; set; }\\n public CatalogItem() { }\\n // Additional code ...\", \"\\n}\\n```\\n\\nYou also need a DbContext that represents a session with the database. For the catalog micro\", \"service, the CatalogContext class derives from the DbContext base class, as shown in the following e\", \"xample:\\n\\n```\\npublic class CatalogContext : DbContext\\n{\\n public CatalogContext(DbContextOptions<Catal\", \"ogContext> options) : base(options)\\n { }\\n public DbSet<CatalogItem> CatalogItems { get; set; }\\n publ\", \"ic DbSet<CatalogBrand> CatalogBrands { get; set; }\\n public DbSet<CatalogType> CatalogTypes { get; se\", \"t; }\\n // Additional code ...\\n}\\n```\\n\\nYou can have additional DbContext implementations. For example, \", \"in the sample Catalog.API microservice, there's a second DbContext named CatalogContextSeed where it\", \" automatically populates the sample data the first time it tries to access the database. This method\", \" is useful for demo data and for automated testing scenarios, as well.\\n\\nWithin the DbContext, you us\", \"e the OnModelCreating method to customize object/database entity mappings and other [EF extensibilit\", \"y points.](https://devblogs.microsoft.com/dotnet/implementing-seeding-custom-conventions-and-interce\", \"ptors-in-ef-core-1-0/)\\n\\n## Querying data from Web API controllers\\n\\nInstances of your entity classes \", \"are typically retrieved from the database using Language-Integrated Query (LINQ), as shown in the fo\", \"llowing example:\\n\\n```\\n[Route(\\\"api/v1/[controller]\\\")]\\npublic class CatalogController : ControllerBase\", \"\\n{\\n private readonly CatalogContext _catalogContext;\\n private readonly CatalogSettings _settings;\\n p\", \"rivate readonly ICatalogIntegrationEventService _catalogIntegrationEventService;\\n public CatalogCont\", \"roller(\\n CatalogContext context,\\n IOptionsSnapshot<CatalogSettings> settings,\\n ICatalogIntegrationEv\", \"entService catalogIntegrationEventService)\\n {\\n _catalogContext = context ?? throw new ArgumentNullEx\", \"ception(nameof(context));\\n _catalogIntegrationEventService = catalogIntegrationEventService\\n ?? thro\", \"w new ArgumentNullException(nameof(catalogIntegrationEventService));\\n _settings = settings.Value;\\n c\", \"ontext.ChangeTracker.QueryTrackingBehavior = QueryTrackingBehavior.NoTracking;\\n }\\n // GET api/v1/[co\", \"ntroller]/items[?pageSize=3&pageIndex=10]\\n [HttpGet]\\n [Route(\\\"items\\\")]\\n [ProducesResponseType(typeof\", \"(PaginatedItemsViewModel<CatalogItem>),\\n(int)HttpStatusCode.OK)]\\n [ProducesResponseType(typeof(IEnum\", \"erable<CatalogItem>), (int)HttpStatusCode.OK)]\\n [ProducesResponseType((int)HttpStatusCode.BadRequest\", \")]\\n public async Task<IActionResult> ItemsAsync(\\n [FromQuery]int pageSize = 10,\\n [FromQuery]int page\", \"Index = 0,\\n string ids = null)\\n {\\n if (!string.IsNullOrEmpty(ids))\\n {\\n var items = await GetItemsByI\", \"dsAsync(ids);\\n if (!items.Any())\\n {\\n return BadRequest(\\\"ids value invalid. Must be comma-separated l\", \"ist of \\nnumbers\\\");\\n }\\n return Ok(items);\\n }\\n var totalItems = await _catalogContext.CatalogItems\\n .L\", \"ongCountAsync();\\n var itemsOnPage = await _catalogContext.CatalogItems\\n .OrderBy(c => c.Name)\\n .Skip\", \"(pageSize * pageIndex)\\n```\\n\\n```\\n .Take(pageSize)\\n .ToListAsync();\\n itemsOnPage = ChangeUriPlaceholde\", \"r(itemsOnPage);\\n var model = new PaginatedItemsViewModel<CatalogItem>(\\n pageIndex, pageSize, totalIt\", \"ems, itemsOnPage);\\n return Ok(model);\\n }\\n //...\\n}\\n```\\n\\n## Saving data\\n\\nData is created, deleted, and\", \" modified in the database using instances of your entity classes. You could add code like the follow\", \"ing hard-coded example (mock data, in this case) to your Web API controllers.\\n\\n```\\nvar catalogItem =\", \" new CatalogItem() {CatalogTypeId=2, CatalogBrandId=2,\\n Name=\\\"Roslyn T-Shirt\\\", Price = 12};\\n_context\", \".Catalog.Add(catalogItem);\\n_context.SaveChanges();\\n```\\n\\n### Dependency Injection in ASP.NET Core and\", \" Web API controllers\\n\\nIn ASP.NET Core, you can use Dependency Injection (DI) out of the box. You do \", \"not need to set up a third-party Inversion of Control (IoC) container, although you can plug your pr\", \"eferred IoC container into the ASP.NET Core infrastructure if you want. In this case, it means that \", \"you can directly inject the required EF DBContext or additional repositories through the controller \", \"constructor.\\n\\nIn the CatalogController class mentioned earlier, CatalogContext (which inherits from \", \"DbContext) type is injected along with the other required objects in the CatalogController() constru\", \"ctor.\\n\\nAn important configuration to set up in the Web API project is the DbContext class registrati\", \"on into the service's IoC container. You typically do so in the *Program.cs* file by calling the bui\", \"lder.Services.AddDbContext<CatalogContext>() method, as shown in the following **simplified** exampl\", \"e:\\n\\n```\\n// Additional code...\\nbuilder.Services.AddDbContext<CatalogContext>(options =>\\n{\\n options.Us\", \"eSqlServer(builder.Configuration[\\\"ConnectionString\\\"],\\n sqlServerOptionsAction: sqlOptions =>\\n {\\n sql\", \"Options.MigrationsAssembly(\\n typeof(Program).GetTypeInfo().Assembly.GetName().Name);\\n //Configuring \", \"Connection Resiliency:\\n sqlOptions.\\n EnableRetryOnFailure(maxRetryCount: 5,\\n maxRetryDelay: TimeSpan\", \".FromSeconds(30),\\n errorNumbersToAdd: null);\\n });\\n```\\n\\n```\\n // Changing default behavior when client\", \" evaluation occurs to throw.\\n // Default in EFCore would be to log warning when client evaluation is\", \" done.\\n options.ConfigureWarnings(warnings => warnings.Throw(\\n RelationalEventId.QueryClientEvaluati\", \"onWarning));\\n});\\n```\\n\\n## **Additional resources**\\n\\n- **Querying Data** [https://learn.microsoft.com/\", \"ef/core/querying/index](https://docs.microsoft.com/ef/core/querying/index)\\n- **Saving Data** [https:\", \"//learn.microsoft.com/ef/core/saving/index](https://docs.microsoft.com/ef/core/saving/index)\\n\\n## <sp\", \"an id=\\\"page-123-0\\\"></span>**The DB connection string and environment variables used by Docker contai\", \"ners**\\n\\nYou can use the ASP.NET Core settings and add a ConnectionString property to your settings.j\", \"son file as shown in the following example:\\n\\n```\\n{\\n \\\"ConnectionString\\\": \\\"Server=tcp:127.0.0.1,5433;I\", \"nitial \\nCatalog=Microsoft.eShopOnContainers.Services.CatalogDb;User Id=sa;Password=[PLACEHOLDER]\\\",\\n \", \"\\\"ExternalCatalogBaseUrl\\\": \\\"http://host.docker.internal:5101\\\",\\n \\\"Logging\\\": {\\n \\\"IncludeScopes\\\": false,\", \"\\n \\\"LogLevel\\\": {\\n \\\"Default\\\": \\\"Debug\\\",\\n \\\"System\\\": \\\"Information\\\",\\n \\\"Microsoft\\\": \\\"Information\\\"\\n }\\n }\\n}\\n`\", \"``\\n\\nThe settings.json file can have default values for the ConnectionString property or for any othe\", \"r property. However, those properties will be overridden by the values of environment variables that\", \" you specify in the docker-compose.override.yml file, when using Docker.\\n\\nFrom your docker-compose.y\", \"ml or docker-compose.override.yml files, you can initialize those environment variables so that Dock\", \"er will set them up as OS environment variables for you, as shown in the following docker-compose.ov\", \"erride.yml file (the connection string and other lines wrap in this example, but it would not wrap i\", \"n your own file).\\n\\n```\\n# docker-compose.override.yml\\n#\\ncatalog-api:\\n environment:\\n -\\nConnectionStrin\", \"g=Server=sqldata;Database=Microsoft.eShopOnContainers.Services.CatalogDb;Use\\nr Id=sa;Password=[PLACE\", \"HOLDER]\\n # Additional environment variables for this service\\n ports:\\n - \\\"5101:80\\\"\\n```\\n\\nThe docker-co\", \"mpose.yml files at the solution level are not only more flexible than configuration files at the pro\", \"ject or microservice level, but also more secure if you override the environment variables declared \", \"at the docker-compose files with values set from your deployment tools, like from Azure DevOps Servi\", \"ces Docker deployment tasks.\\n\\nFinally, you can get that value from your code by using builder.Config\", \"uration\\\\[\\\"ConnectionString\\\"\\\\], as shown in an earlier code example.\\n\\nHowever, for production environ\", \"ments, you might want to explore additional ways on how to store secrets like the connection strings\", \". An excellent way to manage application secrets is using [Azure Key](https://azure.microsoft.com/se\", \"rvices/key-vault/)  [Vault.](https://azure.microsoft.com/services/key-vault/)\\n\\nAzure Key Vault helps\", \" to store and safeguard cryptographic keys and secrets used by your cloud applications and services.\", \" A secret is anything you want to keep strict control of, like API keys, connection strings, passwor\", \"ds, etc. and strict control includes usage logging, setting expiration, managing access, *among othe\", \"rs*.\\n\\nAzure Key Vault allows a detailed control level of the application secrets usage without the n\", \"eed to let anyone know them. The secrets can even be rotated for enhanced security without disruptin\", \"g development or operations.\\n\\nApplications have to be registered in the organization's Active Direct\", \"ory, so they can use the Key Vault.\\n\\nYou can check the *Key Vault Concepts documentation* for more d\", \"etails.\\n\\n### **Implementing versioning in ASP.NET Web APIs**\\n\\nAs business requirements change, new c\", \"ollections of resources may be added, the relationships between resources might change, and the stru\", \"cture of the data in resources might be amended. Updating a Web API to handle new requirements is a \", \"relatively straightforward process, but you must consider the effects that such changes will have on\", \" client applications consuming the Web API. Although the developer designing and implementing a Web \", \"API has full control over that API, the developer does not have the same degree of control over clie\", \"nt applications that might be built by third-party organizations operating remotely.\\n\\nVersioning ena\", \"bles a Web API to indicate the features and resources that it exposes. A client application can then\", \" submit requests to a specific version of a feature or resource. There are several approaches to imp\", \"lement versioning:\\n\\n- URI versioning\\n- Query string versioning\\n- Header versioning\\n\\nQuery string and\", \" URI versioning are the simplest to implement. Header versioning is a good approach. However, header\", \" versioning is not as explicit and straightforward as URI versioning. Because URL versioning is the \", \"simplest and most explicit, the eShopOnContainers sample application uses URI versioning.\\n\\nWith URI \", \"versioning, as in the eShopOnContainers sample application, each time you modify the Web API or chan\", \"ge the schema of resources, you add a version number to the URI for each resource. Existing URIs sho\", \"uld continue to operate as before, returning resources that conform to the schema that matches the r\", \"equested version.\\n\\nAs shown in the following code example, the version can be set by using the Route\", \" attribute in the Web API controller, which makes the version explicit in the URI (v1 in this case).\", \"\\n\\n```\\n[Route(\\\"api/v1/[controller]\\\")]\\npublic class CatalogController : ControllerBase\\n{\\n // Implement\", \"ation ...\\n```\\n\\nThis versioning mechanism is simple and depends on the server routing the request to \", \"the appropriate endpoint. However, for a more sophisticated versioning and the best method when usin\", \"g REST, you should use hypermedia and implement [HATEOAS \\\\(Hypertext as the Engine of Application](h\", \"ttps://docs.microsoft.com/azure/architecture/best-practices/api-design#use-hateoas-to-enable-navigat\", \"ion-to-related-resources)  [State\\\\).](https://docs.microsoft.com/azure/architecture/best-practices/a\", \"pi-design#use-hateoas-to-enable-navigation-to-related-resources)\\n\\n### **Additional resources**\\n\\n- **\", \"ASP.NET API Versioning** <https://github.com/dotnet/aspnet-api-versioning>\\n- **Scott Hanselman. ASP.\", \"NET Core RESTful Web API versioning made easy** <https://www.hanselman.com/blog/ASPNETCoreRESTfulWeb\", \"APIVersioningMadeEasy.aspx>\\n- **Versioning a RESTful web API** [https://learn.microsoft.com/azure/ar\", \"chitecture/best-practices/api-design#versioning-a](https://docs.microsoft.com/azure/architecture/bes\", \"t-practices/api-design#versioning-a-restful-web-api)[restful-web-api](https://docs.microsoft.com/azu\", \"re/architecture/best-practices/api-design#versioning-a-restful-web-api)\\n- **Roy Fielding. Versioning\", \", Hypermedia, and REST** <https://www.infoq.com/articles/roy-fielding-on-versioning>\\n\\n## <span id=\\\"p\", \"age-125-0\\\"></span>**Generating Swagger description metadata from your ASP.NET Core Web API**\\n\\n[Swagg\", \"er](https://swagger.io/) is a commonly used open source framework backed by a large ecosystem of too\", \"ls that helps you design, build, document, and consume your RESTful APIs. It is becoming the standar\", \"d for the APIs description metadata domain. You should include Swagger description metadata with any\", \" kind of microservice, either data-driven microservices or more advanced domain-driven microservices\", \" (as explained in the following section).\\n\\nThe heart of Swagger is the Swagger specification, which \", \"is API description metadata in a JSON or YAML file. The specification creates the RESTful contract f\", \"or your API, detailing all its resources and operations in both a human- and machine-readable format\", \" for easy development, discovery, and integration.\\n\\nThe specification is the basis of the OpenAPI Sp\", \"ecification (OAS) and is developed in an open, transparent, and collaborative community to standardi\", \"ze the way RESTful interfaces are defined. The specification defines the structure for how a service\", \" can be discovered and how its capabilities understood. For more information, including a web editor\", \" and examples of Swagger specifications from companies like Spotify, Uber, Slack, and Microsoft, see\", \" the Swagger site [\\\\(https://swagger.io\\\\)](https://swagger.io/).\\n\\n## **Why use Swagger?**\\n\\nThe main \", \"reasons to generate Swagger metadata for your APIs are the following.\\n\\n**Ability for other products \", \"to automatically consume and integrate your APIs**. Dozens of products and [commercial tools](https:\", \"//swagger.io/commercial-tools/) and many [libraries and frameworks](https://swagger.io/open-source-i\", \"ntegrations/) support Swagger. Microsoft has high-level products and tools that can automatically co\", \"nsume Swagger-based APIs, such as the following:\\n\\n- [AutoRest.](https://github.com/Azure/AutoRest) Y\", \"ou can automatically generate .NET client classes for calling Swagger. This tool can be used from th\", \"e CLI and it also integrates with Visual Studio for easy use through the GUI.\\n- [Microsoft Flow.](ht\", \"tps://flow.microsoft.com/) You can automatically [use and integrate your API](https://flow.microsoft\", \".com/blog/integrating-custom-api/) into a high-level Microsoft Flow workflow, with no programming sk\", \"ills required.\\n- [Microsoft PowerApps.](https://powerapps.microsoft.com/) You can automatically cons\", \"ume your API from [PowerApps mobile apps](https://powerapps.microsoft.com/blog/register-and-use-cust\", \"om-apis-in-powerapps/) built with [PowerApps Studio,](https://powerapps.microsoft.com/build-powerapp\", \"s/) with no programming skills required.\\n- [Azure App Service Logic Apps.](https://docs.microsoft.co\", \"m/azure/app-service-logic/app-service-logic-what-are-logic-apps) You can automatically [use and inte\", \"grate your API into an Azure](https://docs.microsoft.com/azure/app-service-logic/app-service-logic-c\", \"ustom-hosted-api)  [App Service Logic App,](https://docs.microsoft.com/azure/app-service-logic/app-s\", \"ervice-logic-custom-hosted-api) with no programming skills required.\\n\\n**Ability to automatically gen\", \"erate API documentation**. When you create large-scale RESTful APIs, such as complex microservice-ba\", \"sed applications, you need to handle many endpoints with different data models used in the request a\", \"nd response payloads. Having proper documentation and having a solid API explorer, as you get with S\", \"wagger, is key for the success of your API and adoption by developers.\\n\\nSwagger's metadata is what M\", \"icrosoft Flow, PowerApps, and Azure Logic Apps use to understand how to use APIs and connect to them\", \".\\n\\nThere are several options to automate Swagger metadata generation for ASP.NET Core REST API appli\", \"cations, in the form of functional API help pages, based on *swagger-ui*.\\n\\nProbably the best know is\", \" [Swashbuckle,](https://github.com/domaindrivendev/Swashbuckle.AspNetCore) which is currently used i\", \"n [eShopOnContainers](https://github.com/dotnet-architecture/eShopOnContainers) and we'll cover in s\", \"ome detail in this guide but there's also the option to use [NSwag,](https://github.com/RSuter/NSwag\", \") which can generate Typescript and C# API clients, as well as C# controllers, from a Swagger or Ope\", \"nAPI specification and even by scanning the .dll that contains the controllers, using [NSwagStudio.]\", \"(https://github.com/RSuter/NSwag/wiki/NSwagStudio)\\n\\n### **How to automate API Swagger metadata gener\", \"ation with the Swashbuckle NuGet package**\\n\\nGenerating Swagger metadata manually (in a JSON or YAML \", \"file) can be tedious work. However, you can automate API discovery of ASP.NET Web API services by us\", \"ing the [Swashbuckle NuGet package](https://aka.ms/swashbuckledotnetcore) to dynamically generate Sw\", \"agger API metadata.\\n\\nSwashbuckle automatically generates Swagger metadata for your ASP.NET Web API p\", \"rojects. It supports ASP.NET Core Web API projects and the traditional ASP.NET Web API and any other\", \" flavor, such as Azure API App, Azure Mobile App, Azure Service Fabric microservices based on ASP.NE\", \"T. It also supports plain Web API deployed on containers, as in for the reference application.\\n\\nSwas\", \"hbuckle combines API Explorer and Swagger or [swagger-ui](https://github.com/swagger-api/swagger-ui)\", \" to provide a rich discovery and documentation experience for your API consumers. In addition to its\", \" Swagger metadata generator engine, Swashbuckle also contains an embedded version of swagger-ui, whi\", \"ch it will automatically serve up once Swashbuckle is installed.\\n\\nThis means you can complement your\", \" API with a nice discovery UI to help developers to use your API. It requires a small amount of code\", \" and maintenance because it is automatically generated, allowing you to focus on building your API. \", \"The result for the API Explorer looks like Figure 6-8.\\n\\n![](_page_127_Picture_3.jpeg)\\n\\n*Figure 6-8. \", \"Swashbuckle API Explorer based on Swagger metadata\\u2014eShopOnContainers catalog microservice*\\n\\nThe Swas\", \"hbuckle generated Swagger UI API documentation includes all published actions. The API explorer is n\", \"ot the most important thing here. Once you have a Web API that can describe itself in Swagger metada\", \"ta, your API can be used seamlessly from Swagger-based tools, including client proxy-class code gene\", \"rators that can target many platforms. For example, as mentioned, [AutoRest](https://github.com/Azur\", \"e/AutoRest) automatically generates .NET client classes. But additional tools like [swagger-codegen]\", \"(https://github.com/swagger-api/swagger-codegen) are also available, which allow code generation of \", \"API client libraries, server stubs, and documentation automatically.\\n\\nCurrently, Swashbuckle consist\", \"s of five internal NuGet packages under the high-level metapackage [Swashbuckle.AspNetCore](https://\", \"www.nuget.org/packages/Swashbuckle.AspNetCore) for ASP.NET Core applications.\\n\\nAfter you have instal\", \"led these NuGet packages in your Web API project, you need to configure Swagger in the *Program.cs* \", \"class, as in the following **simplified** code:\\n\\n```\\n// Add framework services.\\nbuilder.Services.Add\", \"SwaggerGen(options =>\\n{\\n```\\n\\n```\\n options.DescribeAllEnumsAsStrings();\\n options.SwaggerDoc(\\\"v1\\\", new\", \" OpenApiInfo\\n {\\n Title = \\\"eShopOnContainers - Catalog HTTP API\\\",\\n Version = \\\"v1\\\",\\n Description = \\\"Th\", \"e Catalog Microservice HTTP API. This is a Data-Driven/CRUD \\nmicroservice sample\\\"\\n });\\n});\\n// Other \", \"startup code...\\napp.UseSwagger()\\n .UseSwaggerUI(c =>\\n {\\n c.SwaggerEndpoint(\\\"/swagger/v1/swagger.json\", \"\\\", \\\"My API V1\\\");\\n });\\n ```\\n :::\\nOnce this is done, you can start your application and browse the fol\", \"lowing Swagger JSON and \\nUI endpoints using URLs like these:\\n:::{custom-style=CodeBox}\\n```console\\n h\", \"ttp://<your-root-url>/swagger/v1/swagger.json\\n http://<your-root-url>/swagger/\\n```\\n\\nYou previously s\", \"aw the generated UI created by Swashbuckle for a URL like http://<your-rooturl>/swagger. In Figure 6\", \"-9, you can also see how you can test any API method.\\n\\n![](_page_129_Figure_0.jpeg)\\n\\n*Figure 6-9. Sw\", \"ashbuckle UI testing the Catalog/Items API method*\\n\\nThe Swagger UI API detail shows a sample of the \", \"response and can be used to execute the real API, which is great for developer discovery. Figure 6-1\", \"0 shows the Swagger JSON metadata generated from the eShopOnContainers microservice (which is what t\", \"he tools use underneath) when you request http://<your-root-url>/swagger/v1/swagger.json using [Post\", \"man.](https://www.getpostman.com/)\\n\\n![](_page_130_Figure_0.jpeg)\\n\\n*Figure 6-10. Swagger JSON metadat\", \"a*\\n\\nIt is that simple. And because it is automatically generated, the Swagger metadata will grow whe\", \"n you add more functionality to your API.\\n\\n### **Additional resources**\\n\\n- **ASP.NET Web API Help Pa\", \"ges using Swagger** [https://learn.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swag\", \"ger](https://docs.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger)\\n- **Get star\", \"ted with Swashbuckle and ASP.NET Core** [https://learn.microsoft.com/aspnet/core/tutorials/getting-s\", \"tarted-with-swashbuckle](https://docs.microsoft.com/aspnet/core/tutorials/getting-started-with-swash\", \"buckle)\\n- **Get started with NSwag and ASP.NET Core** [https://learn.microsoft.com/aspnet/core/tutor\", \"ials/getting-started-with-nswag](https://docs.microsoft.com/aspnet/core/tutorials/getting-started-wi\", \"th-nswag)\\n\\n## <span id=\\\"page-130-0\\\"></span>Defining your multi-container application with docker-com\", \"pose.yml\\n\\nIn this guide, the [docker-compose.yml](https://docs.docker.com/compose/compose-file/) fil\", \"e was introduced in the section Step 4. Define your services in docker-compose.yml when building a m\", \"ulti-container Docker application. However, there are additional ways to use the docker-compose file\", \"s that are worth exploring in further detail.\\n\\nFor example, you can explicitly describe how you want\", \" to deploy your multi-container application in the docker-compose.yml file. Optionally, you can also\", \" describe how you are going to build your custom Docker images. (Custom Docker images can also be bu\", \"ilt with the Docker CLI.)\\n\\nBasically, you define each of the containers you want to deploy plus cert\", \"ain characteristics for each container deployment. Once you have a multi-container deployment descri\", \"ption file, you can deploy the whole solution in a single action orchestrated by the [docker-compose\", \" up](https://docs.docker.com/compose/overview/) CLI command, or you can deploy it transparently from\", \" Visual Studio. Otherwise, you would need to use the Docker CLI to deploy container-by-container in \", \"multiple steps by using the docker run command from the command line. Therefore, each service define\", \"d in docker-compose.yml must specify exactly one image or build. Other keys are optional, and are an\", \"alogous to their docker run command-line counterparts.\\n\\nThe following YAML code is the definition of\", \" a possible global but single docker-compose.yml file for the eShopOnContainers sample. This code is\", \" not the actual docker-compose file from eShopOnContainers. Instead, it is a simplified and consolid\", \"ated version in a single file, which is not the best way to work with docker-compose files, as will \", \"be explained later.\\n\\n```\\nversion: '3.4'\\nservices:\\n webmvc:\\n image: eshop/webmvc\\n environment:\\n - Cat\", \"alogUrl=http://catalog-api\\n - OrderingUrl=http://ordering-api\\n - BasketUrl=http://basket-api\\n ports:\", \"\\n - \\\"5100:80\\\"\\n depends_on:\\n - catalog-api\\n - ordering-api\\n - basket-api\\n catalog-api:\\n image: eshop/\", \"catalog-api\\n environment:\\n - ConnectionString=Server=sqldata;Initial Catalog=CatalogData;User \\nId=sa\", \";Password=[PLACEHOLDER]\\n expose:\\n - \\\"80\\\"\\n ports:\\n - \\\"5101:80\\\"\\n #extra hosts can be used for standalo\", \"ne SQL Server or services at the dev PC\\n extra_hosts:\\n - \\\"CESARDLSURFBOOK:10.0.75.1\\\"\\n depends_on:\\n -\", \" sqldata\\n ordering-api:\\n image: eshop/ordering-api\\n environment:\\n - ConnectionString=Server=sqldata;\", \"Database=Services.OrderingDb;User \\nId=sa;Password=[PLACEHOLDER]\\n ports:\\n - \\\"5102:80\\\"\\n #extra hosts c\", \"an be used for standalone SQL Server or services at the dev PC\\n extra_hosts:\\n - \\\"CESARDLSURFBOOK:10.\", \"0.75.1\\\"\\n depends_on:\\n - sqldata\\n```\\n\\n```\\n basket-api:\\n image: eshop/basket-api\\n environment:\\n - Conn\", \"ectionString=sqldata\\n ports:\\n - \\\"5103:80\\\"\\n depends_on:\\n - sqldata\\n sqldata:\\n environment:\\n - SA_PASS\", \"WORD=[PLACEHOLDER]\\n - ACCEPT_EULA=Y\\n ports:\\n - \\\"5434:1433\\\"\\n basketdata:\\n image: redis\\n```\\n\\nThe root \", \"key in this file is services. Under that key, you define the services you want to deploy and run whe\", \"n you execute the docker-compose up command or when you deploy from Visual Studio by using this dock\", \"er-compose.yml file. In this case, the docker-compose.yml file has multiple services defined, as des\", \"cribed in the following table.\\n\\n| Service name | Description                                        \", \"                                                        |\\n|--------------|--------------------------\", \"----------------------------------------------------------------------------------|\\n| webmvc       |\", \" Container including the ASP.NET Core MVC<br>application consuming the microservices from<br>server-\", \"side C# |\\n| catalog-api  | Container including the Catalog ASP.NET Core<br>Web API microservice     \", \"                                  |\\n| ordering-api | Container including the Ordering ASP.NET<br>Cor\", \"e Web API microservice                                      |\\n| sqldata      | Container running SQL\", \" Server for Linux,<br>holding the microservices databases                             |\\n| basket-api\", \"   | Container with the Basket ASP.NET Core Web<br>API microservice                                 \", \"            |\\n| basketdata   | Container running the REDIS cache service,<br>with the basket databas\", \"e as a REDIS cache                    |\\n\\n### **A simple Web Service API container**\\n\\nFocusing on a s\", \"ingle container, the catalog-api container-microservice has a straightforward definition:\\n\\n```\\n cata\", \"log-api:\\n image: eshop/catalog-api\\n environment:\\n - ConnectionString=Server=sqldata;Initial Catalog=\", \"CatalogData;User \\nId=sa;Password=[PLACEHOLDER]\\n expose:\\n```\\n\\n```\\n - \\\"80\\\"\\n ports:\\n - \\\"5101:80\\\"\\n #extr\", \"a hosts can be used for standalone SQL Server or services at the dev PC\\n extra_hosts:\\n - \\\"CESARDLSUR\", \"FBOOK:10.0.75.1\\\"\\n depends_on:\\n - sqldata\\n```\\n\\nThis containerized service has the following basic con\", \"figuration:\\n\\n- It is based on the custom **eshop/catalog-api** image. For simplicity's sake, there i\", \"s no build: key setting in the file. This means that the image must have been previously built (with\", \" docker build) or have been downloaded (with the docker pull command) from any Docker registry.\\n- It\", \" defines an environment variable named ConnectionString with the connection string to be used by Ent\", \"ity Framework to access the SQL Server instance that contains the catalog data model. In this case, \", \"the same SQL Server container is holding multiple databases. Therefore, you need less memory in your\", \" development machine for Docker. However, you could also deploy one SQL Server container for each mi\", \"croservice database.\\n- The SQL Server name is **sqldata**, which is the same name used for the conta\", \"iner that is running the SQL Server instance for Linux. This is convenient; being able to use this n\", \"ame resolution (internal to the Docker host) will resolve the network address so you don't need to k\", \"now the internal IP for the containers you are accessing from other containers.\\n\\nBecause the connect\", \"ion string is defined by an environment variable, you could set that variable through a different me\", \"chanism and at a different time. For example, you could set a different connection string when deplo\", \"ying to production in the final hosts, or by doing it from your CI/CD pipelines in Azure DevOps Serv\", \"ices or your preferred DevOps system.\\n\\n- It exposes port 80 for internal access to the **catalog-api\", \"** service within the Docker host. The host is currently a Linux VM because it is based on a Docker \", \"image for Linux, but you could configure the container to run on a Windows image instead.\\n- It forwa\", \"rds the exposed port 80 on the container to port 5101 on the Docker host machine (the Linux VM).\\n- I\", \"t links the web service to the **sqldata** service (the SQL Server instance for Linux database runni\", \"ng in a container). When you specify this dependency, the catalog-api container will not start until\", \" the sqldata container has already started; this aspect is important because catalogapi needs to hav\", \"e the SQL Server database up and running first. However, this kind of container dependency is not en\", \"ough in many cases, because Docker checks only at the container level. Sometimes the service (in thi\", \"s case SQL Server) might still not be ready, so it is advisable to implement retry logic with expone\", \"ntial backoff in your client microservices. That way, if a dependency container is not ready for a s\", \"hort time, the application will still be resilient.\\n- It is configured to allow access to external s\", \"ervers: the extra\\\\_hosts setting allows you to access external servers or machines outside of the Do\", \"cker host (that is, outside the default Linux VM,\\n\\nwhich is a development Docker host), such as a lo\", \"cal SQL Server instance on your development PC.\\n\\nThere are also other, more advanced docker-compose.\", \"yml settings that we'll discuss in the following sections.\\n\\n## **Using docker-compose files to targe\", \"t multiple environments**\\n\\nThe docker-compose.\\\\*.yml files are definition files and can be used by m\", \"ultiple infrastructures that understand that format. The most straightforward tool is the docker-com\", \"pose command.\\n\\nTherefore, by using the docker-compose command you can target the following main scen\", \"arios.\\n\\n## **Development environments**\\n\\nWhen you develop applications, it is important to be able t\", \"o run an application in an isolated development environment. You can use the docker-compose CLI comm\", \"and to create that environment or Visual Studio, which uses docker-compose under the covers.\\n\\nThe do\", \"cker-compose.yml file allows you to configure and document all your application's service dependenci\", \"es (other services, cache, databases, queues, etc.). Using the docker-compose CLI command, you can c\", \"reate and start one or more containers for each dependency with a single command (docker-compose up)\", \".\\n\\nThe docker-compose.yml files are configuration files interpreted by Docker engine but also serve \", \"as convenient documentation files about the composition of your multi-container application.\\n\\n### **\", \"Testing environments**\\n\\nAn important part of any continuous deployment (CD) or continuous integratio\", \"n (CI) process are the unit tests and integration tests. These automated tests require an isolated e\", \"nvironment so they are not impacted by the users or any other change in the application's data.\\n\\nWit\", \"h Docker Compose, you can create and destroy that isolated environment very easily in a few commands\", \" from your command prompt or scripts, like the following commands:\\n\\n```\\ndocker-compose -f docker-com\", \"pose.yml -f docker-compose-test.override.yml up -d\\n./run_unit_tests\\ndocker-compose -f docker-compose\", \".yml -f docker-compose-test.override.yml down\\n```\\n\\n### **Production deployments**\\n\\nYou can also use \", \"Compose to deploy to a remote Docker Engine. A typical case is to deploy to a single Docker host ins\", \"tance (like a production VM or server provisioned with [Docker Machine\\\\)](https://docs.docker.com/ma\", \"chine/overview/).\\n\\nIf you are using any other orchestrator (Azure Service Fabric, Kubernetes, etc.),\", \" you might need to add setup and metadata configuration settings like those in docker-compose.yml, b\", \"ut in the format required by the other orchestrator.\\n\\nIn any case, docker-compose is a convenient to\", \"ol and metadata format for development, testing and production workflows, although the production wo\", \"rkflow might vary on the orchestrator you are using.\\n\\n## **Using multiple docker-compose files to ha\", \"ndle several environments**\\n\\nWhen targeting different environments, you should use multiple compose \", \"files. This approach lets you create multiple configuration variants depending on the environment.\\n\\n\", \"## **Overriding the base docker-compose file**\\n\\nYou could use a single docker-compose.yml file as in\", \" the simplified examples shown in previous sections. However, that is not recommended for most appli\", \"cations.\\n\\nBy default, Compose reads two files, a docker-compose.yml and an optional dockercompose.ov\", \"erride.yml file. As shown in Figure 6-11, when you are using Visual Studio and enabling Docker suppo\", \"rt, Visual Studio also creates an additional docker-compose.vs.debug.g.yml file for debugging the ap\", \"plication, you can take a look at this file in folder obj\\\\Docker\\\\ in the main solution folder.\\n\\n![](\", \"_page_135_Picture_5.jpeg)\\n\\n*Figure 6-11. docker-compose files in Visual Studio 2019*\\n\\n### **docker-c\", \"ompose** project file structure:\\n\\n- *.dockerignore* used to ignore files\\n- *docker-compose.yml* used\", \" to compose microservices\\n- *docker-compose.override.yml* used to configure microservices environmen\", \"t\\n\\nYou can edit the docker-compose files with any editor, like Visual Studio Code or Sublime, and ru\", \"n the application with the docker-compose up command.\\n\\nBy convention, the docker-compose.yml file co\", \"ntains your base configuration and other static settings. That means that the service configuration \", \"should not change depending on the deployment environment you are targeting.\\n\\nThe docker-compose.ove\", \"rride.yml file, as its name suggests, contains configuration settings that override the base configu\", \"ration, such as configuration that depends on the deployment environment. You can have multiple over\", \"ride files with different names also. The override files usually contain additional information need\", \"ed by the application but specific to an environment or to a deployment.\\n\\n### **Targeting multiple e\", \"nvironments**\\n\\nA typical use case is when you define multiple compose files so you can target multip\", \"le environments, like production, staging, CI, or development. To support these differences, you can\", \" split your Compose configuration into multiple files, as shown in Figure 6-12.\\n\\n![](_page_136_Figur\", \"e_1.jpeg)\\n\\n*Figure 6-12. Multiple docker-compose files overriding values in the base docker-compose.\", \"yml file*\\n\\nYou can combine multiple docker-compose\\\\*.yml files to handle different environments. You\", \" start with the base docker-compose.yml file. This base file contains the base or static configurati\", \"on settings that do not change depending on the environment. For example, the eShopOnContainers app \", \"has the following docker-compose.yml file (simplified with fewer services) as the base file.\\n\\n```\\n#d\", \"ocker-compose.yml (Base)\\nversion: '3.4'\\nservices:\\n basket-api:\\n image: eshop/basket-api:${TAG:-lates\", \"t}\\n build:\\n context: .\\n dockerfile: src/Services/Basket/Basket.API/Dockerfile\\n depends_on:\\n - basket\", \"data\\n - identity-api\\n - rabbitmq\\n catalog-api:\\n image: eshop/catalog-api:${TAG:-latest}\\n build:\\n con\", \"text: .\\n dockerfile: src/Services/Catalog/Catalog.API/Dockerfile\\n depends_on:\\n - sqldata\\n - rabbitmq\", \"\\n marketing-api:\\n image: eshop/marketing-api:${TAG:-latest}\\n build:\\n context: .\\n dockerfile: src/Ser\", \"vices/Marketing/Marketing.API/Dockerfile\\n depends_on:\\n - sqldata\\n - nosqldata\\n - identity-api\\n - rab\", \"bitmq\\n webmvc:\\n image: eshop/webmvc:${TAG:-latest}\\n```\\n\\n```\\n build:\\n context: .\\n dockerfile: src/Web\", \"/WebMVC/Dockerfile\\n depends_on:\\n - catalog-api\\n - ordering-api\\n - identity-api\\n - basket-api\\n - mark\", \"eting-api\\n sqldata:\\n image: mcr.microsoft.com/mssql/server:2019-latest\\n nosqldata:\\n image: mongo\\n ba\", \"sketdata:\\n image: redis\\n rabbitmq:\\n image: rabbitmq:3-management\\n```\\n\\nThe values in the base docker-\", \"compose.yml file should not change because of different target deployment environments.\\n\\nIf you focu\", \"s on the webmvc service definition, for instance, you can see how that information is much the same \", \"no matter what environment you might be targeting. You have the following information:\\n\\n- The servic\", \"e name: webmvc.\\n- The container's custom image: eshop/webmvc.\\n- The command to build the custom Dock\", \"er image, indicating which Dockerfile to use.\\n- Dependencies on other services, so this container do\", \"es not start until the other dependency containers have started.\\n\\nYou can have additional configurat\", \"ion, but the important point is that in the base dockercompose.yml file, you just want to set the in\", \"formation that is common across environments. Then in the docker-compose.override.yml or similar fil\", \"es for production or staging, you should place configuration that is specific for each environment.\\n\", \"\\nUsually, the docker-compose.override.yml is used for your development environment, as in the follow\", \"ing example from eShopOnContainers:\\n\\n```\\n#docker-compose.override.yml (Extended config for DEVELOPME\", \"NT env.)\\nversion: '3.4'\\nservices:\\n# Simplified number of services here:\\n basket-api:\\n environment:\\n \", \"- ASPNETCORE_ENVIRONMENT=Development\\n - ASPNETCORE_URLS=http://0.0.0.0:80\\n - ConnectionString=${ESHO\", \"P_AZURE_REDIS_BASKET_DB:-basketdata}\\n - identityUrl=http://identity-api\\n - IdentityUrlExternal=http:\", \"//${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5105\\n```\\n\\n```\\n - EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-ra\", \"bbitmq}\\n - EventBusUserName=${ESHOP_SERVICE_BUS_USERNAME}\\n - EventBusPassword=${ESHOP_SERVICE_BUS_PA\", \"SSWORD}\\n - AzureServiceBusEnabled=False\\n - ApplicationInsights__InstrumentationKey=${INSTRUMENTATION\", \"_KEY}\\n - OrchestratorType=${ORCHESTRATOR_TYPE}\\n - UseLoadTest=${USE_LOADTEST:-False}\\n ports:\\n - \\\"510\", \"3:80\\\"\\n catalog-api:\\n environment:\\n - ASPNETCORE_ENVIRONMENT=Development\\n - ASPNETCORE_URLS=http://0.\", \"0.0.0:80\\n - ConnectionString=${ESHOP_AZURE_CATALOG_DB:-\\nServer=sqldata;Database=Microsoft.eShopOnCon\", \"tainers.Services.CatalogDb;User \\nId=sa;Password=[PLACEHOLDER]}\\n - PicBaseUrl=${ESHOP_AZURE_STORAGE_C\", \"ATALOG_URL:-\\nhttp://host.docker.internal:5202/api/v1/catalog/items/[0]/pic/}\\n - EventBusConnection=$\", \"{ESHOP_AZURE_SERVICE_BUS:-rabbitmq}\\n - EventBusUserName=${ESHOP_SERVICE_BUS_USERNAME}\\n - EventBusPas\", \"sword=${ESHOP_SERVICE_BUS_PASSWORD}\\n - AzureStorageAccountName=${ESHOP_AZURE_STORAGE_CATALOG_NAME}\\n \", \"- AzureStorageAccountKey=${ESHOP_AZURE_STORAGE_CATALOG_KEY}\\n - UseCustomizationData=True\\n - AzureSer\", \"viceBusEnabled=False\\n - AzureStorageEnabled=False\\n - ApplicationInsights__InstrumentationKey=${INSTR\", \"UMENTATION_KEY}\\n - OrchestratorType=${ORCHESTRATOR_TYPE}\\n ports:\\n - \\\"5101:80\\\"\\n marketing-api:\\n envir\", \"onment:\\n - ASPNETCORE_ENVIRONMENT=Development\\n - ASPNETCORE_URLS=http://0.0.0.0:80\\n - ConnectionStri\", \"ng=${ESHOP_AZURE_MARKETING_DB:-\\nServer=sqldata;Database=Microsoft.eShopOnContainers.Services.Marketi\", \"ngDb;User \\nId=sa;Password=[PLACEHOLDER]}\\n - MongoConnectionString=${ESHOP_AZURE_COSMOSDB:-mongodb://\", \"nosqldata}\\n - MongoDatabase=MarketingDb\\n - EventBusConnection=${ESHOP_AZURE_SERVICE_BUS:-rabbitmq}\\n \", \"- EventBusUserName=${ESHOP_SERVICE_BUS_USERNAME}\\n - EventBusPassword=${ESHOP_SERVICE_BUS_PASSWORD}\\n \", \"- identityUrl=http://identity-api\\n - IdentityUrlExternal=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:510\", \"5\\n - CampaignDetailFunctionUri=${ESHOP_AZUREFUNC_CAMPAIGN_DETAILS_URI}\\n - PicBaseUrl=${ESHOP_AZURE_S\", \"TORAGE_MARKETING_URL:-\\nhttp://host.docker.internal:5110/api/v1/campaigns/[0]/pic/}\\n - AzureStorageAc\", \"countName=${ESHOP_AZURE_STORAGE_MARKETING_NAME}\\n - AzureStorageAccountKey=${ESHOP_AZURE_STORAGE_MARK\", \"ETING_KEY}\\n - AzureServiceBusEnabled=False\\n - AzureStorageEnabled=False\\n - ApplicationInsights__Inst\", \"rumentationKey=${INSTRUMENTATION_KEY}\\n - OrchestratorType=${ORCHESTRATOR_TYPE}\\n - UseLoadTest=${USE_\", \"LOADTEST:-False}\\n ports:\\n - \\\"5110:80\\\"\\n webmvc:\\n```\\n\\n```\\n environment:\\n - ASPNETCORE_ENVIRONMENT=Deve\", \"lopment\\n - ASPNETCORE_URLS=http://0.0.0.0:80\\n - PurchaseUrl=http://webshoppingapigw\\n - IdentityUrl=h\", \"ttp://10.0.75.1:5105\\n - MarketingUrl=http://webmarketingapigw\\n - CatalogUrlHC=http://catalog-api/hc\\n\", \" - OrderingUrlHC=http://ordering-api/hc\\n - IdentityUrlHC=http://identity-api/hc\\n - BasketUrlHC=http:\", \"//basket-api/hc\\n - MarketingUrlHC=http://marketing-api/hc\\n - PaymentUrlHC=http://payment-api/hc\\n - S\", \"ignalrHubUrl=http://${ESHOP_EXTERNAL_DNS_NAME_OR_IP}:5202\\n - UseCustomizationData=True\\n - Applicatio\", \"nInsights__InstrumentationKey=${INSTRUMENTATION_KEY}\\n - OrchestratorType=${ORCHESTRATOR_TYPE}\\n - Use\", \"LoadTest=${USE_LOADTEST:-False}\\n ports:\\n - \\\"5100:80\\\"\\n sqldata:\\n environment:\\n - SA_PASSWORD=[PLACEHO\", \"LDER]\\n - ACCEPT_EULA=Y\\n ports:\\n - \\\"5433:1433\\\"\\n nosqldata:\\n ports:\\n - \\\"27017:27017\\\"\\n basketdata:\\n por\", \"ts:\\n - \\\"6379:6379\\\"\\n rabbitmq:\\n ports:\\n - \\\"15672:15672\\\"\\n - \\\"5672:5672\\\"\\n```\\n\\nIn this example, the deve\", \"lopment override configuration exposes some ports to the host, defines environment variables with re\", \"direct URLs, and specifies connection strings for the development environment. These settings are al\", \"l just for the development environment.\\n\\nWhen you run docker-compose up (or launch it from Visual St\", \"udio), the command reads the overrides automatically as if it were merging both files.\\n\\nSuppose that\", \" you want another Compose file for the production environment, with different configuration values, \", \"ports, or connection strings. You can create another override file, like file named docker-compose.p\", \"rod.yml with different settings and environment variables. That file might be stored in a different \", \"Git repo or managed and secured by a different team.\\n\\n### **How to deploy with a specific override f\", \"ile**\\n\\nTo use multiple override files, or an override file with a different name, you can use the -f\", \" option with the docker-compose command and specify the files. Compose merges files in the order the\", \"y are specified on the command line. The following example shows how to deploy with override files.\\n\", \"\\n```\\ndocker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\\n```\\n\\n## **Using environme\", \"nt variables in docker-compose files**\\n\\nIt is convenient, especially in production environments, to \", \"be able to get configuration information from environment variables, as we have shown in previous ex\", \"amples. You can reference an environment variable in your docker-compose files using the syntax \\\\${M\", \"Y\\\\_VAR}. The following line from a docker-compose.prod.yml file shows how to reference the value of \", \"an environment variable.\\n\\n```\\nIdentityUrl=http://${ESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP}:5105\\n```\\n\\nEnv\", \"ironment variables are created and initialized in different ways, depending on your host environment\", \" (Linux, Windows, Cloud cluster, etc.). However, a convenient approach is to use an .env file. The d\", \"ocker-compose files support declaring default environment variables in the .env file. These values f\", \"or the environment variables are the default values. But they can be overridden by the values you mi\", \"ght have defined in each of your environments (host OS or environment variables from your cluster). \", \"You place this .env file in the folder where the docker-compose command is executed from.\\n\\nThe follo\", \"wing example shows an .env file like the [.env](https://github.com/dotnet-architecture/eShopOnContai\", \"ners/blob/main/src/.env) file for the eShopOnContainers application.\\n\\n```\\n# .env file\\nESHOP_EXTERNAL\", \"_DNS_NAME_OR_IP=host.docker.internal\\nESHOP_PROD_EXTERNAL_DNS_NAME_OR_IP=10.121.122.92\\n```\\n\\nDocker-co\", \"mpose expects each line in an .env file to be in the format <variable>=<value>.\\n\\nThe values set in t\", \"he run-time environment always override the values defined inside the .env file. In a similar way, v\", \"alues passed via command-line arguments also override the default values set in the .env file.\\n\\n### \", \"**Additional resources**\\n\\n- **Overview of Docker Compose** <https://docs.docker.com/compose/overview\", \"/>\\n- **Multiple Compose files** <https://docs.docker.com/compose/multiple-compose-files/>\\n\\n### **Bui\", \"lding optimized ASP.NET Core Docker images**\\n\\nIf you are exploring Docker and .NET on sources on the\", \" Internet, you will find Dockerfiles that demonstrate the simplicity of building a Docker image by c\", \"opying your source into a container. These examples suggest that by using a simple configuration, yo\", \"u can have a Docker image with the environment packaged with your application. The following example\", \" shows a simple Dockerfile in this vein.\\n\\n```\\nFROM mcr.microsoft.com/dotnet/sdk:7.0\\nWORKDIR /app\\nENV\", \" ASPNETCORE_URLS http://+:80\\nEXPOSE 80\\nCOPY . .\\nRUN dotnet restore\\nENTRYPOINT [\\\"dotnet\\\", \\\"run\\\"]\\n```\\n\", \"\\nA Dockerfile like this will work. However, you can substantially optimize your images, especially y\", \"our production images.\\n\\nIn the container and microservices model, you are constantly starting contai\", \"ners. The typical way of using containers does not restart a sleeping container, because the contain\", \"er is disposable. Orchestrators (like Kubernetes and Azure Service Fabric) create new instances of i\", \"mages. What this means is that you would need to optimize by precompiling the application when it is\", \" built so the instantiation process will be faster. When the container is started, it should be read\", \"y to run. Don't restore and compile at run time using the dotnet restore and dotnet build CLI comman\", \"ds as you may see in blog posts about .NET and Docker.\\n\\nThe .NET team has been doing important work \", \"to make .NET and ASP.NET Core a container-optimized framework. Not only is .NET a lightweight framew\", \"ork with a small memory footprint; the team has focused on optimized Docker images for three main sc\", \"enarios and published them in the Docker Hub registry at *dotnet/*, beginning with version 2.1:\\n\\n- 1\", \". **Development**: The priority is the ability to quickly iterate and debug changes, and where size \", \"is secondary.\\n- 2. **Build**: The priority is compiling the application, and the image includes bina\", \"ries and other dependencies to optimize binaries.\\n- 3. **Production**: The focus is fast deploying a\", \"nd starting of containers, so these images are limited to the binaries and content needed to run the\", \" application.\\n\\nThe .NET team provides four basic variants in [dotnet/](https://hub.docker.com/_/micr\", \"osoft-dotnet/) (at Docker Hub):\\n\\n- 1. **sdk**: for development and build scenarios\\n- 2. **aspnet**: \", \"for ASP.NET production scenarios\\n- 3. **runtime**: for .NET production scenarios\\n- 4. **runtime-deps\", \"**: for production scenarios of [self-contained applications](https://docs.microsoft.com/dotnet/core\", \"/deploying/index#publish-self-contained)\\n\\nFor faster startup, runtime images also automatically set \", \"aspnetcore\\\\_urls to port 80 and use Ngen to create a native image cache of assemblies.\\n\\n### **Additi\", \"onal resources**\\n\\n- **Building Optimized Docker Images with ASP.NET Core** [https://learn.microsoft.\", \"com/archive/blogs/stevelasker/building-optimized-docker-images](https://docs.microsoft.com/archive/b\", \"logs/stevelasker/building-optimized-docker-images-with-asp-net-core)[with-asp-net-core](https://docs\", \".microsoft.com/archive/blogs/stevelasker/building-optimized-docker-images-with-asp-net-core)\\n- **Bui\", \"lding Docker Images for .NET Applications** [https://learn.microsoft.com/dotnet/core/docker/building\", \"-net-docker-images](https://docs.microsoft.com/aspnet/core/host-and-deploy/docker/building-net-docke\", \"r-images)\\n\\n## <span id=\\\"page-141-0\\\"></span>Use a database server running as a container\\n\\nYou can hav\", \"e your databases (SQL Server, PostgreSQL, MySQL, etc.) on regular standalone servers, in on-premises\", \" clusters, or in PaaS services in the cloud like Azure SQL DB. However, for development and test env\", \"ironments, having your databases running as containers is convenient, because you don't have any ext\", \"ernal dependency and simply running the docker-compose up command starts the whole application. Havi\", \"ng those databases as containers is also great for integration tests, because the database is starte\", \"d in the container and is always populated with the same sample data, so tests can be more predictab\", \"le.\\n\\n## <span id=\\\"page-142-0\\\"></span>**SQL Server running as a container with a microservice-related\", \" database**\\n\\nIn eShopOnContainers, there's a container named sqldata, as defined in the [docker-comp\", \"ose.yml](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/docker-compose.yml) \", \"file, that runs a SQL Server for Linux instance with the SQL databases for all microservices that ne\", \"ed one.\\n\\nA key point in microservices is that each microservice owns its related data, so it should \", \"have its own database. However, the databases can be anywhere. In this case, they are all in the sam\", \"e container to keep Docker memory requirements as low as possible. Keep in mind that this is a good-\", \"enough solution for development and, perhaps, testing but not for production.\\n\\nThe SQL Server contai\", \"ner in the sample application is configured with the following YAML code in the docker-compose.yml f\", \"ile, which is executed when you run docker-compose up. Note that the YAML code has consolidated conf\", \"iguration information from the generic docker-compose.yml file and the docker-compose.override.yml f\", \"ile. (Usually you would separate the environment settings from the base or static information relate\", \"d to the SQL Server image.)\\n\\n```\\n sqldata:\\n image: mcr.microsoft.com/mssql/server:2017-latest\\n envir\", \"onment:\\n - SA_PASSWORD=Pass@word\\n - ACCEPT_EULA=Y\\n ports:\\n - \\\"5434:1433\\\"\\n```\\n\\nIn a similar way, inst\", \"ead of using docker-compose, the following docker run command can run that container:\\n\\n```\\ndocker ru\", \"n -e 'ACCEPT_EULA=Y' -e 'SA_PASSWORD=Pass@word' -p 5433:1433 -d \\nmcr.microsoft.com/mssql/server:2017\", \"-latest\\n```\\n\\nHowever, if you are deploying a multi-container application like eShopOnContainers, it \", \"is more convenient to use the docker-compose up command so that it deploys all the required containe\", \"rs for the application.\\n\\nWhen you start this SQL Server container for the first time, the container \", \"initializes SQL Server with the password that you provide. Once SQL Server is running as a container\", \", you can update the database by connecting through any regular SQL connection, such as from SQL Ser\", \"ver Management Studio, Visual Studio, or C# code.\\n\\nThe eShopOnContainers application initializes eac\", \"h microservice database with sample data by seeding it with data on startup, as explained in the fol\", \"lowing section.\\n\\nHaving SQL Server running as a container is not just useful for a demo where you mi\", \"ght not have access to an instance of SQL Server. As noted, it is also great for development and tes\", \"ting\\n\\nenvironments so that you can easily run integration tests starting from a clean SQL Server ima\", \"ge and known data by seeding new sample data.\\n\\n## **Additional resources**\\n\\n- **Run the SQL Server D\", \"ocker image on Linux, Mac, or Windows** [https://learn.microsoft.com/sql/linux/sql-server-linux-setu\", \"p-docker](https://docs.microsoft.com/sql/linux/sql-server-linux-setup-docker)\\n- **Connect and query \", \"SQL Server on Linux with sqlcmd** [https://learn.microsoft.com/sql/linux/sql-server-linux-connect-an\", \"d-query-sqlcmd](https://docs.microsoft.com/sql/linux/sql-server-linux-connect-and-query-sqlcmd)\\n\\n## \", \"<span id=\\\"page-143-0\\\"></span>**Seeding with test data on Web application startup**\\n\\nTo add data to t\", \"he database when the application starts up, you can add code like the following to the Main method i\", \"n the Program class of the Web API project:\\n\\n```\\npublic static int Main(string[] args)\\n{\\n var config\", \"uration = GetConfiguration();\\n Log.Logger = CreateSerilogLogger(configuration);\\n try\\n {\\n Log.Informa\", \"tion(\\\"Configuring web host ({ApplicationContext})...\\\", AppName);\\n var host = CreateHostBuilder(confi\", \"guration, args);\\n Log.Information(\\\"Applying migrations ({ApplicationContext})...\\\", AppName);\\n host.M\", \"igrateDbContext<CatalogContext>((context, services) =>\\n {\\n var env = services.GetService<IWebHostEnv\", \"ironment>();\\n var settings = services.GetService<IOptions<CatalogSettings>>();\\n var logger = service\", \"s.GetService<ILogger<CatalogContextSeed>>();\\n new CatalogContextSeed()\\n .SeedAsync(context, env, set\", \"tings, logger)\\n .Wait();\\n })\\n .MigrateDbContext<IntegrationEventLogContext>((_, __) => { });\\n Log.In\", \"formation(\\\"Starting web host ({ApplicationContext})...\\\", AppName);\\n host.Run();\\n return 0;\\n }\\n catch\", \" (Exception ex)\\n {\\n Log.Fatal(ex, \\\"Program terminated unexpectedly ({ApplicationContext})!\\\", AppName\", \");\\n return 1;\\n }\\n finally\\n {\\n Log.CloseAndFlush();\\n }\\n}\\n```\\n\\nThere's an important caveat when applyi\", \"ng migrations and seeding a database during container startup. Since the database server might not b\", \"e available for whatever reason, you must handle retries while waiting for the server to be availabl\", \"e. This retry logic is handled by the MigrateDbContext() extension method, as shown in the following\", \" code:\\n\\n```\\npublic static IWebHost MigrateDbContext<TContext>(\\n this IWebHost host,\\n Action<TContext\", \",\\n IServiceProvider> seeder)\\n where TContext : DbContext\\n{\\n var underK8s = host.IsInKubernetes();\\n u\", \"sing (var scope = host.Services.CreateScope())\\n {\\n var services = scope.ServiceProvider;\\n var logger\", \" = services.GetRequiredService<ILogger<TContext>>();\\n var context = services.GetService<TContext>();\", \"\\n try\\n {\\n logger.LogInformation(\\\"Migrating database associated with context \\n{DbContextName}\\\", typeo\", \"f(TContext).Name);\\n if (underK8s)\\n {\\n InvokeSeeder(seeder, context, services);\\n }\\n else\\n {\\n var retr\", \"y = Policy.Handle<SqlException>()\\n .WaitAndRetry(new TimeSpan[]\\n {\\n               TimeSpan.FromSecon\", \"ds(3),\\n TimeSpan.FromSeconds(5),\\n TimeSpan.FromSeconds(8),\\n });\\n //if the sql server container is no\", \"t created on run docker compose this\\n //migration can't fail for network related exception. The retr\", \"y options for \\nDbContext only\\n //apply to transient exceptions\\n // Note that this is NOT applied whe\", \"n running some orchestrators (let the \\norchestrator to recreate the failing service)\\n retry.Execute(\", \"() => InvokeSeeder(seeder, context, services));\\n }\\n logger.LogInformation(\\\"Migrated database associa\", \"ted with context \\n{DbContextName}\\\", typeof(TContext).Name);\\n }\\n catch (Exception ex)\\n {\\n logger.LogE\", \"rror(ex, \\\"An error occurred while migrating the database used on \\ncontext {DbContextName}\\\", typeof(T\", \"Context).Name);\\n if (underK8s)\\n {\\n throw; // Rethrow under k8s because we rely on k8s to re-run the\\n\", \"```\\n\\n```\\npod\\n }\\n }\\n }\\n return host;\\n}\\n```\\n\\nThe following code in the custom CatalogContextSeed class\", \" populates the data.\\n\\n```\\npublic class CatalogContextSeed\\n{\\n public static async Task SeedAsync(IApp\", \"licationBuilder applicationBuilder)\\n {\\n var context = (CatalogContext)applicationBuilder\\n .Applicati\", \"onServices.GetService(typeof(CatalogContext));\\n using (context)\\n {\\n context.Database.Migrate();\\n if \", \"(!context.CatalogBrands.Any())\\n {\\n context.CatalogBrands.AddRange(\\n GetPreconfiguredCatalogBrands())\", \";\\n await context.SaveChangesAsync();\\n }\\n if (!context.CatalogTypes.Any())\\n {\\n context.CatalogTypes.A\", \"ddRange(\\n GetPreconfiguredCatalogTypes());\\n await context.SaveChangesAsync();\\n }\\n }\\n }\\n static IEnum\", \"erable<CatalogBrand> GetPreconfiguredCatalogBrands()\\n {\\n return new List<CatalogBrand>()\\n {\\n new Cat\", \"alogBrand() { Brand = \\\"Azure\\\"},\\n new CatalogBrand() { Brand = \\\".NET\\\" },\\n new CatalogBrand() { Brand \", \"= \\\"Visual Studio\\\" },\\n new CatalogBrand() { Brand = \\\"SQL Server\\\" }\\n };\\n }\\n static IEnumerable<Catalog\", \"Type> GetPreconfiguredCatalogTypes()\\n {\\n return new List<CatalogType>()\\n {\\n new CatalogType() { Type\", \" = \\\"Mug\\\"},\\n new CatalogType() { Type = \\\"T-Shirt\\\" },\\n new CatalogType() { Type = \\\"Backpack\\\" },\\n new C\", \"atalogType() { Type = \\\"USB Memory Stick\\\" }\\n };\\n }\\n}\\n```\\n\\nWhen you run integration tests, having a wa\", \"y to generate data consistent with your integration tests is useful. Being able to create everything\", \" from scratch, including an instance of SQL Server running on a container, is great for test environ\", \"ments.\\n\\n## <span id=\\\"page-146-0\\\"></span>**EF Core InMemory database versus SQL Server running as a c\", \"ontainer**\\n\\nAnother good choice when running tests is to use the Entity Framework InMemory database \", \"provider. You can specify that configuration in the ConfigureServices method of the Startup class in\", \" your Web API project:\\n\\n```\\npublic class Startup\\n{\\n // Other Startup code ...\\n public void Configure\", \"Services(IServiceCollection services)\\n {\\n services.AddSingleton<IConfiguration>(Configuration);\\n // \", \"DbContext using an InMemory database provider\\n services.AddDbContext<CatalogContext>(opt => opt.UseI\", \"nMemoryDatabase());\\n //(Alternative: DbContext using a SQL Server provider\\n //services.AddDbContext<\", \"CatalogContext>(c =>\\n //{\\n // c.UseSqlServer(Configuration[\\\"ConnectionString\\\"]);\\n //\\n //});\\n }\\n // O\", \"ther Startup code ...\\n}\\n```\\n\\nThere is an important catch, though. The in-memory database does not su\", \"pport many constraints that are specific to a particular database. For instance, you might add a uni\", \"que index on a column in your EF Core model and write a test against your in-memory database to chec\", \"k that it does not let you add a duplicate value. But when you are using the in-memory database, you\", \" cannot handle unique indexes on a column. Therefore, the in-memory database does not behave exactly\", \" the same as a real SQL Server database\\u2014it does not emulate database-specific constraints.\\n\\nEven so,\", \" an in-memory database is still useful for testing and prototyping. But if you want to create accura\", \"te integration tests that take into account the behavior of a specific database implementation, you \", \"need to use a real database like SQL Server. For that purpose, running SQL Server in a container is \", \"a great choice and more accurate than the EF Core InMemory database provider.\\n\\n### <span id=\\\"page-14\", \"6-1\\\"></span>**Using a Redis cache service running in a container**\\n\\nYou can run Redis on a container\", \", especially for development and testing and for proof-of-concept scenarios. This scenario is conven\", \"ient, because you can have all your dependencies running on containers\\u2014not just for your local devel\", \"opment machines, but for your testing environments in your CI/CD pipelines.\\n\\nHowever, when you run R\", \"edis in production, it is better to look for a high-availability solution like Redis Microsoft Azure\", \", which runs as a PaaS (Platform as a Service). In your code, you just need to change your connectio\", \"n strings.\\n\\nRedis provides a Docker image with Redis. That image is available from Docker Hub at thi\", \"s URL: [https://hub.docker.com/\\\\\\\\_/redis/](https://hub.docker.com/_/redis/)\\n\\nYou can directly run a \", \"Docker Redis container by executing the following Docker CLI command in your command prompt:\\n\\n```\\ndo\", \"cker run --name some-redis -d redis\\n```\\n\\nThe Redis image includes expose:6379 (the port used by Redi\", \"s), so standard container linking will make it automatically available to the linked containers.\\n\\nIn\", \" eShopOnContainers, the basket-api microservice uses a Redis cache running as a container. That bask\", \"etdata container is defined as part of the multi-container *docker-compose.yml* file, as shown in th\", \"e following example:\\n\\n```\\n#docker-compose.yml file\\n#...\\n basketdata:\\n image: redis\\n expose:\\n - \\\"6379\", \"\\\"\\n```\\n\\nThis code in the docker-compose.yml defines a container named basketdata based on the redis i\", \"mage and publishing the port 6379 internally. This configuration means that it will only be accessib\", \"le from other containers running within the Docker host.\\n\\nFinally, in the *docker-compose.override.y\", \"ml* file, the basket-api microservice for the eShopOnContainers sample defines the connection string\", \" to use for that Redis container:\\n\\n```\\n basket-api:\\n environment:\\n # Other data ...\\n - ConnectionStr\", \"ing=basketdata\\n - EventBusConnection=rabbitmq\\n```\\n\\nAs mentioned before, the name of the microservice\", \" basketdata is resolved by Docker's internal network DNS.\\n\\n## <span id=\\\"page-147-0\\\"></span>Implement\", \"ing event-based communication between microservices (integration events)\\n\\nAs described earlier, when\", \" you use event-based communication, a microservice publishes an event when something notable happens\", \", such as when it updates a business entity. Other microservices subscribe to those events. When a m\", \"icroservice receives an event, it can update its own business entities, which might lead to more eve\", \"nts being published. This is the essence of the eventual consistency concept. This publish/subscribe\", \" system is usually performed by using an implementation of an event bus. The event bus can be design\", \"ed as an interface with the API needed to subscribe and unsubscribe to events and to publish events.\", \" It can also have one or more implementations based on any inter-process or messaging communication,\", \" such as a messaging queue or a service bus that supports asynchronous communication and a publish/s\", \"ubscribe model.\\n\\nYou can use events to implement business transactions that span multiple services, \", \"which give you eventual consistency between those services. An eventually consistent transaction con\", \"sists of a series of distributed actions. At each action, the microservice updates a business entity\", \" and publishes an event that triggers the next action. Figure 6-18 below, shows a PriceUpdated event\", \" published through an event bus, so the price update is propagated to the Basket and other microserv\", \"ices.\\n\\n![](_page_148_Figure_2.jpeg)\\n\\n*Figure 6-18. Event-driven communication based on an event bus*\", \"\\n\\nThis section describes how you can implement this type of communication with .NET by using a gener\", \"ic event bus interface, as shown in Figure 6-18. There are multiple potential implementations, each \", \"using a different technology or infrastructure such as RabbitMQ, Azure Service Bus, or any other thi\", \"rd-party open-source or commercial service bus.\\n\\n### <span id=\\\"page-148-0\\\"></span>**Using message br\", \"okers and service buses for production systems**\\n\\nAs noted in the architecture section, you can choo\", \"se from multiple messaging technologies for implementing your abstract event bus. But these technolo\", \"gies are at different levels. For instance, RabbitMQ, a messaging broker transport, is at a lower le\", \"vel than commercial products like Azure Service Bus, NServiceBus, MassTransit, or Brighter. Most of \", \"these products can work on top of either RabbitMQ or Azure Service Bus. Your choice of product depen\", \"ds on how many features and how much out-of-the-box scalability you need for your application.\\n\\nFor \", \"implementing just an event bus proof-of-concept for your development environment, as in the eShopOnC\", \"ontainers sample, a simple implementation on top of RabbitMQ running as a container might be enough.\", \" But for mission-critical and production systems that need high scalability, you might want to evalu\", \"ate and use Azure Service Bus.\\n\\nIf you require high-level abstractions and richer features like [Sag\", \"as](https://docs.particular.net/nservicebus/sagas/) for long-running processes that make distributed\", \" development easier, other commercial and open-source service buses like NServiceBus, MassTransit, a\", \"nd Brighter are worth evaluating. In this case, the abstractions and API to use would usually be dir\", \"ectly the ones provided by those high-level service buses instead of your own abstractions (like the\", \" [simple event bus abstractions provided at eShopOnContainers\\\\)](https://github.com/dotnet-architect\", \"ure/eShopOnContainers/blob/dev/src/BuildingBlocks/EventBus/EventBus/Abstractions/IEventBus.cs). For \", \"that matter,\\n\\nyou can research the [forked eShopOnContainers using NServiceBus](https://go.particula\", \"r.net/eShopOnContainers) (additional derived sample implemented by Particular Software).\\n\\nOf course,\", \" you could always build your own service bus features on top of lower-level technologies like Rabbit\", \"MQ and Docker, but the work needed to \\\"reinvent the wheel\\\" might be too costly for a custom enterpri\", \"se application.\\n\\nTo reiterate: the sample event bus abstractions and implementation showcased in the\", \" eShopOnContainers sample are intended to be used only as a proof of concept. Once you have decided \", \"that you want to have asynchronous and event-driven communication, as explained in the current secti\", \"on, you should choose the service bus product that best fits your needs for production.\\n\\n## <span id\", \"=\\\"page-149-0\\\"></span>**Integration events**\\n\\nIntegration events are used for bringing domain state i\", \"n sync across multiple microservices or external systems. This functionality is done by publishing i\", \"ntegration events outside the microservice. When an event is published to multiple receiver microser\", \"vices (to as many microservices as are subscribed to the integration event), the appropriate event h\", \"andler in each receiver microservice handles the event.\\n\\nAn integration event is basically a data-ho\", \"lding class, as in the following example:\\n\\n```\\npublic class ProductPriceChangedIntegrationEvent : In\", \"tegrationEvent\\n{\\n public int ProductId { get; private set; }\\n public decimal NewPrice { get; private\", \" set; }\\n public decimal OldPrice { get; private set; }\\n public ProductPriceChangedIntegrationEvent(i\", \"nt productId, decimal newPrice,\\n decimal oldPrice)\\n {\\n ProductId = productId;\\n NewPrice = newPrice;\\n\", \" OldPrice = oldPrice;\\n }\\n}\\n```\\n\\nThe integration events can be defined at the application level of ea\", \"ch microservice, so they are decoupled from other microservices, in a way comparable to how ViewMode\", \"ls are defined in the server and client. What is not recommended is sharing a common integration eve\", \"nts library across multiple microservices; doing that would be coupling those microservices with a s\", \"ingle event definition data library. You do not want to do that for the same reasons that you do not\", \" want to share a common domain model across multiple microservices: microservices must be completely\", \" autonomous. For more information, see this blog post on [the amount of data to put in events.](http\", \"s://particular.net/blog/putting-your-events-on-a-diet) Be careful not to take this too far, as this \", \"other blog post describes [the problem data deficient messages](https://ardalis.com/data-deficient-m\", \"essages/)  [can produce](https://ardalis.com/data-deficient-messages/). Your design of your events s\", \"hould aim to be \\\"just right\\\" for the needs of their consumers.\\n\\nThere are only a few kinds of librar\", \"ies you should share across microservices. One is libraries that are final application blocks, like \", \"the [Event Bus client API,](https://github.com/dotnet-architecture/eShopOnContainers/tree/main/src/B\", \"uildingBlocks/EventBus) as in eShopOnContainers. Another is libraries that constitute tools that cou\", \"ld also be shared as NuGet components, like JSON serializers.\\n\\n## <span id=\\\"page-150-0\\\"></span>**The\", \" event bus**\\n\\nAn event bus allows publish/subscribe-style communication between microservices withou\", \"t requiring the components to explicitly be aware of each other, as shown in Figure 6-19.\\n\\n![](_page\", \"_150_Figure_2.jpeg)\\n\\n*Figure 6-19. Publish/subscribe basics with an event bus*\\n\\nThe above diagram sh\", \"ows that microservice A publishes to Event Bus, which distributes to subscribing microservices B and\", \" C, without the publisher needing to know the subscribers. The event bus is related to the Observer \", \"pattern and the publish-subscribe pattern.\\n\\n### **Observer pattern**\\n\\nIn the [Observer pattern,](htt\", \"ps://en.wikipedia.org/wiki/Observer_pattern) your primary object (known as the Observable) notifies \", \"other interested objects (known as Observers) with relevant information (events).\\n\\n### **Publish/Sub\", \"scribe (Pub/Sub) pattern**\\n\\nThe purpose of the [Publish/Subscribe pattern](https://docs.microsoft.co\", \"m/previous-versions/msp-n-p/ff649664(v=pandp.10)) is the same as the Observer pattern: you want to n\", \"otify other services when certain events take place. But there is an important difference between th\", \"e Observer and Pub/Sub patterns. In the observer pattern, the broadcast is performed directly from t\", \"he observable to the observers, so they \\\"know\\\" each other. But when using a Pub/Sub pattern, there i\", \"s a third component, called broker, or message broker or event bus, which is known by both the publi\", \"sher and subscriber. Therefore, when using the Pub/Sub pattern the publisher and the subscribers are\", \" precisely decoupled thanks to the mentioned event bus or message broker.\\n\\n### **The middleman or ev\", \"ent bus**\\n\\nHow do you achieve anonymity between publisher and subscriber? An easy way is let a middl\", \"eman take care of all the communication. An event bus is one such middleman.\\n\\nAn event bus is typica\", \"lly composed of two parts:\\n\\n- The abstraction or interface.\\n- One or more implementations.\\n\\nIn Figur\", \"e 6-19 you can see how, from an application point of view, the event bus is nothing more than a Pub/\", \"Sub channel. The way you implement this asynchronous communication can vary. It can have multiple im\", \"plementations so that you can swap between them, depending on the environment requirements (for exam\", \"ple, production versus development environments).\\n\\nIn Figure 6-20, you can see an abstraction of an \", \"event bus with multiple implementations based on infrastructure messaging technologies like RabbitMQ\", \", Azure Service Bus, or another event/message broker.\\n\\n![](_page_151_Picture_2.jpeg)\\n\\n*Figure 6- 20.\", \" Multiple implementations of an event bus*\\n\\nIt's good to have the event bus defined through an inter\", \"face so it can be implemented with several technologies, like RabbitMQ, Azure Service bus or others.\", \" However, and as mentioned previously, using your own abstractions (the event bus interface) is good\", \" only if you need basic event bus features supported by your abstractions. If you need richer servic\", \"e bus features, you should probably use the API and abstractions provided by your preferred commerci\", \"al service bus instead of your own abstractions.\\n\\n### **Defining an event bus interface**\\n\\nLet's sta\", \"rt with some implementation code for the event bus interface and possible implementations for explor\", \"ation purposes. The interface should be generic and straightforward, as in the following interface.\\n\", \"\\n```\\npublic interface IEventBus\\n{\\n void Publish(IntegrationEvent @event);\\n void Subscribe<T, TH>()\\n \", \"where T : IntegrationEvent\\n where TH : IIntegrationEventHandler<T>;\\n void SubscribeDynamic<TH>(strin\", \"g eventName)\\n where TH : IDynamicIntegrationEventHandler;\\n```\\n\\n```\\n void UnsubscribeDynamic<TH>(stri\", \"ng eventName)\\n where TH : IDynamicIntegrationEventHandler;\\n void Unsubscribe<T, TH>()\\n where TH : II\", \"ntegrationEventHandler<T>\\n where T : IntegrationEvent;\\n}\\n```\\n\\nThe Publish method is straightforward.\", \" The event bus will broadcast the integration event passed to it to any microservice, or even an ext\", \"ernal application, subscribed to that event. This method is used by the microservice that is publish\", \"ing the event.\\n\\nThe Subscribe methods (you can have several implementations depending on the argumen\", \"ts) are used by the microservices that want to receive events. This method has two arguments. The fi\", \"rst is the integration event to subscribe to (IntegrationEvent). The second argument is the integrat\", \"ion event handler (or callback method), named IIntegrationEventHandler<T>, to be executed when the r\", \"eceiver microservice gets that integration event message.\\n\\n### <span id=\\\"page-152-0\\\"></span>**Additi\", \"onal resources**\\n\\nSome production-ready messaging solutions:\\n\\n- **Azure Service Bus** [https://learn\", \".microsoft.com/azure/service-bus-messaging/](https://docs.microsoft.com/azure/service-bus-messaging/\", \")\\n- **NServiceBus** <https://particular.net/nservicebus>\\n- **MassTransit** <https://masstransit-proj\", \"ect.com/>\\n\\n## <span id=\\\"page-152-1\\\"></span>Implementing an event bus with RabbitMQ for the developme\", \"nt or test environment\\n\\nWe should start by saying that if you create your custom event bus based on \", \"[RabbitMQ](https://www.rabbitmq.com/) running in a container, as the eShopOnContainers application d\", \"oes, it should be used only for your development and test environments. Don't use it for your produc\", \"tion environment, unless you are building it as a part of a production-ready service bus as describe\", \"d in the [Additional resources section below.](#page-24-2) A simple custom event bus might be missin\", \"g many production-ready critical features that a commercial service bus has.\\n\\nOne of the event bus c\", \"ustom implementations in eShopOnContainers is basically a library using the RabbitMQ API. (There's a\", \"nother implementation based on Azure Service Bus.)\\n\\nThe event bus implementation with RabbitMQ lets \", \"microservices subscribe to events, publish events, and receive events, as shown in Figure 6-21.\\n\\n![]\", \"(_page_153_Figure_0.jpeg)\\n\\nRabbitMQ functions as an intermediary between message publisher and subsc\", \"ribers, to handle distribution. In the code, the EventBusRabbitMQ class implements the generic IEven\", \"tBus interface. This implementation is based on Dependency Injection so that you can swap from this \", \"dev/test version to a production version.\\n\\n```\\npublic class EventBusRabbitMQ : IEventBus, IDisposabl\", \"e\\n{\\n // Implementation using RabbitMQ API\\n //...\\n}\\n```\\n\\nThe RabbitMQ implementation of a sample dev/\", \"test event bus is boilerplate code. It has to handle the connection to the RabbitMQ server and provi\", \"de code for publishing a message event to the queues. It also has to implement a dictionary of colle\", \"ctions of integration event handlers for each event type; these event types can have a different ins\", \"tantiation and different subscriptions for each receiver microservice, as shown in Figure 6-21.\\n\\n###\", \" <span id=\\\"page-153-0\\\"></span>**Implementing a simple publish method with RabbitMQ**\\n\\nThe following \", \"code is a *simplified* version of an event bus implementation for RabbitMQ, to showcase the whole sc\", \"enario. You don't really handle the connection this way. To see the full implementation, see the act\", \"ual code in the [dotnet-architecture/eShopOnContainers](https://github.com/dotnet-architecture/eShop\", \"OnContainers/blob/main/src/BuildingBlocks/EventBus/EventBusRabbitMQ/EventBusRabbitMQ.cs) repository.\", \"\\n\\n```\\npublic class EventBusRabbitMQ : IEventBus, IDisposable\\n{\\n // Member objects and other methods \", \"...\\n // ...\\n public void Publish(IntegrationEvent @event)\\n {\\n var eventName = @event.GetType().Name;\", \"\\n var factory = new ConnectionFactory() { HostName = _connectionString };\\n using (var connection = f\", \"actory.CreateConnection())\\n using (var channel = connection.CreateModel())\\n```\\n\\n```\\n {\\n channel.Exch\", \"angeDeclare(exchange: _brokerName,\\n type: \\\"direct\\\");\\n string message = JsonConvert.SerializeObject(@\", \"event);\\n var body = Encoding.UTF8.GetBytes(message);\\n channel.BasicPublish(exchange: _brokerName,\\n r\", \"outingKey: eventName,\\n basicProperties: null,\\n body: body);\\n }\\n }\\n}\\n```\\n\\nThe [actual code](https://g\", \"ithub.com/dotnet-architecture/eShopOnContainers/blob/main/src/BuildingBlocks/EventBus/EventBusRabbit\", \"MQ/EventBusRabbitMQ.cs) of the Publish method in the eShopOnContainers application is improved by us\", \"ing a [Polly](https://github.com/App-vNext/Polly) retry policy, which retries the task some times in\", \" case the RabbitMQ container is not ready. This scenario can occur when docker-compose is starting t\", \"he containers; for example, the RabbitMQ container might start more slowly than the other containers\", \".\\n\\nAs mentioned earlier, there are many possible configurations in RabbitMQ, so this code should be \", \"used only for dev/test environments.\\n\\n## <span id=\\\"page-154-0\\\"></span>**Implementing the subscriptio\", \"n code with the RabbitMQ API**\\n\\nAs with the publish code, the following code is a simplification of \", \"part of the event bus implementation for RabbitMQ. Again, you usually do not need to change it unles\", \"s you are improving it.\\n\\n```\\npublic class EventBusRabbitMQ : IEventBus, IDisposable\\n{\\n // Member obj\", \"ects and other methods ...\\n // ...\\n public void Subscribe<T, TH>()\\n where T : IntegrationEvent\\n wher\", \"e TH : IIntegrationEventHandler<T>\\n {\\n var eventName = _subsManager.GetEventKey<T>();\\n var containsK\", \"ey = _subsManager.HasSubscriptionsForEvent(eventName);\\n if (!containsKey)\\n {\\n if (!_persistentConnec\", \"tion.IsConnected)\\n {\\n _persistentConnection.TryConnect();\\n }\\n using (var channel = _persistentConnec\", \"tion.CreateModel())\\n {\\n channel.QueueBind(queue: _queueName,\\n exchange: BROKER_NAME,\\n routingKey: ev\", \"entName);\\n }\\n }\\n _subsManager.AddSubscription<T, TH>();\\n }\\n}\\n```\\n\\nEach event type has a related chan\", \"nel to get events from RabbitMQ. You can then have as many event handlers per channel and event type\", \" as needed.\\n\\nThe Subscribe method accepts an IIntegrationEventHandler object, which is like a callba\", \"ck method in the current microservice, plus its related IntegrationEvent object. The code then adds \", \"that event handler to the list of event handlers that each integration event type can have per clien\", \"t microservice. If the client code has not already been subscribed to the event, the code creates a \", \"channel for the event type so it can receive events in a push style from RabbitMQ when that event is\", \" published from any other service.\\n\\nAs mentioned above, the event bus implemented in eShopOnContaine\", \"rs has only an educational purpose, since it only handles the main scenarios, so it's not ready for \", \"production.\\n\\nFor production scenarios check the additional resources below, specific for RabbitMQ, a\", \"nd the [Implementing event-based communication between microservices](#page-24-2) section.\\n\\n## <span\", \" id=\\\"page-155-0\\\"></span>**Additional resources**\\n\\nA production-ready solution with support for Rabbi\", \"tMQ.\\n\\n- **NServiceBus** Fully-supported commercial service bus with advanced management and monitori\", \"ng tooling for .NET <https://particular.net/>\\n- **EasyNetQ** Open Source .NET API client for RabbitM\", \"Q <https://easynetq.com/>\\n- **MassTransit** Free, open-source distributed application framework for \", \".NET <https://masstransit-project.com/>\\n- **Rebus** Open source .NET Service Bus <https://github.com\", \"/rebus-org/Rebus>\\n\\n## <span id=\\\"page-155-1\\\"></span>Subscribing to events\\n\\nThe first step for using t\", \"he event bus is to subscribe the microservices to the events they want to receive. That functionalit\", \"y should be done in the receiver microservices.\\n\\nThe following simple code shows what each receiver \", \"microservice needs to implement when starting the service (that is, in the Startup class) so it subs\", \"cribes to the events it needs. In this case, the basketapi microservice needs to subscribe to Produc\", \"tPriceChangedIntegrationEvent and the OrderStartedIntegrationEvent messages.\\n\\nFor instance, when sub\", \"scribing to the ProductPriceChangedIntegrationEvent event, that makes the basket microservice aware \", \"of any changes to the product price and lets it warn the user about the change if that product is in\", \" the user's basket.\\n\\n```\\nvar eventBus = app.ApplicationServices.GetRequiredService<IEventBus>();\\neve\", \"ntBus.Subscribe<ProductPriceChangedIntegrationEvent,\\n```\\n\\n```\\n ProductPriceChangedIntegrationEventHa\", \"ndler>();\\neventBus.Subscribe<OrderStartedIntegrationEvent,\\n OrderStartedIntegrationEventHandler>();\\n\", \"```\\n\\nAfter this code runs, the subscriber microservice will be listening through RabbitMQ channels. \", \"When any message of type ProductPriceChangedIntegrationEvent arrives, the code invokes the event han\", \"dler that is passed to it and processes the event.\\n\\n## <span id=\\\"page-156-0\\\"></span>**Publishing eve\", \"nts through the event bus**\\n\\nFinally, the message sender (origin microservice) publishes the integra\", \"tion events with code similar to the following example. (This approach is a simplified example that \", \"does not take atomicity into account.) You would implement similar code whenever an event must be pr\", \"opagated across multiple microservices, usually right after committing data or transactions from the\", \" origin microservice.\\n\\nFirst, the event bus implementation object (based on RabbitMQ or based on a s\", \"ervice bus) would be injected at the controller constructor, as in the following code:\\n\\n```\\n[Route(\\\"\", \"api/v1/[controller]\\\")]\\npublic class CatalogController : ControllerBase\\n{\\n private readonly CatalogCo\", \"ntext _context;\\n private readonly IOptionsSnapshot<Settings> _settings;\\n private readonly IEventBus \", \"_eventBus;\\n public CatalogController(CatalogContext context,\\n IOptionsSnapshot<Settings> settings,\\n \", \"IEventBus eventBus)\\n {\\n _context = context;\\n _settings = settings;\\n _eventBus = eventBus;\\n }\\n // ...\", \"\\n}\\n```\\n\\nThen you use it from your controller's methods, like in the UpdateProduct method:\\n\\n```\\n[Rout\", \"e(\\\"items\\\")]\\n[HttpPost]\\npublic async Task<IActionResult> UpdateProduct([FromBody]CatalogItem product)\", \"\\n{\\n var item = await _context.CatalogItems.SingleOrDefaultAsync(\\n i => i.Id == product.Id);\\n // ...\\n\", \" if (item.Price != product.Price)\\n {\\n var oldPrice = item.Price;\\n item.Price = product.Price;\\n _cont\", \"ext.CatalogItems.Update(item);\\n var @event = new ProductPriceChangedIntegrationEvent(item.Id,\\n item.\", \"Price,\\n oldPrice);\\n // Commit changes in original transaction\\n await _context.SaveChangesAsync();\\n /\", \"/ Publish integration event to the event bus\\n // (RabbitMQ or a service bus underneath)\\n```\\n\\n```\\n _e\", \"ventBus.Publish(@event);\\n // ...\\n }\\n // ...\\n}\\n```\\n\\nIn this case, since the origin microservice is a \", \"simple CRUD microservice, that code is placed right into a Web API controller.\\n\\nIn more advanced mic\", \"roservices, like when using CQRS approaches, it can be implemented in the CommandHandler class, with\", \"in the Handle() method.\\n\\n## **Designing atomicity and resiliency when publishing to the event bus**\\n\", \"\\nWhen you publish integration events through a distributed messaging system like your event bus, you\", \" have the problem of atomically updating the original database and publishing an event (that is, eit\", \"her both operations complete or none of them). For instance, in the simplified example shown earlier\", \", the code commits data to the database when the product price is changed and then publishes a Produ\", \"ctPriceChangedIntegrationEvent message. Initially, it might look essential that these two operations\", \" be performed atomically. However, if you are using a distributed transaction involving the database\", \" and the message broker, as you do in older systems like [Microsoft Message Queuing](https://docs.mi\", \"crosoft.com/previous-versions/windows/desktop/legacy/ms711472(v=vs.85))  [\\\\(MSMQ\\\\),](https://docs.mi\", \"crosoft.com/previous-versions/windows/desktop/legacy/ms711472(v=vs.85)) this approach is not recomme\", \"nded for the reasons described by the [CAP theorem.](https://www.quora.com/What-Is-CAP-Theorem-1)\\n\\nB\", \"asically, you use microservices to build scalable and highly available systems. Simplifying somewhat\", \", the CAP theorem says that you cannot build a (distributed) database (or a microservice that owns i\", \"ts model) that's continually available, strongly consistent, *and* tolerant to any partition. You mu\", \"st choose two of these three properties.\\n\\nIn microservices-based architectures, you should choose av\", \"ailability and tolerance, and you should de-emphasize strong consistency. Therefore, in most modern \", \"microservice-based applications, you usually do not want to use distributed transactions in messagin\", \"g, as you do when you implement [distributed transactions](https://docs.microsoft.com/previous-versi\", \"ons/windows/desktop/ms681205(v=vs.85)) based on the Windows Distributed Transaction Coordinator (DTC\", \") with [MSMQ.](https://docs.microsoft.com/previous-versions/windows/desktop/legacy/ms711472(v=vs.85)\", \")\\n\\nLet's go back to the initial issue and its example. If the service crashes after the database is \", \"updated (in this case, right after the line of code with \\\\_context.SaveChangesAsync()), but before t\", \"he integration event is published, the overall system could become inconsistent. This approach might\", \" be business critical, depending on the specific business operation you are dealing with.\\n\\nAs mentio\", \"ned earlier in the architecture section, you can have several approaches for dealing with this issue\", \":\\n\\n- Using the full [Event Sourcing pattern.](https://docs.microsoft.com/azure/architecture/patterns\", \"/event-sourcing)\\n- Using transaction log mining.\\n- Using the [Outbox pattern.](https://www.kamilgrzy\", \"bek.com/design/the-outbox-pattern/) This is a transactional table to store the integration events (e\", \"xtending the local transaction).\\n\\nFor this scenario, using the full Event Sourcing (ES) pattern is o\", \"ne of the best approaches, if not *the* best. However, in many application scenarios, you might not \", \"be able to implement a full ES system. ES means storing only domain events in your transactional dat\", \"abase, instead of storing current state\\n\\ndata. Storing only domain events can have great benefits, s\", \"uch as having the history of your system available and being able to determine the state of your sys\", \"tem at any moment in the past. However, implementing a full ES system requires you to rearchitect mo\", \"st of your system and introduces many other complexities and requirements. For example, you would wa\", \"nt to use a database specifically made for event sourcing, such as [Event Store,](https://eventstore\", \".org/) or a document-oriented database such as Azure Cosmos DB, MongoDB, Cassandra, CouchDB, or Rave\", \"nDB. ES is a great approach for this problem, but not the easiest solution unless you are already fa\", \"miliar with event sourcing.\\n\\nThe option to use transaction log mining initially looks transparent. H\", \"owever, to use this approach, the microservice has to be coupled to your RDBMS transaction log, such\", \" as the SQL Server transaction log. This approach is probably not desirable. Another drawback is tha\", \"t the low-level updates recorded in the transaction log might not be at the same level as your high-\", \"level integration events. If so, the process of reverse-engineering those transaction log operations\", \" can be difficult.\\n\\nA balanced approach is a mix of a transactional database table and a simplified \", \"ES pattern. You can use a state such as \\\"ready to publish the event,\\\" which you set in the original \", \"event when you commit it to the integration events table. You then try to publish the event to the e\", \"vent bus. If the publishevent action succeeds, you start another transaction in the origin service a\", \"nd move the state from \\\"ready to publish the event\\\" to \\\"event already published.\\\"\\n\\nIf the publish-ev\", \"ent action in the event bus fails, the data still will not be inconsistent within the origin microse\", \"rvice\\u2014it is still marked as \\\"ready to publish the event,\\\" and with respect to the rest of the servic\", \"es, it will eventually be consistent. You can always have background jobs checking the state of the \", \"transactions or integration events. If the job finds an event in the \\\"ready to publish the event\\\" st\", \"ate, it can try to republish that event to the event bus.\\n\\nNotice that with this approach, you are p\", \"ersisting only the integration events for each origin microservice, and only the events that you wan\", \"t to communicate to other microservices or external systems. In contrast, in a full ES system, you s\", \"tore all domain events as well.\\n\\nTherefore, this balanced approach is a simplified ES system. You ne\", \"ed a list of integration events with their current state (\\\"ready to publish\\\" versus \\\"published\\\"). Bu\", \"t you only need to implement these states for the integration events. And in this approach, you do n\", \"ot need to store all your domain data as events in the transactional database, as you would in a ful\", \"l ES system.\\n\\nIf you are already using a relational database, you can use a transactional table to s\", \"tore integration events. To achieve atomicity in your application, you use a two-step process based \", \"on local transactions. Basically, you have an IntegrationEvent table in the same database where you \", \"have your domain entities. That table works as an insurance for achieving atomicity so that you incl\", \"ude persisted integration events into the same transactions that are committing your domain data.\\n\\nS\", \"tep by step, the process goes like this:\\n\\n- 1. The application begins a local database transaction.\\n\", \"- 2. It then updates the state of your domain entities and inserts an event into the integration eve\", \"nt table.\\n- 3. Finally, it commits the transaction, so you get the desired atomicity and then\\n\\n4. Yo\", \"u publish the event somehow (next).\\n\\nWhen implementing the steps of publishing the events, you have \", \"these choices:\\n\\n- Publish the integration event right after committing the transaction and use anoth\", \"er local transaction to mark the events in the table as being published. Then, use the table just as\", \" an artifact to track the integration events in case of issues in the remote microservices, and perf\", \"orm compensatory actions based on the stored integration events.\\n- Use the table as a kind of queue.\", \" A separate application thread or process queries the integration event table, publishes the events \", \"to the event bus, and then uses a local transaction to mark the events as published.\\n\\nFigure 6-22 sh\", \"ows the architecture for the first of these approaches.\\n\\n![](_page_159_Figure_5.jpeg)\\n\\n*Figure 6-22.\", \" Atomicity when publishing events to the event bus*\\n\\nThe approach illustrated in Figure 6-22 is miss\", \"ing an additional worker microservice that is in charge of checking and confirming the success of th\", \"e published integration events. In case of failure, that additional checker worker microservice can \", \"read events from the table and republish them, that is, repeat step number 2.\\n\\nAbout the second appr\", \"oach: you use the EventLog table as a queue and always use a worker microservice to publish the mess\", \"ages. In that case, the process is like that shown in Figure 6-23. This shows an additional microser\", \"vice, and the table is the single source when publishing events.\\n\\n![](_page_160_Figure_0.jpeg)\\n\\nFor \", \"simplicity, the eShopOnContainers sample uses the first approach (with no additional processes or ch\", \"ecker microservices) plus the event bus. However, the eShopOnContainers sample is not handling all p\", \"ossible failure cases. In a real application deployed to the cloud, you must embrace the fact that i\", \"ssues will arise eventually, and you must implement that check and resend logic. Using the table as \", \"a queue can be more effective than the first approach if you have that table as a single source of e\", \"vents when publishing them (with the worker) through the event bus.\\n\\n### **Implementing atomicity wh\", \"en publishing integration events through the event bus**\\n\\nThe following code shows how you can creat\", \"e a single transaction involving multiple DbContext objects\\u2014one context related to the original data\", \" being updated, and the second context related to the IntegrationEventLog table.\\n\\nThe transaction in\", \" the example code below will not be resilient if connections to the database have any issue at the t\", \"ime when the code is running. This can happen in cloud-based systems like Azure SQL DB, which might \", \"move databases across servers. For implementing resilient transactions across multiple contexts, see\", \" the Implementing resilient Entity Framework Core SQL connections section later in this guide.\\n\\nFor \", \"clarity, the following example shows the whole process in a single piece of code. However, the eShop\", \"OnContainers implementation is refactored and splits this logic into multiple classes so it's easier\", \" to maintain.\\n\\n```\\n// Update Product from the Catalog microservice\\n//\\npublic async Task<IActionResul\", \"t> UpdateProduct([FromBody]CatalogItem productToUpdate)\\n{\\n var catalogItem =\\n await _catalogContext.\", \"CatalogItems.SingleOrDefaultAsync(i => i.Id ==\\n productToUpdate.Id);\\n```\\n\\n```\\n if (catalogItem == nu\", \"ll) return NotFound();\\n bool raiseProductPriceChangedEvent = false;\\n IntegrationEvent priceChangedEv\", \"ent = null;\\n if (catalogItem.Price != productToUpdate.Price)\\n raiseProductPriceChangedEvent = true;\\n\", \" if (raiseProductPriceChangedEvent) // Create event if price has changed\\n {\\n var oldPrice = catalogI\", \"tem.Price;\\n priceChangedEvent = new ProductPriceChangedIntegrationEvent(catalogItem.Id,\\n productToUp\", \"date.Price,\\n oldPrice);\\n }\\n // Update current product\\n catalogItem = productToUpdate;\\n // Just save \", \"the updated product if the Product's Price hasn't changed.\\n if (!raiseProductPriceChangedEvent)\\n {\\n \", \"await _catalogContext.SaveChangesAsync();\\n }\\n else // Publish to event bus only if product price cha\", \"nged\\n {\\n // Achieving atomicity between original DB and the IntegrationEventLog\\n // with a local tra\", \"nsaction\\n using (var transaction = _catalogContext.Database.BeginTransaction())\\n {\\n _catalogContext.\", \"CatalogItems.Update(catalogItem);\\n await _catalogContext.SaveChangesAsync();\\n await _integrationEven\", \"tLogService.SaveEventAsync(priceChangedEvent);\\n transaction.Commit();\\n }\\n // Publish the integration\", \" event through the event bus\\n _eventBus.Publish(priceChangedEvent);\\n _integrationEventLogService.Mar\", \"kEventAsPublishedAsync(\\n priceChangedEvent);\\n }\\n return Ok();\\n}\\n```\\n\\nAfter the ProductPriceChangedIn\", \"tegrationEvent integration event is created, the transaction that stores the original domain operati\", \"on (update the catalog item) also includes the persistence of the event in the EventLog table. This \", \"makes it a single transaction, and you will always be able to check whether event messages were sent\", \".\\n\\nThe event log table is updated atomically with the original database operation, using a local tra\", \"nsaction against the same database. If any of the operations fail, an exception is thrown and the tr\", \"ansaction rolls back any completed operation, thus maintaining consistency between the domain operat\", \"ions and the event messages saved to the table.\\n\\n## **Receiving messages from subscriptions: event h\", \"andlers in receiver microservices**\\n\\nIn addition to the event subscription logic, you need to implem\", \"ent the internal code for the integration event handlers (like a callback method). The event handler\", \" is where you specify where the event messages of a certain type will be received and processed.\\n\\nAn\", \" event handler first receives an event instance from the event bus. Then it locates the component to\", \" be processed related to that integration event, propagating and persisting the event as a change in\", \" state in the receiver microservice. For example, if a ProductPriceChanged event originates in the c\", \"atalog microservice, it is handled in the basket microservice and changes the state in this receiver\", \" basket microservice as well, as shown in the following code.\\n\\n```\\nnamespace Microsoft.eShopOnContai\", \"ners.Services.Basket.API.IntegrationEvents.EventHandling\\n{\\n public class ProductPriceChangedIntegrat\", \"ionEventHandler :\\n IIntegrationEventHandler<ProductPriceChangedIntegrationEvent>\\n {\\n private readonl\", \"y IBasketRepository _repository;\\n public ProductPriceChangedIntegrationEventHandler(\\n IBasketReposit\", \"ory repository)\\n {\\n _repository = repository;\\n }\\n public async Task Handle(ProductPriceChangedIntegr\", \"ationEvent @event)\\n {\\n var userIds = await _repository.GetUsers();\\n foreach (var id in userIds)\\n {\\n \", \"var basket = await _repository.GetBasket(id);\\n await UpdatePriceInBasketItems(@event.ProductId, @eve\", \"nt.NewPrice, basket);\\n }\\n }\\n private async Task UpdatePriceInBasketItems(int productId, decimal newP\", \"rice,\\n CustomerBasket basket)\\n {\\n var itemsToUpdate = basket?.Items?.Where(x => int.Parse(x.ProductI\", \"d) ==\\n productId).ToList();\\n if (itemsToUpdate != null)\\n {\\n foreach (var item in itemsToUpdate)\\n {\\n \", \"if(item.UnitPrice != newPrice)\\n {\\n var originalPrice = item.UnitPrice;\\n item.UnitPrice = newPrice;\\n \", \"item.OldUnitPrice = originalPrice;\\n }\\n }\\n await _repository.UpdateBasket(basket);\\n }\\n }\\n }\\n}\\n```\\n\\nTh\", \"e event handler needs to verify whether the product exists in any of the basket instances. It also u\", \"pdates the item price for each related basket line item. Finally, it creates an alert to be displaye\", \"d to the user about the price change, as shown in Figure 6-24.\\n\\n![](_page_163_Figure_1.jpeg)\\n\\n*Figur\", \"e 6-24. Displaying an item price change in a basket, as communicated by integration events*\\n\\n### <sp\", \"an id=\\\"page-163-0\\\"></span>**Idempotency in update message events**\\n\\nAn important aspect of update me\", \"ssage events is that a failure at any point in the communication should cause the message to be retr\", \"ied. Otherwise a background task might try to publish an event that has already been published, crea\", \"ting a race condition. Make sure that the updates are either idempotent or that they provide enough \", \"information to ensure that you can detect a duplicate, discard it, and send back only one response.\\n\", \"\\nAs noted earlier, idempotency means that an operation can be performed multiple times without chang\", \"ing the result. In a messaging environment, as when communicating events, an event is idempotent if \", \"it can be delivered multiple times without changing the result for the receiver microservice. This m\", \"ay be necessary because of the nature of the event itself, or because of the way the system handles \", \"the event. Message idempotency is important in any application that uses messaging, not just in appl\", \"ications that implement the event bus pattern.\\n\\nAn example of an idempotent operation is a SQL state\", \"ment that inserts data into a table only if that data is not already in the table. It does not matte\", \"r how many times you run that insert SQL statement; the result will be the same\\u2014the table will conta\", \"in that data. Idempotency like this can also be necessary when dealing with messages if the messages\", \" could potentially be sent and therefore\\n\\nprocessed more than once. For instance, if retry logic cau\", \"ses a sender to send exactly the same message more than once, you need to make sure that it is idemp\", \"otent.\\n\\nIt is possible to design idempotent messages. For example, you can create an event that says\", \" \\\"set the product price to \\\\$25\\\" instead of \\\"add \\\\$5 to the product price.\\\" You could safely process\", \" the first message any number of times and the result will be the same. That is not true for the sec\", \"ond message. But even in the first case, you might not want to process the first event, because the \", \"system could also have sent a newer price-change event and you would be overwriting the new price.\\n\\n\", \"Another example might be an order-completed event that's propagated to multiple subscribers. The app\", \" has to make sure that order information is updated in other systems only once, even if there are du\", \"plicated message events for the same order-completed event.\\n\\nIt is convenient to have some kind of i\", \"dentity per event so that you can create logic that enforces that each event is processed only once \", \"per receiver.\\n\\nSome message processing is inherently idempotent. For example, if a system generates \", \"image thumbnails, it might not matter how many times the message about the generated thumbnail is pr\", \"ocessed; the outcome is that the thumbnails are generated and they are the same every time. On the o\", \"ther hand, operations such as calling a payment gateway to charge a credit card may not be idempoten\", \"t at all. In these cases, you need to ensure that processing a message multiple times has the effect\", \" that you expect.\\n\\n### **Additional resources**\\n\\nto the command handler).\\n\\n\\u2022 **Honoring message idem\", \"potency** [https://learn.microsoft.com/previous-versions/msp-n-p/jj591565\\\\(v=pandp.10\\\\)#honoring](ht\", \"tps://docs.microsoft.com/previous-versions/msp-n-p/jj591565(v=pandp.10)#honoring-message-idempotency\", \")[message-idempotency](https://docs.microsoft.com/previous-versions/msp-n-p/jj591565(v=pandp.10)#hon\", \"oring-message-idempotency)\\n\\n### <span id=\\\"page-164-0\\\"></span>**Deduplicating integration event messa\", \"ges**\\n\\nYou can make sure that message events are sent and processed only once per subscriber at diff\", \"erent levels. One way is to use a deduplication feature offered by the messaging infrastructure you \", \"are using. Another is to implement custom logic in your destination microservice. Having validations\", \" at both the transport level and the application level is your best bet.\\n\\n### **Deduplicating messag\", \"e events at the EventHandler level**\\n\\nOne way to make sure that an event is processed only once by a\", \"ny receiver is by implementing certain logic when processing the message events in event handlers. F\", \"or example, that is the approach used in the eShopOnContainers application, as you can see in the [s\", \"ource code of the](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/Services/O\", \"rdering/Ordering.API/Application/IntegrationEvents/EventHandling/UserCheckoutAcceptedIntegrationEven\", \"tHandler.cs)  [UserCheckoutAcceptedIntegrationEventHandler class](https://github.com/dotnet-architec\", \"ture/eShopOnContainers/blob/main/src/Services/Ordering/Ordering.API/Application/IntegrationEvents/Ev\", \"entHandling/UserCheckoutAcceptedIntegrationEventHandler.cs) when it receives a UserCheckoutAcceptedI\", \"ntegrationEvent integration event. (In this case, the CreateOrderCommand is wrapped with an Identifi\", \"edCommand, using the eventMsg.RequestId as an identifier, before sending it\\n\\n## **Deduplicating mess\", \"ages when using RabbitMQ**\\n\\nWhen intermittent network failures happen, messages can be duplicated, a\", \"nd the message receiver must be ready to handle these duplicated messages. If possible, receivers sh\", \"ould handle messages in an idempotent way, which is better than explicitly handling them with dedupl\", \"ication.\\n\\nAccording to the [RabbitMQ documentation](https://www.rabbitmq.com/reliability.html#consum\", \"er), \\\"If a message is delivered to a consumer and then requeued (because it was not acknowledged bef\", \"ore the consumer connection dropped, for example) then RabbitMQ will set the redelivered flag on it \", \"when it is delivered again (whether to the same consumer or a different one).\\n\\nIf the \\\"redelivered\\\" \", \"flag is set, the receiver must take that into account, because the message might already have been p\", \"rocessed. But that is not guaranteed; the message might never have reached the receiver after it lef\", \"t the message broker, perhaps because of network issues. On the other hand, if the \\\"redelivered\\\" fla\", \"g is not set, it is guaranteed that the message has not been sent more than once. Therefore, the rec\", \"eiver needs to deduplicate messages or process messages in an idempotent way only if the \\\"redelivere\", \"d\\\" flag is set in the message.\\n\\n### **Additional resources**\\n\\n- **Forked eShopOnContainers using NSe\", \"rviceBus (Particular Software)** <https://go.particular.net/eShopOnContainers>\\n- **Event Driven Mess\", \"aging** [https://patterns.arcitura.com/soa-patterns/design\\\\\\\\_patterns/event\\\\\\\\_driven\\\\\\\\_messaging](ht\", \"tps://patterns.arcitura.com/soa-patterns/design_patterns/event_driven_messaging)\\n- **Jimmy Bogard. R\", \"efactoring Towards Resilience: Evaluating Coupling** <https://jimmybogard.com/refactoring-towards-re\", \"silience-evaluating-coupling/>\\n- **Publish-Subscribe channel** [https://www.enterpriseintegrationpat\", \"terns.com/patterns/messaging/PublishSubscribeChannel.](https://www.enterpriseintegrationpatterns.com\", \"/patterns/messaging/PublishSubscribeChannel.html) [html](https://www.enterpriseintegrationpatterns.c\", \"om/patterns/messaging/PublishSubscribeChannel.html)\\n- **Communicating Between Bounded Contexts** [ht\", \"tps://learn.microsoft.com/previous-versions/msp-n-p/jj591572\\\\(v=pandp.10\\\\)](https://docs.microsoft.c\", \"om/previous-versions/msp-n-p/jj591572(v=pandp.10))\\n- **Eventual Consistency** [https://en.wikipedia.\", \"org/wiki/Eventual\\\\\\\\_consistency](https://en.wikipedia.org/wiki/Eventual_consistency)\\n- **Philip Brow\", \"n. Strategies for Integrating Bounded Contexts** <https://www.culttt.com/2014/11/26/strategies-integ\", \"rating-bounded-contexts/>\\n- **Chris Richardson. Developing Transactional Microservices Using Aggrega\", \"tes, Event Sourcing and CQRS - Part 2** <https://www.infoq.com/articles/microservices-aggregates-eve\", \"nts-cqrs-part-2-richardson>\\n- **Chris Richardson. Event Sourcing pattern** <https://microservices.io\", \"/patterns/data/event-sourcing.html>\\n- **Introducing Event Sourcing** [https://learn.microsoft.com/pr\", \"evious-versions/msp-n-p/jj591559\\\\(v=pandp.10\\\\)](https://docs.microsoft.com/previous-versions/msp-n-p\", \"/jj591559(v=pandp.10))\\n\\n\\u2022 **Event Store database**. Official site. <https://geteventstore.com/>\\n\\n\\u2022 *\", \"*The CAP Theorem**\\n\\n- **Patrick Nommensen. Event-Driven Data Management for Microservices** <https:/\", \"/dzone.com/articles/event-driven-data-management-for-microservices-1>\\n- [https://en.wikipedia.org/wi\", \"ki/CAP\\\\\\\\_theorem](https://en.wikipedia.org/wiki/CAP_theorem)\\n- **What is CAP Theorem?** <https://www\", \".quora.com/What-Is-CAP-Theorem-1>\\n- **Data Consistency Primer** [https://learn.microsoft.com/previou\", \"s-versions/msp-n-p/dn589800\\\\(v=pandp.10\\\\)](https://docs.microsoft.com/previous-versions/msp-n-p/dn58\", \"9800(v=pandp.10))\\n- **Internet** [https://learn.microsoft.com/archive/blogs/rickatmicrosoft/the-cap-\", \"theorem-why-everything](https://docs.microsoft.com/archive/blogs/rickatmicrosoft/the-cap-theorem-why\", \"-everything-is-different-with-the-cloud-and-internet/)[is-different-with-the-cloud-and-internet/](ht\", \"tps://docs.microsoft.com/archive/blogs/rickatmicrosoft/the-cap-theorem-why-everything-is-different-w\", \"ith-the-cloud-and-internet/)\\n\\n\\u2022 **Rick Saling. The CAP Theorem: Why \\\"Everything is Different\\\" with t\", \"he Cloud and** \\n\\n- **Eric Brewer. CAP Twelve Years Later: How the \\\"Rules\\\" Have Changed** <https://ww\", \"w.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed>\\n- **CAP, PACELC, and Microse\", \"rvices** <https://ardalis.com/cap-pacelc-and-microservices/>\\n- **Azure Service Bus. Brokered Messagi\", \"ng: Duplicate Detection** [https://github.com/microsoftarchive/msdn-code-gallery](https://github.com\", \"/microsoftarchive/msdn-code-gallery-microsoft/tree/master/Windows%20Azure%20Product%20Team/Brokered%\", \"20Messaging%20Duplicate%20Detection)[microsoft/tree/master/Windows%20Azure%20Product%20Team/Brokered\", \"%20Messaging%20](https://github.com/microsoftarchive/msdn-code-gallery-microsoft/tree/master/Windows\", \"%20Azure%20Product%20Team/Brokered%20Messaging%20Duplicate%20Detection) [Duplicate%20Detection](http\", \"s://github.com/microsoftarchive/msdn-code-gallery-microsoft/tree/master/Windows%20Azure%20Product%20\", \"Team/Brokered%20Messaging%20Duplicate%20Detection)\\n- **Reliability Guide** (RabbitMQ documentation) \", \"<https://www.rabbitmq.com/reliability.html#consumer>\\n\\n## <span id=\\\"page-166-0\\\"></span>Testing ASP.NE\", \"T Core services and web apps\\n\\nControllers are a central part of any ASP.NET Core API service and ASP\", \".NET MVC Web application. As such, you should have confidence they behave as intended for your appli\", \"cation. Automated tests can provide you with this confidence and can detect errors before they reach\", \" production.\\n\\nYou need to test how the controller behaves based on valid or invalid inputs, and test\", \" controller responses based on the result of the business operation it performs. However, you should\", \" have these types of tests for your microservices:\\n\\n- Unit tests. These tests ensure that individual\", \" components of the application work as expected. Assertions test the component API.\\n- Integration te\", \"sts. These tests ensure that component interactions work as expected against external artifacts like\", \" databases. Assertions can test component API, UI, or the side effects of actions like database I/O,\", \" logging, etc.\\n\\n- Functional tests for each microservice. These tests ensure that the application wo\", \"rks as expected from the user's perspective.\\n- Service tests. These tests ensure that end-to-end ser\", \"vice use cases, including testing multiple services at the same time, are tested. For this type of t\", \"esting, you need to prepare the environment first. In this case, it means starting the services (for\", \" example, by using dockercompose up).\\n\\n## **Implementing unit tests for ASP.NET Core Web APIs**\\n\\nUni\", \"t testing involves testing a part of an application in isolation from its infrastructure and depende\", \"ncies. When you unit test controller logic, only the content of a single action or method is tested,\", \" not the behavior of its dependencies or of the framework itself. Unit tests do not detect issues in\", \" the interaction between components\\u2014that is the purpose of integration testing.\\n\\nAs you unit test yo\", \"ur controller actions, make sure you focus only on their behavior. A controller unit test avoids thi\", \"ngs like filters, routing, or model binding (the mapping of request data to a ViewModel or DTO). Bec\", \"ause they focus on testing just one thing, unit tests are generally simple to write and quick to run\", \". A well-written set of unit tests can be run frequently without much overhead.\\n\\nUnit tests are impl\", \"emented based on test frameworks like xUnit.net, MSTest, Moq, or NUnit. For the eShopOnContainers sa\", \"mple application, we are using xUnit.\\n\\nWhen you write a unit test for a Web API controller, you inst\", \"antiate the controller class directly using the new keyword in C#, so that the test will run as fast\", \" as possible. The following example shows how to do this when using [xUnit](https://xunit.net/) as t\", \"he Test framework.\\n\\n```\\n[Fact]\\npublic async Task Get_order_detail_success()\\n{\\n //Arrange\\n var fakeOr\", \"derId = \\\"12\\\";\\n var fakeOrder = GetFakeOrder();\\n //...\\n //Act\\n var orderController = new OrderControl\", \"ler(\\n _orderServiceMock.Object,\\n _basketServiceMock.Object,\\n _identityParserMock.Object);\\n orderCont\", \"roller.ControllerContext.HttpContext = _contextMock.Object;\\n var actionResult = await orderControlle\", \"r.Detail(fakeOrderId);\\n //Assert\\n var viewResult = Assert.IsType<ViewResult>(actionResult);\\n Assert.\", \"IsAssignableFrom<Order>(viewResult.ViewData.Model);\\n}\\n```\\n\\n## **Implementing integration and functio\", \"nal tests for each microservice**\\n\\nAs noted, integration tests and functional tests have different p\", \"urposes and goals. However, the way you implement both when testing ASP.NET Core controllers is simi\", \"lar, so in this section we concentrate on integration tests.\\n\\nIntegration testing ensures that an ap\", \"plication's components function correctly when assembled. ASP.NET Core supports integration testing \", \"using unit test frameworks and a built-in test web host that can be used to handle requests without \", \"network overhead.\\n\\nUnlike unit testing, integration tests frequently involve application infrastruct\", \"ure concerns, such as a database, file system, network resources, or web requests and responses. Uni\", \"t tests use fakes or mock objects in place of these concerns. But the purpose of integration tests i\", \"s to confirm that the system works as expected with these systems, so for integration testing you do\", \" not use fakes or mock objects. Instead, you include the infrastructure, like database access or ser\", \"vice invocation from other services.\\n\\nBecause integration tests exercise larger segments of code tha\", \"n unit tests, and because integration tests rely on infrastructure elements, they tend to be orders \", \"of magnitude slower than unit tests. Thus, it is a good idea to limit how many integration tests you\", \" write and run.\\n\\nASP.NET Core includes a built-in test web host that can be used to handle HTTP requ\", \"ests without network overhead, meaning that you can run those tests faster than when using a real we\", \"b host. The test web host (TestServer) is available in a NuGet component as Microsoft.AspNetCore.Tes\", \"tHost. It can be added to integration test projects and used to host ASP.NET Core applications.\\n\\nAs \", \"you can see in the following code, when you create integration tests for ASP.NET Core controllers, y\", \"ou instantiate the controllers through the test host. This functionality is comparable to an HTTP re\", \"quest, but it runs faster.\\n\\n```\\npublic class PrimeWebDefaultRequestShould\\n{\\n private readonly TestSe\", \"rver _server;\\n private readonly HttpClient _client;\\n public PrimeWebDefaultRequestShould()\\n {\\n // Ar\", \"range\\n _server = new TestServer(new WebHostBuilder()\\n .UseStartup<Startup>());\\n _client = _server.Cr\", \"eateClient();\\n }\\n [Fact]\\n public async Task ReturnHelloWorld()\\n {\\n // Act\\n var response = await _cli\", \"ent.GetAsync(\\\"/\\\");\\n response.EnsureSuccessStatusCode();\\n var responseString = await response.Content\", \".ReadAsStringAsync();\\n // Assert\\n Assert.Equal(\\\"Hello World!\\\", responseString);\\n }\\n}\\n```\\n\\n## **Addit\", \"ional resources**\\n\\n- **Steve Smith. Testing controllers** (ASP.NET Core) [https://learn.microsoft.co\", \"m/aspnet/core/mvc/controllers/testing](https://docs.microsoft.com/aspnet/core/mvc/controllers/testin\", \"g)\\n- **Steve Smith. Integration testing** (ASP.NET Core) [https://learn.microsoft.com/aspnet/core/te\", \"st/integration-tests](https://docs.microsoft.com/aspnet/core/test/integration-tests)\\n- **Unit testin\", \"g in .NET using dotnet test** [https://learn.microsoft.com/dotnet/core/testing/unit-testing-with-dot\", \"net-test](https://docs.microsoft.com/dotnet/core/testing/unit-testing-with-dotnet-test)\\n- **xUnit.ne\", \"t**. Official site. <https://xunit.net/>\\n- **Unit Test Basics.** [https://learn.microsoft.com/visual\", \"studio/test/unit-test-basics](https://docs.microsoft.com/visualstudio/test/unit-test-basics)\\n- **Moq\", \"**. GitHub repo. <https://github.com/moq/moq>\\n- **NUnit**. Official site. <https://nunit.org/>\\n\\n### \", \"**Implementing service tests on a multi-container application**\\n\\nAs noted earlier, when you test mul\", \"ti-container applications, all the microservices need to be running within the Docker host or contai\", \"ner cluster. End-to-end service tests that include multiple operations involving several microservic\", \"es require you to deploy and start the whole application in the Docker host by running docker-compos\", \"e up (or a comparable mechanism if you are using an orchestrator). Once the whole application and al\", \"l its services is running, you can execute end-to-end integration and functional tests.\\n\\nThere are a\", \" few approaches you can use. In the docker-compose.yml file that you use to deploy the application a\", \"t the solution level you can expand the entry point to use [dotnet test.](https://docs.microsoft.com\", \"/dotnet/core/tools/dotnet-test) You can also use another compose file that would run your tests in t\", \"he image you are targeting. By using another compose file for integration tests that includes your m\", \"icroservices and databases on containers, you can make sure that the related data is always reset to\", \" its original state before running the tests.\\n\\nOnce the compose application is up and running, you c\", \"an take advantage of breakpoints and exceptions if you are running Visual Studio. Or you can run the\", \" integration tests automatically in your CI pipeline in Azure DevOps Services or any other CI/CD sys\", \"tem that supports Docker containers.\\n\\n### <span id=\\\"page-169-0\\\"></span>**Testing in eShopOnContainer\", \"s**\\n\\nThe reference application (eShopOnContainers) tests were recently restructured and now there ar\", \"e four categories:\\n\\n1. **Unit** tests, just plain old regular unit tests, contained in the **{Micros\", \"erviceName}.UnitTests** projects\\n\\n- 2. **Microservice functional/integration tests**, with test case\", \"s involving the infrastructure for each microservice but isolated from the others and are contained \", \"in the **{MicroserviceName}.FunctionalTests** projects.\\n- 3. **Application functional/integration te\", \"sts**, which focus on microservices integration, with test cases that exert several microservices. T\", \"hese tests are located in project **Application.FunctionalTests**.\\n\\nWhile unit and integration tests\", \" are organized in a test folder within the microservice project, application and load tests are mana\", \"ged separately under the root folder, as shown in Figure 6-25.\\n\\n![](_page_170_Figure_3.jpeg)\\n\\n*Figur\", \"e 6-25. Test folder structure in eShopOnContainers*\\n\\nMicroservice and Application functional/integra\", \"tion tests are run from Visual Studio, using the regular tests runner, but first you need to start t\", \"he required infrastructure services, with a set of dockercompose files contained in the solution tes\", \"t folder:\\n\\n### **docker-compose-test.yml**\\n\\n```\\nversion: '3.4'\\nservices:\\n redis.data:\\n image: redis:\", \"alpine\\n rabbitmq:\\n image: rabbitmq:3-management-alpine\\n sqldata:\\n```\\n\\n```\\n image: mcr.microsoft.com/\", \"mssql/server:2017-latest\\n nosqldata:\\n image: mongo\\n```\\n\\n## **docker-compose-test.override.yml**\\n\\n```\", \"\\nversion: '3.4'\\nservices:\\n redis.data:\\n ports:\\n - \\\"6379:6379\\\"\\n rabbitmq:\\n ports:\\n - \\\"15672:15672\\\"\\n -\", \" \\\"5672:5672\\\"\\n sqldata:\\n environment:\\n - SA_PASSWORD=Pass@word\\n - ACCEPT_EULA=Y\\n ports:\\n - \\\"5433:1433\", \"\\\"\\n nosqldata:\\n ports:\\n - \\\"27017:27017\\\"\\n```\\n\\nSo, to run the functional/integration tests you must fir\", \"st run this command, from the solution test folder:\\n\\ndocker-compose -f docker-compose-test.yml -f do\", \"cker-compose-test.override.yml up\\n\\nAs you can see, these docker-compose files only start the Redis, \", \"RabbitMQ, SQL Server, and MongoDB microservices.\\n\\n### **Additional resources**\\n\\n- **Unit & Integrati\", \"on testing** on the eShopOnContainers [https://github.com/dotnet-architecture/eShopOnContainers/wiki\", \"/Unit-and-integration](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Unit-and-integr\", \"ation-testing)[testing](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Unit-and-integ\", \"ration-testing)\\n- **Load testing** on the eShopOnContainers <https://github.com/dotnet-architecture/\", \"eShopOnContainers/wiki/Load-testing>\\n\\n## <span id=\\\"page-171-0\\\"></span>Implement background tasks in \", \"microservices with IHostedService and the BackgroundService class\\n\\nBackground tasks and scheduled jo\", \"bs are something you might need to use in any application, whether or not it follows the microservic\", \"es architecture pattern. The difference when using a microservices architecture is that you can impl\", \"ement the background task in a separate process/container for hosting so you can scale it down/up ba\", \"sed on your need.\\n\\nFrom a generic point of view, in .NET we called these type of tasks *Hosted Servi\", \"ces*, because they are services/logic that you host within your host/application/microservice. Note \", \"that in this case, the hosted service simply means a class with the background task logic.\\n\\nSince .N\", \"ET Core 2.0, the framework provides a new interface named [IHostedService](https://docs.microsoft.co\", \"m/dotnet/api/microsoft.extensions.hosting.ihostedservice) helping you to easily implement hosted ser\", \"vices. The basic idea is that you can register multiple background tasks (hosted services) that run \", \"in the background while your web host or host is running, as shown in the image 6-26.\\n\\n![](_page_172\", \"_Figure_2.jpeg)\\n\\n*Figure 6-26. Using IHostedService in a WebHost vs. a Host*\\n\\nASP.NET Core 1.x and 2\", \".x support IWebHost for background processes in web apps. .NET Core 2.1 and later versions support I\", \"Host for background processes with plain console apps. Note the difference made between WebHost and \", \"Host.\\n\\nA WebHost (base class implementing IWebHost) in ASP.NET Core 2.0 is the infrastructure artifa\", \"ct you use to provide HTTP server features to your process, such as when you're implementing an MVC \", \"web app or Web API service. It provides all the new infrastructure goodness in ASP.NET Core, enablin\", \"g you to use dependency injection, insert middlewares in the request pipeline, and similar. The WebH\", \"ost uses these very same IHostedServices for background tasks.\\n\\nA Host (base class implementing IHos\", \"t) was introduced in .NET Core 2.1. Basically, a Host allows you to have a similar infrastructure th\", \"an what you have with WebHost (dependency injection, hosted services, etc.), but in this case, you j\", \"ust want to have a simple and lighter process as the host, with nothing related to MVC, Web API or H\", \"TTP server features.\\n\\nTherefore, you can choose and either create a specialized host-process with IH\", \"ost to handle the hosted services and nothing else, such a microservice made just for hosting the IH\", \"ostedServices, or you can alternatively extend an existing ASP.NET Core WebHost, such as an existing\", \" ASP.NET Core Web API or MVC app.\\n\\nEach approach has pros and cons depending on your business and sc\", \"alability needs. The bottom line is basically that if your background tasks have nothing to do with \", \"HTTP (IWebHost) you should use IHost.\\n\\n## <span id=\\\"page-173-0\\\"></span>**Registering hosted services\", \" in your WebHost or Host**\\n\\nLet's drill down further on the IHostedService interface since its usage\", \" is pretty similar in a WebHost or in a Host.\\n\\nSignalR is one example of an artifact using hosted se\", \"rvices, but you can also use it for much simpler things like:\\n\\n- A background task polling a databas\", \"e looking for changes.\\n- A scheduled task updating some cache periodically.\\n- An implementation of Q\", \"ueueBackgroundWorkItem that allows a task to be executed on a background thread.\\n- Processing messag\", \"es from a message queue in the background of a web app while sharing common services such as ILogger\", \".\\n- A background task started with Task.Run().\\n\\nYou can basically offload any of those actions to a \", \"background task that implements IHostedService.\\n\\nThe way you add one or multiple IHostedServices int\", \"o your WebHost or Host is by registering them up through the [AddHostedService](https://docs.microso\", \"ft.com/dotnet/api/microsoft.extensions.dependencyinjection.servicecollectionhostedserviceextensions.\", \"addhostedservice) extension method in an ASP.NET Core WebHost (or in a Host in .NET Core 2.1 and abo\", \"ve). Basically, you have to register the hosted services within application startup in *Program.cs*.\", \"\\n\\n```\\n//Other DI registrations;\\n// Register Hosted Services\\nbuilder.Services.AddHostedService<GraceP\", \"eriodManagerService>();\\nbuilder.Services.AddHostedService<MyHostedServiceB>();\\nbuilder.Services.AddH\", \"ostedService<MyHostedServiceC>();\\n//...\\n```\\n\\nIn that code, the GracePeriodManagerService hosted serv\", \"ice is real code from the Ordering business microservice in eShopOnContainers, while the other two a\", \"re just two additional samples.\\n\\nThe IHostedService background task execution is coordinated with th\", \"e lifetime of the application (host or microservice, for that matter). You register tasks when the a\", \"pplication starts and you have the opportunity to do some graceful action or clean-up when the appli\", \"cation is shutting down.\\n\\nWithout using IHostedService, you could always start a background thread t\", \"o run any task. The difference is precisely at the app's shutdown time when that thread would simply\", \" be killed without having the opportunity to run graceful clean-up actions.\\n\\n### <span id=\\\"page-173-\", \"1\\\"></span>**The IHostedService interface**\\n\\nWhen you register an IHostedService, .NET calls the Star\", \"tAsync() and StopAsync() methods of your IHostedService type during application start and stop respe\", \"ctively. For more details, see [IHostedService interface.](https://docs.microsoft.com/aspnet/core/fu\", \"ndamentals/host/hosted-services#ihostedservice-interface)\\n\\nAs you can imagine, you can create multip\", \"le implementations of IHostedService and register each of them in *Program.cs*, as shown previously.\", \" All those hosted services will be started and stopped along with the application/microservice.\\n\\nAs \", \"a developer, you are responsible for handling the stopping action of your services when StopAsync() \", \"method is triggered by the host.\\n\\n## <span id=\\\"page-174-0\\\"></span>**Implementing IHostedService with\", \" a custom hosted service class deriving from the BackgroundService base class**\\n\\nYou could go ahead \", \"and create your custom hosted service class from scratch and implement the IHostedService, as you ne\", \"ed to do when using .NET Core 2.0 and later.\\n\\nHowever, since most background tasks will have similar\", \" needs in regard to the cancellation tokens management and other typical operations, there is a conv\", \"enient abstract base class you can derive from, named BackgroundService (available since .NET Core 2\", \".1).\\n\\nThat class provides the main work needed to set up the background task.\\n\\nThe next code is the \", \"abstract BackgroundService base class as implemented in .NET.\\n\\n```\\n// Copyright (c) .NET Foundation.\", \" Licensed under the Apache License, Version 2.0.\\n/// <summary>\\n/// Base class for implementing a lon\", \"g running <see cref=\\\"IHostedService\\\"/>.\\n/// </summary>\\npublic abstract class BackgroundService : IHo\", \"stedService, IDisposable\\n{\\n private Task _executingTask;\\n private readonly CancellationTokenSource _\", \"stoppingCts =\\n new CancellationTokenSource();\\n protected abstract Task ExecuteAsync(CancellationToke\", \"n stoppingToken);\\n public virtual Task StartAsync(CancellationToken cancellationToken)\\n {\\n // Store \", \"the task we're executing\\n _executingTask = ExecuteAsync(_stoppingCts.Token);\\n // If the task is comp\", \"leted then return it,\\n // this will bubble cancellation and failure to the caller\\n if (_executingTas\", \"k.IsCompleted)\\n {\\n return _executingTask;\\n }\\n // Otherwise it's running\\n return Task.CompletedTask;\\n\", \" }\\n public virtual async Task StopAsync(CancellationToken cancellationToken)\\n {\\n // Stop called with\", \"out start\\n if (_executingTask == null)\\n {\\n return;\\n }\\n```\\n\\n```\\n try\\n {\\n // Signal cancellation to th\", \"e executing method\\n _stoppingCts.Cancel();\\n }\\n finally\\n {\\n // Wait until the task completes or the s\", \"top token triggers\\n await Task.WhenAny(_executingTask, Task.Delay(Timeout.Infinite,\\n cancellationTok\", \"en));\\n }\\n }\\n public virtual void Dispose()\\n {\\n _stoppingCts.Cancel();\\n }\\n}\\n```\\n\\nWhen deriving from t\", \"he previous abstract base class, thanks to that inherited implementation, you just need to implement\", \" the ExecuteAsync() method in your own custom hosted service class, as in the following simplified c\", \"ode from eShopOnContainers which is polling a database and publishing integration events into the Ev\", \"ent Bus when needed.\\n\\n```\\npublic class GracePeriodManagerService : BackgroundService\\n{\\n private read\", \"only ILogger<GracePeriodManagerService> _logger;\\n private readonly OrderingBackgroundSettings _setti\", \"ngs;\\n private readonly IEventBus _eventBus;\\n public GracePeriodManagerService(IOptions<OrderingBackg\", \"roundSettings> settings,\\n IEventBus eventBus,\\n                               ILogger<GracePeriodMana\", \"gerService> logger)\\n {\\n // Constructor's parameters validations...\\n }\\n protected override async Task\", \" ExecuteAsync(CancellationToken stoppingToken)\\n {\\n _logger.LogDebug($\\\"GracePeriodManagerService is s\", \"tarting.\\\");\\n stoppingToken.Register(() =>\\n _logger.LogDebug($\\\" GracePeriod background task is stoppi\", \"ng.\\\"));\\n while (!stoppingToken.IsCancellationRequested)\\n {\\n _logger.LogDebug($\\\"GracePeriod task doin\", \"g background work.\\\");\\n // This eShopOnContainers method is querying a database table\\n // and publish\", \"ing events into the Event Bus (RabbitMQ / ServiceBus)\\n CheckConfirmedGracePeriodOrders();\\n try {\\n aw\", \"ait Task.Delay(_settings.CheckUpdateTime, stoppingToken);\\n }\\n catch (TaskCanceledException exception\", \") {\\n _logger.LogCritical(exception, \\\"TaskCanceledException Error\\\",\\nexception.Message);\\n```\\n\\n```\\n }\\n \", \"}\\n _logger.LogDebug($\\\"GracePeriod background task is stopping.\\\");\\n }\\n .../...\\n}\\n```\\n\\nIn this specifi\", \"c case for eShopOnContainers, it's executing an application method that's querying a database table \", \"looking for orders with a specific state and when applying changes, it is publishing integration eve\", \"nts through the event bus (underneath it can be using RabbitMQ or Azure Service Bus).\\n\\nOf course, yo\", \"u could run any other business background task, instead.\\n\\nBy default, the cancellation token is set \", \"with a 5 seconds timeout, although you can change that value when building your WebHost using the Us\", \"eShutdownTimeout extension of the IWebHostBuilder. This means that our service is expected to cancel\", \" within 5 seconds otherwise it will be more abruptly killed.\\n\\nThe following code would be changing t\", \"hat time to 10 seconds.\\n\\n```\\nWebHost.CreateDefaultBuilder(args)\\n .UseShutdownTimeout(TimeSpan.FromSe\", \"conds(10))\\n ...\\n```\\n\\n### **Summary class diagram**\\n\\nThe following image shows a visual summary of th\", \"e classes and interfaces involved when implementing IHostedServices.\\n\\n![](_page_176_Figure_9.jpeg)\\n\\n\", \"*Figure 6-27. Class diagram showing the multiple classes and interfaces related to IHostedService*\\n\\n\", \"Class diagram: IWebHost and IHost can host many services, which inherit from BackgroundService, whic\", \"h implements IHostedService.\\n\\n## **Deployment considerations and takeaways**\\n\\nIt is important to not\", \"e that the way you deploy your ASP.NET Core WebHost or .NET Host might impact the final solution. Fo\", \"r instance, if you deploy your WebHost on IIS or a regular Azure App Service, your host can be shut \", \"down because of app pool recycles. But if you are deploying your host as a container into an orchest\", \"rator like Kubernetes, you can control the assured number of live instances of your host. In additio\", \"n, you could consider other approaches in the cloud especially made for these scenarios, like Azure \", \"Functions. Finally, if you need the service to be running all the time and are deploying on a Window\", \"s Server you could use a Windows Service.\\n\\nBut even for a WebHost deployed into an app pool, there a\", \"re scenarios like repopulating or flushing application's in-memory cache that would be still applica\", \"ble.\\n\\nThe IHostedService interface provides a convenient way to start background tasks in an ASP.NET\", \" Core web application (in .NET Core 2.0 and later versions) or in any process/host (starting in .NET\", \" Core 2.1 with IHost). Its main benefit is the opportunity you get with the graceful cancellation to\", \" clean-up the code of your background tasks when the host itself is shutting down.\\n\\n### <span id=\\\"pa\", \"ge-177-0\\\"></span>**Additional resources**\\n\\n- **Building a scheduled task in ASP.NET Core/Standard 2.\", \"0** [https://blog.maartenballiauw.be/post/2017/08/01/building-a-scheduled-cache-updater-in](https://\", \"blog.maartenballiauw.be/post/2017/08/01/building-a-scheduled-cache-updater-in-aspnet-core-2.html)[as\", \"pnet-core-2.html](https://blog.maartenballiauw.be/post/2017/08/01/building-a-scheduled-cache-updater\", \"-in-aspnet-core-2.html)\\n- **Implementing IHostedService in ASP.NET Core 2.0** <https://www.stevejgor\", \"don.co.uk/asp-net-core-2-ihostedservice>\\n- **GenericHost Sample using ASP.NET Core 2.1** <https://gi\", \"thub.com/aspnet/Hosting/tree/release/2.1/samples/GenericHostSample>\\n\\n## <span id=\\\"page-177-1\\\"></span\", \">Implement API Gateways with Ocelot\\n\\n### **Important**\\n\\nThe reference microservice application [eSho\", \"pOnContainers](https://github.com/dotnet-architecture/eShopOnContainers) is currently using features\", \" provided by [Envoy](https://www.envoyproxy.io/) to implement the API Gateway instead of the earlier\", \" referenced [Ocelot.](https://github.com/ThreeMammals/Ocelot) We made this design choice because of \", \"Envoy's built-in support for the WebSocket protocol, required by the new gRPC inter-service communic\", \"ations implemented in eShopOnContainers. However, we've retained this section in the guide so you ca\", \"n consider Ocelot as a simple, capable, and lightweight API Gateway suitable for production-grade sc\", \"enarios. Also, latest Ocelot version contains a breaking change on its json schema. Consider using O\", \"celot < v16.0.0, or use the key Routes instead of ReRoutes.\\n\\n### <span id=\\\"page-177-2\\\"></span>**Arch\", \"itect and design your API Gateways**\\n\\nThe following architecture diagram shows how API Gateways were\", \" implemented with Ocelot in eShopOnContainers.\\n\\n![](_page_178_Figure_2.jpeg)\\n\\n*Figure 6-28. eShopOnC\", \"ontainers architecture with API Gateways*\\n\\nThat diagram shows how the whole application is deployed \", \"into a single Docker host or development PC with \\\"Docker for Windows\\\" or \\\"Docker for Mac\\\". However, \", \"deploying into any orchestrator would be similar, but any container in the diagram could be scaled o\", \"ut in the orchestrator.\\n\\nIn addition, the infrastructure assets such as databases, cache, and messag\", \"e brokers should be offloaded from the orchestrator and deployed into high available systems for inf\", \"rastructure, like Azure SQL Database, Azure Cosmos DB, Azure Redis, Azure Service Bus, or any HA clu\", \"stering solution onpremises.\\n\\nAs you can also notice in the diagram, having several API Gateways all\", \"ows multiple development teams to be autonomous (in this case Marketing features vs. Shopping featur\", \"es) when developing and deploying their microservices plus their own related API Gateways.\\n\\nIf you h\", \"ad a single monolithic API Gateway that would mean a single point to be updated by several developme\", \"nt teams, which could couple all the microservices with a single part of the application.\\n\\nGoing muc\", \"h further in the design, sometimes a fine-grained API Gateway can also be limited to a single busine\", \"ss microservice depending on the chosen architecture. Having the API Gateway's boundaries dictated b\", \"y the business or domain will help you to get a better design.\\n\\nFor instance, fine granularity in th\", \"e API Gateway tier can be especially useful for more advanced composite UI applications that are bas\", \"ed on microservices, because the concept of a fine-grained API Gateway is similar to a UI compositio\", \"n service.\\n\\nWe delve into more details in the previous section Creating composite UI based on micros\", \"ervices.\\n\\nAs a key takeaway, for many medium- and large-size applications, using a custom-built API \", \"Gateway product is usually a good approach, but not as a single monolithic aggregator or unique cent\", \"ral custom API Gateway unless that API Gateway allows multiple independent configuration areas for t\", \"he several development teams creating autonomous microservices.\\n\\n## **Sample microservices/container\", \"s to reroute through the API Gateways**\\n\\nAs an example, eShopOnContainers has around six internal mi\", \"croservice-types that have to be published through the API Gateways, as shown in the following image\", \".\\n\\n![](_page_179_Picture_2.jpeg)\\n\\nAbout the Identity service, in the design it's left out of the API\", \" Gateway routing because it's the only cross-cutting concern in the system, although with Ocelot it'\", \"s also possible to include it as part of the rerouting lists.\\n\\nAll those services are currently impl\", \"emented as ASP.NET Core Web API services, as you can tell from the code. Let's focus on one of the m\", \"icroservices like the Catalog microservice code.\\n\\n![](_page_179_Picture_6.jpeg)\\n\\n*Figure 6-30. Sampl\", \"e Web API microservice (Catalog microservice)*\\n\\nYou can see that the Catalog microservice is a typic\", \"al ASP.NET Core Web API project with several controllers and methods like in the following code.\\n\\n``\", \"`\\n[HttpGet]\\n[Route(\\\"items/{id:int}\\\")]\\n[ProducesResponseType((int)HttpStatusCode.BadRequest)]\\n[Produc\", \"esResponseType((int)HttpStatusCode.NotFound)]\\n[ProducesResponseType(typeof(CatalogItem),(int)HttpSta\", \"tusCode.OK)]\\npublic async Task<IActionResult> GetItemById(int id)\\n{\\n if (id <= 0)\\n {\\n return BadRequ\", \"est();\\n }\\n var item = await _catalogContext.CatalogItems.\\n SingleOrDefaultAsync(ci => ci.Id == id);\\n\", \" //\\u2026\\n if (item != null)\\n {\\n return Ok(item);\\n }\\n return NotFound();\\n}\\n```\\n\\nThe HTTP request will end\", \" up running that kind of C# code accessing the microservice database and any additional required act\", \"ion.\\n\\nRegarding the microservice URL, when the containers are deployed in your local development PC \", \"(local Docker host), each microservice's container always has an internal port (usually port 80) spe\", \"cified in its dockerfile, as in the following dockerfile:\\n\\n```\\nFROM mcr.microsoft.com/dotnet/aspnet:\", \"7.0 AS base\\nWORKDIR /app\\nEXPOSE 80\\n```\\n\\nThe port 80 shown in the code is internal within the Docker \", \"host, so it can't be reached by client apps.\\n\\nClient apps can access only the external ports (if any\", \") published when deploying with dockercompose.\\n\\nThose external ports shouldn't be published when dep\", \"loying to a production environment. For this specific reason, why you want to use the API Gateway, t\", \"o avoid the direct communication between the client apps and the microservices.\\n\\nHowever, when devel\", \"oping, you want to access the microservice/container directly and run it through Swagger. That's why\", \" in eShopOnContainers, the external ports are still specified even when they won't be used by the AP\", \"I Gateway or the client apps.\\n\\nHere's an example of the docker-compose.override.yml file for the Cat\", \"alog microservice:\\n\\n```\\ncatalog-api:\\n environment:\\n - ASPNETCORE_ENVIRONMENT=Development\\n - ASPNETCO\", \"RE_URLS=http://0.0.0.0:80\\n - ConnectionString=YOUR_VALUE\\n - ... Other Environment Variables\\n```\\n\\n```\", \"\\n ports:\\n - \\\"5101:80\\\" # Important: In a production environment you should remove the external \\nport \", \"(5101) kept here for microservice debugging purposes.\\n # The API Gateway redirects and access throug\", \"h the internal port (80).\\n```\\n\\nYou can see how in the docker-compose.override.yml configuration the \", \"internal port for the Catalog container is port 80, but the port for external access is 5101. But th\", \"is port shouldn't be used by the application when using an API Gateway, only to debug, run, and test\", \" just the Catalog microservice.\\n\\nNormally, you won't be deploying with docker-compose into a product\", \"ion environment because the right production deployment environment for microservices is an orchestr\", \"ator like Kubernetes or Service Fabric. When deploying to those environments you use different confi\", \"guration files where you won't publish directly any external port for the microservices but, you'll \", \"always use the reverse proxy from the API Gateway.\\n\\nRun the catalog microservice in your local Docke\", \"r host. Either run the full eShopOnContainers solution from Visual Studio (it runs all the services \", \"in the docker-compose files), or start the Catalog microservice with the following docker-compose co\", \"mmand in CMD or PowerShell positioned at the folder where the docker-compose.yml and docker-compose.\", \"override.yml are placed.\\n\\n```\\ndocker-compose run --service-ports catalog-api\\n```\\n\\nThis command only \", \"runs the catalog-api service container plus dependencies that are specified in the docker-compose.ym\", \"l. In this case, the SQL Server container and RabbitMQ container.\\n\\nThen, you can directly access the\", \" Catalog microservice and see its methods through the Swagger UI accessing directly through that \\\"ex\", \"ternal\\\" port, in this case http://host.docker.internal:5101/swagger:\\n\\n![](_page_182_Picture_0.jpeg)\\n\", \"\\n*Figure 6-31. Testing the Catalog microservice with its Swagger UI*\\n\\nAt this point, you could set a\", \" breakpoint in C# code in Visual Studio, test the microservice with the methods exposed in Swagger U\", \"I, and finally clean-up everything with the docker-compose down command.\\n\\nHowever, direct-access com\", \"munication to the microservice, in this case through the external port 5101, is precisely what you w\", \"ant to avoid in your application. And you can avoid that by setting the additional level of indirect\", \"ion of the API Gateway (Ocelot, in this case). That way, the client app won't directly access the mi\", \"croservice.\\n\\n### <span id=\\\"page-182-0\\\"></span>**Implementing your API Gateways with Ocelot**\\n\\nOcelot\", \" is basically a set of middleware that you can apply in a specific order.\\n\\nOcelot is designed to wor\", \"k with ASP.NET Core only. The latest version of the package is 18.0 which targets .NET 6 and hence i\", \"s not suitable for .NET Framework applications.\\n\\nYou install Ocelot and its dependencies in your ASP\", \".NET Core project with [Ocelot's NuGet package](https://www.nuget.org/packages/Ocelot/), from Visual\", \" Studio.\\n\\n```\\nInstall-Package Ocelot\\n```\\n\\nIn eShopOnContainers, its API Gateway implementation is a \", \"simple ASP.NET Core WebHost project, and Ocelot's middleware handles all the API Gateway features, a\", \"s shown in the following image:\\n\\n![](_page_183_Picture_3.jpeg)\\n\\n*Figure 6-32. The OcelotApiGw base p\", \"roject in eShopOnContainers*\\n\\nThis ASP.NET Core WebHost project is built with two simple files: Prog\", \"ram.cs and Startup.cs.\\n\\nThe Program.cs just needs to create and configure the typical ASP.NET Core B\", \"uildWebHost.\\n\\n```\\nnamespace OcelotApiGw\\n{\\n public class Program\\n {\\n public static void Main(string[]\", \" args)\\n {\\n BuildWebHost(args).Run();\\n }\\n public static IWebHost BuildWebHost(string[] args)\\n {\\n var \", \"builder = WebHost.CreateDefaultBuilder(args);\\n builder.ConfigureServices(s => s.AddSingleton(builder\", \"))\\n .ConfigureAppConfiguration(\\n ic => ic.AddJsonFile(Path.Combine(\\\"configuration\\\",\\n \\\"configuration.\", \"json\\\")))\\n .UseStartup<Startup>();\\n var host = builder.Build();\\n return host;\\n }\\n }\\n}\\n```\\n\\nThe import\", \"ant point here for Ocelot is the configuration.json file that you must provide to the builder throug\", \"h the AddJsonFile() method. That configuration.json is where you specify all the API Gateway ReRoute\", \"s, meaning the external endpoints with specific ports and the correlated internal endpoints, usually\", \" using different ports.\\n\\n```\\n{\\n \\\"ReRoutes\\\": [],\\n \\\"GlobalConfiguration\\\": {}\\n}\\n```\\n\\nThere are two sect\", \"ions to the configuration. An array of ReRoutes and a GlobalConfiguration. The ReRoutes are the obje\", \"cts that tell Ocelot how to treat an upstream request. The Global configuration allows overrides of \", \"ReRoute specific settings. It's useful if you don't want to manage lots of ReRoute specific settings\", \".\\n\\nHere's a simplified example of [ReRoute configuration file](https://github.com/dotnet-architectur\", \"e/eShopOnContainers/blob/main/src/ApiGateways/Mobile.Bff.Shopping/apigw/configuration.json) from one\", \" of the API Gateways from eShopOnContainers.\\n\\n```\\n{\\n \\\"ReRoutes\\\": [\\n {\\n \\\"DownstreamPathTemplate\\\": \\\"/a\", \"pi/{version}/{everything}\\\",\\n \\\"DownstreamScheme\\\": \\\"http\\\",\\n \\\"DownstreamHostAndPorts\\\": [\\n {\\n \\\"Host\\\": \\\"c\", \"atalog-api\\\",\\n \\\"Port\\\": 80\\n }\\n ],\\n \\\"UpstreamPathTemplate\\\": \\\"/api/{version}/c/{everything}\\\",\\n \\\"Upstream\", \"HttpMethod\\\": [ \\\"POST\\\", \\\"PUT\\\", \\\"GET\\\" ]\\n },\\n {\\n \\\"DownstreamPathTemplate\\\": \\\"/api/{version}/{everything}\", \"\\\",\\n \\\"DownstreamScheme\\\": \\\"http\\\",\\n \\\"DownstreamHostAndPorts\\\": [\\n {\\n \\\"Host\\\": \\\"basket-api\\\",\\n \\\"Port\\\": 80\\n \", \"}\\n ],\\n \\\"UpstreamPathTemplate\\\": \\\"/api/{version}/b/{everything}\\\",\\n \\\"UpstreamHttpMethod\\\": [ \\\"POST\\\", \\\"PU\", \"T\\\", \\\"GET\\\" ],\\n \\\"AuthenticationOptions\\\": {\\n \\\"AuthenticationProviderKey\\\": \\\"IdentityApiKey\\\",\\n \\\"AllowedSc\", \"opes\\\": []\\n }\\n }\\n ],\\n \\\"GlobalConfiguration\\\": {\\n \\\"RequestIdKey\\\": \\\"OcRequestId\\\",\\n \\\"AdministrationPath\\\":\", \" \\\"/administration\\\"\\n }\\n }\\n```\\n\\nThe main functionality of an Ocelot API Gateway is to take incoming HT\", \"TP requests and forward them on to a downstream service, currently as another HTTP request. Ocelot's\", \" describes the routing of one request to another as a ReRoute.\\n\\nFor instance, let's focus on one of \", \"the ReRoutes in the configuration.json from above, the configuration for the Basket microservice.\\n\\n`\", \"``\\n{\\n \\\"DownstreamPathTemplate\\\": \\\"/api/{version}/{everything}\\\",\\n \\\"DownstreamScheme\\\": \\\"http\\\",\\n \\\"Downst\", \"reamHostAndPorts\\\": [\\n {\\n \\\"Host\\\": \\\"basket-api\\\",\\n \\\"Port\\\": 80\\n }\\n ],\\n \\\"UpstreamPathTemplate\\\": \\\"/api/{ve\", \"rsion}/b/{everything}\\\",\\n \\\"UpstreamHttpMethod\\\": [ \\\"POST\\\", \\\"PUT\\\", \\\"GET\\\" ],\\n \\\"AuthenticationOptions\\\": {\", \"\\n \\\"AuthenticationProviderKey\\\": \\\"IdentityApiKey\\\",\\n \\\"AllowedScopes\\\": []\\n }\\n}\\n```\\n\\nThe DownstreamPathTe\", \"mplate, Scheme, and DownstreamHostAndPorts make the internal microservice URL that this request will\", \" be forwarded to.\\n\\nThe port is the internal port used by the service. When using containers, the por\", \"t specified at its dockerfile.\\n\\nThe Host is a service name that depends on the service name resoluti\", \"on you are using. When using docker-compose, the services names are provided by the Docker Host, whi\", \"ch is using the service names provided in the docker-compose files. If using an orchestrator like Ku\", \"bernetes or Service Fabric, that name should be resolved by the DNS or name resolution provided by e\", \"ach orchestrator.\\n\\nDownstreamHostAndPorts is an array that contains the host and port of any downstr\", \"eam services that you wish to forward requests to. Usually this configuration will just contain one \", \"entry but sometimes you might want to load balance requests to your downstream services and Ocelot l\", \"ets you add more than one entry and then select a load balancer. But if using Azure and any orchestr\", \"ator it is probably a better idea to load balance with the cloud and orchestrator infrastructure.\\n\\nT\", \"he UpstreamPathTemplate is the URL that Ocelot will use to identify which DownstreamPathTemplate to \", \"use for a given request from the client. Finally, the UpstreamHttpMethod is used so Ocelot can disti\", \"nguish between different requests (GET, POST, PUT) to the same URL.\\n\\nAt this point, you could have a\", \" single Ocelot API Gateway (ASP.NET Core WebHost) using one or [multiple merged configuration.json f\", \"iles](https://ocelot.readthedocs.io/en/latest/features/configuration.html#merging-configuration-file\", \"s) or you can also store the [configuration in a Consul KV store.](https://ocelot.readthedocs.io/en/\", \"latest/features/configuration.html#store-configuration-in-consul)\\n\\nBut as introduced in the architec\", \"ture and design sections, if you really want to have autonomous microservices, it might be better to\", \" split that single monolithic API Gateway into multiple API Gateways and/or BFF (Backend for Fronten\", \"d). For that purpose, let's see how to implement that approach with Docker containers.\\n\\n## **Using a\", \" single Docker container image to run multiple different API Gateway / BFF container types**\\n\\nIn eSh\", \"opOnContainers, we're using a single Docker container image with the Ocelot API Gateway but then, at\", \" run time, we create different services/containers for each type of API-Gateway/BFF by providing a d\", \"ifferent configuration.json file, using a docker volume to access a different PC folder for each ser\", \"vice.\\n\\n![](_page_186_Figure_2.jpeg)\\n\\n*Figure 6-33. Reusing a single Ocelot Docker image across multi\", \"ple API Gateway types*\\n\\nIn eShopOnContainers, the \\\"Generic Ocelot API Gateway Docker Image\\\" is creat\", \"ed with the project named 'OcelotApiGw' and the image name \\\"eshop/ocelotapigw\\\" that is specified in \", \"the dockercompose.yml file. Then, when deploying to Docker, there will be four API-Gateway container\", \"s created from that same Docker image, as shown in the following extract from the docker-compose.yml\", \" file.\\n\\n```\\n mobileshoppingapigw:\\n image: eshop/ocelotapigw:${TAG:-latest}\\n```\\n\\n```\\n build:\\n context\", \": .\\n dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\\n mobilemarketingapigw:\\n image: eshop/ocelotap\", \"igw:${TAG:-latest}\\n build:\\n context: .\\n dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\\n webshoppi\", \"ngapigw:\\n image: eshop/ocelotapigw:${TAG:-latest}\\n build:\\n context: .\\n dockerfile: src/ApiGateways/A\", \"piGw-Base/Dockerfile\\n webmarketingapigw:\\n image: eshop/ocelotapigw:${TAG:-latest}\\n build:\\n context: \", \".\\n dockerfile: src/ApiGateways/ApiGw-Base/Dockerfile\\n```\\n\\nAdditionally, as you can see in the follow\", \"ing docker-compose.override.yml file, the only difference between those API Gateway containers is th\", \"e Ocelot configuration file, which is different for each service container and it's specified at run\", \" time through a Docker volume.\\n\\n```\\nmobileshoppingapigw:\\n environment:\\n - ASPNETCORE_ENVIRONMENT=Dev\", \"elopment\\n - IdentityUrl=http://identity-api\\n ports:\\n - \\\"5200:80\\\"\\n volumes:\\n - ./src/ApiGateways/Mobi\", \"le.Bff.Shopping/apigw:/app/configuration\\nmobilemarketingapigw:\\n environment:\\n - ASPNETCORE_ENVIRONME\", \"NT=Development\\n - IdentityUrl=http://identity-api\\n ports:\\n - \\\"5201:80\\\"\\n volumes:\\n - ./src/ApiGateway\", \"s/Mobile.Bff.Marketing/apigw:/app/configuration\\nwebshoppingapigw:\\n environment:\\n - ASPNETCORE_ENVIRO\", \"NMENT=Development\\n - IdentityUrl=http://identity-api\\n ports:\\n - \\\"5202:80\\\"\\n volumes:\\n - ./src/ApiGate\", \"ways/Web.Bff.Shopping/apigw:/app/configuration\\nwebmarketingapigw:\\n environment:\\n - ASPNETCORE_ENVIRO\", \"NMENT=Development\\n - IdentityUrl=http://identity-api\\n ports:\\n - \\\"5203:80\\\"\\n```\\n\\n### volumes**:**\\n\\n**-\", \"** ./src/ApiGateways/Web.Bff.Marketing/apigw:/app/configuration\\n\\nBecause of that previous code, and \", \"as shown in the Visual Studio Explorer below, the only file needed to define each specific business/\", \"BFF API Gateway is just a configuration.json file, because the four API Gateways are based on the sa\", \"me Docker image.\\n\\n![](_page_188_Picture_3.jpeg)\\n\\n*Figure 6-34. The only file needed to define each A\", \"PI Gateway / BFF with Ocelot is a configuration file*\\n\\nBy splitting the API Gateway into multiple AP\", \"I Gateways, different development teams focusing on different subsets of microservices can manage th\", \"eir own API Gateways by using independent Ocelot configuration files. Plus, at the same time they ca\", \"n reuse the same Ocelot Docker image.\\n\\nNow, if you run eShopOnContainers with the API Gateways (incl\", \"uded by default in VS when opening eShopOnContainers-ServicesAndWebApps.sln solution or if running \\\"\", \"docker-compose up\\\"), the following sample routes will be performed.\\n\\nFor instance, when visiting the\", \" upstream URL\\n\\nhttp://host.docker.internal:5202/api/v1/c/catalog/items/2/ served by the webshoppinga\", \"pigw API Gateway, you get the same result from the internal Downstream URL http://catalog-api/api/v1\", \"/2 within the Docker host, as in the following browser.\\n\\n![](_page_188_Picture_9.jpeg)\\n\\n*Figure 6-35\", \". Accessing a microservice through a URL provided by the API Gateway*\\n\\nBecause of testing or debuggi\", \"ng reasons, if you wanted to directly access to the Catalog Docker container (only at the developmen\", \"t environment) without passing through the API Gateway, since 'catalog-api' is a DNS resolution inte\", \"rnal to the Docker host (service discovery handled by dockercompose service names), the only way to \", \"directly access the container is through the external port published in the docker-compose.override.\", \"yml, which is provided only for development tests, such as http://host.docker.internal:5101/api/v1/C\", \"atalog/items/1 in the following browser.\\n\\n![](_page_189_Picture_0.jpeg)\\n\\nBut the application is conf\", \"igured so it accesses all the microservices through the API Gateways, not through the direct port \\\"s\", \"hortcuts\\\".\\n\\n## **The Gateway aggregation pattern in eShopOnContainers**\\n\\nAs introduced previously, a\", \" flexible way to implement requests aggregation is with custom services, by code. You could also imp\", \"lement request aggregation with the [Request Aggregation feature in Ocelot,](https://ocelot.readthed\", \"ocs.io/en/latest/features/requestaggregation.html#request-aggregation)  but it might not be as flexi\", \"ble as you need. Therefore, the selected way to implement aggregation in eShopOnContainers is with a\", \"n explicit ASP.NET Core Web API service for each aggregator.\\n\\nAccording to that approach, the API Ga\", \"teway composition diagram is in reality a bit more extended when considering the aggregator services\", \" that are not shown in the simplified global architecture diagram shown previously.\\n\\nIn the followin\", \"g diagram, you can also see how the aggregator services work with their related API Gateways.\\n\\n![](_\", \"page_189_Figure_9.jpeg)\\n\\n*Figure 6-37. eShopOnContainers architecture with aggregator services*\\n\\nZoo\", \"ming in further, on the \\\"Shopping\\\" business area in the following image, you can see that chattiness\", \" between the client apps and the microservices is reduced when using the aggregator services in the \", \"API Gateways.\\n\\n![](_page_190_Figure_2.jpeg)\\n\\n*Figure 6-38. Zoom in vision of the Aggregator services\", \"*\\n\\nYou can notice how when the diagram shows the possible requests coming from the API Gateways it c\", \"an get complex. On the other hand, when you use the aggregator pattern, you can see how the arrows i\", \"n blue would simplify the communication from a client app perspective. This pattern not only helps t\", \"o reduce the chattiness and latency in the communication, it also improves the user experience signi\", \"ficantly for the remote apps (mobile and SPA apps).\\n\\nIn the case of the \\\"Marketing\\\" business area an\", \"d microservices, it is a simple use case so there was no need to use aggregators, but it could also \", \"be possible, if needed.\\n\\n### **Authentication and authorization in Ocelot API Gateways**\\n\\nIn an Ocel\", \"ot API Gateway, you can sit the authentication service, such as an ASP.NET Core Web API service usin\", \"g [IdentityServer](https://docs.microsoft.com/dotnet/architecture/cloud-native/identity-server) prov\", \"iding the auth token, either out or inside the API Gateway.\\n\\nSince eShopOnContainers is using multip\", \"le API Gateways with boundaries based on BFF and business areas, the Identity/Auth service is left o\", \"ut of the API Gateways, as highlighted in yellow in the following diagram.\\n\\n![](_page_191_Figure_0.j\", \"peg)\\n\\nHowever, Ocelot also supports sitting the Identity/Auth microservice within the API Gateway bo\", \"undary, as in this other diagram.\\n\\n![](_page_191_Figure_3.jpeg)\\n\\n*Figure 6-40. Authentication in Oce\", \"lot*\\n\\nAs the previous diagram shows, when the Identity microservice is beneath the API gateway (AG):\", \" 1) AG requests an auth token from identity microservice, 2) The identity microservice returns token\", \" to AG, 3- 4) AG requests from microservices using the auth token. Because eShopOnContainers applica\", \"tion has split the API Gateway into multiple BFF (Backend for Frontend) and business areas API Gatew\", \"ays, another option would have been to create an additional API Gateway for cross-cutting concerns. \", \"That choice would be fair in a more complex microservice based architecture with multiple cross-cutt\", \"ing concerns microservices. Since there's only one cross-cutting concern in eShopOnContainers, it wa\", \"s decided to just handle the security service out of the API Gateway realm, for simplicity's sake.\\n\\n\", \"In any case, if the app is secured at the API Gateway level, the authentication module of the Ocelot\", \" API Gateway is visited at first when trying to use any secured microservice. That redirects the HTT\", \"P request to visit the Identity or auth microservice to get the access token so you can visit the pr\", \"otected services with the access\\\\_token.\\n\\nThe way you secure with authentication any service at the \", \"API Gateway level is by setting the AuthenticationProviderKey in its related settings at the configu\", \"ration.json.\\n\\n```\\n {\\n \\\"DownstreamPathTemplate\\\": \\\"/api/{version}/{everything}\\\",\\n \\\"DownstreamScheme\\\": \", \"\\\"http\\\",\\n \\\"DownstreamHostAndPorts\\\": [\\n {\\n \\\"Host\\\": \\\"basket-api\\\",\\n \\\"Port\\\": 80\\n }\\n ],\\n \\\"UpstreamPathTemp\", \"late\\\": \\\"/api/{version}/b/{everything}\\\",\\n \\\"UpstreamHttpMethod\\\": [],\\n \\\"AuthenticationOptions\\\": {\\n \\\"Aut\", \"henticationProviderKey\\\": \\\"IdentityApiKey\\\",\\n \\\"AllowedScopes\\\": []\\n }\\n }\\n```\\n\\nWhen Ocelot runs, it will\", \" look at the ReRoutes AuthenticationOptions.AuthenticationProviderKey and check that there is an Aut\", \"hentication Provider registered with the given key. If there isn't, then Ocelot will not start up. I\", \"f there is, then the ReRoute will use that provider when it executes.\\n\\nBecause the Ocelot WebHost is\", \" configured with the authenticationProviderKey = \\\"IdentityApiKey\\\", that will require authentication \", \"whenever that service has any requests without any auth token.\\n\\n```\\nnamespace OcelotApiGw\\n{\\n public \", \"class Startup\\n {\\n private readonly IConfiguration _cfg;\\n public Startup(IConfiguration configuration\", \") => _cfg = configuration;\\n public void ConfigureServices(IServiceCollection services)\\n {\\n var ident\", \"ityUrl = _cfg.GetValue<string>(\\\"IdentityUrl\\\");\\n var authenticationProviderKey = \\\"IdentityApiKey\\\";\\n /\", \"/\\u2026\\n services.AddAuthentication()\\n .AddJwtBearer(authenticationProviderKey, x =>\\n {\\n x.Authority = id\", \"entityUrl;\\n x.RequireHttpsMetadata = false;\\n x.TokenValidationParameters = new\\nMicrosoft.IdentityMod\", \"el.Tokens.TokenValidationParameters()\\n {\\n ValidAudiences = new[] { \\\"orders\\\", \\\"basket\\\", \\\"locations\\\",\\n\", \"\\\"marketing\\\", \\\"mobileshoppingagg\\\", \\\"webshoppingagg\\\" }\\n };\\n });\\n //...\\n```\\n\\n```\\n }\\n }\\n}\\n```\\n\\nThen, you\", \" also need to set authorization with the [Authorize] attribute on any resource to be accessed like t\", \"he microservices, such as in the following Basket microservice controller.\\n\\n```\\nnamespace Microsoft.\", \"eShopOnContainers.Services.Basket.API.Controllers\\n{\\n [Route(\\\"api/v1/[controller]\\\")]\\n [Authorize]\\n pu\", \"blic class BasketController : Controller\\n {\\n //...\\n }\\n}\\n```\\n\\nThe ValidAudiences such as \\\"basket\\\" are\", \" correlated with the audience defined in each microservice with AddJwtBearer() at the ConfigureServi\", \"ces() of the Startup class, such as in the code below.\\n\\n```\\n// prevent from mapping \\\"sub\\\" claim to n\", \"ameidentifier.\\nJwtSecurityTokenHandler.DefaultInboundClaimTypeMap.Clear();\\nvar identityUrl = Configu\", \"ration.GetValue<string>(\\\"IdentityUrl\\\");\\nservices.AddAuthentication(options =>\\n{\\n options.DefaultAuth\", \"enticateScheme = JwtBearerDefaults.AuthenticationScheme;\\n options.DefaultChallengeScheme = JwtBearer\", \"Defaults.AuthenticationScheme;\\n}).AddJwtBearer(options =>\\n{\\n options.Authority = identityUrl;\\n optio\", \"ns.RequireHttpsMetadata = false;\\n options.Audience = \\\"basket\\\";\\n});\\n```\\n\\nIf you try to access any sec\", \"ured microservice, like the Basket microservice with a ReRoute URL based on the API Gateway like htt\", \"p://host.docker.internal:5202/api/v1/b/basket/1, then you'll get a 401 Unauthorized unless you provi\", \"de a valid token. On the other hand, if a ReRoute URL is authenticated, Ocelot will invoke whatever \", \"downstream scheme is associated with it (the internal microservice URL).\\n\\n**Authorization at Ocelot'\", \"s ReRoutes tier.** Ocelot supports claims-based authorization evaluated after the authentication. Yo\", \"u set the authorization at a route level by adding the following lines to the ReRoute configuration.\", \"\\n\\n```\\n\\\"RouteClaimsRequirement\\\": {\\n \\\"UserType\\\": \\\"employee\\\"\\n}\\n```\\n\\nIn that example, when the authoriza\", \"tion middleware is called, Ocelot will find if the user has the claim type 'UserType' in the token a\", \"nd if the value of that claim is 'employee'. If it isn't, then the user will not be authorized and t\", \"he response will be 403 forbidden.\\n\\n## <span id=\\\"page-194-0\\\"></span>**Using Kubernetes Ingress plus \", \"Ocelot API Gateways**\\n\\nWhen using Kubernetes (like in an Azure Kubernetes Service cluster), you usua\", \"lly unify all the HTTP requests through the [Kubernetes Ingress tier](https://kubernetes.io/docs/con\", \"cepts/services-networking/ingress/) based on *Nginx*.\\n\\nIn Kubernetes, if you don't use any ingress a\", \"pproach, then your services and pods have IPs only routable by the cluster network.\\n\\nBut if you use \", \"an ingress approach, you'll have a middle tier between the Internet and your services (including you\", \"r API Gateways), acting as a reverse proxy.\\n\\nAs a definition, an Ingress is a collection of rules th\", \"at allow inbound connections to reach the cluster services. An ingress is configured to provide serv\", \"ices externally reachable URLs, load balance traffic, SSL termination and more. Users request ingres\", \"s by POSTing the Ingress resource to the API server.\\n\\nIn eShopOnContainers, when developing locally \", \"and using just your development machine as the Docker host, you are not using any ingress but only t\", \"he multiple API Gateways.\\n\\nHowever, when targeting a \\\"production\\\" environment based on Kubernetes, e\", \"ShopOnContainers is using an ingress in front of the API gateways. That way, the clients still call \", \"the same base URL but the requests are routed to multiple API Gateways or BFF.\\n\\nAPI Gateways are fro\", \"nt-ends or fa\\u00e7ades surfacing only the services but not the web applications that are usually out of \", \"their scope. In addition, the API Gateways might hide certain internal microservices.\\n\\nThe ingress, \", \"however, is just redirecting HTTP requests but not trying to hide any microservice or web app.\\n\\nHavi\", \"ng an ingress Nginx tier in Kubernetes in front of the web applications plus the several Ocelot API \", \"Gateways / BFF is the ideal architecture, as shown in the following diagram.\\n\\n![](_page_194_Figure_1\", \"1.jpeg)\\n\\n*Figure 6-41. The ingress tier in eShopOnContainers when deployed into Kubernetes*\\n\\nA Kuber\", \"netes Ingress acts as a reverse proxy for all traffic to the app, including the web applications, th\", \"at are out of the Api gateway scope. When you deploy eShopOnContainers into Kubernetes, it\\n\\nexposes \", \"just a few services or endpoints via *ingress*, basically the following list of postfixes on the URL\", \"s:\\n\\n- / for the client SPA web application \\u2022 /webmvc for the client MVC web application\\n- /webstatus\", \" for the client web app showing the status/healthchecks\\n- /webshoppingapigw for the web BFF and shop\", \"ping business processes\\n- /webmarketingapigw for the web BFF and marketing business processes\\n- /mob\", \"ileshoppingapigw for the mobile BFF and shopping business processes\\n- /mobilemarketingapigw for the \", \"mobile BFF and marketing business processes\\n\\nWhen deploying to Kubernetes, each Ocelot API Gateway i\", \"s using a different \\\"configuration.json\\\" file for each *pod* running the API Gateways. Those \\\"config\", \"uration.json\\\" files are provided by mounting (originally with the deploy.ps1 script) a volume create\", \"d based on a Kubernetes *config map* named 'ocelot'. Each container mounts its related configuration\", \" file in the container's folder named /app/configuration.\\n\\nIn the source code files of eShopOnContai\", \"ners, the original \\\"configuration.json\\\" files can be found within the k8s/ocelot/ folder. There's on\", \"e file for each BFF/APIGateway.\\n\\n## <span id=\\\"page-195-0\\\"></span>**Additional cross-cutting features\", \" in an Ocelot API Gateway**\\n\\nThere are other important features to research and use, when using an O\", \"celot API Gateway, described in the following links.\\n\\n- **Service discovery in the client side integ\", \"rating Ocelot with Consul or Eureka** <https://ocelot.readthedocs.io/en/latest/features/servicedisco\", \"very.html>\\n- **Caching at the API Gateway tier** <https://ocelot.readthedocs.io/en/latest/features/c\", \"aching.html>\\n- **Logging at the API Gateway tier** <https://ocelot.readthedocs.io/en/latest/features\", \"/logging.html>\\n- **Quality of Service (Retries and Circuit breakers) at the API Gateway tier** <http\", \"s://ocelot.readthedocs.io/en/latest/features/qualityofservice.html>\\n- **Rate limiting** <https://oce\", \"lot.readthedocs.io/en/latest/features/ratelimiting.html>\\n- **Swagger for Ocelot** <https://github.co\", \"m/Burgyn/MMLib.SwaggerForOcelot>\\n\\n# <span id=\\\"page-196-0\\\"></span>Tackle Business Complexity in a Mic\", \"roservice with DDD and CQRS Patterns\\n\\n*Design a domain model for each microservice or Bounded Contex\", \"t that reflects understanding of the business domain.*\\n\\nThis section focuses on more advanced micros\", \"ervices that you implement when you need to tackle complex subsystems, or microservices derived from\", \" the knowledge of domain experts with everchanging business rules. The architecture patterns used in\", \" this section are based on domain-driven design (DDD) and Command and Query Responsibility Segregati\", \"on (CQRS) approaches, as illustrated in Figure 7-1.\\n\\n![](_page_197_Figure_2.jpeg)\\n\\n*Figure 7-1. Exte\", \"rnal microservice architecture versus internal architecture patterns for each microservice*\\n\\nHowever\", \", most of the techniques for data driven microservices, such as how to implement an ASP.NET Core Web\", \" API service or how to expose Swagger metadata with Swashbuckle or NSwag, are also applicable to the\", \" more advanced microservices implemented internally with DDD patterns. This section is an extension \", \"of the previous sections, because most of the practices explained earlier also apply here or for any\", \" kind of microservice.\\n\\nThis section first provides details on the simplified CQRS patterns used in \", \"the eShopOnContainers reference application. Later, you will get an overview of the DDD techniques t\", \"hat enable you to find common patterns that you can reuse in your applications.\\n\\nDDD is a large topi\", \"c with a rich set of resources for learning. You can start with books like [Domain-](https://domainl\", \"anguage.com/ddd/)[Driven Design](https://domainlanguage.com/ddd/) by Eric Evans and additional mater\", \"ials from Vaughn Vernon, Jimmy Nilsson, Greg Young, Udi Dahan, Jimmy Bogard, and many other DDD/CQRS\", \" experts. But most of all you need to try to learn how to apply DDD techniques from the conversation\", \"s, whiteboarding, and domain modeling sessions with the experts in your concrete business domain.\\n\\n#\", \"## **Additional resources**\\n\\n### **DDD (Domain-Driven Design)**\\n\\n- **Eric Evans. Domain Language** <\", \"https://domainlanguage.com/>\\n- **Martin Fowler. Domain-Driven Design** <https://martinfowler.com/tag\", \"s/domain%20driven%20design.html>\\n- **Jimmy Bogard. Strengthening your domain: a primer** <https://lo\", \"stechies.com/jimmybogard/2010/02/04/strengthening-your-domain-a-primer/>\\n\\n## **DDD books**\\n\\n- **Eric\", \" Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software** [https://www.amazon.com\", \"/Domain-Driven-Design-Tackling-Complexity-](https://www.amazon.com/Domain-Driven-Design-Tackling-Com\", \"plexity-Software/dp/0321125215/)[Software/dp/0321125215/](https://www.amazon.com/Domain-Driven-Desig\", \"n-Tackling-Complexity-Software/dp/0321125215/)\\n- **Eric Evans. Domain-Driven Design Reference: Defin\", \"itions and Pattern Summaries** [https://www.amazon.com/Domain-Driven-Design-Reference-Definitions-20\", \"14-09-](https://www.amazon.com/Domain-Driven-Design-Reference-Definitions-2014-09-22/dp/B01N8YB4ZO/)\", \" [22/dp/B01N8YB4ZO/](https://www.amazon.com/Domain-Driven-Design-Reference-Definitions-2014-09-22/dp\", \"/B01N8YB4ZO/)\\n- **Vaughn Vernon. Implementing Domain-Driven Design** [https://www.amazon.com/Impleme\", \"nting-Domain-Driven-Design-Vaughn-](https://www.amazon.com/Implementing-Domain-Driven-Design-Vaughn-\", \"Vernon/dp/0321834577/)[Vernon/dp/0321834577/](https://www.amazon.com/Implementing-Domain-Driven-Desi\", \"gn-Vaughn-Vernon/dp/0321834577/)\\n- **Vaughn Vernon. Domain-Driven Design Distilled** <https://www.am\", \"azon.com/Domain-Driven-Design-Distilled-Vaughn-Vernon/dp/0134434420/>\\n- **Jimmy Nilsson. Applying Do\", \"main-Driven Design and Patterns** [https://www.amazon.com/Applying-Domain-Driven-Design-Patterns-](h\", \"ttps://www.amazon.com/Applying-Domain-Driven-Design-Patterns-Examples/dp/0321268202/)[Examples/dp/03\", \"21268202/](https://www.amazon.com/Applying-Domain-Driven-Design-Patterns-Examples/dp/0321268202/)\\n- \", \"**Cesar de la Torre. N-Layered Domain-Oriented Architecture Guide with .NET** [https://www.amazon.co\", \"m/N-Layered-Domain-Oriented-Architecture-Guide-](https://www.amazon.com/N-Layered-Domain-Oriented-Ar\", \"chitecture-Guide-NET/dp/8493903612/)[NET/dp/8493903612/](https://www.amazon.com/N-Layered-Domain-Ori\", \"ented-Architecture-Guide-NET/dp/8493903612/)\\n- **Abel Avram and Floyd Marinescu. Domain-Driven Desig\", \"n Quickly** <https://www.amazon.com/Domain-Driven-Design-Quickly-Abel-Avram/dp/1411609255/>\\n- **Scot\", \"t Millett, Nick Tune - Patterns, Principles, and Practices of Domain-Driven Design** [https://www.wi\", \"ley.com/Patterns%2C+Principles%2C+and+Practices+of+Domain+Driven+Des](https://www.wiley.com/Patterns\", \"%2C+Principles%2C+and+Practices+of+Domain+Driven+Design-p-9781118714706) [ign-p-9781118714706](https\", \"://www.wiley.com/Patterns%2C+Principles%2C+and+Practices+of+Domain+Driven+Design-p-9781118714706)\\n\\n#\", \"## **DDD training**\\n\\n\\u2022 **Julie Lerman and Steve Smith. Domain-Driven Design Fundamentals** <https://\", \"www.pluralsight.com/courses/fundamentals-domain-driven-design>\\n\\n## <span id=\\\"page-198-0\\\"></span>Appl\", \"y simplified CQRS and DDD patterns in a microservice\\n\\nCQRS is an architectural pattern that separate\", \"s the models for reading and writing data. The related term [Command Query Separation \\\\(CQS\\\\)](https\", \"://martinfowler.com/bliki/CommandQuerySeparation.html) was originally defined by Bertrand Meyer in h\", \"is book *Object-Oriented Software Construction*. The basic idea is that you can divide a system's op\", \"erations into two sharply separated categories:\\n\\n- Queries. These queries return a result and don't \", \"change the state of the system, and they're free of side effects.\\n- Commands. These commands change \", \"the state of a system.\\n\\nCQS is a simple concept: it is about methods within the same object being ei\", \"ther queries or commands. Each method either returns state or mutates state, but not both. Even a si\", \"ngle repository pattern object can comply with CQS. CQS can be considered a foundational principle f\", \"or CQRS.\\n\\n[Command and Query Responsibility Segregation \\\\(CQRS\\\\)](https://martinfowler.com/bliki/CQR\", \"S.html) was introduced by Greg Young and strongly promoted by Udi Dahan and others. It's based on th\", \"e CQS principle, although it's more detailed. It can be considered a pattern based on commands and e\", \"vents plus optionally on asynchronous messages. In many cases, CQRS is related to more advanced scen\", \"arios, like having a different physical database for reads (queries) than for writes (updates). More\", \"over, a more evolved CQRS system might implement [Event-Sourcing \\\\(ES\\\\)](https://martinfowler.com/ea\", \"aDev/EventSourcing.html) for your updates database, so you would only store events in the domain mod\", \"el instead of storing the current-state data. However, this approach is not used in this guide. This\", \" guide uses the simplest CQRS approach, which consists of just separating the queries from the comma\", \"nds.\\n\\nThe separation aspect of CQRS is achieved by grouping query operations in one layer and comman\", \"ds in another layer. Each layer has its own data model (note that we say model, not necessarily a di\", \"fferent database) and is built using its own combination of patterns and technologies. More importan\", \"tly, the two layers can be within the same tier or microservice, as in the example (ordering microse\", \"rvice) used for this guide. Or they could be implemented on different microservices or processes so \", \"they can be optimized and scaled out separately without affecting one another.\\n\\nCQRS means having tw\", \"o objects for a read/write operation where in other contexts there's one. There are reasons to have \", \"a denormalized reads database, which you can learn about in more advanced CQRS literature. But we ar\", \"en't using that approach here, where the goal is to have more flexibility in the queries instead of \", \"limiting the queries with constraints from DDD patterns like aggregates.\\n\\nAn example of this kind of\", \" service is the ordering microservice from the eShopOnContainers reference application. This service\", \" implements a microservice based on a simplified CQRS approach. It uses a single data source or data\", \"base, but two logical models plus DDD patterns for the transactional domain, as shown in Figure 7-2.\", \"\\n\\n![](_page_200_Picture_2.jpeg)\\n\\n*Figure 7-2. Simplified CQRS- and DDD-based microservice*\\n\\nThe Logi\", \"cal \\\"Ordering\\\" Microservice includes its Ordering database, which can be, but doesn't have to be, th\", \"e same Docker host. Having the database in the same Docker host is good for development, but not for\", \" production.\\n\\nThe application layer can be the Web API itself. The important design aspect here is t\", \"hat the microservice has split the queries and ViewModels (data models especially created for the cl\", \"ient applications) from the commands, domain model, and transactions following the CQRS pattern. Thi\", \"s approach keeps the queries independent from restrictions and constraints coming from DDD patterns \", \"that only make sense for transactions and updates, as explained in later sections.\\n\\n### <span id=\\\"pa\", \"ge-200-0\\\"></span>**Additional resources**\\n\\n\\u2022 **Greg Young. Versioning in an Event Sourced System** (\", \"Free to read online e-book) <https://leanpub.com/esversioning/read>\\n\\n## <span id=\\\"page-200-1\\\"></span\", \">Apply CQRS and CQS approaches in a DDD microservice in eShopOnContainers\\n\\nThe design of the orderin\", \"g microservice at the eShopOnContainers reference application is based on CQRS principles. However, \", \"it uses the simplest approach, which is just separating the queries from the commands and using the \", \"same database for both actions.\\n\\nThe essence of those patterns, and the important point here, is tha\", \"t queries are idempotent: no matter how many times you query a system, the state of that system won'\", \"t change. In other words, queries are side-effect free.\\n\\nTherefore, you could use a different \\\"reads\", \"\\\" data model than the transactional logic \\\"writes\\\" domain model, even though the ordering microservi\", \"ces are using the same database. Hence, this is a simplified CQRS approach.\\n\\nOn the other hand, comm\", \"ands, which trigger transactions and data updates, change state in the system. With commands, you ne\", \"ed to be careful when dealing with complexity and ever-changing business rules. This is where you wa\", \"nt to apply DDD techniques to have a better modeled system.\\n\\nThe DDD patterns presented in this guid\", \"e should not be applied universally. They introduce constraints on your design. Those constraints pr\", \"ovide benefits such as higher quality over time, especially in commands and other code that modifies\", \" system state. However, those constraints add complexity with fewer benefits for reading and queryin\", \"g data.\\n\\nOne such pattern is the Aggregate pattern, which we examine more in later sections. Briefly\", \", in the Aggregate pattern, you treat many domain objects as a single unit as a result of their rela\", \"tionship in the domain. You might not always gain advantages from this pattern in queries; it can in\", \"crease the complexity of query logic. For read-only queries, you do not get the advantages of treati\", \"ng multiple objects as a single Aggregate. You only get the complexity.\\n\\nAs shown in Figure 7-2 in t\", \"he previous section, this guide suggests using DDD patterns only in the transactional/updates area o\", \"f your microservice (that is, as triggered by commands). Queries can follow a simpler approach and s\", \"hould be separated from commands, following a CQRS approach.\\n\\nFor implementing the \\\"queries side\\\", y\", \"ou can choose between many approaches, from your full-blown ORM like EF Core, AutoMapper projections\", \", stored procedures, views, materialized views or a micro ORM.\\n\\nIn this guide and in eShopOnContaine\", \"rs (specifically the ordering microservice) we chose to implement straight queries using a micro ORM\", \" like [Dapper.](https://github.com/StackExchange/dapper-dot-net) This guide lets you implement any q\", \"uery based on SQL statements to get the best performance, thanks to a light framework with little ov\", \"erhead.\\n\\nWhen you use this approach, any updates to your model that impact how entities are persiste\", \"d to a SQL database also need separate updates to SQL queries used by Dapper or any other separate (\", \"non-EF) approaches to querying.\\n\\n### <span id=\\\"page-201-0\\\"></span>**CQRS and DDD patterns are not to\", \"p-level architectures**\\n\\nIt's important to understand that CQRS and most DDD patterns (like DDD laye\", \"rs or a domain model with aggregates) are not architectural styles, but only architecture patterns. \", \"Microservices, SOA, and event-driven architecture (EDA) are examples of architectural styles. They d\", \"escribe a system of many components, such as many microservices. CQRS and DDD patterns describe some\", \"thing inside a single system or component; in this case, something inside a microservice.\\n\\nDifferent\", \" Bounded Contexts (BCs) will employ different patterns. They have different responsibilities, and th\", \"at leads to different solutions. It is worth emphasizing that forcing the same pattern everywhere le\", \"ads to failure. Do not use CQRS and DDD patterns everywhere. Many subsystems, BCs, or microservices \", \"are simpler and can be implemented more easily using simple CRUD services or using another approach.\", \"\\n\\nThere is only one application architecture: the architecture of the system or end-to-end applicati\", \"on you are designing (for example, the microservices architecture). However, the design of each Boun\", \"ded Context or microservice within that application reflects its own tradeoffs and internal design d\", \"ecisions at an architecture patterns level. Do not try to apply the same architectural patterns as C\", \"QRS or DDD everywhere.\\n\\n## **Additional resources**\\n\\n- **Martin Fowler. CQRS** <https://martinfowler\", \".com/bliki/CQRS.html>\\n- **Greg Young. CQRS Documents** [https://cqrs.files.wordpress.com/2010/11/cqr\", \"s\\\\\\\\_documents.pdf](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf)\\n- **Udi Dahan. Clari\", \"fied CQRS** <https://udidahan.com/2009/12/09/clarified-cqrs/>\\n\\n## <span id=\\\"page-202-0\\\"></span>Imple\", \"ment reads/queries in a CQRS microservice\\n\\nFor reads/queries, the ordering microservice from the eSh\", \"opOnContainers reference application implements the queries independently from the DDD model and tra\", \"nsactional area. This implementation was done primarily because the demands for queries and for tran\", \"sactions are drastically different. Writes execute transactions that must be compliant with the doma\", \"in logic. Queries, on the other hand, are idempotent and can be segregated from the domain rules.\\n\\nT\", \"he approach is simple, as shown in Figure 7-3. The API interface is implemented by the Web API contr\", \"ollers using any infrastructure, such as a micro Object Relational Mapper (ORM) like Dapper, and ret\", \"urning dynamic ViewModels depending on the needs of the UI applications.\\n\\n![](_page_202_Figure_9.jpe\", \"g)\\n\\n*Figure 7-3. The simplest approach for queries in a CQRS microservice*\\n\\nThe simplest approach fo\", \"r the queries-side in a simplified CQRS approach can be implemented by querying the database with a \", \"Micro-ORM like Dapper, returning dynamic ViewModels. The query\\n\\ndefinitions query the database and r\", \"eturn a dynamic ViewModel built on the fly for each query. Since the queries are idempotent, they wo\", \"n't change the data no matter how many times you run a query. Therefore, you don't need to be restri\", \"cted by any DDD pattern used in the transactional side, like aggregates and other patterns, and that\", \" is why queries are separated from the transactional area. You query the database for the data that \", \"the UI needs and return a dynamic ViewModel that does not need to be statically defined anywhere (no\", \" classes for the ViewModels) except in the SQL statements themselves.\\n\\nSince this approach is simple\", \", the code required for the queries side (such as code using a micro ORM like [Dapper\\\\)](https://git\", \"hub.com/StackExchange/Dapper) can be implemented [within the same Web API project.](https://github.c\", \"om/dotnet-architecture/eShopOnContainers/blob/main/src/Services/Ordering/Ordering.API/Application/Qu\", \"eries/OrderQueries.cs) Figure 7-4 shows this approach. The queries are defined in the **Ordering.API\", \"** microservice project within the eShopOnContainers solution.\\n\\n![](_page_203_Picture_2.jpeg)\\n\\n*Figu\", \"re 7-4. Queries in the Ordering microservice in eShopOnContainers*\\n\\n## <span id=\\\"page-203-0\\\"></span>\", \"**Use ViewModels specifically made for client apps, independent from domain model constraints**\\n\\nSin\", \"ce the queries are performed to obtain the data needed by the client applications, the returned type\", \" can be specifically made for the clients, based on the data returned by the queries. These models, \", \"or Data Transfer Objects (DTOs), are called ViewModels.\\n\\nThe returned data (ViewModel) can be the re\", \"sult of joining data from multiple entities or tables in the database, or even across multiple aggre\", \"gates defined in the domain model for the transactional area. In this case, because you are creating\", \" queries independent of the domain model, the aggregates boundaries and constraints are ignored and \", \"you're free to query any table and column you might need. This approach provides great flexibility a\", \"nd productivity for the developers creating or updating the queries.\\n\\nThe ViewModels can be static t\", \"ypes defined in classes (as is implemented in the ordering microservice). Or they can be created dyn\", \"amically based on the queries performed, which is agile for developers.\\n\\n### <span id=\\\"page-203-1\\\"><\", \"/span>**Use Dapper as a micro ORM to perform queries**\\n\\nYou can use any micro ORM, Entity Framework \", \"Core, or even plain ADO.NET for querying. In the sample application, Dapper was selected for the ord\", \"ering microservice in eShopOnContainers as a good example of a popular micro ORM. It can run plain S\", \"QL queries with great performance, because it's a light framework. Using Dapper, you can write a SQL\", \" query that can access and join multiple tables.\\n\\nDapper is an open-source project (original created\", \" by Sam Saffron), and is part of the building blocks used in [Stack Overflow.](https://stackoverflow\", \".com/) To use Dapper, you just need to install it through the [Dapper NuGet package,](https://www.nu\", \"get.org/packages/Dapper) as shown in the following figure:\\n\\nYou also need to add a using directive s\", \"o your code has access to the Dapper extension methods.\\n\\nWhen you use Dapper in your code, you direc\", \"tly use the [SqlConnection](https://learn.microsoft.com/dotnet/api/microsoft.data.sqlclient.sqlconne\", \"ction) class available in the [Microsoft.Data.SqlClient](https://learn.microsoft.com/dotnet/api/micr\", \"osoft.data.sqlclient) namespace. Through the QueryAsync method and other extension methods that exte\", \"nd the [SqlConnection](https://learn.microsoft.com/dotnet/api/microsoft.data.sqlclient.sqlconnection\", \") class, you can run queries in a straightforward and performant way.\\n\\n## <span id=\\\"page-204-0\\\"></sp\", \"an>**Dynamic versus static ViewModels**\\n\\nWhen returning ViewModels from the server-side to client ap\", \"ps, you can think about those ViewModels as DTOs (Data Transfer Objects) that can be different to th\", \"e internal domain entities of your entity model because the ViewModels hold the data the way the cli\", \"ent app needs. Therefore, in many cases, you can aggregate data coming from multiple domain entities\", \" and compose the ViewModels precisely according to how the client app needs that data.\\n\\nThose ViewMo\", \"dels or DTOs can be defined explicitly (as data holder classes), like the OrderSummary class shown i\", \"n a later code snippet. Or, you could just return dynamic ViewModels or dynamic DTOs based on the at\", \"tributes returned by your queries as a dynamic type.\\n\\n### **ViewModel as dynamic type**\\n\\nAs shown in\", \" the following code, a ViewModel can be directly returned by the queries by just returning a *dynami\", \"c* type that internally is based on the attributes returned by a query. That means that the subset o\", \"f attributes to be returned is based on the query itself. Therefore, if you add a new column to the \", \"query or join, that data is dynamically added to the returned ViewModel.\\n\\n```\\nusing Dapper;\\nusing Mi\", \"crosoft.Extensions.Configuration;\\nusing System.Data.SqlClient;\\nusing System.Threading.Tasks;\\nusing S\", \"ystem.Dynamic;\\nusing System.Collections.Generic;\\npublic class OrderQueries : IOrderQueries\\n{\\n public\", \" async Task<IEnumerable<dynamic>> GetOrdersAsync()\\n {\\n using (var connection = new SqlConnection(_co\", \"nnectionString))\\n {\\n connection.Open();\\n return await connection.QueryAsync<dynamic>(\\n @\\\"SELECT o.[I\", \"d] as ordernumber,\\n o.[OrderDate] as [date],os.[Name] as [status],\\n SUM(oi.units*oi.unitprice) as to\", \"tal\\n FROM [ordering].[Orders] o\\n LEFT JOIN[ordering].[orderitems] oi ON o.Id = oi.orderid\\n LEFT JOIN\", \"[ordering].[orderstatus] os on o.OrderStatusId = os.Id\\n```\\n\\n```\\n GROUP BY o.[Id], o.[OrderDate], os.\", \"[Name]\\\");\\n }\\n }\\n}\\n```\\n\\nThe important point is that by using a dynamic type, the returned collection \", \"of data is dynamically assembled as the ViewModel.\\n\\n**Pros:** This approach reduces the need to modi\", \"fy static ViewModel classes whenever you update the SQL sentence of a query, making this design appr\", \"oach agile when coding, straightforward, and quick to evolve in regard to future changes.\\n\\n**Cons:**\", \" In the long term, dynamic types can negatively impact the clarity and the compatibility of a servic\", \"e with client apps. In addition, middleware software like Swashbuckle cannot provide the same level \", \"of documentation on returned types if using dynamic types.\\n\\n## **ViewModel as predefined DTO classes\", \"**\\n\\n**Pros**: Having static, predefined ViewModel classes, like \\\"contracts\\\" based on explicit DTO cl\", \"asses, is definitely better for public APIs but also for long-term microservices, even if they are o\", \"nly used by the same application.\\n\\nIf you want to specify response types for Swagger, you need to us\", \"e explicit DTO classes as the return type. Therefore, predefined DTO classes allow you to offer rich\", \"er information from Swagger. That improves the API documentation and compatibility when consuming an\", \" API.\\n\\n**Cons**: As mentioned earlier, when updating the code, it takes some more steps to update th\", \"e DTO classes.\\n\\n*Tip based on our experience*: In the queries implemented at the Ordering microservi\", \"ce in eShopOnContainers, we started developing by using dynamic ViewModels as it was straightforward\", \" and agile on the early development stages. But, once the development was stabilized, we chose to re\", \"factor the APIs and use static or pre-defined DTOs for the ViewModels, because it is clearer for the\", \" microservice's consumers to know explicit DTO types, used as \\\"contracts\\\".\\n\\nIn the following example\", \", you can see how the query is returning data by using an explicit ViewModel DTO class: the OrderSum\", \"mary class.\\n\\n```\\nusing Dapper;\\nusing Microsoft.Extensions.Configuration;\\nusing System.Data.SqlClient\", \";\\nusing System.Threading.Tasks;\\nusing System.Dynamic;\\nusing System.Collections.Generic;\\npublic class\", \" OrderQueries : IOrderQueries\\n{\\n public async Task<IEnumerable<OrderSummary>> GetOrdersAsync()\\n {\\n u\", \"sing (var connection = new SqlConnection(_connectionString))\\n {\\n connection.Open();\\n return await co\", \"nnection.QueryAsync<OrderSummary>(\\n @\\\"SELECT o.[Id] as ordernumber,\\n o.[OrderDate] as [date],os.[Nam\", \"e] as [status],\\n```\\n\\n```\\n SUM(oi.units*oi.unitprice) as total\\n FROM [ordering].[Orders] o\\n LEFT JOIN\", \"[ordering].[orderitems] oi ON o.Id = oi.orderid\\n LEFT JOIN[ordering].[orderstatus] os on o.OrderStat\", \"usId = os.Id\\n GROUP BY o.[Id], o.[OrderDate], os.[Name]\\n ORDER BY o.[Id]\\\");\\n }\\n }\\n}\\n```\\n\\n## **Descri\", \"be response types of Web APIs**\\n\\nDevelopers consuming web APIs and microservices are most concerned \", \"with what is returned specifically response types and error codes (if not standard). The response ty\", \"pes are handled in the XML comments and data annotations.\\n\\nWithout proper documentation in the Swagg\", \"er UI, the consumer lacks knowledge of what types are being returned or what HTTP codes can be retur\", \"ned. That problem is fixed by adding the [Microsoft.AspNetCore.Mvc.ProducesResponseTypeAttribute,](h\", \"ttps://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.mvc.producesresponsetypeattribute) so Swas\", \"hbuckle can generate richer information about the API return model and values, as shown in the follo\", \"wing code:\\n\\n```\\nnamespace Microsoft.eShopOnContainers.Services.Ordering.API.Controllers\\n{\\n [Route(\\\"a\", \"pi/v1/[controller]\\\")]\\n [Authorize]\\n public class OrdersController : Controller\\n {\\n //Additional code\", \"...\\n [Route(\\\"\\\")]\\n [HttpGet]\\n [ProducesResponseType(typeof(IEnumerable<OrderSummary>),\\n (int)HttpStat\", \"usCode.OK)]\\n public async Task<IActionResult> GetOrders()\\n {\\n var userid = _identityService.GetUserI\", \"dentity();\\n var orders = await _orderQueries\\n .GetOrdersFromUserAsync(Guid.Parse(userid));\\n return O\", \"k(orders);\\n }\\n }\\n}\\n```\\n\\nHowever, the ProducesResponseType attribute cannot use dynamic as a type but\", \" requires to use explicit types, like the OrderSummary ViewModel DTO, shown in the following example\", \":\\n\\n```\\npublic class OrderSummary\\n{\\n public int ordernumber { get; set; }\\n public DateTime date { get\", \"; set; }\\n public string status { get; set; }\\n public double total { get; set; }\\n}\\n// or using C# 8 r\", \"ecord types:\\npublic record OrderSummary(int ordernumber, DateTime date, string status, double total)\", \";\\n```\\n\\nThis is another reason why explicit returned types are better than dynamic types, in the long\", \" term. When using the ProducesResponseType attribute, you can also specify what is the expected outc\", \"ome regarding possible HTTP errors/codes, like 200, 400, etc.\\n\\nIn the following image, you can see h\", \"ow Swagger UI shows the ResponseType information.\\n\\n![](_page_207_Figure_2.jpeg)\\n\\n*Figure 7-5. Swagge\", \"r UI showing response types and possible HTTP status codes from a Web API*\\n\\nThe image shows some exa\", \"mple values based on the ViewModel types and the possible HTTP status codes that can be returned.\\n\\n#\", \"## <span id=\\\"page-207-0\\\"></span>**Additional resources**\\n\\n- **Dapper** <https://github.com/StackExch\", \"ange/dapper-dot-net>\\n- **Julie Lerman. Data Points - Dapper, Entity Framework and Hybrid Apps (MSDN \", \"magazine article)**\\n\\n[https://learn.microsoft.com/archive/msdn-magazine/2016/may/data-points-dapper-\", \"entity](https://docs.microsoft.com/archive/msdn-magazine/2016/may/data-points-dapper-entity-framewor\", \"k-and-hybrid-apps)[framework-and-hybrid-apps](https://docs.microsoft.com/archive/msdn-magazine/2016/\", \"may/data-points-dapper-entity-framework-and-hybrid-apps)\\n\\n- **ASP.NET Core Web API Help Pages using \", \"Swagger** [https://learn.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using](https://docs.\", \"microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger?tabs=visual-studio)[swagger?tab\", \"s=visual-studio](https://docs.microsoft.com/aspnet/core/tutorials/web-api-help-pages-using-swagger?t\", \"abs=visual-studio)\\n- **Create record types** [https://learn.microsoft.com/dotnet/csharp/whats-new/tu\", \"torials/records](https://docs.microsoft.com/dotnet/csharp/tutorials/records)\\n\\n## <span id=\\\"page-208-\", \"0\\\"></span>Design a DDD-oriented microservice\\n\\nDomain-driven design (DDD) advocates modeling based on\", \" the reality of business as relevant to your use cases. In the context of building applications, DDD\", \" talks about problems as domains. It describes independent problem areas as Bounded Contexts (each B\", \"ounded Context correlates to a microservice), and emphasizes a common language to talk about these p\", \"roblems. It also suggests many technical concepts and patterns, like domain entities with rich model\", \"s (no [anemic-domain](https://martinfowler.com/bliki/AnemicDomainModel.html)  [model\\\\)](https://mart\", \"infowler.com/bliki/AnemicDomainModel.html), value objects, aggregates, and aggregate root (or root e\", \"ntity) rules to support the internal implementation. This section introduces the design and implemen\", \"tation of those internal patterns.\\n\\nSometimes these DDD technical rules and patterns are perceived a\", \"s obstacles that have a steep learning curve for implementing DDD approaches. But the important part\", \" is not the patterns themselves, but organizing the code so it is aligned to the business problems, \", \"and using the same business terms (ubiquitous language). In addition, DDD approaches should be appli\", \"ed only if you are implementing complex microservices with significant business rules. Simpler respo\", \"nsibilities, like a CRUD service, can be managed with simpler approaches.\\n\\nWhere to draw the boundar\", \"ies is the key task when designing and defining a microservice. DDD patterns help you understand the\", \" complexity in the domain. For the domain model for each Bounded Context, you identify and define th\", \"e entities, value objects, and aggregates that model your domain. You build and refine a domain mode\", \"l that is contained within a boundary that defines your context. And that is explicit in the form of\", \" a microservice. The components within those boundaries end up being your microservices, although in\", \" some cases a BC or business microservices can be composed of several physical services. DDD is abou\", \"t boundaries and so are microservices.\\n\\n## <span id=\\\"page-208-1\\\"></span>**Keep the microservice cont\", \"ext boundaries relatively small**\\n\\nDetermining where to place boundaries between Bounded Contexts ba\", \"lances two competing goals. First, you want to initially create the smallest possible microservices,\", \" although that should not be the main driver; you should create a boundary around things that need c\", \"ohesion. Second, you want to avoid chatty communications between microservices. These goals can cont\", \"radict one another. You should balance them by decomposing the system into as many small microservic\", \"es as you can until you see communication boundaries growing quickly with each additional attempt to\", \" separate a new Bounded Context. Cohesion is key within a single bounded context.\\n\\nIt is similar to \", \"the [Inappropriate Intimacy code smell](https://sourcemaking.com/refactoring/smells/inappropriate-in\", \"timacy) when implementing classes. If two microservices need to collaborate a lot with each other, t\", \"hey should probably be the same microservice.\\n\\nAnother way to look at this aspect is autonomy. If a \", \"microservice must rely on another service to directly service a request, it is not truly autonomous.\", \"\\n\\n## <span id=\\\"page-209-0\\\"></span>**Layers in DDD microservices** Most enterprise applications with \", \"significant business and technical complexity are defined by\\n\\nmultiple layers. The layers are a logi\", \"cal artifact, and are not related to the deployment of the service. They exist to help developers ma\", \"nage the complexity in the code. Different layers (like the domain model layer versus the presentati\", \"on layer, etc.) might have different types, which mandate translations between those types.\\n\\nFor exa\", \"mple, an entity could be loaded from the database. Then part of that information, or an aggregation \", \"of information including additional data from other entities, can be sent to the client UI through a\", \" REST Web API. The point here is that the domain entity is contained within the domain model layer a\", \"nd should not be propagated to other areas that it does not belong to, like to the presentation laye\", \"r.\\n\\nAdditionally, you need to have always-valid entities (see the Designing validations in the domai\", \"n model layer section) controlled by aggregate roots (root entities). Therefore, entities should not\", \" be bound to client views, because at the UI level some data might still not be validated. This reas\", \"on is what the ViewModel is for. The ViewModel is a data model exclusively for presentation layer ne\", \"eds. The domain entities do not belong directly to the ViewModel. Instead, you need to translate bet\", \"ween ViewModels and domain entities and vice versa.\\n\\nWhen tackling complexity, it is important to ha\", \"ve a domain model controlled by aggregate roots that make sure that all the invariants and rules rel\", \"ated to that group of entities (aggregate) are performed through a single entry-point or gate, the a\", \"ggregate root.\\n\\nFigure 7-5 shows how a layered design is implemented in the eShopOnContainers applic\", \"ation.\\n\\n![](_page_210_Figure_1.jpeg)\\n\\n*Figure 7-5. DDD layers in the ordering microservice in eShopO\", \"nContainers*\\n\\nThe three layers in a DDD microservice like Ordering. Each layer is a VS project: Appl\", \"ication layer is Ordering.API, Domain layer is Ordering.Domain and the Infrastructure layer is Order\", \"ing.Infrastructure. You want to design the system so that each layer communicates only with certain \", \"other layers. That approach may be easier to enforce if layers are implemented as different class li\", \"braries, because you can clearly identify what dependencies are set between libraries. For instance,\", \" the domain model layer should not take a dependency on any other layer (the domain model classes sh\", \"ould be Plain Old Class Objects, or [POCO,](https://docs.microsoft.com/dotnet/standard/glossary#poco\", \") classes). As shown in Figure 7-6, the **Ordering.Domain** layer library has dependencies only on t\", \"he .NET libraries or NuGet packages, but not on any other custom library, such as data library or pe\", \"rsistence library.\\n\\n![](_page_210_Figure_4.jpeg)\\n\\n*Figure 7-6. Layers implemented as libraries allow\", \" better control of dependencies between layers*\\n\\n### **The domain model layer**\\n\\nEric Evans's excell\", \"ent book [Domain Driven Design](https://domainlanguage.com/ddd/) says the following about the domain\", \" model layer and the application layer.\\n\\n**Domain Model Layer**: Responsible for representing concep\", \"ts of the business, information about the business situation, and business rules. State that reflect\", \"s the business situation is controlled and used here, even though the technical details of storing i\", \"t are delegated to the infrastructure. This layer is the heart of business software.\\n\\nThe domain mod\", \"el layer is where the business is expressed. When you implement a microservice domain model layer in\", \" .NET, that layer is coded as a class library with the domain entities that capture data plus behavi\", \"or (methods with logic).\\n\\ncompletely ignore data persistence details. These persistence tasks should\", \" be performed by the infrastructure layer. Therefore, this layer should not take direct dependencies\", \" on the infrastructure, which means that an important rule is that your domain model entity classes \", \"should be POCOs.\\n\\nFollowing the [Persistence Ignorance](https://deviq.com/persistence-ignorance/) an\", \"d the [Infrastructure Ignorance](https://ayende.com/blog/3137/infrastructure-ignorance) principles, \", \"this layer must\\n\\nDomain entities should not have any direct dependency (like deriving from a base cl\", \"ass) on any data access infrastructure framework like Entity Framework or NHibernate. Ideally, your \", \"domain entities should not derive from or implement any type defined in any infrastructure framework\", \".\\n\\nMost modern ORM frameworks like Entity Framework Core allow this approach, so that your domain mo\", \"del classes are not coupled to the infrastructure. However, having POCO entities is not always possi\", \"ble when using certain NoSQL databases and frameworks, like Actors and Reliable Collections in Azure\", \" Service Fabric.\\n\\nEven when it is important to follow the Persistence Ignorance principle for your D\", \"omain model, you should not ignore persistence concerns. It is still important to understand the phy\", \"sical data model and how it maps to your entity object model. Otherwise you can create impossible de\", \"signs.\\n\\nAlso, this aspect does not mean you can take a model designed for a relational database and \", \"directly move it to a NoSQL or document-oriented database. In some entity models, the model might fi\", \"t, but usually it does not. There are still constraints that your entity model must adhere to, based\", \" both on the storage technology and ORM technology.\\n\\n### **The application layer**\\n\\nMoving on to the\", \" application layer, we can again cite Eric Evans's book [Domain Driven Design:](https://domainlangua\", \"ge.com/ddd/)\\n\\n**Application Layer:** Defines the jobs the software is supposed to do and directs the\", \" expressive domain objects to work out problems. The tasks this layer is responsible for are meaning\", \"ful to the business or necessary for interaction with the application layers of other systems. This \", \"layer is kept thin. It does not contain business rules or knowledge, but only coordinates tasks and \", \"delegates work to collaborations of domain objects in the next layer down. It does not have state re\", \"flecting the business situation, but it can have state that reflects the progress of a task for the \", \"user or the program.\\n\\nA microservice's application layer in .NET is commonly coded as an ASP.NET Cor\", \"e Web API project. The project implements the microservice's interaction, remote network access, and\", \" the external Web APIs used from the UI or client apps. It includes queries if using a CQRS approach\", \", commands accepted by the microservice, and even the event-driven communication between microservic\", \"es (integration events). The ASP.NET Core Web API that represents the application layer must not con\", \"tain business rules or domain knowledge (especially domain rules for transactions or updates); these\", \" should be owned by the domain model class library. The application layer must only coordinate tasks\", \" and must not hold or define any domain state (domain model). It delegates the execution of business\", \" rules to the domain model classes themselves (aggregate roots and domain entities), which will ulti\", \"mately update the data within those domain entities.\\n\\nBasically, the application logic is where you \", \"implement all use cases that depend on a given front end. For example, the implementation related to\", \" a Web API service.\\n\\nThe goal is that the domain logic in the domain model layer, its invariants, th\", \"e data model, and related business rules must be completely independent from the presentation and ap\", \"plication layers. Most of all, the domain model layer must not directly depend on any infrastructure\", \" framework.\\n\\n## <span id=\\\"page-212-0\\\"></span>**The infrastructure layer**\\n\\nThe infrastructure layer \", \"is how the data that is initially held in domain entities (in memory) is persisted in databases or a\", \"nother persistent store. An example is using Entity Framework Core code to implement the Repository \", \"pattern classes that use a DBContext to persist data in a relational database.\\n\\nIn accordance with t\", \"he previously mentioned [Persistence Ignorance](https://deviq.com/persistence-ignorance/) and [Infra\", \"structure Ignorance](https://ayende.com/blog/3137/infrastructure-ignorance) principles, the infrastr\", \"ucture layer must not \\\"contaminate\\\" the domain model layer. You must keep the domain model entity cl\", \"asses agnostic from the infrastructure that you use to persist data (EF or any other framework) by n\", \"ot taking hard dependencies on frameworks. Your domain model layer class library should have only yo\", \"ur domain code, just POCO entity classes implementing the heart of your software and completely deco\", \"upled from infrastructure technologies.\\n\\nThus, your layers or class libraries and projects should ul\", \"timately depend on your domain model layer (library), not vice versa, as shown in Figure 7-7.\\n\\n![](_\", \"page_212_Figure_7.jpeg)\\n\\n*Figure 7-7. Dependencies between layers in DDD*\\n\\nDependencies in a DDD Ser\", \"vice, the Application layer depends on Domain and Infrastructure, and Infrastructure depends on Doma\", \"in, but Domain doesn't depend on any layer. This layer design should be independent for each microse\", \"rvice. As noted earlier, you can implement the most complex microservices following DDD patterns, wh\", \"ile implementing simpler data-driven microservices (simple CRUD in a single layer) in a simpler way.\", \"\\n\\n## **Additional resources**\\n\\n- **DevIQ. Persistence Ignorance principle** <https://deviq.com/persi\", \"stence-ignorance/>\\n- **Oren Eini. Infrastructure Ignorance** <https://ayende.com/blog/3137/infrastru\", \"cture-ignorance>\\n- **Angel Lopez. Layered Architecture In Domain-Driven Design** <https://ajlopez.wo\", \"rdpress.com/2008/09/12/layered-architecture-in-domain-driven-design/>\\n\\n## <span id=\\\"page-213-0\\\"></sp\", \"an>Design a microservice domain model\\n\\n*Define one rich domain model for each business microservice \", \"or Bounded Context.*\\n\\nYour goal is to create a single cohesive domain model for each business micros\", \"ervice or Bounded Context (BC). Keep in mind, however, that a BC or business microservice could some\", \"times be composed of several physical services that share a single domain model. The domain model mu\", \"st capture the rules, behavior, business language, and constraints of the single Bounded Context or \", \"business microservice that it represents.\\n\\n### <span id=\\\"page-213-1\\\"></span>**The Domain Entity patt\", \"ern**\\n\\nEntities represent domain objects and are primarily defined by their identity, continuity, an\", \"d persistence over time, and not only by the attributes that comprise them. As Eric Evans says, \\\"an \", \"object primarily defined by its identity is called an Entity.\\\" Entities are very important in the do\", \"main model, since they are the base for a model. Therefore, you should identify and design them care\", \"fully.\\n\\n*An entity's identity can cross multiple microservices or Bounded Contexts.*\\n\\nThe same ident\", \"ity (that is, the same Id value, although perhaps not the same domain entity) can be modeled across \", \"multiple Bounded Contexts or microservices. However, that does not imply that the same entity, with \", \"the same attributes and logic would be implemented in multiple Bounded Contexts. Instead, entities i\", \"n each Bounded Context limit their attributes and behaviors to those required in that Bounded Contex\", \"t's domain.\\n\\nFor instance, the buyer entity might have most of a person's attributes that are define\", \"d in the user entity in the profile or identity microservice, including the identity. But the buyer \", \"entity in the ordering microservice might have fewer attributes, because only certain buyer data is \", \"related to the order process. The context of each microservice or Bounded Context impacts its domain\", \" model.\\n\\n*Domain entities must implement behavior in addition to implementing data attributes.*\\n\\nA d\", \"omain entity in DDD must implement the domain logic or behavior related to the entity data (the obje\", \"ct accessed in memory). For example, as part of an order entity class you must have business logic a\", \"nd operations implemented as methods for tasks such as adding an order item, data validation, and to\", \"tal calculation. The entity's methods take care of the invariants and rules of the entity instead of\", \" having those rules spread across the application layer.\\n\\nFigure 7-8 shows a domain entity that impl\", \"ements not only data attributes but operations or methods with related domain logic.\\n\\n![](_page_214_\", \"Figure_2.jpeg)\\n\\n*Figure 7-8. Example of a domain entity design implementing data plus behavior*\\n\\nA d\", \"omain model entity implements behaviors through methods, that is, it's not an \\\"anemic\\\" model. Of cou\", \"rse, sometimes you can have entities that do not implement any logic as part of the entity class. Th\", \"is can happen in child entities within an aggregate if the child entity does not have any special lo\", \"gic because most of the logic is defined in the aggregate root. If you have a complex microservice t\", \"hat has logic implemented in the service classes instead of in the domain entities, you could be fal\", \"ling into the anemic domain model, explained in the following section.\\n\\n### **Rich domain model vers\", \"us anemic domain model**\\n\\nIn his post [AnemicDomainModel,](https://martinfowler.com/bliki/AnemicDoma\", \"inModel.html) Martin Fowler describes an anemic domain model this way:\\n\\nThe basic symptom of an Anem\", \"ic Domain Model is that at first blush it looks like the real thing. There are objects, many named a\", \"fter the nouns in the domain space, and these objects are connected with the rich relationships and \", \"structure that true domain models have. The catch comes when you look at the behavior, and you reali\", \"ze that there is hardly any behavior on these objects, making them little more than bags of getters \", \"and setters.\\n\\nOf course, when you use an anemic domain model, those data models will be used from a \", \"set of service objects (traditionally named the *business layer*) which capture all the domain or bu\", \"siness logic. The business layer sits on top of the data model and uses the data model just as data.\", \"\\n\\nThe anemic domain model is just a procedural style design. Anemic entity objects are not real obje\", \"cts because they lack behavior (methods). They only hold data properties and thus it is not objector\", \"iented design. By putting all the behavior out into service objects (the business layer), you essent\", \"ially end up with [spaghetti code](https://en.wikipedia.org/wiki/Spaghetti_code) or [transaction scr\", \"ipts,](https://martinfowler.com/eaaCatalog/transactionScript.html) and therefore you lose the advant\", \"ages that a domain model provides.\\n\\nRegardless, if your microservice or Bounded Context is very simp\", \"le (a CRUD service), the anemic domain model in the form of entity objects with just data properties\", \" might be good enough, and it might not be worth implementing more complex DDD patterns. In that cas\", \"e, it will be simply a persistence model, because you have intentionally created an entity with only\", \" data for CRUD purposes.\\n\\nThat is why microservices architectures are perfect for a multi-architectu\", \"ral approach depending on each Bounded Context. For instance, in eShopOnContainers, the ordering mic\", \"roservice implements DDD patterns, but the catalog microservice, which is a simple CRUD service, doe\", \"s not.\\n\\nSome people say that the anemic domain model is an anti-pattern. It really depends on what y\", \"ou are implementing. If the microservice you are creating is simple enough (for example, a CRUD serv\", \"ice), following the anemic domain model it is not an anti-pattern. However, if you need to tackle th\", \"e complexity of a microservice's domain that has a lot of ever-changing business rules, the anemic d\", \"omain model might be an anti-pattern for that microservice or Bounded Context. In that case, designi\", \"ng it as a rich model with entities containing data plus behavior as well as implementing additional\", \" DDD patterns (aggregates, value objects, etc.) might have huge benefits for the long-term success o\", \"f such a microservice.\\n\\n## **Additional resources**\\n\\n- **DevIQ. Domain Entity** <https://deviq.com/e\", \"ntity/>\\n- **Martin Fowler. The Domain Model** <https://martinfowler.com/eaaCatalog/domainModel.html>\", \"\\n- **Martin Fowler. The Anemic Domain Model** <https://martinfowler.com/bliki/AnemicDomainModel.html\", \">\\n\\n### **The Value Object pattern**\\n\\nAs Eric Evans has noted, \\\"Many objects do not have conceptual i\", \"dentity. These objects describe certain characteristics of a thing.\\\"\\n\\nAn entity requires an identity\", \", but there are many objects in a system that do not, like the Value Object pattern. A value object \", \"is an object with no conceptual identity that describes a domain aspect. These are objects that you \", \"instantiate to represent design elements that only concern you temporarily. You care about *what* th\", \"ey are, not *who* they are. Examples include numbers and strings, but can also be higher-level conce\", \"pts like groups of attributes.\\n\\nSomething that is an entity in a microservice might not be an entity\", \" in another microservice, because in the second case, the Bounded Context might have a different mea\", \"ning. For example, an address in an e-commerce application might not have an identity at all, since \", \"it might only represent a group of attributes of the customer's profile for a person or company. In \", \"this case, the address should be classified as a value object. However, in an application for an ele\", \"ctric power utility company, the customer address could be important for the business domain. Theref\", \"ore, the address must have an identity so the billing system can be directly linked to the address. \", \"In that case, an address should be classified as a domain entity.\\n\\nA person with a name and surname \", \"is usually an entity because a person has identity, even if the name and surname coincide with anoth\", \"er set of values, such as if those names also refer to a different person.\\n\\nValue objects are hard t\", \"o manage in relational databases and ORMs like Entity Framework (EF), whereas in document-oriented d\", \"atabases they are easier to implement and use.\\n\\nEF Core 2.0 and later versions include the [Owned En\", \"tities](https://devblogs.microsoft.com/dotnet/announcing-entity-framework-core-2-0/#owned-entities-a\", \"nd-table-splitting) feature that makes it easier to handle value objects, as we'll see in detail lat\", \"er on.\\n\\n## **Additional resources**\\n\\n\\u2022 **Value Object**\\n\\n- **Martin Fowler. Value Object pattern** <\", \"https://martinfowler.com/bliki/ValueObject.html>\\n- <https://deviq.com/value-object/>\\n- **Value Objec\", \"ts in Test-Driven Development** <https://leanpub.com/tdd-ebook/read#leanpub-auto-value-objects>\\n- **\", \"Eric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software.** (Book; includes a \", \"discussion of value objects) [https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-](http\", \"s://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/)[Software/dp/032\", \"1125215/](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/)\\n\\n\", \"### **The Aggregate pattern**\\n\\nA domain model contains clusters of different data entities and proce\", \"sses that can control a significant area of functionality, such as order fulfillment or inventory. A\", \" more fine-grained DDD unit is the aggregate, which describes a cluster or group of entities and beh\", \"aviors that can be treated as a cohesive unit.\\n\\nYou usually define an aggregate based on the transac\", \"tions that you need. A classic example is an order that also contains a list of order items. An orde\", \"r item will usually be an entity. But it will be a child entity within the order aggregate, which wi\", \"ll also contain the order entity as its root entity, typically called an aggregate root.\\n\\nIdentifyin\", \"g aggregates can be hard. An aggregate is a group of objects that must be consistent together, but y\", \"ou cannot just pick a group of objects and label them an aggregate. You must start with a domain con\", \"cept and think about the entities that are used in the most common transactions related to that conc\", \"ept. Those entities that need to be transactionally consistent are what forms an aggregate. Thinking\", \" about transaction operations is probably the best way to identify aggregates.\\n\\n### **The Aggregate \", \"Root or Root Entity pattern**\\n\\nAn aggregate is composed of at least one entity: the aggregate root, \", \"also called root entity or primary entity. Additionally, it can have multiple child entities and val\", \"ue objects, with all entities and objects working together to implement required behavior and transa\", \"ctions.\\n\\nThe purpose of an aggregate root is to ensure the consistency of the aggregate; it should b\", \"e the only entry point for updates to the aggregate through methods or operations in the aggregate r\", \"oot class. You should make changes to entities within the aggregate only via the aggregate root. It \", \"is the aggregate's consistency guardian, considering all the invariants and consistency rules you mi\", \"ght need to comply with in your aggregate. If you change a child entity or value object independentl\", \"y, the\\n\\naggregate root cannot ensure that the aggregate is in a valid state. It would be like a tabl\", \"e with a loose leg. Maintaining consistency is the main purpose of the aggregate root.\\n\\nIn Figure 7-\", \"9, you can see sample aggregates like the buyer aggregate, which contains a single entity (the aggre\", \"gate root Buyer). The order aggregate contains multiple entities and a value object.\\n\\n![](_page_217_\", \"Figure_3.jpeg)\\n\\n*Figure 7-9. Example of aggregates with multiple or single entities*\\n\\nA DDD domain m\", \"odel is composed from aggregates, an aggregate can have just one entity or more, and can include val\", \"ue objects as well. Note that the Buyer aggregate could have additional child entities, depending on\", \" your domain, as it does in the ordering microservice in the eShopOnContainers reference application\", \". Figure 7-9 just illustrates a case in which the buyer has a single entity, as an example of an agg\", \"regate that contains only an aggregate root.\\n\\nIn order to maintain separation of aggregates and keep\", \" clear boundaries between them, it is a good practice in a DDD domain model to disallow direct navig\", \"ation between aggregates and only having the foreign key (FK) field, as implemented in the [Ordering\", \" microservice domain model](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/S\", \"ervices/Ordering/Ordering.Domain/AggregatesModel/OrderAggregate/Order.cs) in eShopOnContainers. The \", \"Order entity only has a foreign key field for the buyer, but not an EF Core navigation property, as \", \"shown in the following code:\\n\\n```\\npublic class Order : Entity, IAggregateRoot\\n{\\n private DateTime _o\", \"rderDate;\\n public Address Address { get; private set; }\\n private int? _buyerId; // FK pointing to a \", \"different aggregate root\\n public OrderStatus OrderStatus { get; private set; }\\n private readonly Lis\", \"t<OrderItem> _orderItems;\\n public IReadOnlyCollection<OrderItem> OrderItems => _orderItems;\\n // ... \", \"Additional code\\n}\\n```\\n\\nIdentifying and working with aggregates requires research and experience. For\", \" more information, see the following Additional resources list.\\n\\n## **Additional resources**\\n\\n- **Va\", \"ughn Vernon. Effective Aggregate Design - Part I: Modeling a Single Aggregate** (from [https://dddco\", \"mmunity.org/\\\\)](https://dddcommunity.org/) [https://dddcommunity.org/wp-content/uploads/files/pdf\\\\\\\\_\", \"articles/Vernon\\\\\\\\_2011\\\\\\\\_1.pdf](https://dddcommunity.org/wp-content/uploads/files/pdf_articles/Verno\", \"n_2011_1.pdf)\\n- **Together** (from [https://dddcommunity.org/\\\\)](https://dddcommunity.org/) [https:/\", \"/dddcommunity.org/wp-content/uploads/files/pdf\\\\\\\\_articles/Vernon\\\\\\\\_2011\\\\\\\\_2.pdf](https://dddcommunit\", \"y.org/wp-content/uploads/files/pdf_articles/Vernon_2011_2.pdf)\\n\\n\\u2022 **Vaughn Vernon. Effective Aggrega\", \"te Design - Part II: Making Aggregates Work** \\n\\n- **Vaughn Vernon. Effective Aggregate Design - Part\", \" III: Gaining Insight Through Discovery** (from [https://dddcommunity.org/\\\\)](https://dddcommunity.o\", \"rg/) [https://dddcommunity.org/wp-content/uploads/files/pdf\\\\\\\\_articles/Vernon\\\\\\\\_2011\\\\\\\\_3.pdf](https:\", \"//dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011_3.pdf)\\n- **Sergey Grybniak. DDD\", \" Tactical Design Patterns** [https://www.codeproject.com/Articles/1164363/Domain-Driven-Design-Tacti\", \"cal-Design-](https://www.codeproject.com/Articles/1164363/Domain-Driven-Design-Tactical-Design-Patte\", \"rns-Part)[Patterns-Part](https://www.codeproject.com/Articles/1164363/Domain-Driven-Design-Tactical-\", \"Design-Patterns-Part)\\n- **Chris Richardson. Developing Transactional Microservices Using Aggregates*\", \"* <https://www.infoq.com/articles/microservices-aggregates-events-cqrs-part-1-richardson>\\n- **DevIQ.\", \" The Aggregate pattern** <https://deviq.com/aggregate-pattern/>\\n\\n## <span id=\\\"page-218-0\\\"></span>Imp\", \"lement a microservice domain model with .NET\\n\\nIn the previous section, the fundamental design princi\", \"ples and patterns for designing a domain model were explained. Now it's time to explore possible way\", \"s to implement the domain model by using .NET (plain C# code) and EF Core. Your domain model will be\", \" composed simply of your code. It will have just the EF Core model requirements, but not real depend\", \"encies on EF. You shouldn't have hard dependencies or references to EF Core or any other ORM in your\", \" domain model.\\n\\n### <span id=\\\"page-218-1\\\"></span>**Domain model structure in a custom .NET Standard \", \"Library**\\n\\nThe folder organization used for the eShopOnContainers reference application demonstrates\", \" the DDD model for the application. You might find that a different folder organization more clearly\", \" communicates the design choices made for your application. As you can see in Figure 7-10, in the or\", \"dering domain model there are two aggregates, the order aggregate and the buyer aggregate. Each aggr\", \"egate is a group of domain entities and value objects, although you could have an aggregate composed\", \" of a single domain entity (the aggregate root or root entity) as well.\\n\\n![](_page_219_Figure_0.jpeg\", \")\\n\\n*Figure 7-10. Domain model structure for the ordering microservice in eShopOnContainers*\\n\\nAdditio\", \"nally, the domain model layer includes the repository contracts (interfaces) that are the infrastruc\", \"ture requirements of your domain model. In other words, these interfaces express what repositories a\", \"nd the methods the infrastructure layer must implement. It's critical that the implementation of the\", \" repositories be placed outside of the domain model layer, in the infrastructure layer library, so t\", \"he domain model layer isn't \\\"contaminated\\\" by API or classes from infrastructure technologies, like \", \"Entity Framework.\\n\\nYou can also see a [SeedWork](https://martinfowler.com/bliki/Seedwork.html) folde\", \"r that contains custom base classes that you can use as a base for your domain entities and value ob\", \"jects, so you don't have redundant code in each domain's object class.\\n\\n### <span id=\\\"page-219-0\\\"></\", \"span>**Structure aggregates in a custom .NET Standard library**\\n\\nAn aggregate refers to a cluster of\", \" domain objects grouped together to match transactional consistency. Those objects could be instance\", \"s of entities (one of which is the aggregate root or root entity) plus any additional value objects.\", \"\\n\\nTransactional consistency means that an aggregate is guaranteed to be consistent and up to date at\", \" the end of a business action. For example, the order aggregate from the eShopOnContainers ordering \", \"microservice domain model is composed as shown in Figure 7-11.\\n\\n![](_page_220_Figure_0.jpeg)\\n\\nIf you\", \" open any of the files in an aggregate folder, you can see how it's marked as either a custom base c\", \"lass or interface, like entity or value object, as implemented in the [SeedWork](https://github.com/\", \"dotnet-architecture/eShopOnContainers/tree/main/src/Services/Ordering/Ordering.Domain/SeedWork) fold\", \"er.\\n\\n## <span id=\\\"page-220-0\\\"></span>**Implement domain entities as POCO classes**\\n\\nYou implement a \", \"domain model in .NET by creating POCO classes that implement your domain entities. In the following \", \"example, the Order class is defined as an entity and also as an aggregate root. Because the Order cl\", \"ass derives from the Entity base class, it can reuse common code related to entities. Bear in mind t\", \"hat these base classes and interfaces are defined by you in the domain model project, so it is your \", \"code, not infrastructure code from an ORM like EF.\\n\\n```\\n// COMPATIBLE WITH ENTITY FRAMEWORK CORE 5.0\", \"\\n// Entity is a custom base class with the ID\\npublic class Order : Entity, IAggregateRoot\\n{\\n private\", \" DateTime _orderDate;\\n public Address Address { get; private set; }\\n private int? _buyerId;\\n public \", \"OrderStatus OrderStatus { get; private set; }\\n private int _orderStatusId;\\n private string _descript\", \"ion;\\n private int? _paymentMethodId;\\n private readonly List<OrderItem> _orderItems;\\n public IReadOnl\", \"yCollection<OrderItem> OrderItems => _orderItems;\\n public Order(string userId, Address address, int \", \"cardTypeId, string cardNumber, string\\ncardSecurityNumber,\\n string cardHolderName, DateTime cardExpir\", \"ation, int? buyerId = null, int?\\npaymentMethodId = null)\\n {\\n _orderItems = new List<OrderItem>();\\n _\", \"buyerId = buyerId;\\n _paymentMethodId = paymentMethodId;\\n _orderStatusId = OrderStatus.Submitted.Id;\\n\", \" _orderDate = DateTime.UtcNow;\\n```\\n\\n```\\n Address = address;\\n // ...Additional code ...\\n }\\n public vo\", \"id AddOrderItem(int productId, string productName,\\n decimal unitPrice, decimal discount,\\n string pic\", \"tureUrl, int units = 1)\\n {\\n //...\\n // Domain rules/logic for adding the OrderItem to the order\\n // .\", \"..\\n var orderItem = new OrderItem(productId, productName, unitPrice, discount,\\npictureUrl, units);\\n \", \"_orderItems.Add(orderItem);\\n }\\n // ...\\n // Additional methods with domain rules/logic related to the\", \" Order aggregate\\n // ...\\n}\\n```\\n\\nIt's important to note that this is a domain entity implemented as a\", \" POCO class. It doesn't have any direct dependency on Entity Framework Core or any other infrastruct\", \"ure framework. This implementation is as it should be in DDD, just C# code implementing a domain mod\", \"el.\\n\\nIn addition, the class is decorated with an interface named IAggregateRoot. That interface is a\", \"n empty interface, sometimes called a *marker interface*, that's used just to indicate that this ent\", \"ity class is also an aggregate root.\\n\\nA marker interface is sometimes considered as an anti-pattern;\", \" however, it's also a clean way to mark a class, especially when that interface might be evolving. A\", \"n attribute could be the other choice for the marker, but it's quicker to see the base class (Entity\", \") next to the IAggregate interface instead of putting an Aggregate attribute marker above the class.\", \" It's a matter of preferences, in any case.\\n\\nHaving an aggregate root means that most of the code re\", \"lated to consistency and business rules of the aggregate's entities should be implemented as methods\", \" in the Order aggregate root class (for example, AddOrderItem when adding an OrderItem object to the\", \" aggregate). You should not create or update OrderItems objects independently or directly; the Aggre\", \"gateRoot class must keep control and consistency of any update operation against its child entities.\", \"\\n\\n### <span id=\\\"page-221-0\\\"></span>**Encapsulate data in the Domain Entities**\\n\\nA common problem in \", \"entity models is that they expose collection navigation properties as publicly accessible list types\", \". This allows any collaborator developer to manipulate the contents of these collection types, which\", \" may bypass important business rules related to the collection, possibly leaving the object in an in\", \"valid state. The solution to this is to expose read-only access to related collections and explicitl\", \"y provide methods that define ways in which clients can manipulate them.\\n\\nIn the previous code, note\", \" that many attributes are read-only or private and are only updatable by the class methods, so any u\", \"pdate considers business domain invariants and logic specified within the class methods.\\n\\nFor exampl\", \"e, following DDD patterns, **you should** *not* **do the following** from any command handler method\", \" or application layer class (actually, it should be impossible for you to do so):\\n\\n```\\n// WRONG ACCO\", \"RDING TO DDD PATTERNS \\u2013 CODE AT THE APPLICATION LAYER OR\\n// COMMAND HANDLERS\\n// Code in command hand\", \"ler methods or Web API controllers\\n//... (WRONG) Some code with business logic out of the domain cla\", \"sses ...\\nOrderItem myNewOrderItem = new OrderItem(orderId, productId, productName,\\n pictureUrl, unit\", \"Price, discount, units);\\n//... (WRONG) Accessing the OrderItems collection directly from the applica\", \"tion layer // or \\ncommand handlers\\nmyOrder.OrderItems.Add(myNewOrderItem);\\n//...\\nIn this case, the A\", \"dd method is purely an operation to add data, with direct access to the OrderItems\\n```\\n\\ncollection. \", \"Therefore, most of the domain logic, rules, or validations related to that operation with the child \", \"entities will be spread across the application layer (command handlers and Web API controllers).\\n\\nIf\", \" you go around the aggregate root, the aggregate root cannot guarantee its invariants, its validity,\", \" or its consistency. Eventually you'll have spaghetti code or transactional script code.\\n\\nTo follow \", \"DDD patterns, entities must not have public setters in any entity property. Changes in an entity sho\", \"uld be driven by explicit methods with explicit ubiquitous language about the change they're perform\", \"ing in the entity.\\n\\nFurthermore, collections within the entity (like the order items) should be read\", \"-only properties (the AsReadOnly method explained later). You should be able to update it only from \", \"within the aggregate root class methods or the child entity methods.\\n\\nAs you can see in the code for\", \" the Order aggregate root, all setters should be private or at least readonly externally, so that an\", \"y operation against the entity's data or its child entities has to be performed through methods in t\", \"he entity class. This maintains consistency in a controlled and object-oriented way instead of imple\", \"menting transactional script code.\\n\\nThe following code snippet shows the proper way to code the task\", \" of adding an OrderItem object to the Order aggregate.\\n\\n```\\n// RIGHT ACCORDING TO DDD--CODE AT THE A\", \"PPLICATION LAYER OR COMMAND HANDLERS\\n// The code in command handlers or WebAPI controllers, related \", \"only to application stuff\\n// There is NO code here related to OrderItem object's business logic\\nmyOr\", \"der.AddOrderItem(productId, productName, pictureUrl, unitPrice, discount, units);\\n// The code relate\", \"d to OrderItem params validations or domain rules should\\n// be WITHIN the AddOrderItem method.\\n//...\", \"\\n```\\n\\nIn this snippet, most of the validations or logic related to the creation of an OrderItem obje\", \"ct will be under the control of the Order aggregate root\\u2014in the AddOrderItem method\\u2014especially valid\", \"ations and logic related to other elements in the aggregate. For instance, you might get the same pr\", \"oduct item as the result of multiple calls to AddOrderItem. In that method, you could examine the pr\", \"oduct items and consolidate the same product items into a single OrderItem object with several units\", \".\\n\\nAdditionally, if there are different discount amounts but the product ID is the same, you would l\", \"ikely apply the higher discount. This principle applies to any other domain logic for the OrderItem \", \"object.\\n\\nIn addition, the new OrderItem(params) operation will also be controlled and performed by t\", \"he AddOrderItem method from the Order aggregate root. Therefore, most of the logic or validations re\", \"lated to that operation (especially anything that impacts the consistency between other child entiti\", \"es) will be in a single place within the aggregate root. That is the ultimate purpose of the aggrega\", \"te root pattern.\\n\\nWhen you use Entity Framework Core 1.1 or later, a DDD entity can be better expres\", \"sed because it allows [mapping to fields](https://docs.microsoft.com/ef/core/modeling/backing-field)\", \" in addition to properties. This is useful when protecting collections of child entities or value ob\", \"jects. With this enhancement, you can use simple private fields instead of properties and you can im\", \"plement any update to the field collection in public methods and provide read-only access through th\", \"e AsReadOnly method.\\n\\nIn DDD, you want to update the entity only through methods in the entity (or t\", \"he constructor) in order to control any invariant and the consistency of the data, so properties are\", \" defined only with a get accessor. The properties are backed by private fields. Private members can \", \"only be accessed from within the class. However, there is one exception: EF Core needs to set these \", \"fields as well (so it can return the object with the proper values).\\n\\n### **Map properties with only\", \" get accessors to the fields in the database table**\\n\\nMapping properties to database table columns i\", \"s not a domain responsibility but part of the infrastructure and persistence layer. We mention this \", \"here just so you're aware of the new capabilities in EF Core 1.1 or later related to how you can mod\", \"el entities. Additional details on this topic are explained in the infrastructure and persistence se\", \"ction.\\n\\nWhen you use EF Core 1.0 or later, within the DbContext you need to map the properties that \", \"are defined only with getters to the actual fields in the database table. This is done with the HasF\", \"ield method of the PropertyBuilder class.\\n\\n### **Map fields without properties**\\n\\nWith the feature i\", \"n EF Core 1.1 or later to map columns to fields, it's also possible to not use properties. Instead, \", \"you can just map columns from a table to fields. A common use case for this is private fields for an\", \" internal state that doesn't need to be accessed from outside the entity.\\n\\nFor example, in the prece\", \"ding OrderAggregate code example, there are several private fields, like the \\\\_paymentMethodId field\", \", that have no related property for either a setter or getter. That field could also be calculated w\", \"ithin the order's business logic and used from the order's methods, but it needs to be persisted in \", \"the database as well. So in EF Core (since v1.1), there's a way to map a field without a related pro\", \"perty to a column in the database. This is also explained in the [Infrastructure layer](#page-212-0)\", \" section of this guide.\\n\\n## **Additional resources**\\n\\n- **Vaughn Vernon. Modeling Aggregates with DD\", \"D and Entity Framework.** Note that this is *not* Entity Framework Core. <https://kalele.io/blog-pos\", \"ts/modeling-aggregates-with-ddd-and-entity-framework/>\\n- **Devs** [https://learn.microsoft.com/archi\", \"ve/msdn-magazine/2013/august/data-points-coding-for](https://docs.microsoft.com/archive/msdn-magazin\", \"e/2013/august/data-points-coding-for-domain-driven-design-tips-for-data-focused-devs)[domain-driven-\", \"design-tips-for-data-focused-devs](https://docs.microsoft.com/archive/msdn-magazine/2013/august/data\", \"-points-coding-for-domain-driven-design-tips-for-data-focused-devs)\\n\\n\\u2022 **Julie Lerman. Data Points -\", \" Coding for Domain-Driven Design: Tips for Data-Focused** \\n\\n- **Udi Dahan. How to create fully encap\", \"sulated Domain Models** <https://udidahan.com/2008/02/29/how-to-create-fully-encapsulated-domain-mod\", \"els/>\\n- **Steve Smith. What is the difference between a DTO and a POCO?** [https://ardalis.com/dto](\", \"https://ardalis.com/dto-or-poco/)[or-poco/](https://ardalis.com/dto-or-poco/)\\n\\n## <span id=\\\"page-224\", \"-0\\\"></span>Seedwork (reusable base classes and interfaces for your domain model)\\n\\nThe solution folde\", \"r contains a *SeedWork* folder. This folder contains custom base classes that you can use as a base \", \"for your domain entities and value objects. Use these base classes so you don't have redundant code \", \"in each domain's object class. The folder for these types of classes is called *SeedWork* and not so\", \"mething like *Framework*. It's called *SeedWork* because the folder contains just a small subset of \", \"reusable classes that cannot really be considered a framework. *Seedwork* is a term introduced by [M\", \"ichael Feathers](https://www.artima.com/forums/flat.jsp?forum=106&thread=8826) and popularized by [M\", \"artin Fowler](https://martinfowler.com/bliki/Seedwork.html) but you could also name that folder Comm\", \"on, SharedKernel, or similar.\\n\\nFigure 7-12 shows the classes that form the seedwork of the domain mo\", \"del in the ordering microservice. It has a few custom base classes like Entity, ValueObject, and Enu\", \"meration, plus a few interfaces. These interfaces (IRepository and IUnitOfWork) inform the infrastru\", \"cture layer about what needs to be implemented. Those interfaces are also used through Dependency In\", \"jection from the application layer.\\n\\n*Figure 7-12. A sample set of domain model \\\"seedwork\\\" base clas\", \"ses and interfaces*\\n\\nThis is the type of copy and paste reuse that many developers share between pro\", \"jects, not a formal framework. You can have seedworks in any layer or library. However, if the set o\", \"f classes and interfaces gets large enough, you might want to create a single class library.\\n\\n## <sp\", \"an id=\\\"page-225-0\\\"></span>**The custom Entity base class**\\n\\nThe following code is an example of an E\", \"ntity base class where you can place code that can be used the same way by any domain entity, such a\", \"s the entity ID, [equality operators,](https://docs.microsoft.com/dotnet/csharp/language-reference/o\", \"perators/equality-operators) a domain event list per entity, etc.\\n\\n```\\n// COMPATIBLE WITH ENTITY FRA\", \"MEWORK CORE (1.1 and later)\\npublic abstract class Entity\\n{\\n int? _requestedHashCode;\\n int _Id;\\n priv\", \"ate List<INotification> _domainEvents;\\n public virtual int Id\\n {\\n get\\n {\\n return _Id;\\n }\\n protected \", \"set\\n {\\n _Id = value;\\n }\\n }\\n public List<INotification> DomainEvents => _domainEvents;\\n public void A\", \"ddDomainEvent(INotification eventItem)\\n {\\n _domainEvents = _domainEvents ?? new List<INotification>(\", \");\\n _domainEvents.Add(eventItem);\\n }\\n public void RemoveDomainEvent(INotification eventItem)\\n {\\n if \", \"(_domainEvents is null) return;\\n _domainEvents.Remove(eventItem);\\n }\\n public bool IsTransient()\\n {\\n \", \"return this.Id == default(Int32);\\n }\\n public override bool Equals(object obj)\\n {\\n if (obj == null ||\", \" !(obj is Entity))\\n return false;\\n if (Object.ReferenceEquals(this, obj))\\n return true;\\n if (this.Ge\", \"tType() != obj.GetType())\\n return false;\\n Entity item = (Entity)obj;\\n if (item.IsTransient() || this\", \".IsTransient())\\n return false;\\n else\\n return item.Id == this.Id;\\n }\\n public override int GetHashCode\", \"()\\n {\\n if (!IsTransient())\\n```\\n\\n```\\n {\\n if (!_requestedHashCode.HasValue)\\n _requestedHashCode = this\", \".Id.GetHashCode() ^ 31;\\n // XOR for random distribution. See:\\n // https://learn.microsoft.com/archiv\", \"e/blogs/ericlippert/guidelines-and-rules-\\nfor-gethashcode\\n return _requestedHashCode.Value;\\n }\\n else\", \"\\n return base.GetHashCode();\\n }\\n public static bool operator ==(Entity left, Entity right)\\n {\\n if (O\", \"bject.Equals(left, null))\\n return (Object.Equals(right, null));\\n else\\n return left.Equals(right);\\n }\", \"\\n public static bool operator !=(Entity left, Entity right)\\n {\\n return !(left == right);\\n }\\n}\\n```\\n\\nT\", \"he previous code using a domain event list per entity will be explained in the next sections when fo\", \"cusing on domain events.\\n\\n## <span id=\\\"page-226-0\\\"></span>**Repository contracts (interfaces) in the\", \" domain model layer**\\n\\nRepository contracts are simply .NET interfaces that express the contract req\", \"uirements of the repositories to be used for each aggregate.\\n\\nThe repositories themselves, with EF C\", \"ore code or any other infrastructure dependencies and code (Linq, SQL, etc.), must not be implemente\", \"d within the domain model; the repositories should only implement the interfaces you define in the d\", \"omain model.\\n\\nA pattern related to this practice (placing the repository interfaces in the domain mo\", \"del layer) is the Separated Interface pattern. As [explained](https://www.martinfowler.com/eaaCatalo\", \"g/separatedInterface.html) by Martin Fowler, \\\"Use Separated Interface to define an interface in one \", \"package but implement it in another. This way a client that needs the dependency to the interface ca\", \"n be completely unaware of the implementation.\\\"\\n\\nFollowing the Separated Interface pattern enables t\", \"he application layer (in this case, the Web API project for the microservice) to have a dependency o\", \"n the requirements defined in the domain model, but not a direct dependency to the infrastructure/pe\", \"rsistence layer. In addition, you can use Dependency Injection to isolate the implementation, which \", \"is implemented in the infrastructure/ persistence layer using repositories.\\n\\nFor example, the follow\", \"ing example with the IOrderRepository interface defines what operations the OrderRepository class wi\", \"ll need to implement at the infrastructure layer. In the current implementation of the application, \", \"the code just needs to add or update orders to the database, since queries are split following the s\", \"implified CQRS approach.\\n\\n```\\n// Defined at IOrderRepository.cs\\npublic interface IOrderRepository : \", \"IRepository<Order>\\n{\\n```\\n\\n```\\n Order Add(Order order);\\n void Update(Order order);\\n Task<Order> GetAs\", \"ync(int orderId);\\n}\\n// Defined at IRepository.cs (Part of the Domain Seedwork)\\npublic interface IRep\", \"ository<T> where T : IAggregateRoot\\n{\\n IUnitOfWork UnitOfWork { get; }\\n}\\n```\\n\\n## <span id=\\\"page-227-\", \"0\\\"></span>**Additional resources**\\n\\n\\u2022 **Martin Fowler. Separated Interface.** <https://www.martinfow\", \"ler.com/eaaCatalog/separatedInterface.html>\\n\\n## <span id=\\\"page-227-1\\\"></span>Implement value objects\", \"\\n\\nAs discussed in earlier sections about entities and aggregates, identity is fundamental for entiti\", \"es. However, there are many objects and data items in a system that do not require an identity and i\", \"dentity tracking, such as value objects.\\n\\nA value object can reference other entities. For example, \", \"in an application that generates a route that describes how to get from one point to another, that r\", \"oute would be a value object. It would be a snapshot of points on a specific route, but this suggest\", \"ed route would not have an identity, even though internally it might refer to entities like City, Ro\", \"ad, etc.\\n\\nFigure 7-13 shows the Address value object within the Order aggregate.\\n\\n![](_page_228_Figu\", \"re_2.jpeg)\\n\\n*Figure 7-13. Address value object within the Order aggregate*\\n\\nAs shown in Figure 7-13,\", \" an entity is usually composed of multiple attributes. For example, the Order entity can be modeled \", \"as an entity with an identity and composed internally of a set of attributes such as OrderId, OrderD\", \"ate, OrderItems, etc. But the address, which is simply a complex-value composed of country/region, s\", \"treet, city, etc., and has no identity in this domain, must be modeled and treated as a value object\", \".\\n\\n### <span id=\\\"page-228-0\\\"></span>**Important characteristics of value objects**\\n\\nThere are two ma\", \"in characteristics for value objects:\\n\\n- They have no identity.\\n- They are immutable.\\n\\nThe first cha\", \"racteristic was already discussed. Immutability is an important requirement. The values of a value o\", \"bject must be immutable once the object is created. Therefore, when the object is\\n\\nconstructed, you \", \"must provide the required values, but you must not allow them to change during the object's lifetime\", \".\\n\\nValue objects allow you to perform certain tricks for performance, thanks to their immutable natu\", \"re. This is especially true in systems where there may be thousands of value object instances, many \", \"of which have the same values. Their immutable nature allows them to be reused; they can be intercha\", \"ngeable objects, since their values are the same and they have no identity. This type of optimizatio\", \"n can sometimes make a difference between software that runs slowly and software with good performan\", \"ce. Of course, all these cases depend on the application environment and deployment context.\\n\\n## <sp\", \"an id=\\\"page-229-0\\\"></span>**Value object implementation in C#**\\n\\nIn terms of implementation, you can\", \" have a value object base class that has basic utility methods like equality based on the comparison\", \" between all the attributes (since a value object must not be based on identity) and other fundament\", \"al characteristics. The following example shows a value object base class used in the ordering micro\", \"service from eShopOnContainers.\\n\\n```\\npublic abstract class ValueObject\\n{\\n protected static bool Equa\", \"lOperator(ValueObject left, ValueObject right)\\n {\\n if (ReferenceEquals(left, null) ^ ReferenceEquals\", \"(right, null))\\n {\\n return false;\\n }\\n return ReferenceEquals(left, right) || left.Equals(right);\\n }\\n \", \"protected static bool NotEqualOperator(ValueObject left, ValueObject right)\\n {\\n return !(EqualOperat\", \"or(left, right));\\n }\\n protected abstract IEnumerable<object> GetEqualityComponents();\\n public overri\", \"de bool Equals(object obj)\\n {\\n if (obj == null || obj.GetType() != GetType())\\n {\\n return false;\\n }\\n \", \"var other = (ValueObject)obj;\\n return this.GetEqualityComponents().SequenceEqual(other.GetEqualityCo\", \"mponents());\\n }\\n public override int GetHashCode()\\n {\\n return GetEqualityComponents()\\n .Select(x => \", \"x != null ? x.GetHashCode() : 0)\\n .Aggregate((x, y) => x ^ y);\\n }\\n // Other utility methods\\n}\\n```\\n\\nT\", \"he ValueObject is an abstract class type, but in this example, it doesn't overload the == and != ope\", \"rators. You could choose to do so, making comparisons delegate to the Equals override. For example, \", \"consider the following operator overloads to the ValueObject type:\\n\\n```\\npublic static bool operator \", \"==(ValueObject one, ValueObject two)\\n{\\n return EqualOperator(one, two);\\n}\\npublic static bool operato\", \"r !=(ValueObject one, ValueObject two)\\n{\\n return NotEqualOperator(one, two);\\n}\\nYou can use this clas\", \"s when implementing your actual value object, as with the Address \\nvalue object shown in the followi\", \"ng example:\\npublic class Address : ValueObject\\n{\\n public String Street { get; private set; }\\n public\", \" String City { get; private set; }\\n public String State { get; private set; }\\n public String Country\", \" { get; private set; }\\n public String ZipCode { get; private set; }\\n public Address() { }\\n public Ad\", \"dress(string street, string city, string state, string country, string\\nzipcode)\\n {\\n Street = street;\", \"\\n City = city;\\n State = state;\\n Country = country;\\n ZipCode = zipcode;\\n }\\n protected override IEnume\", \"rable<object> GetEqualityComponents()\\n {\\n // Using a yield return statement to return each element o\", \"ne at a time\\n yield return Street;\\n yield return City;\\n yield return State;\\n yield return Country;\\n \", \"yield return ZipCode;\\n }\\n}\\n```\\n\\nThis value object implementation of Address has no identity, and the\", \"refore no ID field is defined for it, either in the Address class definition or the ValueObject clas\", \"s definition.\\n\\nHaving no ID field in a class to be used by Entity Framework (EF) was not possible un\", \"til EF Core 2.0, which greatly helps to implement better value objects with no ID. That is precisely\", \" the explanation of the next section.\\n\\nIt could be argued that value objects, being immutable, shoul\", \"d be read-only (that is, have get-only properties), and that's indeed true. However, value objects a\", \"re usually serialized and deserialized to go through message queues, and being read-only stops the d\", \"eserializer from assigning values, so you just leave them as private set, which is read-only enough \", \"to be practical.\\n\\n### **Value object comparison semantics** Two instances of the Address type can be\", \" compared using all the following methods:\\n\\n```\\nvar one = new Address(\\\"1 Microsoft Way\\\", \\\"Redmond\\\", \", \"\\\"WA\\\", \\\"US\\\", \\\"98052\\\");\\nvar two = new Address(\\\"1 Microsoft Way\\\", \\\"Redmond\\\", \\\"WA\\\", \\\"US\\\", \\\"98052\\\");\\nCons\", \"ole.WriteLine(EqualityComparer<Address>.Default.Equals(one, two)); // True\\nConsole.WriteLine(object.\", \"Equals(one, two)); // True\\nConsole.WriteLine(one.Equals(two)); // True\\nConsole.WriteLine(one == two)\", \"; // True\\n```\\n\\nWhen all the values are the same, the comparisons are correctly evaluated as true. If\", \" you didn't choose to overload the == and != operators, then the last comparison of one == two would\", \" evaluate as false. For more information, see Overload ValueObject equality operators.\\n\\n## <span id=\", \"\\\"page-231-0\\\"></span>**How to persist value objects in the database with EF Core 2.0 and later**\\n\\nYou\", \" just saw how to define a value object in your domain model. But how can you actually persist it int\", \"o the database using Entity Framework Core since it usually targets entities with identity?\\n\\n### **B\", \"ackground and older approaches using EF Core 1.1**\\n\\nAs background, a limitation when using EF Core 1\", \".0 and 1.1 was that you could not use [complex types](https://docs.microsoft.com/dotnet/api/system.c\", \"omponentmodel.dataannotations.schema.complextypeattribute) as defined in EF 6.x in the traditional .\", \"NET Framework. Therefore, if using EF Core 1.0 or 1.1, you needed to store your value object as an E\", \"F entity with an ID field. Then, so it looked more like a value object with no identity, you could h\", \"ide its ID so you make clear that the identity of a value object is not important in the domain mode\", \"l. You could hide that ID by using the ID as a [shadow property.](https://docs.microsoft.com/ef/core\", \"/modeling/shadow-properties)  Since that configuration for hiding the ID in the model is set up in t\", \"he EF infrastructure level, it would be kind of transparent for your domain model.\\n\\nIn the initial v\", \"ersion of eShopOnContainers (.NET Core 1.1), the hidden ID needed by EF Core infrastructure was impl\", \"emented in the following way in the DbContext level, using Fluent API at the infrastructure project.\", \" Therefore, the ID was hidden from the domain model point of view, but still present in the infrastr\", \"ucture.\\n\\n```\\n// Old approach with EF Core 1.1\\n// Fluent API within the OrderingContext:DbContext in \", \"the Infrastructure project\\nvoid ConfigureAddress(EntityTypeBuilder<Address> addressConfiguration)\\n{\\n\", \" addressConfiguration.ToTable(\\\"address\\\", DEFAULT_SCHEMA);\\n addressConfiguration.Property<int>(\\\"Id\\\") \", \"// Id is a shadow property\\n .IsRequired();\\n addressConfiguration.HasKey(\\\"Id\\\"); // Id is a shadow pro\", \"perty\\n}\\n```\\n\\nHowever, the persistence of that value object into the database was performed like a re\", \"gular entity in a different table.\\n\\nWith EF Core 2.0 and later, there are new and better ways to per\", \"sist value objects.\\n\\n## <span id=\\\"page-232-0\\\"></span>**Persist value objects as owned entity types i\", \"n EF Core 2.0 and later** Even with some gaps between the canonical value object pattern in DDD and \", \"the owned entity type in\\n\\nEF Core, it's currently the best way to persist value objects with EF Core\", \" 2.0 and later. You can see limitations at the end of this section.\\n\\nThe owned entity type feature w\", \"as added to EF Core since version 2.0.\\n\\nAn owned entity type allows you to map types that do not hav\", \"e their own identity explicitly defined in the domain model and are used as properties, such as a va\", \"lue object, within any of your entities. An owned entity type shares the same CLR type with another \", \"entity type (that is, it's just a regular class). The entity containing the defining navigation is t\", \"he owner entity. When querying the owner, the owned types are included by default.\\n\\nJust by looking \", \"at the domain model, an owned type looks like it doesn't have any identity. However, under the cover\", \"s, owned types do have the identity, but the owner navigation property is part of this identity.\\n\\nTh\", \"e identity of instances of owned types is not completely their own. It consists of three components:\", \"\\n\\n- The identity of the owner\\n- The navigation property pointing to them\\n- In the case of collection\", \"s of owned types, an independent component (supported in EF Core 2.2 and later).\\n\\nFor example, in th\", \"e Ordering domain model at eShopOnContainers, as part of the Order entity, the Address value object \", \"is implemented as an owned entity type within the owner entity, which is the Order entity. Address i\", \"s a type with no identity property defined in the domain model. It is used as a property of the Orde\", \"r type to specify the shipping address for a particular order.\\n\\nBy convention, a shadow primary key \", \"is created for the owned type and it will be mapped to the same table as the owner by using table sp\", \"litting. This allows to use owned types similarly to how complex types are used in EF6 in the tradit\", \"ional .NET Framework.\\n\\nIt is important to note that owned types are never discovered by convention i\", \"n EF Core, so you have to declare them explicitly.\\n\\nIn eShopOnContainers, in the OrderingContext.cs \", \"file, within the OnModelCreating() method, multiple infrastructure configurations are applied. One o\", \"f them is related to the Order entity.\\n\\n```\\n// Part of the OrderingContext.cs class at the Ordering.\", \"Infrastructure project\\n//\\nprotected override void OnModelCreating(ModelBuilder modelBuilder)\\n{\\n mode\", \"lBuilder.ApplyConfiguration(new ClientRequestEntityTypeConfiguration());\\n modelBuilder.ApplyConfigur\", \"ation(new PaymentMethodEntityTypeConfiguration());\\n modelBuilder.ApplyConfiguration(new OrderEntityT\", \"ypeConfiguration());\\n modelBuilder.ApplyConfiguration(new OrderItemEntityTypeConfiguration());\\n //..\", \".Additional type configurations\\n}\\n```\\n\\nIn the following code, the persistence infrastructure is defi\", \"ned for the Order entity:\\n\\n```\\n// Part of the OrderEntityTypeConfiguration.cs class\\n//\\npublic void C\", \"onfigure(EntityTypeBuilder<Order> orderConfiguration)\\n{\\n orderConfiguration.ToTable(\\\"orders\\\", Orderi\", \"ngContext.DEFAULT_SCHEMA);\\n orderConfiguration.HasKey(o => o.Id);\\n orderConfiguration.Ignore(b => b.\", \"DomainEvents);\\n orderConfiguration.Property(o => o.Id)\\n .ForSqlServerUseSequenceHiLo(\\\"orderseq\\\", Ord\", \"eringContext.DEFAULT_SCHEMA);\\n //Address value object persisted as owned entity in EF Core 2.0\\n orde\", \"rConfiguration.OwnsOne(o => o.Address);\\n orderConfiguration.Property<DateTime>(\\\"OrderDate\\\").IsRequir\", \"ed();\\n //...Additional validations, constraints and code...\\n //...\\n}\\n```\\n\\nIn the previous code, the \", \"orderConfiguration.OwnsOne(o => o.Address) method specifies that the Address property is an owned en\", \"tity of the Order type.\\n\\nBy default, EF Core conventions name the database columns for the propertie\", \"s of the owned entity type as EntityProperty\\\\_OwnedEntityProperty. Therefore, the internal propertie\", \"s of Address will appear in the Orders table with the names Address\\\\_Street, Address\\\\_City (and so o\", \"n for State, Country, and ZipCode).\\n\\nYou can append the Property().HasColumnName() fluent method to \", \"rename those columns. In the case where Address is a public property, the mappings would be like the\", \" following:\\n\\n```\\norderConfiguration.OwnsOne(p => p.Address)\\n .Property(p=>p.Street).HasColumnName(\\\"S\", \"hippingStreet\\\");\\norderConfiguration.OwnsOne(p => p.Address)\\n .Property(p=>p.City).HasColumnName(\\\"Shi\", \"ppingCity\\\");\\n```\\n\\nIt's possible to chain the OwnsOne method in a fluent mapping. In the following hy\", \"pothetical example, OrderDetails owns BillingAddress and ShippingAddress, which are both Address typ\", \"es. Then OrderDetails is owned by the Order type.\\n\\n```\\norderConfiguration.OwnsOne(p => p.OrderDetail\", \"s, cb =>\\n {\\n cb.OwnsOne(c => c.BillingAddress);\\n cb.OwnsOne(c => c.ShippingAddress);\\n });\\n//...\\n//..\", \".\\npublic class Order\\n{\\n public int Id { get; set; }\\n public OrderDetails OrderDetails { get; set; }\\n\", \"}\\npublic class OrderDetails\\n{\\n public Address BillingAddress { get; set; }\\n```\\n\\n```\\n public Address \", \"ShippingAddress { get; set; }\\n}\\npublic class Address\\n{\\n public string Street { get; set; }\\n public s\", \"tring City { get; set; }\\n}\\n```\\n\\n## **Additional details on owned entity types**\\n\\n- Owned types are d\", \"efined when you configure a navigation property to a particular type using the OwnsOne fluent API.\\n-\", \" The definition of an owned type in our metadata model is a composite of: the owner type, the naviga\", \"tion property, and the CLR type of the owned type.\\n- The identity (key) of an owned type instance in\", \" our stack is a composite of the identity of the owner type and the definition of the owned type.\\n\\n#\", \"# **Owned entities capabilities**\\n\\n- Owned types can reference other entities, either owned (nested \", \"owned types) or non-owned (regular reference navigation properties to other entities).\\n- You can map\", \" the same CLR type as different owned types in the same owner entity through separate navigation pro\", \"perties.\\n- Table splitting is set up by convention, but you can opt out by mapping the owned type to\", \" a different table using ToTable.\\n- Eager loading is performed automatically on owned types, that is\", \", there's no need to call .Include() on the query.\\n- Can be configured with attribute [Owned], using\", \" EF Core 2.1 and later.\\n- Can handle collections of owned types (using version 2.2 and later).\\n\\n### \", \"**Owned entities limitations**\\n\\n- You can't create a DbSet<T> of an owned type (by design).\\n- You ca\", \"n't call ModelBuilder.Entity<T>() on owned types (currently by design).\\n- No support for optional (t\", \"hat is, nullable) owned types that are mapped with the owner in the same table (that is, using table\", \" splitting). This is because mapping is done for each property, there is no separate sentinel for th\", \"e null complex value as a whole.\\n- No inheritance-mapping support for owned types, but you should be\", \" able to map two leaf types of the same inheritance hierarchies as different owned types. EF Core wi\", \"ll not reason about the fact that they are part of the same hierarchy.\\n\\n## **Main differences with E\", \"F6's complex types**\\n\\n\\u2022 Table splitting is optional, that is, they can optionally be mapped to a sep\", \"arate table and still be owned types.\\n\\n## <span id=\\\"page-235-0\\\"></span>**Additional resources**\\n\\n- *\", \"*Martin Fowler. ValueObject pattern** <https://martinfowler.com/bliki/ValueObject.html>\\n- includes a\", \" discussion of value objects) [https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-](htt\", \"ps://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/)[Software/dp/03\", \"21125215/](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexity-Software/dp/0321125215/)\\n\", \"\\n\\u2022 **Eric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software.** (Book;\\n\\n- **V\", \"aughn Vernon. Implementing Domain-Driven Design.** (Book; includes a discussion of value objects) [h\", \"ttps://www.amazon.com/Implementing-Domain-Driven-Design-Vaughn-](https://www.amazon.com/Implementing\", \"-Domain-Driven-Design-Vaughn-Vernon/dp/0321834577/)[Vernon/dp/0321834577/](https://www.amazon.com/Im\", \"plementing-Domain-Driven-Design-Vaughn-Vernon/dp/0321834577/)\\n- **Owned Entity Types** [https://lear\", \"n.microsoft.com/ef/core/modeling/owned-entities](https://docs.microsoft.com/ef/core/modeling/owned-e\", \"ntities)\\n- **Shadow Properties** [https://learn.microsoft.com/ef/core/modeling/shadow-properties](ht\", \"tps://docs.microsoft.com/ef/core/modeling/shadow-properties)\\n- **Complex types and/or value objects*\", \"*. Discussion in the EF Core GitHub repo (Issues tab) <https://github.com/dotnet/efcore/issues/246>\\n\", \"- **ValueObject.cs.** Base value object class in eShopOnContainers. [https://github.com/dotnet](http\", \"s://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/\", \"SeedWork/ValueObject.cs)[architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Doma\", \"in/SeedWor](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/\", \"Ordering.Domain/SeedWork/ValueObject.cs) [k/ValueObject.cs](https://github.com/dotnet-architecture/e\", \"ShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/SeedWork/ValueObject.cs)\\n- **ValueOb\", \"ject.cs.** Base value object class in CSharpFunctionalExtensions. [https://github.com/vkhorikov/CSha\", \"rpFunctionalExtensions/blob/master/CSharpFunctionalExte](https://github.com/vkhorikov/CSharpFunction\", \"alExtensions/blob/master/CSharpFunctionalExtensions/ValueObject/ValueObject.cs) [nsions/ValueObject/\", \"ValueObject.cs](https://github.com/vkhorikov/CSharpFunctionalExtensions/blob/master/CSharpFunctional\", \"Extensions/ValueObject/ValueObject.cs)\\n- **Address class.** Sample value object class in eShopOnCont\", \"ainers. [https://github.com/dotnet](https://github.com/dotnet-architecture/eShopOnContainers/blob/de\", \"v/src/Services/Ordering/Ordering.Domain/AggregatesModel/OrderAggregate/Address.cs)[architecture/eSho\", \"pOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/Aggregat](https://github.com/dotnet-arc\", \"hitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/AggregatesModel/OrderAggr\", \"egate/Address.cs) [esModel/OrderAggregate/Address.cs](https://github.com/dotnet-architecture/eShopOn\", \"Containers/blob/dev/src/Services/Ordering/Ordering.Domain/AggregatesModel/OrderAggregate/Address.cs)\", \"\\n\\n<span id=\\\"page-235-1\\\"></span>Use enumeration classes instead of enum types\\n\\n[Enumerations](https:/\", \"/docs.microsoft.com/dotnet/csharp/language-reference/builtin-types/enum) (or *enum types* for short)\", \" are a thin language wrapper around an integral type. You might want to limit their use to when you \", \"are storing one value from a closed set of values. Classification based on sizes (small, medium, lar\", \"ge) is a good example. Using enums for control flow or more robust abstractions can be a [code smell\", \".](https://deviq.com/antipatterns/code-smells) This type of usage leads to fragile code with many co\", \"ntrol flow statements checking values of the enum.\\n\\nInstead, you can create Enumeration classes that\", \" enable all the rich features of an object-oriented language.\\n\\n[types](https://docs.microsoft.com/do\", \"tnet/csharp/language-reference/builtin-types/enum) if that's your preference. The use of enumeration\", \" classes is more related to business-related concepts.\\n\\nHowever, this isn't a critical topic and in \", \"many cases, for simplicity, you can still use regular [enum](https://docs.microsoft.com/dotnet/cshar\", \"p/language-reference/builtin-types/enum) \\n\\n## <span id=\\\"page-236-0\\\"></span>**Implement an Enumeratio\", \"n base class**\\n\\nThe ordering microservice in eShopOnContainers provides a sample Enumeration base cl\", \"ass implementation, as shown in the following example:\\n\\n```\\npublic abstract class Enumeration : ICom\", \"parable\\n{\\n public string Name { get; private set; }\\n public int Id { get; private set; }\\n protected \", \"Enumeration(int id, string name) => (Id, Name) = (id, name);\\n public override string ToString() => N\", \"ame;\\n public static IEnumerable<T> GetAll<T>() where T : Enumeration =>\\n typeof(T).GetFields(Binding\", \"Flags.Public |\\n BindingFlags.Static |\\n BindingFlags.DeclaredOnly)\\n .Select(f => f.GetValue(null))\\n .\", \"Cast<T>();\\n public override bool Equals(object obj)\\n {\\n if (obj is not Enumeration otherValue)\\n {\\n r\", \"eturn false;\\n }\\n var typeMatches = GetType().Equals(obj.GetType());\\n var valueMatches = Id.Equals(ot\", \"herValue.Id);\\n return typeMatches && valueMatches;\\n }\\n public int CompareTo(object other) => Id.Comp\", \"areTo(((Enumeration)other).Id);\\n // Other utility methods ...\\n}\\nYou can use this class as a type in \", \"any entity or value object, as for the following \\nCardType : Enumeration class:\\npublic class CardTyp\", \"e\\n : Enumeration\\n{\\n public static CardType Amex = new(1, nameof(Amex));\\n public static CardType Visa\", \" = new(2, nameof(Visa));\\n public static CardType MasterCard = new(3, nameof(MasterCard));\\n public Ca\", \"rdType(int id, string name)\\n : base(id, name)\\n {\\n```\\n\\n## <span id=\\\"page-237-0\\\"></span>**Additional r\", \"esources**\\n\\n\\u2022 **Jimmy Bogard. Enumeration classes** <https://lostechies.com/jimmybogard/2008/08/12/e\", \"numeration-classes/>\\n\\n\\u2022 **Enumeration.cs.** Base Enumeration class in eShopOnContainers\\n\\n<https://ar\", \"dalis.com/enum-alternatives-in-c>\\n\\n\\u2022 **Steve Smith. Enum Alternatives in C#**\\n\\n- [https://github.com\", \"/dotnet](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ord\", \"ering.Domain/SeedWork/Enumeration.cs)[architecture/eShopOnContainers/blob/dev/src/Services/Ordering/\", \"Ordering.Domain/SeedWor](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Servi\", \"ces/Ordering/Ordering.Domain/SeedWork/Enumeration.cs) [k/Enumeration.cs](https://github.com/dotnet-a\", \"rchitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/SeedWork/Enumeration.cs\", \")\\n- **CardType.cs**. Sample Enumeration class in eShopOnContainers. [https://github.com/dotnet](http\", \"s://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/\", \"AggregatesModel/BuyerAggregate/CardType.cs)[architecture/eShopOnContainers/blob/dev/src/Services/Ord\", \"ering/Ordering.Domain/Aggregat](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/sr\", \"c/Services/Ordering/Ordering.Domain/AggregatesModel/BuyerAggregate/CardType.cs) [esModel/BuyerAggreg\", \"ate/CardType.cs](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Orde\", \"ring/Ordering.Domain/AggregatesModel/BuyerAggregate/CardType.cs)\\n- **SmartEnum**. Ardalis Classes to\", \" help produce strongly typed smarter enums in .NET. <https://www.nuget.org/packages/Ardalis.SmartEnu\", \"m/>\\n\\n## <span id=\\\"page-237-1\\\"></span>Design validations in the domain model layer\\n\\nIn DDD, validatio\", \"n rules can be thought as invariants. The main responsibility of an aggregate is to enforce invarian\", \"ts across state changes for all the entities within that aggregate.\\n\\nDomain entities should always b\", \"e valid entities. There are a certain number of invariants for an object that should always be true.\", \" For example, an order item object always has to have a quantity that must be a positive integer, pl\", \"us an article name and price. Therefore, invariants enforcement is the responsibility of the domain \", \"entities (especially of the aggregate root) and an entity object should not be able to exist without\", \" being valid. Invariant rules are simply expressed as contracts, and exceptions or notifications are\", \" raised when they are violated.\\n\\nThe reasoning behind this is that many bugs occur because objects a\", \"re in a state they should never have been in.\\n\\nLet's propose we now have a SendUserCreationEmailServ\", \"ice that takes a UserProfile \\u2026 how can we rationalize in that service that Name is not null? Do we c\", \"heck it again? Or more likely \\u2026 you just don't bother to check and \\\"hope for the best\\\"\\u2014you hope that\", \" someone bothered to validate it before sending it to you. Of course, using TDD one of the first tes\", \"ts we should be writing is that if I send a customer with a null name that it should raise an error.\", \" But once we start writing these kinds of tests over and over again we realize \\u2026 \\\"what if we never a\", \"llowed name to become null? we wouldn't have all of these tests!\\\".\\n\\n## <span id=\\\"page-238-0\\\"></span>\", \"**Implement validations in the domain model layer**\\n\\nValidations are usually implemented in domain e\", \"ntity constructors or in methods that can update the entity. There are multiple ways to implement va\", \"lidations, such as verifying data and raising exceptions if the validation fails. There are also mor\", \"e advanced patterns such as using the Specification pattern for validations, and the Notification pa\", \"ttern to return a collection of errors instead of returning an exception for each validation as it o\", \"ccurs.\\n\\n## **Validate conditions and throw exceptions**\\n\\nThe following code example shows the simple\", \"st approach to validation in a domain entity by raising an exception. In the references table at the\", \" end of this section you can see links to more advanced implementations based on the patterns we hav\", \"e discussed previously.\\n\\n```\\npublic void SetAddress(Address address)\\n{\\n _shippingAddress = address??\", \" throw new ArgumentNullException(nameof(address));\\n}\\nA better example would demonstrate the need to \", \"ensure that either the internal state did \\nnot change, or that all the mutations for a method occurr\", \"ed. For example, the following \\nimplementation would leave the object in an invalid state:\\npublic vo\", \"id SetAddress(string line1, string line2,\\n string city, string state, int zip)\\n{\\n _shippingAddress.l\", \"ine1 = line1 ?? throw new ...\\n _shippingAddress.line2 = line2;\\n _shippingAddress.city = city ?? thro\", \"w new ...\\n _shippingAddress.state = (IsValid(state) ? state : throw new \\u2026);\\n}\\n```\\n\\nIf the value of t\", \"he state is invalid, the first address line and the city have already been changed. That might make \", \"the address invalid.\\n\\nA similar approach can be used in the entity's constructor, raising an excepti\", \"on to make sure that the entity is valid once it is created.\\n\\n### **Use validation attributes in the\", \" model based on data annotations**\\n\\nData annotations, like the Required or MaxLength attributes, can\", \" be used to configure EF Core database field properties, as explained in detail in the [Table mappin\", \"g](#page-264-0) section, but [they no longer work](https://github.com/dotnet/efcore/issues/3680)  [f\", \"or entity validation in EF Core](https://github.com/dotnet/efcore/issues/3680) (neither does the [IV\", \"alidatableObject.Validate](https://docs.microsoft.com/dotnet/api/system.componentmodel.dataannotatio\", \"ns.ivalidatableobject.validate) method), as they have done since EF 4.x in .NET Framework.\\n\\nData ann\", \"otations and the [IValidatableObject](https://docs.microsoft.com/dotnet/api/system.componentmodel.da\", \"taannotations.ivalidatableobject) interface can still be used for model validation during model bind\", \"ing, prior to the controller's actions invocation as usual, but that model is meant to be a ViewMode\", \"l or DTO and that's an MVC or API concern not a domain model concern.\\n\\nHaving made the conceptual di\", \"fference clear, you can still use data annotations and IValidatableObject in the entity class for va\", \"lidation, if your actions receive an entity class object parameter, which is not recommended. In tha\", \"t case, validation will occur upon model binding, just before invoking the action and you can check \", \"the controller's ModelState.IsValid property to check the result, but then again, it happens in the \", \"controller, not before persisting the entity object in the DbContext, as it had done since EF 4.x.\\n\\n\", \"You can still implement custom validation in the entity class using data annotations and the IValida\", \"tableObject.Validate method, by overriding the DbContext's SaveChanges method.\\n\\n[GitHub](https://git\", \"hub.com/dotnet/efcore/issues/3680#issuecomment-155502539). That sample doesn't do attribute-based va\", \"lidations, but they should be easy to implement using reflection in the same override.\\n\\nYou can see \", \"a sample implementation for validating IValidatableObject entities in [this comment on](https://gith\", \"ub.com/dotnet/efcore/issues/3680#issuecomment-155502539) \\n\\nHowever, from a DDD point of view, the do\", \"main model is best kept lean with the use of exceptions in your entity's behavior methods, or by imp\", \"lementing the Specification and Notification patterns to enforce validation rules.\\n\\nIt can make sens\", \"e to use data annotations at the application layer in ViewModel classes (instead of domain entities)\", \" that will accept input, to allow for model validation within the UI layer. However, this should not\", \" be done at the exclusion of validation within the domain model.\\n\\n## **Validate entities by implemen\", \"ting the Specification pattern and the Notification pattern**\\n\\nFinally, a more elaborate approach to\", \" implementing validations in the domain model is by implementing the Specification pattern in conjun\", \"ction with the Notification pattern, as explained in some of the additional resources listed later.\\n\", \"\\nIt is worth mentioning that you can also use just one of those patterns\\u2014for example, validating man\", \"ually with control statements, but using the Notification pattern to stack and return a list of vali\", \"dation errors.\\n\\n### **Use deferred validation in the domain**\\n\\nThere are various approaches to deal \", \"with deferred validations in the domain. In his book [Implementing Domain-Driven Design,](https://ww\", \"w.amazon.com/Implementing-Domain-Driven-Design-Vaughn-Vernon/dp/0321834577) Vaughn Vernon discusses \", \"these in the section on validation.\\n\\n### **Two-step validation**\\n\\nAlso consider two-step validation.\", \" Use field-level validation on your command Data Transfer Objects (DTOs) and domain-level validation\", \" inside your entities. You can do this by returning a result object instead of exceptions in order t\", \"o make it easier to deal with the validation errors.\\n\\nUsing field validation with data annotations, \", \"for example, you do not duplicate the validation definition. The execution, though, can be both serv\", \"er-side and client-side in the case of DTOs (commands and ViewModels, for instance).\\n\\n### <span id=\\\"\", \"page-239-0\\\"></span>**Additional resources**\\n\\n- **Rachel Appel. Introduction to model validation in A\", \"SP.NET Core MVC** [https://learn.microsoft.com/aspnet/core/mvc/models/validation](https://docs.micro\", \"soft.com/aspnet/core/mvc/models/validation)\\n- **Rick Anderson. Adding validation** [https://learn.mi\", \"crosoft.com/aspnet/core/tutorials/first-mvc-app/validation](https://docs.microsoft.com/aspnet/core/t\", \"utorials/first-mvc-app/validation)\\n\\n- **Martin Fowler. Replacing Throwing Exceptions with Notificati\", \"on in Validations** <https://martinfowler.com/articles/replaceThrowWithNotification.html>\\n- **Specif\", \"ication and Notification Patterns** <https://www.codeproject.com/Tips/790758/Specification-and-Notif\", \"ication-Patterns>\\n- **Lev Gorodinski. Validation in Domain-Driven Design (DDD)** <http://gorodinski.\", \"com/blog/2012/05/19/validation-in-domain-driven-design-ddd/>\\n- **Colin Jack. Domain Model Validation\", \"** <https://colinjack.blogspot.com/2008/03/domain-model-validation.html>\\n- **Jimmy Bogard. Validatio\", \"n in a DDD world** <https://lostechies.com/jimmybogard/2009/02/15/validation-in-a-ddd-world/>\\n\\n## <s\", \"pan id=\\\"page-240-0\\\"></span>Client-side validation (validation in the presentation layers)\\n\\nEven when\", \" the source of truth is the domain model and ultimately you must have validation at the domain model\", \" level, validation can still be handled at both the domain model level (server side) and the UI (cli\", \"ent side).\\n\\nClient-side validation is a great convenience for users. It saves time they would otherw\", \"ise spend waiting for a round trip to the server that might return validation errors. In business te\", \"rms, even a few fractions of seconds multiplied hundreds of times each day adds up to a lot of time,\", \" expense, and frustration. Straightforward and immediate validation enables users to work more effic\", \"iently and produce better quality input and output.\\n\\nJust as the view model and the domain model are\", \" different, view model validation and domain model validation might be similar but serve a different\", \" purpose. If you are concerned about DRY (the Don't Repeat Yourself principle), consider that in thi\", \"s case code reuse might also mean coupling, and in enterprise applications it is more important not \", \"to couple the server side to the client side than to follow the DRY principle.\\n\\nEven when using clie\", \"nt-side validation, you should always validate your commands or input DTOs in server code, because t\", \"he server APIs are a possible attack vector. Usually, doing both is your best bet because if you hav\", \"e a client application, from a UX perspective, it is best to be proactive and not allow the user to \", \"enter invalid information.\\n\\nTherefore, in client-side code you typically validate the ViewModels. Yo\", \"u could also validate the client output DTOs or commands before you send them to the services.\\n\\nThe \", \"implementation of client-side validation depends on what kind of client application you are building\", \". It will be different if you are validating data in a web MVC web application with most of the code\", \" in .NET, a SPA web application with that validation being coded in JavaScript or TypeScript, or a m\", \"obile app coded with Xamarin and C#.\\n\\n## <span id=\\\"page-241-0\\\"></span>**Additional resources**\\n\\n### \", \"**Validation in Xamarin mobile apps** \\u2022 **Validate Text Input and Show Errors**\\n\\n- [https://develope\", \"r.xamarin.com/recipes/ios/standard\\\\\\\\_controls/text\\\\\\\\_field/validate\\\\\\\\_input/](https://developer.xama\", \"rin.com/recipes/ios/standard_controls/text_field/validate_input/)\\n- **Validation Callback** <https:/\", \"/developer.xamarin.com/samples/xamarin-forms/XAML/ValidationCallback/>\\n\\n## **Validation in ASP.NET C\", \"ore apps**\\n\\n\\u2022 **Rick Anderson. Adding validation** [https://learn.microsoft.com/aspnet/core/tutorial\", \"s/first-mvc-app/validation](https://docs.microsoft.com/aspnet/core/tutorials/first-mvc-app/validatio\", \"n)\\n\\n## **Validation in SPA Web apps (Angular 2, TypeScript, JavaScript, Blazor WebAssembly)**\\n\\n- **F\", \"orm Validation** <https://angular.io/guide/form-validation>\\n- **Validation.** Breeze documentation. \", \"<https://breeze.github.io/doc-js/validation.html>\\n- **ASP.NET Core Blazor forms and input components\", \"**\\n\\nIn summary, these are the most important concepts in regards to validation:\\n\\n- Entities and aggr\", \"egates should enforce their own consistency and be \\\"always valid\\\". Aggregate roots are responsible f\", \"or multi-entity consistency within the same aggregate.\\n- If you think that an entity needs to enter \", \"an invalid state, consider using a different object model\\u2014for example, using a temporary DTO until y\", \"ou create the final domain entity.\\n- If you need to create several related objects, such as an aggre\", \"gate, and they are only valid once all of them have been created, consider using the Factory pattern\", \".\\n- In most of the cases, having redundant validation in the client side is good, because the applic\", \"ation can be proactive.\\n\\n## <span id=\\\"page-241-1\\\"></span>Domain events: Design and implementation\\n\\nU\", \"se domain events to explicitly implement side effects of changes within your domain. In other words,\", \" and using DDD terminology, use domain events to explicitly implement side effects across multiple a\", \"ggregates. Optionally, for better scalability and less impact in database locks, use eventual consis\", \"tency between aggregates within the same domain.\\n\\n## <span id=\\\"page-242-0\\\"></span>**What is a domain\", \" event?**\\n\\nAn event is something that has happened in the past. A domain event is, something that ha\", \"ppened in the domain that you want other parts of the same domain (in-process) to be aware of. The n\", \"otified parts usually react somehow to the events.\\n\\nAn important benefit of domain events is that si\", \"de effects can be expressed explicitly.\\n\\nFor example, if you're just using Entity Framework and ther\", \"e has to be a reaction to some event, you would probably code whatever you need close to what trigge\", \"rs the event. So the rule gets coupled, implicitly, to the code, and you have to look into the code \", \"to, hopefully, realize the rule is implemented there.\\n\\nOn the other hand, using domain events makes \", \"the concept explicit, because there's a DomainEvent and at least one DomainEventHandler involved.\\n\\nF\", \"or example, in the eShopOnContainers application, when an order is created, the user becomes a buyer\", \", so an OrderStartedDomainEvent is raised and handled in the ValidateOrAddBuyerAggregateWhenOrderSta\", \"rtedDomainEventHandler, so the underlying concept is\\n\\nevident.\\n\\nIn short, domain events help you to \", \"express, explicitly, the domain rules, based in the ubiquitous language provided by the domain exper\", \"ts. Domain events also enable a better separation of concerns among classes within the same domain.\\n\", \"\\nIt's important to ensure that, just like a database transaction, either all the operations related \", \"to a domain event finish successfully or none of them do.\\n\\nDomain events are similar to messaging-st\", \"yle events, with one important difference. With real messaging, message queuing, message brokers, or\", \" a service bus using AMQP, a message is always sent asynchronously and communicated across processes\", \" and machines. This is useful for integrating multiple Bounded Contexts, microservices, or even diff\", \"erent applications. However, with domain events, you want to raise an event from the domain operatio\", \"n you're currently running, but you want any side effects to occur within the same domain.\\n\\nThe doma\", \"in events and their side effects (the actions triggered afterwards that are managed by event handler\", \"s) should occur almost immediately, usually in-process, and within the same domain. Thus, domain eve\", \"nts could be synchronous or asynchronous. Integration events, however, should always be asynchronous\", \".\\n\\n### <span id=\\\"page-242-1\\\"></span>**Domain events versus integration events**\\n\\nSemantically, domai\", \"n and integration events are the same thing: notifications about something that just happened. Howev\", \"er, their implementation must be different. Domain events are just messages pushed to a domain event\", \" dispatcher, which could be implemented as an in-memory mediator based on an IoC container or any ot\", \"her method.\\n\\nOn the other hand, the purpose of integration events is to propagate committed transact\", \"ions and updates to additional subsystems, whether they are other microservices, Bounded Contexts or\", \" even external applications. Hence, they should occur only if the entity is successfully persisted, \", \"otherwise it's as if the entire operation never happened.\\n\\nAs mentioned before, integration events m\", \"ust be based on asynchronous communication between multiple microservices (other Bounded Contexts) o\", \"r even external systems/applications. Thus, the event bus interface needs some infrastructure that a\", \"llows inter-process and distributed\\n\\ncommunication between potentially remote services. It can be ba\", \"sed on a commercial service bus, queues, a shared database used as a mailbox, or any other distribut\", \"ed and ideally push based messaging system.\\n\\n## <span id=\\\"page-243-0\\\"></span>**Domain events as a pr\", \"eferred way to trigger side effects across multiple aggregates within the same domain**\\n\\nIf executin\", \"g a command related to one aggregate instance requires additional domain rules to be run on one or m\", \"ore additional aggregates, you should design and implement those side effects to be triggered by dom\", \"ain events. As shown in Figure 7-14, and as one of the most important use cases, a domain event shou\", \"ld be used to propagate state changes across multiple aggregates within the same domain model.\\n\\n![](\", \"_page_243_Figure_5.jpeg)\\n\\n*Figure 7-14. Domain events to enforce consistency between multiple aggreg\", \"ates within the same domain*\\n\\nFigure 7-14 shows how consistency between aggregates is achieved by do\", \"main events. When the user initiates an order, the Order Aggregate sends an OrderStarted domain even\", \"t. The OrderStarted domain event is handled by the Buyer Aggregate to create a Buyer object in the o\", \"rdering microservice, based on the original user info from the identity microservice (with informati\", \"on provided in the CreateOrder command).\\n\\nAlternately, you can have the aggregate root subscribed fo\", \"r events raised by members of its aggregates (child entities). For instance, each OrderItem child en\", \"tity can raise an event when the item price is higher than a specific amount, or when the product it\", \"em amount is too high. The aggregate root can then receive those events and perform a global calcula\", \"tion or aggregation.\\n\\nIt's important to understand that this event-based communication is not implem\", \"ented directly within the aggregates; you need to implement domain event handlers.\\n\\nHandling the dom\", \"ain events is an application concern. The domain model layer should only focus on the domain logic\\u2014t\", \"hings that a domain expert would understand, not application infrastructure like handlers and side-e\", \"ffect persistence actions using repositories. Therefore, the application layer level is where you sh\", \"ould have domain event handlers triggering actions when a domain event is raised.\\n\\nDomain events can\", \" also be used to trigger any number of application actions, and what is more important, must be open\", \" to increase that number in the future in a decoupled way. For instance, when the order is started, \", \"you might want to publish a domain event to propagate that info to other aggregates or even to raise\", \" application actions like notifications.\\n\\nThe key point is the open number of actions to be executed\", \" when a domain event occurs. Eventually, the actions and rules in the domain and application will gr\", \"ow. The complexity or number of sideeffect actions when something happens will grow, but if your cod\", \"e were coupled with \\\"glue\\\" (that is, creating specific objects with new), then every time you needed\", \" to add a new action you would also need to change working and tested code.\\n\\nThis change could resul\", \"t in new bugs and this approach also goes against the [Open/Closed principle](https://en.wikipedia.o\", \"rg/wiki/Open/closed_principle) from [SOLID.](https://en.wikipedia.org/wiki/SOLID) Not only that, the\", \" original class that was orchestrating the operations would grow and grow, which goes against the [S\", \"ingle Responsibility Principle \\\\(SRP\\\\).](https://en.wikipedia.org/wiki/Single_responsibility_princip\", \"le)\\n\\nOn the other hand, if you use domain events, you can create a fine-grained and decoupled implem\", \"entation by segregating responsibilities using this approach:\\n\\n- 1. Send a command (for example, Cre\", \"ateOrder).\\n- 2. Receive the command in a command handler.\\n  - Execute a single aggregate's transacti\", \"on.\\n  - (Optional) Raise domain events for side effects (for example, OrderStartedDomainEvent).\\n- 3.\", \" Handle domain events (within the current process) that will execute an open number of side effects \", \"in multiple aggregates or application actions. For example:\\n  - Verify or create buyer and payment m\", \"ethod.\\n  - Create and send a related integration event to the event bus to propagate states across m\", \"icroservices or trigger external actions like sending an email to the buyer.\\n  - Handle other side e\", \"ffects.\\n\\nAs shown in Figure 7-15, starting from the same domain event, you can handle multiple actio\", \"ns related to other aggregates in the domain or additional application actions you need to perform a\", \"cross microservices connecting with integration events and the event bus.\\n\\n![](_page_245_Figure_0.jp\", \"eg)\\n\\n*Figure 7-15. Handling multiple actions per domain*\\n\\nThere can be several handlers for the same\", \" domain event in the Application Layer, one handler can solve consistency between aggregates and ano\", \"ther handler can publish an integration event, so other microservices can do something with it. The \", \"event handlers are typically in the application layer, because you'll use infrastructure objects lik\", \"e repositories or an application API for the microservice's behavior. In that sense, event handlers \", \"are similar to command handlers, so both are part of the application layer. The important difference\", \" is that a command should be processed only once. A domain event could be processed zero or *n* time\", \"s, because it can be received by multiple receivers or event handlers with a different purpose for e\", \"ach handler.\\n\\nHaving an open number of handlers per domain event allows you to add as many domain ru\", \"les as needed, without affecting current code. For instance, implementing the following business rul\", \"e might be as easy as adding a few event handlers (or even just one):\\n\\nWhen the total amount purchas\", \"ed by a customer in the store, across any number of orders, exceeds \\\\$6,000, apply a 10% off discoun\", \"t to every new order and notify the customer with an email about that discount for future orders.\\n\\n#\", \"## <span id=\\\"page-245-0\\\"></span>**Implement domain events**\\n\\nIn C#, a domain event is simply a data-\", \"holding structure or class, like a DTO, with all the information related to what just happened in th\", \"e domain, as shown in the following example:\\n\\n```\\npublic class OrderStartedDomainEvent : INotificati\", \"on\\n{\\n public string UserId { get; }\\n public string UserName { get; }\\n public int CardTypeId { get; }\", \"\\n public string CardNumber { get; }\\n public string CardSecurityNumber { get; }\\n```\\n\\n```\\n public stri\", \"ng CardHolderName { get; }\\n public DateTime CardExpiration { get; }\\n public Order Order { get; }\\n pu\", \"blic OrderStartedDomainEvent(Order order, string userId, string userName,\\n int cardTypeId, string ca\", \"rdNumber,\\n string cardSecurityNumber, string cardHolderName,\\n DateTime cardExpiration)\\n {\\n Order = o\", \"rder;\\n UserId = userId;\\n UserName = userName;\\n CardTypeId = cardTypeId;\\n CardNumber = cardNumber;\\n C\", \"ardSecurityNumber = cardSecurityNumber;\\n CardHolderName = cardHolderName;\\n CardExpiration = cardExpi\", \"ration;\\n }\\n}\\n```\\n\\nThis is essentially a class that holds all the data related to the OrderStarted ev\", \"ent.\\n\\nIn terms of the ubiquitous language of the domain, since an event is something that happened i\", \"n the past, the class name of the event should be represented as a past-tense verb, like OrderStarte\", \"dDomainEvent or OrderShippedDomainEvent. That's how the domain event is implemented in the ordering \", \"microservice in eShopOnContainers.\\n\\nAs noted earlier, an important characteristic of events is that \", \"since an event is something that happened in the past, it shouldn't change. Therefore, it must be an\", \" immutable class. You can see in the previous code that the properties are read-only. There's no way\", \" to update the object, you can only set values when you create it.\\n\\nIt's important to highlight here\", \" that if domain events were to be handled asynchronously, using a queue that required serializing an\", \"d deserializing the event objects, the properties would have to be \\\"private set\\\" instead of read-onl\", \"y, so the deserializer would be able to assign the values upon dequeuing. This is not an issue in th\", \"e Ordering microservice, as the domain event pub/sub is implemented synchronously using MediatR.\\n\\n##\", \"# **Raise domain events**\\n\\nThe next question is how to raise a domain event so it reaches its relate\", \"d event handlers. You can use multiple approaches.\\n\\nUdi Dahan originally proposed (for example, in s\", \"everal related posts, such as [Domain Events](https://udidahan.com/2008/08/25/domain-events-take-2/)\", \" \\u2013 Take 2) using a static class for managing and raising the events. This might include a static cla\", \"ss named DomainEvents that would raise domain events immediately when it's called, using syntax like\", \" DomainEvents.Raise(Event myEvent). Jimmy Bogard wrote a blog post [\\\\(Strengthening your domain:](ht\", \"tps://lostechies.com/jimmybogard/2010/04/08/strengthening-your-domain-domain-events/)  [Domain Event\", \"s\\\\)](https://lostechies.com/jimmybogard/2010/04/08/strengthening-your-domain-domain-events/) that re\", \"commends a similar approach.\\n\\nHowever, when the domain events class is static, it also dispatches to\", \" handlers immediately. This makes testing and debugging more difficult, because the event handlers w\", \"ith side-effects logic are executed immediately after the event is raised. When you're testing and d\", \"ebugging, you just want to focus on what is happening in the current aggregate classes; you don't wa\", \"nt to suddenly be\\n\\nredirected to other event handlers for side effects related to other aggregates o\", \"r application logic. This is why other approaches have evolved, as explained in the next section.\\n\\n#\", \"# **The deferred approach to raise and dispatch events**\\n\\nInstead of dispatching to a domain event h\", \"andler immediately, a better approach is to add the domain events to a collection and then to dispat\", \"ch those domain events *right before* or *right after* committing the transaction (as with SaveChang\", \"es in EF). (This approach was described by Jimmy Bogard in this post [A better domain events pattern\", \".](https://lostechies.com/jimmybogard/2014/05/13/a-better-domain-events-pattern/))\\n\\nDeciding if you \", \"send the domain events right before or right after committing the transaction is important, since it\", \" determines whether you will include the side effects as part of the same transaction or in differen\", \"t transactions. In the latter case, you need to deal with eventual consistency across multiple aggre\", \"gates. This topic is discussed in the next section.\\n\\nThe deferred approach is what eShopOnContainers\", \" uses. First, you add the events happening in your entities into a collection or list of events per \", \"entity. That list should be part of the entity object, or even better, part of your base entity clas\", \"s, as shown in the following example of the Entity base class:\\n\\n```\\npublic abstract class Entity\\n{\\n \", \"//...\\n private List<INotification> _domainEvents;\\n public List<INotification> DomainEvents => _domai\", \"nEvents;\\n public void AddDomainEvent(INotification eventItem)\\n {\\n _domainEvents = _domainEvents ?? n\", \"ew List<INotification>();\\n _domainEvents.Add(eventItem);\\n }\\n public void RemoveDomainEvent(INotifica\", \"tion eventItem)\\n {\\n _domainEvents?.Remove(eventItem);\\n }\\n //... Additional code\\n}\\n```\\n\\nWhen you want\", \" to raise an event, you just add it to the event collection from code at any method of the aggregate\", \"-root entity.\\n\\nThe following code, part of the [Order aggregate-root at eShopOnContainers,](https://\", \"github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.Domain/Aggr\", \"egatesModel/OrderAggregate/Order.cs) shows an example:\\n\\n```\\nvar orderStartedDomainEvent = new OrderS\", \"tartedDomainEvent(this, //Order object\\n cardTypeId, cardNumber,\\n cardSecurityNumber,\\n               \", \"                      cardHolderName,\\n                                     cardExpiration);\\nthis.Add\", \"DomainEvent(orderStartedDomainEvent);\\n```\\n\\nNotice that the only thing that the AddDomainEvent method\", \" is doing is adding an event to the list. No event is dispatched yet, and no event handler is invoke\", \"d yet.\\n\\nYou actually want to dispatch the events later on, when you commit the transaction to the da\", \"tabase. If you are using Entity Framework Core, that means in the SaveChanges method of your EF DbCo\", \"ntext, as in the following code:\\n\\n```\\n// EF Core DbContext\\npublic class OrderingContext : DbContext,\", \" IUnitOfWork\\n{\\n // ...\\n public async Task<bool> SaveEntitiesAsync(CancellationToken cancellationToke\", \"n =\\ndefault(CancellationToken))\\n {\\n // Dispatch Domain Events collection.\\n // Choices:\\n // A) Right \", \"BEFORE committing data (EF SaveChanges) into the DB. This makes\\n // a single transaction including s\", \"ide effects from the domain event\\n // handlers that are using the same DbContext with Scope lifetime\", \"\\n // B) Right AFTER committing data (EF SaveChanges) into the DB. This makes\\n // multiple transactio\", \"ns. You will need to handle eventual consistency and\\n // compensatory actions in case of failures.\\n \", \"await _mediator.DispatchDomainEventsAsync(this);\\n // After this line runs, all the changes (from the\", \" Command Handler and Domain\\n // event handlers) performed through the DbContext will be committed\\n v\", \"ar result = await base.SaveChangesAsync();\\n }\\n}\\n```\\n\\nWith this code, you dispatch the entity events \", \"to their respective event handlers.\\n\\nThe overall result is that you've decoupled the raising of a do\", \"main event (a simple add into a list in memory) from dispatching it to an event handler. In addition\", \", depending on what kind of dispatcher you are using, you could dispatch the events synchronously or\", \" asynchronously.\\n\\nBe aware that transactional boundaries come into significant play here. If your un\", \"it of work and transaction can span more than one aggregate (as when using EF Core and a relational \", \"database), this can work well. But if the transaction cannot span aggregates, you have to implement \", \"additional steps to achieve consistency. This is another reason why persistence ignorance is not uni\", \"versal; it depends on the storage system you use.\\n\\n### **Single transaction across aggregates versus\", \" eventual consistency across aggregates**\\n\\nThe question of whether to perform a single transaction a\", \"cross aggregates versus relying on eventual consistency across those aggregates is a controversial o\", \"ne. Many DDD authors like Eric Evans and Vaughn Vernon advocate the rule that one transaction = one \", \"aggregate and therefore argue for eventual consistency across aggregates. For example, in his book *\", \"Domain-Driven Design*, Eric Evans says this:\\n\\nAny rule that spans Aggregates will not be expected to\", \" be up-to-date at all times. Through event processing, batch processing, or other update mechanisms,\", \" other dependencies can be resolved within some specific time. (page 128)\\n\\nVaughn Vernon says the fo\", \"llowing in [Effective Aggregate Design. Part II: Making Aggregates Work](https://dddcommunity.org/wp\", \"-content/uploads/files/pdf_articles/Vernon_2011_2.pdf)  [Together:](https://dddcommunity.org/wp-cont\", \"ent/uploads/files/pdf_articles/Vernon_2011_2.pdf)\\n\\nThus, if executing a command on one aggregate ins\", \"tance requires that additional business rules execute on one or more aggregates, use eventual consis\", \"tency [\\u2026] There is a practical way to support eventual consistency in a DDD model. An aggregate meth\", \"od publishes a domain event that is in time delivered to one or more asynchronous subscribers.\\n\\nThis\", \" rationale is based on embracing fine-grained transactions instead of transactions spanning many agg\", \"regates or entities. The idea is that in the second case, the number of database locks will be subst\", \"antial in large-scale applications with high scalability needs. Embracing the fact that highly scala\", \"ble applications need not have instant transactional consistency between multiple aggregates helps w\", \"ith accepting the concept of eventual consistency. Atomic changes are often not needed by the busine\", \"ss, and it is in any case the responsibility of the domain experts to say whether particular operati\", \"ons need atomic transactions or not. If an operation always needs an atomic transaction between mult\", \"iple aggregates, you might ask whether your aggregate should be larger or wasn't correctly designed.\", \"\\n\\nHowever, other developers and architects like Jimmy Bogard are okay with spanning a single transac\", \"tion across several aggregates\\u2014but only when those additional aggregates are related to side effects\", \" for the same original command. For instance, in [A better domain events pattern,](https://lostechie\", \"s.com/jimmybogard/2014/05/13/a-better-domain-events-pattern/) Bogard says this:\\n\\nTypically, I want t\", \"he side effects of a domain event to occur within the same logical transaction, but not necessarily \", \"in the same scope of raising the domain event [\\u2026] Just before we commit our transaction, we dispatch\", \" our events to their respective handlers.\\n\\nIf you dispatch the domain events right *before* committi\", \"ng the original transaction, it is because you want the side effects of those events to be included \", \"in the same transaction. For example, if the EF DbContext SaveChanges method fails, the transaction \", \"will roll back all changes, including the result of any side effect operations implemented by the re\", \"lated domain event handlers. This is because the DbContext life scope is by default defined as \\\"scop\", \"ed.\\\" Therefore, the DbContext object is shared across multiple repository objects being instantiated\", \" within the same scope or object graph. This coincides with the HttpRequest scope when developing We\", \"b API or MVC apps.\\n\\nActually, both approaches (single atomic transaction and eventual consistency) c\", \"an be right. It really depends on your domain or business requirements and what the domain experts t\", \"ell you. It also depends on how scalable you need the service to be (more granular transactions have\", \" less impact with regard to database locks). And it depends on how much investment you're willing to\", \" make in your code, since eventual consistency requires more complex code in order to detect possibl\", \"e inconsistencies across aggregates and the need to implement compensatory actions. Consider that if\", \" you commit changes to the original aggregate and afterwards, when the events are being dispatched, \", \"if there's an issue and the event handlers cannot commit their side effects, you'll have inconsisten\", \"cies between aggregates.\\n\\nA way to allow compensatory actions would be to store the domain events in\", \" additional database tables so they can be part of the original transaction. Afterwards, you could h\", \"ave a batch process that detects inconsistencies and runs compensatory actions by comparing the list\", \" of events with the current state of the aggregates. The compensatory actions are part of a complex \", \"topic that will require deep analysis from your side, which includes discussing it with the business\", \" user and domain experts.\\n\\nIn any case, you can choose the approach you need. But the initial deferr\", \"ed approach\\u2014raising the events before committing, so you use a single transaction\\u2014is the simplest ap\", \"proach when using EF Core and a relational database. It's easier to implement and valid in many busi\", \"ness cases. It's also the approach used in the ordering microservice in eShopOnContainers. But how d\", \"o you actually dispatch those events to their respective event handlers? What's the\\n\\n\\\\_mediator obje\", \"ct you see in the previous example? It has to do with the techniques and artifacts you use to map be\", \"tween events and their event handlers.\\n\\n## **The domain event dispatcher: mapping from events to eve\", \"nt handlers**\\n\\nOnce you're able to dispatch or publish the events, you need some kind of artifact th\", \"at will publish the event, so that every related handler can get it and process side effects based o\", \"n that event.\\n\\nOne approach is a real messaging system or even an event bus, possibly based on a ser\", \"vice bus as opposed to in-memory events. However, for the first case, real messaging would be overki\", \"ll for processing domain events, since you just need to process those events within the same process\", \" (that is, within the same domain and application layer).\\n\\n### **How to subscribe to domain events**\", \"\\n\\nWhen you use MediatR, each event handler must use an event type that is provided on the generic pa\", \"rameter of the INotificationHandler interface, as you can see in the following code:\\n\\n```\\npublic cla\", \"ss ValidateOrAddBuyerAggregateWhenOrderStartedDomainEventHandler\\n : INotificationHandler<OrderStarte\", \"dDomainEvent>\\n```\\n\\nBased on the relationship between event and event handler, which can be considere\", \"d the subscription, the MediatR artifact can discover all the event handlers for each event and trig\", \"ger each one of those event handlers.\\n\\n### **How to handle domain events**\\n\\nFinally, the event handl\", \"er usually implements application layer code that uses infrastructure repositories to obtain the req\", \"uired additional aggregates and to execute side-effect domain logic. The following [domain event han\", \"dler code at eShopOnContainers,](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/s\", \"rc/Services/Ordering/Ordering.API/Application/DomainEventHandlers/ValidateOrAddBuyerAggregateWhenOrd\", \"erStartedDomainEventHandler.cs) shows an implementation example.\\n\\n```\\npublic class ValidateOrAddBuye\", \"rAggregateWhenOrderStartedDomainEventHandler\\n : INotificationHandler<OrderStartedDomainEvent>\\n{\\n pri\", \"vate readonly ILogger _logger;\\n private readonly IBuyerRepository _buyerRepository;\\n private readonl\", \"y IOrderingIntegrationEventService _orderingIntegrationEventService;\\n public ValidateOrAddBuyerAggre\", \"gateWhenOrderStartedDomainEventHandler(\\n ILogger<ValidateOrAddBuyerAggregateWhenOrderStartedDomainEv\", \"entHandler> logger,\\n IBuyerRepository buyerRepository,\\n IOrderingIntegrationEventService orderingInt\", \"egrationEventService)\\n {\\n _buyerRepository = buyerRepository ?? throw new\\nArgumentNullException(name\", \"of(buyerRepository));\\n _orderingIntegrationEventService = orderingIntegrationEventService ?? throw n\", \"ew\\nArgumentNullException(nameof(orderingIntegrationEventService));\\n```\\n\\n```\\n _logger = logger ?? thr\", \"ow new ArgumentNullException(nameof(logger));\\n }\\n public async Task Handle(\\n OrderStartedDomainEvent\", \" domainEvent, CancellationToken cancellationToken)\\n {\\n var cardTypeId = domainEvent.CardTypeId != 0 \", \"? domainEvent.CardTypeId : 1;\\n var buyer = await _buyerRepository.FindAsync(domainEvent.UserId);\\n va\", \"r buyerExisted = buyer is not null;\\n if (!buyerExisted)\\n {\\n buyer = new Buyer(domainEvent.UserId, do\", \"mainEvent.UserName);\\n }\\n buyer.VerifyOrAddPaymentMethod(\\n cardTypeId,\\n $\\\"Payment Method on {DateTime\", \".UtcNow}\\\",\\n domainEvent.CardNumber,\\n domainEvent.CardSecurityNumber,\\n domainEvent.CardHolderName,\\n d\", \"omainEvent.CardExpiration,\\n domainEvent.Order.Id);\\n var buyerUpdated = buyerExisted ?\\n _buyerReposit\", \"ory.Update(buyer) :\\n _buyerRepository.Add(buyer);\\n await _buyerRepository.UnitOfWork\\n .SaveEntitiesA\", \"sync(cancellationToken);\\n var integrationEvent = new OrderStatusChangedToSubmittedIntegrationEvent(\\n\", \" domainEvent.Order.Id, domainEvent.Order.OrderStatus.Name, buyer.Name);\\n await _orderingIntegrationE\", \"ventService.AddAndSaveEventAsync(integrationEvent);\\n OrderingApiTrace.LogOrderBuyerAndPaymentValidat\", \"edOrUpdated(\\n _logger, buyerUpdated.Id, domainEvent.Order.Id);\\n }\\n}\\n```\\n\\nThe previous domain event h\", \"andler code is considered application layer code because it uses infrastructure repositories, as exp\", \"lained in the next section on the infrastructure-persistence layer. Event handlers could also use ot\", \"her infrastructure components.\\n\\n### **Domain events can generate integration events to be published \", \"outside of the microservice boundaries**\\n\\nFinally, it's important to mention that you might sometime\", \"s want to propagate events across multiple microservices. That propagation is an integration event, \", \"and it could be published through an event bus from any specific domain event handler.\\n\\n### <span id\", \"=\\\"page-251-0\\\"></span>**Conclusions on domain events**\\n\\nAs stated, use domain events to explicitly im\", \"plement side effects of changes within your domain. To use DDD terminology, use domain events to exp\", \"licitly implement side effects across one or multiple aggregates. Additionally, and for better scala\", \"bility and less impact on database locks, use eventual consistency between aggregates within the sam\", \"e domain.\\n\\nThe reference app uses [MediatR](https://github.com/jbogard/MediatR) to propagate domain \", \"events synchronously across aggregates, within a single transaction. However, you could also use som\", \"e AMQP implementation like [RabbitMQ](https://www.rabbitmq.com/) or [Azure Service Bus](https://docs\", \".microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview) to propagate domain event\", \"s asynchronously, using eventual consistency but, as mentioned above, you have to consider the need \", \"for compensatory actions in case of failures.\\n\\n## <span id=\\\"page-252-0\\\"></span>**Additional resource\", \"s**\\n\\n\\u2022 **Greg Young. What is a Domain Event?**\\n\\n- [https://cqrs.files.wordpress.com/2010/11/cqrs\\\\\\\\_d\", \"ocuments.pdf#page=25](https://cqrs.files.wordpress.com/2010/11/cqrs_documents.pdf#page=25)\\n- <https:\", \"//www.infoq.com/news/2015/09/domain-events-consistency>\\n\\n\\u2022 **Jan Stenberg. Domain Events and Eventua\", \"l Consistency**\\n\\n- **Jimmy Bogard. A better domain events pattern** <https://lostechies.com/jimmybog\", \"ard/2014/05/13/a-better-domain-events-pattern/>\\n- **Vaughn Vernon. Effective Aggregate Design Part I\", \"I: Making Aggregates Work Together** [https://dddcommunity.org/wp-content/uploads/files/pdf\\\\\\\\_articl\", \"es/Vernon\\\\\\\\_2011\\\\\\\\_2.pdf](https://dddcommunity.org/wp-content/uploads/files/pdf_articles/Vernon_2011\", \"_2.pdf)\\n- **Jimmy Bogard. Strengthening your domain: Domain Events** [https://lostechies.com/jimmybo\", \"gard/2010/04/08/strengthening-your-domain-domain](https://lostechies.com/jimmybogard/2010/04/08/stre\", \"ngthening-your-domain-domain-events/)[events/](https://lostechies.com/jimmybogard/2010/04/08/strengt\", \"hening-your-domain-domain-events/)\\n- **Udi Dahan. How to create fully encapsulated Domain Models** <\", \"https://udidahan.com/2008/02/29/how-to-create-fully-encapsulated-domain-models/>\\n- **Udi Dahan. Doma\", \"in Events \\u2013 Take 2** <https://udidahan.com/2008/08/25/domain-events-take-2/>\\n- **Udi Dahan. Domain E\", \"vents \\u2013 Salvation** <https://udidahan.com/2009/06/14/domain-events-salvation/>\\n- **Cesar de la Torre\", \". Domain Events vs. Integration Events in DDD and microservices architectures**\\n\\n[https://devblogs.m\", \"icrosoft.com/cesardelatorre/domain-events-vs-integration-events-in](https://devblogs.microsoft.com/c\", \"esardelatorre/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architec\", \"tures/)[domain-driven-design-and-microservices-architectures/](https://devblogs.microsoft.com/cesard\", \"elatorre/domain-events-vs-integration-events-in-domain-driven-design-and-microservices-architectures\", \"/)\\n\\n## <span id=\\\"page-252-1\\\"></span>Design the infrastructure persistence layer\\n\\nData persistence co\", \"mponents provide access to the data hosted within the boundaries of a microservice (that is, a micro\", \"service's database). They contain the actual implementation of components such as repositories and [\", \"Unit of Work](https://martinfowler.com/eaaCatalog/unitOfWork.html) classes, like custom Entity Frame\", \"work (EF) [DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext)\", \" objects. EF DbContext implements both the Repository and the Unit of Work patterns.\\n\\n### <span id=\\\"\", \"page-252-2\\\"></span>**The Repository pattern**\\n\\nThe Repository pattern is a Domain-Driven Design patt\", \"ern intended to keep persistence concerns outside of the system's domain model. One or more persiste\", \"nce abstractions - interfaces - are defined in the domain model, and these abstractions have impleme\", \"ntations in the form of persistence-specific adapters defined elsewhere in the application.\\n\\nReposit\", \"ory implementations are classes that encapsulate the logic required to access data sources. They cen\", \"tralize common data access functionality, providing better maintainability and decoupling the infras\", \"tructure or technology used to access databases from the domain model. If you use an Object-Relation\", \"al Mapper (ORM) like Entity Framework, the code that must be implemented is simplified, thanks to LI\", \"NQ and strong typing. This lets you focus on the data persistence logic rather than on data access p\", \"lumbing.\\n\\nThe Repository pattern is a well-documented way of working with a data source. In the book\", \" [Patterns](https://www.amazon.com/Patterns-Enterprise-Application-Architecture-Martin/dp/0321127420\", \"/)  [of Enterprise Application Architecture,](https://www.amazon.com/Patterns-Enterprise-Application\", \"-Architecture-Martin/dp/0321127420/) Martin Fowler describes a repository as follows:\\n\\nA repository \", \"performs the tasks of an intermediary between the domain model layers and data mapping, acting in a \", \"similar way to a set of domain objects in memory. Client objects declaratively build queries and sen\", \"d them to the repositories for answers. Conceptually, a repository encapsulates a set of objects sto\", \"red in the database and operations that can be performed on them, providing a way that is closer to \", \"the persistence layer. Repositories, also, support the purpose of separating, clearly and in one dir\", \"ection, the dependency between the work domain and the data allocation or mapping.\\n\\n### **Define one\", \" repository per aggregate**\\n\\nFor each aggregate or aggregate root, you should create one repository \", \"class. You may be able to leverage C# Generics to reduce the total number concrete classes you need \", \"to maintain (as demonstrated later in this chapter). In a microservice based on Domain-Driven Design\", \" (DDD) patterns, the only channel you should use to update the database should be the repositories. \", \"This is because they have a one-to-one relationship with the aggregate root, which controls the aggr\", \"egate's invariants and transactional consistency. It's okay to query the database through other chan\", \"nels (as you can do following a CQRS approach), because queries don't change the state of the databa\", \"se. However, the transactional area (that is, the updates) must always be controlled by the reposito\", \"ries and the aggregate roots.\\n\\nBasically, a repository allows you to populate data in memory that co\", \"mes from the database in the form of the domain entities. Once the entities are in memory, they can \", \"be changed and then persisted back to the database through transactions.\\n\\nAs noted earlier, if you'r\", \"e using the CQS/CQRS architectural pattern, the initial queries are performed by side queries out of\", \" the domain model, performed by simple SQL statements using Dapper. This approach is much more flexi\", \"ble than repositories because you can query and join any tables you need, and these queries aren't r\", \"estricted by rules from the aggregates. That data goes to the presentation layer or client app.\\n\\nIf \", \"the user makes changes, the data to be updated comes from the client app or presentation layer to th\", \"e application layer (such as a Web API service). When you receive a command in a command handler, yo\", \"u use repositories to get the data you want to update from the database. You update it in memory wit\", \"h the data passed with the commands, and you then add or update the data (domain entities) in the da\", \"tabase through a transaction.\\n\\nIt's important to emphasize again that you should only define one rep\", \"ository for each aggregate root, as shown in Figure 7-17. To achieve the goal of the aggregate root \", \"to maintain transactional consistency between all the objects within the aggregate, you should never\", \" create a repository for each table in the database.\\n\\n![](_page_254_Figure_1.jpeg)\\n\\n*Figure 7-17. Th\", \"e relationship between repositories, aggregates, and database tables*\\n\\nThe above diagram shows the r\", \"elationships between Domain and Infrastructure layers: Buyer Aggregate depends on the IBuyerReposito\", \"ry and Order Aggregate depends on the IOrderRepository interfaces, these interfaces are implemented \", \"in the Infrastructure layer by the corresponding repositories that depend on UnitOfWork, also implem\", \"ented there, that accesses the tables in the Data tier.\\n\\n### **Enforce one aggregate root per reposi\", \"tory**\\n\\nIt can be valuable to implement your repository design in such a way that it enforces the ru\", \"le that only aggregate roots should have repositories. You can create a generic or base repository t\", \"ype that constrains the type of entities it works with to ensure they have the IAggregateRoot marker\", \" interface.\\n\\nThus, each repository class implemented at the infrastructure layer implements its own \", \"contract or interface, as shown in the following code:\\n\\n```\\nnamespace Microsoft.eShopOnContainers.Se\", \"rvices.Ordering.Infrastructure.Repositories\\n{\\n public class OrderRepository : IOrderRepository\\n {\\n /\", \"/ ...\\n```\\n\\n```\\n }\\n}\\n```\\n\\nEach specific repository interface implements the generic IRepository inter\", \"face:\\n\\n```\\npublic interface IOrderRepository : IRepository<Order>\\n{\\n Order Add(Order order);\\n // ...\", \"\\n}\\n```\\n\\nHowever, a better way to have the code enforce the convention that each repository is relate\", \"d to a single aggregate is to implement a generic repository type. That way, it's explicit that you'\", \"re using a repository to target a specific aggregate. That can be easily done by implementing a gene\", \"ric IRepository base interface, as in the following code:\\n\\n```\\npublic interface IRepository<T> where\", \" T : IAggregateRoot\\n{\\n //....\\n}\\n```\\n\\n## **The Repository pattern makes it easier to test your applic\", \"ation logic**\\n\\nThe Repository pattern allows you to easily test your application with unit tests. Re\", \"member that unit tests only test your code, not infrastructure, so the repository abstractions make \", \"it easier to achieve that goal.\\n\\nAs noted in an earlier section, it's recommended that you define an\", \"d place the repository interfaces in the domain model layer so the application layer, such as your W\", \"eb API microservice, doesn't depend directly on the infrastructure layer where you've implemented th\", \"e actual repository classes. By doing this and using Dependency Injection in the controllers of your\", \" Web API, you can implement mock repositories that return fake data instead of data from the databas\", \"e. This decoupled approach allows you to create and run unit tests that focus the logic of your appl\", \"ication without requiring connectivity to the database.\\n\\nConnections to databases can fail and, more\", \" importantly, running hundreds of tests against a database is bad for two reasons. First, it can tak\", \"e a long time because of the large number of tests. Second, the database records might change and im\", \"pact the results of your tests, especially if your tests are running in parallel, so that they might\", \" not be consistent. Unit tests typically can run in parallel; integration tests may not support para\", \"llel execution depending on their implementation. Testing against the database isn't a unit test but\", \" an integration test. You should have many unit tests running fast, but fewer integration tests agai\", \"nst the databases.\\n\\nIn terms of separation of concerns for unit tests, your logic operates on domain\", \" entities in memory. It assumes the repository class has delivered those. Once your logic modifies t\", \"he domain entities, it assumes the repository class will store them correctly. The important point h\", \"ere is to create unit tests against your domain model and its domain logic. Aggregate roots are the \", \"main consistency boundaries in DDD.\\n\\nThe repositories implemented in eShopOnContainers rely on EF Co\", \"re's DbContext implementation of the Repository and Unit of Work patterns using its change tracker, \", \"so they don't duplicate this functionality.\\n\\n## **The difference between the Repository pattern and \", \"the legacy Data Access class (DAL class) pattern**\\n\\nA typical DAL object directly performs data acce\", \"ss and persistence operations against storage, often at the level of a single table and row. Simple \", \"CRUD operations implemented with a set of DAL classes frequently do not support transactions (though\", \" this is not always the case). Most DAL class approaches make minimal use of abstractions, resulting\", \" in tight coupling between application or Business Logic Layer (BLL) classes that call the DAL objec\", \"ts.\\n\\nWhen using repository, the implementation details of persistence are encapsulated away from the\", \" domain model. The use of an abstraction provides ease of extending behavior through patterns like D\", \"ecorators or Proxies. For instance, cross-cutting concerns like [caching,](https://ardalis.com/build\", \"ing-a-cachedrepository-in-aspnet-core/) logging, and error handling can all be applied using these p\", \"atterns rather than hard-coded in the data access code itself. It's also trivial to support multiple\", \" repository adapters which may be used in different environments, from local development to shared s\", \"taging environments to production.\\n\\n## **Implementing Unit of Work**\\n\\nA [unit of work](https://marti\", \"nfowler.com/eaaCatalog/unitOfWork.html) refers to a single transaction that involves multiple insert\", \", update, or delete operations. In simple terms, it means that for a specific user action, such as a\", \" registration on a website, all the insert, update, and delete operations are handled in a single tr\", \"ansaction. This is more efficient than handling multiple database operations in a chattier way.\\n\\nThe\", \"se multiple persistence operations are performed later in a single action when your code from the ap\", \"plication layer commands it. The decision about applying the in-memory changes to the actual databas\", \"e storage is typically based on the Unit of Work pattern. In EF, the Unit of Work pattern is impleme\", \"nted by a [DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext)\", \" and is executed when a call is made to SaveChanges.\\n\\nIn many cases, this pattern or way of applying\", \" operations against the storage can increase application performance and reduce the possibility of i\", \"nconsistencies. It also reduces transaction blocking in the database tables, because all the intende\", \"d operations are committed as part of one transaction. This is more efficient in comparison to execu\", \"ting many isolated operations against the database. Therefore, the selected ORM can optimize the exe\", \"cution against the database by grouping several update actions within the same transaction, as oppos\", \"ed to many small and separate transaction executions.\\n\\nThe Unit of Work pattern can be implemented w\", \"ith or without using the Repository pattern.\\n\\n### **Repositories shouldn't be mandatory**\\n\\nCustom re\", \"positories are useful for the reasons cited earlier, and that is the approach for the ordering micro\", \"service in eShopOnContainers. However, it isn't an essential pattern to implement in a DDD design or\", \" even in general .NET development.\\n\\nFor instance, Jimmy Bogard, when providing direct feedback for t\", \"his guide, said the following:\\n\\nThis'll probably be my biggest feedback. I'm really not a fan of rep\", \"ositories, mainly because they hide the important details of the underlying persistence mechanism. I\", \"t's why I go for MediatR for commands, too. I can use the full power of the persistence layer, and p\", \"ush all that domain behavior into my aggregate roots. I don't usually want to mock my repositories \\u2013\", \" I still need to have that\\n\\nintegration test with the real thing. Going CQRS meant that we didn't re\", \"ally have a need for repositories any more.\\n\\nRepositories might be useful, but they are not critical\", \" for your DDD design in the way that the\\n\\nAggregate pattern and a rich domain model are. Therefore, \", \"use the Repository pattern or not, as you see fit.\\n\\n## <span id=\\\"page-257-0\\\"></span>**Additional res\", \"ources**\\n\\n### **Repository pattern** \\u2022 **Edward Hieatt and Rob Mee. Repository pattern.**\\n\\n- <https:\", \"//martinfowler.com/eaaCatalog/repository.html>\\n- **The Repository pattern** [https://learn.microsoft\", \".com/previous-versions/msp-n-p/ff649690\\\\(v=pandp.10\\\\)](https://docs.microsoft.com/previous-versions/\", \"msp-n-p/ff649690(v=pandp.10))\\n- **Eric Evans. Domain-Driven Design: Tackling Complexity in the Heart\", \" of Software.** (Book; includes a discussion of the Repository pattern) [https://www.amazon.com/Doma\", \"in-Driven-Design-Tackling-Complexity-](https://www.amazon.com/Domain-Driven-Design-Tackling-Complexi\", \"ty-Software/dp/0321125215/)[Software/dp/0321125215/](https://www.amazon.com/Domain-Driven-Design-Tac\", \"kling-Complexity-Software/dp/0321125215/)\\n\\n### **Unit of Work pattern**\\n\\n- **Martin Fowler. Unit of \", \"Work pattern.** <https://martinfowler.com/eaaCatalog/unitOfWork.html>\\n- **Implementing the Repositor\", \"y and Unit of Work Patterns in an ASP.NET MVC Application**\\n\\n[https://learn.microsoft.com/aspnet/mvc\", \"/overview/older-versions/getting-started-with-ef-5](https://docs.microsoft.com/aspnet/mvc/overview/o\", \"lder-versions/getting-started-with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-pat\", \"terns-in-an-asp-net-mvc-application) [using-mvc-4/implementing-the-repository-and-unit-of-work-patte\", \"rns-in-an-asp-net-mvc](https://docs.microsoft.com/aspnet/mvc/overview/older-versions/getting-started\", \"-with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-appli\", \"cation)[application](https://docs.microsoft.com/aspnet/mvc/overview/older-versions/getting-started-w\", \"ith-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-applica\", \"tion)\\n\\n## <span id=\\\"page-257-1\\\"></span>Implement the infrastructure persistence layer with Entity Fr\", \"amework Core\\n\\nWhen you use relational databases such as SQL Server, Oracle, or PostgreSQL, a recomme\", \"nded approach is to implement the persistence layer based on Entity Framework (EF). EF supports LINQ\", \" and provides strongly typed objects for your model, as well as simplified persistence into your dat\", \"abase.\\n\\nEntity Framework has a long history as part of the .NET Framework. When you use .NET, you sh\", \"ould also use Entity Framework Core, which runs on Windows or Linux in the same way as .NET. EF Core\", \" is a complete rewrite of Entity Framework that's implemented with a much smaller footprint and impo\", \"rtant improvements in performance.\\n\\n## <span id=\\\"page-258-0\\\"></span>**Introduction to Entity Framewo\", \"rk Core**\\n\\nEntity Framework (EF) Core is a lightweight, extensible, and cross-platform version of th\", \"e popular Entity Framework data access technology. It was introduced with .NET Core in mid-2016.\\n\\nSi\", \"nce an introduction to EF Core is already available in Microsoft documentation, here we simply provi\", \"de links to that information.\\n\\n## **Additional resources**\\n\\n- **Entity Framework Core** [https://lea\", \"rn.microsoft.com/ef/core/](https://docs.microsoft.com/ef/core/)\\n- **Getting started with ASP.NET Cor\", \"e and Entity Framework Core using Visual Studio** [https://learn.microsoft.com/aspnet/core/data/ef-m\", \"vc/](https://docs.microsoft.com/aspnet/core/data/ef-mvc/)\\n- **DbContext Class** [https://learn.micro\", \"soft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext](https://docs.microsoft.com/dotnet/api/m\", \"icrosoft.entityframeworkcore.dbcontext)\\n- **Compare EF Core & EF6.x** [https://learn.microsoft.com/e\", \"f/efcore-and-ef6/index](https://docs.microsoft.com/ef/efcore-and-ef6/index)\\n\\n## <span id=\\\"page-258-1\", \"\\\"></span>**Infrastructure in Entity Framework Core from a DDD perspective**\\n\\nFrom a DDD point of vie\", \"w, an important capability of EF is the ability to use POCO domain entities, also known in EF termin\", \"ology as POCO *code-first entities*. If you use POCO domain entities, your domain model classes are \", \"persistence-ignorant, following the [Persistence Ignorance](https://deviq.com/persistence-ignorance/\", \") and the [Infrastructure Ignorance](https://ayende.com/blog/3137/infrastructure-ignorance) principl\", \"es.\\n\\nPer DDD patterns, you should encapsulate domain behavior and rules within the entity class itse\", \"lf, so it can control invariants, validations, and rules when accessing any collection. Therefore, i\", \"t is not a good practice in DDD to allow public access to collections of child entities or value obj\", \"ects. Instead, you want to expose methods that control how and when your fields and property collect\", \"ions can be updated, and what behavior and actions should occur when that happens.\\n\\nSince EF Core 1.\", \"1, to satisfy those DDD requirements, you can have plain fields in your entities instead of public p\", \"roperties. If you do not want an entity field to be externally accessible, you can just create the a\", \"ttribute or field instead of a property. You can also use private property setters.\\n\\nIn a similar wa\", \"y, you can now have read-only access to collections by using a public property typed as IReadOnlyCol\", \"lection<T>, which is backed by a private field member for the collection (like a List<T>) in your en\", \"tity that relies on EF for persistence. Previous versions of Entity Framework required collection pr\", \"operties to support ICollection<T>, which meant that any developer using the parent entity class cou\", \"ld add or remove items through its property collections. That possibility would be against the recom\", \"mended patterns in DDD.\\n\\nYou can use a private collection while exposing a read-only IReadOnlyCollec\", \"tion<T> object, as shown in the following code example:\\n\\n```\\npublic class Order : Entity\\n```\\n\\n{\\n\\n```\", \"\\n // Using private fields, allowed since EF Core 1.1\\n private DateTime _orderDate;\\n // Other fields \", \"...\\n private readonly List<OrderItem> _orderItems;\\n public IReadOnlyCollection<OrderItem> OrderItems\", \" => _orderItems;\\n protected Order() { }\\n public Order(int buyerId, int paymentMethodId, Address addr\", \"ess)\\n {\\n // Initializations ...\\n }\\n public void AddOrderItem(int productId, string productName,\\n dec\", \"imal unitPrice, decimal discount,\\n string pictureUrl, int units = 1)\\n {\\n // Validation logic...\\n var\", \" orderItem = new OrderItem(productId, productName,\\n unitPrice, discount,\\n pictureUrl, units);\\n _orde\", \"rItems.Add(orderItem);\\n }\\n}\\n```\\n\\nThe OrderItems property can only be accessed as read-only using IRe\", \"adOnlyCollection<OrderItem>. This type is read-only so it is protected against regular external upda\", \"tes.\\n\\nEF Core provides a way to map the domain model to the physical database without \\\"contaminating\", \"\\\" the domain model. It is pure .NET POCO code, because the mapping action is implemented in the pers\", \"istence layer. In that mapping action, you need to configure the fields-to-database mapping. In the \", \"following example of the OnModelCreating method from OrderingContext and the OrderEntityTypeConfigur\", \"ation class, the call to SetPropertyAccessMode tells EF Core to access the OrderItems property throu\", \"gh its field.\\n\\n```\\n// At OrderingContext.cs from eShopOnContainers\\nprotected override void OnModelCr\", \"eating(ModelBuilder modelBuilder)\\n{\\n // ...\\n modelBuilder.ApplyConfiguration(new OrderEntityTypeConf\", \"iguration());\\n // Other entities' configuration ...\\n}\\n// At OrderEntityTypeConfiguration.cs from eSh\", \"opOnContainers\\nclass OrderEntityTypeConfiguration : IEntityTypeConfiguration<Order>\\n{\\n public void C\", \"onfigure(EntityTypeBuilder<Order> orderConfiguration)\\n {\\n orderConfiguration.ToTable(\\\"orders\\\", Order\", \"ingContext.DEFAULT_SCHEMA);\\n // Other configuration\\n var navigation =\\n orderConfiguration.Metadata.F\", \"indNavigation(nameof(Order.OrderItems));\\n //EF access the OrderItem collection property through its \", \"backing field\\n navigation.SetPropertyAccessMode(PropertyAccessMode.Field);\\n```\\n\\n```\\n // Other config\", \"uration\\n }\\n}\\n```\\n\\nWhen you use fields instead of properties, the OrderItem entity is persisted as if\", \" it had a List<OrderItem> property. However, it exposes a single accessor, the AddOrderItem method, \", \"for adding new items to the order. As a result, behavior and data are tied together and will be cons\", \"istent throughout any application code that uses the domain model.\\n\\n## <span id=\\\"page-260-0\\\"></span>\", \"**Implement custom repositories with Entity Framework Core** At the implementation level, a reposito\", \"ry is simply a class with data persistence code coordinated by a\\n\\nunit of work (DBContext in EF Core\", \") when performing updates, as shown in the following class:\\n\\n```\\n// using directives...\\nnamespace Mi\", \"crosoft.eShopOnContainers.Services.Ordering.Infrastructure.Repositories\\n{\\n public class BuyerReposit\", \"ory : IBuyerRepository\\n {\\n private readonly OrderingContext _context;\\n public IUnitOfWork UnitOfWork\", \"\\n {\\n get\\n {\\n return _context;\\n }\\n }\\n public BuyerRepository(OrderingContext context)\\n {\\n _context = \", \"context ?? throw new ArgumentNullException(nameof(context));\\n }\\n public Buyer Add(Buyer buyer)\\n {\\n r\", \"eturn _context.Buyers.Add(buyer).Entity;\\n }\\n public async Task<Buyer> FindAsync(string buyerIdentity\", \"Guid)\\n {\\n var buyer = await _context.Buyers\\n .Include(b => b.Payments)\\n .Where(b => b.FullName == bu\", \"yerIdentityGuid)\\n .SingleOrDefaultAsync();\\n return buyer;\\n }\\n }\\n}\\n```\\n\\nThe IBuyerRepository interfac\", \"e comes from the domain model layer as a contract. However, the repository implementation is done at\", \" the persistence and infrastructure layer.\\n\\nThe EF DbContext comes through the constructor through D\", \"ependency Injection. It is shared between multiple repositories within the same HTTP request scope, \", \"thanks to its default lifetime (ServiceLifetime.Scoped) in the IoC container (which can also be expl\", \"icitly set with services.AddDbContext<>).\\n\\n## **Methods to implement in a repository (updates or tra\", \"nsactions versus queries)**\\n\\nWithin each repository class, you should put the persistence methods th\", \"at update the state of entities contained by its related aggregate. Remember there is one-to-one rel\", \"ationship between an aggregate and its related repository. Consider that an aggregate root entity ob\", \"ject might have embedded child entities within its EF graph. For example, a buyer might have multipl\", \"e payment methods as related child entities.\\n\\nSince the approach for the ordering microservice in eS\", \"hopOnContainers is also based on CQS/CQRS, most of the queries are not implemented in custom reposit\", \"ories. Developers have the freedom to create the queries and joins they need for the presentation la\", \"yer without the restrictions imposed by aggregates, custom repositories per aggregate, and DDD in ge\", \"neral. Most of the custom repositories suggested by this guide have several update or transactional \", \"methods but just the query methods needed to get data to be updated. For example, the BuyerRepositor\", \"y repository implements a FindAsync method, because the application needs to know whether a particul\", \"ar buyer exists before creating a new buyer related to the order.\\n\\nHowever, the real query methods t\", \"o get data to send to the presentation layer or client apps are implemented, as mentioned, in the CQ\", \"RS queries based on flexible queries using Dapper.\\n\\n### **Using a custom repository versus using EF \", \"DbContext directly**\\n\\nThe Entity Framework DbContext class is based on the Unit of Work and Reposito\", \"ry patterns and can be used directly from your code, such as from an ASP.NET Core MVC controller. Th\", \"e Unit of Work and Repository patterns result in the simplest code, as in the CRUD catalog microserv\", \"ice in eShopOnContainers. In cases where you want the simplest code possible, you might want to dire\", \"ctly use the DbContext class, as many developers do.\\n\\nHowever, implementing custom repositories prov\", \"ides several benefits when implementing more complex microservices or applications. The Unit of Work\", \" and Repository patterns are intended to encapsulate the infrastructure persistence layer so it is d\", \"ecoupled from the application and domainmodel layers. Implementing these patterns can facilitate the\", \" use of mock repositories simulating access to the database.\\n\\nIn Figure 7-18, you can see the differ\", \"ences between not using repositories (directly using the EF DbContext) versus using repositories, wh\", \"ich makes it easier to mock those repositories.\\n\\n*Figure 7-18. Using custom repositories versus a pl\", \"ain DbContext*\\n\\nFigure 7-18 shows that using a custom repository adds an abstraction layer that can \", \"be used to ease testing by mocking the repository. There are multiple alternatives when mocking. You\", \" could mock just repositories or you could mock a whole unit of work. Usually mocking just the repos\", \"itories is enough, and the complexity to abstract and mock a whole unit of work is usually not neede\", \"d.\\n\\nLater, when we focus on the application layer, you will see how Dependency Injection works in AS\", \"P.NET Core and how it is implemented when using repositories.\\n\\nIn short, custom repositories allow y\", \"ou to test code more easily with unit tests that are not impacted by the data tier state. If you run\", \" tests that also access the actual database through the Entity Framework, they are not unit tests bu\", \"t integration tests, which are a lot slower.\\n\\nIf you were using DbContext directly, you would have t\", \"o mock it or to run unit tests by using an inmemory SQL Server with predictable data for unit tests.\", \" But mocking the DbContext or controlling fake data requires more work than mocking at the repositor\", \"y level. Of course, you could always test the MVC controllers.\\n\\n### <span id=\\\"page-262-0\\\"></span>**E\", \"F DbContext and IUnitOfWork instance lifetime in your IoC container**\\n\\nThe DbContext object (exposed\", \" as an IUnitOfWork object) should be shared among multiple repositories within the same HTTP request\", \" scope. For example, this is true when the operation being executed must deal with multiple aggregat\", \"es, or simply because you are using multiple repository instances. It is also important to mention t\", \"hat the IUnitOfWork interface is part of your domain layer, not an EF Core type.\\n\\nIn order to do tha\", \"t, the instance of the DbContext object has to have its service lifetime set to ServiceLifetime.Scop\", \"ed. This is the default lifetime when registering a DbContext with builder.Services.AddDbContext in \", \"your IoC container from the *Program.cs* file in your ASP.NET Core Web API project. The following co\", \"de illustrates this.\\n\\n```\\n// Add framework services.\\nbuilder.Services.AddMvc(options =>\\n{\\n options.F\", \"ilters.Add(typeof(HttpGlobalExceptionFilter));\\n}).AddControllersAsServices();\\nbuilder.Services.AddEn\", \"tityFrameworkSqlServer()\\n .AddDbContext<OrderingContext>(options =>\\n {\\n options.UseSqlServer(Configu\", \"ration[\\\"ConnectionString\\\"],\\n sqlOptions =>\\nsqlOptions.MigrationsAssembly(typeof(Startup).GetTypeInfo\", \"().\\nAssembly.GetName().Name));\\n },\\n ServiceLifetime.Scoped // Note that Scoped is the default choice\", \"\\n // in AddDbContext. It is shown here only for\\n                         // pedagogic purposes.\\n );\\n\", \"```\\n\\nThe DbContext instantiation mode should not be configured as ServiceLifetime.Transient or Servi\", \"ceLifetime.Singleton.\\n\\n## <span id=\\\"page-263-0\\\"></span>**The repository instance lifetime in your Io\", \"C container**\\n\\nIn a similar way, repository's lifetime should usually be set as scoped (InstancePerL\", \"ifetimeScope in Autofac). It could also be transient (InstancePerDependency in Autofac), but your se\", \"rvice will be more efficient in regards to memory when using the scoped lifetime.\\n\\n```\\n// Registerin\", \"g a Repository in Autofac IoC container\\nbuilder.RegisterType<OrderRepository>()\\n .As<IOrderRepositor\", \"y>()\\n .InstancePerLifetimeScope();\\n```\\n\\nUsing the singleton lifetime for the repository could cause \", \"you serious concurrency problems when your DbContext is set to scoped (InstancePerLifetimeScope) lif\", \"etime (the default lifetimes for a DBContext). As long as your service lifetimes for your repositori\", \"es and your DbContext are both Scoped, you'll avoid these issues.\\n\\n### **Additional resources**\\n\\n- *\", \"*Implementing the Repository and Unit of Work Patterns in an ASP.NET MVC Application**\\n  - [https://\", \"www.asp.net/mvc/overview/older-versions/getting-started-with-ef-5-using-mvc-](https://www.asp.net/mv\", \"c/overview/older-versions/getting-started-with-ef-5-using-mvc-4/implementing-the-repository-and-unit\", \"-of-work-patterns-in-an-asp-net-mvc-application)[4/implementing-the-repository-and-unit-of-work-patt\", \"erns-in-an-asp-net-mvc-application](https://www.asp.net/mvc/overview/older-versions/getting-started-\", \"with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-applic\", \"ation)\\n- **Jonathan Allen. Implementation Strategies for the Repository Pattern with Entity Framewor\", \"k, Dapper, and Chain**\\n  - <https://www.infoq.com/articles/repository-implementation-strategies>\\n- *\", \"*Cesar de la Torre. Comparing ASP.NET Core IoC container service lifetimes with Autofac IoC containe\", \"r instance scopes**\\n  - [https://devblogs.microsoft.com/cesardelatorre/comparing-asp-net-core-ioc-se\", \"rvice-life](https://devblogs.microsoft.com/cesardelatorre/comparing-asp-net-core-ioc-service-life-ti\", \"mes-and-autofac-ioc-instance-scopes/)[times-and-autofac-ioc-instance-scopes/](https://devblogs.micro\", \"soft.com/cesardelatorre/comparing-asp-net-core-ioc-service-life-times-and-autofac-ioc-instance-scope\", \"s/)\\n\\n## <span id=\\\"page-264-0\\\"></span>**Table mapping**\\n\\nsaw how domain entities (for example, a prod\", \"uct or order domain) can be used to generate a related database schema. EF is strongly designed arou\", \"nd the concept of *conventions*. Conventions address questions like \\\"What will the name of a table b\", \"e?\\\" or \\\"What property is the primary key?\\\" Conventions are typically based on conventional names. Fo\", \"r example, it is typical for the primary key to be a property that ends with Id.\\n\\nTable mapping iden\", \"tifies the table data to be queried from and saved to the database. Previously you\\n\\nBy convention, e\", \"ach entity will be set up to map to a table with the same name as the DbSet<TEntity> property that e\", \"xposes the entity on the derived context. If no DbSet<TEntity> value is provided for the given entit\", \"y, the class name is used.\\n\\n## **Data Annotations versus Fluent API**\\n\\nThere are many additional EF \", \"Core conventions, and most of them can be changed by using either data annotations or Fluent API, im\", \"plemented within the OnModelCreating method.\\n\\nData annotations must be used on the entity model clas\", \"ses themselves, which is a more intrusive way from a DDD point of view. This is because you are cont\", \"aminating your model with data annotations related to the infrastructure database. On the other hand\", \", Fluent API is a convenient way to change most conventions and mappings within your data persistenc\", \"e infrastructure layer, so the entity model will be clean and decoupled from the persistence infrast\", \"ructure.\\n\\n### **Fluent API and the OnModelCreating method**\\n\\nAs mentioned, in order to change conven\", \"tions and mappings, you can use the OnModelCreating method in the DbContext class.\\n\\nThe ordering mic\", \"roservice in eShopOnContainers implements explicit mapping and configuration, when needed, as shown \", \"in the following code.\\n\\n```\\n// At OrderingContext.cs from eShopOnContainers\\nprotected override void \", \"OnModelCreating(ModelBuilder modelBuilder)\\n{\\n // ...\\n modelBuilder.ApplyConfiguration(new OrderEntit\", \"yTypeConfiguration());\\n // Other entities' configuration ...\\n}\\n// At OrderEntityTypeConfiguration.cs\", \" from eShopOnContainers\\nclass OrderEntityTypeConfiguration : IEntityTypeConfiguration<Order>\\n{\\n publ\", \"ic void Configure(EntityTypeBuilder<Order> orderConfiguration)\\n {\\n orderConfiguration.ToTable(\\\"order\", \"s\\\", OrderingContext.DEFAULT_SCHEMA);\\n orderConfiguration.HasKey(o => o.Id);\\n orderConfiguration.Igno\", \"re(b => b.DomainEvents);\\n orderConfiguration.Property(o => o.Id)\\n .UseHiLo(\\\"orderseq\\\", OrderingConte\", \"xt.DEFAULT_SCHEMA);\\n```\\n\\n```\\n //Address value object persisted as owned entity type supported since \", \"EF Core 2.0\\n orderConfiguration\\n .OwnsOne(o => o.Address, a =>\\n {\\n a.WithOwner();\\n });\\n orderConfigu\", \"ration\\n .Property<int?>(\\\"_buyerId\\\")\\n .UsePropertyAccessMode(PropertyAccessMode.Field)\\n .HasColumnNam\", \"e(\\\"BuyerId\\\")\\n .IsRequired(false);\\n orderConfiguration\\n .Property<DateTime>(\\\"_orderDate\\\")\\n .UseProper\", \"tyAccessMode(PropertyAccessMode.Field)\\n .HasColumnName(\\\"OrderDate\\\")\\n .IsRequired();\\n orderConfigurat\", \"ion\\n .Property<int>(\\\"_orderStatusId\\\")\\n .UsePropertyAccessMode(PropertyAccessMode.Field)\\n .HasColumnN\", \"ame(\\\"OrderStatusId\\\")\\n .IsRequired();\\n orderConfiguration\\n .Property<int?>(\\\"_paymentMethodId\\\")\\n .UseP\", \"ropertyAccessMode(PropertyAccessMode.Field)\\n .HasColumnName(\\\"PaymentMethodId\\\")\\n .IsRequired(false);\\n\", \" orderConfiguration.Property<string>(\\\"Description\\\").IsRequired(false);\\n var navigation =\\norderConfig\", \"uration.Metadata.FindNavigation(nameof(Order.OrderItems));\\n // DDD Patterns comment:\\n //Set as field\", \" (New since EF 1.1) to access the OrderItem collection property \\nthrough its field\\n navigation.SetPr\", \"opertyAccessMode(PropertyAccessMode.Field);\\n orderConfiguration.HasOne<PaymentMethod>()\\n .WithMany()\", \"\\n .HasForeignKey(\\\"_paymentMethodId\\\")\\n .IsRequired(false)\\n .OnDelete(DeleteBehavior.Restrict);\\n order\", \"Configuration.HasOne<Buyer>()\\n .WithMany()\\n .IsRequired(false)\\n .HasForeignKey(\\\"_buyerId\\\");\\n orderCo\", \"nfiguration.HasOne(o => o.OrderStatus)\\n .WithMany()\\n .HasForeignKey(\\\"_orderStatusId\\\");\\n }\\n}\\n```\\n\\nYou\", \" could set all the Fluent API mappings within the same OnModelCreating method, but it's advisable to\", \" partition that code and have multiple configuration classes, one per entity, as shown in the exampl\", \"e. Especially for large models, it is advisable to have separate configuration classes for configuri\", \"ng different entity types.\\n\\nThe code in the example shows a few explicit declarations and mapping. H\", \"owever, EF Core conventions do many of those mappings automatically, so the actual code you would ne\", \"ed in your case might be smaller.\\n\\n## **The Hi/Lo algorithm in EF Core**\\n\\nAn interesting aspect of c\", \"ode in the preceding example is that it uses the [Hi/Lo algorithm](https://vladmihalcea.com/the-hilo\", \"-algorithm/) as the key generation strategy.\\n\\nThe Hi/Lo algorithm is useful when you need unique key\", \"s before committing changes. As a summary, the Hi-Lo algorithm assigns unique identifiers to table r\", \"ows while not depending on storing the row in the database immediately. This lets you start using th\", \"e identifiers right away, as happens with regular sequential database IDs.\\n\\nThe Hi/Lo algorithm desc\", \"ribes a mechanism for getting a batch of unique IDs from a related database sequence. These IDs are \", \"safe to use because the database guarantees the uniqueness, so there will be no collisions between u\", \"sers. This algorithm is interesting for these reasons:\\n\\n- It does not break the Unit of Work pattern\", \".\\n- It gets sequence IDs in batches, to minimize round trips to the database.\\n- It generates a human\", \" readable identifier, unlike techniques that use GUIDs.\\n\\nEF Core supports [HiLo](https://stackoverfl\", \"ow.com/questions/282099/whats-the-hi-lo-algorithm) with the UseHiLo method, as shown in the precedin\", \"g example.\\n\\n### **Map fields instead of properties**\\n\\nWith this feature, available since EF Core 1.1\", \", you can directly map columns to fields. It is possible to not use properties in the entity class, \", \"and just to map columns from a table to fields. A common use for that would be private fields for an\", \"y internal state that do not need to be accessed from outside the entity.\\n\\nYou can do this with sing\", \"le fields or also with collections, like a List<> field. This point was mentioned earlier when we di\", \"scussed modeling the domain model classes, but here you can see how that mapping is performed with t\", \"he PropertyAccessMode.Field configuration highlighted in the previous code.\\n\\n### **Use shadow proper\", \"ties in EF Core, hidden at the infrastructure level**\\n\\nShadow properties in EF Core are properties t\", \"hat do not exist in your entity class model. The values and states of these properties are maintaine\", \"d purely in the [ChangeTracker](https://docs.microsoft.com/ef/core/api/microsoft.entityframeworkcore\", \".changetracking.changetracker) class at the infrastructure level.\\n\\n## <span id=\\\"page-267-0\\\"></span>*\", \"*Implement the Query Specification pattern**\\n\\nAs introduced earlier in the design section, the Query\", \" Specification pattern is a Domain-Driven Design pattern designed as the place where you can put the\", \" definition of a query with optional sorting and paging logic.\\n\\nThe Query Specification pattern defi\", \"nes a query in an object. For example, in order to encapsulate a paged query that searches for some \", \"products you can create a PagedProduct specification that takes the necessary input parameters (page\", \"Number, pageSize, filter, etc.). Then, within any Repository method (usually a List() overload) it w\", \"ould accept an IQuerySpecification and run the expected query based on that specification.\\n\\nAn examp\", \"le of a generic Specification interface is the following code, which is similar to code used in the \", \"[eShopOnWeb](https://github.com/dotnet-architecture/eShopOnWeb) reference application.\\n\\n```\\n// GENER\", \"IC SPECIFICATION INTERFACE\\n// https://github.com/dotnet-architecture/eShopOnWeb\\npublic interface ISp\", \"ecification<T>\\n{\\n Expression<Func<T, bool>> Criteria { get; }\\n List<Expression<Func<T, object>>> Inc\", \"ludes { get; }\\n List<string> IncludeStrings { get; }\\n}\\n```\\n\\nThen, the implementation of a generic sp\", \"ecification base class is the following.\\n\\n```\\n// GENERIC SPECIFICATION IMPLEMENTATION (BASE CLASS)\\n/\", \"/ https://github.com/dotnet-architecture/eShopOnWeb\\npublic abstract class BaseSpecification<T> : ISp\", \"ecification<T>\\n{\\n public BaseSpecification(Expression<Func<T, bool>> criteria)\\n {\\n Criteria = criter\", \"ia;\\n }\\n public Expression<Func<T, bool>> Criteria { get; }\\n public List<Expression<Func<T, object>>>\", \" Includes { get; } =\\n new List<Expression<Func<T, object>>>();\\n public List<string> IncludeStrings {\", \" get; } = new List<string>();\\n protected virtual void AddInclude(Expression<Func<T, object>> include\", \"Expression)\\n {\\n Includes.Add(includeExpression);\\n }\\n // string-based includes allow for including ch\", \"ildren of children\\n // e.g. Basket.Items.Product\\n protected virtual void AddInclude(string includeSt\", \"ring)\\n {\\n IncludeStrings.Add(includeString);\\n }\\n}\\n```\\n\\nThe following specification loads a single ba\", \"sket entity given either the basket's ID or the ID of the buyer to whom the basket belongs. It will \", \"[eagerly load](https://docs.microsoft.com/ef/core/querying/related-data) the basket's Items collecti\", \"on.\\n\\n```\\n// SAMPLE QUERY SPECIFICATION IMPLEMENTATION\\npublic class BasketWithItemsSpecification : Ba\", \"seSpecification<Basket>\\n{\\n public BasketWithItemsSpecification(int basketId)\\n : base(b => b.Id == ba\", \"sketId)\\n {\\n AddInclude(b => b.Items);\\n }\\n public BasketWithItemsSpecification(string buyerId)\\n : bas\", \"e(b => b.BuyerId == buyerId)\\n {\\n AddInclude(b => b.Items);\\n }\\n}\\n```\\n\\nAnd finally, you can see below \", \"how a generic EF Repository can use such a specification to filter and eager-load data related to a \", \"given entity type T.\\n\\n```\\n// GENERIC EF REPOSITORY WITH SPECIFICATION\\n// https://github.com/dotnet-a\", \"rchitecture/eShopOnWeb\\npublic IEnumerable<T> List(ISpecification<T> spec)\\n{\\n // fetch a Queryable th\", \"at includes all expression-based includes\\n var queryableResultWithIncludes = spec.Includes\\n .Aggrega\", \"te(_dbContext.Set<T>().AsQueryable(),\\n (current, include) => current.Include(include));\\n // modify t\", \"he IQueryable to include any string-based include statements\\n var secondaryResult = spec.IncludeStri\", \"ngs\\n .Aggregate(queryableResultWithIncludes,\\n (current, include) => current.Include(include));\\n // r\", \"eturn the result of the query using the specification's criteria expression\\n return secondaryResult\\n\", \" .Where(spec.Criteria)\\n .AsEnumerable();\\n}\\n```\\n\\nIn addition to encapsulating filtering logic, the sp\", \"ecification can specify the shape of the data to be returned, including which properties to populate\", \".\\n\\nAlthough we don't recommend returning IQueryable from a repository, it's perfectly fine to use th\", \"em within the repository to build up a set of results. You can see this approach used in the List me\", \"thod above, which uses intermediate IQueryable expressions to build up the query's list of includes \", \"before executing the query with the specification's criteria on the last line.\\n\\nLearn [how the speci\", \"fication pattern is applied in the eShopOnWeb sample.](https://github.com/dotnet-architecture/eShopO\", \"nWeb/wiki/Patterns#specification)\\n\\n## **Additional resources**\\n\\n\\u2022 **Backing Fields**\\n\\n- **Table Mapp\", \"ing** [https://learn.microsoft.com/ef/core/modeling/relational/tables](https://docs.microsoft.com/ef\", \"/core/modeling/relational/tables)\\n- <https://www.talkingdotnet.com/use-hilo-to-generate-keys-with-en\", \"tity-framework-core/>\\n- [https://learn.microsoft.com/ef/core/modeling/backing-field](https://docs.mi\", \"crosoft.com/ef/core/modeling/backing-field)\\n\\n\\u2022 **Use HiLo to generate keys with Entity Framework Cor\", \"e**\\n\\n<https://ardalis.com/encapsulated-collections-in-entity-framework-core>\\n\\n\\u2022 **Steve Smith. Encap\", \"sulated Collections in Entity Framework Core**\\n\\n- **Shadow Properties** [https://learn.microsoft.com\", \"/ef/core/modeling/shadow-properties](https://docs.microsoft.com/ef/core/modeling/shadow-properties)\\n\", \"- **The Specification pattern** <https://deviq.com/specification-pattern/>\\n\\n**Ardalis.Specification \", \"NuGet Package** Used by eShopOnWeb. <https://www.nuget.org/packages/Ardalis.Specification>\\n\\n## <span\", \" id=\\\"page-269-0\\\"></span>Use NoSQL databases as a persistence infrastructure\\n\\nWhen you use NoSQL data\", \"bases for your infrastructure data tier, you typically do not use an ORM like Entity Framework Core.\", \" Instead you use the API provided by the NoSQL engine, such as Azure Cosmos DB, MongoDB, Cassandra, \", \"RavenDB, CouchDB, or Azure Storage Tables.\\n\\nHowever, when you use a NoSQL database, especially a doc\", \"ument-oriented database like Azure Cosmos DB, CouchDB, or RavenDB, the way you design your model wit\", \"h DDD aggregates is partially similar to how you can do it in EF Core, in regards to the identificat\", \"ion of aggregate roots, child entity classes, and value object classes. But, ultimately, the databas\", \"e selection will impact in your design.\\n\\nWhen you use a document-oriented database, you implement an\", \" aggregate as a single document, serialized in JSON or another format. However, the use of the datab\", \"ase is transparent from a domain model code point of view. When using a NoSQL database, you still ar\", \"e using entity classes and aggregate root classes, but with more flexibility than when using EF Core\", \" because the persistence is not relational.\\n\\nThe difference is in how you persist that model. If you\", \" implemented your domain model based on POCO entity classes, agnostic to the infrastructure persiste\", \"nce, it might look like you could move to a different persistence infrastructure, even from relation\", \"al to NoSQL. However, that should not be your goal. There are always constraints and trade-offs in t\", \"he different database technologies, so you will not be able to have the same model for relational or\", \" NoSQL databases. Changing persistence models is not a trivial task, because transactions and persis\", \"tence operations will be very different.\\n\\nFor example, in a document-oriented database, it is okay f\", \"or an aggregate root to have multiple child collection properties. In a relational database, queryin\", \"g multiple child collection properties is not\\n\\neasily optimized, because you get a UNION ALL SQL sta\", \"tement back from EF. Having the same domain model for relational databases or NoSQL databases is not\", \" simple, and you should not try to do it. You really have to design your model with an understanding\", \" of how the data is going to be used in each particular database. A benefit when using NoSQL databas\", \"es is that the entities are more denormalized, so you do not set a\\n\\ntable mapping. Your domain model\", \" can be more flexible than when using a relational database.\\n\\nWhen you design your domain model base\", \"d on aggregates, moving to NoSQL and documentoriented databases might be even easier than using a re\", \"lational database, because the aggregates you design are similar to serialized documents in a docume\", \"nt-oriented database. Then you can include in those \\\"bags\\\" all the information you might need for th\", \"at aggregate.\\n\\nFor instance, the following JSON code is a sample implementation of an order aggregat\", \"e when using a document-oriented database. It is similar to the order aggregate we implemented in th\", \"e eShopOnContainers sample, but without using EF Core underneath.\\n\\n```\\n{\\n \\\"id\\\": \\\"2024001\\\",\\n \\\"orderDa\", \"te\\\": \\\"2/25/2024\\\",\\n \\\"buyerId\\\": \\\"1234567\\\",\\n \\\"address\\\": [\\n {\\n \\\"street\\\": \\\"100 One Microsoft Way\\\",\\n \\\"city\", \"\\\": \\\"Redmond\\\",\\n \\\"state\\\": \\\"WA\\\",\\n \\\"zip\\\": \\\"98052\\\",\\n \\\"country\\\": \\\"U.S.\\\"\\n }\\n ],\\n \\\"orderItems\\\": [\\n {\\\"id\\\": 20\", \"240011, \\\"productId\\\": \\\"123456\\\", \\\"productName\\\": \\\".NET T-Shirt\\\",\\n \\\"unitPrice\\\": 25, \\\"units\\\": 2, \\\"discoun\", \"t\\\": 0},\\n {\\\"id\\\": 20240012, \\\"productId\\\": \\\"123457\\\", \\\"productName\\\": \\\".NET Mug\\\",\\n \\\"unitPrice\\\": 15, \\\"units\", \"\\\": 1, \\\"discount\\\": 0}\\n ]\\n}\\n```\\n\\n### <span id=\\\"page-270-0\\\"></span>**Introduction to Azure Cosmos DB an\", \"d the native Cosmos DB API**\\n\\n[Azure Cosmos DB](https://docs.microsoft.com/azure/cosmos-db/introduct\", \"ion) is Microsoft's globally distributed database service for mission-critical applications. Azure C\", \"osmos DB provides [turn-key global distribution,](https://docs.microsoft.com/azure/cosmos-db/distrib\", \"ute-data-globally) [elastic scaling of throughput and storage](https://docs.microsoft.com/azure/cosm\", \"os-db/partition-data) worldwide, single-digit millisecond latencies at the 99th percentile, [five we\", \"ll-defined consistency](https://docs.microsoft.com/azure/cosmos-db/consistency-levels)  [levels,](ht\", \"tps://docs.microsoft.com/azure/cosmos-db/consistency-levels) and guaranteed high availability, all b\", \"acked by [industry-leading SLAs.](https://azure.microsoft.com/support/legal/sla/cosmos-db/) Azure Co\", \"smos DB [automatically indexes data](https://www.vldb.org/pvldb/vol8/p1668-shukla.pdf) without requi\", \"ring you to deal with schema and index management. It is multi-model and supports document, key-valu\", \"e, graph, and columnar data models.\\n\\n![](_page_271_Figure_0.jpeg)\\n\\n*Figure 7-19. Azure Cosmos DB glo\", \"bal distribution*\\n\\nWhen you use a C# model to implement the aggregate to be used by the Azure Cosmos\", \" DB API, the aggregate can be similar to the C# POCO classes used with EF Core. The difference is in\", \" the way to use them from the application and infrastructure layers, as in the following code:\\n\\n```\\n\", \"// C# EXAMPLE OF AN ORDER AGGREGATE BEING PERSISTED WITH AZURE COSMOS DB API\\n// *** Domain Model Cod\", \"e ***\\n// Aggregate: Create an Order object with its child entities and/or value objects.\\n// Then, us\", \"e AggregateRoot's methods to add the nested objects so invariants and\\n// logic is consistent across \", \"the nested properties (value objects and entities).\\nOrder orderAggregate = new Order\\n{\\n Id = \\\"202400\", \"1\\\",\\n OrderDate = new DateTime(2005, 7, 1),\\n BuyerId = \\\"1234567\\\",\\n PurchaseOrderNumber = \\\"PO180091864\", \"70\\\"\\n}\\nAddress address = new Address\\n{\\n Street = \\\"100 One Microsoft Way\\\",\\n City = \\\"Redmond\\\",\\n State =\", \" \\\"WA\\\",\\n Zip = \\\"98052\\\",\\n Country = \\\"U.S.\\\"\\n}\\norderAggregate.UpdateAddress(address);\\nOrderItem orderIte\", \"m1 = new OrderItem\\n{\\n```\\n\\n```\\n Id = 20240011,\\n ProductId = \\\"123456\\\",\\n ProductName = \\\".NET T-Shirt\\\",\\n\", \" UnitPrice = 25,\\n Units = 2,\\n Discount = 0;\\n};\\n//Using methods with domain logic within the entity. \", \"No anemic-domain model\\norderAggregate.AddOrderItem(orderItem1);\\n// *** End of Domain Model Code ***\\n\", \"// *** Infrastructure Code using Cosmos DB Client API ***\\nUri collectionUri = UriFactory.CreateDocum\", \"entCollectionUri(databaseName,\\n collectionName);\\nawait client.CreateDocumentAsync(collectionUri, ord\", \"erAggregate);\\n// As your app evolves, let's say your object has a new schema. You can insert\\n// Orde\", \"rV2 objects without any changes to the database tier.\\nOrder2 newOrder = GetOrderV2Sample(\\\"IdForSales\", \"Order2\\\");\\nawait client.CreateDocumentAsync(collectionUri, newOrder);\\n```\\n\\nYou can see that the way y\", \"ou work with your domain model can be similar to the way you use it in your domain model layer when \", \"the infrastructure is EF. You still use the same aggregate root methods to ensure consistency, invar\", \"iants, and validations within the aggregate.\\n\\nHowever, when you persist your model into the NoSQL da\", \"tabase, the code and API change dramatically compared to EF Core code or any other code related to r\", \"elational databases.\\n\\n## <span id=\\\"page-272-0\\\"></span>**Implement .NET code targeting MongoDB and Az\", \"ure Cosmos DB**\\n\\n### **Use Azure Cosmos DB from .NET containers**\\n\\nYou can access Azure Cosmos DB da\", \"tabases from .NET code running in containers, like from any other .NET application. For instance, th\", \"e Locations.API and Marketing.API microservices in eShopOnContainers are implemented so they can con\", \"sume Azure Cosmos DB databases.\\n\\nHowever, there's a limitation in Azure Cosmos DB from a Docker deve\", \"lopment environment point of view. Even though there's an on-premises [Azure Cosmos DB Emulator](htt\", \"ps://docs.microsoft.com/azure/cosmos-db/local-emulator) that can run in a local development machine,\", \" it only supports Windows. Linux and macOS aren't supported.\\n\\nThere's also the possibility to run th\", \"is emulator on Docker, but just on Windows Containers, not with Linux Containers. That's an initial \", \"handicap for the development environment if your application is deployed as Linux containers, since,\", \" currently, you can't deploy Linux and Windows Containers on Docker for Windows at the same time. Ei\", \"ther all containers being deployed have to be for Linux or for Windows.\\n\\nThe ideal and more straight\", \"forward deployment for a dev/test solution is to be able to deploy your database systems as containe\", \"rs along with your custom containers so your dev/test environments are always consistent.\\n\\n## **Use \", \"MongoDB API for local dev/test Linux/Windows containers plus Azure Cosmos DB**\\n\\nCosmos DB databases \", \"support MongoDB API for .NET as well as the native MongoDB wire protocol. This means that by using e\", \"xisting drivers, your application written for MongoDB can now communicate with Cosmos DB and use Cos\", \"mos DB databases instead of MongoDB databases, as shown in Figure 7-20.\\n\\n![](_page_273_Picture_2.jpe\", \"g)\\n\\n*Figure 7-20. Using MongoDB API and protocol to access Azure Cosmos DB*\\n\\nThis is a very convenie\", \"nt approach for proof of concepts in Docker environments with Linux containers because the [MongoDB \", \"Docker image](https://hub.docker.com/r/_/mongo/) is a multi-arch image that supports Docker Linux co\", \"ntainers and Docker Windows containers.\\n\\nAs shown in the following image, by using the MongoDB API, \", \"eShopOnContainers supports MongoDB Linux and Windows containers for the local development environmen\", \"t but then, you can move to a scalable, PaaS cloud solution as Azure Cosmos DB by simply [changing t\", \"he MongoDB connection](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)  [string \", \"to point to Azure Cosmos DB.](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)\\n\\n!\", \"[](_page_274_Picture_0.jpeg)\\n\\n*Figure 7-21. eShopOnContainers using MongoDB containers for dev-env o\", \"r Azure Cosmos DB for production*\\n\\nThe production Azure Cosmos DB would be running in Azure's cloud \", \"as a PaaS and scalable service.\\n\\nYour custom .NET containers can run on a local development Docker h\", \"ost (that is using Docker for Windows in a Windows 10 machine) or be deployed into a production envi\", \"ronment, like Kubernetes in Azure AKS or Azure Service Fabric. In this second environment, you would\", \" deploy only the .NET custom containers but not the MongoDB container since you'd be using Azure Cos\", \"mos DB in the cloud for handling the data in production.\\n\\nA clear benefit of using the MongoDB API i\", \"s that your solution could run in both database engines, MongoDB or Azure Cosmos DB, so migrations t\", \"o different environments should be easy. However, sometimes it is worthwhile to use a native API (th\", \"at is the native Cosmos DB API) in order to take full advantage of the capabilities of a specific da\", \"tabase engine.\\n\\nFor further comparison between simply using MongoDB versus Cosmos DB in the cloud, s\", \"ee the [Benefits of using Azure Cosmos DB in this page.](https://docs.microsoft.com/azure/cosmos-db/\", \"mongodb-introduction)\\n\\n### **Analyze your approach for production applications: MongoDB API vs. Cosm\", \"os DB API**\\n\\nIn eShopOnContainers, we're using MongoDB API because our priority was fundamentally to\", \" have a consistent dev/test environment using a NoSQL database that could also work with Azure Cosmo\", \"s DB. However, if you are planning to use MongoDB API to access Azure Cosmos DB in Azure for product\", \"ion applications, you should analyze the differences in capabilities and performance when using Mong\", \"oDB API to access Azure Cosmos DB databases compared to using the native Azure Cosmos DB API. If it \", \"is similar you can use MongoDB API and you get the benefit of supporting two NoSQL database engines \", \"at the same time.\\n\\nYou could also use MongoDB clusters as the production database in Azure's cloud, \", \"too, with [MongoDB Azure Service.](https://www.mongodb.com/scale/mongodb-azure-service) But that is \", \"not a PaaS service provided by Microsoft. In this case, Azure is just hosting that solution coming f\", \"rom MongoDB.\\n\\nCosmos DB, as we did in eShopOnContainers because it was a convenient choice for Linux\", \" containers. The decision should be based on the specific needs and tests you need to do for your pr\", \"oduction application.\\n\\nBasically, this is just a disclaimer stating that you shouldn't always use Mo\", \"ngoDB API against Azure\\n\\n## **The code: Use MongoDB API in .NET applications**\\n\\nMongoDB API for .NET\", \" is based on NuGet packages that you need to add to your projects, like in the Locations.API project\", \" shown in the following figure.\\n\\n![](_page_276_Figure_0.jpeg)\\n\\n*Figure 7-22. MongoDB API NuGet packa\", \"ges references in a .NET project*\\n\\nLet's investigate the code in the following sections.\\n\\n### **A Mo\", \"del used by MongoDB API**\\n\\nFirst, you need to define a model that will hold the data coming from the\", \" database in your application's memory space. Here's an example of the model used for Locations at e\", \"ShopOnContainers.\\n\\n```\\nusing MongoDB.Bson;\\nusing MongoDB.Bson.Serialization.Attributes;\\nusing MongoD\", \"B.Driver.GeoJsonObjectModel;\\nusing System.Collections.Generic;\\npublic class Locations\\n{\\n```\\n\\n```\\n [B\", \"sonId]\\n [BsonRepresentation(BsonType.ObjectId)]\\n public string Id { get; set; }\\n public int Location\", \"Id { get; set; }\\n public string Code { get; set; }\\n [BsonRepresentation(BsonType.ObjectId)]\\n public \", \"string Parent_Id { get; set; }\\n public string Description { get; set; }\\n public double Latitude { ge\", \"t; set; }\\n public double Longitude { get; set; }\\n public GeoJsonPoint<GeoJson2DGeographicCoordinates\", \"> Location\\n { get; private set; }\\n public GeoJsonPolygon<GeoJson2DGeographicCoordinates> Polygon\\n { \", \"get; private set; }\\n public void SetLocation(double lon, double lat) => SetPosition(lon, lat);\\n publ\", \"ic void SetArea(List<GeoJson2DGeographicCoordinates> coordinatesList)\\n => SetPolygon(coordinatesList\", \");\\n private void SetPosition(double lon, double lat)\\n {\\n Latitude = lat;\\n Longitude = lon;\\n Location\", \" = new GeoJsonPoint<GeoJson2DGeographicCoordinates>(\\n new GeoJson2DGeographicCoordinates(lon, lat));\", \"\\n }\\n private void SetPolygon(List<GeoJson2DGeographicCoordinates> coordinatesList)\\n {\\n Polygon = new\", \" GeoJsonPolygon<GeoJson2DGeographicCoordinates>(\\n new GeoJsonPolygonCoordinates<GeoJson2DGeographicC\", \"oordinates>(\\n new GeoJsonLinearRingCoordinates<GeoJson2DGeographicCoordinates>(\\n coordinatesList)));\", \"\\n }\\n}\\n```\\n\\nYou can see there are a few attributes and types coming from the MongoDB NuGet packages.\\n\", \"\\nNoSQL databases are usually very well suited for working with non-relational hierarchical data. In \", \"this example, we are using MongoDB types especially made for geo-locations, like GeoJson2DGeographic\", \"Coordinates.\\n\\n### **Retrieve the database and the collection**\\n\\nIn eShopOnContainers, we have create\", \"d a custom database context where we implement the code to retrieve the database and the MongoCollec\", \"tions, as in the following code.\\n\\n```\\npublic class LocationsContext\\n{\\n private readonly IMongoDataba\", \"se _database = null;\\n public LocationsContext(IOptions<LocationSettings> settings)\\n {\\n var client = \", \"new MongoClient(settings.Value.ConnectionString);\\n if (client != null)\\n _database = client.GetDataba\", \"se(settings.Value.Database);\\n }\\n public IMongoCollection<Locations> Locations\\n {\\n```\\n\\n```\\n get\\n {\\n r\", \"eturn _database.GetCollection<Locations>(\\\"Locations\\\");\\n }\\n }\\n}\\n```\\n\\n## **Retrieve the data**\\n\\ncode t\", \"o the following when querying through the MongoDB API. Note that the \\\\_context object is an instance\", \" of the previous LocationsContext class.\\n\\nIn C# code, like Web API controllers or custom Repositorie\", \"s implementation, you can write similar\\n\\n```\\npublic async Task<Locations> GetAsync(int locationId)\\n{\", \"\\n var filter = Builders<Locations>.Filter.Eq(\\\"LocationId\\\", locationId);\\n return await _context.Locat\", \"ions\\n .Find(filter)\\n .FirstOrDefaultAsync();\\n}\\n```\\n\\n### **Use an env-var in the docker-compose.overr\", \"ide.yml file for the MongoDB connection string**\\n\\nWhen creating a MongoClient object, it needs a fun\", \"damental parameter which is precisely the ConnectionString parameter pointing to the right database.\", \" In the case of eShopOnContainers, the connection string can point to a local MongoDB Docker contain\", \"er or to a \\\"production\\\" Azure Cosmos DB database. That connection string comes from the environment \", \"variables defined in the dockercompose.override.yml files used when deploying with docker-compose or\", \" Visual Studio, as in the following yml code.\\n\\n```\\n# docker-compose.override.yml\\nversion: '3.4'\\nserv\", \"ices:\\n # Other services\\n locations-api:\\n environment:\\n # Other settings\\n - ConnectionString=${ESHOP_\", \"AZURE_COSMOSDB:-mongodb://nosqldata}\\n```\\n\\nThe ConnectionString environment variable is resolved this\", \" way: If the ESHOP\\\\_AZURE\\\\_COSMOSDB global variable is defined in the .env file with the Azure Cosmo\", \"s DB connection string, it will use it to access the Azure Cosmos DB database in the cloud. If it's \", \"not defined, it will take the mongodb://nosqldata value and use the development MongoDB container.\\n\\n\", \"The following code shows the .env file with the Azure Cosmos DB connection string global environment\", \" variable, as implemented in eShopOnContainers:\\n\\n```\\n# .env file, in eShopOnContainers root folder\\n#\", \" Other Docker environment variables\\nESHOP_EXTERNAL_DNS_NAME_OR_IP=host.docker.internal\\nESHOP_PROD_EX\", \"TERNAL_DNS_NAME_OR_IP=<YourDockerHostIP>\\n#ESHOP_AZURE_COSMOSDB=<YourAzureCosmosDBConnData>\\n```\\n\\n```\\n\", \"#Other environment variables for additional Azure infrastructure assets\\n#ESHOP_AZURE_REDIS_BASKET_DB\", \"=<YourAzureRedisBasketInfo>\\n#ESHOP_AZURE_STORAGE_CATALOG_URL=<YourAzureStorage_Catalog_BLOB_URL>\\n#ES\", \"HOP_AZURE_SERVICE_BUS=<YourAzureServiceBusInfo>\\n```\\n\\nUncomment the ESHOP\\\\_AZURE\\\\_COSMOSDB line and u\", \"pdate it with your Azure Cosmos DB connection string obtained from the Azure portal as explained in \", \"[Connect a MongoDB application to](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-accoun\", \"t)  [Azure Cosmos DB.](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)\\n\\nIf the E\", \"SHOP\\\\_AZURE\\\\_COSMOSDB global variable is empty, meaning it's commented out in the .env file, then th\", \"e container uses a default MongoDB connection string. This connection string points to the local Mon\", \"goDB container deployed in eShopOnContainers that is named nosqldata and was defined at the docker-c\", \"ompose file, as shown in the following .yml code:\\n\\n```\\n# docker-compose.yml\\nversion: '3.4'\\nservices:\", \"\\n # ...Other services...\\n nosqldata:\\n image: mongo\\n```\\n\\n### **Additional resources**\\n\\n- **Modeling d\", \"ocument data for NoSQL databases** [https://learn.microsoft.com/azure/cosmos-db/modeling-data](https\", \"://docs.microsoft.com/azure/cosmos-db/modeling-data)\\n- **Vaughn Vernon. The Ideal Domain-Driven Desi\", \"gn Aggregate Store?** <https://kalele.io/blog-posts/the-ideal-domain-driven-design-aggregate-store/>\", \"\\n- **Introduction to Azure Cosmos DB: API for MongoDB** [https://learn.microsoft.com/azure/cosmos-db\", \"/mongodb-introduction](https://docs.microsoft.com/azure/cosmos-db/mongodb-introduction)\\n- **Azure Co\", \"smos DB: Build a MongoDB API web app with .NET and the Azure portal** [https://learn.microsoft.com/a\", \"zure/cosmos-db/create-mongodb-dotnet](https://docs.microsoft.com/azure/cosmos-db/create-mongodb-dotn\", \"et)\\n- **Use the Azure Cosmos DB Emulator for local development and testing** [https://learn.microsof\", \"t.com/azure/cosmos-db/local-emulator](https://docs.microsoft.com/azure/cosmos-db/local-emulator)\\n- *\", \"*Connect a MongoDB application to Azure Cosmos DB** [https://learn.microsoft.com/azure/cosmos-db/con\", \"nect-mongodb-account](https://docs.microsoft.com/azure/cosmos-db/connect-mongodb-account)\\n- **The Co\", \"smos DB Emulator Docker image (Windows Container)** <https://hub.docker.com/r/microsoft/azure-cosmos\", \"db-emulator/>\\n- **The MongoDB Docker image (Linux and Windows Container)** [https://hub.docker.com/\\\\\", \"\\\\_/mongo/](https://hub.docker.com/_/mongo/)\\n- **Use MongoChef (Studio 3T) with an Azure Cosmos DB: A\", \"PI for MongoDB account** [https://learn.microsoft.com/azure/cosmos-db/mongodb-mongochef](https://doc\", \"s.microsoft.com/azure/cosmos-db/mongodb-mongochef)\\n\\n## <span id=\\\"page-280-0\\\"></span>Design the micro\", \"service application layer and Web API\\n\\n## <span id=\\\"page-280-1\\\"></span>**Use SOLID principles and De\", \"pendency Injection**\\n\\nSOLID principles are critical techniques to be used in any modern and mission-\", \"critical application, such as developing a microservice with DDD patterns. SOLID is an acronym that \", \"groups five fundamental principles:\\n\\n- Single Responsibility principle\\n- Open/closed principle\\n- Lis\", \"kov substitution principle\\n- Interface Segregation principle\\n- Dependency Inversion principle\\n\\nSOLID\", \" is more about how you design your application or microservice internal layers and about decoupling \", \"dependencies between them. It is not related to the domain, but to the application's technical desig\", \"n. The final principle, the Dependency Inversion principle, allows you to decouple the infrastructur\", \"e layer from the rest of the layers, which allows a better decoupled implementation of the DDD layer\", \"s.\\n\\nDependency Injection (DI) is one way to implement the Dependency Inversion principle. It is a te\", \"chnique for achieving loose coupling between objects and their dependencies. Rather than directly in\", \"stantiating collaborators, or using static references (that is, using new\\u2026), the objects that a clas\", \"s needs in order to perform its actions are provided to (or \\\"injected into\\\") the class. Most often, \", \"classes will declare their dependencies via their constructor, allowing them to follow the Explicit \", \"Dependencies principle. Dependency Injection is usually based on specific Inversion of Control (IoC)\", \" containers. ASP.NET Core provides a simple built-in IoC container, but you can also use your favori\", \"te IoC container, like Autofac or Ninject.\\n\\nBy following the SOLID principles, your classes will ten\", \"d naturally to be small, well-factored, and easily tested. But how can you know if too many dependen\", \"cies are being injected into your classes? If you use DI through the constructor, it will be easy to\", \" detect that by just looking at the number of parameters for your constructor. If there are too many\", \" dependencies, this is generally a sign (a [code](https://deviq.com/code-smells/)  [smell\\\\)](https:/\", \"/deviq.com/code-smells/) that your class is trying to do too much, and is probably violating the Sin\", \"gle Responsibility principle.\\n\\nIt would take another guide to cover SOLID in detail. Therefore, this\", \" guide requires you to have only a minimum knowledge of these topics.\\n\\n### **Additional resources**\\n\", \"\\n\\u2022 **SOLID: Fundamental OOP Principles** <https://deviq.com/solid/>\\n\\n- **Inversion of Control Contai\", \"ners and the Dependency Injection pattern** <https://martinfowler.com/articles/injection.html>\\n- **S\", \"teve Smith. New is Glue** <https://ardalis.com/new-is-glue>\\n\\n## <span id=\\\"page-281-0\\\"></span>Impleme\", \"nt the microservice application layer using the Web API\\n\\n## <span id=\\\"page-281-1\\\"></span>**Use Depen\", \"dency Injection to inject infrastructure objects into your application layer**\\n\\nAs mentioned previou\", \"sly, the application layer can be implemented as part of the artifact (assembly) you are building, s\", \"uch as within a Web API project or an MVC web app project. In the case of a microservice built with \", \"ASP.NET Core, the application layer will usually be your Web API library. If you want to separate wh\", \"at is coming from ASP.NET Core (its infrastructure plus your controllers) from your custom applicati\", \"on layer code, you could also place your application layer in a separate class library, but that is \", \"optional.\\n\\nFor instance, the application layer code of the ordering microservice is directly impleme\", \"nted as part of the **Ordering.API** project (an ASP.NET Core Web API project), as shown in Figure 7\", \"-23.\\n\\n![](_page_281_Picture_6.jpeg)\\n\\n*Figure 7-23. The application layer in the Ordering.API ASP.NET\", \" Core Web API project*\\n\\nASP.NET Core includes a simple [built-in IoC container](https://docs.microso\", \"ft.com/aspnet/core/fundamentals/dependency-injection) (represented by the IServiceProvider interface\", \") that supports constructor injection by default, and ASP.NET makes certain services available throu\", \"gh DI. ASP.NET Core uses the term *service* for any of the types you register that will be injected \", \"through DI. You configure the built-in container's services in your application's *Program.cs* file.\", \" Your\\n\\ndependencies are implemented in the services that a type needs and that you register in the I\", \"oC container.\\n\\nTypically, you want to inject dependencies that implement infrastructure objects. A t\", \"ypical dependency to inject is a repository. But you could inject any other infrastructure dependenc\", \"y that you may have. For simpler implementations, you could directly inject your Unit of Work patter\", \"n object (the EF DbContext object), because the DBContext is also the implementation of your infrast\", \"ructure persistence objects.\\n\\nIn the following example, you can see how .NET is injecting the requir\", \"ed repository objects through the constructor. The class is a command handler, which will get covere\", \"d in the next section.\\n\\n```\\npublic class CreateOrderCommandHandler\\n : IRequestHandler<CreateOrderCom\", \"mand, bool>\\n{\\n private readonly IOrderRepository _orderRepository;\\n private readonly IIdentityServic\", \"e _identityService;\\n private readonly IMediator _mediator;\\n private readonly IOrderingIntegrationEve\", \"ntService _orderingIntegrationEventService;\\n private readonly ILogger<CreateOrderCommandHandler> _lo\", \"gger;\\n // Using DI to inject infrastructure persistence Repositories\\n public CreateOrderCommandHandl\", \"er(IMediator mediator,\\n IOrderingIntegrationEventService orderingIntegrationEventService,\\n IOrderRep\", \"ository orderRepository,\\n IIdentityService identityService,\\n ILogger<CreateOrderCommandHandler> logg\", \"er)\\n {\\n _orderRepository = orderRepository ?? throw new\\nArgumentNullException(nameof(orderRepository\", \"));\\n _identityService = identityService ?? throw new\\nArgumentNullException(nameof(identityService));\", \"\\n _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator));\\n _orderingIntegrationEv\", \"entService = orderingIntegrationEventService ?? throw new\\nArgumentNullException(nameof(orderingInteg\", \"rationEventService));\\n _logger = logger ?? throw new ArgumentNullException(nameof(logger));\\n }\\n publ\", \"ic async Task<bool> Handle(CreateOrderCommand message, CancellationToken \\ncancellationToken)\\n {\\n // \", \"Add Integration event to clean the basket\\n var orderStartedIntegrationEvent = new\\nOrderStartedIntegr\", \"ationEvent(message.UserId);\\n await \\n_orderingIntegrationEventService.AddAndSaveEventAsync(orderStart\", \"edIntegrationEvent);\\n // Add/Update the Buyer AggregateRoot\\n // DDD patterns comment: Add child enti\", \"ties and value-objects through the Order \\nAggregate-Root\\n // methods and constructor so validations,\", \" invariants and business logic\\n // make sure that consistency is preserved across the whole aggregat\", \"e\\n var address = new Address(message.Street, message.City, message.State,\\nmessage.Country, message.Z\", \"ipCode);\\n var order = new Order(message.UserId, message.UserName, address,\\nmessage.CardTypeId, messa\", \"ge.CardNumber, message.CardSecurityNumber, message.CardHolderName,\\nmessage.CardExpiration);\\n foreach\", \" (var item in message.OrderItems)\\n```\\n\\n```\\n {\\n order.AddOrderItem(item.ProductId, item.ProductName, \", \"item.UnitPrice,\\nitem.Discount, item.PictureUrl, item.Units);\\n }\\n _logger.LogInformation(\\\"----- Creat\", \"ing Order - Order: {@Order}\\\", order);\\n _orderRepository.Add(order);\\n return await _orderRepository.U\", \"nitOfWork\\n .SaveEntitiesAsync(cancellationToken);\\n }\\n}\\n```\\n\\nThe class uses the injected repositories\", \" to execute the transaction and persist the state changes. It does not matter whether that class is \", \"a command handler, an ASP.NET Core Web API controller method, or a [DDD Application Service.](https:\", \"//lostechies.com/jimmybogard/2008/08/21/services-in-domain-driven-design/) It is ultimately a simple\", \" class that uses repositories, domain entities, and other application coordination in a fashion simi\", \"lar to a command handler. Dependency Injection works the same way for all the mentioned classes, as \", \"in the example using DI based on the constructor.\\n\\n## **Register the dependency implementation types\", \" and interfaces or abstractions**\\n\\nBefore you use the objects injected through constructors, you nee\", \"d to know where to register the interfaces and classes that produce the objects injected into your a\", \"pplication classes through DI. (Like DI based on the constructor, as shown previously.)\\n\\n### **Use t\", \"he built-in IoC container provided by ASP.NET Core**\\n\\nWhen you use the built-in IoC container provid\", \"ed by ASP.NET Core, you register the types you want to inject in the *Program.cs* file, as in the fo\", \"llowing code:\\n\\n```\\n// Register out-of-the-box framework services.\\nbuilder.Services.AddDbContext<Cata\", \"logContext>(c =>\\n c.UseSqlServer(Configuration[\\\"ConnectionString\\\"]),\\n ServiceLifetime.Scoped);\\nbuild\", \"er.Services.AddMvc();\\n// Register custom application dependencies.\\nbuilder.Services.AddScoped<IMyCus\", \"tomRepository, MyCustomSQLRepository>();\\n```\\n\\nThe most common pattern when registering types in an I\", \"oC container is to register a pair of types\\u2014an interface and its related implementation class. Then \", \"when you request an object from the IoC container through any constructor, you request an object of \", \"a certain type of interface. For instance, in the previous example, the last line states that when a\", \"ny of your constructors have a dependency on IMyCustomRepository (interface or abstraction), the IoC\", \" container will inject an instance of the MyCustomSQLServerRepository implementation class.\\n\\n### **U\", \"se the Scrutor library for automatic types registration**\\n\\nWhen using DI in .NET, you might want to \", \"be able to scan an assembly and automatically register its types by convention. This feature is not \", \"currently available in ASP.NET Core. However, you can use the [Scrutor](https://github.com/khellang/\", \"Scrutor) library for that. This approach is convenient when you have dozens of types that need to be\", \" registered in your IoC container.\\n\\n## **Additional resources**\\n\\n<https://www.mking.net/blog/registe\", \"ring-services-with-scrutor>\\n\\n\\u2022 **Matthew King. Registering services with Scrutor**\\n\\n\\u2022 **Kristian Hel\", \"lang. Scrutor.** GitHub repo. <https://github.com/khellang/Scrutor>\\n\\n## **Use Autofac as an IoC cont\", \"ainer**\\n\\nYou can also use additional IoC containers and plug them into the ASP.NET Core pipeline, as\", \" in the ordering microservice in eShopOnContainers, which uses [Autofac.](https://autofac.org/) When\", \" using Autofac you typically register the types via modules, which allow you to split the registrati\", \"on types between multiple files depending on where your types are, just as you could have the applic\", \"ation types distributed across multiple class libraries.\\n\\nFor example, the following is the [Autofac\", \" application module](https://github.com/dotnet-architecture/eShopOnContainers/blob/main/src/Services\", \"/Ordering/Ordering.API/Infrastructure/AutofacModules/ApplicationModule.cs) for th[e Ordering.API Web\", \" API](https://github.com/dotnet-architecture/eShopOnContainers/tree/main/src/Services/Ordering/Order\", \"ing.API) project with the types you will want to inject.\\n\\n```\\npublic class ApplicationModule : Autof\", \"ac.Module\\n{\\n public string QueriesConnectionString { get; }\\n public ApplicationModule(string qconstr\", \")\\n {\\n QueriesConnectionString = qconstr;\\n }\\n protected override void Load(ContainerBuilder builder)\\n\", \" {\\n builder.Register(c => new OrderQueries(QueriesConnectionString))\\n .As<IOrderQueries>()\\n .Instanc\", \"ePerLifetimeScope();\\n builder.RegisterType<BuyerRepository>()\\n .As<IBuyerRepository>()\\n .InstancePer\", \"LifetimeScope();\\n builder.RegisterType<OrderRepository>()\\n .As<IOrderRepository>()\\n .InstancePerLife\", \"timeScope();\\n builder.RegisterType<RequestManager>()\\n .As<IRequestManager>()\\n .InstancePerLifetimeSc\", \"ope();\\n }\\n}\\n```\\n\\nAutofac also has a feature to [scan assemblies and register types by name conventio\", \"ns.](https://autofac.readthedocs.io/en/latest/register/scanning.html)\\n\\nThe registration process and \", \"concepts are very similar to the way you can register types with the builtin ASP.NET Core IoC contai\", \"ner, but the syntax when using Autofac is a bit different.\\n\\nIn the example code, the abstraction IOr\", \"derRepository is registered along with the implementation class OrderRepository. This means that whe\", \"never a constructor is declaring a dependency through the IOrderRepository abstraction or interface,\", \" the IoC container will inject an instance of the OrderRepository class.\\n\\nThe instance scope type de\", \"termines how an instance is shared between requests for the same service or dependency. When a reque\", \"st is made for a dependency, the IoC container can return the following: \\u2022 A single instance per lif\", \"etime scope (referred to in the ASP.NET Core IoC container as *scoped*).\\n\\n- A new instance per depen\", \"dency (referred to in the ASP.NET Core IoC container as *transient*).\\n- A single instance shared acr\", \"oss all objects using the IoC container (referred to in the ASP.NET Core IoC container as *singleton\", \"*).\\n\\n## **Additional resources**\\n\\n- **Introduction to Dependency Injection in ASP.NET Core** [https:\", \"//learn.microsoft.com/aspnet/core/fundamentals/dependency-injection](https://docs.microsoft.com/aspn\", \"et/core/fundamentals/dependency-injection)\\n- **Autofac.** Official documentation. <https://docs.auto\", \"fac.org/en/latest/>\\n- **Comparing ASP.NET Core IoC container service lifetimes with Autofac IoC cont\", \"ainer instance scopes - Cesar de la Torre.** [https://devblogs.microsoft.com/cesardelatorre/comparin\", \"g-asp-net-core-ioc-service-life](https://devblogs.microsoft.com/cesardelatorre/comparing-asp-net-cor\", \"e-ioc-service-life-times-and-autofac-ioc-instance-scopes/)[times-and-autofac-ioc-instance-scopes/](h\", \"ttps://devblogs.microsoft.com/cesardelatorre/comparing-asp-net-core-ioc-service-life-times-and-autof\", \"ac-ioc-instance-scopes/)\\n\\n## <span id=\\\"page-285-0\\\"></span>**Implement the Command and Command Handle\", \"r patterns**\\n\\nIn the DI-through-constructor example shown in the previous section, the IoC container\", \" was injecting repositories through a constructor in a class. But exactly where were they injected? \", \"In a simple Web API (for example, the catalog microservice in eShopOnContainers), you inject them at\", \" the MVC controllers' level, in a controller constructor, as part of the request pipeline of ASP.NET\", \" Core. However, in the initial code of this section (the [CreateOrderCommandHandler](https://github.\", \"com/dotnet-architecture/eShopOnContainers/blob/main/src/Services/Ordering/Ordering.API/Application/C\", \"ommands/CreateOrderCommandHandler.cs) class from the Ordering.API service in eShopOnContainers), the\", \" injection of dependencies is done through the constructor of a particular command handler. Let us e\", \"xplain what a command handler is and why you would want to use it.\\n\\nThe Command pattern is intrinsic\", \"ally related to the CQRS pattern that was introduced earlier in this guide. CQRS has two sides. The \", \"first area is queries, using simplified queries with the [Dapper](https://github.com/StackExchange/d\", \"apper-dot-net) micro ORM, which was explained previously. The second area is commands, which are the\", \" starting point for transactions, and the input channel from outside the service.\\n\\nAs shown in Figur\", \"e 7-24, the pattern is based on accepting commands from the client-side, processing them based on th\", \"e domain model rules, and finally persisting the states with transactions.\\n\\n![](_page_286_Figure_1.j\", \"peg)\\n\\n*Figure 7-24. High-level view of the commands or \\\"transactional side\\\" in a CQRS pattern*\\n\\nFigu\", \"re 7-24 shows that the UI app sends a command through the API that gets to a CommandHandler, that de\", \"pends on the Domain model and the Infrastructure, to update the database.\\n\\n### **The command class**\", \"\\n\\nA command is a request for the system to perform an action that changes the state of the system. C\", \"ommands are imperative, and should be processed just once.\\n\\nSince commands are imperatives, they are\", \" typically named with a verb in the imperative mood (for example, \\\"create\\\" or \\\"update\\\"), and they mi\", \"ght include the aggregate type, such as CreateOrderCommand. Unlike an event, a command is not a fact\", \" from the past; it is only a request, and thus may be refused.\\n\\nCommands can originate from the UI a\", \"s a result of a user initiating a request, or from a process manager when the process manager is dir\", \"ecting an aggregate to perform an action.\\n\\nAn important characteristic of a command is that it shoul\", \"d be processed just once by a single receiver. This is because a command is a single action or trans\", \"action you want to perform in the application. For example, the same order creation command should n\", \"ot be processed more than once. This is an important difference between commands and events. Events \", \"may be processed multiple times, because many systems or microservices might be interested in the ev\", \"ent.\\n\\nIn addition, it is important that a command be processed only once in case the command is not \", \"idempotent. A command is idempotent if it can be executed multiple times without changing the result\", \", either because of the nature of the command, or because of the way the system handles the command.\", \"\\n\\nIt is a good practice to make your commands and updates idempotent when it makes sense under your \", \"domain's business rules and invariants. For instance, to use the same example, if for any reason (re\", \"try logic, hacking, etc.) the same CreateOrder command reaches your system multiple times, you shoul\", \"d be able to identify it and ensure that you do not create multiple orders. To do so, you need to at\", \"tach some kind of identity in the operations and identify whether the command or update was already \", \"processed.\\n\\nYou send a command to a single receiver; you do not publish a command. Publishing is for\", \" events that state a fact\\u2014that something has happened and might be interesting for event receivers. \", \"In the case of events, the publisher has no concerns about which receivers get the event or what the\", \"y do it. But domain or integration events are a different story already introduced in previous secti\", \"ons.\\n\\nA command is implemented with a class that contains data fields or collections with all the in\", \"formation that is needed in order to execute that command. A command is a special kind of Data Trans\", \"fer Object (DTO), one that is specifically used to request changes or transactions. The command itse\", \"lf is based on exactly the information that is needed for processing the command, and nothing more.\\n\", \"\\nThe following example shows the simplified CreateOrderCommand class. This is an immutable command t\", \"hat is used in the ordering microservice in eShopOnContainers.\\n\\n```\\n// DDD and CQRS patterns comment\", \": Note that it is recommended to implement immutable \\nCommands\\n// In this case, its immutability is \", \"achieved by having all the setters as private\\n// plus only being able to update the data just once, \", \"when creating the object through its \\nconstructor.\\n// References on Immutable Commands:\\n// http://cq\", \"rs.nu/Faq\\n// https://docs.spine3.org/motivation/immutability.html\\n// http://blog.gauffin.org/2012/06\", \"/griffin-container-introducing-command-support/\\n// https://learn.microsoft.com/dotnet/csharp/program\", \"ming-guide/classes-and-structs/how-to-\\nimplement-a-lightweight-class-with-auto-implemented-propertie\", \"s\\n[DataContract]\\npublic class CreateOrderCommand\\n : IRequest<bool>\\n{\\n [DataMember]\\n private readonly\", \" List<OrderItemDTO> _orderItems;\\n [DataMember]\\n public string UserId { get; private set; }\\n [DataMem\", \"ber]\\n public string UserName { get; private set; }\\n [DataMember]\\n public string City { get; private \", \"set; }\\n [DataMember]\\n public string Street { get; private set; }\\n [DataMember]\\n public string State \", \"{ get; private set; }\\n [DataMember]\\n public string Country { get; private set; }\\n [DataMember]\\n publ\", \"ic string ZipCode { get; private set; }\\n [DataMember]\\n```\\n\\n```\\n public string CardNumber { get; priv\", \"ate set; }\\n [DataMember]\\n public string CardHolderName { get; private set; }\\n [DataMember]\\n public D\", \"ateTime CardExpiration { get; private set; }\\n [DataMember]\\n public string CardSecurityNumber { get; \", \"private set; }\\n [DataMember]\\n public int CardTypeId { get; private set; }\\n [DataMember]\\n public IEnu\", \"merable<OrderItemDTO> OrderItems => _orderItems;\\n public CreateOrderCommand()\\n {\\n _orderItems = new \", \"List<OrderItemDTO>();\\n }\\n public CreateOrderCommand(List<BasketItem> basketItems, string userId, str\", \"ing userName,\\nstring city, string street, string state, string country, string zipcode,\\n string card\", \"Number, string cardHolderName, DateTime cardExpiration,\\n string cardSecurityNumber, int cardTypeId) \", \": this()\\n {\\n _orderItems = basketItems.ToOrderItemsDTO().ToList();\\n UserId = userId;\\n UserName = use\", \"rName;\\n City = city;\\n Street = street;\\n State = state;\\n Country = country;\\n ZipCode = zipcode;\\n Card\", \"Number = cardNumber;\\n CardHolderName = cardHolderName;\\n CardExpiration = cardExpiration;\\n CardSecuri\", \"tyNumber = cardSecurityNumber;\\n CardTypeId = cardTypeId;\\n CardExpiration = cardExpiration;\\n }\\n publi\", \"c class OrderItemDTO\\n {\\n public int ProductId { get; set; }\\n public string ProductName { get; set; }\", \"\\n public decimal UnitPrice { get; set; }\\n public decimal Discount { get; set; }\\n public int Units { \", \"get; set; }\\n public string PictureUrl { get; set; }\\n }\\n}\\n```\\n\\nBasically, the command class contains \", \"all the data you need for performing a business transaction by using the domain model objects. Thus,\", \" commands are simply data structures that contain read-only data, and no behavior. The command's nam\", \"e indicates its purpose. In many languages like C#, commands are represented as classes, but they ar\", \"e not true classes in the real object-oriented sense.\\n\\nprocessed directly by the domain model. They \", \"do not need to change during their projected lifetime. In a C# class, immutability can be achieved b\", \"y not having any setters or other methods that change the internal state.\\n\\nAs an additional characte\", \"ristic, commands are immutable, because the expected usage is that they are\\n\\nKeep in mind that if yo\", \"u intend or expect commands to go through a serializing/deserializing process, the properties must h\", \"ave a private setter, and the [DataMember] (or [JsonProperty]) attribute. Otherwise, the deserialize\", \"r won't be able to reconstruct the object at the destination with the required values. You can also \", \"use truly read-only properties if the class has a constructor with parameters for all properties, wi\", \"th the usual camelCase naming convention, and annotate the constructor as [JsonConstructor]. However\", \", this option requires more code.\\n\\nFor example, the command class for creating an order is probably \", \"similar in terms of data to the order you want to create, but you probably do not need the same attr\", \"ibutes. For instance, CreateOrderCommand does not have an order ID, because the order has not been c\", \"reated yet.\\n\\nMany command classes can be simple, requiring only a few fields about some state that n\", \"eeds to be changed. That would be the case if you are just changing the status of an order from \\\"in \", \"process\\\" to \\\"paid\\\" or \\\"shipped\\\" by using a command similar to the following:\\n\\n```\\n[DataContract]\\npub\", \"lic class UpdateOrderStatusCommand\\n :IRequest<bool>\\n{\\n [DataMember]\\n public string Status { get; pri\", \"vate set; }\\n [DataMember]\\n public string OrderId { get; private set; }\\n [DataMember]\\n public string \", \"BuyerIdentityGuid { get; private set; }\\n}\\n```\\n\\nSome developers make their UI request objects separat\", \"e from their command DTOs, but that is just a matter of preference. It is a tedious separation with \", \"not much additional value, and the objects are almost exactly the same shape. For instance, in eShop\", \"OnContainers, some commands come directly from the client-side.\\n\\n### **The Command handler class**\\n\\n\", \"You should implement a specific command handler class for each command. That is how the pattern work\", \"s, and it's where you'll use the command object, the domain objects, and the infrastructure reposito\", \"ry objects. The command handler is in fact the heart of the application layer in terms of CQRS and D\", \"DD. However, all the domain logic should be contained in the domain classes\\u2014within the aggregate roo\", \"ts (root entities), child entities, or [domain services,](https://lostechies.com/jimmybogard/2008/08\", \"/21/services-in-domain-driven-design/) but not within the command handler, which is a class from the\", \" application layer.\\n\\nThe command handler class offers a strong stepping stone in the way to achieve \", \"the Single Responsibility Principle (SRP) mentioned in a previous section.\\n\\nresult should be either \", \"successful execution of the command, or an exception. In the case of an exception, the system state \", \"should be unchanged.\\n\\nA command handler receives a command and obtains a result from the aggregate t\", \"hat is used. The\\n\\nThe command handler usually takes the following steps:\\n\\n- It receives the command \", \"object, like a DTO (from the [mediator](https://en.wikipedia.org/wiki/Mediator_pattern) or other inf\", \"rastructure object).\\n- It validates that the command is valid (if not validated by the mediator).\\n- \", \"It instantiates the aggregate root instance that is the target of the current command.\\n- It executes\", \" the method on the aggregate root instance, getting the required data from the command.\\n- It persist\", \"s the new state of the aggregate to its related database. This last operation is the actual transact\", \"ion.\\n\\nTypically, a command handler deals with a single aggregate driven by its aggregate root (root \", \"entity). If multiple aggregates should be impacted by the reception of a single command, you could u\", \"se domain events to propagate states or actions across multiple aggregates.\\n\\nThe important point her\", \"e is that when a command is being processed, all the domain logic should be inside the domain model \", \"(the aggregates), fully encapsulated and ready for unit testing. The command handler just acts as a \", \"way to get the domain model from the database, and as the final step, to tell the infrastructure lay\", \"er (repositories) to persist the changes when the model is changed. The advantage of this approach i\", \"s that you can refactor the domain logic in an isolated, fully encapsulated, rich, behavioral domain\", \" model without changing code in the application or infrastructure layers, which are the plumbing lev\", \"el (command handlers, Web API, repositories, etc.).\\n\\nWhen command handlers get complex, with too muc\", \"h logic, that can be a code smell. Review them, and if you find domain logic, refactor the code to m\", \"ove that domain behavior to the methods of the domain objects (the aggregate root and child entity).\", \"\\n\\nAs an example of a command handler class, the following code shows the same CreateOrderCommandHand\", \"ler class that you saw at the beginning of this chapter. In this case, it also highlights the Handle\", \" method and the operations with the domain model objects/aggregates.\\n\\n```\\npublic class CreateOrderCo\", \"mmandHandler\\n : IRequestHandler<CreateOrderCommand, bool>\\n{\\n private readonly IOrderRepository _orde\", \"rRepository;\\n private readonly IIdentityService _identityService;\\n private readonly IMediator _media\", \"tor;\\n private readonly IOrderingIntegrationEventService _orderingIntegrationEventService;\\n private r\", \"eadonly ILogger<CreateOrderCommandHandler> _logger;\\n // Using DI to inject infrastructure persistenc\", \"e Repositories\\n public CreateOrderCommandHandler(IMediator mediator,\\n IOrderingIntegrationEventServi\", \"ce orderingIntegrationEventService,\\n IOrderRepository orderRepository,\\n```\\n\\n```\\n IIdentityService id\", \"entityService,\\n ILogger<CreateOrderCommandHandler> logger)\\n {\\n _orderRepository = orderRepository ??\", \" throw new\\nArgumentNullException(nameof(orderRepository));\\n _identityService = identityService ?? th\", \"row new\\nArgumentNullException(nameof(identityService));\\n _mediator = mediator ?? throw new ArgumentN\", \"ullException(nameof(mediator));\\n _orderingIntegrationEventService = orderingIntegrationEventService \", \"?? throw new\\nArgumentNullException(nameof(orderingIntegrationEventService));\\n _logger = logger ?? th\", \"row new ArgumentNullException(nameof(logger));\\n }\\n public async Task<bool> Handle(CreateOrderCommand\", \" message, CancellationToken \\ncancellationToken)\\n {\\n // Add Integration event to clean the basket\\n va\", \"r orderStartedIntegrationEvent = new\\nOrderStartedIntegrationEvent(message.UserId);\\n await \\n_ordering\", \"IntegrationEventService.AddAndSaveEventAsync(orderStartedIntegrationEvent);\\n // Add/Update the Buyer\", \" AggregateRoot\\n // DDD patterns comment: Add child entities and value-objects through the Order \\nAgg\", \"regate-Root\\n // methods and constructor so validations, invariants and business logic\\n // make sure \", \"that consistency is preserved across the whole aggregate\\n var address = new Address(message.Street, \", \"message.City, message.State,\\nmessage.Country, message.ZipCode);\\n var order = new Order(message.UserI\", \"d, message.UserName, address,\\nmessage.CardTypeId, message.CardNumber, message.CardSecurityNumber, me\", \"ssage.CardHolderName,\\nmessage.CardExpiration);\\n foreach (var item in message.OrderItems)\\n {\\n order.A\", \"ddOrderItem(item.ProductId, item.ProductName, item.UnitPrice,\\nitem.Discount, item.PictureUrl, item.U\", \"nits);\\n }\\n _logger.LogInformation(\\\"----- Creating Order - Order: {@Order}\\\", order);\\n _orderRepositor\", \"y.Add(order);\\n return await _orderRepository.UnitOfWork\\n .SaveEntitiesAsync(cancellationToken);\\n }\\n}\", \"\\n```\\n\\nThese are additional steps a command handler should take:\\n\\n- Use the command's data to operate\", \" with the aggregate root's methods and behavior.\\n- Internally within the domain objects, raise domai\", \"n events while the transaction is executed, but that is transparent from a command handler point of \", \"view.\\n- If the aggregate's operation result is successful and after the transaction is finished, rai\", \"se integration events. (These might also be raised by infrastructure classes like repositories.)\\n\\n##\", \" **Additional resources**\\n\\n\\u2022 **Commands and events**\\n\\n- **Mark Seemann. At the Boundaries, Applicati\", \"ons are Not Object-Oriented** <https://blog.ploeh.dk/2011/05/31/AttheBoundaries,ApplicationsareNotOb\", \"ject-Oriented/>\\n- <https://cqrs.nu/faq/Command%20and%20Events>\\n- <https://cqrs.nu/faq/Command%20Hand\", \"lers>\\n\\n\\u2022 **What does a command handler do?**\\n\\n<https://jimmybogard.com/domain-command-patterns-handl\", \"ers/>\\n\\n\\u2022 **Jimmy Bogard. Domain Command Patterns \\u2013 Handlers**\\n\\n\\u2022 **Jimmy Bogard. Domain Command Patt\", \"erns \\u2013 Validation** <https://jimmybogard.com/domain-command-patterns-validation/>\\n\\n## <span id=\\\"page\", \"-292-0\\\"></span>**The Command process pipeline: how to trigger a command handler**\\n\\nThe next question\", \" is how to invoke a command handler. You could manually call it from each related ASP.NET Core contr\", \"oller. However, that approach would be too coupled and is not ideal.\\n\\nThe other two main options, wh\", \"ich are the recommended options, are:\\n\\n- Through an in-memory Mediator pattern artifact.\\n- With an a\", \"synchronous message queue, in between controllers and handlers.\\n\\n### **Use the Mediator pattern (in-\", \"memory) in the command pipeline**\\n\\nAs shown in Figure 7-25, in a CQRS approach you use an intelligen\", \"t mediator, similar to an in-memory bus, which is smart enough to redirect to the right command hand\", \"ler based on the type of the command or DTO being received. The single black arrows between componen\", \"ts represent the dependencies between objects (in many cases, injected through DI) with their relate\", \"d interactions.\\n\\n![](_page_293_Figure_0.jpeg)\\n\\n*Figure 7-25. Using the Mediator pattern in process i\", \"n a single CQRS microservice*\\n\\nThe above diagram shows a zoom-in from image 7-24: the ASP.NET Core c\", \"ontroller sends the command to MediatR's command pipeline, so they get to the appropriate handler.\\n\\n\", \"The reason that using the Mediator pattern makes sense is that in enterprise applications, the proce\", \"ssing requests can get complicated. You want to be able to add an open number of crosscutting concer\", \"ns like logging, validations, audit, and security. In these cases, you can rely on a mediator pipeli\", \"ne (see [Mediator pattern\\\\)](https://en.wikipedia.org/wiki/Mediator_pattern) to provide a means for \", \"these extra behaviors or crosscutting concerns.\\n\\nA mediator is an object that encapsulates the \\\"how\\\"\", \" of this process: it coordinates execution based on state, the way a command handler is invoked, or \", \"the payload you provide to the handler. With a mediator component, you can apply cross-cutting conce\", \"rns in a centralized and transparent way by applying decorators (or [pipeline behaviors](https://git\", \"hub.com/jbogard/MediatR/wiki/Behaviors) since [MediatR 3\\\\)](https://www.nuget.org/packages/MediatR/3\", \".0.0). For more information, see the [Decorator](https://en.wikipedia.org/wiki/Decorator_pattern)  [\", \"pattern.](https://en.wikipedia.org/wiki/Decorator_pattern)\\n\\nDecorators and behaviors are similar to \", \"[Aspect Oriented Programming \\\\(AOP\\\\),](https://en.wikipedia.org/wiki/Aspect-oriented_programming) on\", \"ly applied to a specific process pipeline managed by the mediator component. Aspects in AOP that imp\", \"lement crosscutting concerns are applied based on *aspect weavers* injected at compilation time or b\", \"ased on object call interception. Both typical AOP approaches are sometimes said to work \\\"like magic\", \",\\\" because it is not easy to see how AOP does its work. When dealing with serious issues or bugs, AO\", \"P can be difficult to debug. On the other hand, these decorators/behaviors are explicit and applied \", \"only in the context of the mediator, so debugging is much more predictable and easy.\\n\\nFor example, i\", \"n the eShopOnContainers ordering microservice, has an implementation of two sample behaviors, a [Log\", \"Behavior](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Or\", \"dering.API/Application/Behaviors/LoggingBehavior.cs) class and a [ValidatorBehavior](https://github.\", \"com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Be\", \"haviors/ValidatorBehavior.cs) class. The implementation of the behaviors is explained in the next se\", \"ction by showing how eShopOnContainers uses [MediatR](https://www.nuget.org/packages/MediatR) [behav\", \"iors.](https://github.com/jbogard/MediatR/wiki/Behaviors)\\n\\n## **Use message queues (out-of-proc) in \", \"the command's pipeline**\\n\\nAnother choice is to use asynchronous messages based on brokers or message\", \" queues, as shown in Figure 7-26. That option could also be combined with the mediator component rig\", \"ht before the command handler.\\n\\n![](_page_294_Figure_3.jpeg)\\n\\n*Figure 7-26. Using message queues (ou\", \"t of the process and inter-process communication) with CQRS commands*\\n\\nCommand's pipeline can also b\", \"e handled by a high availability message queue to deliver the commands to the appropriate handler. U\", \"sing message queues to accept the commands can further complicate your command's pipeline, because y\", \"ou will probably need to split the pipeline into two processes connected through the external messag\", \"e queue. Still, it should be used if you need to have improved scalability and performance based on \", \"asynchronous messaging. Consider that in the case of Figure 7-26, the controller just posts the comm\", \"and message into the queue and returns. Then the command handlers process the messages at their own \", \"pace. That is a great benefit of queues: the message queue can act as a buffer in cases when hyper s\", \"calability is needed, such as for stocks or any other scenario with a high volume of ingress data.\\n\\n\", \"However, because of the asynchronous nature of message queues, you need to figure out how to communi\", \"cate with the client application about the success or failure of the command's process. As a rule, y\", \"ou should never use \\\"fire and forget\\\" commands. Every business application needs to know if a comman\", \"d was processed successfully, or at least validated and accepted.\\n\\nThus, being able to respond to th\", \"e client after validating a command message that was submitted to an asynchronous queue adds complex\", \"ity to your system, as compared to an in-process command process that returns the operation's result\", \" after running the transaction. Using queues, you might need to return the result of the command pro\", \"cess through other operation result messages, which will require additional components and custom co\", \"mmunication in your system.\\n\\nAdditionally, async commands are one-way commands, which in many cases \", \"might not be needed, as is explained in the following interesting exchange between Burtsev Alexey an\", \"d Greg Young in an [online conversation:](https://groups.google.com/forum/#!msg/dddcqrs/xhJHVxDx2pM/\", \"WP9qP8ifYCwJ)\\n\\n[Burtsev Alexey] I find lots of code where people use async command handling or one-w\", \"ay command messaging without any reason to do so (they are not doing some long operation, they are n\", \"ot executing external async code, they do not even cross-application boundary to be using message bu\", \"s). Why do they introduce this unnecessary complexity? And actually, I haven't seen a CQRS code exam\", \"ple with blocking command handlers so far, though it will work just fine in most cases.\\n\\n[Greg Young\", \"] [\\u2026] an asynchronous command doesn't exist; it's actually another event. If I must accept what you \", \"send me and raise an event if I disagree, it's no longer you telling me to do something [that is, it\", \"'s not a command]. It's you telling me something has been done. This seems like a slight difference \", \"at first, but it has many implications.\\n\\nAsynchronous commands greatly increase the complexity of a \", \"system, because there is no simple way to indicate failures. Therefore, asynchronous commands are no\", \"t recommended other than when scaling requirements are needed or in special cases when communicating\", \" the internal microservices through messaging. In those cases, you must design a separate reporting \", \"and recovery system for failures.\\n\\nIn the initial version of eShopOnContainers, it was decided to us\", \"e synchronous command processing, started from HTTP requests and driven by the Mediator pattern. Tha\", \"t easily allows you to return the success or failure of the process, as in the [CreateOrderCommandHa\", \"ndler](https://github.com/dotnet-architecture/eShopOnContainers/blob/netcore1.1/src/Services/Orderin\", \"g/Ordering.API/Application/Commands/CreateOrderCommandHandler.cs) implementation.\\n\\nIn any case, this\", \" should be a decision based on your application's or microservice's business requirements.\\n\\n## <span\", \" id=\\\"page-295-0\\\"></span>**Implement the command process pipeline with a mediator pattern (MediatR)**\", \"\\n\\nAs a sample implementation, this guide proposes using the in-process pipeline based on the Mediato\", \"r pattern to drive command ingestion and route commands, in memory, to the right command handlers. T\", \"he guide also proposes applying [behaviors](https://github.com/jbogard/MediatR/wiki/Behaviors) in or\", \"der to separate cross-cutting concerns.\\n\\nFor implementation in .NET, there are multiple open-source \", \"libraries available that implement the Mediator pattern. The library used in this guide is the [Medi\", \"atR](https://github.com/jbogard/MediatR) open-source library (created by Jimmy Bogard), but you coul\", \"d use another approach. MediatR is a small and simple library that allows you to process in-memory m\", \"essages like a command, while applying decorators or behaviors.\\n\\nUsing the Mediator pattern helps yo\", \"u to reduce coupling and to isolate the concerns of the requested work, while automatically connecti\", \"ng to the handler that performs that work\\u2014in this case, to command handlers.\\n\\nAnother good reason to\", \" use the Mediator pattern was explained by Jimmy Bogard when reviewing this guide:\\n\\nI think it might\", \" be worth mentioning testing here \\u2013 it provides a nice consistent window into the behavior of your s\", \"ystem. Request-in, response-out. We've found that aspect quite valuable in building consistently beh\", \"aving tests.\\n\\nFirst, let's look at a sample WebAPI controller where you actually would use the media\", \"tor object. If you weren't using the mediator object, you'd need to inject all the dependencies for \", \"that controller, things like a logger object and others. Therefore, the constructor would be complic\", \"ated. On the other hand, if you use the mediator object, the constructor of your controller can be a\", \" lot simpler, with just a few dependencies instead of many dependencies if you had one per cross-cut\", \"ting operation, as in the following example:\\n\\n```\\npublic class MyMicroserviceController : Controller\", \"\\n{\\n public MyMicroserviceController(IMediator mediator,\\n IMyMicroserviceQueries microserviceQueries)\", \"\\n {\\n // ...\\n }\\n}\\n```\\n\\nYou can see that the mediator provides a clean and lean Web API controller con\", \"structor. In addition, within the controller methods, the code to send a command to the mediator obj\", \"ect is almost one line:\\n\\n```\\n[Route(\\\"new\\\")]\\n[HttpPost]\\npublic async Task<IActionResult> ExecuteBusin\", \"essOperation([FromBody]RunOpCommand\\n runOperationCommand)\\n{\\n var commandResult = await _mediator.Sen\", \"dAsync(runOperationCommand);\\n return commandResult ? (IActionResult)Ok() : (IActionResult)BadRequest\", \"();\\n}\\n```\\n\\n### **Implement idempotent Commands**\\n\\nIn **eShopOnContainers**, a more advanced example \", \"than the above is submitting a CreateOrderCommand object from the Ordering microservice. But since t\", \"he Ordering business process is a bit more complex and, in our case, it actually starts in the Baske\", \"t microservice, this action of submitting the CreateOrderCommand object is performed from an integra\", \"tion-event handler named [UserCheckoutAcceptedIntegrationEventHandler](https://github.com/dotnet-arc\", \"hitecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/IntegrationEvent\", \"s/EventHandling/UserCheckoutAcceptedIntegrationEventHandler.cs) instead of a simple WebAPI controlle\", \"r called from the client App as in the previous simpler example.\\n\\nNevertheless, the action of submit\", \"ting the Command to MediatR is pretty similar, as shown in the following code.\\n\\n```\\nvar createOrderC\", \"ommand = new CreateOrderCommand(eventMsg.Basket.Items,\\n eventMsg.UserId, eventMsg.City,\\n eventMsg.St\", \"reet, eventMsg.State,\\n eventMsg.Country, eventMsg.ZipCode,\\n eventMsg.CardNumber,\\n eventMsg.CardHolde\", \"rName,\\n eventMsg.CardExpiration,\\n eventMsg.CardSecurityNumber,\\n eventMsg.CardTypeId);\\n```\\n\\n```\\nvar r\", \"equestCreateOrder = new IdentifiedCommand<CreateOrderCommand,bool>(createOrderCommand,\\neventMsg.Requ\", \"estId);\\nresult = await _mediator.Send(requestCreateOrder);\\nHowever, this case is also slightly more \", \"advanced because we're also implementing idempotent\\n```\\n\\ncommands. The CreateOrderCommand process sh\", \"ould be idempotent, so if the same message comes duplicated through the network, because of any reas\", \"on, like retries, the same business order will be processed just once.\\n\\nembedding it into a generic \", \"IdentifiedCommand, which is tracked by an ID of every message coming through the network that has to\", \" be idempotent.\\n\\nThis is implemented by wrapping the business command (in this case CreateOrderComma\", \"nd) and\\n\\nIn the code below, you can see that the IdentifiedCommand is nothing more than a DTO with a\", \"nd ID plus the wrapped business command object.\\n\\n```\\npublic class IdentifiedCommand<T, R> : IRequest\", \"<R>\\n where T : IRequest<R>\\n{\\n public T Command { get; }\\n public Guid Id { get; }\\n public IdentifiedC\", \"ommand(T command, Guid id)\\n {\\n Command = command;\\n Id = id;\\n }\\n}\\n```\\n\\nThen the CommandHandler for th\", \"e IdentifiedCommand named [IdentifiedCommandHandler.cs](https://github.com/dotnet-architecture/eShop\", \"OnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Commands/IdentifiedCommandHandl\", \"er.cs) will basically check if the ID coming as part of the message already exists in a table. If it\", \" already exists, that command won't be processed again, so it behaves as an idempotent command. That\", \" infrastructure code is performed by the \\\\_requestManager.ExistAsync method call below.\\n\\n```\\n// Iden\", \"tifiedCommandHandler.cs\\npublic class IdentifiedCommandHandler<T, R> : IRequestHandler<IdentifiedComm\", \"and<T, R>, R>\\n where T : IRequest<R>\\n{\\n private readonly IMediator _mediator;\\n private readonly IReq\", \"uestManager _requestManager;\\n private readonly ILogger<IdentifiedCommandHandler<T, R>> _logger;\\n pub\", \"lic IdentifiedCommandHandler(\\n IMediator mediator,\\n IRequestManager requestManager,\\n ILogger<Identif\", \"iedCommandHandler<T, R>> logger)\\n {\\n _mediator = mediator;\\n _requestManager = requestManager;\\n _logg\", \"er = logger ?? throw new System.ArgumentNullException(nameof(logger));\\n }\\n /// <summary>\\n /// Create\", \"s the result value to return if a previous request was found\\n /// </summary>\\n /// <returns></returns\", \">\\n```\\n\\n```\\n protected virtual R CreateResultForDuplicateRequest()\\n {\\n return default(R);\\n }\\n /// <su\", \"mmary>\\n /// This method handles the command. It just ensures that no other request exists with \\nthe \", \"same ID, and if this is the case\\n /// just enqueues the original inner command.\\n /// </summary>\\n ///\", \" <param name=\\\"message\\\">IdentifiedCommand which contains both original command & \\nrequest ID</param>\\n\", \" /// <returns>Return value of inner command or default value if request same ID was \\nfound</returns>\", \"\\n public async Task<R> Handle(IdentifiedCommand<T, R> message, CancellationToken \\ncancellationToken)\", \"\\n {\\n var alreadyExists = await _requestManager.ExistAsync(message.Id);\\n if (alreadyExists)\\n {\\n retur\", \"n CreateResultForDuplicateRequest();\\n }\\n else\\n {\\n await _requestManager.CreateRequestForCommandAsync\", \"<T>(message.Id);\\n try\\n {\\n var command = message.Command;\\n var commandName = command.GetGenericTypeNa\", \"me();\\n var idProperty = string.Empty;\\n var commandId = string.Empty;\\n switch (command)\\n {\\n case Crea\", \"teOrderCommand createOrderCommand:\\n idProperty = nameof(createOrderCommand.UserId);\\n commandId = cre\", \"ateOrderCommand.UserId;\\n break;\\n case CancelOrderCommand cancelOrderCommand:\\n idProperty = nameof(ca\", \"ncelOrderCommand.OrderNumber);\\n commandId = $\\\"{cancelOrderCommand.OrderNumber}\\\";\\n break;\\n case ShipO\", \"rderCommand shipOrderCommand:\\n idProperty = nameof(shipOrderCommand.OrderNumber);\\n commandId = $\\\"{sh\", \"ipOrderCommand.OrderNumber}\\\";\\n break;\\n default:\\n idProperty = \\\"Id?\\\";\\n commandId = \\\"n/a\\\";\\n break;\\n }\\n\", \" _logger.LogInformation(\\n \\\"----- Sending command: {CommandName} - {IdProperty}: {CommandId} \\n({@Comm\", \"and})\\\",\\n commandName,\\n              idProperty,\\n              commandId,\\n```\\n\\n```\\n command);\\n // Sen\", \"d the embedded business command to mediator so it runs its related \\nCommandHandler\\n var result = awa\", \"it _mediator.Send(command, cancellationToken);\\n _logger.LogInformation(\\n \\\"----- Command result: {@Re\", \"sult} - {CommandName} - {IdProperty}: \\n{CommandId} ({@Command})\\\",\\n result,\\n              commandName\", \",\\n              idProperty,\\n              commandId,\\n              command);\\n return result;\\n }\\n cat\", \"ch\\n {\\n return default(R);\\n }\\n }\\n }\\n}\\n```\\n\\nSince the IdentifiedCommand acts like a business command's\", \" envelope, when the business command needs to be processed because it is not a repeated ID, then it \", \"takes that inner business command and resubmits it to Mediator, as in the last part of the code show\", \"n above when running \\\\_mediator.Send(message.Command), from the [IdentifiedCommandHandler.cs.](https\", \"://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Appl\", \"ication/Commands/IdentifiedCommandHandler.cs)\\n\\nWhen doing that, it will link and run the business co\", \"mmand handler, in this case, the [CreateOrderCommandHandler,](https://github.com/dotnet-architecture\", \"/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.API/Application/Commands/CreateOrderComma\", \"ndHandler.cs) which is running transactions against the Ordering database, as shown in the following\", \" code.\\n\\n```\\n// CreateOrderCommandHandler.cs\\npublic class CreateOrderCommandHandler\\n : IRequestHandle\", \"r<CreateOrderCommand, bool>\\n{\\n private readonly IOrderRepository _orderRepository;\\n private readonly\", \" IIdentityService _identityService;\\n private readonly IMediator _mediator;\\n private readonly IOrderi\", \"ngIntegrationEventService _orderingIntegrationEventService;\\n private readonly ILogger<CreateOrderCom\", \"mandHandler> _logger;\\n // Using DI to inject infrastructure persistence Repositories\\n public CreateO\", \"rderCommandHandler(IMediator mediator,\\n IOrderingIntegrationEventService orderingIntegrationEventSer\", \"vice,\\n IOrderRepository orderRepository,\\n IIdentityService identityService,\\n ILogger<CreateOrderComm\", \"andHandler> logger)\\n {\\n _orderRepository = orderRepository ?? throw new\\nArgumentNullException(nameof\", \"(orderRepository));\\n _identityService = identityService ?? throw new\\nArgumentNullException(nameof(id\", \"entityService));\\n _mediator = mediator ?? throw new ArgumentNullException(nameof(mediator));\\n _order\", \"ingIntegrationEventService = orderingIntegrationEventService ?? throw new\\nArgumentNullException(name\", \"of(orderingIntegrationEventService));\\n _logger = logger ?? throw new ArgumentNullException(nameof(lo\", \"gger));\\n```\\n\\n```\\n }\\n public async Task<bool> Handle(CreateOrderCommand message, CancellationToken \\nc\", \"ancellationToken)\\n {\\n // Add Integration event to clean the basket\\n var orderStartedIntegrationEvent\", \" = new\\nOrderStartedIntegrationEvent(message.UserId);\\n await \\n_orderingIntegrationEventService.AddAnd\", \"SaveEventAsync(orderStartedIntegrationEvent);\\n // Add/Update the Buyer AggregateRoot\\n // DDD pattern\", \"s comment: Add child entities and value-objects through the Order \\nAggregate-Root\\n // methods and co\", \"nstructor so validations, invariants and business logic\\n // make sure that consistency is preserved \", \"across the whole aggregate\\n var address = new Address(message.Street, message.City, message.State,\\nm\", \"essage.Country, message.ZipCode);\\n var order = new Order(message.UserId, message.UserName, address,\\n\", \"message.CardTypeId, message.CardNumber, message.CardSecurityNumber, message.CardHolderName,\\nmessage.\", \"CardExpiration);\\n foreach (var item in message.OrderItems)\\n {\\n order.AddOrderItem(item.ProductId, it\", \"em.ProductName, item.UnitPrice,\\nitem.Discount, item.PictureUrl, item.Units);\\n }\\n _logger.LogInformat\", \"ion(\\\"----- Creating Order - Order: {@Order}\\\", order);\\n _orderRepository.Add(order);\\n return await _o\", \"rderRepository.UnitOfWork\\n .SaveEntitiesAsync(cancellationToken);\\n }\\n}\\n```\\n\\n### **Register the types\", \" used by MediatR**\\n\\nIn order for MediatR to be aware of your command handler classes, you need to re\", \"gister the mediator classes and the command handler classes in your IoC container. By default, Media\", \"tR uses Autofac as the IoC container, but you can also use the built-in ASP.NET Core IoC container o\", \"r any other container supported by MediatR.\\n\\nThe following code shows how to register Mediator's typ\", \"es and commands when using Autofac modules.\\n\\n```\\npublic class MediatorModule : Autofac.Module\\n{\\n pro\", \"tected override void Load(ContainerBuilder builder)\\n {\\n builder.RegisterAssemblyTypes(typeof(IMediat\", \"or).GetTypeInfo().Assembly)\\n .AsImplementedInterfaces();\\n // Register all the Command classes (they \", \"implement IRequestHandler)\\n // in assembly holding the Commands\\n builder.RegisterAssemblyTypes(typeo\", \"f(CreateOrderCommand).GetTypeInfo().Assembly)\\n .AsClosedTypesOf(typeof(IRequestHandler<,>));\\n // Oth\", \"er types registration\\n```\\n\\n```\\n //...\\n }\\n}\\n```\\n\\nThis is where \\\"the magic happens\\\" with MediatR.\\n\\nAs \", \"each command handler implements the generic IRequestHandler<T> interface, when you register the asse\", \"mblies using RegisteredAssemblyTypes method all the types marked as IRequestHandler also gets regist\", \"ered with their Commands. For example:\\n\\n```\\npublic class CreateOrderCommandHandler\\n : IRequestHandle\", \"r<CreateOrderCommand, bool>\\n{\\n```\\n\\nThat is the code that correlates commands with command handlers. \", \"The handler is just a simple class, but it inherits from RequestHandler<T>, where T is the command t\", \"ype, and MediatR makes sure it is invoked with the correct payload (the command).\\n\\n## <span id=\\\"page\", \"-301-0\\\"></span>**Apply cross-cutting concerns when processing commands with the Behaviors in MediatR\", \"**\\n\\nThere is one more thing: being able to apply cross-cutting concerns to the mediator pipeline. Yo\", \"u can also see at the end of the Autofac registration module code how it registers a behavior type, \", \"specifically, a custom LoggingBehavior class and a ValidatorBehavior class. But you could add other \", \"custom behaviors, too.\\n\\n```\\npublic class MediatorModule : Autofac.Module\\n{\\n protected override void \", \"Load(ContainerBuilder builder)\\n {\\n builder.RegisterAssemblyTypes(typeof(IMediator).GetTypeInfo().Ass\", \"embly)\\n .AsImplementedInterfaces();\\n // Register all the Command classes (they implement IRequestHan\", \"dler)\\n // in assembly holding the Commands\\n builder.RegisterAssemblyTypes(\\n typeof(CreateOrderComman\", \"d).GetTypeInfo().Assembly).\\n AsClosedTypesOf(typeof(IRequestHandler<,>));\\n // Other types registrati\", \"on\\n //...\\n builder.RegisterGeneric(typeof(LoggingBehavior<,>)).\\n As(typeof(IPipelineBehavior<,>));\\n \", \"builder.RegisterGeneric(typeof(ValidatorBehavior<,>)).\\n As(typeof(IPipelineBehavior<,>));\\n }\\n}\\n```\\n\\n\", \"That [LoggingBehavior](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Service\", \"s/Ordering/Ordering.API/Application/Behaviors/LoggingBehavior.cs) class can be implemented as the fo\", \"llowing code, which logs information about the command handler being executed and whether it was suc\", \"cessful or not.\\n\\n```\\npublic class LoggingBehavior<TRequest, TResponse>\\n : IPipelineBehavior<TRequest\", \", TResponse>\\n{\\n private readonly ILogger<LoggingBehavior<TRequest, TResponse>> _logger;\\n public Logg\", \"ingBehavior(ILogger<LoggingBehavior<TRequest, TResponse>> logger) =>\\n _logger = logger;\\n```\\n\\n```\\n pu\", \"blic async Task<TResponse> Handle(TRequest request,\\n RequestHandlerDelegate<TResponse> next)\\n {\\n _lo\", \"gger.LogInformation($\\\"Handling {typeof(TRequest).Name}\\\");\\n var response = await next();\\n _logger.Log\", \"Information($\\\"Handled {typeof(TResponse).Name}\\\");\\n return response;\\n }\\n}\\nJust by implementing this b\", \"ehavior class and by registering it in the pipeline (in the MediatorModule\\n```\\n\\nabove), all the comm\", \"ands processed through MediatR will be logging information about the execution.\\n\\nThe eShopOnContaine\", \"rs ordering microservice also applies a second behavior for basic validations, the [ValidatorBehavio\", \"r](https://github.com/dotnet-architecture/eShopOnContainers/blob/dev/src/Services/Ordering/Ordering.\", \"API/Application/Behaviors/ValidatorBehavior.cs) class that relies on the [FluentValidation](https://\", \"github.com/JeremySkinner/FluentValidation) library, as shown in the following code:\\n\\n```\\npublic clas\", \"s ValidatorBehavior<TRequest, TResponse>\\n : IPipelineBehavior<TRequest, TResponse>\\n{\\n private readon\", \"ly IValidator<TRequest>[] _validators;\\n public ValidatorBehavior(IValidator<TRequest>[] validators) \", \"=>\\n _validators = validators;\\n public async Task<TResponse> Handle(TRequest request,\\n RequestHandler\", \"Delegate<TResponse> next)\\n {\\n var failures = _validators\\n .Select(v => v.Validate(request))\\n .Select\", \"Many(result => result.Errors)\\n .Where(error => error != null)\\n .ToList();\\n if (failures.Any())\\n {\\n t\", \"hrow new OrderingDomainException(\\n $\\\"Command Validation Errors for type {typeof(TRequest).Name}\\\",\\n n\", \"ew ValidationException(\\\"Validation exception\\\", failures));\\n }\\n var response = await next();\\n return \", \"response;\\n }\\n}\\n```\\n\\nHere the behavior is raising an exception if validation fails, but you could als\", \"o return a result object, containing the command result if it succeeded or the validation messages i\", \"n case it didn't. This would probably make it easier to display validation results to the user.\\n\\nThe\", \"n, based on the [FluentValidation](https://github.com/JeremySkinner/FluentValidation) library, you w\", \"ould create validation for the data passed with CreateOrderCommand, as in the following code:\\n\\n```\\np\", \"ublic class CreateOrderCommandValidator : AbstractValidator<CreateOrderCommand>\\n{\\n public CreateOrde\", \"rCommandValidator()\\n {\\n RuleFor(command => command.City).NotEmpty();\\n RuleFor(command => command.Str\", \"eet).NotEmpty();\\n```\\n\\n```\\n RuleFor(command => command.State).NotEmpty();\\n RuleFor(command => command\", \".Country).NotEmpty();\\n RuleFor(command => command.ZipCode).NotEmpty();\\n RuleFor(command => command.C\", \"ardNumber).NotEmpty().Length(12, 19);\\n RuleFor(command => command.CardHolderName).NotEmpty();\\n RuleF\", \"or(command =>\\ncommand.CardExpiration).NotEmpty().Must(BeValidExpirationDate).WithMessage(\\\"Please spe\", \"cify \\na valid card expiration date\\\");\\n RuleFor(command => command.CardSecurityNumber).NotEmpty().Len\", \"gth(3);\\n RuleFor(command => command.CardTypeId).NotEmpty();\\n RuleFor(command => command.OrderItems).\", \"Must(ContainOrderItems).WithMessage(\\\"No \\norder items found\\\");\\n }\\n private bool BeValidExpirationDate\", \"(DateTime dateTime)\\n {\\n return dateTime >= DateTime.UtcNow;\\n }\\n private bool ContainOrderItems(IEnum\", \"erable<OrderItemDTO> orderItems)\\n {\\n return orderItems.Any();\\n }\\n}\\n```\\n\\nYou could create additional \", \"validations. This is a very clean and elegant way to implement your command validations.\\n\\nIn a simil\", \"ar way, you could implement other behaviors for additional aspects or cross-cutting concerns that yo\", \"u want to apply to commands when handling them.\\n\\n### **Additional resources**\\n\\n### The mediator patt\", \"ern\\n\\n\\u2022 **Mediator pattern** [https://en.wikipedia.org/wiki/Mediator\\\\\\\\_pattern](https://en.wikipedia.\", \"org/wiki/Mediator_pattern)\\n\\n### The decorator pattern\\n\\n\\u2022 **Decorator pattern** [https://en.wikipedia\", \".org/wiki/Decorator\\\\\\\\_pattern](https://en.wikipedia.org/wiki/Decorator_pattern)\\n\\n### MediatR (Jimmy \", \"Bogard)\\n\\n- **MediatR.** GitHub repo. <https://github.com/jbogard/MediatR>\\n- **CQRS with MediatR and \", \"AutoMapper** <https://lostechies.com/jimmybogard/2015/05/05/cqrs-with-mediatr-and-automapper/>\\n- **P\", \"ut your controllers on a diet: POSTs and commands.** [https://lostechies.com/jimmybogard/2013/12/19/\", \"put-your-controllers-on-a-diet-posts-and](https://lostechies.com/jimmybogard/2013/12/19/put-your-con\", \"trollers-on-a-diet-posts-and-commands/)[commands/](https://lostechies.com/jimmybogard/2013/12/19/put\", \"-your-controllers-on-a-diet-posts-and-commands/)\\n\\n- **Tackling cross-cutting concerns with a mediato\", \"r pipeline** [https://lostechies.com/jimmybogard/2014/09/09/tackling-cross-cutting-concerns-with-a](\", \"https://lostechies.com/jimmybogard/2014/09/09/tackling-cross-cutting-concerns-with-a-mediator-pipeli\", \"ne/)[mediator-pipeline/](https://lostechies.com/jimmybogard/2014/09/09/tackling-cross-cutting-concer\", \"ns-with-a-mediator-pipeline/)\\n- <https://lostechies.com/jimmybogard/2016/06/01/cqrs-and-rest-the-per\", \"fect-match/> \\u2022 **MediatR Pipeline Examples**\\n- <https://lostechies.com/jimmybogard/2016/10/13/mediat\", \"r-pipeline-examples/>\\n- **Vertical Slice Test Fixtures for MediatR and ASP.NET Core** [https://loste\", \"chies.com/jimmybogard/2016/10/24/vertical-slice-test-fixtures-for-mediatr-and](https://lostechies.co\", \"m/jimmybogard/2016/10/24/vertical-slice-test-fixtures-for-mediatr-and-asp-net-core/)[asp-net-core/](\", \"https://lostechies.com/jimmybogard/2016/10/24/vertical-slice-test-fixtures-for-mediatr-and-asp-net-c\", \"ore/)\\n- **MediatR Extensions for Microsoft Dependency Injection Released** [https://lostechies.com/j\", \"immybogard/2016/07/19/mediatr-extensions-for-microsoft](https://lostechies.com/jimmybogard/2016/07/1\", \"9/mediatr-extensions-for-microsoft-dependency-injection-released/)[dependency-injection-released/](h\", \"ttps://lostechies.com/jimmybogard/2016/07/19/mediatr-extensions-for-microsoft-dependency-injection-r\", \"eleased/)\\n\\n### Fluent validation\\n\\n\\u2022 **Jeremy Skinner. FluentValidation.** GitHub repo. <https://gith\", \"ub.com/JeremySkinner/FluentValidation>\\n\\n\\u2022 **CQRS and REST: the perfect match**\\n\\n*CHAPTER* 7\\n\\n# <span\", \" id=\\\"page-305-0\\\"></span>Implement resilient applications\\n\\n*Your microservice and cloud-based applica\", \"tions must embrace the partial failures that will certainly occur eventually. You must design your a\", \"pplication to be resilient to those partial failures.*\\n\\nResiliency is the ability to recover from fa\", \"ilures and continue to function. It isn't about avoiding failures but accepting the fact that failur\", \"es will happen and responding to them in a way that avoids downtime or data loss. The goal of resili\", \"ency is to return the application to a fully functioning state after a failure.\\n\\nIt's challenging en\", \"ough to design and deploy a microservices-based application. But you also need to keep your applicat\", \"ion running in an environment where some sort of failure is certain. Therefore, your application sho\", \"uld be resilient. It should be designed to cope with partial failures, like network outages or nodes\", \" or VMs crashing in the cloud. Even microservices (containers) being moved to a different node withi\", \"n a cluster can cause intermittent short failures within the application.\\n\\nThe many individual compo\", \"nents of your application should also incorporate health monitoring features. By following the guide\", \"lines in this chapter, you can create an application that can work smoothly in spite of transient do\", \"wntime or the normal hiccups that occur in complex and cloud-based deployments.\\n\\n### **Important**\\n\\n\", \"eShopOnContainer had been using the [Polly library](https://thepollyproject.azurewebsites.net/) to i\", \"mplement resiliency using Typed Clients up until the release 3.0.0.\\n\\nStarting with release 3.0.0, th\", \"e HTTP calls resiliency is implemented using a [Linkerd mesh,](https://linkerd.io/) that handles ret\", \"ries in a transparent and configurable fashion, within a Kubernetes cluster, without having to handl\", \"e those concerns in the code.\\n\\nThe Polly library is still used to add resilience to database connect\", \"ions, specially while starting up the services.\\n\\n### **Warning**\\n\\nAll code samples and images in thi\", \"s section were valid before using Linkerd and are not updated to reflect the current actual code. So\", \" they make sense in the context of this section.\\n\\n## <span id=\\\"page-306-0\\\"></span>Handle partial fai\", \"lure\\n\\nIn distributed systems like microservices-based applications, there's an ever-present risk of \", \"partial failure. For instance, a single microservice/container can fail or might not be available to\", \" respond for a short time, or a single VM or server can crash. Since clients and services are separa\", \"te processes, a service might not be able to respond in a timely way to a client's request. The serv\", \"ice might be overloaded and responding very slowly to requests or might simply not be accessible for\", \" a short time because of network issues.\\n\\nFor example, consider the Order details page from the eSho\", \"pOnContainers sample application. If the ordering microservice is unresponsive when the user tries t\", \"o submit an order, a bad implementation of the client process (the MVC web application)\\u2014for example,\", \" if the client code were to use synchronous RPCs with no timeout\\u2014would block threads indefinitely wa\", \"iting for a response. Besides creating a bad user experience, every unresponsive wait consumes or bl\", \"ocks a thread, and threads are extremely valuable in highly scalable applications. If there are many\", \" blocked threads, eventually the application's runtime can run out of threads. In that case, the app\", \"lication can become globally unresponsive instead of just partially unresponsive, as shown in Figure\", \" 8-1.\\n\\n![](_page_306_Figure_4.jpeg)\\n\\n*Figure 8-1. Partial failures because of dependencies that impa\", \"ct service thread availability*\\n\\nIn a large microservices-based application, any partial failure can\", \" be amplified, especially if most of the internal microservices interaction is based on synchronous \", \"HTTP calls (which is considered an antipattern). Think about a system that receives millions of inco\", \"ming calls per day. If your system has a bad design that's based on long chains of synchronous HTTP \", \"calls, these incoming calls might result in many more millions of outgoing calls (let's suppose a ra\", \"tio of 1:4) to dozens of internal microservices as synchronous dependencies. This situation is shown\", \" in Figure 8-2, especially dependency #3, that starts a chain, calling dependency #4, which then cal\", \"ls #5.\\n\\n![](_page_307_Figure_1.jpeg)\\n\\nIntermittent failure is guaranteed in a distributed and cloud-\", \"based system, even if every dependency itself has excellent availability. It's a fact you need to co\", \"nsider.\\n\\nIf you do not design and implement techniques to ensure fault tolerance, even small downtim\", \"es can be amplified. As an example, 50 dependencies each with 99.99% of availability would result in\", \" several hours of downtime each month because of this ripple effect. When a microservice dependency \", \"fails while handling a high volume of requests, that failure can quickly saturate all available requ\", \"est threads in each service and crash the whole application.\\n\\n![](_page_307_Figure_6.jpeg)\\n\\n*Figure \", \"8-3. Partial failure amplified by microservices with long chains of synchronous HTTP calls*\\n\\nTo mini\", \"mize this problem, in the section Asynchronous microservice integration enforce microservice's auton\", \"omy, this guide encourages you to use asynchronous communication across the internal microservices.\\n\", \"\\nIn addition, it's essential that you design your microservices and client applications to handle pa\", \"rtial failures\\u2014that is, to build resilient microservices and client applications.\\n\\n## <span id=\\\"page\", \"-308-0\\\"></span>Strategies to handle partial failure\\n\\nTo deal with partial failures, use one of the s\", \"trategies described here.\\n\\n**Use asynchronous communication (for example, message-based communicatio\", \"n) across internal microservices**. It's highly advisable not to create long chains of synchronous H\", \"TTP calls across the internal microservices because that incorrect design will eventually become the\", \" main cause of bad outages. On the contrary, except for the front-end communications between the cli\", \"ent applications and the first level of microservices or fine-grained API Gateways, it's recommended\", \" to use only asynchronous (message-based) communication once past the initial request/response cycle\", \", across the internal microservices. Eventual consistency and event-driven architectures will help t\", \"o minimize ripple effects. These approaches enforce a higher level of microservice autonomy and ther\", \"efore prevent against the problem noted here.\\n\\n**Use retries with exponential backoff**. This techni\", \"que helps to avoid short and intermittent failures by performing call retries a certain number of ti\", \"mes, in case the service was not available only for a short time. This might occur due to intermitte\", \"nt network issues or when a microservice/container is moved to a different node in a cluster. Howeve\", \"r, if these retries are not designed properly with circuit breakers, it can aggravate the ripple eff\", \"ects, ultimately even causing a [Denial of Service \\\\(DoS\\\\).](https://en.wikipedia.org/wiki/Denial-of\", \"-service_attack)\\n\\n**Work around network timeouts**. In general, clients should be designed not to bl\", \"ock indefinitely and to always use timeouts when waiting for a response. Using timeouts ensures that\", \" resources are never tied up indefinitely.\\n\\n**Use the Circuit Breaker pattern**. In this approach, t\", \"he client process tracks the number of failed requests. If the error rate exceeds a configured limit\", \", a \\\"circuit breaker\\\" trips so that further attempts fail immediately. (If a large number of request\", \"s are failing, that suggests the service is unavailable and that sending requests is pointless.) Aft\", \"er a timeout period, the client should try again and, if the new requests are successful, close the \", \"circuit breaker.\\n\\n**Provide fallbacks**. In this approach, the client process performs fallback logi\", \"c when a request fails, such as returning cached data or a default value. This is an approach suitab\", \"le for queries, and is more complex for updates or commands.\\n\\n**Limit the number of queued requests*\", \"*. Clients should also impose an upper bound on the number of outstanding requests that a client mic\", \"roservice can send to a particular service. If the limit has been reached, it's probably pointless t\", \"o make additional requests, and those attempts should fail immediately. In terms of implementation, \", \"the Polly [Bulkhead Isolation](https://github.com/App-vNext/Polly/wiki/Bulkhead) policy can be used \", \"to fulfill this requirement. This approach is essentially a parallelization throttle with [Semaphore\", \"Slim](https://docs.microsoft.com/dotnet/api/system.threading.semaphoreslim) as the implementation. I\", \"t also permits a \\\"queue\\\" outside the bulkhead. You can proactively shed excess load even before exec\", \"ution (for example, because capacity is deemed full). This makes its response to\\n\\ncertain failure sc\", \"enarios faster than a circuit breaker would be, since the circuit breaker waits for the failures. Th\", \"e BulkheadPolicy object in [Polly](https://thepollyproject.azurewebsites.net/) exposes how full the \", \"bulkhead and queue are, and offers events on overflow so can also be used to drive automated horizon\", \"tal scaling.\\n\\n## <span id=\\\"page-309-0\\\"></span>**Additional resources**\\n\\n- **Resiliency patterns** [h\", \"ttps://learn.microsoft.com/azure/architecture/framework/resiliency/reliability-patterns](https://doc\", \"s.microsoft.com/azure/architecture/framework/resiliency/reliability-patterns)\\n- [https://learn.micro\", \"soft.com/previous-versions/msp-n-p/jj591574\\\\(v=pandp.10\\\\)](https://docs.microsoft.com/previous-versi\", \"ons/msp-n-p/jj591574(v=pandp.10))\\n- <https://github.com/App-vNext/Polly/wiki/Bulkhead>\\n\\n\\u2022 **Bulkhead\", \".** GitHub repo. Implementation with Polly policy.\\n\\n\\u2022 **Adding Resilience and Optimizing Performance\", \"**\\n\\n- **Designing resilient applications for Azure** [https://learn.microsoft.com/azure/architecture\", \"/framework/resiliency/app-design](https://docs.microsoft.com/azure/architecture/framework/resiliency\", \"/app-design)\\n- **Transient fault handling** [https://learn.microsoft.com/azure/architecture/best-pra\", \"ctices/transient-faults](https://docs.microsoft.com/azure/architecture/best-practices/transient-faul\", \"ts)\\n\\n## <span id=\\\"page-309-1\\\"></span>Implement retries with exponential backoff\\n\\n*[Retries with expo\", \"nential backoff](https://docs.microsoft.com/azure/architecture/patterns/retry)* is a technique that \", \"retries an operation, with an exponentially increasing wait time, up to a maximum retry count has be\", \"en reached (the [exponential backoff\\\\)](https://en.wikipedia.org/wiki/Exponential_backoff). This tec\", \"hnique embraces the fact that cloud resources might intermittently be unavailable for more than a fe\", \"w seconds for any reason. For example, an orchestrator might be moving a container to another node i\", \"n a cluster for load balancing. During that time, some requests might fail. Another example could be\", \" a database like SQL Azure, where a database can be moved to another server for load balancing, caus\", \"ing the database to be unavailable for a few seconds.\\n\\n<span id=\\\"page-309-2\\\"></span>There are many a\", \"pproaches to implement retries logic with exponential backoff.\\n\\n## Implement resilient Entity Framew\", \"ork Core SQL connections\\n\\nFor Azure SQL DB, Entity Framework (EF) Core already provides internal dat\", \"abase connection resiliency and retry logic. But you need to enable the Entity Framework execution s\", \"trategy for each [DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbc\", \"ontext) connection if you want to have [resilient EF Core connections.](https://docs.microsoft.com/e\", \"f/core/miscellaneous/connection-resiliency)\\n\\nFor instance, the following code at the EF Core connect\", \"ion level enables resilient SQL connections that are retried if the connection fails.\\n\\n```\\n// Progra\", \"m.cs from any ASP.NET Core Web API\\n// Other code ...\\nbuilder.Services.AddDbContext<CatalogContext>(o\", \"ptions =>\\n {\\n```\\n\\n```\\n options.UseSqlServer(builder.Configuration[\\\"ConnectionString\\\"],\\n sqlServerOpt\", \"ionsAction: sqlOptions =>\\n {\\n sqlOptions.EnableRetryOnFailure(\\n maxRetryCount: 10,\\n maxRetryDelay: T\", \"imeSpan.FromSeconds(30),\\n errorNumbersToAdd: null);\\n });\\n });\\n```\\n\\n## <span id=\\\"page-310-0\\\"></span>*\", \"*Execution strategies and explicit transactions using BeginTransaction and multiple DbContexts**\\n\\nWh\", \"en retries are enabled in EF Core connections, each operation you perform using EF Core becomes its \", \"own retryable operation. Each query and each call to SaveChanges will be retried as a unit if a tran\", \"sient failure occurs.\\n\\nHowever, if your code initiates a transaction using BeginTransaction, you're \", \"defining your own group of operations that need to be treated as a unit. Everything inside the trans\", \"action has to be rolled back if a failure occurs.\\n\\nIf you try to execute that transaction when using\", \" an EF execution strategy (retry policy) and you call SaveChanges from multiple DbContexts, you'll g\", \"et an exception like this one:\\n\\nSystem.InvalidOperationException: The configured execution strategy\\n\", \"\\n'SqlServerRetryingExecutionStrategy' does not support user initiated transactions. Use the executio\", \"n strategy returned by 'DbContext.Database.CreateExecutionStrategy()' to execute all the operations \", \"in the transaction as a retriable unit.\\n\\nThe solution is to manually invoke the EF execution strateg\", \"y with a delegate representing everything that needs to be executed. If a transient failure occurs, \", \"the execution strategy will invoke the delegate again. For example, the following code shows how it'\", \"s implemented in eShopOnContainers with two multiple DbContexts (\\\\_catalogContext and the Integratio\", \"nEventLogContext) when updating a product and then saving the ProductPriceChangedIntegrationEvent ob\", \"ject, which needs to use a different DbContext.\\n\\n```\\npublic async Task<IActionResult> UpdateProduct(\", \"\\n [FromBody]CatalogItem productToUpdate)\\n{\\n // Other code ...\\n var oldPrice = catalogItem.Price;\\n va\", \"r raiseProductPriceChangedEvent = oldPrice != productToUpdate.Price;\\n // Update current product\\n cat\", \"alogItem = productToUpdate;\\n // Save product's data and publish integration event through the Event \", \"Bus\\n // if price has changed\\n if (raiseProductPriceChangedEvent)\\n {\\n //Create Integration Event to b\", \"e published through the Event Bus\\n var priceChangedEvent = new ProductPriceChangedIntegrationEvent(\\n\", \" catalogItem.Id, productToUpdate.Price, oldPrice);\\n```\\n\\n```\\n // Achieving atomicity between original\", \" Catalog database operation and the\\n // IntegrationEventLog thanks to a local transaction\\n await _ca\", \"talogIntegrationEventService.SaveEventAndCatalogContextChangesAsync(\\n priceChangedEvent);\\n // Publis\", \"h through the Event Bus and mark the saved event as published\\n await _catalogIntegrationEventService\", \".PublishThroughEventBusAsync(\\n priceChangedEvent);\\n }\\n // Just save the updated product because the \", \"Product's Price hasn't changed.\\n else\\n {\\n await _catalogContext.SaveChangesAsync();\\n }\\n}\\n```\\n\\nThe fi\", \"rst [DbContext](https://docs.microsoft.com/dotnet/api/microsoft.entityframeworkcore.dbcontext) is \\\\_\", \"catalogContext and the second DbContext is within the \\\\_catalogIntegrationEventService object. The C\", \"ommit action is performed across all DbContext objects using an EF execution strategy.\\n\\nTo achieve t\", \"his multiple DbContext commit, the SaveEventAndCatalogContextChangesAsync uses a ResilientTransactio\", \"n class, as shown in the following code:\\n\\n```\\npublic class CatalogIntegrationEventService : ICatalog\", \"IntegrationEventService\\n{\\n //\\u2026\\n public async Task SaveEventAndCatalogContextChangesAsync(\\n Integrati\", \"onEvent evt)\\n {\\n // Use of an EF Core resiliency strategy when using multiple DbContexts\\n // within \", \"an explicit BeginTransaction():\\n // https://learn.microsoft.com/ef/core/miscellaneous/connection-res\", \"iliency\\n await ResilientTransaction.New(_catalogContext).ExecuteAsync(async () =>\\n {\\n // Achieving a\", \"tomicity between original catalog database\\n // operation and the IntegrationEventLog thanks to a loc\", \"al transaction\\n await _catalogContext.SaveChangesAsync();\\n await _eventLogService.SaveEventAsync(evt\", \",\\n _catalogContext.Database.CurrentTransaction.GetDbTransaction());\\n });\\n }\\n}\\n```\\n\\nThe ResilientTran\", \"saction.ExecuteAsync method basically begins a transaction from the passed DbContext (\\\\_catalogConte\", \"xt) and then makes the EventLogService use that transaction to save changes from the IntegrationEven\", \"tLogContext and then commits the whole transaction.\\n\\n```\\npublic class ResilientTransaction\\n{\\n privat\", \"e DbContext _context;\\n private ResilientTransaction(DbContext context) =>\\n _context = context ?? thr\", \"ow new ArgumentNullException(nameof(context));\\n public static ResilientTransaction New (DbContext co\", \"ntext) =>\\n new ResilientTransaction(context);\\n public async Task ExecuteAsync(Func<Task> action)\\n {\\n\", \"```\\n\\n```\\n // Use of an EF Core resiliency strategy when using multiple DbContexts\\n // within an expl\", \"icit BeginTransaction():\\n // https://learn.microsoft.com/ef/core/miscellaneous/connection-resiliency\", \"\\n var strategy = _context.Database.CreateExecutionStrategy();\\n await strategy.ExecuteAsync(async () \", \"=>\\n {\\n await using var transaction = await _context.Database.BeginTransactionAsync();\\n await action(\", \");\\n await transaction.CommitAsync();\\n });\\n }\\n}\\n```\\n\\n## <span id=\\\"page-312-0\\\"></span>**Additional res\", \"ources**\\n\\n\\u2022 **Connection Resiliency and Command Interception with EF in an ASP.NET MVC Application**\", \" [https://learn.microsoft.com/aspnet/mvc/overview/getting-started/getting-started-with-ef-](https://\", \"docs.microsoft.com/aspnet/mvc/overview/getting-started/getting-started-with-ef-using-mvc/connection-\", \"resiliency-and-command-interception-with-the-entity-framework-in-an-asp-net-mvc-application)\\n\\n[using\", \"-mvc/connection-resiliency-and-command-interception-with-the-entity-framework-in](https://docs.micro\", \"soft.com/aspnet/mvc/overview/getting-started/getting-started-with-ef-using-mvc/connection-resiliency\", \"-and-command-interception-with-the-entity-framework-in-an-asp-net-mvc-application)[an-asp-net-mvc-ap\", \"plication](https://docs.microsoft.com/aspnet/mvc/overview/getting-started/getting-started-with-ef-us\", \"ing-mvc/connection-resiliency-and-command-interception-with-the-entity-framework-in-an-asp-net-mvc-a\", \"pplication)\\n\\n\\u2022 **Cesar de la Torre. Using Resilient Entity Framework Core SQL Connections and Transa\", \"ctions**\\n\\n[https://devblogs.microsoft.com/cesardelatorre/using-resilient-entity-framework-core-sql](\", \"https://devblogs.microsoft.com/cesardelatorre/using-resilient-entity-framework-core-sql-connections-\", \"and-transactions-retries-with-exponential-backoff/)[connections-and-transactions-retries-with-expone\", \"ntial-backoff/](https://devblogs.microsoft.com/cesardelatorre/using-resilient-entity-framework-core-\", \"sql-connections-and-transactions-retries-with-exponential-backoff/)\\n\\n## <span id=\\\"page-312-1\\\"></span\", \">Use IHttpClientFactory to implement resilient HTTP requests\\n\\n[IHttpClientFactory](https://docs.micr\", \"osoft.com/dotnet/api/system.net.http.ihttpclientfactory) is a contract implemented by DefaultHttpCli\", \"entFactory, an opinionated factory, available since .NET Core 2.1, for creating [HttpClient](https:/\", \"/docs.microsoft.com/dotnet/api/system.net.http.httpclient) instances to be used in your applications\", \".\\n\\n### <span id=\\\"page-312-2\\\"></span>**Issues with the original HttpClient class available in .NET**\\n\", \"\\nThe original and well-known [HttpClient](https://docs.microsoft.com/dotnet/api/system.net.http.http\", \"client) class can be easily used, but in some cases, it isn't being properly used by many developers\", \".\\n\\nThough this class implements IDisposable, declaring and instantiating it within a using statement\", \" is not preferred because when the HttpClient object gets disposed of, the underlying socket is not \", \"immediately released, which can lead to a *socket exhaustion* problem. For more information about th\", \"is issue, see the blog post [You're using HttpClient wrong and it's destabilizing your software](htt\", \"ps://aspnetmonsters.com/2016/08/2016-08-27-httpclientwrong/).\\n\\nTherefore, HttpClient is intended to \", \"be instantiated once and reused throughout the life of an application. Instantiating an HttpClient c\", \"lass for every request will exhaust the number of sockets available under heavy loads. That issue wi\", \"ll result in SocketException errors. Possible approaches to solve that problem are based on the crea\", \"tion of the HttpClient object as singleton or static, as explained in this [Microsoft article on Htt\", \"pClient usage.](https://docs.microsoft.com/dotnet/csharp/tutorials/console-webapiclient) This can be\", \" a good solution for short-lived console apps or similar, that run a few times a day.\\n\\nAnother issue\", \" that developers run into is when using a shared instance of HttpClient in long-running processes. I\", \"n a situation where the HttpClient is instantiated as a singleton or a static object, it fails to ha\", \"ndle the DNS changes as described in this [issue](https://github.com/dotnet/runtime/issues/18348) of\", \" the dotnet/runtime GitHub repository.\\n\\nbecause it creates a new concrete instance of [HttpMessageHa\", \"ndler,](https://docs.microsoft.com/dotnet/api/system.net.http.httpmessagehandler) which is the one t\", \"hat has *sockets exhaustion* and DNS changes issues mentioned above.\\n\\nHowever, the issue isn't reall\", \"y with HttpClient per se, but with the [default constructor for HttpClient,](https://docs.microsoft.\", \"com/dotnet/api/system.net.http.httpclient.-ctor#system-net-http-httpclient-ctor) \\n\\nTo address the is\", \"sues mentioned above and to make HttpClient instances manageable, .NET Core 2.1 introduced two appro\", \"aches, one of them being [IHttpClientFactory](https://docs.microsoft.com/dotnet/api/system.net.http.\", \"ihttpclientfactory). It's an interface that's used to configure and create HttpClient instances in a\", \"n app through Dependency Injection (DI). It also provides extensions for Polly-based middleware to t\", \"ake advantage of delegating handlers in HttpClient.\\n\\nThe alternative is to use SocketsHttpHandler wi\", \"th configured PooledConnectionLifetime. This approach is applied to long-lived, static or singleton \", \"HttpClient instances. To learn more about different strategies, see [HttpClient guidelines for .NET.\", \"](https://docs.microsoft.com/dotnet/fundamentals/networking/http/httpclient-guidelines)\\n\\n[Polly](htt\", \"ps://thepollyproject.azurewebsites.net/) is a transient-fault-handling library that helps developers\", \" add resiliency to their applications, by using some pre-defined policies in a fluent and thread-saf\", \"e manner.\\n\\n## <span id=\\\"page-313-0\\\"></span>**Benefits of using IHttpClientFactory**\\n\\nThe current imp\", \"lementation of [IHttpClientFactory,](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclie\", \"ntfactory) that also implements [IHttpMessageHandlerFactory,](https://docs.microsoft.com/dotnet/api/\", \"system.net.http.ihttpmessagehandlerfactory)  offers the following benefits:\\n\\n- Provides a central lo\", \"cation for naming and configuring logical HttpClient objects. For example, you may configure a clien\", \"t (Service Agent) that's pre-configured to access a specific microservice.\\n- Codify the concept of o\", \"utgoing middleware via delegating handlers in HttpClient and implementing Polly-based middleware to \", \"take advantage of Polly's policies for resiliency.\\n- HttpClient already has the concept of delegatin\", \"g handlers that could be linked together for outgoing HTTP requests. You can register HTTP clients i\", \"nto the factory and you can use a Polly handler to use Polly policies for Retry, CircuitBreakers, an\", \"d so on.\\n- Manage the lifetime of [HttpMessageHandler](https://docs.microsoft.com/dotnet/api/system.\", \"net.http.httpmessagehandler) to avoid the mentioned problems/issues that can occur when managing Htt\", \"pClient lifetimes yourself.\\n\\n### **Tip**\\n\\nThe HttpClient instances injected by DI can be disposed of\", \" safely, because the associated HttpMessageHandler is managed by the factory. Injected HttpClient in\", \"stances are *Transient* from a DI perspective, while HttpMessageHandler instances can be regarded as\", \" *Scoped*. HttpMessageHandler instances have their own DI scopes, **separate** from the application \", \"scopes (for example, ASP.NET incoming request scopes). For more information, see [Using HttpClientFa\", \"ctory in .NET.](https://docs.microsoft.com/dotnet/core/extensions/httpclient-factory#message-handler\", \"-scopes-in-ihttpclientfactory)\\n\\n### **Note**\\n\\nThe implementation of IHttpClientFactory (DefaultHttpC\", \"lientFactory) is tightly tied to the DI implementation in the Microsoft.Extensions.DependencyInjecti\", \"on NuGet package. If you need to use HttpClient without DI or with other DI implementations, conside\", \"r using a static or singleton HttpClient with PooledConnectionLifetime set up. For more information,\", \" see [HttpClient guidelines for .NET.](https://docs.microsoft.com/dotnet/fundamentals/networking/htt\", \"p/httpclient-guidelines)\\n\\n## <span id=\\\"page-314-0\\\"></span>**Multiple ways to use IHttpClientFactory*\", \"*\\n\\nThere are several ways that you can use IHttpClientFactory in your application:\\n\\n- Basic usage \\u2022 \", \"Use Named Clients\\n- Use Typed Clients\\n- Use Generated Clients\\n\\nFor the sake of brevity, this guidanc\", \"e shows the most structured way to use IHttpClientFactory, which is to use Typed Clients (Service Ag\", \"ent pattern). However, all options are documented and are currently listed in this [article covering\", \" the IHttpClientFactory](https://docs.microsoft.com/aspnet/core/fundamentals/http-requests#consumpti\", \"on-patterns) usage.\\n\\n### **Note**\\n\\nIf your app requires cookies, it might be better to avoid using [\", \"IHttpClientFactory](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclientfactory) in you\", \"r app. For alternative ways of managing clients, see [Guidelines for using HTTP clients](https://doc\", \"s.microsoft.com/dotnet/fundamentals/networking/http/httpclient-guidelines)\\n\\n## <span id=\\\"page-314-1\\\"\", \"></span>**How to use Typed Clients with IHttpClientFactory**\\n\\nSo, what's a \\\"Typed Client\\\"? It's just\", \" an HttpClient that's pre-configured for some specific use. This configuration can include specific \", \"values such as the base server, HTTP headers or time outs.\\n\\nThe following diagram shows how Typed Cl\", \"ients are used with IHttpClientFactory:\\n\\n![](_page_315_Figure_0.jpeg)\\n\\n*Figure 8-4. Using IHttpClien\", \"tFactory with Typed Client classes.*\\n\\nIn the above image, a ClientService (used by a controller or c\", \"lient code) uses an HttpClient created by the registered IHttpClientFactory. This factory assigns an\", \" HttpMessageHandler from a pool to the HttpClient. The HttpClient can be configured with Polly's pol\", \"icies when registering the IHttpClientFactory in the DI container with the extension method [AddHttp\", \"Client.](https://docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.httpclientfa\", \"ctoryservicecollectionextensions.addhttpclient)\\n\\nTo configure the above structure, add [IHttpClientF\", \"actory](https://docs.microsoft.com/dotnet/api/system.net.http.ihttpclientfactory) in your applicatio\", \"n by installing the Microsoft.Extensions.Http NuGet package that includes the [AddHttpClient](https:\", \"//docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.httpclientfactoryservicecol\", \"lectionextensions.addhttpclient) extension method for [IServiceCollection.](https://docs.microsoft.c\", \"om/dotnet/api/microsoft.extensions.dependencyinjection.iservicecollection) This extension method reg\", \"isters the internal DefaultHttpClientFactory class to be used as a singleton for the interface IHttp\", \"ClientFactory. It defines a transient configuration for the [HttpMessageHandlerBuilder.](https://doc\", \"s.microsoft.com/dotnet/api/microsoft.extensions.http.httpmessagehandlerbuilder) This message handler\", \" [\\\\(HttpMessageHandler](https://docs.microsoft.com/dotnet/api/system.net.http.httpmessagehandler) ob\", \"ject), taken from a pool, is used by the HttpClient returned from the factory.\\n\\nIn the next snippet,\", \" you can see how AddHttpClient() can be used to register Typed Clients (Service Agents) that need to\", \" use HttpClient.\\n\\n```\\n// Program.cs\\n//Add http client services at ConfigureServices(IServiceCollecti\", \"on services)\\nbuilder.Services.AddHttpClient<ICatalogService, CatalogService>();\\nbuilder.Services.Add\", \"HttpClient<IBasketService, BasketService>();\\nbuilder.Services.AddHttpClient<IOrderingService, Orderi\", \"ngService>();\\n```\\n\\nRegistering the client services as shown in the previous snippet, makes the Defau\", \"ltClientFactory create a standard HttpClient for each service. The typed client is registered as tra\", \"nsient with DI container. In the preceding code, AddHttpClient() registers *CatalogService*, *Basket\", \"Service*, *OrderingService* as transient services so they can be injected and consumed directly with\", \"out any need for additional registrations.\\n\\nYou could also add instance-specific configuration in th\", \"e registration to, for example, configure the base address, and add some resiliency policies, as sho\", \"wn in the following:\\n\\n```\\nbuilder.Services.AddHttpClient<ICatalogService, CatalogService>(client =>\\n\", \"{\\n client.BaseAddress = new Uri(builder.Configuration[\\\"BaseUrl\\\"]);\\n})\\n .AddPolicyHandler(GetRetryPol\", \"icy())\\n .AddPolicyHandler(GetCircuitBreakerPolicy());\\n```\\n\\nIn this next example, you can see the con\", \"figuration of one of the above policies:\\n\\n```\\nstatic IAsyncPolicy<HttpResponseMessage> GetRetryPolic\", \"y()\\n{\\n return HttpPolicyExtensions\\n .HandleTransientHttpError()\\n .OrResult(msg => msg.StatusCode == \", \"System.Net.HttpStatusCode.NotFound)\\n .WaitAndRetryAsync(6, retryAttempt => TimeSpan.FromSeconds(Math\", \".Pow(2,\\nretryAttempt)));\\n}\\n```\\n\\nYou can find more details about using Polly in the Next article.\\n\\n##\", \"# **HttpClient lifetimes**\\n\\nEach time you get an HttpClient object from the IHttpClientFactory, a ne\", \"w instance is returned. But each HttpClient uses an HttpMessageHandler that's pooled and reused by t\", \"he IHttpClientFactory to reduce resource consumption, as long as the HttpMessageHandler's lifetime h\", \"asn't expired.\\n\\nPooling of handlers is desirable as each handler typically manages its own underlyin\", \"g HTTP connections; creating more handlers than necessary can result in connection delays. Some hand\", \"lers also keep connections open indefinitely, which can prevent the handler from reacting to DNS cha\", \"nges.\\n\\nThe HttpMessageHandler objects in the pool have a lifetime that's the length of time that an \", \"HttpMessageHandler instance in the pool can be reused. The default value is two minutes, but it can \", \"be overridden per Typed Client. To override it, call SetHandlerLifetime() on the [IHttpClientBuilder\", \"](https://docs.microsoft.com/dotnet/api/microsoft.extensions.dependencyinjection.ihttpclientbuilder)\", \" that's returned when creating the client, as shown in the following code:\\n\\n```\\n//Set 5 min as the l\", \"ifetime for the HttpMessageHandler objects in the pool used for the \\nCatalog Typed Client\\nbuilder.Se\", \"rvices.AddHttpClient<ICatalogService, CatalogService>()\\n .SetHandlerLifetime(TimeSpan.FromMinutes(5)\", \");\\n```\\n\\nEach Typed Client can have its own configured handler lifetime value. Set the lifetime to In\", \"finiteTimeSpan to disable handler expiry.\\n\\n### **Implement your Typed Client classes that use the in\", \"jected and configured HttpClient** As a previous step, you need to have your Typed Client classes de\", \"fined, such as the classes in the\\n\\nsample code, like 'BasketService', 'CatalogService', 'OrderingSer\", \"vice', etc. \\u2013 A Typed Client is a class that accepts an HttpClient object (injected through its cons\", \"tructor) and uses it to call some remote HTTP service. For example:\\n\\n```\\npublic class CatalogService\", \" : ICatalogService\\n{\\n private readonly HttpClient _httpClient;\\n private readonly string _remoteServi\", \"ceBaseUrl;\\n public CatalogService(HttpClient httpClient)\\n {\\n _httpClient = httpClient;\\n }\\n public as\", \"ync Task<Catalog> GetCatalogItems(int page, int take,\\n int? brand, int? type)\\n {\\n var uri = API.Cata\", \"log.GetAllCatalogItems(_remoteServiceBaseUrl,\\n page, take, brand, type);\\n var responseString = await\", \" _httpClient.GetStringAsync(uri);\\n var catalog = JsonConvert.DeserializeObject<Catalog>(responseStri\", \"ng);\\n return catalog;\\n }\\n}\\n```\\n\\nThe Typed Client (CatalogService in the example) is activated by DI \", \"(Dependency Injection), which means it can accept any registered service in its constructor, in addi\", \"tion to HttpClient.\\n\\nA Typed Client is effectively a transient object, that means a new instance is \", \"created each time one is needed. It receives a new HttpClient instance each time it's constructed. H\", \"owever, the HttpMessageHandler objects in the pool are the objects that are reused by multiple HttpC\", \"lient instances.\\n\\n### **Use your Typed Client classes**\\n\\nFinally, once you have your typed classes i\", \"mplemented, you can have them registered and configured with AddHttpClient(). After that you can use\", \" them wherever services are injected by DI, such as in Razor page code or an MVC web app controller,\", \" shown in the below code from eShopOnContainers:\\n\\n```\\nnamespace Microsoft.eShopOnContainers.WebMVC.C\", \"ontrollers\\n{\\n public class CatalogController : Controller\\n {\\n private ICatalogService _catalogSvc;\\n \", \"public CatalogController(ICatalogService catalogSvc) =>\\n _catalogSvc = catalogSvc;\\n public async Tas\", \"k<IActionResult> Index(int? BrandFilterApplied,\\n int? TypesFilterApplied,\\n```\\n\\n```\\n int? page,\\n [Fro\", \"mQuery]string errorMsg)\\n {\\n var itemsPage = 10;\\n var catalog = await _catalogSvc.GetCatalogItems(pag\", \"e ?? 0,\\n itemsPage,\\n                               BrandFilterApplied,\\n                             \", \"  TypesFilterApplied);\\n //\\u2026 Additional code\\n }\\n }\\n}\\n```\\n\\nUp to this point, the above code snippet on\", \"ly shows the example of performing regular HTTP requests. But the 'magic' comes in the following sec\", \"tions where it shows how all the HTTP requests made by HttpClient can have resilient policies such a\", \"s retries with exponential backoff, circuit breakers, security features using auth tokens, or even a\", \"ny other custom feature. And all of these can be done just by adding policies and delegating handler\", \"s to your registered Typed Clients.\\n\\n### <span id=\\\"page-318-0\\\"></span>**Additional resources**\\n\\n- **\", \"HttpClient guidelines for .NET** [https://learn.microsoft.com/en-us/dotnet/fundamentals/networking/h\", \"ttp/httpclient](https://docs.microsoft.com/dotnet/fundamentals/networking/http/httpclient-guidelines\", \")[guidelines](https://docs.microsoft.com/dotnet/fundamentals/networking/http/httpclient-guidelines)\\n\", \"- **Using HttpClientFactory in .NET** [https://learn.microsoft.com/en-us/dotnet/core/extensions/http\", \"client-factory](https://docs.microsoft.com/dotnet/core/extensions/httpclient-factory)\\n- **Using Http\", \"ClientFactory in ASP.NET Core** [https://learn.microsoft.com/aspnet/core/fundamentals/http-requests]\", \"(https://docs.microsoft.com/aspnet/core/fundamentals/http-requests)\\n- **HttpClientFactory source cod\", \"e in the dotnet/runtime GitHub repository** <https://github.com/dotnet/runtime/tree/release/7.0/src/\", \"libraries/Microsoft.Extensions.Http/>\\n- **Polly (.NET resilience and transient-fault-handling librar\", \"y)** <https://thepollyproject.azurewebsites.net/>\\n\\n## <span id=\\\"page-318-1\\\"></span>Implement HTTP ca\", \"ll retries with exponential backoff with IHttpClientFactory and Polly policies\\n\\nThe recommended appr\", \"oach for retries with exponential backoff is to take advantage of more advanced .NET libraries like \", \"the open-source [Polly library.](https://github.com/App-vNext/Polly)\\n\\nPolly is a .NET library that p\", \"rovides resilience and transient-fault handling capabilities. You can implement those capabilities b\", \"y applying Polly policies such as Retry, Circuit Breaker, Bulkhead Isolation, Timeout, and Fallback.\", \" Polly targets .NET Framework 4.x and .NET Standard 1.0, 1.1, and 2.0 (which supports .NET Core and \", \"later).\\n\\nThe following steps show how you can use Http retries with Polly integrated into IHttpClien\", \"tFactory, which is explained in the previous section.\\n\\n## **Install .NET packages**\\n\\nFirst, you will\", \" need to install the Microsoft.Extensions.Http.Polly package.\\n\\n- [Install with Visual Studio](https:\", \"//docs.microsoft.com/nuget/consume-packages/install-use-packages-visual-studio) \\u2022 [Install with dotn\", \"et CLI](https://docs.microsoft.com/nuget/consume-packages/install-use-packages-dotnet-cli)\\n- [Instal\", \"l with nuget.exe CLI](https://docs.microsoft.com/nuget/consume-packages/install-use-packages-nuget-c\", \"li)\\n- [Install with Package Manager Console \\\\(PowerShell\\\\)](https://docs.microsoft.com/nuget/consume\", \"-packages/install-use-packages-powershell)\\n\\n## **Reference the .NET 7 packages**\\n\\nIHttpClientFactory\", \" is available since .NET Core 2.1, however, we recommend you use the latest .NET 7 packages from NuG\", \"et in your project. You typically also need to reference the extension package Microsoft.Extensions.\", \"Http.Polly.\\n\\n### **Configure a client with Polly's Retry policy, in app startup**\\n\\nThe **AddPolicyHa\", \"ndler()** method is what adds policies to the HttpClient objects you'll use. In this case, it's addi\", \"ng a Polly's policy for Http Retries with exponential backoff.\\n\\nTo have a more modular approach, the\", \" Http Retry Policy can be defined in a separate method within the *Program.cs* file, as shown in the\", \" following code:\\n\\n```\\nstatic IAsyncPolicy<HttpResponseMessage> GetRetryPolicy()\\n{\\n return HttpPolicy\", \"Extensions\\n .HandleTransientHttpError()\\n .OrResult(msg => msg.StatusCode == System.Net.HttpStatusCod\", \"e.NotFound)\\n .WaitAndRetryAsync(6, retryAttempt => TimeSpan.FromSeconds(Math.Pow(2,\\n retryAttempt)))\", \";\\n}\\n```\\n\\nAs shown in previous sections, you need to define a named or typed client HttpClient config\", \"uration in your standard *Program.cs* app configuration. Now you add incremental code specifying the\", \" policy for the Http retries with exponential backoff, as follows:\\n\\n```\\n// Program.cs\\nbuilder.Servic\", \"es.AddHttpClient<IBasketService, BasketService>()\\n .SetHandlerLifetime(TimeSpan.FromMinutes(5)) //Se\", \"t lifetime to five minutes\\n .AddPolicyHandler(GetRetryPolicy());\\n```\\n\\nWith Polly, you can define a R\", \"etry policy with the number of retries, the exponential backoff configuration, and the actions to ta\", \"ke when there's an HTTP exception, such as logging the error. In this case, the policy is configured\", \" to try six times with an exponential retry, starting at two seconds.\\n\\n### <span id=\\\"page-319-0\\\"></s\", \"pan>**Add a jitter strategy to the retry policy**\\n\\nA regular Retry policy can affect your system in \", \"cases of high concurrency and scalability and under high contention. To overcome peaks of similar re\", \"tries coming from many clients in partial outages, a good workaround is to add a jitter strategy to \", \"the retry algorithm/policy. This strategy can improve the overall performance of the end-to-end syst\", \"em. As recommended in [Polly: Retry with Jitter,](https://github.com/App-vNext/Polly/wiki/Retry-with\", \"-jitter) a good jitter strategy can be implemented by smooth and evenly distributed retry intervals \", \"applied with a well-controlled median initial retry delay on an exponential backoff. This approach h\", \"elps to spread out the spikes when the issue arises. The principle is illustrated by the following e\", \"xample:\\n\\n```\\nvar delay = Backoff.DecorrelatedJitterBackoffV2(medianFirstRetryDelay:\\nTimeSpan.FromSec\", \"onds(1), retryCount: 5);\\nvar retryPolicy = Policy\\n .Handle<FooException>()\\n .WaitAndRetryAsync(delay\", \");\\n```\\n\\n## <span id=\\\"page-320-0\\\"></span>**Additional resources**\\n\\n- **Retry pattern** [https://learn\", \".microsoft.com/azure/architecture/patterns/retry](https://docs.microsoft.com/azure/architecture/patt\", \"erns/retry)\\n- **Polly and IHttpClientFactory** [https://github.com/App-vNext/Polly/wiki/Polly-and-](\", \"https://github.com/App-vNext/Polly/wiki/Polly-and-HttpClientFactory)[HttpClientFactory](https://gith\", \"ub.com/App-vNext/Polly/wiki/Polly-and-HttpClientFactory)\\n- **Polly (.NET resilience and transient-fa\", \"ult-handling library)** [https://github.com/App](https://github.com/App-vNext/Polly)[vNext/Polly](ht\", \"tps://github.com/App-vNext/Polly)\\n- **Polly: Retry with Jitter** <https://github.com/App-vNext/Polly\", \"/wiki/Retry-with-jitter>\\n- **Marc Brooker. Jitter: Making Things Better With Randomness** <https://b\", \"rooker.co.za/blog/2015/03/21/backoff.html>\\n\\n## <span id=\\\"page-320-1\\\"></span>Implement the Circuit Br\", \"eaker pattern\\n\\nAs noted earlier, you should handle faults that might take a variable amount of time \", \"to recover from, as might happen when you try to connect to a remote service or resource. Handling t\", \"his type of fault can improve the stability and resiliency of an application.\\n\\nIn a distributed envi\", \"ronment, calls to remote resources and services can fail due to transient faults, such as slow netwo\", \"rk connections and timeouts, or if resources are responding slowly or are temporarily unavailable. T\", \"hese faults typically correct themselves after a short time, and a robust cloud application should b\", \"e prepared to handle them by using a strategy like the \\\"Retry pattern\\\".\\n\\nHowever, there can also be \", \"situations where faults are due to unanticipated events that might take much longer to fix. These fa\", \"ults can range in severity from a partial loss of connectivity to the complete failure of a service.\", \" In these situations, it might be pointless for an application to continually retry an operation tha\", \"t's unlikely to succeed.\\n\\nInstead, the application should be coded to accept that the operation has \", \"failed and handle the failure accordingly.\\n\\nUsing Http retries carelessly could result in creating a\", \" Denial of Service [\\\\(DoS\\\\)](https://en.wikipedia.org/wiki/Denial-of-service_attack) attack within y\", \"our own software. As a microservice fails or performs slowly, multiple clients might repeatedly retr\", \"y failed requests. That creates a dangerous risk of exponentially increasing traffic targeted at the\", \" failing service.\\n\\nTherefore, you need some kind of defense barrier so that excessive requests stop \", \"when it isn't worth to keep trying. That defense barrier is precisely the circuit breaker.\\n\\nThe Circ\", \"uit Breaker pattern has a different purpose than the \\\"Retry pattern\\\". The \\\"Retry pattern\\\"\\n\\nenables a\", \"n application to retry an operation in the expectation that the operation will eventually succeed. T\", \"he Circuit Breaker pattern prevents an application from performing an operation that's likely to fai\", \"l. An application can combine these two patterns. However, the retry logic should be sensitive to an\", \"y exception returned by the circuit breaker, and it should abandon retry attempts if the circuit bre\", \"aker indicates that a fault is not transient.\\n\\n## <span id=\\\"page-321-0\\\"></span>**Implement Circuit B\", \"reaker pattern with IHttpClientFactory and Polly**\\n\\nAs when implementing retries, the recommended ap\", \"proach for circuit breakers is to take advantage of proven .NET libraries like Polly and its native \", \"integration with IHttpClientFactory.\\n\\nAdding a circuit breaker policy into your IHttpClientFactory o\", \"utgoing middleware pipeline is as simple as adding a single incremental piece of code to what you al\", \"ready have when using IHttpClientFactory.\\n\\nThe only addition here to the code used for HTTP call ret\", \"ries is the code where you add the Circuit Breaker policy to the list of policies to use, as shown i\", \"n the following incremental code.\\n\\n```\\n// Program.cs\\nvar retryPolicy = GetRetryPolicy();\\nvar circuit\", \"BreakerPolicy = GetCircuitBreakerPolicy();\\nbuilder.Services.AddHttpClient<IBasketService, BasketServ\", \"ice>()\\n .SetHandlerLifetime(TimeSpan.FromMinutes(5)) // Sample: default lifetime is 2 \\nminutes\\n .Add\", \"HttpMessageHandler<HttpClientAuthorizationDelegatingHandler>()\\n .AddPolicyHandler(retryPolicy)\\n .Add\", \"PolicyHandler(circuitBreakerPolicy);\\n```\\n\\nThe AddPolicyHandler() method is what adds policies to the\", \" HttpClient objects you'll use. In this case, it's adding a Polly policy for a circuit breaker.\\n\\nTo \", \"have a more modular approach, the Circuit Breaker Policy is defined in a separate method called GetC\", \"ircuitBreakerPolicy(), as shown in the following code:\\n\\n```\\n// also in Program.cs\\nstatic IAsyncPolic\", \"y<HttpResponseMessage> GetCircuitBreakerPolicy()\\n{\\n return HttpPolicyExtensions\\n .HandleTransientHtt\", \"pError()\\n .CircuitBreakerAsync(5, TimeSpan.FromSeconds(30));\\n}\\n```\\n\\nIn the code example above, the c\", \"ircuit breaker policy is configured so it breaks or opens the circuit when there have been five cons\", \"ecutive faults when retrying the Http requests. When that happens, the circuit will break for 30 sec\", \"onds: in that period, calls will be failed immediately by the circuitbreaker rather than actually be\", \" placed. The policy automatically interprets [relevant exceptions and](https://docs.microsoft.com/as\", \"pnet/core/fundamentals/http-requests#handle-transient-faults)  [HTTP status codes](https://docs.micr\", \"osoft.com/aspnet/core/fundamentals/http-requests#handle-transient-faults) as faults.\\n\\nCircuit breake\", \"rs should also be used to redirect requests to a fallback infrastructure if you had issues in a part\", \"icular resource that's deployed in a different environment than the client application or\\n\\nservice t\", \"hat's performing the HTTP call. That way, if there's an outage in the datacenter that impacts only y\", \"our backend microservices but not your client applications, the client applications can redirect to \", \"the fallback services. Polly is planning a new policy to automate this [failover policy](https://git\", \"hub.com/App-vNext/Polly/wiki/Polly-Roadmap#failover-policy) scenario.\\n\\nAll those features are for ca\", \"ses where you're managing the failover from within the .NET code, as opposed to having it managed au\", \"tomatically for you by Azure, with location transparency. From a usage point of view, when using Htt\", \"pClient, there's no need to add anything new here\\n\\nbecause the code is the same than when using Http\", \"Client with IHttpClientFactory, as shown in previous sections.\\n\\n## <span id=\\\"page-322-0\\\"></span>**Te\", \"st Http retries and circuit breakers in eShopOnContainers**\\n\\nWhenever you start the eShopOnContainer\", \"s solution in a Docker host, it needs to start multiple containers. Some of the containers are slowe\", \"r to start and initialize, like the SQL Server container. This is especially true the first time you\", \" deploy the eShopOnContainers application into Docker because it needs to set up the images and the \", \"database. The fact that some containers start slower than others can cause the rest of the services \", \"to initially throw HTTP exceptions, even if you set dependencies between containers at the docker-co\", \"mpose level, as explained in previous sections. Those dockercompose dependencies between containers \", \"are just at the process level. The container's entry point process might be started, but SQL Server \", \"might not be ready for queries. The result can be a cascade of errors, and the application can get a\", \"n exception when trying to consume that particular container.\\n\\nYou might also see this type of error\", \" on startup when the application is deploying to the cloud. In that case, orchestrators might be mov\", \"ing containers from one node or VM to another (that is, starting new instances) when balancing the n\", \"umber of containers across the cluster's nodes.\\n\\nThe way 'eShopOnContainers' solves those issues whe\", \"n starting all the containers is by using the Retry pattern illustrated earlier.\\n\\n### **Test the cir\", \"cuit breaker in eShopOnContainers**\\n\\nThere are a few ways you can break/open the circuit and test it\", \" with eShopOnContainers.\\n\\nOne option is to lower the allowed number of retries to 1 in the circuit b\", \"reaker policy and redeploy the whole solution into Docker. With a single retry, there's a good chanc\", \"e that an HTTP request will fail during deployment, the circuit breaker will open, and you get an er\", \"ror.\\n\\nAnother option is to use custom middleware that's implemented in the **Basket** microservice. \", \"When this middleware is enabled, it catches all HTTP requests and returns status code 500. You can e\", \"nable the middleware by making a GET request to the failing URI, like the following:\\n\\n- GET http://l\", \"ocalhost:5103/failing This request returns the current state of the middleware. If the middleware is\", \" enabled, the request return status code 500. If the middleware is disabled, there's no response.\\n- \", \"GET http://localhost:5103/failing?enable This request enables the middleware.\\n\\n\\u2022 GET http://localhos\", \"t:5103/failing?disable This request disables the middleware.\\n\\nFor instance, once the application is \", \"running, you can enable the middleware by making a request using the following URI in any browser. N\", \"ote that the ordering microservice uses port 5103.\\n\\nhttp://localhost:5103/failing?enable\\n\\nYou can th\", \"en check the status using the URI http://localhost:5103/failing, as shown in Figure 8-5.\\n\\n![](_page_\", \"323_Picture_4.jpeg)\\n\\n*Figure 8-5. Checking the state of the \\\"Failing\\\" ASP.NET middleware \\u2013 In this c\", \"ase, disabled.*\\n\\nAt this point, the Basket microservice responds with status code 500 whenever you c\", \"all invoke it.\\n\\nOnce the middleware is running, you can try making an order from the MVC web applica\", \"tion. Because the requests fail, the circuit will open.\\n\\nIn the following example, you can see that \", \"the MVC web application has a catch block in the logic for placing an order. If the code catches an \", \"open-circuit exception, it shows the user a friendly message telling them to wait.\\n\\n```\\npublic class\", \" CartController : Controller\\n{\\n //\\u2026\\n public async Task<IActionResult> Index()\\n {\\n try\\n {\\n var user =\", \" _appUserParser.Parse(HttpContext.User);\\n //Http requests using the Typed Client (Service Agent)\\n va\", \"r vm = await _basketSvc.GetBasket(user);\\n return View(vm);\\n }\\n catch (BrokenCircuitException)\\n {\\n //\", \" Catches error when Basket.api is in circuit-opened mode\\n HandleBrokenCircuitException();\\n }\\n return\", \" View();\\n }\\n private void HandleBrokenCircuitException()\\n {\\n TempData[\\\"BasketInoperativeMsg\\\"] = \\\"Bas\", \"ket Service is inoperative, please try later \\non. (Business message due to Circuit-Breaker)\\\";\\n }\\n}\\n`\", \"``\\n\\nHere's a summary. The Retry policy tries several times to make the HTTP request and gets HTTP er\", \"rors. When the number of retries reaches the maximum number set for the Circuit Breaker policy (in t\", \"his case, 5), the application throws a BrokenCircuitException. The result is a friendly message, as \", \"shown in Figure 8-6.\\n\\n![](_page_324_Picture_0.jpeg)\\n\\nYou can implement different logic for when to o\", \"pen/break the circuit. Or you can try an HTTP request against a different back-end microservice if t\", \"here's a fallback datacenter or redundant back-end system.\\n\\nFinally, another possibility for the Cir\", \"cuitBreakerPolicy is to use Isolate (which forces open and holds open the circuit) and Reset (which \", \"closes it again). These could be used to build a utility HTTP endpoint that invokes Isolate and Rese\", \"t directly on the policy. Such an HTTP endpoint could also be used, suitably secured, in production \", \"for temporarily isolating a downstream system, such as when you want to upgrade it. Or it could trip\", \" the circuit manually to protect a downstream system you suspect to be faulting.\\n\\n### <span id=\\\"page\", \"-324-0\\\"></span>**Additional resources**\\n\\n\\u2022 **Circuit Breaker pattern** [https://learn.microsoft.com/\", \"azure/architecture/patterns/circuit-breaker](https://docs.microsoft.com/azure/architecture/patterns/\", \"circuit-breaker)\\n\\n## <span id=\\\"page-324-1\\\"></span>Health monitoring\\n\\nHealth monitoring can allow nea\", \"r-real-time information about the state of your containers and microservices. Health monitoring is c\", \"ritical to multiple aspects of operating microservices and is especially important when orchestrator\", \"s perform partial application upgrades in phases, as explained later.\\n\\nMicroservices-based applicati\", \"ons often use heartbeats or health checks to enable their performance monitors, schedulers, and orch\", \"estrators to keep track of the multitude of services. If services cannot send some sort of \\\"I'm aliv\", \"e\\\" signal, either on demand or on a schedule, your application might face risks when you deploy upda\", \"tes, or it might just detect failures too late and not be able to stop cascading failures that can e\", \"nd up in major outages.\\n\\nIn the typical model, services send reports about their status, and that in\", \"formation is aggregated to provide an overall view of the state of health of your application. If yo\", \"u're using an orchestrator, you can provide health information to your orchestrator's cluster, so th\", \"at the cluster can act accordingly. If you invest in high-quality health reporting that's customized\", \" for your application, you can detect and fix issues for your running application much more easily.\\n\", \"\\n## <span id=\\\"page-325-0\\\"></span>**Implement health checks in ASP.NET Core services**\\n\\nWhen developi\", \"ng an ASP.NET Core microservice or web application, you can use the built-in health checks feature t\", \"hat was released in ASP .NET Core 2.2 [\\\\(Microsoft.Extensions.Diagnostics.HealthChecks\\\\)](https://ww\", \"w.nuget.org/packages/Microsoft.Extensions.Diagnostics.HealthChecks). Like many ASP.NET Core features\", \", health checks\\n\\ncome with a set of services and a middleware.\\n\\nany external resource needed for you\", \"r application (like a SQL Server database or a remote API) is working properly. When you use this fe\", \"ature, you can also decide what it means that the resource is healthy, as we explain later. To use t\", \"his feature effectively, you need to first configure services in your microservices. Second, you\\n\\nHe\", \"alth check services and middleware are easy to use and provide capabilities that let you validate if\", \"\\n\\nneed a front-end application that queries for the health reports. That front-end application could\", \" be a custom reporting application, or it could be an orchestrator itself that can react accordingly\", \" to the health states.\\n\\n## **Use the HealthChecks feature in your back-end ASP.NET microservices**\\n\\n\", \"In this section, you'll learn how to implement the HealthChecks feature in a sample ASP.NET Core 7.0\", \" Web API application when using the [Microsoft.Extensions.Diagnostics.HealthChecks](https://www.nuge\", \"t.org/packages/Microsoft.Extensions.Diagnostics.HealthChecks) package. The Implementation of this fe\", \"ature in a large-scale microservices like the eShopOnContainers is explained in the next section.\\n\\nT\", \"o begin, you need to define what constitutes a healthy status for each microservice. In the sample a\", \"pplication, we define the microservice is healthy if its API is accessible via HTTP and its related \", \"SQL Server database is also available.\\n\\nIn .NET 7, with the built-in APIs, you can configure the ser\", \"vices, add a Health Check for the microservice and its dependent SQL Server database in this way:\\n\\n`\", \"``\\n// Program.cs from .NET 7 Web API sample\\n//...\\n// Registers required services for health checks\\nb\", \"uilder.Services.AddHealthChecks()\\n // Add a health check for a SQL Server database\\n .AddCheck(\\n \\\"Ord\", \"eringDB-check\\\",\\n new SqlConnectionHealthCheck(builder.Configuration[\\\"ConnectionString\\\"]),\\n HealthSta\", \"tus.Unhealthy,\\n new string[] { \\\"orderingdb\\\" });\\n```\\n\\nIn the previous code, the services.AddHealthChe\", \"cks() method configures a basic HTTP check that returns a status code **200** with \\\"Healthy\\\". Furthe\", \"r, the AddCheck() extension method configures a custom SqlConnectionHealthCheck that checks the rela\", \"ted SQL Database's health.\\n\\nThe AddCheck() method adds a new health check with a specified name and \", \"the implementation of type IHealthCheck. You can add multiple Health Checks using AddCheck method, s\", \"o a microservice won't provide a \\\"healthy\\\" status until all its checks are healthy.\\n\\nSqlConnectionHe\", \"althCheck is a custom class that implements IHealthCheck, which takes a connection string as a const\", \"ructor parameter and executes a simple query to check if the connection to the SQL database is succe\", \"ssful. It returns HealthCheckResult.Healthy() if the query was executed successfully and a FailureSt\", \"atus with the actual exception when it fails.\\n\\n```\\n// Sample SQL Connection Health Check\\npublic clas\", \"s SqlConnectionHealthCheck : IHealthCheck\\n{\\n private const string DefaultTestQuery = \\\"Select 1\\\";\\n pu\", \"blic string ConnectionString { get; }\\n public string TestQuery { get; }\\n public SqlConnectionHealthC\", \"heck(string connectionString)\\n : this(connectionString, testQuery: DefaultTestQuery)\\n {\\n }\\n public S\", \"qlConnectionHealthCheck(string connectionString, string testQuery)\\n {\\n ConnectionString = connection\", \"String ?? throw new\\nArgumentNullException(nameof(connectionString));\\n TestQuery = testQuery;\\n }\\n pub\", \"lic async Task<HealthCheckResult> CheckHealthAsync(HealthCheckContext context,\\nCancellationToken can\", \"cellationToken = default(CancellationToken))\\n {\\n using (var connection = new SqlConnection(Connectio\", \"nString))\\n {\\n try\\n {\\n await connection.OpenAsync(cancellationToken);\\n if (TestQuery != null)\\n {\\n var\", \" command = connection.CreateCommand();\\n command.CommandText = TestQuery;\\n await command.ExecuteNonQu\", \"eryAsync(cancellationToken);\\n }\\n }\\n catch (DbException ex)\\n {\\n return new HealthCheckResult(status: \", \"context.Registration.FailureStatus,\\nexception: ex);\\n }\\n }\\n return HealthCheckResult.Healthy();\\n }\\n}\\n\", \"```\\n\\nNote that in the previous code, Select 1 is the query used to check the Health of the database.\", \" To monitor the availability of your microservices, orchestrators like Kubernetes periodically perfo\", \"rm health checks by sending requests to test the microservices. It's important to keep your database\", \" queries efficient so that these operations are quick and don't result in a higher utilization of re\", \"sources. Finally, add a middleware that responds to the url path /hc:\\n\\n```\\n// Program.cs from .NET 7\", \" Web Api sample\\napp.MapHealthChecks(\\\"/hc\\\");\\nWhen the endpoint <yourmicroservice>/hc is invoked, it r\", \"uns all the health checks that are configured\\n```\\n\\nin the AddHealthChecks() method in the Startup cl\", \"ass and shows the result.\\n\\n## **HealthChecks implementation in eShopOnContainers**\\n\\nMicroservices in\", \" eShopOnContainers rely on multiple services to perform its task. For example, the Catalog.API micro\", \"service from eShopOnContainers depends on many services, such as Azure Blob Storage, SQL Server, and\", \" RabbitMQ. Therefore, it has several health checks added using the AddCheck() method. For every depe\", \"ndent service, a custom IHealthCheck implementation that defines its respective health status would \", \"need to be added.\\n\\nThe open-source project [AspNetCore.Diagnostics.HealthChecks](https://github.com/\", \"Xabaril/AspNetCore.Diagnostics.HealthChecks) solves this problem by providing custom health check im\", \"plementations for each of these enterprise services, that are built on top of .NET 7. Each health ch\", \"eck is available as an individual NuGet package that can be easily added to the project. eShopOnCont\", \"ainers uses them extensively in all its microservices.\\n\\nFor instance, in the Catalog.API microservic\", \"e, the following NuGet packages were added:\\n\\n![](_page_327_Picture_7.jpeg)\\n\\n*Figure 8-7. Custom Heal\", \"th Checks implemented in Catalog.API using AspNetCore.Diagnostics.HealthChecks*\\n\\nIn the following co\", \"de, the health check implementations are added for each dependent service and then the middleware is\", \" configured:\\n\\n```\\n// Extension method from Catalog.api microservice\\n//\\npublic static IServiceCollect\", \"ion AddCustomHealthCheck(this IServiceCollection services,\\nIConfiguration configuration)\\n{\\n var acco\", \"untName = configuration.GetValue<string>(\\\"AzureStorageAccountName\\\");\\n var accountKey = configuration\", \".GetValue<string>(\\\"AzureStorageAccountKey\\\");\\n var hcBuilder = services.AddHealthChecks();\\n```\\n\\n```\\n \", \"hcBuilder\\n .AddSqlServer(\\n configuration[\\\"ConnectionString\\\"],\\n name: \\\"CatalogDB-check\\\",\\n tags: new s\", \"tring[] { \\\"catalogdb\\\" });\\n if (!string.IsNullOrEmpty(accountName) && !string.IsNullOrEmpty(accountKe\", \"y))\\n {\\n hcBuilder\\n .AddAzureBlobStorage(\\n$\\\"DefaultEndpointsProtocol=https;AccountName={accountName};\", \"AccountKey={accountKey};Endpoint\\nSuffix=core.windows.net\\\",\\n name: \\\"catalog-storage-check\\\",\\n tags: ne\", \"w string[] { \\\"catalogstorage\\\" });\\n }\\n if (configuration.GetValue<bool>(\\\"AzureServiceBusEnabled\\\"))\\n {\", \"\\n hcBuilder\\n .AddAzureServiceBusTopic(\\n configuration[\\\"EventBusConnection\\\"],\\n topicName: \\\"eshop_even\", \"t_bus\\\",\\n name: \\\"catalog-servicebus-check\\\",\\n tags: new string[] { \\\"servicebus\\\" });\\n }\\n else\\n {\\n hcBui\", \"lder\\n .AddRabbitMQ(\\n $\\\"amqp://{configuration[\\\"EventBusConnection\\\"]}\\\",\\n name: \\\"catalog-rabbitmqbus-ch\", \"eck\\\",\\n tags: new string[] { \\\"rabbitmqbus\\\" });\\n }\\n return services;\\n}\\n```\\n\\nFinally, add the HealthChe\", \"ck middleware to listen to \\\"/hc\\\" endpoint:\\n\\n```\\n// HealthCheck middleware\\napp.UseHealthChecks(\\\"/hc\\\",\", \" new HealthCheckOptions()\\n{\\n Predicate = _ => true,\\n ResponseWriter = UIResponseWriter.WriteHealthCh\", \"eckUIResponse\\n});\\n```\\n\\n### **Query your microservices to report about their health status**\\n\\nWhen yo\", \"u've configured health checks as described in this article and you have the microservice running in \", \"Docker, you can directly check from a browser if it's healthy. You have to publish the container por\", \"t in the Docker host, so you can access the container through the external Docker host IP or through\", \" host.docker.internal, as shown in figure 8-8.\\n\\n![](_page_329_Figure_0.jpeg)\\n\\nIn that test, you can \", \"see that the Catalog.API microservice (running on port 5101) is healthy, returning HTTP status 200 a\", \"nd status information in JSON. The service also checked the health of its SQL Server database depend\", \"ency and RabbitMQ, so the health check reported itself as healthy.\\n\\n## <span id=\\\"page-329-0\\\"></span>\", \"**Use watchdogs**\\n\\nA watchdog is a separate service that can watch health and load across services, \", \"and report health about the microservices by querying with the HealthChecks library introduced earli\", \"er. This can help prevent errors that would not be detected based on the view of a single service. W\", \"atchdogs also are a good place to host code that can perform remediation actions for known condition\", \"s without user interaction.\\n\\nThe eShopOnContainers sample contains a web page that displays sample h\", \"ealth check reports, as shown in Figure 8-9. This is the simplest watchdog you could have since it o\", \"nly shows the state of the microservices and web applications in eShopOnContainers. Usually a watchd\", \"og also takes actions when it detects unhealthy states.\\n\\nFortunately, [AspNetCore.Diagnostics.Health\", \"Checks](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChecks) also provides [AspNetCore.He\", \"althChecks.UI](https://www.nuget.org/packages/AspNetCore.HealthChecks.UI/) NuGet package that can be\", \" used to display the health check results from the configured URIs.\\n\\n![](_page_330_Picture_0.jpeg)\\n\\n\", \"*Figure 8-9. Sample health check report in eShopOnContainers*\\n\\nIn summary, this watchdog service que\", \"ries each microservice's \\\"/hc\\\" endpoint. This will execute all the health checks defined within it a\", \"nd return an overall health state depending on all those checks. The HealthChecksUI is easy to consu\", \"me with a few configuration entries and two lines of code that needs to be added into the *Startup.c\", \"s* of the watchdog service.\\n\\nSample configuration file for health check UI:\\n\\n```\\n// Configuration\\n{\\n\", \" \\\"HealthChecksUI\\\": {\\n \\\"HealthChecks\\\": [\\n {\\n \\\"Name\\\": \\\"Ordering HTTP Check\\\",\\n \\\"Uri\\\": \\\"http://host.dock\", \"er.internal:5102/hc\\\"\\n },\\n {\\n \\\"Name\\\": \\\"Ordering HTTP Background Check\\\",\\n \\\"Uri\\\": \\\"http://host.docker.i\", \"nternal:5111/hc\\\"\\n },\\n //...\\n ]}\\n}\\n```\\n\\n*Program.cs* file that adds HealthChecksUI:\\n\\n```\\n// Program.c\", \"s from WebStatus(Watch Dog) service\\n//\\n// Registers required services for health checks\\nbuilder.Serv\", \"ices.AddHealthChecksUI();\\n// build the app, register other middleware\\napp.UseHealthChecksUI(config =\", \"> config.UIPath = \\\"/hc-ui\\\");\\n```\\n\\n## <span id=\\\"page-331-0\\\"></span>**Health checks when using orchest\", \"rators** To monitor the availability of your microservices, orchestrators like Kubernetes and Servic\", \"e Fabric\\n\\nperiodically perform health checks by sending requests to test the microservices. When an \", \"orchestrator determines that a service/container is unhealthy, it stops routing requests to that ins\", \"tance. It also usually creates a new instance of that container.\\n\\nFor instance, most orchestrators c\", \"an use health checks to manage zero-downtime deployments. Only when the status of a service/containe\", \"r changes to healthy will the orchestrator start routing traffic to service/container instances.\\n\\nHe\", \"alth monitoring is especially important when an orchestrator performs an application upgrade. Some o\", \"rchestrators (like Azure Service Fabric) update services in phases\\u2014for example, they might update on\", \"e-fifth of the cluster surface for each application upgrade. The set of nodes that's upgraded at the\", \" same time is referred to as an *upgrade domain*. After each upgrade domain has been upgraded and is\", \" available to users, that upgrade domain must pass health checks before the deployment moves to the \", \"next upgrade domain.\\n\\nAnother aspect of service health is reporting metrics from the service. This i\", \"s an advanced capability of the health model of some orchestrators, like Service Fabric. Metrics are\", \" important when using an orchestrator because they are used to balance resource usage. Metrics also \", \"can be an indicator of system health. For example, you might have an application that has many micro\", \"services, and each instance reports a requests-per-second (RPS) metric. If one service is using more\", \" resources (memory, processor, etc.) than another service, the orchestrator could move service insta\", \"nces around in the cluster to try to maintain even resource utilization.\\n\\nNote that Azure Service Fa\", \"bric provides its own [Health Monitoring model,](https://docs.microsoft.com/azure/service-fabric/ser\", \"vice-fabric-health-introduction) which is more advanced than simple health checks.\\n\\n### <span id=\\\"pa\", \"ge-331-1\\\"></span>**Advanced monitoring: visualization, analysis, and alerts**\\n\\nThe final part of mon\", \"itoring is visualizing the event stream, reporting on service performance, and alerting when an issu\", \"e is detected. You can use different solutions for this aspect of monitoring.\\n\\nYou can use simple cu\", \"stom applications showing the state of your services, like the custom page shown when explaining the\", \" [AspNetCore.Diagnostics.HealthChecks.](https://github.com/Xabaril/AspNetCore.Diagnostics.HealthChec\", \"ks) Or you could use more advanced tools like [Azure Monitor](https://azure.microsoft.com/services/m\", \"onitor/) to raise alerts based on the stream of events.\\n\\nFinally, if you're storing all the event st\", \"reams, you can use Microsoft Power BI or other solutions like Kibana or Splunk to visualize the data\", \".\\n\\n## <span id=\\\"page-332-0\\\"></span>**Additional resources**\\n\\n\\u2022 **Azure Monitor**\\n\\n<https://github.co\", \"m/Xabaril/AspNetCore.Diagnostics.HealthChecks>\\n\\n\\u2022 **HealthChecks and HealthChecks UI for ASP.NET Cor\", \"e**\\n\\n\\u2022 **Introduction to Service Fabric health monitoring**\\n\\n- [https://learn.microsoft.com/azure/se\", \"rvice-fabric/service-fabric-health-introduction](https://docs.microsoft.com/azure/service-fabric/ser\", \"vice-fabric-health-introduction)\\n- <https://azure.microsoft.com/services/monitor/>\\n\\n# <span id=\\\"page\", \"-333-0\\\"></span>Make secure .NET Microservices and Web Applications\\n\\nThere are so many aspects about \", \"security in microservices and web applications that the topic could easily take several books like t\", \"his one. So, in this section, we'll focus on authentication, authorization, and application secrets.\", \"\\n\\n## <span id=\\\"page-333-1\\\"></span>Implement authentication in .NET microservices and web application\", \"s\\n\\nIt's often necessary for resources and APIs published by a service to be limited to certain trust\", \"ed users or clients. The first step to making these sorts of API-level trust decisions is authentica\", \"tion. Authentication is the process of reliably verifying a user's identity.\\n\\nIn microservice scenar\", \"ios, authentication is typically handled centrally. If you're using an API Gateway, the gateway is a\", \" good place to authenticate, as shown in Figure 9-1. If you use this approach, make sure that the in\", \"dividual microservices cannot be reached directly (without the API Gateway) unless additional securi\", \"ty is in place to authenticate messages whether they come from the gateway or not.\\n\\n![](_page_333_Fi\", \"gure_6.jpeg)\\n\\n*Figure 9-1. Centralized authentication with an API Gateway*\\n\\nWhen the API Gateway cen\", \"tralizes authentication, it adds user information when forwarding requests to the microservices. If \", \"services can be accessed directly, an authentication service like Azure Active\\n\\nDirectory or a dedic\", \"ated authentication microservice acting as a security token service (STS) can be used to authenticat\", \"e users. Trust decisions are shared between services with security tokens or cookies. (These tokens \", \"can be shared between ASP.NET Core applications, if needed, by implementing [cookie sharing.\\\\)](http\", \"s://docs.microsoft.com/aspnet/core/security/cookie-sharing) This pattern is illustrated in Figure 9-\", \"2.\\n\\n![](_page_334_Figure_1.jpeg)\\n\\n*Figure 9-2. Authentication by identity microservice; trust is sha\", \"red using an authorization token*\\n\\nWhen microservices are accessed directly, trust, that includes au\", \"thentication and authorization, is handled by a security token issued by a dedicated microservice, s\", \"hared between microservices.\\n\\n### <span id=\\\"page-334-0\\\"></span>**Authenticate with ASP.NET Core Iden\", \"tity**\\n\\nThe primary mechanism in ASP.NET Core for identifying an application's users is the [ASP.NET\", \" Core](https://docs.microsoft.com/aspnet/core/security/authentication/identity)  [Identity](https://\", \"docs.microsoft.com/aspnet/core/security/authentication/identity) membership system. ASP.NET Core Ide\", \"ntity stores user information (including sign-in information, roles, and claims) in a data store con\", \"figured by the developer. Typically, the ASP.NET Core Identity data store is an Entity Framework sto\", \"re provided in the Microsoft.AspNetCore.Identity.EntityFrameworkCore package. However, custom stores\", \" or other thirdparty packages can be used to store identity information in Azure Table Storage, Cosm\", \"osDB, or other locations.\\n\\n### **Tip**\\n\\nASP.NET Core 2.1 and later provides [ASP.NET Core Identity](\", \"https://docs.microsoft.com/aspnet/core/security/authentication/identity) as a [Razor Class Library](\", \"https://docs.microsoft.com/aspnet/core/razor-pages/ui-class), so you won't see much of the necessary\", \" code in your project, as was the case for previous versions. For details on how to customize the Id\", \"entity code to suit your needs, see [Scaffold Identity in ASP.NET Core projects.](https://docs.micro\", \"soft.com/aspnet/core/security/authentication/scaffold-identity)\\n\\nThe following code is taken from th\", \"e ASP.NET Core Web Application MVC 3.1 project template with individual user account authentication \", \"selected. It shows how to configure ASP.NET Core Identity using Entity Framework Core in the *Progra\", \"m.cs* file.\\n\\n```\\n//...\\nbuilder.Services.AddDbContext<ApplicationDbContext>(options =>\\n options.UseSq\", \"lServer(\\n builder.Configuration.GetConnectionString(\\\"DefaultConnection\\\")));\\n```\\n\\n```\\nbuilder.Service\", \"s.AddDefaultIdentity<IdentityUser>(options =>\\n options.SignIn.RequireConfirmedAccount = true)\\n .AddE\", \"ntityFrameworkStores<ApplicationDbContext>();\\nbuilder.Services.AddRazorPages();\\n//...\\nOnce ASP.NET C\", \"ore Identity is configured, you enable it by adding the \\napp.UseAuthentication() and endpoints.MapRa\", \"zorPages() as shown in the following code in the \\nservice's Program.cs file:\\n//...\\napp.UseRouting();\", \"\\napp.UseAuthentication();\\napp.UseAuthorization();\\napp.UseEndpoints(endpoints =>\\n{\\n endpoints.MapRazo\", \"rPages();\\n});\\n//...\\n```\\n\\n### **Important**\\n\\nThe lines in the preceding code **MUST BE IN THE ORDER S\", \"HOWN** for Identity to work correctly.\\n\\nUsing ASP.NET Core Identity enables several scenarios:\\n\\n- Cr\", \"eate new user information using the UserManager type (userManager.CreateAsync).\\n- Authenticate users\", \" using the SignInManager type. You can use signInManager.SignInAsync to sign in directly, or signInM\", \"anager.PasswordSignInAsync to confirm the user's password is correct and then sign them in.\\n- Identi\", \"fy a user based on information stored in a cookie (which is read by ASP.NET Core Identity middleware\", \") so that subsequent requests from a browser will include a signed-in user's identity and claims.\\n\\nA\", \"SP.NET Core Identity also supports [two-factor authentication.](https://docs.microsoft.com/aspnet/co\", \"re/security/authentication/2fa)\\n\\nFor authentication scenarios that make use of a local user data sto\", \"re and that persist identity between requests using cookies (as is typical for MVC web applications)\", \", ASP.NET Core Identity is a recommended solution.\\n\\n## <span id=\\\"page-335-0\\\"></span>**Authenticate w\", \"ith external providers**\\n\\nASP.NET Core also supports using [external authentication providers](https\", \"://docs.microsoft.com/aspnet/core/security/authentication/social/) to let users sign in via [OAuth 2\", \".0](https://www.digitalocean.com/community/tutorials/an-introduction-to-oauth-2) flows. This means t\", \"hat users can sign in using existing authentication processes from providers like Microsoft, Google,\", \" Facebook, or Twitter and associate those identities with an ASP.NET Core identity in your applicati\", \"on.\\n\\nTo use external authentication, besides including the authentication middleware as mentioned be\", \"fore, using the app.UseAuthentication() method, you also have to register the external provider in *\", \"Program.cs* as shown in the following example:\\n\\n```\\n//...\\nservices.AddDefaultIdentity<IdentityUser>(\", \"options => options.SignIn.RequireConfirmedAccount\\n= true)\\n .AddEntityFrameworkStores<ApplicationDbCo\", \"ntext>();\\nservices.AddAuthentication()\\n .AddMicrosoftAccount(microsoftOptions =>\\n {\\n microsoftOption\", \"s.ClientId =\\nbuilder.Configuration[\\\"Authentication:Microsoft:ClientId\\\"];\\n microsoftOptions.ClientSec\", \"ret =\\nbuilder.Configuration[\\\"Authentication:Microsoft:ClientSecret\\\"];\\n })\\n .AddGoogle(googleOptions \", \"=> { ... })\\n .AddTwitter(twitterOptions => { ... })\\n .AddFacebook(facebookOptions => { ... });\\n//...\", \"\\n```\\n\\nPopular external authentication providers and their associated NuGet packages are shown in the\", \" following table:\\n\\n| Provider  | Package                                              |\\n|-----------\", \"|------------------------------------------------------|\\n| Microsoft | Microsoft.AspNetCore.Authenti\", \"cation.MicrosoftAccount |\\n| Google    | Microsoft.AspNetCore.Authentication.Google           |\\n| Fac\", \"ebook  | Microsoft.AspNetCore.Authentication.Facebook         |\\n| Twitter   | Microsoft.AspNetCore.A\", \"uthentication.Twitter          |\\n|           |                                                      \", \"|\\n\\nIn all cases, you must complete an application registration procedure that is vendor dependent an\", \"d that usually involves:\\n\\n- 1. Getting a Client Application ID.\\n- 2. Getting a Client Application Se\", \"cret.\\n- 3. Configuring a redirection URL, that's handled by the authorization middleware and the reg\", \"istered provider\\n- 4. Optionally, configuring a sign-out URL to properly handle sign out in a Single\", \" Sign On (SSO) scenario.\\n\\nFor details on configuring your app for an external provider, see the [Ext\", \"ernal provider authentication](https://docs.microsoft.com/aspnet/core/security/authentication/social\", \"/)  [in the ASP.NET Core documentation\\\\)](https://docs.microsoft.com/aspnet/core/security/authentica\", \"tion/social/).\\n\\n### **Tip**\\n\\nAll details are handled by the authorization middleware and services pr\", \"eviously mentioned. So, you just have to choose the **Individual User Account** authentication optio\", \"n when you create the ASP.NET Core web application project in Visual Studio, as shown in Figure 9-3,\", \" besides registering the authentication providers previously mentioned.\\n\\n![](_page_337_Figure_0.jpeg\", \")\\n\\n*Figure 9-3. Selecting the Individual User Accounts option, for using external authentication, wh\", \"en creating a web application project in Visual Studio 2019.*\\n\\nIn addition to the external authentic\", \"ation providers listed previously, third-party packages are available that provide middleware for us\", \"ing many more external authentication providers. For a list, see the [AspNet.Security.OAuth.Provider\", \"s](https://github.com/aspnet-contrib/AspNet.Security.OAuth.Providers/tree/dev/src) repository on Git\", \"Hub.\\n\\nYou can also create your own external authentication middleware to solve some special need.\\n\\n#\", \"## <span id=\\\"page-337-0\\\"></span>**Authenticate with bearer tokens**\\n\\nAuthenticating with ASP.NET Cor\", \"e Identity (or Identity plus external authentication providers) works well for many web application \", \"scenarios in which storing user information in a cookie is appropriate. In other scenarios, though, \", \"cookies are not a natural means of persisting and transmitting data.\\n\\nFor example, in an ASP.NET Cor\", \"e Web API that exposes RESTful endpoints that might be accessed by Single Page Applications (SPAs), \", \"by native clients, or even by other Web APIs, you typically want to use bearer token authentication \", \"instead. These types of applications do not work with cookies, but can easily retrieve a bearer toke\", \"n and include it in the authorization header of subsequent requests. To enable token authentication,\", \" ASP.NET Core supports several options for using [OAuth 2.0](https://oauth.net/2/) and [OpenID Conne\", \"ct.](https://openid.net/connect/)\\n\\n## <span id=\\\"page-338-0\\\"></span>**Authenticate with an OpenID Con\", \"nect or OAuth 2.0 Identity provider**\\n\\nIf user information is stored in Azure Active Directory or an\", \"other identity solution that supports OpenID Connect or OAuth 2.0, you can use the **Microsoft.AspNe\", \"tCore.Authentication.OpenIdConnect** package to authenticate using the OpenID\\n\\nConnect workflow. For\", \" example, to authenticate to the Identity.Api microservice in eShopOnContainers, an ASP.NET Core web\", \" application can use middleware from that package as shown in the following simplified example in *P\", \"rogram.cs*:\\n\\n```\\n// Program.cs\\nvar identityUrl = builder.Configuration.GetValue<string>(\\\"IdentityUrl\", \"\\\");\\nvar callBackUrl = builder.Configuration.GetValue<string>(\\\"CallBackUrl\\\");\\nvar sessionCookieLifeti\", \"me = builder.Configuration.GetValue(\\\"SessionCookieLifetimeMinutes\\\",\\n60);\\n// Add Authentication servi\", \"ces\\nservices.AddAuthentication(options =>\\n{\\n options.DefaultScheme = CookieAuthenticationDefaults.Au\", \"thenticationScheme;\\n options.DefaultChallengeScheme = JwtBearerDefaults.AuthenticationScheme;\\n})\\n.Ad\", \"dCookie(setup => setup.ExpireTimeSpan = TimeSpan.FromMinutes(sessionCookieLifetime))\\n.AddOpenIdConne\", \"ct(options =>\\n{\\n options.SignInScheme = CookieAuthenticationDefaults.AuthenticationScheme;\\n options.\", \"Authority = identityUrl.ToString();\\n options.SignedOutRedirectUri = callBackUrl.ToString();\\n options\", \".ClientId = useLoadTest ? \\\"mvctest\\\" : \\\"mvc\\\";\\n options.ClientSecret = \\\"secret\\\";\\n options.ResponseType\", \" = useLoadTest ? \\\"code id_token token\\\" : \\\"code id_token\\\";\\n options.SaveTokens = true;\\n options.GetCl\", \"aimsFromUserInfoEndpoint = true;\\n options.RequireHttpsMetadata = false;\\n options.Scope.Add(\\\"openid\\\")\", \";\\n options.Scope.Add(\\\"profile\\\");\\n options.Scope.Add(\\\"orders\\\");\\n options.Scope.Add(\\\"basket\\\");\\n option\", \"s.Scope.Add(\\\"marketing\\\");\\n options.Scope.Add(\\\"locations\\\");\\n options.Scope.Add(\\\"webshoppingagg\\\");\\n op\", \"tions.Scope.Add(\\\"orders.signalrhub\\\");\\n});\\n// Build the app\\n//\\u2026\\napp.UseAuthentication();\\n//\\u2026\\napp.UseE\", \"ndpoints(endpoints =>\\n{\\n //...\\n});\\n```\\n\\nWhen you use this workflow, the ASP.NET Core Identity middle\", \"ware is not needed, because all user information storage and authentication is handled by the Identi\", \"ty service.\\n\\n## <span id=\\\"page-339-0\\\"></span>**Issue security tokens from an ASP.NET Core service**\\n\", \"\\nIf you prefer to issue security tokens for local ASP.NET Core Identity users rather than using an e\", \"xternal identity provider, you can take advantage of some good third-party libraries.\\n\\n[IdentityServ\", \"er4](https://github.com/IdentityServer/IdentityServer4) and [OpenIddict](https://github.com/openiddi\", \"ct/openiddict-core) are OpenID Connect providers that integrate easily with ASP.NET Core Identity to\", \" let you issue security tokens from an ASP.NET Core service. The [IdentityServer4](https://identitys\", \"erver4.readthedocs.io/en/latest/)  [documentation](https://identityserver4.readthedocs.io/en/latest/\", \") has in-depth instructions for using the library. However, the basic steps to using IdentityServer4\", \" to issue tokens are as follows.\\n\\n- 1. You configure IdentityServer4 in *Program.cs* by making a cal\", \"l to builder.Services.AddIdentityServer.\\n- 2. You call app.UseIdentityServer in *Program.cs* to add \", \"IdentityServer4 to the application's HTTP request processing pipeline. This lets the library serve r\", \"equests to OpenID Connect and OAuth2 endpoints like /connect/token.\\n- 3. You configure identity serv\", \"er by setting the following data:\\n  - The [credentials](https://identityserver4.readthedocs.io/en/la\", \"test/topics/crypto.html) to use for signing.\\n  - The [Identity and API resources](https://identityse\", \"rver4.readthedocs.io/en/latest/topics/resources.html) that users might request access to:\\n    - API \", \"resources represent protected data or functionality that a user can access with an access token. An \", \"example of an API resource would be a web API (or set of APIs) that requires authorization.\\n    - Id\", \"entity resources represent information (claims) that are given to a client to identify a user. The c\", \"laims might include the user name, email address, and so on.\\n  - The [clients](https://identityserve\", \"r4.readthedocs.io/en/latest/topics/clients.html) that will be connecting in order to request tokens.\", \"\\n  - The storage mechanism for user information, such as [ASP.NET Core Identity](https://identityser\", \"ver4.readthedocs.io/en/latest/quickstarts/0_overview.html) or an alternative.\\n\\nWhen you specify clie\", \"nts and resources for IdentityServer4 to use, you can pass an [IEnumerable](https://docs.microsoft.c\", \"om/dotnet/api/system.collections.generic.ienumerable-1) collection of the appropriate type to method\", \"s that take in-memory client or resource stores. Or for more complex scenarios, you can provide clie\", \"nt or resource provider types via Dependency Injection.\\n\\nA sample configuration for IdentityServer4 \", \"to use in-memory resources and clients provided by a custom IClientStore type might look like the fo\", \"llowing example:\\n\\n```\\n// Program.cs\\nbuilder.Services.AddSingleton<IClientStore, CustomClientStore>()\", \";\\nbuilder.Services.AddIdentityServer()\\n .AddSigningCredential(\\\"CN=sts\\\")\\n .AddInMemoryApiResources(My\", \"ApiResourceProvider.GetAllResources())\\n .AddAspNetIdentity<ApplicationUser>();\\n//...\\n```\\n\\n## <span i\", \"d=\\\"page-340-0\\\"></span>**Consume security tokens**\\n\\nAuthenticating against an OpenID Connect endpoint\", \" or issuing your own security tokens covers some scenarios. But what about a service that simply nee\", \"ds to limit access to those users who have valid security tokens that were provided by a different s\", \"ervice?\\n\\nFor that scenario, authentication middleware that handles JWT tokens is available in the\\n\\n*\", \"*Microsoft.AspNetCore.Authentication.JwtBearer** package. JWT stands for \\\"[JSON Web Token](https://t\", \"ools.ietf.org/html/rfc7519)\\\" and is a common security token format (defined by RFC 7519) for communi\", \"cating security claims. A simplified example of how to use middleware to consume such tokens might l\", \"ook like this code fragment, taken from the Ordering.Api microservice of eShopOnContainers.\\n\\n```\\n// \", \"Program.cs\\nvar identityUrl = builder.Configuration.GetValue<string>(\\\"IdentityUrl\\\");\\n// Add Authentic\", \"ation services\\nbuilder.Services.AddAuthentication(options =>\\n{\\n options.DefaultAuthenticateScheme =\\n\", \"AspNetCore.Authentication.JwtBearer.JwtBearerDefaults.AuthenticationScheme;\\n options.DefaultChalleng\", \"eScheme =\\nAspNetCore.Authentication.JwtBearer.JwtBearerDefaults.AuthenticationScheme;\\n}).AddJwtBeare\", \"r(options =>\\n{\\n options.Authority = identityUrl;\\n options.RequireHttpsMetadata = false;\\n options.Aud\", \"ience = \\\"orders\\\";\\n});\\n// Build the app\\napp.UseAuthentication();\\n//\\u2026\\napp.UseEndpoints(endpoints =>\\n{\\n\", \" //...\\n});\\n```\\n\\nThe parameters in this usage are:\\n\\n- Audience represents the receiver of the incomin\", \"g token or the resource that the token grants access to. If the value specified in this parameter do\", \"es not match the parameter in the token, the token will be rejected.\\n- Authority is the address of t\", \"he token-issuing authentication server. The JWT bearer authentication middleware uses this URI to ge\", \"t the public key that can be used to validate the token's signature. The middleware also confirms th\", \"at the iss parameter in the token matches this URI.\\n\\nAnother parameter, RequireHttpsMetadata, is use\", \"ful for testing purposes; you set this parameter to false so you can test in environments where you \", \"don't have certificates. In real-world deployments, JWT bearer tokens should always be passed only o\", \"ver HTTPS.\\n\\nWith this middleware in place, JWT tokens are automatically extracted from authorization\", \" headers. They are then deserialized, validated (using the values in the Audience and Authority para\", \"meters), and stored as user information to be referenced later by MVC actions or authorization filte\", \"rs.\\n\\nThe JWT bearer authentication middleware can also support more advanced scenarios, such as usin\", \"g a local certificate to validate a token if the authority is not available. For this scenario, you \", \"can specify a TokenValidationParameters object in the JwtBearerOptions object.\\n\\n## <span id=\\\"page-34\", \"1-0\\\"></span>Additional resources\\n\\n- **Sharing cookies between applications** [https://learn.microsof\", \"t.com/aspnet/core/security/cookie-sharing](https://docs.microsoft.com/aspnet/core/security/cookie-sh\", \"aring)\\n- **Introduction to Identity** [https://learn.microsoft.com/aspnet/core/security/authenticati\", \"on/identity](https://docs.microsoft.com/aspnet/core/security/authentication/identity)\\n- **Rick Ander\", \"son. Two-factor authentication with SMS** [https://learn.microsoft.com/aspnet/core/security/authenti\", \"cation/2fa](https://docs.microsoft.com/aspnet/core/security/authentication/2fa)\\n- **Enabling authent\", \"ication using Facebook, Google and other external providers** [https://learn.microsoft.com/aspnet/co\", \"re/security/authentication/social/](https://docs.microsoft.com/aspnet/core/security/authentication/s\", \"ocial/)\\n- **Michell Anicas. An Introduction to OAuth 2** <https://www.digitalocean.com/community/tut\", \"orials/an-introduction-to-oauth-2>\\n- **AspNet.Security.OAuth.Providers** (GitHub repo for ASP.NET OA\", \"uth providers) <https://github.com/aspnet-contrib/AspNet.Security.OAuth.Providers/tree/dev/src>\\n- **\", \"IdentityServer4. Official documentation** <https://identityserver4.readthedocs.io/en/latest/>\\n\\n## <s\", \"pan id=\\\"page-341-1\\\"></span>About authorization in .NET microservices and web applications\\n\\nAfter aut\", \"hentication, ASP.NET Core Web APIs need to authorize access. This process allows a service to make A\", \"PIs available to some authenticated users, but not to all. [Authorization](https://docs.microsoft.co\", \"m/aspnet/core/security/authorization/introduction) can be done based on users' roles or based on cus\", \"tom policy, which might include inspecting claims or other heuristics.\\n\\nRestricting access to an ASP\", \".NET Core MVC route is as easy as applying an Authorize attribute to the action method (or to the co\", \"ntroller's class if all the controller's actions require authorization), as shown in following examp\", \"le:\\n\\n```\\npublic class AccountController : Controller\\n{\\n public ActionResult Login()\\n {\\n }\\n [Authoriz\", \"e]\\n```\\n\\n```\\n public ActionResult Logout()\\n {\\n }\\n}\\nBy default, adding an Authorize attribute without \", \"parameters will limit access to authenticated users\\n```\\n\\nfor that controller or action. To further r\", \"estrict an API to be available for only specific users, the attribute can be expanded to specify req\", \"uired roles or policies that users must satisfy.\\n\\n## <span id=\\\"page-342-0\\\"></span>**Implement role-b\", \"ased authorization**\\n\\nstores information about different roles used by the application and keeps tra\", \"ck of which users are assigned to which roles. These assignments can be changed programmatically wit\", \"h the RoleManager type that updates roles in persisted storage, and the UserManager type that can gr\", \"ant or revoke roles from users.\\n\\nASP.NET Core Identity has a built-in concept of roles. In addition \", \"to users, ASP.NET Core Identity\\n\\nIf you're authenticating with JWT bearer tokens, the ASP.NET Core J\", \"WT bearer authentication middleware will populate a user's roles based on role claims found in the t\", \"oken. To limit access to an MVC action or controller to users in specific roles, you can include a R\", \"oles parameter in the Authorize annotation (attribute), as shown in the following code fragment:\\n\\n``\", \"`\\n[Authorize(Roles = \\\"Administrator, PowerUser\\\")]\\npublic class ControlPanelController : Controller\\n{\", \"\\n public ActionResult SetTime()\\n {\\n }\\n [Authorize(Roles = \\\"Administrator\\\")]\\n public ActionResult Shu\", \"tDown()\\n {\\n }\\n}\\n```\\n\\nIn this example, only users in the Administrator or PowerUser roles can access \", \"APIs in the ControlPanel controller (such as executing the SetTime action). The ShutDown API is furt\", \"her restricted to allow access only to users in the Administrator role.\\n\\nTo require a user be in mul\", \"tiple roles, you use multiple Authorize attributes, as shown in the following example:\\n\\n```\\n[Authori\", \"ze(Roles = \\\"Administrator, PowerUser\\\")]\\n[Authorize(Roles = \\\"RemoteEmployee \\\")]\\n[Authorize(Policy = \\\"\", \"CustomPolicy\\\")]\\npublic ActionResult API1 ()\\n{\\n}\\n```\\n\\nIn this example, to call API1, a user must:\\n\\n- \", \"Be in the Administrator *or* PowerUser role, *and*\\n- Be in the RemoteEmployee role, *and*\\n\\n\\u2022 Satisfy\", \" a custom handler for CustomPolicy authorization.\\n\\n## <span id=\\\"page-343-0\\\"></span>**Implement polic\", \"y-based authorization**\\n\\nCustom authorization rules can also be written using [authorization policie\", \"s.](https://docs.asp.net/en/latest/security/authorization/policies.html) This section provides an ov\", \"erview. For more information, see the [ASP.NET Authorization Workshop.](https://github.com/blowdart/\", \"AspNetAuthorizationWorkshop)\\n\\nCustom authorization policies are registered in the Startup.ConfigureS\", \"ervices method using the service.AddAuthorization method. This method takes a delegate that configur\", \"es an AuthorizationOptions argument.\\n\\n```\\nservices.AddAuthorization(options =>\\n{\\n options.AddPolicy(\", \"\\\"AdministratorsOnly\\\", policy =>\\n policy.RequireRole(\\\"Administrator\\\"));\\n options.AddPolicy(\\\"Employees\", \"Only\\\", policy =>\\n policy.RequireClaim(\\\"EmployeeNumber\\\"));\\n options.AddPolicy(\\\"Over21\\\", policy =>\\n po\", \"licy.Requirements.Add(new MinimumAgeRequirement(21)));\\n});\\n```\\n\\nAs shown in the example, policies ca\", \"n be associated with different types of requirements. After the policies are registered, they can be\", \" applied to an action or controller by passing the policy's name as the Policy argument of the Autho\", \"rize attribute (for example, [Authorize(Policy=\\\"EmployeesOnly\\\")]) Policies can have multiple require\", \"ments, not just one (as shown in these examples).\\n\\nIn the previous example, the first AddPolicy call\", \" is just an alternative way of authorizing by role. If [Authorize(Policy=\\\"AdministratorsOnly\\\")] is a\", \"pplied to an API, only users in the Administrator role will be able to access it.\\n\\nThe second [AddPo\", \"licy](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.authorization.authorizationoptions.\", \"addpolicy) call demonstrates an easy way to require that a particular claim should be present for th\", \"e user. The [RequireClaim](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.authorization.\", \"authorizationpolicybuilder.requireclaim) method also optionally takes expected values for the claim.\", \" If values are specified, the requirement is met only if the user has both a claim of the correct ty\", \"pe and one of the specified values. If you're using the JWT bearer authentication middleware, all JW\", \"T properties will be available as user claims.\\n\\nThe most interesting policy shown here is in the thi\", \"rd AddPolicy method, because it uses a custom authorization requirement. By using custom authorizati\", \"on requirements, you can have a great deal of control over how authorization is performed. For this \", \"to work, you must implement these types:\\n\\n- A Requirements type that derives from [IAuthorizationReq\", \"uirement](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.authorization.iauthorizationreq\", \"uirement) and that contains fields specifying the details of the requirement. In the example, this i\", \"s an age field for the sample MinimumAgeRequirement type.\\n- A handler that implements [Authorization\", \"Handler,](https://docs.microsoft.com/dotnet/api/microsoft.aspnetcore.authorization.authorizationhand\", \"ler-1) where T is the type of [IAuthorizationRequirement](https://docs.microsoft.com/dotnet/api/micr\", \"osoft.aspnetcore.authorization.iauthorizationrequirement) that the handler can satisfy. The handler \", \"must implement the [HandleRequirementAsync](https://docs.microsoft.com/dotnet/api/microsoft.aspnetco\", \"re.authorization.authorizationhandler-1.handlerequirementasync) method, which checks whether a speci\", \"fied context that contains information about the user satisfies the requirement.\\n\\nIf the user meets \", \"the requirement, a call to context.Succeed will indicate that the user is authorized. If there are m\", \"ultiple ways that a user might satisfy an authorization requirement, multiple handlers can be create\", \"d.\\n\\nIn addition to registering custom policy requirements with AddPolicy calls, you also need to reg\", \"ister custom requirement handlers via Dependency Injection (services.AddTransient<IAuthorizationHand\", \"ler, MinimumAgeHandler>()).\\n\\nAn example of a custom authorization requirement and handler for checki\", \"ng a user's age (based on a DateOfBirth claim) is available in the ASP.NET Core [authorization docum\", \"entation.](https://docs.asp.net/en/latest/security/authorization/policies.html)\\n\\n## <span id=\\\"page-3\", \"44-0\\\"></span>**Authorization and minimal apis**\\n\\nASP.NET supports minimal APIs as an alternative to \", \"controller-based APIs. Authorization policies are the recommended way to configure authorization for\", \" minimal APIs, as this example demonstrates:\\n\\n```\\n// Program.cs\\nbuilder.Services.AddAuthorizationBui\", \"lder()\\n .AddPolicy(\\\"admin_greetings\\\", policy =>\\n policy\\n .RequireRole(\\\"admin\\\")\\n .RequireScope(\\\"greet\", \"ings_api\\\"));\\n// build the app\\napp.MapGet(\\\"/hello\\\", () => \\\"Hello world!\\\")\\n .RequireAuthorization(\\\"adm\", \"in_greetings\\\");\\n```\\n\\n### <span id=\\\"page-344-1\\\"></span>**Additional resources**\\n\\n- **ASP.NET Core Aut\", \"hentication** [https://learn.microsoft.com/aspnet/core/security/authentication/identity](https://doc\", \"s.microsoft.com/aspnet/core/security/authentication/identity)\\n- **ASP.NET Core Authorization** [http\", \"s://learn.microsoft.com/aspnet/core/security/authorization/introduction](https://docs.microsoft.com/\", \"aspnet/core/security/authorization/introduction)\\n- **Role-based Authorization** [https://learn.micro\", \"soft.com/aspnet/core/security/authorization/roles](https://docs.microsoft.com/aspnet/core/security/a\", \"uthorization/roles)\\n- **Custom Policy-Based Authorization** [https://learn.microsoft.com/aspnet/core\", \"/security/authorization/policies](https://docs.microsoft.com/aspnet/core/security/authorization/poli\", \"cies)\\n- **Authentication and authorization in minimal APIs** [https://learn.microsoft.com/aspnet/cor\", \"e/fundamentals/minimal-apis/security](https://docs.microsoft.com/aspnet/core/fundamentals/minimal-ap\", \"is/security)\\n\\n## <span id=\\\"page-344-2\\\"></span>Store application secrets safely during development\\n\\nT\", \"o connect with protected resources and other services, ASP.NET Core applications typically need to u\", \"se connection strings, passwords, or other credentials that contain sensitive information. These sen\", \"sitive pieces of information are called *secrets*. It's a best practice to not include secrets in so\", \"urce\\n\\ncode and making sure not to store secrets in source control. Instead, you should use the ASP.N\", \"ET Core configuration model to read the secrets from more secure locations.\\n\\nYou must separate the s\", \"ecrets for accessing development and staging resources from the ones used for accessing production r\", \"esources, because different individuals will need access to those different sets of secrets. To stor\", \"e secrets used during development, common approaches are to either store secrets in environment vari\", \"ables or by using the ASP.NET Core Secret Manager tool. For more secure storage in production enviro\", \"nments, microservices can store secrets in an Azure Key Vault.\\n\\n## <span id=\\\"page-345-0\\\"></span>**St\", \"ore secrets in environment variables**\\n\\nOne way to keep secrets out of source code is for developers\", \" to set string-based secrets as [environment variables](https://docs.microsoft.com/aspnet/core/secur\", \"ity/app-secrets#environment-variables) on their development machines. When you use environment varia\", \"bles to store secrets with hierarchical names, such as the ones nested in configuration sections, yo\", \"u must name the variables to include the complete hierarchy of its sections, delimited with colons (\", \":).\\n\\nFor example, setting an environment variable Logging:LogLevel:Default to Debug value would be e\", \"quivalent to a configuration value from the following JSON file:\\n\\n```\\n{\\n \\\"Logging\\\": {\\n \\\"LogLevel\\\": {\", \"\\n \\\"Default\\\": \\\"Debug\\\"\\n }\\n }\\n}\\n```\\n\\nTo access these values from environment variables, the application\", \" just needs to call AddEnvironmentVariables on its ConfigurationBuilder when constructing an IConfig\", \"urationRoot object.\\n\\n### **Note**\\n\\nEnvironment variables are commonly stored as plain text, so if th\", \"e machine or process with the environment variables is compromised, the environment variable values \", \"will be visible.\\n\\n### <span id=\\\"page-345-1\\\"></span>**Store secrets with the ASP.NET Core Secret Mana\", \"ger**\\n\\nThe ASP.NET Core [Secret Manager](https://docs.microsoft.com/aspnet/core/security/app-secrets\", \"#secret-manager) tool provides another method of keeping secrets out of source code **during develop\", \"ment**. To use the Secret Manager tool, install the package **Microsoft.Extensions.Configuration.Sec\", \"retManager** in your project file. Once that dependency is present and has been restored, the dotnet\", \" user-secrets command can be used to set the value of secrets from the command line. These secrets w\", \"ill be stored in a JSON file in the user's profile directory (details vary by OS), away from source \", \"code.\\n\\nSecrets set by the Secret Manager tool are organized by the UserSecretsId property of the pro\", \"ject that's using the secrets. Therefore, you must be sure to set the UserSecretsId property in your\", \" project file, as shown in the snippet below. The default value is a GUID assigned by Visual Studio,\", \" but the actual string is not important as long as it's unique in your computer.\\n\\n```\\n<PropertyGroup\", \">\\n <UserSecretsId>UniqueIdentifyingString</UserSecretsId>\\n</PropertyGroup>\\n```\\n\\nUsing secrets stored\", \" with Secret Manager in an application is accomplished by calling AddUserSecrets<T> on the Configura\", \"tionBuilder instance to include secrets for the application in its configuration. The generic parame\", \"ter T should be a type from the assembly that the UserSecretId was applied to. Usually, using AddUse\", \"rSecrets<Startup> is fine.\\n\\nThe AddUserSecrets<Startup>() is included in the default options for the\", \" Development environment when using the CreateDefaultBuilder method in *Program.cs*.\\n\\n## <span id=\\\"p\", \"age-346-0\\\"></span>Use Azure Key Vault to protect secrets at production time\\n\\nSecrets stored as envir\", \"onment variables or stored by the Secret Manager tool are still stored locally and unencrypted on th\", \"e machine. A more secure option for storing secrets is [Azure Key Vault,](https://azure.microsoft.co\", \"m/services/key-vault/) which provides a secure, central location for storing keys and secrets.\\n\\nThe \", \"**Azure.Extensions.AspNetCore.Configuration.Secrets** package allows an ASP.NET Core application to \", \"read configuration information from Azure Key Vault. To start using secrets from an Azure Key Vault,\", \" you follow these steps:\\n\\n- 1. Register your application as an Azure AD application. (Access to key \", \"vaults is managed by Azure AD.) This can be done through the Azure management portal.\\n  - Alternativ\", \"ely, if you want your application to authenticate using a certificate instead of a password or clien\", \"t secret, you can use the [New-AzADApplication](https://docs.microsoft.com/powershell/module/az.reso\", \"urces/new-azadapplication) PowerShell cmdlet. The certificate that you register with Azure Key Vault\", \" needs only your public key. Your application will use the private key.\\n- 2. Give the registered app\", \"lication access to the key vault by creating a new service principal. You can do this using the foll\", \"owing PowerShell commands:\\n\\n```\\n$sp = New-AzADServicePrincipal -ApplicationId \\\"<Application ID guid>\", \"\\\"\\nSet-AzKeyVaultAccessPolicy -VaultName \\\"<VaultName>\\\" -ServicePrincipalName \\n$sp.ServicePrincipalNam\", \"es[0] -PermissionsToSecrets all -ResourceGroupName \\\"<KeyVault \\nResource Group>\\\"\\n```\\n\\n3. Include the \", \"key vault as a configuration source in your application by calling the AzureKeyVaultConfigurationExt\", \"ensions.AddAzureKeyVault extension method when you create an [IConfigurationRoot](https://docs.micro\", \"soft.com/dotnet/api/microsoft.extensions.configuration.iconfigurationroot) instance.\\n\\nNote that call\", \"ing AddAzureKeyVault requires the application ID that was registered and given access to the key vau\", \"lt in the previous steps. Or you can firstly running the Azure CLI command: az login, then using an \", \"overload of AddAzureKeyVault that takes a DefaultAzureCredential in place of the client.\\n\\n## **Impor\", \"tant**\\n\\nWe recommend that you register Azure Key Vault as the last configuration provider, so it can\", \" override configuration values from previous providers.\\n\\n## <span id=\\\"page-347-0\\\"></span>**Additiona\", \"l resources**\\n\\n[https://learn.microsoft.com/azure/architecture/multitenant-identity](https://docs.mi\", \"crosoft.com/azure/architecture/multitenant-identity)\\n\\n\\u2022 **Using Azure Key Vault to protect applicati\", \"on secrets**\\n\\n\\u2022 **Safe storage of app secrets during development**\\n\\n- [https://learn.microsoft.com/a\", \"spnet/core/security/app-secrets](https://docs.microsoft.com/aspnet/core/security/app-secrets) \\u2022 **Co\", \"nfiguring data protection**\\n- [https://learn.microsoft.com/aspnet/core/security/data-protection/conf\", \"iguration/overview](https://docs.microsoft.com/aspnet/core/security/data-protection/configuration/ov\", \"erview)\\n- **Data Protection key management and lifetime in ASP.NET Core** [https://learn.microsoft.c\", \"om/aspnet/core/security/data-protection/configuration/default](https://docs.microsoft.com/aspnet/cor\", \"e/security/data-protection/configuration/default-settings)[settings](https://docs.microsoft.com/aspn\", \"et/core/security/data-protection/configuration/default-settings)\\n\\n**CHAPTER** 9\\n\\n# <span id=\\\"page-34\", \"8-0\\\"></span>.NET Microservices Architecture key takeaways\\n\\nAs a summary and key takeaways, the follo\", \"wing are the most important conclusions from this guide.\\n\\n**Benefits of using containers.** Containe\", \"r-based solutions provide important cost savings because they help reduce deployment problems caused\", \" by failing dependencies in production environments. Containers significantly improve DevOps and pro\", \"duction operations.\\n\\n**Containers will be ubiquitous.** Docker-based containers are becoming the de \", \"facto standard in the industry, supported by key vendors in the Windows and Linux ecosystems, such a\", \"s Microsoft, Amazon AWS, Google, and IBM. Docker will probably soon be ubiquitous in both the cloud \", \"and on-premises datacenters.\\n\\n**Containers as a unit of deployment.** A Docker container is becoming\", \" the standard unit of deployment for any server-based application or service.\\n\\n**Microservices.** Th\", \"e microservices architecture is becoming the preferred approach for distributed and large or complex\", \" mission-critical applications based on many independent subsystems in the form of autonomous servic\", \"es. In a microservice-based architecture, the application is built as a collection of services that \", \"are developed, tested, versioned, deployed, and scaled independently. Each service can include any r\", \"elated autonomous database.\\n\\n**Domain-driven design and SOA.** The microservices architecture patter\", \"ns derive from serviceoriented architecture (SOA) and domain-driven design (DDD). When you design an\", \"d develop microservices for environments with evolving business needs and rules, it's important to c\", \"onsider DDD approaches and patterns.\\n\\n**Microservices challenges.** Microservices offer many powerfu\", \"l capabilities, like independent deployment, strong subsystem boundaries, and technology diversity. \", \"However, they also raise many new challenges related to distributed application development, such as\", \" fragmented and independent data models, resilient communication between microservices, eventual con\", \"sistency, and operational complexity that results from aggregating logging and monitoring informatio\", \"n from multiple microservices. These aspects introduce a much higher complexity level than a traditi\", \"onal monolithic application. As a result, only specific scenarios are suitable for microservice-base\", \"d applications. These include large and complex applications with multiple evolving subsystems. In t\", \"hese cases, it's worth investing in a more complex software architecture, because it will provide be\", \"tter long-term agility and application maintenance.\\n\\n**Containers for any application.** Containers \", \"are convenient for microservices, but can also be useful for monolithic applications based on the tr\", \"aditional .NET Framework, when using Windows Containers. The benefits of using Docker, such as solvi\", \"ng many deployment-to-production issues and providing state-of-the-art Dev and Test environments, ap\", \"ply to many different types of applications. **CLI versus IDE.** With Microsoft tools, you can devel\", \"op containerized .NET applications using your\\n\\npreferred approach. You can develop with a CLI and an\", \" editor-based environment by using the Docker CLI and Visual Studio Code. Or you can use an IDE-focu\", \"sed approach with Visual Studio and its unique features for Docker, such as multi-container debuggin\", \"g.\\n\\n**Resilient cloud applications.** In cloud-based systems and distributed systems in general, the\", \"re is always the risk of partial failure. Since clients and services are separate processes (contain\", \"ers), a service might not be able to respond in a timely way to a client's request. For example, a s\", \"ervice might be down because of a partial failure or for maintenance; the service might be overloade\", \"d and responding slowly to requests; or it might not be accessible for a short time because of netwo\", \"rk issues. Therefore, a cloud-based application must embrace those failures and have a strategy in p\", \"lace to respond to those failures. These strategies can include retry policies (resending messages o\", \"r retrying requests) and implementing circuit-breaker patterns to avoid exponential load of repeated\", \" requests. Basically, cloud-based applications must have resilient mechanisms\\u2014either based on cloud \", \"infrastructure or custom, as the high-level ones provided by orchestrators or service buses.\\n\\n**Secu\", \"rity.** Our modern world of containers and microservices can expose new vulnerabilities. There are s\", \"everal ways to implement basic application security, based on authentication and authorization. Howe\", \"ver, container security must consider additional key components that result in inherently safer appl\", \"ications. A critical element of building safer apps is having a secure way of communicating with oth\", \"er apps and systems, something that often requires credentials, tokens, passwords, and the like, com\", \"monly referred to as application secrets. Any secure solution must follow security best practices, s\", \"uch as encrypting secrets while in transit and at rest, and preventing secrets from leaking when con\", \"sumed by the final application. Those secrets need to be stored and kept safely, as when using Azure\", \" Key Vault.\\n\\n**Orchestrators.** Container-based orchestrators, such as Azure Kubernetes Service and \", \"Azure Service Fabric are key part of any significant microservice and container-based application. T\", \"hese applications carry with them high complexity, scalability needs, and go through constant evolut\", \"ion. This guide has introduced orchestrators and their role in microservice-based and container-base\", \"d solutions. If your application needs are moving you toward complex containerized apps, you will fi\", \"nd it useful to seek out additional resources for learning more about orchestrators.\"]"