"[\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\fEDITION v1.0.3 \\n\\nRefer changelog for the \", \"book updates and community contributions. \\n\\nPUBLISHED BY \\n\\nMicrosoft Developer Division, .NET, and V\", \"isual Studio product teams \\n\\nA division of Microsoft Corporation \\n\\nOne Microsoft Way \\n\\nRedmond, Wash\", \"ington 98052-6399 \\n\\nCopyright \\u00a9 2023 by Microsoft Corporation \\n\\nAll rights reserved. No part of the \", \"contents of this book may be reproduced or transmitted in any \\nform or by any means without the writ\", \"ten permission of the publisher. \\n\\nThis book is provided \\u201cas-is\\u201d and expresses the author\\u2019s views an\", \"d opinions. The views, opinions, and \\ninformation expressed in this book, including URL and other In\", \"ternet website references, may change \\nwithout notice. \\n\\nSome examples depicted herein are provided \", \"for illustration only and are fictitious. No real association \\nor connection is intended or should b\", \"e inferred. \\n\\nMicrosoft and the trademarks listed at https://www.microsoft.com on the \\u201cTrademarks\\u201d w\", \"ebpage are \\ntrademarks of the Microsoft group of companies. \\n\\nMac and macOS are trademarks of Apple \", \"Inc. \\n\\nThe Docker whale logo is a registered trademark of Docker, Inc. Used by permission. \\n\\nAll oth\", \"er marks and logos are property of their respective owners. \\n\\nAuthors: \\n\\nRob Vettor, Principal MTC (\", \"Microsoft Technology Center) Architect for Cloud App Innovation - \\nthinkingincloudnative.com, Micros\", \"oft \\n\\nSteve \\u201cardalis\\u201d Smith, Software Architect and Trainer - Ardalis.com \\n\\nParticipants and Reviewe\", \"rs: \\n\\nCesar De la Torre, Principal Program Manager, .NET team, Microsoft \\n\\nNish Anil, Senior Program\", \" Manager, .NET team, Microsoft \\n\\nJeremy Likness, Senior Program Manager, .NET team, Microsoft \\n\\nCeci\", \"l Phillip, Senior Cloud Advocate, Microsoft \\n\\nSumit Ghosh, Principal Consultant at Neudesic \\n\\nEditor\", \"s: \\n\\nMaira Wenzel, Program Manager, .NET team, Microsoft \\n\\n \\n\\fDavid Pine, Senior Content Developer, \", \".NET docs, Microsoft \\n\\nVersion \\n\\nThis guide has been written to cover .NET 7 version along with many\", \" additional updates related to \\nthe same \\u201cwave\\u201d of technologies (that is, Azure and additional third\", \"-party technologies) coinciding in \\ntime with the .NET 7 release. \\n\\nWho should use this guide \\n\\nThe \", \"audience for this guide is mainly developers, development leads, and architects who are \\ninterested \", \"in learning how to build applications designed for the cloud. \\n\\nA secondary audience is technical de\", \"cision-makers who plan to choose whether to build their \\napplications using a cloud-native approach.\", \" \\n\\nHow you can use this guide \\n\\nThis guide begins by defining cloud native and introducing a referen\", \"ce application built using cloud-\\nnative principles and technologies. Beyond these first two chapter\", \"s, the rest of the book is broken up \\ninto specific chapters focused on topics common to most cloud-\", \"native applications. You can jump to \\nany of these chapters to learn about cloud-native approaches t\", \"o: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nData and data access \\n\\nCommunication patterns \\n\\nScaling and scalability \\n\\nAppli\", \"cation resiliency \\n\\n\\u2022  Monitoring and health \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nIdentity and security \\n\\nDevOps \\n\\nThis guide i\", \"s available both in PDF form and online. Feel free to forward this document or links to its \\nonline \", \"version to your team to help ensure common understanding of these topics. Most of these \\ntopics bene\", \"fit from a consistent understanding of the underlying principles and patterns, as well as \\nthe trade\", \"-offs involved in decisions related to these topics. Our goal with this document is to equip \\nteams \", \"and their leaders with the information they need to make well-informed decisions for their \\napplicat\", \"ions\\u2019 architecture, development, and hosting. \\n\\n \\n\\fContents \\n\\nIntroduction to cloud-native applicati\", \"ons ............................................................................ 1 \\n\\nCloud-native co\", \"mputing ............................................................................................\", \"...................................................... 3 \\n\\nWhat is Cloud Native? ...................\", \"....................................................................................................\", \"..................................... 4 \\n\\nThe pillars of cloud native ..............................\", \"....................................................................................................\", \".............. 5 \\n\\nThe cloud .......................................................................\", \"....................................................................................................\", \"..... 5 \\n\\nModern design ............................................................................\", \"......................................................................................... 6 \\n\\nMicros\", \"ervices ............................................................................................\", \"............................................................................ 9 \\n\\nContainers ........\", \"....................................................................................................\", \"............................................................... 12 \\n\\nBacking services ..............\", \"....................................................................................................\", \".............................................. 15 \\n\\nAutomation .....................................\", \"....................................................................................................\", \"................................ 17 \\n\\nCandidate apps for cloud native ..............................\", \"....................................................................................................\", \"... 19 \\n\\nModernizing legacy apps ...................................................................\", \"........................................................................... 19 \\n\\nSummary ...........\", \"....................................................................................................\", \"............................................................... 21 \\n\\nIntroducing eShopOnContainers r\", \"eference app ................................................................ 22 \\n\\nFeatures and requ\", \"irements ...........................................................................................\", \".................................................... 23 \\n\\nOverview of the code .....................\", \"....................................................................................................\", \"................................. 25 \\n\\nUnderstanding microservices .................................\", \"....................................................................................................\", \"...... 27 \\n\\nMapping eShopOnContainers to Azure Services ............................................\", \"......................................................... 27 \\n\\nContainer orchestration and clusterin\", \"g ..................................................................................................\", \"................. 28 \\n\\nAPI Gateway .................................................................\", \"....................................................................................................\", \".. 28 \\n\\nData .......................................................................................\", \"................................................................................................ 29 \", \"\\n\\nEvent Bus ........................................................................................\", \"..................................................................................... 30 \\n\\nResilienc\", \"y ..................................................................................................\", \"........................................................................... 30 \\n\\nDeploying eShopOnCo\", \"ntainers to Azure ..................................................................................\", \".................................. 30 \\n\\nAzure Kubernetes Service ...................................\", \"....................................................................................................\", \"...... 30 \\n\\nDeploying to Azure Kubernetes Service using Helm .......................................\", \".................................................. 30 \\n\\nAzure Functions and Logic Apps (Serverless) \", \"....................................................................................................\", \"... 32 \\n\\nCentralized configuration .................................................................\", \"................................................................................. 33 \\n\\ni \\n\\nContents \", \"\\n\\n \\n \\n\\fAzure App Configuration .....................................................................\", \"......................................................................... 33 \\n\\nAzure Key Vault .....\", \"....................................................................................................\", \"........................................................ 34 \\n\\nConfiguration in eShop ...............\", \"....................................................................................................\", \"............................... 34 \\n\\nReferences ....................................................\", \"....................................................................................................\", \"................... 34 \\n\\nScaling cloud-native applications .........................................\", \"............................................... 36 \\n\\nLeveraging containers and orchestrators .......\", \"....................................................................................................\", \"......... 36 \\n\\nChallenges with monolithic deployments ..............................................\", \"................................................................ 36 \\n\\nWhat are the benefits of conta\", \"iners and orchestrators? ...........................................................................\", \"....... 38 \\n\\nWhat are the scaling benefits? ........................................................\", \"............................................................................ 40 \\n\\nWhat scenarios are\", \" ideal for containers and orchestrators?............................................................\", \"............... 42 \\n\\nWhen should you avoid using containers and orchestrators? .....................\", \".................................................. 42 \\n\\nDevelopment resources ......................\", \"....................................................................................................\", \"....................... 42 \\n\\nLeveraging serverless functions .......................................\", \"............................................................................................... 46 \\n\", \"\\nWhat is serverless? ...............................................................................\", \"............................................................................ 47 \\n\\nWhat challenges ar\", \"e solved by serverless? ............................................................................\", \"................................ 47 \\n\\nWhat is the difference between a microservice and a serverless\", \" function? ............................................. 47 \\n\\nWhat scenarios are appropriate for ser\", \"verless? ...........................................................................................\", \"........ 47 \\n\\nWhen should you avoid serverless? ....................................................\", \"...................................................................... 48 \\n\\nCombining containers and\", \" serverless approaches .............................................................................\", \"..................... 49 \\n\\nWhen does it make sense to use containers with serverless? ..............\", \".......................................................... 49 \\n\\nWhen should you avoid using containe\", \"rs with Azure Functions? ................................................................ 49 \\n\\nHow t\", \"o combine serverless and Docker containers .........................................................\", \".................................. 49 \\n\\nHow to combine serverless and Kubernetes with KEDA .........\", \"......................................................................... 50 \\n\\nDeploying containers \", \"in Azure ...........................................................................................\", \"............................................. 50 \\n\\nAzure Container Registry ........................\", \"....................................................................................................\", \".................. 50 \\n\\nACR Tasks ..................................................................\", \"....................................................................................................\", \"...... 52 \\n\\nAzure Kubernetes Service ...............................................................\", \".............................................................................. 52 \\n\\nAzure Bridge to \", \"Kubernetes .........................................................................................\", \"................................................ 53 \\n\\nScaling containers and serverless applications\", \" ...................................................................................................\", \"...... 53 \\n\\nThe simple solution: scaling up ........................................................\", \".......................................................................... 53 \\n\\nScaling out cloud-na\", \"tive apps ..........................................................................................\", \".......................................... 54 \\n\\nOther container deployment options .................\", \"....................................................................................................\", \"...... 55 \\n\\nii \\n\\nContents \\n\\n \\n\\fWhen does it make sense to deploy to App Service for Containers? ....\", \"..................................................... 55 \\n\\nHow to deploy to App Service for Containe\", \"rs .................................................................................................\", \"..... 55 \\n\\nWhen does it make sense to deploy to Azure Container Instances? .........................\", \"................................. 55 \\n\\nHow to deploy an app to Azure Container Instances ...........\", \"............................................................................. 55 \\n\\nReferences ......\", \"....................................................................................................\", \"................................................................. 56 \\n\\nCloud-native communication pa\", \"tterns ............................................................................... 58 \\n\\nCommunic\", \"ation considerations ...............................................................................\", \"....................................................... 58 \\n\\nFront-end client communication ........\", \"....................................................................................................\", \"........................ 60 \\n\\nSimple Gateways ......................................................\", \"....................................................................................................\", \".... 62 \\n\\nAzure Application Gateway ................................................................\", \".......................................................................... 63 \\n\\nAzure API Management\", \" ...................................................................................................\", \".............................................. 63 \\n\\nReal-time communication ........................\", \"....................................................................................................\", \"................ 66 \\n\\nService-to-service communication .............................................\", \"................................................................................... 67 \\n\\nQueries ...\", \"....................................................................................................\", \".......................................................................... 68 \\n\\nCommands ...........\", \"....................................................................................................\", \"........................................................... 71 \\n\\nEvents ............................\", \"....................................................................................................\", \".................................................... 74 \\n\\ngRPC .....................................\", \"....................................................................................................\", \".................................................. 80 \\n\\nWhat is gRPC? ..............................\", \"....................................................................................................\", \"................................. 80 \\n\\ngRPC Benefits ...............................................\", \"....................................................................................................\", \"................. 80 \\n\\nProtocol Buffers ............................................................\", \"....................................................................................................\", \" 81 \\n\\ngRPC support in .NET .........................................................................\", \"............................................................................ 81 \\n\\ngRPC usage .......\", \"....................................................................................................\", \".............................................................. 82 \\n\\ngRPC implementation ............\", \"....................................................................................................\", \".................................... 83 \\n\\nLooking ahead ............................................\", \"....................................................................................................\", \"................... 85 \\n\\nService Mesh communication infrastructure .................................\", \"............................................................................ 85 \\n\\nSummary ..........\", \"....................................................................................................\", \"................................................................ 86 \\n\\nCloud-native data patterns ...\", \"............................................................................................... 88 \\n\", \"\\nDatabase-per-microservice, why? ...................................................................\", \"............................................................... 89 \\n\\nCross-service queries .........\", \"....................................................................................................\", \".............................................. 90 \\n\\nDistributed transactions .......................\", \"....................................................................................................\", \".......................... 91 \\n\\nHigh volume data ...................................................\", \"....................................................................................................\", \".......... 93 \\n\\nCQRS ...............................................................................\", \"....................................................................................................\", \".. 93 \\n\\niii \\n\\nContents \\n\\n \\n\\fEvent sourcing .........................................................\", \"....................................................................................................\", \"...... 94 \\n\\nRelational vs. NoSQL data ..............................................................\", \"................................................................................... 96 \\n\\nThe CAP the\", \"orem ...............................................................................................\", \".............................................................. 97 \\n\\nConsiderations for relational vs\", \". NoSQL systems ....................................................................................\", \"............ 99 \\n\\nDatabase as a Service ............................................................\", \"......................................................................................... 99 \\n\\nAzure\", \" relational databases ..............................................................................\", \"........................................................... 100 \\n\\nAzure SQL Database................\", \"....................................................................................................\", \".................................. 100 \\n\\nOpen-source databases in Azure ............................\", \"................................................................................................. 10\", \"1 \\n\\nNoSQL data in Azure ............................................................................\", \"........................................................................ 102 \\n\\nNewSQL databases ....\", \"....................................................................................................\", \"................................................ 106 \\n\\nData migration to the cloud .................\", \"....................................................................................................\", \"................. 108 \\n\\nCaching in a cloud-native app ..............................................\", \"......................................................................................... 108 \\n\\nWhy?\", \" ...................................................................................................\", \"................................................................................. 108 \\n\\nCaching arch\", \"itecture ...........................................................................................\", \".......................................................... 109 \\n\\nAzure Cache for Redis .............\", \"....................................................................................................\", \"................................. 110 \\n\\nElasticsearch in a cloud-native app ........................\", \"....................................................................................................\", \". 110 \\n\\nSummary ....................................................................................\", \"........................................................................................ 111 \\n\\nCloud\", \"-native resiliency .................................................................................\", \"...................... 113 \\n\\nApplication resiliency patterns .......................................\", \"............................................................................................... 114 \", \"\\n\\nCircuit breaker pattern ..........................................................................\", \"....................................................................... 116 \\n\\nTesting for resiliency\", \" ...................................................................................................\", \".................................................. 117 \\n\\nAzure platform resiliency .................\", \"....................................................................................................\", \"............................ 117 \\n\\nDesign with resiliency ..........................................\", \"....................................................................................................\", \"..... 118 \\n\\nDesign with redundancy..................................................................\", \"............................................................................ 118 \\n\\nDesign for scalab\", \"ility ..............................................................................................\", \"....................................................... 120 \\n\\nBuilt-in retry in services ...........\", \"....................................................................................................\", \"................................ 121 \\n\\nResilient communications ....................................\", \"....................................................................................................\", \"........ 122 \\n\\nService mesh ........................................................................\", \"............................................................................................ 123 \\n\\nI\", \"stio and Envoy .....................................................................................\", \"........................................................................... 124 \\n\\nIntegration with A\", \"zure Kubernetes Services ...........................................................................\", \"............................ 125 \\n\\nMonitoring and health ...........................................\", \"............................................................. 126 \\n\\nObservability patterns .........\", \"....................................................................................................\", \".......................................... 126 \\n\\niv \\n\\nContents \\n\\n \\n\\fWhen to use logging ............\", \"....................................................................................................\", \".................................... 126 \\n\\nChallenges with detecting and responding to potential app\", \" health issues ........................................... 130 \\n\\nChallenges with reacting to critica\", \"l problems in cloud-native apps .......................................................... 130 \\n\\nLog\", \"ging with Elastic Stack ............................................................................\", \"................................................................... 131 \\n\\nElastic Stack ............\", \"....................................................................................................\", \"...................................................... 131 \\n\\nWhat are the advantages of Elastic Stac\", \"k? .................................................................................................\", \"......... 132 \\n\\nLogstash ...........................................................................\", \".................................................................................................. 1\", \"32 \\n\\nElasticsearch .................................................................................\", \".................................................................................... 133 \\n\\nVisualizi\", \"ng information with Kibana web dashboards ..........................................................\", \".......................... 133 \\n\\nInstalling Elastic Stack on Azure .................................\", \".............................................................................................. 134 \\n\", \"\\nReferences ........................................................................................\", \"................................................................................. 134 \\n\\nMonitoring i\", \"n Azure Kubernetes Services ........................................................................\", \"......................................... 134 \\n\\nAzure Monitor for Containers .......................\", \"....................................................................................................\", \"........ 134 \\n\\nLog.Finalize() ......................................................................\", \".............................................................................................. 136 \\n\", \"\\nAzure Monitor .....................................................................................\", \"................................................................................. 136 \\n\\nGathering lo\", \"gs and metrics .....................................................................................\", \"................................................... 137 \\n\\nReporting data ...........................\", \"....................................................................................................\", \"................................. 137 \\n\\nDashboards .................................................\", \"....................................................................................................\", \".................. 138 \\n\\nAlerts ....................................................................\", \"....................................................................................................\", \"........... 140 \\n\\nReferences .......................................................................\", \".................................................................................................. 1\", \"41 \\n\\nCloud-native identity .........................................................................\", \"................................. 142 \\n\\nReferences .................................................\", \"....................................................................................................\", \"........................ 142 \\n\\nAuthentication and authorization in cloud-native apps ...............\", \"...................................................................... 142 \\n\\nReferences ............\", \"....................................................................................................\", \"......................................................... 143 \\n\\nAzure Active Directory .............\", \"....................................................................................................\", \"..................................... 143 \\n\\nReferences .............................................\", \"....................................................................................................\", \"........................ 143 \\n\\nIdentityServer for cloud-native applications ........................\", \".................................................................................... 144 \\n\\nCommon we\", \"b app scenarios ....................................................................................\", \"................................................. 144 \\n\\nGetting started ............................\", \"....................................................................................................\", \"................................ 145 \\n\\nConfiguration ...............................................\", \"....................................................................................................\", \"................ 145 \\n\\nJavaScript clients ..........................................................\", \".................................................................................................. 1\", \"46 \\n\\nReferences ....................................................................................\", \"..................................................................................... 146 \\n\\nv \\n\\nCont\", \"ents \\n\\n \\n\\fCloud-native security ....................................................................\", \"...................................... 147 \\n\\nAzure security for cloud-native apps ..................\", \"....................................................................................................\", \".... 147 \\n\\nThreat modeling .........................................................................\", \".................................................................................... 148 \\n\\nPrinciple\", \" of least privilege ................................................................................\", \"........................................................... 148 \\n\\nPenetration testing ..............\", \"....................................................................................................\", \"...................................... 149 \\n\\nMonitoring ............................................\", \"....................................................................................................\", \"........................ 149 \\n\\nSecuring the build ..................................................\", \"....................................................................................................\", \".... 149 \\n\\nBuilding secure code ....................................................................\", \"................................................................................ 150 \\n\\nBuilt-in secu\", \"rity ...............................................................................................\", \"................................................................ 150 \\n\\nAzure network infrastructure \", \"....................................................................................................\", \"................................. 150 \\n\\nRole-based access control for restricting access to Azure re\", \"sources........................................................ 152 \\n\\nSecurity Principals ..........\", \"....................................................................................................\", \"............................................ 152 \\n\\nRoles ...........................................\", \"....................................................................................................\", \"..................................... 153 \\n\\nScopes .................................................\", \"....................................................................................................\", \"........................... 154 \\n\\nDeny .............................................................\", \"....................................................................................................\", \"................... 154 \\n\\nChecking access ..........................................................\", \"....................................................................................................\", \" 154 \\n\\nSecuring secrets ............................................................................\", \".................................................................................. 155 \\n\\nAzure Key V\", \"ault ...............................................................................................\", \"................................................................ 155 \\n\\nKubernetes ..................\", \"....................................................................................................\", \".................................................. 155 \\n\\nEncryption in transit and at rest .........\", \"....................................................................................................\", \".................. 156 \\n\\nKeeping secure ............................................................\", \"....................................................................................................\", \" 160 \\n\\nDevOps ......................................................................................\", \"........................................... 161 \\n\\nAzure DevOps .....................................\", \"....................................................................................................\", \".............................. 162 \\n\\nGitHub Actions ................................................\", \"....................................................................................................\", \"................. 163 \\n\\nSource control .............................................................\", \"....................................................................................................\", \"..... 163 \\n\\nRepository per microservice ............................................................\", \".......................................................................... 164 \\n\\nSingle repository .\", \"....................................................................................................\", \"....................................................... 166 \\n\\nStandard directory structure .........\", \"....................................................................................................\", \"......................... 167 \\n\\nTask management ....................................................\", \"....................................................................................................\", \"...... 167 \\n\\nCI/CD pipelines .......................................................................\", \"............................................................................................. 169 \\n\\n\", \"Azure Builds .......................................................................................\", \"............................................................................... 170 \\n\\nAzure DevOps r\", \"eleases ............................................................................................\", \".................................................... 172 \\n\\nvi \\n\\nContents \\n\\n \\n\\fEverybody gets a build\", \" pipeline ..........................................................................................\", \"..................................... 173 \\n\\nVersioning releases ....................................\", \"....................................................................................................\", \"................ 173 \\n\\nFeature flags ...............................................................\", \"....................................................................................................\", \"...... 173 \\n\\nImplementing feature flags ............................................................\", \"............................................................................ 174 \\n\\nInfrastructure as\", \" code ..............................................................................................\", \"......................................................... 175 \\n\\nAzure Resource Manager templates ...\", \"....................................................................................................\", \"............... 175 \\n\\nTerraform ....................................................................\", \"....................................................................................................\", \"... 176 \\n\\nAzure CLI Scripts and Tasks...............................................................\", \"......................................................................... 177 \\n\\nCloud Native Applica\", \"tion Bundles .......................................................................................\", \"........................................ 178 \\n\\nDevOps Decisions ....................................\", \"....................................................................................................\", \".................. 180 \\n\\nReferences ................................................................\", \"....................................................................................................\", \"..... 180 \\n\\nSummary: Architecting cloud-native apps ................................................\", \"....................... 181 \\n\\nvii \\n\\nContents \\n\\n \\n\\fCHAPTER  1 \\n\\nIntroduction to cloud-\\nnative applica\", \"tions \\n\\nAnother day, at the office, working on \\u201cthe next big thing.\\u201d \\n\\nYour cellphone rings. It\\u2019s yo\", \"ur friendly recruiter - the one who calls daily with exciting new \\nopportunities. \\n\\nBut this time it\", \"\\u2019s different: Start-up, equity, and plenty of funding. \\n\\nThe mention of the cloud, microservices, an\", \"d cutting-edge technology pushes you over the edge. \\n\\nFast forward a few weeks and you\\u2019re now a new \", \"employee in a design session architecting a major \\neCommerce application. You\\u2019re going to compete wi\", \"th the leading eCommerce sites. \\n\\nHow will you build it? \\n\\nIf you follow the guidance from past 15 y\", \"ears, you\\u2019ll most likely build the system shown in Figure 1.1. \\n\\nFigure 1-1. Traditional monolithic \", \"design \\n\\nYou construct a large core application containing all of your domain logic. It includes mod\", \"ules such as \\nIdentity, Catalog, Ordering, and more. They directly communicate with each other withi\", \"n a single \\nserver process. The modules share a large relational database. The core exposes function\", \"ality via an \\nHTML interface and a mobile app. \\n\\nCongratulations! You just created a monolithic appl\", \"ication. \\n\\nNot all is bad. Monoliths offer some distinct advantages. For example, they\\u2019re straightfo\", \"rward to\\u2026 \\n\\n1 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nb\", \"uild \\n\\ntest \\n\\ndeploy \\n\\ntroubleshoot \\n\\nvertically scale \\n\\nMany successful apps that exist today were \", \"created as monoliths. The app is a hit and continues to \\nevolve, iteration after iteration, adding m\", \"ore functionality. \\n\\nAt some point, however, you begin to feel uncomfortable. You find yourself losi\", \"ng control of the \\napplication. As time goes on, the feeling becomes more intense, and you eventuall\", \"y enter a state \\nknown as the Fear Cycle: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nThe app has become s\", \"o overwhelmingly complicated that no single person understands it. \\n\\nYou fear making changes - each \", \"change has unintended and costly side effects. \\n\\nNew features/fixes become tricky, time-consuming, a\", \"nd expensive to implement. \\n\\nEach release becomes as small as possible and requires a full deploymen\", \"t of the entire \\napplication. \\n\\nOne unstable component can crash the entire system. \\n\\nNew technologi\", \"es and frameworks aren\\u2019t an option. \\n\\nIt\\u2019s difficult to implement agile delivery methodologies. \\n\\nAr\", \"chitectural erosion sets in as the code base deteriorates with never-ending \\u201cquick fixes.\\u201d \\n\\nFinally\", \", the consultants come in and tell you to rewrite it. \\n\\nSound familiar? \\n\\nMany organizations have ad\", \"dressed this monolithic fear cycle by adopting a cloud-native approach to \\nbuilding systems. Figure \", \"1-2 shows the same system built applying cloud-native techniques and \\npractices. \\n\\n2 \\n\\nCHAPTER 1 | I\", \"ntroduction to cloud-native applications \\n\\n \\n \\n\\fFigure 1-2. Cloud-native design \\n\\nNote how the appli\", \"cation is decomposed across a set of small isolated microservices. Each service is \\nself-contained a\", \"nd encapsulates its own code, data, and dependencies. Each is deployed in a software \\ncontainer and \", \"managed by a container orchestrator. Instead of a large relational database, each \\nservice owns it o\", \"wn datastore, the type of which vary based upon the data needs. Note how some \\nservices depend on a \", \"relational database, but other on NoSQL databases. One service stores its state \\nin a distributed ca\", \"che. Note how all traffic routes through an API Gateway service that is responsible \\nfor routing tra\", \"ffic to the core back-end services and enforcing many cross-cutting concerns. Most \\nimportantly, the\", \" application takes full advantage of the scalability, availability, and resiliency features \\nfound i\", \"n modern cloud platforms. \\n\\nCloud-native computing \\n\\nHmm\\u2026 We just used the term, Cloud Native. Your \", \"first thought might be, \\u201cWhat exactly does that \\nmean?\\u201d Another industry buzzword concocted by softw\", \"are vendors to market more stuff?\\u201d \\n\\nFortunately it\\u2019s far different, and hopefully this book will he\", \"lp convince you. \\n\\nWithin a short time, cloud native has become a driving trend in the software indu\", \"stry. It\\u2019s a new way \\nto construct large, complex systems. The approach takes full advantage of mode\", \"rn software \\ndevelopment practices, technologies, and cloud infrastructure. Cloud native changes the\", \" way you \\ndesign, implement, deploy, and operationalize systems. \\n\\nUnlike the continuous hype that d\", \"rives our industry, cloud native is for-real. Consider the Cloud Native \\nComputing Foundation (CNCF)\", \", a consortium of over 400 major corporations. Its charter is to make \\n\\n3 \\n\\nCHAPTER 1 | Introduction\", \" to cloud-native applications \\n\\n \\n \\n \\n\\fcloud-native computing ubiquitous across technology and cloud\", \" stacks. As one of the most influential \\nopen-source groups, it hosts many of the fastest-growing op\", \"en source-projects in GitHub. These \\nprojects include Kubernetes, Prometheus, Helm, Envoy, and gRPC.\", \" \\n\\nThe CNCF fosters an ecosystem of open-source and vendor-neutrality. Following that lead, this boo\", \"k \\npresents cloud-native principles, patterns, and best practices that are technology agnostic. At t\", \"he \\nsame time, we discuss the services and infrastructure available in the Microsoft Azure cloud for\", \" \\nconstructing cloud-native systems. \\n\\nSo, what exactly is Cloud Native? Sit back, relax, and let us\", \" help you explore this new world. \\n\\nWhat is Cloud Native? \\n\\nStop what you\\u2019re doing and ask your coll\", \"eagues to define the term \\u201cCloud Native\\u201d. There\\u2019s a good \\nchance you\\u2019ll get several different answer\", \"s. \\n\\nLet\\u2019s start with a simple definition: \\n\\nCloud-native architecture and technologies are an appro\", \"ach to designing, constructing, and operating \\nworkloads that are built in the cloud and take full a\", \"dvantage of the cloud computing model. \\n\\nThe Cloud Native Computing Foundation provides the official\", \" definition: \\n\\nCloud-native technologies empower organizations to build and run scalable application\", \"s in modern, \\ndynamic environments such as public, private, and hybrid clouds. Containers, service m\", \"eshes, \\nmicroservices, immutable infrastructure, and declarative APIs exemplify this approach. \\n\\nThe\", \"se techniques enable loosely coupled systems that are resilient, manageable, and observable. \\nCombin\", \"ed with robust automation, they allow engineers to make high-impact changes frequently and \\npredicta\", \"bly with minimal toil. \\n\\nCloud native is about speed and agility. Business systems are evolving from\", \" enabling business \\ncapabilities to weapons of strategic transformation that accelerate business vel\", \"ocity and growth. It\\u2019s \\nimperative to get new ideas to market immediately. \\n\\nAt the same time, busin\", \"ess systems have also become increasingly complex with users demanding \\nmore. They expect rapid resp\", \"onsiveness, innovative features, and zero downtime. Performance \\nproblems, recurring errors, and the\", \" inability to move fast are no longer acceptable. Your users will visit \\nyour competitor. Cloud-nati\", \"ve systems are designed to embrace rapid change, large scale, and \\nresilience. \\n\\nHere are some compa\", \"nies who have implemented cloud-native techniques. Think about the speed, \\nagility, and scalability \", \"they\\u2019ve achieved. \\n\\nCompany \\n\\nExperience \\n\\nNetflix \\n\\nUber \\n\\n4 \\n\\nHas 600+ services in production. Dep\", \"loys 100 \\ntimes per day. \\n\\nHas 1,000+ services in production. Deploys \\nseveral thousand times each w\", \"eek. \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fCompany \\n\\nExperience \\n\\nWeChat \\n\\n\", \"Has 3,000+ services in production. Deploys \\n1,000 times a day. \\n\\nAs you can see, Netflix, Uber, and,\", \" WeChat expose cloud-native systems that consist of many \\nindependent services. This architectural s\", \"tyle enables them to rapidly respond to market conditions. \\nThey instantaneously update small areas \", \"of a live, complex application, without a full redeployment. \\nThey individually scale services as ne\", \"eded. \\n\\nThe pillars of cloud native \\n\\nThe speed and agility of cloud native derive from many factors\", \". Foremost is cloud infrastructure. But \\nthere\\u2019s more: Five other foundational pillars shown in Figu\", \"re 1-3 also provide the bedrock for cloud-\\nnative systems. \\n\\nFigure 1-3. Cloud-native foundational p\", \"illars \\n\\nLet\\u2019s take some time to better understand the significance of each pillar. \\n\\nThe cloud \\n\\nCl\", \"oud-native systems take full advantage of the cloud service model. \\n\\nDesigned to thrive in a dynamic\", \", virtualized cloud environment, these systems make extensive use of \\nPlatform as a Service (PaaS) c\", \"ompute infrastructure and managed services. They treat the underlying \\ninfrastructure as disposable \", \"- provisioned in minutes and resized, scaled, or destroyed on demand \\u2013 \\nvia automation. \\n\\nConsider t\", \"he widely accepted DevOps concept of Pets vs. Cattle. In a traditional data center, servers \\nare tre\", \"ated as Pets: a physical machine, given a meaningful name, and cared for. You scale by adding \\nmore \", \"resources to the same machine (scaling up). If the server becomes sick, you nurse it back to \\nhealth\", \". Should the server become unavailable, everyone notices. \\n\\nThe Cattle service model is different. Y\", \"ou provision each instance as a virtual machine or container. \\nThey\\u2019re identical and assigned a syst\", \"em identifier such as Service-01, Service-02, and so on. You scale \\nby creating more of them (scalin\", \"g out). When one becomes unavailable, nobody notices. \\n\\n5 \\n\\nCHAPTER 1 | Introduction to cloud-native\", \" applications \\n\\n \\n \\n \\n\\fThe cattle model embraces immutable infrastructure. Servers aren\\u2019t repaired o\", \"r modified. If one fails or \\nrequires updating, it\\u2019s destroyed and a new one is provisioned \\u2013 all do\", \"ne via automation. \\n\\nCloud-native systems embrace the Cattle service model. They continue to run as \", \"the infrastructure \\nscales in or out with no regard to the machines upon which they\\u2019re running. \\n\\nTh\", \"e Azure cloud platform supports this type of highly elastic infrastructure with automatic scaling, \\n\", \"self-healing, and monitoring capabilities. \\n\\nModern design \\n\\nHow would you design a cloud-native app\", \"? What would your architecture look like? To what \\nprinciples, patterns, and best practices would yo\", \"u adhere? What infrastructure and operational \\nconcerns would be important? \\n\\nThe Twelve-Factor Appl\", \"ication \\n\\nA widely accepted methodology for constructing cloud-based applications is the Twelve-Fact\", \"or \\nApplication. It describes a set of principles and practices that developers follow to construct \", \"\\napplications optimized for modern cloud environments. Special attention is given to portability acr\", \"oss \\nenvironments and declarative automation. \\n\\nWhile applicable to any web-based application, many \", \"practitioners consider Twelve-Factor a solid \\nfoundation for building cloud-native apps. Systems bui\", \"lt upon these principles can deploy and scale \\nrapidly and add features to react quickly to market c\", \"hanges. \\n\\nThe following table highlights the Twelve-Factor methodology: \\n\\nFactor \\n\\nExplanation \\n\\n1 -\", \" Code Base \\n\\n2 - Dependencies \\n\\n3 - Configurations \\n\\n4 - Backing Services \\n\\nA single code base for e\", \"ach microservice, stored \\nin its own repository. Tracked with version \\ncontrol, it can deploy to mul\", \"tiple environments \\n(QA, Staging, Production). \\n\\nEach microservice isolates and packages its own \\nde\", \"pendencies, embracing changes without \\nimpacting the entire system. \\n\\nConfiguration information is m\", \"oved out of the \\nmicroservice and externalized through a \\nconfiguration management tool outside of t\", \"he \\ncode. The same deployment can propagate \\nacross environments with the correct \\nconfiguration app\", \"lied. \\n\\nAncillary resources (data stores, caches, \\nmessage brokers) should be exposed via an \\naddres\", \"sable URL. Doing so decouples the \\nresource from the application, enabling it to be \\ninterchangeable\", \". \\n\\n6 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fFactor \\n\\nExplanation \\n\\n5 - Buil\", \"d, Release, Run \\n\\n6 - Processes \\n\\n7 - Port Binding \\n\\n8 - Concurrency \\n\\n9 - Disposability \\n\\n10 - Dev/\", \"Prod Parity \\n\\n11 - Logging \\n\\n12 - Admin Processes \\n\\nEach release must enforce a strict separation \\na\", \"cross the build, release, and run stages. Each \\nshould be tagged with a unique ID and support \\nthe a\", \"bility to roll back. Modern CI/CD systems \\nhelp fulfill this principle. \\n\\nEach microservice should e\", \"xecute in its own \\nprocess, isolated from other running services. \\nExternalize required state to a b\", \"acking service \\nsuch as a distributed cache or data store. \\n\\nEach microservice should be self-contai\", \"ned with \\nits interfaces and functionality exposed on its \\nown port. Doing so provides isolation fro\", \"m \\nother microservices. \\n\\nWhen capacity needs to increase, scale out \\nservices horizontally across m\", \"ultiple identical \\nprocesses (copies) as opposed to scaling-up a \\nsingle large instance on the most \", \"powerful \\nmachine available. Develop the application to be \\nconcurrent making scaling out in cloud \\n\", \"environments seamless. \\n\\nService instances should be disposable. Favor \\nfast startup to increase sca\", \"lability opportunities \\nand graceful shutdowns to leave the system in a \\ncorrect state. Docker conta\", \"iners along with an \\norchestrator inherently satisfy this requirement. \\n\\nKeep environments across th\", \"e application \\nlifecycle as similar as possible, avoiding costly \\nshortcuts. Here, the adoption of c\", \"ontainers can \\ngreatly contribute by promoting the same \\nexecution environment. \\n\\nTreat logs generat\", \"ed by microservices as event \\nstreams. Process them with an event \\naggregator. Propagate log data to\", \" data-\\nmining/log management tools like Azure \\nMonitor or Splunk and eventually to long-term \\narchiv\", \"al. \\n\\nRun administrative/management tasks, such as \\ndata cleanup or computing analytics, as one-off \", \"\\nprocesses. Use independent tools to invoke \\nthese tasks from the production environment, \\nbut separ\", \"ately from the application. \\n\\n7 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fIn th\", \"e book, Beyond the Twelve-Factor App, author Kevin Hoffman details each of the original 12 \\nfactors \", \"(written in 2011). Additionally, he discusses three extra factors that reflect today\\u2019s modern \\ncloud\", \" application design. \\n\\nNew Factor \\n\\nExplanation \\n\\n13 - API First \\n\\n14 - Telemetry \\n\\n15 - Authenticat\", \"ion/ Authorization \\n\\nMake everything a service. Assume your code \\nwill be consumed by a front-end cl\", \"ient, gateway, \\nor another service. \\n\\nOn a workstation, you have deep visibility into \\nyour applicat\", \"ion and its behavior. In the cloud, \\nyou don\\u2019t. Make sure your design includes the \\ncollection of mo\", \"nitoring, domain-specific, and \\nhealth/system data. \\n\\nImplement identity from the start. Consider \\nR\", \"BAC (role-based access control) features \\navailable in public clouds. \\n\\nWe\\u2019ll refer to many of the 1\", \"2+ factors in this chapter and throughout the book. \\n\\nAzure Well-Architected Framework \\n\\nDesigning a\", \"nd deploying cloud-based workloads can be challenging, especially when implementing \\ncloud-native ar\", \"chitecture. Microsoft provides industry standard best practices to help you and your \\nteam deliver r\", \"obust cloud solutions. \\n\\nThe Microsoft Well-Architected Framework provides a set of guiding tenets t\", \"hat can be used to \\nimprove the quality of a cloud-native workload. The framework consists of five p\", \"illars of architecture \\nexcellence: \\n\\nTenets \\n\\nDescription \\n\\nCost management \\n\\nOperational excellenc\", \"e \\n\\nPerformance efficiency \\n\\nFocus on generating incremental value early. \\nApply Build-Measure-Learn\", \" principles to \\naccelerate time to market while avoiding \\ncapital-intensive solutions. Using a pay-a\", \"s-you-\\ngo strategy, invest as you scale out, rather than \\ndelivering a large investment up front. \\n\\n\", \"Automate the environment and operations to \\nincrease speed and reduce human error. Roll \\nproblem upd\", \"ates back or forward quickly. \\nImplement monitoring and diagnostics from the \\nstart. \\n\\nEfficiently m\", \"eet demands placed on your \\nworkloads. Favor horizontal scaling (scaling out) \\nand design it into yo\", \"ur systems. Continually \\nconduct performance and load testing to \\nidentify potential bottlenecks. \\n\\n\", \"8 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fReliability \\n\\nSecurity \\n\\nTenets \\n\\nD\", \"escription \\n\\nBuild workloads that are both resilient and \\navailable. Resiliency enables workloads to\", \" \\nrecover from failures and continue functioning. \\nAvailability ensures users access to your \\nworklo\", \"ad at all times. Design applications to \\nexpect failures and recover from them. \\n\\nImplement security\", \" across the entire lifecycle of \\nan application, from design and implementation \\nto deployment and o\", \"perations. Pay close \\nattention to identity management, infrastructure \\naccess, application security\", \", and data \\nsovereignty and encryption. \\n\\nTo get started, Microsoft provides a set of online assessm\", \"ents to help you assess your current cloud \\nworkloads against the five well-architected pillars. \\n\\nM\", \"icroservices \\n\\nCloud-native systems embrace microservices, a popular architectural style for constru\", \"cting modern \\napplications. \\n\\nBuilt as a distributed set of small, independent services that interac\", \"t through a shared fabric, \\nmicroservices share the following characteristics: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nEac\", \"h implements a specific business capability within a larger domain context. \\n\\nEach is developed auto\", \"nomously and can be deployed independently. \\n\\nEach is self-contained encapsulating its own data stor\", \"age technology, dependencies, and \\nprogramming platform. \\n\\nEach runs in its own process and communic\", \"ates with others using standard communication \\nprotocols such as HTTP/HTTPS, gRPC, WebSockets, or AM\", \"QP. \\n\\n\\u2022 \\n\\nThey compose together to form an application. \\n\\nFigure 1-4 contrasts a monolithic applicat\", \"ion approach with a microservices approach. Note how the \\nmonolith is composed of a layered architec\", \"ture, which executes in a single process. It typically \\nconsumes a relational database. The microser\", \"vice approach, however, segregates functionality into \\nindependent services, each with its own logic\", \", state, and data. Each microservice hosts its own \\ndatastore. \\n\\n9 \\n\\nCHAPTER 1 | Introduction to clo\", \"ud-native applications \\n\\n \\n \\n\\fFigure 1-4. Monolithic versus microservices architecture \\n\\nNote how mi\", \"croservices promote the Processes principle from the Twelve-Factor Application, \\ndiscussed earlier i\", \"n the chapter. \\n\\nFactor #6 specifies \\u201cEach microservice should execute in its own process, isolated \", \"from other running \\nservices.\\u201d \\n\\nWhy microservices? \\n\\nMicroservices provide agility. \\n\\nEarlier in th\", \"e chapter, we compared an eCommerce application built as a monolith to that with \\nmicroservices. In \", \"the example, we saw some clear benefits: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nEach microservice has an autonomous lifecycle and\", \" can evolve independently and deploy \\nfrequently. You don\\u2019t have to wait for a quarterly release to \", \"deploy a new feature or update. \\nYou can update a small area of a live application with less risk of\", \" disrupting the entire system. \\nThe update can be made without a full redeployment of the applicatio\", \"n. \\n\\nEach microservice can scale independently. Instead of scaling the entire application as a singl\", \"e \\nunit, you scale out only those services that require more processing power to meet desired \\nperfo\", \"rmance levels and service-level agreements. Fine-grained scaling provides for greater \\ncontrol of yo\", \"ur system and helps reduce overall costs as you scale portions of your system, \\nnot everything. \\n\\nAn\", \" excellent reference guide for understanding microservices is .NET Microservices: Architecture for \\n\", \"Containerized .NET Applications. The book deep dives into microservices design and architecture. It\\u2019\", \"s \\na companion for a full-stack microservice reference architecture available as a free download fro\", \"m \\nMicrosoft. \\n\\nDeveloping microservices \\n\\nMicroservices can be created upon any modern development \", \"platform. \\n\\n10 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n \\n\\fThe Microsoft .NET p\", \"latform is an excellent choice. Free and open source, it has many built-in features \\nthat simplify m\", \"icroservice development. .NET is cross-platform. Applications can be built and run on \\nWindows, macO\", \"S, and most flavors of Linux. \\n\\n.NET is highly performant and has scored well in comparison to Node.\", \"js and other competing \\nplatforms. Interestingly, TechEmpower conducted an extensive set of performa\", \"nce benchmarks across \\nmany web application platforms and frameworks. .NET scored in the top 10 - we\", \"ll above Node.js and \\nother competing platforms. \\n\\n.NET is maintained by Microsoft and the .NET comm\", \"unity on GitHub. \\n\\nMicroservice challenges \\n\\nWhile distributed cloud-native microservices can provid\", \"e immense agility and speed, they present \\nmany challenges: \\n\\nCommunication \\n\\nHow will front-end cli\", \"ent applications communicate with backed-end core microservices? Will you \\nallow direct communicatio\", \"n? Or, might you abstract the back-end microservices with a gateway \\nfacade that provides flexibilit\", \"y, control, and security? \\n\\nHow will back-end core microservices communicate with each other? Will y\", \"ou allow direct HTTP calls \\nthat can increase coupling and impact performance and agility? Or might \", \"you consider decoupled \\nmessaging with queue and topic technologies? \\n\\nCommunication is covered in t\", \"he Cloud-native communication patterns chapter. \\n\\nResiliency \\n\\nA microservices architecture moves yo\", \"ur system from in-process to out-of-process network \\ncommunication. In a distributed architecture, w\", \"hat happens when Service B isn\\u2019t responding to a \\nnetwork call from Service A? Or, what happens when\", \" Service C becomes temporarily unavailable and \\nother services calling it become blocked? \\n\\nResilien\", \"cy is covered in the Cloud-native resiliency chapter. \\n\\nDistributed Data \\n\\nBy design, each microserv\", \"ice encapsulates its own data, exposing operations via its public interface. If \\nso, how do you quer\", \"y data or implement a transaction across multiple services? \\n\\nDistributed data is covered in the Clo\", \"ud-native data patterns chapter. \\n\\nSecrets \\n\\nHow will your microservices securely store and manage s\", \"ecrets and sensitive configuration data? \\n\\nSecrets are covered in detail Cloud-native security. \\n\\n11\", \" \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fManage Complexity with Dapr \\n\\nDapr i\", \"s a distributed, open-source application runtime. Through an architecture of pluggable \\ncomponents, \", \"it dramatically simplifies the plumbing behind distributed applications. It provides a \\ndynamic glue\", \" that binds your application with pre-built infrastructure capabilities and components \\nfrom the Dap\", \"r runtime. Figure 1-5 shows Dapr from 20,000 feet. \\n\\nFigure 1-5. Dapr at 20,000 feet. \\n\\nIn the top r\", \"ow of the figure, note how Dapr provides language-specific SDKs for popular development \\nplatforms. \", \"Dapr v1 includes support for .NET, Go, Node.js, Python, PHP, Java, and JavaScript. \\n\\nWhile language-\", \"specific SDKs enhance the developer experience, Dapr is platform agnostic. Under the \\nhood, Dapr\\u2019s p\", \"rogramming model exposes capabilities through standard HTTP/gRPC communication \\nprotocols. Any progr\", \"amming platform can call Dapr via its native HTTP and gRPC APIs. \\n\\nThe blue boxes across the center \", \"of the figure represent the Dapr building blocks. Each exposes pre-\\nbuilt plumbing code for a distri\", \"buted application capability that your application can consume. \\n\\nThe components row represents a la\", \"rge set of pre-defined infrastructure components that your \\napplication can consume. Think of compon\", \"ents as infrastructure code you don\\u2019t have to write. \\n\\nThe bottom row highlights the portability of \", \"Dapr and the diverse environments across which it can \\nrun. \\n\\nMicrosoft features a free ebook Dapr f\", \"or .NET Developers for learning Dapr. \\n\\nLooking ahead, Dapr has the potential to have a profound imp\", \"act on cloud-native application \\ndevelopment. \\n\\nContainers \\n\\nIt\\u2019s natural to hear the term container\", \" mentioned in any cloud native conversation. In the book, Cloud \\nNative Patterns, author Cornelia Da\", \"vis observes that, \\u201cContainers are a great enabler of cloud-native \\n\\n12 \\n\\nCHAPTER 1 | Introduction t\", \"o cloud-native applications \\n\\n \\n \\n \\n\\fsoftware.\\u201d The Cloud Native Computing Foundation places microse\", \"rvice containerization as the first \\nstep in their Cloud-Native Trail Map - guidance for enterprises\", \" beginning their cloud-native journey. \\n\\nContainerizing a microservice is simple and straightforward\", \". The code, its dependencies, and runtime \\nare packaged into a binary called a container image. Imag\", \"es are stored in a container registry, which \\nacts as a repository or library for images. A registry\", \" can be located on your development computer, in \\nyour data center, or in a public cloud. Docker its\", \"elf maintains a public registry via Docker Hub. The \\nAzure cloud features a private container regist\", \"ry to store container images close to the cloud \\napplications that will run them. \\n\\nWhen an applicat\", \"ion starts or scales, you transform the container image into a running container \\ninstance. The inst\", \"ance runs on any computer that has a container runtime engine installed. You can \\nhave as many insta\", \"nces of the containerized service as needed. \\n\\nFigure 1-6 shows three different microservices, each \", \"in its own container, all running on a single host. \\n\\nFigure 1-6. Multiple containers running on a c\", \"ontainer host \\n\\nNote how each container maintains its own set of dependencies and runtime, which can\", \" be different \\nfrom one another. Here, we see different versions of the Product microservice running\", \" on the same \\nhost. Each container shares a slice of the underlying host operating system, memory, a\", \"nd processor, \\nbut is isolated from one another. \\n\\nNote how well the container model embraces the De\", \"pendencies principle from the Twelve-Factor \\nApplication. \\n\\nFactor #2 specifies that \\u201cEach microserv\", \"ice isolates and packages its own dependencies, embracing \\nchanges without impacting the entire syst\", \"em.\\u201d \\n\\nContainers support both Linux and Windows workloads. The Azure cloud openly embraces both. \\nI\", \"nterestingly, it\\u2019s Linux, not Windows Server, that has become the more popular operating system in \\n\", \"Azure. \\n\\n13 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n \\n\\fWhile several container\", \" vendors exist, Docker has captured the lion\\u2019s share of the market. The \\ncompany has been driving th\", \"e software container movement. It has become the de facto standard for \\npackaging, deploying, and ru\", \"nning cloud-native applications. \\n\\nWhy containers? \\n\\nContainers provide portability and guarantee co\", \"nsistency across environments. By encapsulating \\neverything into a single package, you isolate the m\", \"icroservice and its dependencies from the \\nunderlying infrastructure. \\n\\nYou can deploy the container\", \" in any environment that hosts the Docker runtime engine. Containerized \\nworkloads also eliminate th\", \"e expense of pre-configuring each environment with frameworks, software \\nlibraries, and runtime engi\", \"nes. \\n\\nBy sharing the underlying operating system and host resources, a container has a much smaller\", \" \\nfootprint than a full virtual machine. The smaller size increases the density, or number of \\nmicro\", \"services, that a given host can run at one time. \\n\\nContainer orchestration \\n\\nWhile tools such as Doc\", \"ker create images and run containers, you also need tools to manage them. \\nContainer management is d\", \"one with a special software program called a container orchestrator. \\nWhen operating at scale with m\", \"any independent running containers, orchestration is essential. \\n\\nFigure 1-7 shows management tasks \", \"that container orchestrators automate. \\n\\nFigure 1-7. What container orchestrators do \\n\\nThe following\", \" table describes common orchestration tasks. \\n\\nTasks \\n\\nExplanation \\n\\nScheduling \\n\\nAffinity/anti-affi\", \"nity \\n\\nHealth monitoring \\n\\nFailover \\n\\nAutomatically provision container instances. \\n\\nProvision conta\", \"iners nearby or far apart from \\neach other, helping availability and \\nperformance. \\n\\nAutomatically d\", \"etect and correct failures. \\n\\nAutomatically reprovision a failed instance to a \\nhealthy machine. \\n\\n1\", \"4 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n \\n\\fScaling \\n\\nNetworking \\n\\nService Di\", \"scovery \\n\\nRolling Upgrades \\n\\nTasks \\n\\nExplanation \\n\\nAutomatically add or remove a container \\ninstance\", \" to meet demand. \\n\\nManage a networking overlay for container \\ncommunication. \\n\\nEnable containers to \", \"locate each other. \\n\\nCoordinate incremental upgrades with zero \\ndowntime deployment. Automatically r\", \"oll back \\nproblematic changes. \\n\\nNote how container orchestrators embrace the Disposability and Conc\", \"urrency principles from the \\nTwelve-Factor Application. \\n\\nFactor #9 specifies that \\u201cService instance\", \"s should be disposable, favoring fast startups to increase \\nscalability opportunities and graceful s\", \"hutdowns to leave the system in a correct state.\\u201d Docker \\ncontainers along with an orchestrator inhe\", \"rently satisfy this requirement.\\u201d \\n\\nFactor #8 specifies that \\u201cServices scale out across a large numb\", \"er of small identical processes (copies) as \\nopposed to scaling-up a single large instance on the mo\", \"st powerful machine available.\\u201d \\n\\nWhile several container orchestrators exist, Kubernetes has become\", \" the de facto standard for the \\ncloud-native world. It\\u2019s a portable, extensible, open-source platfor\", \"m for managing containerized \\nworkloads. \\n\\nYou could host your own instance of Kubernetes, but then \", \"you\\u2019d be responsible for provisioning and \\nmanaging its resources - which can be complex. The Azure \", \"cloud features Kubernetes as a managed \\nservice. Both Azure Kubernetes Service (AKS) and Azure Red H\", \"at OpenShift (ARO) enable you to fully \\nleverage the features and power of Kubernetes as a managed s\", \"ervice, without having to install and \\nmaintain it. \\n\\nContainer orchestration is covered in detail i\", \"n Scaling Cloud-Native Applications. \\n\\nBacking services \\n\\nCloud-native systems depend upon many diff\", \"erent ancillary resources, such as data stores, message \\nbrokers, monitoring, and identity services.\", \" These services are known as backing services. \\n\\nFigure 1-8 shows many common backing services that \", \"cloud-native systems consume. \\n\\n15 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fFi\", \"gure 1-8. Common backing services \\n\\nYou could host your own backing services, but then you\\u2019d be resp\", \"onsible for licensing, provisioning, \\nand managing those resources. \\n\\nCloud providers offer a rich a\", \"ssortment of managed backing services. Instead of owning the service, \\nyou simply consume it. The cl\", \"oud provider operates the resource at scale and bears the responsibility \\nfor performance, security,\", \" and maintenance. Monitoring, redundancy, and availability are built into the \\nservice. Providers gu\", \"arantee service level performance and fully support their managed services - \\nopen a ticket and they\", \" fix your issue. \\n\\nCloud-native systems favor managed backing services from cloud vendors. The savin\", \"gs in time and \\nlabor can be significant. The operational risk of hosting your own and experiencing \", \"trouble can get \\nexpensive fast. \\n\\nA best practice is to treat a backing service as an attached reso\", \"urce, dynamically bound to a \\nmicroservice with configuration information (a URL and credentials) st\", \"ored in an external \\nconfiguration. This guidance is spelled out in the Twelve-Factor Application, d\", \"iscussed earlier in the \\nchapter. \\n\\nFactor #4 specifies that backing services \\u201cshould be exposed via\", \" an addressable URL. Doing so \\ndecouples the resource from the application, enabling it to be interc\", \"hangeable.\\u201d \\n\\nFactor #3 specifies that \\u201cConfiguration information is moved out of the microservice a\", \"nd externalized \\nthrough a configuration management tool outside of the code.\\u201d \\n\\nWith this pattern, \", \"a backing service can be attached and detached without code changes. You might \\npromote a microservi\", \"ce from QA to a staging environment. You update the microservice \\nconfiguration to point to the back\", \"ing services in staging and inject the settings into your container \\nthrough an environment variable\", \". \\n\\n16 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n \\n\\fCloud vendors provide APIs f\", \"or you to communicate with their proprietary backing services. These \\nlibraries encapsulate the prop\", \"rietary plumbing and complexity. However, communicating directly with \\nthese APIs will tightly coupl\", \"e your code to that specific backing service. It\\u2019s a widely accepted practice \\nto insulate the imple\", \"mentation details of the vendor API. Introduce an intermediation layer, or \\nintermediate API, exposi\", \"ng generic operations to your service code and wrap the vendor code inside \\nit. This loose coupling \", \"enables you to swap out one backing service for another or move your code to \\na different cloud envi\", \"ronment without having to make changes to the mainline service code. Dapr, \\ndiscussed earlier, follo\", \"ws this model with its set of prebuilt building blocks. \\n\\nOn a final thought, backing services also \", \"promote the Statelessness principle from the Twelve-Factor \\nApplication, discussed earlier in the ch\", \"apter. \\n\\nFactor #6 specifies that, \\u201cEach microservice should execute in its own process, isolated fr\", \"om other \\nrunning services. Externalize required state to a backing service such as a distributed ca\", \"che or data \\nstore.\\u201d \\n\\nBacking services are discussed in Cloud-native data patterns and Cloud-native\", \" communication \\npatterns. \\n\\nAutomation \\n\\nAs you\\u2019ve seen, cloud-native systems embrace microservices,\", \" containers, and modern system design \\nto achieve speed and agility. But, that\\u2019s only part of the st\", \"ory. How do you provision the cloud \\nenvironments upon which these systems run? How do you rapidly d\", \"eploy app features and updates? \\nHow do you round out the full picture? \\n\\nEnter the widely accepted \", \"practice of Infrastructure as Code, or IaC. \\n\\nWith IaC, you automate platform provisioning and appli\", \"cation deployment. You essentially apply \\nsoftware engineering practices such as testing and version\", \"ing to your DevOps practices. Your \\ninfrastructure and deployments are automated, consistent, and re\", \"peatable. \\n\\nAutomating infrastructure \\n\\nTools like Azure Resource Manager, Azure Bicep, Terraform fr\", \"om HashiCorp, and the Azure CLI, enable \\nyou to declaratively script the cloud infrastructure you re\", \"quire. Resource names, locations, capacities, \\nand secrets are parameterized and dynamic. The script\", \" is versioned and checked into source control \\nas an artifact of your project. You invoke the script\", \" to provision a consistent and repeatable \\ninfrastructure across system environments, such as QA, st\", \"aging, and production. \\n\\nUnder the hood, IaC is idempotent, meaning that you can run the same script\", \" over and over without \\nside effects. If the team needs to make a change, they edit and rerun the sc\", \"ript. Only the updated \\nresources are affected. \\n\\nIn the article, What is Infrastructure as Code, Au\", \"thor Sam Guckenheimer describes how, \\u201cTeams who \\nimplement IaC can deliver stable environments rapid\", \"ly and at scale. They avoid manual configuration \\nof environments and enforce consistency by represe\", \"nting the desired state of their environments via \\ncode. Infrastructure deployments with IaC are rep\", \"eatable and prevent runtime issues caused by \\nconfiguration drift or missing dependencies. DevOps te\", \"ams can work together with a unified set of \\n\\n17 \\n\\nCHAPTER 1 | Introduction to cloud-native applicat\", \"ions \\n\\n \\n \\n\\fpractices and tools to deliver applications and their supporting infrastructure rapidly,\", \" reliably, and at \\nscale.\\u201d \\n\\nAutomating deployments \\n\\nThe Twelve-Factor Application, discussed earli\", \"er, calls for separate steps when transforming \\ncompleted code into a running application. \\n\\nFactor \", \"#5 specifies that \\u201cEach release must enforce a strict separation across the build, release and run \\n\", \"stages. Each should be tagged with a unique ID and support the ability to roll back.\\u201d \\n\\nModern CI/CD\", \" systems help fulfill this principle. They provide separate build and delivery steps that \\nhelp ensu\", \"re consistent and quality code that\\u2019s readily available to users. \\n\\nFigure 1-9 shows the separation \", \"across the deployment process. \\n\\nFigure 1-9. Deployment steps in a CI/CD Pipeline \\n\\nIn the previous \", \"figure, pay special attention to separation of tasks: \\n\\n1. \\n\\nThe developer constructs a feature in t\", \"heir development environment, iterating through what \\nis called the \\u201cinner loop\\u201d of code, run, and d\", \"ebug. \\n\\n2.  When complete, that code is pushed into a code repository, such as GitHub, Azure DevOps,\", \" or \\n\\nBitBucket. \\n\\n3. \\n\\n4. \\n\\nThe push triggers a build stage that transforms the code into a binary \", \"artifact. The work is \\nimplemented with a Continuous Integration (CI) pipeline. It automatically bui\", \"lds, tests, and \\npackages the application. \\n\\nThe release stage picks up the binary artifact, applies\", \" external application and environment \\nconfiguration information, and produces an immutable release.\", \" The release is deployed to a \\nspecified environment. The work is implemented with a Continuous Deli\", \"very (CD) pipeline. \\nEach release should be identifiable. You can say, \\u201cThis deployment is running R\", \"elease 2.1.1 of \\nthe application.\\u201d \\n\\n18 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n\", \" \\n \\n\\f5. \\n\\nFinally, the released feature is run in the target execution environment. Releases are \\nim\", \"mutable meaning that any change must create a new release. \\n\\nApplying these practices, organizations\", \" have radically evolved how they ship software. Many have \\nmoved from quarterly releases to on-deman\", \"d updates. The goal is to catch problems early in the \\ndevelopment cycle when they\\u2019re less expensive\", \" to fix. The longer the duration between integrations, \\nthe more expensive problems become to resolv\", \"e. With consistency in the integration process, teams \\ncan commit code changes more frequently, lead\", \"ing to better collaboration and software quality. \\n\\nInfrastructure as code and deployment automation\", \", along with GitHub and Azure DevOps are \\ndiscussed in detail in DevOps. \\n\\nCandidate apps for cloud \", \"native \\n\\nThink about the apps your organization needs to build. Then, look at the existing apps in y\", \"our \\nportfolio. How many of them warrant a cloud-native architecture? All of them? Perhaps some? \\n\\nA\", \"pplying cost/benefit analysis, there\\u2019s a good chance some wouldn\\u2019t support the effort. The cost of \\n\", \"becoming cloud native would far exceed the business value of the application. \\n\\nWhat type of applica\", \"tion might be a candidate for cloud native? \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nStrategic enterprise systems that \", \"need to constantly evolve business capabilities/features \\n\\nAn application that requires a high relea\", \"se velocity - with high confidence \\n\\nA system where individual features must release without a full \", \"redeployment of the entire \\nsystem \\n\\nAn application developed by teams with expertise in different t\", \"echnology stacks \\n\\nAn application with components that must scale independently \\n\\nSmaller, less impa\", \"ctful line-of-business applications might fare well with a simple monolithic \\narchitecture hosted in\", \" a Cloud PaaS environment. \\n\\nThen there are legacy systems. While we\\u2019d all like to build new applica\", \"tions, we\\u2019re often responsible \\nfor modernizing legacy workloads that are critical to the business. \", \"\\n\\nModernizing legacy apps \\n\\nThe free Microsoft e-book Modernize existing .NET applications with Azur\", \"e cloud and Windows \\nContainers provides guidance about migrating on-premises workloads into cloud. \", \"Figure 1-10 shows \\nthat there isn\\u2019t a single, one-size-fits-all strategy for modernizing legacy appl\", \"ications. \\n\\n19 \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\fFigure 1-10. Strategie\", \"s for migrating legacy workloads \\n\\nMonolithic apps that are non-critical might benefit from a quick \", \"lift-and-shift (Cloud Infrastructure-\\nReady) migration. Here, the on-premises workload is rehosted t\", \"o a cloud-based VM, without changes. \\nThis approach uses the IaaS (Infrastructure as a Service) mode\", \"l. Azure includes several tools such as \\nAzure Migrate, Azure Site Recovery, and Azure Database Migr\", \"ation Service to help streamline the \\nmove. While this strategy can yield some cost savings, such ap\", \"plications typically weren\\u2019t designed to \\nunlock and leverage the benefits of cloud computing. \\n\\nLeg\", \"acy apps that are critical to the business often benefit from an enhanced Cloud Optimized \\nmigration\", \". This approach includes deployment optimizations that enable key cloud services - without \\nchanging\", \" the core architecture of the application. For example, you might containerize the application \\nand \", \"deploy it to a container orchestrator, like Azure Kubernetes Services, discussed later in this book.\", \" \\nOnce in the cloud, the application can consume cloud backing services such as databases, message \\n\", \"queues, monitoring, and distributed caching. \\n\\nFinally, monolithic apps that provide strategic enter\", \"prise functions might best benefit from a Cloud-\\nNative approach, the subject of this book. This app\", \"roach provides agility and velocity. But, it comes at \\na cost of replatforming, rearchitecting, and \", \"rewriting code. Over time, a legacy application could be \\ndecomposed into microservices, containeriz\", \"ed, and ultimately replatformed into a cloud-native \\narchitecture. \\n\\nIf you and your team believe a \", \"cloud-native approach is appropriate, it behooves you to rationalize \\nthe decision with your organiz\", \"ation. What exactly is the business problem that a cloud-native \\napproach will solve? How would it a\", \"lign with business needs? \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n20 \\n\\nRapid releases of features with increased confidence? \\n\\nFin\", \"e-grained scalability - more efficient usage of resources? \\n\\nCHAPTER 1 | Introduction to cloud-nativ\", \"e applications \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\nImproved system resiliency? \\n\\nImproved system performance? \\n\\n\\u2022  More\", \" visibility into operations? \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nBlend development platforms and data stores to arrive at the \", \"best tool for the job? \\n\\nFuture-proof application investment? \\n\\nThe right migration strategy depends\", \" on organizational priorities and the systems you\\u2019re targeting. \\nFor many, it may be more cost effec\", \"tive to cloud-optimize a monolithic application or add coarse-\\ngrained services to an N-Tier app. In\", \" these cases, you can still make full use of cloud PaaS capabilities \\nlike the ones offered by Azure\", \" App Service. \\n\\nSummary \\n\\nIn this chapter, we introduced cloud-native computing. We provided a defin\", \"ition along with the key \\ncapabilities that drive a cloud-native application. We looked at the types\", \" of applications that might \\njustify this investment and effort. \\n\\nWith the introduction behind, we \", \"now dive into a much more detailed look at cloud native. \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nCloud Native Computi\", \"ng Foundation \\n\\n.NET Microservices: Architecture for Containerized .NET applications \\n\\n\\u2022  Microsoft \", \"Azure Well-Architected Framework \\n\\n\\u2022  Modernize existing .NET applications with Azure cloud and Wind\", \"ows Containers \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nCloud Native Patterns by Cornelia Davis \\n\\nCloud native applicat\", \"ions: Ship faster, reduce risk, and grow your business \\n\\nDapr for .NET Developers \\n\\nDapr documents \\n\", \"\\nBeyond the Twelve-Factor Application \\n\\n\\u2022  What is Infrastructure as Code \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n21 \\n\\nUber En\", \"gineering\\u2019s Micro Deploy: Deploying Daily with Confidence \\n\\nHow Netflix Deploys Code \\n\\nOverload Cont\", \"rol for Scaling WeChat Microservices \\n\\nCHAPTER 1 | Introduction to cloud-native applications \\n\\n \\n \\n\\f\", \"CHAPTER  2 \\n\\nIntroducing \\neShopOnContainers \\nreference app \\n\\nMicrosoft, in partnership with leading \", \"community experts, has produced a full-featured cloud-native \\nmicroservices reference application, e\", \"ShopOnContainers. This application is built to showcase using \\n.NET and Docker, and optionally Azure\", \", Kubernetes, and Visual Studio, to build an online storefront. \\n\\nFigure 2-1. eShopOnContainers Samp\", \"le App Screenshot. \\n\\n22 \\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n \\n\\fBefore st\", \"arting this chapter, we recommend that you download the eShopOnContainers reference \\napplication. If\", \" you do so, it should be easier for you to follow along with the information presented. \\n\\nFeatures a\", \"nd requirements \\n\\nLet\\u2019s start with a review of the application\\u2019s features and requirements. The eSho\", \"pOnContainers \\napplication represents an online store that sells various physical products like t-sh\", \"irts and coffee \\nmugs. If you\\u2019ve bought anything online before, the experience of using the store sh\", \"ould be relatively \\nfamiliar. Here are some of the basic features the store implements: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \", \"\\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nList catalog items \\n\\nFilter items by type \\n\\nFilter items by brand \\n\\nAd\", \"d items to the shopping basket \\n\\nEdit or remove items from the basket \\n\\nCheckout \\n\\nRegister an accou\", \"nt \\n\\nSign in \\n\\nSign out \\n\\nReview orders \\n\\nThe application also has the following non-functional requ\", \"irements: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nIt needs to be highly available and it must scale automatically to meet \", \"increased traffic (and \\nscale back down once traffic subsides). \\n\\nIt should provide easy-to-use moni\", \"toring of its health and diagnostic logs to help \\ntroubleshoot any issues it encounters. \\n\\nIt should\", \" support an agile development process, including support for continuous integration \\nand deployment \", \"(CI/CD). \\n\\nIn addition to the two web front ends (traditional and Single Page Application), the \\napp\", \"lication must also support mobile client apps running different kinds of operating \\nsystems. \\n\\n\\u2022 \\n\\nI\", \"t should support cross-platform hosting and cross-platform development. \\n\\n23 \\n\\nCHAPTER 2 | Introduci\", \"ng eShopOnContainers reference app \\n\\n \\n \\n\\fFigure 2-2. eShopOnContainers reference application develo\", \"pment architecture. \\n\\nThe eShopOnContainers application is accessible from web or mobile clients tha\", \"t access the \\napplication over HTTPS targeting either the ASP.NET Core MVC server application or an \", \"appropriate \\nAPI Gateway. API Gateways offer several advantages, such as decoupling back-end service\", \"s from \\nindividual front-end clients and providing better security. The application also makes use o\", \"f a related \\npattern known as Backends-for-Frontends (BFF), which recommends creating separate API g\", \"ateways \\nfor each front-end client. The reference architecture demonstrates breaking up the API gate\", \"ways \\nbased on whether the request is coming from a web or mobile client. \\n\\nThe application\\u2019s functi\", \"onality is broken up into many distinct microservices. There are services \\nresponsible for authentic\", \"ation and identity, listing items from the product catalog, managing users\\u2019 \\nshopping baskets, and p\", \"lacing orders. Each of these separate services has its own persistent storage. \\nThere\\u2019s no single pr\", \"imary data store with which all services interact. Instead, coordination and \\ncommunication between \", \"the services is done on an as-needed basis and by using a message bus. \\n\\nEach of the different micro\", \"services is designed differently, based on their individual requirements. This \\naspect means their t\", \"echnology stack may differ, although they\\u2019re all built using .NET and designed for \\nthe cloud. Simpl\", \"er services provide basic Create-Read-Update-Delete (CRUD) access to the underlying \\ndata stores, wh\", \"ile more advanced services use Domain-Driven Design approaches and patterns to \\nmanage business comp\", \"lexity. \\n\\n24 \\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n \\n\\fFigure 2-3. Differen\", \"t kinds of microservices. \\n\\nOverview of the code \\n\\nBecause it uses microservices, the eShopOnContain\", \"ers app includes quite a few separate projects and \\nsolutions in its GitHub repository. In addition \", \"to separate solutions and executable files, the various \\nservices are designed to run inside their o\", \"wn containers, both during local development and at run \\ntime in production. Figure 2-4 shows the fu\", \"ll Visual Studio solution, in which the various different \\nprojects are organized. \\n\\n25 \\n\\nCHAPTER 2 \", \"| Introducing eShopOnContainers reference app \\n\\n \\n \\n \\n\\f26 \\n\\nCHAPTER 2 | Introducing eShopOnContainer\", \"s reference app \\n\\n \\n \\n \\n\\fFigure 2-4. Projects in Visual Studio solution. \\n\\nThe code is organized to \", \"support the different microservices, and within each microservice, the code is \\nbroken up into domai\", \"n logic, infrastructure concerns, and user interface or service endpoint. In many \\ncases, each servi\", \"ce\\u2019s dependencies can be fulfilled by Azure services in production, and alternative \\noptions for loc\", \"al development. Let\\u2019s examine how the application\\u2019s requirements map to Azure \\nservices. \\n\\nUnderstan\", \"ding microservices \\n\\nThis book focuses on cloud-native applications built using Azure technology. To\", \" learn more about \\nmicroservices best practices and how to architect microservice-based applications\", \", read the \\ncompanion book, .NET Microservices: Architecture for Containerized .NET Applications. \\n\\n\", \"Mapping eShopOnContainers to Azure Services \\n\\nAlthough not required, Azure is well-suited to support\", \"ing the eShopOnContainers because the project \\nwas built to be a cloud-native application. The appli\", \"cation is built with .NET, so it can run on Linux or \\nWindows containers depending on the Docker hos\", \"t. The application is made up of multiple \\nautonomous microservices, each with its own data. The dif\", \"ferent microservices showcase different \\napproaches, ranging from simple CRUD operations to more com\", \"plex DDD and CQRS patterns. \\nMicroservices communicate with clients over HTTP and with one another v\", \"ia message-based \\ncommunication. The application supports multiple platforms for clients as well, si\", \"nce it adopts HTTP \\nas a standard communication protocol and includes ASP.NET Core and Xamarin mobil\", \"e apps that run \\non Android, iOS, and Windows platforms. \\n\\nThe application\\u2019s architecture is shown i\", \"n Figure 2-5. On the left are the client apps, broken up into \\nmobile, traditional Web, and Web Sing\", \"le Page Application (SPA) flavors. On the right are the server-\\nside components that make up the sys\", \"tem, each of which can be hosted in Docker containers and \\nKubernetes clusters. The traditional web \", \"app is powered by the ASP.NET Core MVC application shown \\nin yellow. This app and the mobile and web\", \" SPA applications communicate with the individual \\nmicroservices through one or more API gateways. T\", \"he API gateways follow the \\u201cbackends for front \\nends\\u201d (BFF) pattern, meaning that each gateway is de\", \"signed to support a given front-end client. The \\nindividual microservices are listed to the right of\", \" the API gateways and include both business logic \\nand some kind of persistence store. The different\", \" services make use of SQL Server databases, Redis \\ncache instances, and MongoDB/CosmosDB stores. On \", \"the far right is the system\\u2019s Event Bus, which is \\nused for communication between the microservices.\", \" \\n\\n27 \\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n\\fFigure 2-5. The eShopOnContai\", \"ners Architecture. \\n\\nThe server-side components of this architecture all map easily to Azure service\", \"s. \\n\\nContainer orchestration and clustering \\n\\nThe application\\u2019s container-hosted services, from ASP.\", \"NET Core MVC apps to individual Catalog and \\nOrdering microservices, can be hosted and managed in Az\", \"ure Kubernetes Service (AKS). The \\napplication can run locally on Docker and Kubernetes, and the sam\", \"e containers can then be deployed \\nto staging and production environments hosted in AKS. This proces\", \"s can be automated as we\\u2019ll see in \\nthe next section. \\n\\nAKS provides management services for individ\", \"ual clusters of containers. The application will deploy \\nseparate containers for each microservice i\", \"n the AKS cluster, as shown in the architecture diagram \\nabove. This approach allows each individual\", \" service to scale independently according to its resource \\ndemands. Each microservice can also be de\", \"ployed independently, and ideally such deployments \\nshould incur zero system downtime. \\n\\nAPI Gateway\", \" \\n\\nThe eShopOnContainers application has multiple front-end clients and multiple different back-end \", \"\\nservices. There\\u2019s no one-to-one correspondence between the client applications and the microservice\", \"s \\nthat support them. In such a scenario, there may be a great deal of complexity when writing clien\", \"t \\nsoftware to interface with the various back-end services in a secure manner. Each client would ne\", \"ed to \\naddress this complexity on its own, resulting in duplication and many places in which to make\", \" updates \\nas services change or new policies are implemented. \\n\\nAzure API Management (APIM) helps or\", \"ganizations publish APIs in a consistent, manageable fashion. \\nAPIM consists of three components: th\", \"e API Gateway, and administration portal (the Azure portal), \\nand a developer portal. \\n\\n28 \\n\\nCHAPTER\", \" 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n \\n\\fThe API Gateway accepts API calls and rout\", \"es them to the appropriate back-end API. It can also \\nprovide additional services like verification \", \"of API keys or JWT tokens and API transformation on the \\nfly without code modifications (for instanc\", \"e, to accommodate clients expecting an older interface). \\n\\nThe Azure portal is where you define the \", \"API schema and package different APIs into products. You \\nalso configure user access, view reports, \", \"and configure policies for quotas or transformations. \\n\\nThe developer portal serves as the main reso\", \"urce for developers. It provides developers with API \\ndocumentation, an interactive test console, an\", \"d reports on their own usage. Developers also use the \\nportal to create and manage their own account\", \"s, including subscription and API key support. \\n\\nUsing APIM, applications can expose several differe\", \"nt groups of services, each providing a back end \\nfor a particular front-end client. APIM is recomme\", \"nded for complex scenarios. For simpler needs, the \\nlightweight API Gateway Ocelot can be used. The \", \"eShopOnContainers app uses Ocelot because of its \\nsimplicity and because it can be deployed into the\", \" same application environment as the application \\nitself. Learn more about eShopOnContainers, APIM, \", \"and Ocelot. \\n\\nAnother option if your application is using AKS is to deploy the Azure Gateway Ingress\", \" Controller as a \\npod within your AKS cluster. This approach allows your cluster to integrate with a\", \"n Azure Application \\nGateway, allowing the gateway to load-balance traffic to the AKS pods. Learn mo\", \"re about the Azure \\nGateway Ingress Controller for AKS. \\n\\nData \\n\\nThe various back-end services used \", \"by eShopOnContainers have different storage requirements. \\nSeveral microservices use SQL Server data\", \"bases. The Basket microservice leverages a Redis cache for \\nits persistence. The Locations microserv\", \"ice expects a MongoDB API for its data. Azure supports each \\nof these data formats. \\n\\nFor SQL Server\", \" database support, Azure has products for everything from single databases up to \\nhighly scalable SQ\", \"L Database elastic pools. Individual microservices can be configured to \\ncommunicate with their own \", \"individual SQL Server databases quickly and easily. These databases can \\nbe scaled as needed to supp\", \"ort each separate microservice according to its needs. \\n\\nThe eShopOnContainers application stores th\", \"e user\\u2019s current shopping basket between requests. This \\naspect is managed by the Basket microservic\", \"e that stores the data in a Redis cache. In development, \\nthis cache can be deployed in a container,\", \" while in production it can utilize Azure Cache for Redis. \\nAzure Cache for Redis is a fully managed\", \" service offering high performance and reliability without the \\nneed to deploy and manage Redis inst\", \"ances or containers on your own. \\n\\nThe Locations microservice uses a MongoDB NoSQL database for its \", \"persistence. During \\ndevelopment, the database can be deployed in its own container, while in produc\", \"tion the service can \\nleverage Azure Cosmos DB\\u2019s API for MongoDB. One of the benefits of Azure Cosmo\", \"s DB is its ability \\nto leverage multiple different communication protocols, including a SQL API and\", \" common NoSQL \\nAPIs including MongoDB, Cassandra, Gremlin, and Azure Table Storage. Azure Cosmos DB \", \"offers a \\nfully managed and globally distributed database as a service that can scale to meet the ne\", \"eds of the \\nservices that use it. \\n\\nDistributed data in cloud-native applications is covered in more\", \" detail in chapter 5. \\n\\n29 \\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n\\fEvent Bu\", \"s \\n\\nThe application uses events to communicate changes between different services. This functionalit\", \"y can \\nbe implemented with various implementations, and locally the eShopOnContainers application us\", \"es \\nRabbitMQ. When hosted in Azure, the application would leverage Azure Service Bus for its messagi\", \"ng. \\nAzure Service Bus is a fully managed integration message broker that allows applications and se\", \"rvices \\nto communicate with one another in a decoupled, reliable, asynchronous manner. Azure Service\", \" Bus \\nsupports individual queues as well as separate topics to support publisher-subscriber scenario\", \"s. The \\neShopOnContainers application would leverage topics with Azure Service Bus to support distri\", \"buting \\nmessages from one microservice to any other microservice that needed to react to a given mes\", \"sage. \\n\\nResiliency \\n\\nOnce deployed to production, the eShopOnContainers application would be able to\", \" take advantage \\nof several Azure services available to improve its resiliency. The application publ\", \"ishes health checks, \\nwhich can be integrated with Application Insights to provide reporting and ale\", \"rts based on the app\\u2019s \\navailability. Azure resources also provide diagnostic logs that can be used \", \"to identify and correct bugs \\nand performance issues. Resource logs provide detailed information on \", \"when and how different Azure \\nresources are used by the application. You\\u2019ll learn more about cloud-n\", \"ative resiliency features in \\nchapter 6. \\n\\nDeploying eShopOnContainers to Azure \\n\\nThe eShopOnContain\", \"ers application can be deployed to various Azure platforms. The recommended \\napproach is to deploy t\", \"he application to Azure Kubernetes Services (AKS). Helm, a Kubernetes \\ndeployment tool, is available\", \" to reduce deployment complexity. Optionally, developers may \\nimplement Azure Dev Spaces for Kuberne\", \"tes to streamline their development process. \\n\\nAzure Kubernetes Service \\n\\nTo host eShop in AKS, the \", \"first step is to create an AKS cluster. To do so, you might use the Azure \\nportal, which will walk y\", \"ou through the required steps. You could also create a cluster from the Azure \\nCLI, taking care to e\", \"nable Role-Based Access Control (RBAC) and application routing. The \\neShopOnContainers\\u2019 documentatio\", \"n details the steps for creating your own AKS cluster. Once created, \\nyou can access and manage the \", \"cluster from the Kubernetes dashboard. \\n\\nYou can now deploy the eShop application to the cluster usi\", \"ng Helm. \\n\\nDeploying to Azure Kubernetes Service using Helm \\n\\nHelm is an application package manager\", \" tool that works directly with Kubernetes. It helps you define, \\ninstall, and upgrade Kubernetes app\", \"lications. While simple apps can be deployed to AKS with custom \\nCLI scripts or simple deployment fi\", \"les, complex apps can contain many Kubernetes objects and benefit \\nfrom Helm. \\n\\nUsing Helm, applicat\", \"ions include text-based configuration files, called Helm charts, which declaratively \\ndescribe the a\", \"pplication and configuration in Helm packages. Charts use standard YAML-formatted \\n\\n30 \\n\\nCHAPTER 2 |\", \" Introducing eShopOnContainers reference app \\n\\n \\n \\n\\ffiles to describe a related set of Kubernetes re\", \"sources. They\\u2019re versioned alongside the application \\ncode they describe. Helm Charts range from sim\", \"ple to complex depending on the requirements of the \\ninstallation they describe. \\n\\nHelm is composed \", \"of a command-line client tool, which consumes helm charts and launches \\ncommands to a server compone\", \"nt named, Tiller. Tiller communicates with the Kubernetes API to \\nensure the correct provisioning of\", \" your containerized workloads. Helm is maintained by the Cloud-\\nnative Computing Foundation. \\n\\nThe f\", \"ollowing yaml file presents a Helm template: \\n\\napiVersion: v1 \\nkind: Service \\nmetadata: \\n  name: {{ \", \".Values.app.svc.marketing }} \\n  labels: \\n    app: {{ template \\\"marketing-api.name\\\" . }} \\n    chart: \", \"{{ template \\\"marketing-api.chart\\\" . }} \\n    release: {{ .Release.Name }} \\n    heritage: {{ .Release.\", \"Service }} \\nspec: \\n  type: {{ .Values.service.type }} \\n  ports: \\n    - port: {{ .Values.service.port\", \" }} \\n      targetPort: http \\n      protocol: TCP \\n      name: http \\n  selector: \\n    app: {{ templat\", \"e \\\"marketing-api.name\\\" . }} \\n    release: {{ .Release.Name }} \\n\\nNote how the template describes a dy\", \"namic set of key/value pairs. When the template is invoked, \\nvalues that enclosed in curly braces ar\", \"e pulled in from other yaml-based configuration files. \\n\\nYou\\u2019ll find the eShopOnContainers helm char\", \"ts in the /k8s/helm folder. Figure 2-6 shows how the \\ndifferent components of the application are or\", \"ganized into a folder structure used by helm to define \\nand managed deployments. \\n\\n31 \\n\\nCHAPTER 2 | \", \"Introducing eShopOnContainers reference app \\n\\n \\n \\n\\fFigure 2-6. The eShopOnContainers helm folder. \\n\\n\", \"Each individual component is installed using a helm install command. eShop includes a \\u201cdeploy all\\u201d \\n\", \"script that loops through and installs the components using their respective helm charts. The result\", \" is \\na repeatable process, versioned with the application in source control, that anyone on the team\", \" can \\ndeploy to an AKS cluster with a one-line script command. \\n\\nNote that version 3 of Helm officia\", \"lly removes the need for the Tiller server component. More \\ninformation on this enhancement can be f\", \"ound here. \\n\\nAzure Functions and Logic Apps (Serverless) \\n\\nThe eShopOnContainers sample includes sup\", \"port for tracking online marketing campaigns. An Azure \\nFunction is used to track marketing campaign\", \" details for a given campaign ID. Rather than creating a \\n\\n32 \\n\\nCHAPTER 2 | Introducing eShopOnConta\", \"iners reference app \\n\\n \\n \\n \\n\\ffull microservice, a single Azure Function is simpler and sufficient. A\", \"zure Functions have a simple build \\nand deployment model, especially when configured to run in Kuber\", \"netes. Deploying the function is \\nscripted using Azure Resource Manager (ARM) templates and the Azur\", \"e CLI. This campaign service \\nisn\\u2019t customer-facing and invokes a single operation, making it a grea\", \"t candidate for Azure Functions. \\nThe function requires minimal configuration, including a database \", \"connection string data and image \\nbase URI settings. You configure Azure Functions in the Azure port\", \"al. \\n\\nCentralized configuration \\n\\nUnlike a monolithic app in which everything runs within a single i\", \"nstance, a cloud-native application \\nconsists of independent services distributed across virtual mac\", \"hines, containers, and geographic \\nregions. Managing configuration settings for dozens of interdepen\", \"dent services can be challenging. \\nDuplicate copies of configuration settings across different locat\", \"ions are error prone and difficult to \\nmanage. Centralized configuration is a critical requirement f\", \"or distributed cloud-native applications. \\n\\nAs discussed in Chapter 1, the Twelve-Factor App recomme\", \"ndations require strict separation between \\ncode and configuration. Configuration must be stored ext\", \"ernally from the application and read-in as \\nneeded. Storing configuration values as constants or li\", \"teral values in code is a violation. The same \\nconfiguration values are often be used by many servic\", \"es in the same application. Additionally, we \\nmust support the same values across multiple environme\", \"nts, such as dev, testing, and production. The \\nbest practice is store them in a centralized configu\", \"ration store. \\n\\nThe Azure cloud presents several great options. \\n\\nAzure App Configuration \\n\\nAzure Ap\", \"p Configuration is a fully managed Azure service that stores non-secret configuration \\nsettings in a\", \" secure, centralized location. Stored values can be shared among multiple services and \\napplications\", \". \\n\\nThe service is simple to use and provides several benefits: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nFlexible key/v\", \"alue representations and mappings \\n\\nTagging with Azure labels \\n\\nDedicated UI for management \\n\\nEncryp\", \"tion of sensitive information \\n\\nQuerying and batch retrieval \\n\\nAzure App Configuration maintains cha\", \"nges made to key-value settings for seven days. The point-in-\\ntime snapshot feature enables you to r\", \"econstruct the history of a setting and even rollback for a failed \\ndeployment. \\n\\nApp Configuration \", \"automatically caches each setting to avoid excessive calls to the configuration \\nstore. The refresh \", \"operation waits until the cached value of a setting expires to update that setting, \\neven when its v\", \"alue changes in the configuration store. The default cache expiration time is 30 \\nseconds. You can o\", \"verride the expiration time. \\n\\n33 \\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n\\fA\", \"pp Configuration encrypts all configuration values in transit and at rest. Key names and labels are \", \"\\nused as indexes for retrieving configuration data and aren\\u2019t encrypted. \\n\\nAlthough App Configuratio\", \"n provides hardened security, Azure Key Vault is still the best place for \\nstoring application secre\", \"ts. Key Vault provides hardware-level encryption, granular access policies, and \\nmanagement operatio\", \"ns such as certificate rotation. You can create App Configuration values that \\nreference secrets sto\", \"red in a Key Vault. \\n\\nAzure Key Vault \\n\\nKey Vault is a managed service for securely storing and acce\", \"ssing secrets. A secret is anything that you \\nwant to tightly control access to, such as API keys, p\", \"asswords, or certificates. A vault is a logical group \\nof secrets. \\n\\nKey Vault greatly reduces the c\", \"hances that secrets may be accidentally leaked. When using Key Vault, \\napplication developers no lon\", \"ger need to store security information in their application. This practice \\neliminates the need to s\", \"tore this information inside your code. For example, an application may need \\nto connect to a databa\", \"se. Instead of storing the connection string in the app\\u2019s code, you can store it \\nsecurely in Key Va\", \"ult. \\n\\nYour applications can securely access the information they need by using URIs. These URIs all\", \"ow the \\napplications to retrieve specific versions of a secret. There\\u2019s no need to write custom code\", \" to protect \\nany of the secret information stored in Key Vault. \\n\\nAccess to Key Vault requires prope\", \"r caller authentication and authorization. Typically, each cloud-\\nnative microservice uses a ClientI\", \"d/ClientSecret combination. It\\u2019s important to keep these credentials \\noutside source control. A best\", \" practice is to set them in the application\\u2019s environment. Direct access to \\nKey Vault from AKS can \", \"be achieved using Key Vault FlexVolume. \\n\\nConfiguration in eShop \\n\\nThe eShopOnContainers application\", \" includes local application settings files with each microservice. \\nThese files are checked into sou\", \"rce control, but don\\u2019t include production secrets such as connection \\nstrings or API keys. In produc\", \"tion, individual settings may be overwritten with per-service environment \\nvariables. Injecting secr\", \"ets in environment variables is a common practice for hosted applications, but \\ndoesn\\u2019t provide a ce\", \"ntral configuration store. To support centralized management of configuration \\nsettings, each micros\", \"ervice includes a setting to toggle between its use of local settings or Azure Key \\nVault settings. \", \"\\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n34 \\n\\nThe eShopOnContainers Architecture \\n\\nOrchestrating micro\", \"services and multi-container applications for high scalability and \\navailability \\n\\nAzure API Managem\", \"ent \\n\\nAzure SQL Database Overview \\n\\nAzure Cache for Redis \\n\\nAzure Cosmos DB\\u2019s API for MongoDB \\n\\nCHAP\", \"TER 2 | Introducing eShopOnContainers reference app \\n\\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAzure Service Bus \\n\\nA\", \"zure Monitor overview \\n\\neShopOnContainers: Create Kubernetes cluster in AKS \\n\\neShopOnContainers: Azu\", \"re Dev Spaces \\n\\nAzure Dev Spaces \\n\\n35 \\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n\\n \\n\", \" \\n\\fCHAPTER  3 \\n\\nScaling cloud-native \\napplications \\n\\nOne of the most-often touted advantages of movi\", \"ng to a cloud hosting environment is scalability. \\nScalability, or the ability for an application to\", \" accept additional user load without compromising \\nperformance for each user. It\\u2019s most often achiev\", \"ed by breaking up an application into small pieces \\nthat can each be given whatever resources they r\", \"equire. Cloud vendors enable massive scalability \\nanytime and anywhere in the world. \\n\\nIn this chapt\", \"er, we discuss technologies that enable cloud-native applications to scale to meet user \\ndemand. The\", \"se technologies include: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nContainers \\n\\nOrchestrators \\n\\nServerless computing \\n\\nLeveragin\", \"g containers and orchestrators \\n\\nContainers and orchestrators are designed to solve problems common \", \"to monolithic deployment \\napproaches. \\n\\nChallenges with monolithic deployments \\n\\nTraditionally, most\", \" applications have been deployed as a single unit. Such applications are referred to \\nas a monolith.\", \" This general approach of deploying applications as single units even if they\\u2019re \\ncomposed of multip\", \"le modules or assemblies is known as monolithic architecture, as shown in Figure \\n3-1. \\n\\n36 \\n\\nCHAPTE\", \"R 3 | Scaling cloud-native applications \\n\\n \\n \\n\\fFigure 3-1. Monolithic architecture. \\n\\nAlthough they \", \"have the benefit of simplicity, monolithic architectures face many challenges: \\n\\nDeployment \\n\\nAdditi\", \"onally, they require a restart of the application, which may temporarily impact availability if \\nzer\", \"o-downtime techniques are not applied while deploying. \\n\\nScaling \\n\\nA monolithic application is hoste\", \"d entirely on a single machine instance, often requiring high-\\ncapability hardware. If any part of t\", \"he monolith requires scaling, another copy of the entire application \\nmust be deployed to another ma\", \"chine. With a monolith, you can\\u2019t scale application components \\nindividually - it\\u2019s all or nothing. \", \"Scaling components that don\\u2019t require scaling results in inefficient and \\ncostly resource usage. \\n\\nE\", \"nvironment \\n\\nMonolithic applications are typically deployed to a hosting environment with a pre-inst\", \"alled operating \\nsystem, runtime, and library dependencies. This environment may not match that upon\", \" which the \\napplication was developed or tested. Inconsistencies across application environments are\", \" a common \\nsource of problems for monolithic deployments. \\n\\nCoupling \\n\\nA monolithic application is l\", \"ikely to experience high coupling across its functional components. \\nWithout hard boundaries, system\", \" changes often result in unintended and costly side effects. New \\nfeatures/fixes become tricky, time\", \"-consuming, and expensive to implement. Updates require extensive \\ntesting. Coupling also makes it d\", \"ifficult to refactor components or swap in alternative \\nimplementations. Even when constructed with \", \"a strict separation of concerns, architectural erosion \\nsets in as the monolithic code base deterior\", \"ates with never-ending \\u201cspecial cases.\\u201d \\n\\n37 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \\n \", \"\\n\\fPlatform lock-in \\n\\nA monolithic application is constructed with a single technology stack. While o\", \"ffering uniformity, this \\ncommitment can become a barrier to innovation. New features and components\", \" will be built using \\nthe application\\u2019s current stack - even when more modern technologies may be a \", \"better choice. A \\nlonger-term risk is your technology stack becoming outdated and obsolete. Rearchit\", \"ecting an entire \\napplication to a new, more modern platform is at best expensive and risky. \\n\\nWhat \", \"are the benefits of containers and orchestrators? \\n\\nWe introduced containers in Chapter 1. We highli\", \"ghted how the Cloud Native Computing Foundation \\n(CNCF) ranks containerization as the first step in \", \"their Cloud-Native Trail Map - guidance for \\nenterprises beginning their cloud-native journey. In th\", \"is section, we discuss the benefits of containers. \\n\\nDocker is the most popular container management\", \" platform. It works with containers on both Linux or \\nWindows. Containers provide separate but repro\", \"ducible application environments that run the same \\nway on any system. This aspect makes them perfec\", \"t for developing and hosting cloud-native services. \\nContainers are isolated from one another. Two c\", \"ontainers on the same host hardware can have \\ndifferent versions of software, without causing confli\", \"cts. \\n\\nContainers are defined by simple text-based files that become project artifacts and are check\", \"ed into \\nsource control. While full servers and virtual machines require manual effort to update, co\", \"ntainers are \\neasily version-controlled. Apps built to run in containers can be developed, tested, a\", \"nd deployed \\nusing automated tools as part of a build pipeline. \\n\\nContainers are immutable. Once you\", \" define a container, you can recreate and run it exactly the same \\nway. This immutability lends itse\", \"lf to component-based design. If some parts of an application evolve \\ndifferently than others, why r\", \"edeploy the entire app when you can just deploy the parts that change \\nmost frequently? Different fe\", \"atures and cross-cutting concerns of an app can be broken up into \\nseparate units. Figure 3-2 shows \", \"how a monolithic app can take advantage of containers and \\nmicroservices by delegating certain featu\", \"res or functionality. The remaining functionality in the app \\nitself has also been containerized. \\n\\n\", \"38 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \\n\\fFigure 3-2. Decomposing a monolithic app t\", \"o embrace microservices. \\n\\nEach cloud-native service is built and deployed in a separate container. \", \"Each can update as needed. \\nIndividual services can be hosted on nodes with resources appropriate to\", \" each service. The \\nenvironment each service runs in is immutable, shared across dev, test, and prod\", \"uction environments, \\nand easily versioned. Coupling between different areas of the application occu\", \"rs explicitly as calls or \\nmessages between services, not compile-time dependencies within the monol\", \"ith. You can also choose \\nthe technology that best suites a given capability without requiring chang\", \"es to the rest of the app. \\n\\nContainerized services require automated management. It wouldn\\u2019t be fea\", \"sible to manually administer \\na large set of independently deployed containers. For example, conside\", \"r the following tasks: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nHow will container instances be provisioned across \", \"a cluster of many machines? \\n\\nOnce deployed, how will containers discover and communicate with each \", \"other? \\n\\nHow can containers scale in or out on-demand? \\n\\nHow do you monitor the health of each conta\", \"iner? \\n\\nHow do you protect a container against hardware and software failures? \\n\\nHow do upgrade cont\", \"ainers for a live application with zero downtime? \\n\\nContainer orchestrators address and automate the\", \"se and other concerns. \\n\\nIn the cloud-native eco-system, Kubernetes has become the de facto containe\", \"r orchestrator. It\\u2019s an \\nopen-source platform managed by the Cloud Native Computing Foundation (CNCF\", \"). Kubernetes \\nautomates the deployment, scaling, and operational concerns of containerized workload\", \"s across a \\nmachine cluster. However, installing and managing Kubernetes is notoriously complex. \\n\\n3\", \"9 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \\n \\n\\fA much better approach is to leverage Kub\", \"ernetes as a managed service from a cloud vendor. The \\nAzure cloud features a fully managed Kubernet\", \"es platform entitled Azure Kubernetes Service (AKS). \\nAKS abstracts the complexity and operational o\", \"verhead of managing Kubernetes. You consume \\nKubernetes as a cloud service; Microsoft takes responsi\", \"bility for managing and supporting it. AKS also \\ntightly integrates with other Azure services and de\", \"v tools. \\n\\nAKS is a cluster-based technology. A pool of federated virtual machines, or nodes, is dep\", \"loyed to the \\nAzure cloud. Together they form a highly available environment, or cluster. The cluste\", \"r appears as a \\nseamless, single entity to your cloud-native application. Under the hood, AKS deploy\", \"s your \\ncontainerized services across these nodes following a predefined strategy that evenly distri\", \"butes the \\nload. \\n\\nWhat are the scaling benefits? \\n\\nServices built on containers can leverage scalin\", \"g benefits provided by orchestration tools like \\nKubernetes. By design containers only know about th\", \"emselves. Once you have multiple containers \\nthat need to work together, you should organize them at\", \" a higher level. Organizing large numbers of \\ncontainers and their shared dependencies, such as netw\", \"ork configuration, is where orchestration tools \\ncome in to save the day! Kubernetes creates an abst\", \"raction layer over groups of containers and \\norganizes them into pods. Pods run on worker machines r\", \"eferred to as nodes. This organized structure \\nis referred to as a cluster. Figure 3-3 shows the dif\", \"ferent components of a Kubernetes cluster. \\n\\nFigure 3-3. Kubernetes cluster components. \\n\\nScaling co\", \"ntainerized workloads is a key feature of container orchestrators. AKS supports automatic \\nscaling a\", \"cross two dimensions: Container instances and compute nodes. Together they give AKS the \\nability to \", \"quickly and efficiently respond to spikes in demand and add additional resources. We \\ndiscuss scalin\", \"g in AKS later in this chapter. \\n\\n40 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \\n \\n\\fDeclar\", \"ative versus imperative \\n\\nKubernetes supports both declarative and imperative configuration. The imp\", \"erative approach involves \\nrunning various commands that tell Kubernetes what to do each step of the\", \" way. Run this image. \\nDelete this pod. Expose this port. With the declarative approach, you create \", \"a configuration file, called \\na manifest, to describe what you want instead of what to do. Kubernete\", \"s reads the manifest and \\ntransforms your desired end state into actual end state. \\n\\nImperative comm\", \"ands are great for learning and interactive experimentation. However, you\\u2019ll want to \\ndeclaratively \", \"create Kubernetes manifest files to embrace an infrastructure as code approach, \\nproviding for relia\", \"ble and repeatable deployments. The manifest file becomes a project artifact and is \\nused in your CI\", \"/CD pipeline for automating Kubernetes deployments. \\n\\nIf you\\u2019ve already configured your cluster usin\", \"g imperative commands, you can export a declarative \\nmanifest by using kubectl get svc SERVICENAME -\", \"o yaml > service.yaml. This command produces a \\nmanifest similar to one shown below: \\n\\napiVersion: v\", \"1 \\nkind: Service \\nmetadata: \\n  creationTimestamp: \\\"2019-09-13T13:58:47Z\\\" \\n  labels: \\n    component: \", \"apiserver \\n    provider: kubernetes \\n  name: kubernetes \\n  namespace: default \\n  resourceVersion: \\\"1\", \"53\\\" \\n  selfLink: /api/v1/namespaces/default/services/kubernetes \\n  uid: 9b1fac62-d62e-11e9-8968-0015\", \"5d38010d \\nspec: \\n  clusterIP: 10.96.0.1 \\n  ports: \\n  - name: https \\n    port: 443 \\n    protocol: TCP\", \" \\n    targetPort: 6443 \\n  sessionAffinity: None \\n  type: ClusterIP \\nstatus: \\n  loadBalancer: {} \\n\\nWh\", \"en using declarative configuration, you can preview the changes that will be made before \\ncommitting\", \" them by using kubectl diff -f FOLDERNAME against the folder where your configuration \\nfiles are loc\", \"ated. Once you\\u2019re sure you want to apply the changes, run kubectl apply -f FOLDERNAME. \\nAdd -R to re\", \"cursively process a folder hierarchy. \\n\\nYou can also use declarative configuration with other Kubern\", \"etes features, one of which being \\ndeployments. Declarative deployments help manage releases, update\", \"s, and scaling. They instruct the \\nKubernetes deployment controller on how to deploy new changes, sc\", \"ale out load, or roll back to a \\nprevious revision. If a cluster is unstable, a declarative deployme\", \"nt will automatically return the cluster \\nback to a desired state. For example, if a node should cra\", \"sh, the deployment mechanism will redeploy \\na replacement to achieve your desired state \\n\\n41 \\n\\nCHAPT\", \"ER 3 | Scaling cloud-native applications \\n\\n \\n \\n\\fUsing declarative configuration allows infrastructur\", \"e to be represented as code that can be checked in \\nand versioned alongside the application code. It\", \" provides improved change control and better \\nsupport for continuous deployment using a build and de\", \"ploy pipeline. \\n\\nWhat scenarios are ideal for containers and orchestrators? \\n\\nThe following scenario\", \"s are ideal for using containers and orchestrators. \\n\\nApplications requiring high uptime and scalabi\", \"lity \\n\\nIndividual applications that have high uptime and scalability requirements are ideal candidat\", \"es for \\ncloud-native architectures using microservices, containers, and orchestrators. They can be d\", \"eveloped \\nin containers, tested across versioned environments, and deployed into production with zer\", \"o \\ndowntime. The use of Kubernetes clusters ensures such apps can also scale on demand and recover \\n\", \"automatically from node failures. \\n\\nLarge numbers of applications \\n\\nOrganizations that deploy and ma\", \"intain large numbers of applications benefit from containers and \\norchestrators. The up front effort\", \" of setting up containerized environments and Kubernetes clusters is \\nprimarily a fixed cost. Deploy\", \"ing, maintaining, and updating individual applications has a cost that \\nvaries with the number of ap\", \"plications. Beyond a few applications, the complexity of maintaining \\ncustom applications manually e\", \"xceeds the cost of implementing a solution using containers and \\norchestrators. \\n\\nWhen should you av\", \"oid using containers and orchestrators? \\n\\nIf you\\u2019re unable to build your application following the T\", \"welve-Factor App principles, you should \\nconsider avoiding containers and orchestrators. In these ca\", \"ses, consider a VM-based hosting platform, \\nor possibly some hybrid system. With it, you can always \", \"spin off certain pieces of functionality into \\nseparate containers or even serverless functions. \\n\\nD\", \"evelopment resources \\n\\nThis section shows a short list of development resources that may help you ge\", \"t started using \\ncontainers and orchestrators for your next application. If you\\u2019re looking for guida\", \"nce on how to \\ndesign your cloud-native microservices architecture app, read this book\\u2019s companion, \", \".NET \\nMicroservices: Architecture for Containerized .NET Applications. \\n\\nLocal Kubernetes Developmen\", \"t \\n\\nKubernetes deployments provide great value in production environments, but can also run locally \", \"on \\nyour development machine. While you may work on individual microservices independently, there \\nm\", \"ay be times when you\\u2019ll need to run the entire system locally - just as it will run when deployed to\", \" \\nproduction. There are several tools that can help: Minikube and Docker Desktop. Visual Studio also\", \" \\nprovides tooling for Docker development. \\n\\n42 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n\", \" \\n\\fMinikube \\n\\nWhat is Minikube? The Minikube project says \\u201cMinikube implements a local Kubernetes cl\", \"uster on \\nmacOS, Linux, and Windows.\\u201d Its primary goals are \\u201cto be the best tool for local Kubernete\", \"s \\napplication development and to support all Kubernetes features that fit.\\u201d Installing Minikube is \", \"\\nseparate from Docker, but Minikube supports different hypervisors than Docker Desktop supports. \\nTh\", \"e following Kubernetes features are currently supported by Minikube: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nD\", \"NS \\n\\nNodePorts \\n\\nConfigMaps and secrets \\n\\nDashboards \\n\\nContainer runtimes: Docker, rkt, CRI-O, and c\", \"ontainerd \\n\\nEnabling Container Network Interface (CNI) \\n\\nIngress \\n\\nAfter installing Minikube, you ca\", \"n quickly start using it by running the minikube start command, which \\ndownloads an image and start \", \"the local Kubernetes cluster. Once the cluster is started, you interact \\nwith it using the standard \", \"Kubernetes kubectl commands. \\n\\nDocker Desktop \\n\\nYou can also work with Kubernetes directly from Dock\", \"er Desktop on Windows. It is your only option if \\nyou\\u2019re using Windows Containers, and is a great ch\", \"oice for non-Windows containers as well. Figure 3-\\n4 shows how to enable local Kubernetes support wh\", \"en running Docker Desktop. \\n\\nFigure 3-4. Configuring Kubernetes in Docker Desktop. \\n\\n43 \\n\\nCHAPTER 3 \", \"| Scaling cloud-native applications \\n\\n \\n \\n \\n\\fDocker Desktop is the most popular tool for configuring\", \" and running containerized apps locally. \\nWhen you work with Docker Desktop, you can develop locally\", \" against the exact same set of Docker \\ncontainer images that you\\u2019ll deploy to production. Docker Des\", \"ktop is designed to \\u201cbuild, test, and \\nship\\u201d containerized apps locally. It supports both Linux and \", \"Windows containers. Once you push your \\nimages to an image registry, like Azure Container Registry o\", \"r Docker Hub, AKS can pull and deploy \\nthem to production. \\n\\nVisual Studio Docker Tooling \\n\\nVisual S\", \"tudio supports Docker development for web-based applications. When you create a new \\nASP.NET Core ap\", \"plication, you have an option to configure it with Docker support, as shown in Figure \\n3-5. \\n\\nFigure\", \" 3-5. Visual Studio Enable Docker Support \\n\\nWhen this option is selected, the project is created wit\", \"h a Dockerfile in its root, which can be used to \\nbuild and host the app in a Docker container. An e\", \"xample Dockerfile is shown in Figure 3-6. \\n\\nFROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base \\nWORKDI\", \"R /app \\nEXPOSE 80 \\nEXPOSE 443 \\n\\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build \\nWORKDIR /src \\nCOPY [\", \"\\\"eShopWeb/eShopWeb.csproj\\\", \\\"eShopWeb/\\\"] \\nRUN dotnet restore \\\"eShopWeb/eShopWeb.csproj\\\" \\nCOPY . . \\nW\", \"ORKDIR \\\"/src/eShopWeb\\\" \\nRUN dotnet build \\\"eShopWeb.csproj\\\" -c Release -o /app/build \\n\\nFROM build AS \", \"publish \\nRUN dotnet publish \\\"eShopWeb.csproj\\\" -c Release -o /app/publish \\n\\n44 \\n\\nCHAPTER 3 | Scaling \", \"cloud-native applications \\n\\n \\n \\n \\n \\n \\n \\n\\fFROM base AS final \\nWORKDIR /app \\nCOPY --from=publish /app/\", \"publish . \\nENTRYPOINT [\\\"dotnet\\\", \\\"eShopWeb.dll\\\"] \\n\\nFigure 3-6. Visual Studio generated Dockerfile \\n\\n\", \"Once support is added, you can run your application in a Docker container in Visual Studio. Figure 3\", \"-7 \\nshows the different run options available from a new ASP.NET Core project created with Docker \\ns\", \"upport added. \\n\\nFigure 3-7. Visual Studio Docker Run Options \\n\\nAlso, at any time you can add Docker \", \"support to an existing ASP.NET Core application. From the \\nVisual Studio Solution Explorer, right-cl\", \"ick on the project and select Add > Docker Support, as shown \\nin Figure 3-8. \\n\\n45 \\n\\nCHAPTER 3 | Scal\", \"ing cloud-native applications \\n\\n \\n \\n \\n\\fFigure 3-8. Adding Docker support to Visual Studio \\n\\nVisual S\", \"tudio Code Docker Tooling \\n\\nThere are many extensions available for Visual Studio Code that support \", \"Docker development. \\n\\nMicrosoft provides the Docker for Visual Studio Code extension. This extension\", \" simplifies the process \\nof adding container support to applications. It scaffolds required files, b\", \"uilds Docker images, and \\nenables you to debug your app inside a container. The extension features a\", \" visual explorer that makes \\nit easy to take actions on containers and images such as start, stop, i\", \"nspect, remove, and more. The \\nextension also supports Docker Compose enabling you to manage multipl\", \"e running containers as a \\nsingle unit. \\n\\nLeveraging serverless functions \\n\\nIn the spectrum from man\", \"aging physical machines to leveraging cloud capabilities, serverless lives at \\nthe extreme end. Your\", \" only responsibility is your code, and you only pay when your code runs. Azure \\nFunctions provides a\", \" way to build serverless capabilities into your cloud-native applications. \\n\\n46 \\n\\nCHAPTER 3 | Scalin\", \"g cloud-native applications \\n\\n \\n \\n \\n\\fWhat is serverless? \\n\\nServerless is a relatively new service mo\", \"del of cloud computing. It doesn\\u2019t mean that servers are \\noptional - your code still runs on a serve\", \"r somewhere. The distinction is that the application team no \\nlonger concerns itself with managing s\", \"erver infrastructure. Instead, the cloud vendor own this \\nresponsibility. The development team incre\", \"ases its productivity by delivering business solutions to \\ncustomers, not plumbing. \\n\\nServerless com\", \"puting uses event-triggered stateless containers to host your services. They can scale \\nout and in t\", \"o meet demand as-needed. Serverless platforms like Azure Functions have tight \\nintegration with othe\", \"r Azure services like queues, events, and storage. \\n\\nWhat challenges are solved by serverless? \\n\\nSer\", \"verless platforms address many time-consuming and expensive concerns: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nPurchasi\", \"ng machines and software licenses \\n\\nHousing, securing, configuring, and maintaining the machines and\", \" their networking, power, \\nand A/C requirements \\n\\nPatching and upgrading operating systems and softw\", \"are \\n\\nConfiguring web servers or machine services to host application software \\n\\nConfiguring applica\", \"tion software within its platform \\n\\nMany companies allocate large budgets to support hardware infras\", \"tructure concerns. Moving to the \\ncloud can help reduce these costs; shifting applications to server\", \"less can help eliminate them. \\n\\nWhat is the difference between a microservice and a serverless \\nfunc\", \"tion? \\n\\nTypically, a microservice encapsulates a business capability, such as a shopping cart for an\", \" online \\neCommerce site. It exposes multiple operations that enable a user to manage their shopping \", \"\\nexperience. A function, however, is a small, lightweight block of code that executes a single-purpo\", \"se \\noperation in response to an event. Microservices are typically constructed to respond to request\", \"s, \\noften from an interface. Requests can be HTTP Rest- or gRPC-based. Serverless services respond t\", \"o \\nevents. Its event-driven architecture is ideal for processing short-running, background tasks. \\n\\n\", \"What scenarios are appropriate for serverless? \\n\\nServerless exposes individual short-running functio\", \"ns that are invoked in response to a trigger. This \\nmakes them ideal for processing background tasks\", \". \\n\\nAn application might need to send an email as a step in a workflow. Instead of sending the \\nnoti\", \"fication as part of a microservice request, place the message details onto a queue. An Azure \\nFuncti\", \"on can dequeue the message and asynchronously send the email. Doing so could improve the \\nperformanc\", \"e and scalability of the microservice. Queue-based load leveling can be implemented to \\navoid bottle\", \"necks related to sending the emails. Additionally, this stand-alone service could be reused \\nas a ut\", \"ility across many different applications. \\n\\n47 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \", \"\\n\\fAsynchronous messaging from queues and topics is a common pattern to trigger serverless functions.\", \" \\nHowever, Azure Functions can be triggered by other events, such as changes to Azure Blob Storage. \", \"A \\nservice that supports image uploads could have an Azure Function responsible for optimizing the \\n\", \"image size. The function could be triggered directly by inserts into Azure Blob Storage, keeping \\nco\", \"mplexity out of the microservice operations. \\n\\nMany services have long-running processes as part of \", \"their workflows. Often these tasks are done as \\npart of the user\\u2019s interaction with the application.\", \" These tasks can force the user to wait, negatively \\nimpacting their experience. Serverless computin\", \"g provides a great way to move slower tasks outside \\nof the user interaction loop. These tasks can s\", \"cale with demand without requiring the entire \\napplication to scale. \\n\\nWhen should you avoid serverl\", \"ess? \\n\\nServerless solutions provision and scale on demand. When a new instance is invoked, cold star\", \"ts are a \\ncommon issue. A cold start is the period of time it takes to provision this instance. Norm\", \"ally, this delay \\nmight be a few seconds, but can be longer depending on various factors. Once provi\", \"sioned, a single \\ninstance is kept alive as long as it receives periodic requests. But, if a service\", \" is called less frequently, \\nAzure may remove it from memory and require a cold start when reinvoked\", \". Cold starts are also \\nrequired when a function scales out to a new instance. \\n\\nFigure 3-9 shows a \", \"cold-start pattern. Note the extra steps required when the app is cold. \\n\\nFigure 3-9. Cold start ver\", \"sus warm start. \\n\\nTo avoid cold starts entirely, you might switch from a consumption plan to a dedic\", \"ated plan. You can \\nalso configure one or more pre-warmed instances with the premium plan upgrade. I\", \"n these cases, \\nwhen you need to add another instance, it\\u2019s already up and ready to go. These option\", \"s can help \\nmitigate the cold start issue associated with serverless computing. \\n\\n48 \\n\\nCHAPTER 3 | S\", \"caling cloud-native applications \\n\\n \\n \\n \\n\\fCloud providers bill for serverless based on compute execu\", \"tion time and consumed memory. Long \\nrunning operations or high memory consumption workloads aren\\u2019t \", \"always the best candidates for \\nserverless. Serverless functions favor small chunks of work that can\", \" complete quickly. Most serverless \\nplatforms require individual functions to complete within a few \", \"minutes. Azure Functions defaults to a \\n5-minute time-out duration, which can be configured up to 10\", \" minutes. The Azure Functions premium \\nplan can mitigate this issue as well, defaulting time-outs to\", \" 30 minutes with an unbounded higher \\nlimit that can be configured. Compute time isn\\u2019t calendar time\", \". More advanced functions using the \\nAzure Durable Functions framework may pause execution over a co\", \"urse of several days. The billing is \\nbased on actual execution time - when the function wakes up an\", \"d resumes processing. \\n\\nFinally, leveraging Azure Functions for application tasks adds complexity. I\", \"t\\u2019s wise to first architect \\nyour application with a modular, loosely coupled design. Then, identify\", \" if there are benefits serverless \\nwould offer that justify the additional complexity. \\n\\nCombining c\", \"ontainers and serverless approaches \\n\\nCloud-native applications typically implement services leverag\", \"ing containers and orchestration. There \\nare often opportunities to expose some of the application\\u2019s\", \" services as Azure Functions. However, \\nwith a cloud-native app deployed to Kubernetes, it would be \", \"nice to leverage Azure Functions within \\nthis same toolset. Fortunately, you can wrap Azure Function\", \"s inside Docker containers and deploy \\nthem using the same processes and tools as the rest of your K\", \"ubernetes-based app. \\n\\nWhen does it make sense to use containers with serverless? \\n\\nYour Azure Funct\", \"ion has no knowledge of the platform on which it\\u2019s deployed. For some scenarios, \\nyou may have speci\", \"fic requirements and need to customize the environment on which your function \\ncode will run. You\\u2019ll\", \" need a custom image that supports dependencies or a configuration not \\nsupported by the default ima\", \"ge. In these cases, it makes sense to deploy your function in a custom \\nDocker container. \\n\\nWhen sho\", \"uld you avoid using containers with Azure Functions? \\n\\nIf you want to use consumption billing, you c\", \"an\\u2019t run your function in a container. What\\u2019s more, if you \\ndeploy your function to a Kubernetes clu\", \"ster, you\\u2019ll no longer benefit from the built-in scaling \\nprovided by Azure Functions. You\\u2019ll need t\", \"o use Kubernetes\\u2019 scaling features, described earlier in this \\nchapter. \\n\\nHow to combine serverless \", \"and Docker containers \\n\\nTo wrap an Azure Function in a Docker container, install the Azure Functions\", \" Core Tools and then run \\nthe following command: \\n\\nfunc init ProjectName --worker-runtime dotnet --d\", \"ocker \\n\\nWhen the project is created, it will include a Dockerfile and the worker runtime configured \", \"to dotnet. \\nNow, you can create and test your function locally. Build and run it using the docker bu\", \"ild and docker \\n\\n49 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \\n\\frun commands. For detaile\", \"d steps to get started building Azure Functions with Docker support, see \\nthe Create a function on L\", \"inux using a custom image tutorial. \\n\\nHow to combine serverless and Kubernetes with KEDA \\n\\nIn this c\", \"hapter, you\\u2019ve seen that the Azure Functions\\u2019 platform automatically scales out to meet \\ndemand. Whe\", \"n deploying containerized functions to AKS, however, you lose the built-in scaling \\nfunctionality. T\", \"o the rescue comes Kubernetes-based Event Driven (KEDA). It enables fine-grained \\nautoscaling for ev\", \"ent-driven Kubernetes workloads, including containerized functions. \\n\\nKEDA provides event-driven sca\", \"ling functionality to the Functions\\u2019 runtime in a Docker container. \\nKEDA can scale from zero instan\", \"ces (when no events are occurring) out to n instances, based on load. \\nIt enables autoscaling by exp\", \"osing custom metrics to the Kubernetes autoscaler (Horizontal Pod \\nAutoscaler). Using Functions cont\", \"ainers with KEDA makes it possible to replicate serverless function \\ncapabilities in any Kubernetes \", \"cluster. \\n\\nIt\\u2019s worth noting that the KEDA project is now managed by the Cloud Native Computing Foun\", \"dation \\n(CNCF). \\n\\nDeploying containers in Azure \\n\\nWe\\u2019ve discussed containers in this chapter and in \", \"chapter 1. We\\u2019ve seen that containers provide many \\nbenefits to cloud-native applications, including\", \" portability. In the Azure cloud, you can deploy the \\nsame containerized services across staging and\", \" production environments. Azure provides several \\noptions for hosting these containerized workloads:\", \" \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAzure Kubernetes Services (AKS) \\n\\nAzure Container Instance (ACI) \\n\\nAzure Web Apps for\", \" Containers \\n\\nAzure Container Registry \\n\\nWhen containerizing a microservice, you first build a conta\", \"iner \\u201cimage.\\u201d The image is a binary \\nrepresentation of the service code, dependencies, and runtime. \", \"While you can manually create an \\nimage using the Docker Build command from the Docker API, a better\", \" approach is to create it as part \\nof an automated build process. \\n\\nOnce created, container images a\", \"re stored in container registries. They enable you to build, store, and \\nmanage container images. Th\", \"ere are many registries available, both public and private. Azure \\nContainer Registry (ACR) is a ful\", \"ly managed container registry service in the Azure cloud. It persists \\nyour images inside the Azure \", \"network, reducing the time to deploy them to Azure container hosts. \\nYou can also secure them using \", \"the same security and identity procedures that you use for other \\nAzure resources. \\n\\nYou create an A\", \"zure Container Registry using the Azure portal, Azure CLI, or PowerShell tools. \\nCreating a registry\", \" in Azure is simple. It requires an Azure subscription, resource group, and a unique \\n\\n50 \\n\\nCHAPTER \", \"3 | Scaling cloud-native applications \\n\\n \\n \\n\\fname. Figure 3-10 shows the basic options for creating \", \"a registry, which will be hosted at \\nregistryname.azurecr.io. \\n\\nFigure 3-10. Create container regist\", \"ry \\n\\nOnce you\\u2019ve created the registry, you\\u2019ll need to authenticate with it before you can use it. Ty\", \"pically, \\nyou\\u2019ll log into the registry using the Azure CLI command: \\n\\naz acr login --name *registryn\", \"ame* \\n\\nOnce authenticated, you can use docker commands to push container images to it. Before you ca\", \"n do \\nso, however, you must tag your image with the fully qualified name (URL) of your ACR login ser\", \"ver. It \\nwill have the format registryname.azurecr.io. \\n\\ndocker tag mycontainer myregistry.azurecr.i\", \"o/mycontainer:v1 \\n\\nAfter you\\u2019ve tagged the image, you use the docker push command to push the image \", \"to your ACR \\ninstance. \\n\\ndocker push myregistry.azurecr.io/mycontainer:v1 \\n\\n51 \\n\\nCHAPTER 3 | Scaling\", \" cloud-native applications \\n\\n \\n \\n \\n\\fAfter you push an image to the registry, it\\u2019s a good idea to rem\", \"ove the image from your local Docker \\nenvironment, using this command: \\n\\ndocker rmi myregistry.azure\", \"cr.io/mycontainer:v1 \\n\\nAs a best practice, you shouldn\\u2019t manually push images to a container registr\", \"y. Instead, use a build \\npipeline defined in a tool like GitHub or Azure DevOps. Learn more in the C\", \"loud-Native DevOps \\nchapter. \\n\\nACR Tasks \\n\\nACR Tasks is a set of features available from the Azure C\", \"ontainer Registry. It extends your inner-loop \\ndevelopment cycle by building and managing container \", \"images in the Azure cloud. Instead of \\ninvoking a docker build and docker push locally on your devel\", \"opment machine, they\\u2019re automatically \\nhandled by ACR Tasks in the cloud. \\n\\nThe following AZ CLI com\", \"mand both builds a container image and pushes it to ACR: \\n\\n# create a container registry \\naz acr cre\", \"ate --resource-group myResourceGroup --name myContainerRegistry008 --sku \\nBasic \\n\\n# build container \", \"image in ACR and push it into your container registry \\naz acr build --image sample/hello-world:v1  -\", \"-registry myContainerRegistry008 --file \\nDockerfile . \\n\\nAs you can see from the previous command blo\", \"ck, there\\u2019s no need to install Docker Desktop on your \\ndevelopment machine. Additionally, you can co\", \"nfigure ACR Task triggers to rebuild containers images \\non both source code and base image updates. \", \"\\n\\nAzure Kubernetes Service \\n\\nWe discussed Azure Kubernetes Service (AKS) at length in this chapter. \", \"We\\u2019ve seen that it\\u2019s the de \\nfacto container orchestrator managing containerized cloud-native applic\", \"ations. \\n\\nOnce you deploy an image to a registry, such as ACR, you can configure AKS to automaticall\", \"y pull and \\ndeploy it. With a CI/CD pipeline in place, you might configure a canary release strategy\", \" to minimize \\nthe risk involved when rapidly deploying updates. The new version of the app is initia\", \"lly configured in \\nproduction with no traffic routed to it. Then, the system will route a small perc\", \"entage of users to the \\nnewly deployed version. As the team gains confidence in the new version, it \", \"can roll out more \\ninstances and retire the old. AKS easily supports this style of deployment. \\n\\nAs \", \"with most resources in Azure, you can create an Azure Kubernetes Service cluster using the portal, \\n\", \"command-line, or automation tools like Helm or Terraform. To get started with a new cluster, you \\nne\", \"ed to provide the following information: \\n\\nAzure subscription \\n\\nResource group \\n\\nKubernetes cluster \", \"name \\n\\nRegion \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n52 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\", \"\\u2022 \\n\\n\\u2022 \\n\\nKubernetes version \\n\\nDNS name prefix \\n\\nNode size \\n\\nNode count \\n\\nThis information is sufficie\", \"nt to get started. As part of the creation process in the Azure portal, you can \\nalso configure opti\", \"ons for the following features of your cluster: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nScale \\n\\nAuthentication \\n\\nNetworking \\n\\n\", \"\\u2022  Monitoring \\n\\n\\u2022 \\n\\nTags \\n\\nThis quickstart walks through deploying an AKS cluster using the Azure po\", \"rtal. \\n\\nAzure Bridge to Kubernetes \\n\\nCloud-native applications can grow large and complex, requiring\", \" significant compute resources to \\nrun. In these scenarios, the entire application can\\u2019t be hosted o\", \"n a development machine (especially a \\nlaptop). Azure Bridge to Kubernetes addresses the shortcoming\", \". It enables developers to work with a \\nlocal version of their service while hosting the entire appl\", \"ication in an AKS development cluster. \\n\\nWhen ready, developers test their changes locally while run\", \"ning against the full application in the AKS \\ncluster - without replicating dependencies. Under the \", \"hood, the bridge merges code from the local \\nmachine with services in AKS. Developers can rapidly it\", \"erate and debug code directly in Kubernetes \\nusing Visual Studio or Visual Studio Code. \\n\\nGabe Monro\", \"y, former VP of Product Management at Microsoft, describes it well: \\n\\nImagine you\\u2019re a new employee \", \"trying to fix a bug in a complex microservices application consisting \\nof dozens of components, each\", \" with their own configuration and backing services. To get started, you \\nmust configure your local d\", \"evelopment environment so that it can mimic production including setting \\nup your IDE, building tool\", \" chain, containerized service dependencies, a local Kubernetes environment, \\nmocks for backing servi\", \"ces, and more. With all the time involved setting up your development \\nenvironment, fixing that firs\", \"t bug could take days! Or you could just use Bridge to Kubernetes and \\nAKS. \\n\\nScaling containers and\", \" serverless applications \\n\\nThere are two ways to scale an application: up or out. The former refers \", \"to adding capacity to a single \\nresource, while the latter refers to adding more resources to increa\", \"se capacity. \\n\\nThe simple solution: scaling up \\n\\nUpgrading an existing host server with increased CP\", \"U, memory, disk I/O speed, and network I/O \\nspeed is known as scaling up. Scaling up a cloud-native \", \"application involves choosing more capable \\n\\n53 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n\", \" \\n\\fresources from the cloud vendor. For example, you can create a new node pool with larger VMs in \\n\", \"your Kubernetes cluster. Then, migrate your containerized services to the new pool. \\n\\nServerless app\", \"s scale up by choosing the premium Functions plan or premium instance sizes from a \\ndedicated app se\", \"rvice plan. \\n\\nScaling out cloud-native apps \\n\\nCloud-native applications often experience large fluct\", \"uations in demand and require scale on a \\nmoment\\u2019s notice. They favor scaling out. Scaling out is do\", \"ne horizontally by adding additional \\nmachines (called nodes) or application instances to an existin\", \"g cluster. In Kubernetes, you can scale \\nmanually by adjusting configuration settings for the app (f\", \"or example, scaling a node pool), or \\nthrough autoscaling. \\n\\nAKS clusters can autoscale in one of tw\", \"o ways: \\n\\nFirst, the Horizontal Pod Autoscaler monitors resource demand and automatically scales you\", \"r POD \\nreplicas to meet it. When traffic increases, additional replicas are automatically provisione\", \"d to scale \\nout your services. Likewise, when demand decreases, they\\u2019re removed to scale-in your ser\", \"vices. You \\ndefine the metric on which to scale, for example, CPU usage. You can also specify the mi\", \"nimum and \\nmaximum number of replicas to run. AKS monitors that metric and scales accordingly. \\n\\nNex\", \"t, the AKS Cluster Autoscaler feature enables you to automatically scale compute nodes across a \\nKub\", \"ernetes cluster to meet demand. With it, you can automatically add new VMs to the underlying \\nAzure \", \"Virtual Machine Scale Set whenever more compute capacity of is required. It also removes \\nnodes when\", \" no longer required. \\n\\nFigure 3-11 shows the relationship between these two scaling services. \\n\\nFigu\", \"re 3-11. Scaling out an App Service plan. \\n\\n54 \\n\\nCHAPTER 3 | Scaling cloud-native applications \\n\\n \\n \", \"\\n \\n\\fWorking together, both ensure an optimal number of container instances and compute nodes to \\nsup\", \"port fluctuating demand. The horizontal pod autoscaler optimizes the number of pods required. \\nThe c\", \"luster autoscaler optimizes the number of nodes required. \\n\\nScaling Azure Functions \\n\\nAzure Function\", \"s automatically scale out upon demand. Server resources are dynamically allocated and \\nremoved based\", \" on the number of triggered events. You\\u2019re only charged for compute resources \\nconsumed when your fu\", \"nctions run. Billing is based upon the number of executions, execution time, \\nand memory used. \\n\\nWhi\", \"le the default consumption plan provides an economical and scalable solution for most apps, the \\npre\", \"mium option allows developers flexibility for custom Azure Functions requirements. Upgrading to \\nthe\", \" premium plan provides control over instance sizes, pre-warmed instances (to avoid cold start \\ndelay\", \"s), and dedicated VMs. \\n\\nOther container deployment options \\n\\nAside from Azure Kubernetes Service (A\", \"KS), you can also deploy containers to Azure App Service for \\nContainers and Azure Container Instanc\", \"es. \\n\\nWhen does it make sense to deploy to App Service for Containers? \\n\\nSimple production applicati\", \"ons that don\\u2019t require orchestration are well suited to Azure App Service \\nfor Containers. \\n\\nHow to \", \"deploy to App Service for Containers \\n\\nTo deploy to Azure App Service for Containers, you\\u2019ll need an\", \" Azure Container Registry (ACR) instance \\nand credentials to access it. Push your container image to\", \" the ACR repository so that your Azure App \\nService can pull it when needed. Once complete, you can \", \"configure the app for Continuous \\nDeployment. Doing so will automatically deploy updates whenever th\", \"e image changes in ACR. \\n\\nWhen does it make sense to deploy to Azure Container Instances? \\n\\nAzure Co\", \"ntainer Instances (ACI) enables you to run Docker containers in a managed, serverless cloud \\nenviron\", \"ment, without having to set up virtual machines or clusters. It\\u2019s a great solution for short-\\nrunnin\", \"g workloads that can run in an isolated container. Consider ACI for simple services, testing \\nscenar\", \"ios, task automation, and build jobs. ACI spins-up a container instance, performs the task, and \\nthe\", \"n spins it down. \\n\\nHow to deploy an app to Azure Container Instances \\n\\nTo deploy to Azure Container \", \"Instances (ACI), you need an Azure Container Registry (ACR) and \\ncredentials for accessing it. Once \", \"you push your container image to the repository, it\\u2019s available to pull \\ninto ACI. You can work with\", \" ACI using the Azure portal or command-line interface. ACR provides tight \\nintegration with ACI. Fig\", \"ure 3-12 shows how to push an individual container image to ACR. \\n\\n55 \\n\\nCHAPTER 3 | Scaling cloud-na\", \"tive applications \\n\\n \\n \\n\\fFigure 3-12. Azure Container Registry Run Instance \\n\\nCreating an instance i\", \"n ACI can be done quickly. Specify the image registry, Azure resource group \\ninformation, the amount\", \" of memory to allocate, and the port on which to listen. This quickstart shows \\nhow to deploy a cont\", \"ainer instance to ACI using the Azure portal. \\n\\nOnce the deployment completes, find the newly deploy\", \"ed container\\u2019s IP address and communicate \\nwith it over the port you specified. \\n\\nAzure Container In\", \"stances offers the fastest way to run simple container workloads in Azure. You don\\u2019t \\nneed to config\", \"ure an app service, orchestrator, or virtual machine. For scenarios where you require full \\ncontaine\", \"r orchestration, service discovery, automatic scaling, or coordinated upgrades, we \\nrecommend Azure \", \"Kubernetes Service (AKS). \\n\\nReferences \\n\\n\\u2022  What is Kubernetes? \\n\\n\\u2022 \\n\\nInstalling Kubernetes with Min\", \"ikube \\n\\n\\u2022  MiniKube vs Docker Desktop \\n\\n\\u2022 \\n\\nVisual Studio Tools for Docker \\n\\n56 \\n\\nCHAPTER 3 | Scalin\", \"g cloud-native applications \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nUnd\", \"erstanding serverless cold start \\n\\nPre-warmed Azure Functions instances \\n\\nCreate a function on Linux\", \" using a custom image \\n\\nRun Azure Functions in a Docker Container \\n\\nCreate a function on Linux using\", \" a custom image \\n\\nAzure Functions with Kubernetes Event Driven Autoscaling \\n\\nCanary Release \\n\\nAzure \", \"Dev Spaces with VS Code \\n\\nAzure Dev Spaces with Visual Studio \\n\\nAKS Multiple Node Pools \\n\\nAKS Cluste\", \"r Autoscaler \\n\\nTutorial: Scale applications in AKS \\n\\nAzure Functions scale and hosting \\n\\nAzure Conta\", \"iner Instances Docs \\n\\nDeploy Container Instance from ACR \\n\\n57 \\n\\nCHAPTER 3 | Scaling cloud-native app\", \"lications \\n\\n \\n \\n\\fCHAPTER  4 \\n\\nCloud-native \\ncommunication patterns \\n\\nWhen constructing a cloud-nativ\", \"e system, communication becomes a significant design decision. How \\ndoes a front-end client applicat\", \"ion communicate with a back-end microservice? How do back-end \\nmicroservices communicate with each o\", \"ther? What are the principles, patterns, and best practices to \\nconsider when implementing communica\", \"tion in cloud-native applications? \\n\\nCommunication considerations \\n\\nIn a monolithic application, com\", \"munication is straightforward. The code modules execute together in \\nthe same executable space (proc\", \"ess) on a server. This approach can have performance advantages as \\neverything runs together in shar\", \"ed memory, but results in tightly coupled code that becomes difficult \\nto maintain, evolve, and scal\", \"e. \\n\\nCloud-native systems implement a microservice-based architecture with many small, independent \\n\", \"microservices. Each microservice executes in a separate process and typically runs inside a containe\", \"r \\nthat is deployed to a cluster. \\n\\nA cluster groups a pool of virtual machines together to form a h\", \"ighly available environment. They\\u2019re \\nmanaged with an orchestration tool, which is responsible for d\", \"eploying and managing the \\ncontainerized microservices. Figure 4-1 shows a Kubernetes cluster deploy\", \"ed into the Azure cloud \\nwith the fully managed Azure Kubernetes Services. \\n\\n58 \\n\\nCHAPTER 4 | Cloud-\", \"native communication patterns \\n\\n \\n \\n\\fFigure 4-1. A Kubernetes cluster in Azure \\n\\nAcross the cluster,\", \" microservices communicate with each other through APIs and messaging \\ntechnologies. \\n\\nWhile they pr\", \"ovide many benefits, microservices are no free lunch. Local in-process method calls \\nbetween compone\", \"nts are now replaced with network calls. Each microservice must communicate over \\na network protocol\", \", which adds complexity to your system: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nNetwork congestion, latency, and trans\", \"ient faults are a constant concern. \\n\\nResiliency (that is, retrying failed requests) is essential. \\n\", \"\\nSome calls must be idempotent as to keep consistent state. \\n\\nEach microservice must authenticate an\", \"d authorize calls. \\n\\nEach message must be serialized and then deserialized - which can be expensive.\", \" \\n\\n\\u2022  Message encryption/decryption becomes important. \\n\\nThe book .NET Microservices: Architecture f\", \"or Containerized .NET Applications, available for free from \\nMicrosoft, provides an in-depth coverag\", \"e of communication patterns for microservice applications. In \\nthis chapter, we provide a high-level\", \" overview of these patterns along with implementation options \\navailable in the Azure cloud. \\n\\nIn th\", \"is chapter, we\\u2019ll first address communication between front-end applications and back-end \\nmicroserv\", \"ices. We\\u2019ll then look at back-end microservices communicate with each other. We\\u2019ll explore \\n\\n59 \\n\\nCH\", \"APTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fthe up and gRPC communication technology. Fin\", \"ally, we\\u2019ll look new innovative communication \\npatterns using service mesh technology. We\\u2019ll also se\", \"e how the Azure cloud provides different kinds \\nof backing services to support cloud-native communic\", \"ation. \\n\\nFront-end client communication \\n\\nIn a cloud-native system, front-end clients (mobile, web, \", \"and desktop applications) require a \\ncommunication channel to interact with independent back-end mic\", \"roservices. \\n\\nWhat are the options? \\n\\nTo keep things simple, a front-end client could directly commu\", \"nicate with the back-end microservices, \\nshown in Figure 4-2. \\n\\nFigure 4-2. Direct client to service\", \" communication \\n\\nWith this approach, each microservice has a public endpoint that is accessible by f\", \"ront-end clients. In \\na production environment, you\\u2019d place a load balancer in front of the microser\", \"vices, routing traffic \\nproportionately. \\n\\nWhile simple to implement, direct client communication wo\", \"uld be acceptable only for simple \\nmicroservice applications. This pattern tightly couples front-end\", \" clients to core back-end services, \\nopening the door for many problems, including: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \", \"\\n\\n60 \\n\\nClient susceptibility to back-end service refactoring. \\n\\nA wider attack surface as core back-\", \"end services are directly exposed. \\n\\nDuplication of cross-cutting concerns across each microservice.\", \" \\n\\nOverly complex client code - clients must keep track of multiple endpoints and handle failures \\ni\", \"n a resilient way. \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fInstead, a widely acce\", \"pted cloud design pattern is to implement an API Gateway Service between the \\nfront-end applications\", \" and back-end services. The pattern is shown in Figure 4-3. \\n\\nFigure 4-3. API gateway pattern \\n\\nIn t\", \"he previous figure, note how the API Gateway service abstracts the back-end core microservices. \\nImp\", \"lemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal \\nmicrose\", \"rvices. \\n\\nThe gateway insulates the client from internal service partitioning and refactoring. If yo\", \"u change a \\nback-end service, you accommodate for it in the gateway without breaking the client. It\\u2019\", \"s also your \\nfirst line of defense for cross-cutting concerns, such as identity, caching, resiliency\", \", metering, and \\nthrottling. Many of these cross-cutting concerns can be off-loaded from the back-en\", \"d core services to \\nthe gateway, simplifying the back-end services. \\n\\nCare must be taken to keep the\", \" API Gateway simple and fast. Typically, business logic is kept out of \\nthe gateway. A complex gatew\", \"ay risks becoming a bottleneck and eventually a monolith itself. Larger \\nsystems often expose multip\", \"le API Gateways segmented by client type (mobile, web, desktop) or \\nback-end functionality. The Back\", \"end for Frontends pattern provides direction for implementing \\nmultiple gateways. The pattern is sho\", \"wn in Figure 4-4. \\n\\n61 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fFigure 4-4. Backen\", \"d for frontend pattern \\n\\nNote in the previous figure how incoming traffic is sent to a specific API \", \"gateway - based upon client \\ntype: web, mobile, or desktop app. This approach makes sense as the cap\", \"abilities of each device differ \\nsignificantly across form factor, performance, and display limitati\", \"ons. Typically mobile applications \\nexpose less functionality than a browser or desktop applications\", \". Each gateway can be optimized to \\nmatch the capabilities and functionality of the corresponding de\", \"vice. \\n\\nSimple Gateways \\n\\nTo start, you could build your own API Gateway service. A quick search of \", \"GitHub will provide many \\nexamples. \\n\\nFor simple .NET cloud-native applications, you might consider \", \"the Ocelot Gateway. Open source and \\ncreated for .NET microservices, it\\u2019s lightweight, fast, scalabl\", \"e. Like any API Gateway, its primary \\nfunctionality is to forward incoming HTTP requests to downstre\", \"am services. Additionally, it supports a \\nwide variety of capabilities that are configurable in a .N\", \"ET middleware pipeline. \\n\\nYARP (Yet Another Reverse proxy) is another open source reverse proxy led \", \"by a group of Microsoft \\nproduct teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET\", \" framework as \\nmiddleware and is highly customizable. You\\u2019ll find YARP well-documented with various \", \"usage \\nexamples. \\n\\nFor enterprise cloud-native applications, there are several managed Azure service\", \"s that can help \\njump-start your efforts. \\n\\n62 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \", \"\\n \\n \\n\\fAzure Application Gateway \\n\\nFor simple gateway requirements, you may consider Azure Applicatio\", \"n Gateway. Available as an Azure \\nPaaS service, it includes basic gateway features such as URL routi\", \"ng, SSL termination, and a Web \\nApplication Firewall. The service supports Layer-7 load balancing ca\", \"pabilities. With Layer 7, you can \\nroute requests based on the actual content of an HTTP message, no\", \"t just low-level TCP network \\npackets. \\n\\nThroughout this book, we evangelize hosting cloud-native sy\", \"stems in Kubernetes. A container \\norchestrator, Kubernetes automates the deployment, scaling, and op\", \"erational concerns of \\ncontainerized workloads. Azure Application Gateway can be configured as an AP\", \"I gateway for Azure \\nKubernetes Service cluster. \\n\\nThe Application Gateway Ingress Controller enable\", \"s Azure Application Gateway to work directly with \\nAzure Kubernetes Service. Figure 4.5 shows the ar\", \"chitecture. \\n\\nFigure 4-5. Application Gateway Ingress Controller \\n\\nKubernetes includes a built-in fe\", \"ature that supports HTTP (Level 7) load balancing, called Ingress. \\nIngress defines a set of rules f\", \"or how microservice instances inside AKS can be exposed to the outside \\nworld. In the previous image\", \", the ingress controller interprets the ingress rules configured for the \\ncluster and automatically \", \"configures the Azure Application Gateway. Based on those rules, the \\nApplication Gateway routes traf\", \"fic to microservices running inside AKS. The ingress controller listens \\nfor changes to ingress rule\", \"s and makes the appropriate changes to the Azure Application Gateway. \\n\\nAzure API Management \\n\\nFor m\", \"oderate to large-scale cloud-native systems, you may consider Azure API Management. It\\u2019s a \\ncloud-ba\", \"sed service that not only solves your API Gateway needs, but provides a full-featured \\ndeveloper and\", \" administrative experience. API Management is shown in Figure 4-6. \\n\\n63 \\n\\nCHAPTER 4 | Cloud-native c\", \"ommunication patterns \\n\\n \\n \\n \\n\\fFigure 4-6. Azure API Management \\n\\nTo start, API Management exposes a\", \" gateway server that allows controlled access to back-end \\nservices based upon configurable rules an\", \"d policies. These services can be in the Azure cloud, your \\non-prem data center, or other public clo\", \"uds. API keys and JWT tokens determine who can do what. All \\ntraffic is logged for analytical purpos\", \"es. \\n\\nFor developers, API Management offers a developer portal that provides access to services, \\ndo\", \"cumentation, and sample code for invoking them. Developers can use Swagger/Open API to \\ninspect serv\", \"ice endpoints and analyze their usage. The service works across the major development \\nplatforms: .N\", \"ET, Java, Golang, and more. \\n\\nThe publisher portal exposes a management dashboard where administrato\", \"rs expose APIs and \\nmanage their behavior. Service access can be granted, service health monitored, \", \"and service telemetry \\ngathered. Administrators apply policies to each endpoint to affect behavior. \", \"Policies are pre-built \\nstatements that execute sequentially for each service call. Policies are con\", \"figured for an inbound call, \\noutbound call, or invoked upon an error. Policies can be applied at di\", \"fferent service scopes as to \\nenable deterministic ordering when combining policies. The product shi\", \"ps with a large number of \\nprebuilt policies. \\n\\nHere are examples of how policies can affect the beh\", \"avior of your cloud-native services: \\n\\nRestrict service access. \\n\\nEnforce authentication. \\n\\nThrottle\", \" calls from a single source, if necessary. \\n\\nEnable caching. \\n\\nBlock calls from specific IP addresse\", \"s. \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n64 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\nCon\", \"trol the flow of the service. \\n\\nConvert requests from SOAP to REST or between different data formats\", \", such as from XML to \\nJSON. \\n\\nAzure API Management can expose back-end services that are hosted any\", \"where \\u2013 in the cloud or your \\ndata center. For legacy services that you may expose in your cloud-nat\", \"ive systems, it supports both \\nREST and SOAP APIs. Even other Azure services can be exposed through \", \"API Management. You could \\nplace a managed API on top of an Azure backing service like Azure Service\", \" Bus or Azure Logic Apps. \\nAzure API Management doesn\\u2019t include built-in load-balancing support and \", \"should be used in \\nconjunction with a load-balancing service. \\n\\nAzure API Management is available ac\", \"ross four different tiers: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nDeveloper \\n\\nBasic \\n\\nStandard \\n\\nPremium \\n\\nThe Developer \", \"tier is meant for non-production workloads and evaluation. The other tiers offer \\nprogressively more\", \" power, features, and higher service level agreements (SLAs). The Premium tier \\nprovides Azure Virtu\", \"al Network and multi-region support. All tiers have a fixed price per hour. \\n\\nThe Azure cloud also o\", \"ffers a serverless tier for Azure API Management. Referred to as the \\nconsumption pricing tier, the \", \"service is a variant of API Management designed around the serverless \\ncomputing model. Unlike the \\u201c\", \"pre-allocated\\u201d pricing tiers previously shown, the consumption tier \\nprovides instant provisioning a\", \"nd pay-per-action pricing. \\n\\nIt enables API Gateway features for the following use cases: \\n\\n\\u2022  Micro\", \"services implemented using serverless technologies such as Azure Functions and Azure \\n\\nLogic Apps. \\n\", \"\\n\\u2022 \\n\\nAzure backing service resources such as Service Bus queues and topics, Azure storage, and \\nothe\", \"rs. \\n\\n\\u2022  Microservices where traffic has occasional large spikes but remains low most the time. \\n\\nTh\", \"e consumption tier uses the same underlying service API Management components, but employs \\nan entir\", \"ely different architecture based on dynamically allocated resources. It aligns perfectly with the \\ns\", \"erverless computing model: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nNo infrastructure to manage. \\n\\nNo idle capacity. \\n\\n\", \"High-availability. \\n\\nAutomatic scaling. \\n\\nCost is based on actual usage. \\n\\nThe new consumption tier \", \"is a great choice for cloud-native systems that expose serverless resources \\nas APIs. \\n\\n65 \\n\\nCHAPTER\", \" 4 | Cloud-native communication patterns \\n\\n \\n \\n\\fReal-time communication \\n\\nReal-time, or push, commun\", \"ication is another option for front-end applications that communicate \\nwith back-end cloud-native sy\", \"stems over HTTP. Applications, such as financial-tickers, online \\neducation, gaming, and job-progres\", \"s updates, require instantaneous, real-time responses from the \\nback-end. With normal HTTP communica\", \"tion, there\\u2019s no way for the client to know when new data is \\navailable. The client must continually\", \" poll or send requests to the server. With real-time \\ncommunication, the server can push new data to\", \" the client at any time. \\n\\nReal-time systems are often characterized by high-frequency data flows an\", \"d large numbers of \\nconcurrent client connections. Manually implementing real-time connectivity can \", \"quickly become \\ncomplex, requiring non-trivial infrastructure to ensure scalability and reliable mes\", \"saging to connected \\nclients. You could find yourself managing an instance of Azure Redis Cache and \", \"a set of load \\nbalancers configured with sticky sessions for client affinity. \\n\\nAzure SignalR Servic\", \"e is a fully managed Azure service that simplifies real-time communication for \\nyour cloud-native ap\", \"plications. Technical implementation details like capacity provisioning, scaling, \\nand persistent co\", \"nnections are abstracted away. They\\u2019re handled for you with a 99.9% service-level \\nagreement. You fo\", \"cus on application features, not infrastructure plumbing. \\n\\nOnce enabled, a cloud-based HTTP service\", \" can push content updates directly to connected clients, \\nincluding browser, mobile and desktop appl\", \"ications. Clients are updated without the need to poll the \\nserver. Azure SignalR abstracts the tran\", \"sport technologies that create real-time connectivity, including \\nWebSockets, Server-Side Events, an\", \"d Long Polling. Developers focus on sending messages to all or \\nspecific subsets of connected client\", \"s. \\n\\nFigure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure Sign\", \"alR \\nenabled. \\n\\n66 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n\\fFigure 4-7. Azure Signal\", \"R \\n\\nAnother advantage of Azure SignalR Service comes with implementing Serverless cloud-native \\nserv\", \"ices. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can \\nbe t\", \"ricky because your code doesn\\u2019t maintain long connections with clients. Azure SignalR Service can \\nh\", \"andle this situation since the service already manages connections for you. \\n\\nAzure SignalR Service \", \"closely integrates with other Azure services, such as Azure SQL Database, \\nService Bus, or Redis Cac\", \"he, opening up many possibilities for your cloud-native applications. \\n\\nService-to-service communica\", \"tion \\n\\nMoving from the front-end client, we now address back-end microservices communicate with each\", \" \\nother. \\n\\nWhen constructing a cloud-native application, you\\u2019ll want to be sensitive to how back-end\", \" services \\ncommunicate with each other. Ideally, the less inter-service communication, the better. H\", \"owever, \\navoidance isn\\u2019t always possible as back-end services often rely on one another to complete \", \"an \\noperation. \\n\\nThere are several widely accepted approaches to implementing cross-service communic\", \"ation. The type \\nof communication interaction will often determine the best approach. \\n\\nConsider the\", \" following interaction types: \\n\\n\\u2022 \\n\\nQuery \\u2013 when a calling microservice requires a response from a c\", \"alled microservice, such as, \\n\\u201cHey, give me the buyer information for a given customer Id.\\u201d \\n\\n67 \\n\\nC\", \"HAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\nCommand \\u2013 when the calling microserv\", \"ice needs another microservice to execute an action \\nbut doesn\\u2019t require a response, such as, \\u201cHey, \", \"just ship this order.\\u201d \\n\\nEvent \\u2013 when a microservice, called the publisher, raises an event that sta\", \"te has changed or an \\naction has occurred. Other microservices, called subscribers, who are interest\", \"ed, can react to \\nthe event appropriately. The publisher and the subscribers aren\\u2019t aware of each ot\", \"her. \\n\\nMicroservice systems typically use a combination of these interaction types when executing \\no\", \"perations that require cross-service interaction. Let\\u2019s take a close look at each and how you might \", \"\\nimplement them. \\n\\nQueries \\n\\nMany times, one microservice might need to query another, requiring an \", \"immediate response to \\ncomplete an operation. A shopping basket microservice may need product inform\", \"ation and a price to \\nadd an item to its basket. There are many approaches for implementing query op\", \"erations. \\n\\nRequest/Response Messaging \\n\\nOne option for implementing this scenario is for the callin\", \"g back-end microservice to make direct \\nHTTP requests to the microservices it needs to query, shown \", \"in Figure 4-8. \\n\\nFigure 4-8. Direct HTTP communication \\n\\nWhile direct HTTP calls between microservic\", \"es are relatively simple to implement, care should be \\ntaken to minimize this practice. To start, th\", \"ese calls are always synchronous and will block the \\noperation until a result is returned or the req\", \"uest times outs. What were once self-contained, \\nindependent services, able to evolve independently \", \"and deploy frequently, now become coupled to \\neach other. As coupling among microservices increase, \", \"their architectural benefits diminish. \\n\\n68 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n\", \" \\n\\fExecuting an infrequent request that makes a single direct HTTP call to another microservice migh\", \"t be \\nacceptable for some systems. However, high-volume calls that invoke direct HTTP calls to multi\", \"ple \\nmicroservices aren\\u2019t advisable. They can increase latency and negatively impact the performance\", \", \\nscalability, and availability of your system. Even worse, a long series of direct HTTP communicat\", \"ion can \\nlead to deep and complex chains of synchronous microservices calls, shown in Figure 4-9: \\n\\n\", \"Figure 4-9. Chaining HTTP queries \\n\\nYou can certainly imagine the risk in the design shown in the pr\", \"evious image. What happens if Step \\n#3 fails? Or Step #8 fails? How do you recover? What if Step #6 \", \"is slow because the underlying service \\nis busy? How do you continue? Even if all works correctly, t\", \"hink of the latency this call would incur, \\nwhich is the sum of the latency of each step. \\n\\nThe larg\", \"e degree of coupling in the previous image suggests the services weren\\u2019t optimally modeled. \\nIt woul\", \"d behoove the team to revisit their design. \\n\\nMaterialized View pattern \\n\\nA popular option for remov\", \"ing microservice coupling is the Materialized View pattern. With this \\npattern, a microservice store\", \"s its own local, denormalized copy of data that\\u2019s owned by other services. \\nInstead of the Shopping \", \"Basket microservice querying the Product Catalog and Pricing microservices, \\nit maintains its own lo\", \"cal copy of that data. This pattern eliminates unnecessary coupling and \\nimproves reliability and re\", \"sponse time. The entire operation executes inside a single process. We \\nexplore this pattern and oth\", \"er data concerns in Chapter 5. \\n\\nService Aggregator Pattern \\n\\nAnother option for eliminating microse\", \"rvice-to-microservice coupling is an Aggregator microservice, \\nshown in purple in Figure 4-10. \\n\\n69 \", \"\\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fFigure 4-10. Aggregator microservice \\n\\nTh\", \"e pattern isolates an operation that makes calls to multiple back-end microservices, centralizing it\", \"s \\nlogic into a specialized microservice. The purple checkout aggregator microservice in the previou\", \"s \\nfigure orchestrates the workflow for the Checkout operation. It includes calls to several back-en\", \"d \\nmicroservices in a sequenced order. Data from the workflow is aggregated and returned to the call\", \"er. \\nWhile it still implements direct HTTP calls, the aggregator microservice reduces direct depende\", \"ncies \\namong back-end microservices. \\n\\nRequest/Reply Pattern \\n\\nAnother approach for decoupling synch\", \"ronous HTTP messages is a Request-Reply Pattern, which uses \\nqueuing communication. Communication us\", \"ing a queue is always a one-way channel, with a producer \\nsending the message and consumer receiving\", \" it. With this pattern, both a request queue and response \\nqueue are implemented, shown in Figure 4-\", \"11. \\n\\n70 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fFigure 4-11. Request-reply patte\", \"rn \\n\\nHere, the message producer creates a query-based message that contains a unique correlation ID \", \"and \\nplaces it into a request queue. The consuming service dequeues the messages, processes it and p\", \"laces \\nthe response into the response queue with the same correlation ID. The producer service deque\", \"ues \\nthe message, matches it with the correlation ID and continues processing. We cover queues in de\", \"tail \\nin the next section. \\n\\nCommands \\n\\nAnother type of communication interaction is a command. A mi\", \"croservice may need another \\nmicroservice to perform an action. The Ordering microservice may need t\", \"he Shipping microservice to \\ncreate a shipment for an approved order. In Figure 4-12, one microservi\", \"ce, called a Producer, sends a \\nmessage to another microservice, the Consumer, commanding it to do s\", \"omething. \\n\\n71 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fFigure 4-12. Command inter\", \"action with a queue \\n\\nMost often, the Producer doesn\\u2019t require a response and can fire-and-forget th\", \"e message. If a reply is \\nneeded, the Consumer sends a separate message back to Producer on another \", \"channel. A command \\nmessage is best sent asynchronously with a message queue. supported by a lightwe\", \"ight message \\nbroker. In the previous diagram, note how a queue separates and decouples both service\", \"s. \\n\\nA message queue is an intermediary construct through which a producer and consumer pass a \\nmess\", \"age. Queues implement an asynchronous, point-to-point messaging pattern. The Producer \\nknows where a\", \" command needs to be sent and routes appropriately. The queue guarantees that a \\nmessage is processe\", \"d by exactly one of the consumer instances that are reading from the channel. In \\nthis scenario, eit\", \"her the producer or consumer service can scale out without affecting the other. As \\nwell, technologi\", \"es can be disparate on each side, meaning that we might have a Java microservice \\ncalling a Golang m\", \"icroservice. \\n\\nIn chapter 1, we talked about backing services. Backing services are ancillary resour\", \"ces upon which \\ncloud-native systems depend. Message queues are backing services. The Azure cloud su\", \"pports two \\ntypes of message queues that your cloud-native systems can consume to implement command \", \"\\nmessaging: Azure Storage Queues and Azure Service Bus Queues. \\n\\nAzure Storage Queues \\n\\nAzure storag\", \"e queues offer a simple queueing infrastructure that is fast, affordable, and backed by \\nAzure stora\", \"ge accounts. \\n\\nAzure Storage Queues feature a REST-based queuing mechanism with reliable and persist\", \"ent \\nmessaging. They provide a minimal feature set, but are inexpensive and store millions of messag\", \"es. \\nTheir capacity ranges up to 500 TB. A single message can be up to 64 KB in size. \\n\\nYou can acce\", \"ss messages from anywhere in the world via authenticated calls using HTTP or HTTPS. \\nStorage queues \", \"can scale out to large numbers of concurrent clients to handle traffic spikes. \\n\\n72 \\n\\nCHAPTER 4 | Cl\", \"oud-native communication patterns \\n\\n \\n \\n \\n\\fThat said, there are limitations with the service: \\n\\n\\u2022  M\", \"essage order isn\\u2019t guaranteed. \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nA message can only persist for seven days before it\\u2019s autom\", \"atically removed. \\n\\nSupport for state management, duplicate detection, or transactions isn\\u2019t availab\", \"le. \\n\\nFigure 4-13 shows the hierarchy of an Azure Storage Queue. \\n\\nFigure 4-13. Storage queue hierar\", \"chy \\n\\nIn the previous figure, note how storage queues store their messages in the underlying Azure S\", \"torage \\naccount. \\n\\nFor developers, Microsoft provides several client and server-side libraries for S\", \"torage queue \\nprocessing. Most major platforms are supported including .NET, Java, JavaScript, Ruby,\", \" Python, and \\nGo. Developers should never communicate directly with these libraries. Doing so will t\", \"ightly couple \\nyour microservice code to the Azure Storage Queue service. It\\u2019s a better practice to \", \"insulate the \\nimplementation details of the API. Introduce an intermediation layer, or intermediate \", \"API, that exposes \\ngeneric operations and encapsulates the concrete library. This loose coupling ena\", \"bles you to swap out \\none queuing service for another without having to make changes to the mainline\", \" service code. \\n\\nAzure Storage queues are an economical option to implement command messaging in you\", \"r cloud-\\nnative applications. Especially when a queue size will exceed 80 GB, or a simple feature se\", \"t is \\nacceptable. You only pay for the storage of the messages; there are no fixed hourly charges. \\n\", \"\\nAzure Service Bus Queues \\n\\nFor more complex messaging requirements, consider Azure Service Bus queu\", \"es. \\n\\nSitting atop a robust message infrastructure, Azure Service Bus supports a brokered messaging \", \"model. \\nMessages are reliably stored in a broker (the queue) until received by the consumer. The que\", \"ue \\nguarantees First-In/First-Out (FIFO) message delivery, respecting the order in which messages we\", \"re \\nadded to the queue. \\n\\nThe size of a message can be much larger, up to 256 KB. Messages are persi\", \"sted in the queue for an \\nunlimited period of time. Service Bus supports not only HTTP-based calls, \", \"but also provides full \\n\\n73 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fsupport for t\", \"he AMQP protocol. AMQP is an open-standard across vendors that supports a binary \\nprotocol and highe\", \"r degrees of reliability. \\n\\nService Bus provides a rich set of features, including transaction suppo\", \"rt and a duplicate detection \\nfeature. The queue guarantees \\u201cat most once delivery\\u201d per message. It \", \"automatically discards a \\nmessage that has already been sent. If a producer is in doubt, it can rese\", \"nd the same message, and \\nService Bus guarantees that only one copy will be processed. Duplicate det\", \"ection frees you from \\nhaving to build additional infrastructure plumbing. \\n\\nTwo more enterprise fea\", \"tures are partitioning and sessions. A conventional Service Bus queue is \\nhandled by a single messag\", \"e broker and stored in a single message store. But, Service Bus Partitioning \\nspreads the queue acro\", \"ss multiple message brokers and message stores. The overall throughput is no \\nlonger limited by the \", \"performance of a single message broker or messaging store. A temporary \\noutage of a messaging store \", \"doesn\\u2019t render a partitioned queue unavailable. \\n\\nService Bus Sessions provide a way to group-relate\", \"d messages. Imagine a workflow scenario where \\nmessages must be processed together and the operation\", \" completed at the end. To take advantage, \\nsessions must be explicitly enabled for the queue and eac\", \"h related messaged must contain the same \\nsession ID. \\n\\nHowever, there are some important caveats: S\", \"ervice Bus queues size is limited to 80 GB, which is much \\nsmaller than what\\u2019s available from store \", \"queues. Additionally, Service Bus queues incur a base cost \\nand charge per operation. \\n\\nFigure 4-14 \", \"outlines the high-level architecture of a Service Bus queue. \\n\\nFigure 4-14. Service Bus queue \\n\\nIn t\", \"he previous figure, note the point-to-point relationship. Two instances of the same provider are \\nen\", \"queuing messages into a single Service Bus queue. Each message is consumed by only one of three \\ncon\", \"sumer instances on the right. Next, we discuss how to implement messaging where different \\nconsumers\", \" may all be interested the same message. \\n\\nEvents \\n\\nMessage queuing is an effective way to implement\", \" communication where a producer can \\nasynchronously send a consumer a message. However, what happens\", \" when many different consumers \\n\\n74 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fare i\", \"nterested in the same message? A dedicated message queue for each consumer wouldn\\u2019t scale \\nwell and \", \"would become difficult to manage. \\n\\nTo address this scenario, we move to the third type of message i\", \"nteraction, the event. One \\nmicroservice announces that an action had occurred. Other microservices,\", \" if interested, react to the \\naction, or event. This is also known as the event-driven architectural\", \" style. \\n\\nEventing is a two-step process. For a given state change, a microservice publishes an even\", \"t to a \\nmessage broker, making it available to any other interested microservice. The interested mic\", \"roservice \\nis notified by subscribing to the event in the message broker. You use the Publish/Subscr\", \"ibe pattern \\nto implement event-based communication. \\n\\nFigure 4-15 shows a shopping basket microserv\", \"ice publishing an event with two other microservices \\nsubscribing to it. \\n\\nFigure 4-15. Event-Driven\", \" messaging \\n\\nNote the event bus component that sits in the middle of the communication channel. It\\u2019s\", \" a custom \\nclass that encapsulates the message broker and decouples it from the underlying applicati\", \"on. The \\nordering and inventory microservices independently operate the event with no knowledge of e\", \"ach \\nother, nor the shopping basket microservice. When the registered event is published to the even\", \"t bus, \\nthey act upon it. \\n\\nWith eventing, we move from queuing technology to topics. A topic is sim\", \"ilar to a queue, but supports \\na one-to-many messaging pattern. One microservice publishes a message\", \". Multiple subscribing \\nmicroservices can choose to receive and act upon that message. Figure 4-16 s\", \"hows a topic \\narchitecture. \\n\\n75 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fFigure 4\", \"-16. Topic architecture \\n\\nIn the previous figure, publishers send messages to the topic. At the end,\", \" subscribers receive \\nmessages from subscriptions. In the middle, the topic forwards messages to sub\", \"scriptions based on a \\nset of rules, shown in dark blue boxes. Rules act as a filter that forward sp\", \"ecific messages to a \\nsubscription. Here, a \\u201cGetPrice\\u201d event would be sent to the price and logging \", \"subscriptions as the \\nlogging subscription has chosen to receive all messages. A \\u201cGetInformation\\u201d ev\", \"ent would be sent to \\nthe information and logging subscriptions. \\n\\nThe Azure cloud supports two diff\", \"erent topic services: Azure Service Bus Topics and Azure EventGrid. \\n\\nAzure Service Bus Topics \\n\\nSit\", \"ting on top of the same robust brokered message model of Azure Service Bus queues are Azure \\nService\", \" Bus Topics. A topic can receive messages from multiple independent publishers and send \\nmessages to\", \" up to 2,000 subscribers. Subscriptions can be dynamically added or removed at run time \\nwithout sto\", \"pping the system or recreating the topic. \\n\\nMany advanced features from Azure Service Bus queues are\", \" also available for topics, including \\nDuplicate Detection and Transaction support. By default, Serv\", \"ice Bus topics are handled by a single \\nmessage broker and stored in a single message store. But, Se\", \"rvice Bus Partitioning scales a topic by \\nspreading it across many message brokers and message store\", \"s. \\n\\nScheduled Message Delivery tags a message with a specific time for processing. The message won\\u2019\", \"t \\nappear in the topic before that time. Message Deferral enables you to defer a retrieval of a mess\", \"age \\nto a later time. Both are commonly used in workflow processing scenarios where operations are \\n\", \"processed in a particular order. You can postpone processing of received messages until prior work \\n\", \"has been completed. \\n\\nService Bus topics are a robust and proven technology for enabling publish/sub\", \"scribe communication \\nin your cloud-native systems. \\n\\nAzure Event Grid \\n\\nWhile Azure Service Bus is \", \"a battle-tested messaging broker with a full set of enterprise features, \\nAzure Event Grid is the ne\", \"w kid on the block. \\n\\n76 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fAt first glance,\", \" Event Grid may look like just another topic-based messaging system. However, it\\u2019s \\ndifferent in man\", \"y ways. Focused on event-driven workloads, it enables real-time event processing, \\ndeep Azure integr\", \"ation, and an open-platform - all on serverless infrastructure. It\\u2019s designed for \\ncontemporary clou\", \"d-native and serverless applications \\n\\nAs a centralized eventing backplane, or pipe, Event Grid reac\", \"ts to events inside Azure resources and \\nfrom your own services. \\n\\nEvent notifications are published\", \" to an Event Grid Topic, which, in turn, routes each event to a \\nsubscription. Subscribers map to su\", \"bscriptions and consume the events. Like Service Bus, Event Grid \\nsupports a filtered subscriber mod\", \"el where a subscription sets rule for the events it wishes to receive. \\nEvent Grid provides fast thr\", \"oughput with a guarantee of 10 million events per second enabling near \\nreal-time delivery - far mor\", \"e than what Azure Service Bus can generate. \\n\\nA sweet spot for Event Grid is its deep integration in\", \"to the fabric of Azure infrastructure. An Azure \\nresource, such as Cosmos DB, can publish built-in e\", \"vents directly to other interested Azure resources - \\nwithout the need for custom code. Event Grid c\", \"an publish events from an Azure Subscription, \\nResource Group, or Service, giving developers fine-gr\", \"ained control over the lifecycle of cloud \\nresources. However, Event Grid isn\\u2019t limited to Azure. It\", \"\\u2019s an open platform that can consume custom \\nHTTP events published from applications or third-party \", \"services and route events to external \\nsubscribers. \\n\\nWhen publishing and subscribing to native even\", \"ts from Azure resources, no coding is required. With \\nsimple configuration, you can integrate events\", \" from one Azure resource to another leveraging built-in \\nplumbing for Topics and Subscriptions. Figu\", \"re 4-17 shows the anatomy of Event Grid. \\n\\nFigure 4-17. Event Grid anatomy \\n\\n77 \\n\\nCHAPTER 4 | Cloud-\", \"native communication patterns \\n\\n \\n \\n \\n\\fA major difference between EventGrid and Service Bus is the u\", \"nderlying message exchange pattern. \\n\\nService Bus implements an older style pull model in which the \", \"downstream subscriber actively polls \\nthe topic subscription for new messages. On the upside, this a\", \"pproach gives the subscriber full control \\nof the pace at which it processes messages. It controls w\", \"hen and how many messages to process at \\nany given time. Unread messages remain in the subscription \", \"until processed. A significant \\nshortcoming is the latency between the time the event is generated a\", \"nd the polling operation that \\npulls that message to the subscriber for processing. Also, the overhe\", \"ad of constant polling for the \\nnext event consumes resources and money. \\n\\nEventGrid, however, is di\", \"fferent. It implements a push model in which events are sent to the \\nEventHandlers as received, givi\", \"ng near real-time event delivery. It also reduces cost as the service is \\ntriggered only when it\\u2019s n\", \"eeded to consume an event \\u2013 not continually as with polling. That said, an \\nevent handler must handl\", \"e the incoming load and provide throttling mechanisms to protect itself \\nfrom becoming overwhelmed. \", \"Many Azure services that consume these events, such as Azure \\nFunctions and Logic Apps provide autom\", \"atic autoscaling capabilities to handle increased loads. \\n\\nEvent Grid is a fully managed serverless \", \"cloud service. It dynamically scales based on your traffic and \\ncharges you only for your actual usa\", \"ge, not pre-purchased capacity. The first 100,000 operations per \\nmonth are free \\u2013 operations being \", \"defined as event ingress (incoming event notifications), \\nsubscription delivery attempts, management\", \" calls, and filtering by subject. With 99.99% availability, \\nEventGrid guarantees the delivery of an\", \" event within a 24-hour period, with built-in retry functionality \\nfor unsuccessful delivery. Undeli\", \"vered messages can be moved to a \\u201cdead-letter\\u201d queue for resolution. \\nUnlike Azure Service Bus, Even\", \"t Grid is tuned for fast performance and doesn\\u2019t support features like \\nordered messaging, transacti\", \"ons, and sessions. \\n\\nStreaming messages in the Azure cloud \\n\\nAzure Service Bus and Event Grid provid\", \"e great support for applications that expose single, discrete \\nevents like a new document has been i\", \"nserted into a Cosmos DB. But, what if your cloud-native \\nsystem needs to process a stream of relate\", \"d events? Event streams are more complex. They\\u2019re typically \\ntime-ordered, interrelated, and must be\", \" processed as a group. \\n\\nAzure Event Hub is a data streaming platform and event ingestion service th\", \"at collects, transforms, \\nand stores events. It\\u2019s fine-tuned to capture streaming data, such as cont\", \"inuous event notifications \\nemitted from a telemetry context. The service is highly scalable and can\", \" store and process millions of \\nevents per second. Shown in Figure 4-18, it\\u2019s often a front door for\", \" an event pipeline, decoupling \\ningest stream from event consumption. \\n\\n78 \\n\\nCHAPTER 4 | Cloud-nativ\", \"e communication patterns \\n\\n \\n \\n\\fFigure 4-18. Azure Event Hub \\n\\nEvent Hub supports low latency and co\", \"nfigurable time retention. Unlike queues and topics, Event \\nHubs keep event data after it\\u2019s been rea\", \"d by a consumer. This feature enables other data analytic \\nservices, both internal and external, to \", \"replay the data for further analysis. Events stored in event hub \\nare only deleted upon expiration o\", \"f the retention period, which is one day by default, but \\nconfigurable. \\n\\nEvent Hub supports common \", \"event publishing protocols including HTTPS and AMQP. It also supports \\nKafka 1.0. Existing Kafka app\", \"lications can communicate with Event Hub using the Kafka protocol \\nproviding an alternative to manag\", \"ing large Kafka clusters. Many open-source cloud-native systems \\nembrace Kafka. \\n\\nEvent Hubs impleme\", \"nts message streaming through a partitioned consumer model in which each \\nconsumer only reads a spec\", \"ific subset, or partition, of the message stream. This pattern enables \\ntremendous horizontal scale \", \"for event processing and provides other stream-focused features that are \\nunavailable in queues and \", \"topics. A partition is an ordered sequence of events that is held in an event \\nhub. As newer events \", \"arrive, they\\u2019re added to the end of this sequence. Figure 4-19 shows partitioning \\nin an Event Hub. \", \"\\n\\nFigure 4-19. Event Hub partitioning \\n\\nInstead of reading from the same resource, each consumer gro\", \"up reads across a subset, or partition, \\nof the message stream. \\n\\n79 \\n\\nCHAPTER 4 | Cloud-native comm\", \"unication patterns \\n\\n \\n \\n \\n \\n\\fFor cloud-native applications that must stream large numbers of events\", \", Azure Event Hub can be a \\nrobust and affordable solution. \\n\\ngRPC \\n\\nSo far in this book, we\\u2019ve focu\", \"sed on REST-based communication. We\\u2019ve seen that REST is a flexible \\narchitectural style that define\", \"s CRUD-based operations against entity resources. Clients interact with \\nresources across HTTP with \", \"a request/response communication model. While REST is widely \\nimplemented, a newer communication tec\", \"hnology, gRPC, has gained tremendous momentum across \\nthe cloud-native community. \\n\\nWhat is gRPC? \\n\\n\", \"gRPC is a modern, high-performance framework that evolves the age-old remote procedure call (RPC) \\np\", \"rotocol. At the application level, gRPC streamlines messaging between clients and back-end services.\", \" \\nOriginating from Google, gRPC is open source and part of the Cloud Native Computing Foundation \\n(C\", \"NCF) ecosystem of cloud-native offerings. CNCF considers gRPC an incubating project. Incubating \\nmea\", \"ns end users are using the technology in production applications, and the project has a healthy \\nnum\", \"ber of contributors. \\n\\nA typical gRPC client app will expose a local, in-process function that imple\", \"ments a business \\noperation. Under the covers, that local function invokes another function on a rem\", \"ote machine. What \\nappears to be a local call essentially becomes a transparent out-of-process call \", \"to a remote service. \\nThe RPC plumbing abstracts the point-to-point networking communication, serial\", \"ization, and \\nexecution between computers. \\n\\nIn cloud-native applications, developers often work acr\", \"oss programming languages, frameworks, and \\ntechnologies. This interoperability complicates message \", \"contracts and the plumbing required for \\ncross-platform communication. gRPC provides a \\u201cuniform hori\", \"zontal layer\\u201d that abstracts these \\nconcerns. Developers code in their native platform focused on bu\", \"siness functionality, while gRPC \\nhandles communication plumbing. \\n\\ngRPC offers comprehensive suppor\", \"t across most popular development stacks, including Java, \\nJavaScript, C#, Go, Swift, and NodeJS. \\n\\n\", \"gRPC Benefits \\n\\ngRPC uses HTTP/2 for its transport protocol. While compatible with HTTP 1.1, HTTP/2 \", \"features many \\nadvanced capabilities: \\n\\n\\u2022 \\n\\nA binary framing protocol for data transport - unlike HT\", \"TP 1.1, which is text based. \\n\\n\\u2022  Multiplexing support for sending multiple parallel requests over t\", \"he same connection - HTTP \\n\\n1.1 limits processing to one request/response message at a time. \\n\\n\\u2022 \\n\\n\\u2022\", \" \\n\\n\\u2022 \\n\\n80 \\n\\nBidirectional full-duplex communication for sending both client requests and server resp\", \"onses \\nsimultaneously. \\n\\nBuilt-in streaming enabling requests and responses to asynchronously stream\", \" large data sets. \\n\\nHeader compression that reduces network usage. \\n\\nCHAPTER 4 | Cloud-native commun\", \"ication patterns \\n\\n \\n \\n\\fgRPC is lightweight and highly performant. It can be up to 8x faster than JS\", \"ON serialization with \\nmessages 60-80% smaller. In Microsoft Windows Communication Foundation (WCF) \", \"parlance, gRPC \\nperformance exceeds the speed and efficiency of the highly optimized NetTCP bindings\", \". Unlike \\nNetTCP, which favors the Microsoft stack, gRPC is cross-platform. \\n\\nProtocol Buffers \\n\\ngRP\", \"C embraces an open-source technology called Protocol Buffers. They provide a highly efficient \\nand p\", \"latform-neutral serialization format for serializing structured messages that services send to \\neach\", \" other. Using a cross-platform Interface Definition Language (IDL), developers define a service \\ncon\", \"tract for each microservice. The contract, implemented as a text-based .proto file, describes the \\nm\", \"ethods, inputs, and outputs for each service. The same contract file can be used for gRPC clients an\", \"d \\nservices built on different development platforms. \\n\\nUsing the proto file, the Protobuf compiler,\", \" protoc, generates both client and service code for your \\ntarget platform. The code includes the fol\", \"lowing components: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nStrongly typed objects, shared by the client and service, that repr\", \"esent the service operations \\nand data elements for a message. \\n\\nA strongly typed base class with th\", \"e required network plumbing that the remote gRPC service \\ncan inherit and extend. \\n\\nA client stub th\", \"at contains the required plumbing to invoke the remote gRPC service. \\n\\nAt run time, each message is \", \"serialized as a standard Protobuf representation and exchanged between \\nthe client and remote servic\", \"e. Unlike JSON or XML, Protobuf messages are serialized as compiled \\nbinary bytes. \\n\\nThe book, gRPC \", \"for WCF Developers, available from the Microsoft Architecture site, provides in-depth \\ncoverage of g\", \"RPC and Protocol Buffers. \\n\\ngRPC support in .NET \\n\\ngRPC is integrated into .NET Core 3.0 SDK and lat\", \"er. The following tools support it: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nVisual Studio 2022 with the ASP.NET and web develo\", \"pment workload installed \\n\\nVisual Studio Code \\n\\nThe dotnet CLI \\n\\nThe SDK includes tooling for endpoi\", \"nt routing, built-in IoC, and logging. The open-source Kestrel web \\nserver supports HTTP/2 connectio\", \"ns. Figure 4-20 shows a Visual Studio 2022 template that scaffolds a \\nskeleton project for a gRPC se\", \"rvice. Note how .NET fully supports Windows, Linux, and macOS. \\n\\n81 \\n\\nCHAPTER 4 | Cloud-native commu\", \"nication patterns \\n\\n \\n \\n\\fFigure 4-20. gRPC support in Visual Studio 2022 \\n\\nFigure 4-21 shows the ske\", \"leton gRPC service generated from the built-in scaffolding included in \\nVisual Studio 2022. \\n\\nFigure\", \" 4-21. gRPC project in Visual Studio 2022 \\n\\nIn the previous figure, note the proto description file \", \"and service code. As you\\u2019ll see shortly, Visual \\nStudio generates additional configuration in both t\", \"he Startup class and underlying project file. \\n\\ngRPC usage \\n\\nFavor gRPC for the following scenarios:\", \" \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n82 \\n\\nSynchronous backend microservice-to-microservice communication where an imme\", \"diate \\nresponse is required to continue processing. \\n\\nPolyglot environments that need to support mix\", \"ed programming platforms. \\n\\nLow latency and high throughput communication where performance is criti\", \"cal. \\n\\nPoint-to-point real-time communication - gRPC can push messages in real time without \\npolling\", \" and has excellent support for bi-directional streaming. \\n\\nCHAPTER 4 | Cloud-native communication pa\", \"tterns \\n\\n \\n \\n \\n \\n\\f\\u2022 \\n\\nNetwork constrained environments \\u2013 binary gRPC messages are always smaller tha\", \"n an \\nequivalent text-based JSON message. \\n\\nAt the time, of this writing, gRPC is primarily used wit\", \"h backend services. Modern browsers can\\u2019t \\nprovide the level of HTTP/2 control required to support a\", \" front-end gRPC client. That said, there\\u2019s \\nsupport for gRPC-Web with .NET that enables gRPC communi\", \"cation from browser-based apps built \\nwith JavaScript or Blazor WebAssembly technologies. gRPC-Web e\", \"nables an ASP.NET Core gRPC app \\nto support gRPC features in browser apps: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nStrongly ty\", \"ped, code-generated clients \\n\\nCompact Protobuf messages \\n\\nServer streaming \\n\\ngRPC implementation \\n\\nT\", \"he microservice reference architecture, eShop on Containers, from Microsoft, shows how to \\nimplement\", \" gRPC services in .NET applications. Figure 4-22 presents the back-end architecture. \\n\\nFigure 4-22. \", \"Backend architecture for eShop on Containers \\n\\nIn the previous figure, note how eShop embraces the B\", \"ackend for Frontends pattern (BFF) by \\nexposing multiple API gateways. We discussed the BFF pattern \", \"earlier in this chapter. Pay close \\n\\n83 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fa\", \"ttention to the Aggregator microservice (in gray) that sits between the Web-Shopping API Gateway \\nan\", \"d backend Shopping microservices. The Aggregator receives a single request from a client, \\ndispatche\", \"s it to various microservices, aggregates the results, and sends them back to the requesting \\nclient\", \". Such operations typically require synchronous communication as to produce an immediate \\nresponse. \", \"In eShop, backend calls from the Aggregator are performed using gRPC as shown in Figure \\n4-23. \\n\\nFig\", \"ure 4-23. gRPC in eShop on Containers \\n\\ngRPC communication requires both client and server component\", \"s. In the previous figure, note how \\nthe Shopping Aggregator implements a gRPC client. The client ma\", \"kes synchronous gRPC calls (in red) \\nto backend microservices, each of which implement a gRPC server\", \". Both the client and server take \\nadvantage of the built-in gRPC plumbing from the .NET SDK. Client\", \"-side stubs provide the plumbing \\nto invoke remote gRPC calls. Server-side components provide gRPC p\", \"lumbing that custom service \\nclasses can inherit and consume. \\n\\nMicroservices that expose both a RES\", \"Tful API and gRPC communication require multiple endpoints to \\nmanage traffic. You would open an end\", \"point that listens for HTTP traffic for the RESTful calls and \\nanother for gRPC calls. The gRPC endp\", \"oint must be configured for the HTTP/2 protocol that is \\nrequired for gRPC communication. \\n\\nWhile we\", \" strive to decouple microservices with asynchronous communication patterns, some \\noperations require\", \" direct calls. gRPC should be the primary choice for direct synchronous \\ncommunication between micro\", \"services. Its high-performance communication protocol, based on \\nHTTP/2 and protocol buffers, make i\", \"t a perfect choice. \\n\\n84 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fLooking ahead \\n\\n\", \"Looking ahead, gRPC will continue to gain traction for cloud-native systems. The performance \\nbenefi\", \"ts and ease of development are compelling. However, REST will likely be around for a long time. \\nIt \", \"excels for publicly exposed APIs and for backward compatibility reasons. \\n\\nService Mesh communicatio\", \"n infrastructure \\n\\nThroughout this chapter, we\\u2019ve explored the challenges of microservice communicat\", \"ion. We said that \\ndevelopment teams need to be sensitive to how back-end services communicate with \", \"each other. \\nIdeally, the less inter-service communication, the better. However, avoidance isn\\u2019t alw\", \"ays possible as \\nback-end services often rely on one another to complete operations. \\n\\nWe explored d\", \"ifferent approaches for implementing synchronous HTTP communication and \\nasynchronous messaging. In \", \"each of the cases, the developer is burdened with implementing \\ncommunication code. Communication co\", \"de is complex and time intensive. Incorrect decisions can \\nlead to significant performance issues. \\n\", \"\\nA more modern approach to microservice communication centers around a new and rapidly evolving \\ntec\", \"hnology entitled Service Mesh. A service mesh is a configurable infrastructure layer with built-in \\n\", \"capabilities to handle service-to-service communication, resiliency, and many cross-cutting concerns\", \". \\nIt moves the responsibility for these concerns out of the microservices and into service mesh lay\", \"er. \\nCommunication is abstracted away from your microservices. \\n\\nA key component of a service mesh i\", \"s a proxy. In a cloud-native application, an instance of a proxy is \\ntypically colocated with each m\", \"icroservice. While they execute in separate processes, the two are \\nclosely linked and share the sam\", \"e lifecycle. This pattern, known as the Sidecar pattern, and is shown in \\nFigure 4-24. \\n\\nFigure 4-24\", \". Service mesh with a side car \\n\\n85 \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n \\n\\fNote \", \"in the previous figure how messages are intercepted by a proxy that runs alongside each \\nmicroservic\", \"e. Each proxy can be configured with traffic rules specific to the microservice. It \\nunderstands mes\", \"sages and can route them across your services and the outside world. \\n\\nAlong with managing service-t\", \"o-service communication, the Service Mesh provides support for \\nservice discovery and load balancing\", \". \\n\\nOnce configured, a service mesh is highly functional. The mesh retrieves a corresponding pool of\", \" \\ninstances from a service discovery endpoint. It sends a request to a specific service instance, re\", \"cording \\nthe latency and response type of the result. It chooses the instance most likely to return \", \"a fast \\nresponse based on different factors, including the observed latency for recent requests. \\n\\nA\", \" service mesh manages traffic, communication, and networking concerns at the application level. It \\n\", \"understands messages and requests. A service mesh typically integrates with a container orchestrator\", \". \\nKubernetes supports an extensible architecture in which a service mesh can be added. \\n\\nIn chapter\", \" 6, we deep-dive into Service Mesh technologies including a discussion on its architecture \\nand avai\", \"lable open-source implementations. \\n\\nSummary \\n\\nIn this chapter, we discussed cloud-native communicat\", \"ion patterns. We started by examining how \\nfront-end clients communicate with back-end microservices\", \". Along the way, we talked about API \\nGateway platforms and real-time communication. We then looked \", \"at how microservices communicate \\nwith other back-end services. We looked at both synchronous HTTP c\", \"ommunication and \\nasynchronous messaging across services. We covered gRPC, an upcoming technology in\", \" the cloud-\\nnative world. Finally, we introduced a new and rapidly evolving technology entitled Serv\", \"ice Mesh that \\ncan streamline microservice communication. \\n\\nSpecial emphasis was on managed Azure se\", \"rvices that can help implement communication in cloud-\\nnative systems: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\", \"\\nAzure Application Gateway \\n\\nAzure API Management \\n\\nAzure SignalR Service \\n\\nAzure Storage Queues \\n\\nA\", \"zure Service Bus \\n\\nAzure Event Grid \\n\\nAzure Event Hub \\n\\nWe next move to distributed data in cloud-na\", \"tive systems and the benefits and challenges that it \\npresents. \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n86 \\n\\n.NET\", \" Microservices: Architecture for Containerized .NET applications \\n\\nDesigning Interservice Communicat\", \"ion for Microservices \\n\\nAzure SignalR Service, a fully managed service to add real-time functionalit\", \"y \\n\\nCHAPTER 4 | Cloud-native communication patterns \\n\\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAzure API Gateway Ing\", \"ress Controller \\n\\ngRPC Documentation \\n\\ngRPC for WCF Developers \\n\\nComparing gRPC Services with HTTP A\", \"PIs \\n\\nBuilding gRPC Services with .NET video \\n\\n87 \\n\\nCHAPTER 4 | Cloud-native communication patterns \", \"\\n\\n \\n \\n\\fCHAPTER  5 \\n\\nCloud-native data patterns \\n\\nAs we\\u2019ve seen throughout this book, a cloud-native \", \"approach changes the way you design, deploy, \\nand manage applications. It also changes the way you m\", \"anage and store data. \\n\\nFigure 5-1 contrasts the differences. \\n\\nFigure 5-1. Data management in cloud\", \"-native applications \\n\\nExperienced developers will easily recognize the architecture on the left-sid\", \"e of figure 5-1. In this \\nmonolithic application, business service components collocate together in \", \"a shared services tier, \\nsharing data from a single relational database. \\n\\nIn many ways, a single da\", \"tabase keeps data management simple. Querying data across multiple tables \\nis straightforward. Chang\", \"es to data update together or they all rollback. ACID transactions guarantee \\nstrong and immediate c\", \"onsistency. \\n\\nDesigning for cloud-native, we take a different approach. On the right-side of Figure \", \"5-1, note how \\nbusiness functionality segregates into small, independent microservices. Each microse\", \"rvice \\nencapsulates a specific business capability and its own data. The monolithic database decompo\", \"ses \\n\\n88 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\finto a distributed data model with many s\", \"maller databases, each aligning with a microservice. When \\nthe smoke clears, we emerge with a design\", \" that exposes a database per microservice. \\n\\nDatabase-per-microservice, why? \\n\\nThis database per mic\", \"roservice provides many benefits, especially for systems that must evolve rapidly \\nand support massi\", \"ve scale. With this model\\u2026 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nDomain data is encapsulated within the service \\n\\nData s\", \"chema can evolve without directly impacting other services \\n\\nEach data store can independently scale\", \" \\n\\nA data store failure in one service won\\u2019t directly impact other services \\n\\nSegregating data also \", \"enables each microservice to implement the data store type that is best \\noptimized for its workload,\", \" storage needs, and read/write patterns. Choices include relational, \\ndocument, key-value, and even \", \"graph-based data stores. \\n\\nFigure 5-2 presents the principle of polyglot persistence in a cloud-nati\", \"ve system. \\n\\nFigure 5-2. Polyglot data persistence \\n\\nNote in the previous figure how each microservi\", \"ce supports a different type of data store. \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n89 \\n\\nThe product catalog microservice cons\", \"umes a relational database to accommodate the rich \\nrelational structure of its underlying data. \\n\\nT\", \"he shopping cart microservice consumes a distributed cache that supports its simple, key-\\nvalue data\", \" store. \\n\\nThe ordering microservice consumes both a NoSql document database for write operations \\nal\", \"ong with a highly denormalized key/value store to accommodate high-volumes of read \\noperations. \\n\\nCH\", \"APTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fWhile relational databases remain relevant for microse\", \"rvices with complex data, NoSQL databases \\nhave gained considerable popularity. They provide massive\", \" scale and high availability. Their \\nschemaless nature allows developers to move away from an archit\", \"ecture of typed data classes and \\nORMs that make change expensive and time-consuming. We cover NoSQL\", \" databases later in this \\nchapter. \\n\\nWhile encapsulating data into separate microservices can increa\", \"se agility, performance, and scalability, \\nit also presents many challenges. In the next section, we\", \" discuss these challenges along with patterns \\nand practices to help overcome them. \\n\\nCross-service \", \"queries \\n\\nWhile microservices are independent and focus on specific functional capabilities, like in\", \"ventory, \\nshipping, or ordering, they frequently require integration with other microservices. Often\", \" the \\nintegration involves one microservice querying another for data. Figure 5-3 shows the scenario\", \". \\n\\nFigure 5-3. Querying across microservices \\n\\nIn the preceding figure, we see a shopping basket mi\", \"croservice that adds an item to a user\\u2019s shopping \\nbasket. While the data store for this microservic\", \"e contains basket and line item data, it doesn\\u2019t \\nmaintain product or pricing data. Instead, those d\", \"ata items are owned by the catalog and pricing \\nmicroservices. This aspect presents a problem. How c\", \"an the shopping basket microservice add a \\nproduct to the user\\u2019s shopping basket when it doesn\\u2019t hav\", \"e product nor pricing data in its database? \\n\\nOne option discussed in Chapter 4 is a direct HTTP cal\", \"l from the shopping basket to the catalog and \\npricing microservices. However, in chapter 4, we said\", \" synchronous HTTP calls couple microservices \\ntogether, reducing their autonomy and diminishing thei\", \"r architectural benefits. \\n\\nWe could also implement a request-reply pattern with separate inbound an\", \"d outbound queues for \\neach service. However, this pattern is complicated and requires plumbing to c\", \"orrelate request and \\nresponse messages. While it does decouple the backend microservice calls, the \", \"calling service must \\nstill synchronously wait for the call to complete. Network congestion, transie\", \"nt faults, or an \\noverloaded microservice and can result in long-running and even failed operations.\", \" \\n\\n90 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fInstead, a widely accepted pattern for remov\", \"ing cross-service dependencies is the Materialized View \\nPattern, shown in Figure 5-4. \\n\\nFigure 5-4.\", \" Materialized View Pattern \\n\\nWith this pattern, you place a local data table (known as a read model)\", \" in the shopping basket service. \\nThis table contains a denormalized copy of the data needed from th\", \"e product and pricing \\nmicroservices. Copying the data directly into the shopping basket microservic\", \"e eliminates the need for \\nexpensive cross-service calls. With the data local to the service, you im\", \"prove the service\\u2019s response \\ntime and reliability. Additionally, having its own copy of the data ma\", \"kes the shopping basket service \\nmore resilient. If the catalog service should become unavailable, i\", \"t wouldn\\u2019t directly impact the \\nshopping basket service. The shopping basket can continue operating \", \"with the data from its own \\nstore. \\n\\nThe catch with this approach is that you now have duplicate dat\", \"a in your system. However, \\nstrategically duplicating data in cloud-native systems is an established\", \" practice and not considered an \\nanti-pattern, or bad practice. Keep in mind that one and only one s\", \"ervice can own a data set and have \\nauthority over it. You\\u2019ll need to synchronize the read models wh\", \"en the system of record is updated. \\nSynchronization is typically implemented via asynchronous messa\", \"ging with a publish/subscribe \\npattern, as shown in Figure 5.4. \\n\\nDistributed transactions \\n\\nWhile q\", \"uerying data across microservices is difficult, implementing a transaction across several \\nmicroserv\", \"ices is even more complex. The inherent challenge of maintaining data consistency across \\nindependen\", \"t data sources in different microservices can\\u2019t be understated. The lack of distributed \\ntransaction\", \"s in cloud-native applications means that you must manage distributed transactions \\nprogrammatically\", \". You move from a world of immediate consistency to that of eventual consistency. \\n\\nFigure 5-5 shows\", \" the problem. \\n\\n91 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fFigure 5-5. Implementing a tran\", \"saction across microservices \\n\\nIn the preceding figure, five independent microservices participate i\", \"n a distributed transaction that \\ncreates an order. Each microservice maintains its own data store a\", \"nd implements a local transaction \\nfor its store. To create the order, the local transaction for eac\", \"h individual microservice must succeed, \\nor all must abort and roll back the operation. While built-\", \"in transactional support is available inside \\neach of the microservices, there\\u2019s no support for a di\", \"stributed transaction that would span across all \\nfive services to keep data consistent. \\n\\nInstead, \", \"you must construct this distributed transaction programmatically. \\n\\nA popular pattern for adding dis\", \"tributed transactional support is the Saga pattern. It\\u2019s implemented \\nby grouping local transactions\", \" together programmatically and sequentially invoking each one. If any \\nof the local transactions fai\", \"l, the Saga aborts the operation and invokes a set of compensating \\ntransactions. The compensating t\", \"ransactions undo the changes made by the preceding local \\ntransactions and restore data consistency.\", \" Figure 5-6 shows a failed transaction with the Saga pattern. \\n\\nFigure 5-6. Rolling back a transacti\", \"on \\n\\n92 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n \\n\\fIn the previous figure, the Update Inven\", \"tory operation has failed in the Inventory microservice. The \\nSaga invokes a set of compensating tra\", \"nsactions (in red) to adjust the inventory counts, cancel the \\npayment and the order, and return the\", \" data for each microservice back to a consistent state. \\n\\nSaga patterns are typically choreographed \", \"as a series of related events, or orchestrated as a set of \\nrelated commands. In Chapter 4, we discu\", \"ssed the service aggregator pattern that would be the \\nfoundation for an orchestrated saga implement\", \"ation. We also discussed eventing along with Azure \\nService Bus and Azure Event Grid topics that wou\", \"ld be a foundation for a choreographed saga \\nimplementation. \\n\\nHigh volume data \\n\\nLarge cloud-native\", \" applications often support high-volume data requirements. In these scenarios, \\ntraditional data sto\", \"rage techniques can cause bottlenecks. For complex systems that deploy on a large \\nscale, both Comma\", \"nd and Query Responsibility Segregation (CQRS) and Event Sourcing may improve \\napplication performan\", \"ce. \\n\\nCQRS \\n\\nCQRS, is an architectural pattern that can help maximize performance, scalability, and \", \"security. The \\npattern separates operations that read data from those operations that write data. \\n\\n\", \"For normal scenarios, the same entity model and data repository object are used for both read and \\nw\", \"rite operations. \\n\\nHowever, a high volume data scenario can benefit from separate models and data ta\", \"bles for reads \\nand writes. To improve performance, the read operation could query against a highly \", \"denormalized \\nrepresentation of the data to avoid expensive repetitive table joins and table locks. \", \"The write \\noperation, known as a command, would update against a fully normalized representation of \", \"the data \\nthat would guarantee consistency. You then need to implement a mechanism to keep both \\nrep\", \"resentations in sync. Typically, whenever the write table is modified, it publishes an event that \\nr\", \"eplicates the modification to the read table. \\n\\nFigure 5-7 shows an implementation of the CQRS patte\", \"rn. \\n\\nFigure 5-7. CQRS implementation \\n\\n93 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fIn the \", \"previous figure, separate command and query models are implemented. Each data write \\noperation is sa\", \"ved to the write store and then propagated to the read store. Pay close attention to \\nhow the data p\", \"ropagation process operates on the principle of eventual consistency. The read model \\neventually syn\", \"chronizes with the write model, but there may be some lag in the process. We discuss \\neventual consi\", \"stency in the next section. \\n\\nThis separation enables reads and writes to scale independently. Read \", \"operations use a schema \\noptimized for queries, while the writes use a schema optimized for updates.\", \" Read queries go against \\ndenormalized data, while complex business logic can be applied to the writ\", \"e model. As well, you \\nmight impose tighter security on write operations than those exposing reads. \", \"\\n\\nImplementing CQRS can improve application performance for cloud-native services. However, it does \", \"\\nresult in a more complex design. Apply this principle carefully and strategically to those sections\", \" of \\nyour cloud-native application that will benefit from it. For more on CQRS, see the Microsoft bo\", \"ok .NET \\nMicroservices: Architecture for Containerized .NET Applications. \\n\\nEvent sourcing \\n\\nAnother\", \" approach to optimizing high volume data scenarios involves Event Sourcing. \\n\\nA system typically sto\", \"res the current state of a data entity. If a user changes their phone number, for \\nexample, the cust\", \"omer record is updated with the new number. We always know the current state of a \\ndata entity, but \", \"each update overwrites the previous state. \\n\\nIn most cases, this model works fine. In high volume sy\", \"stems, however, overhead from transactional \\nlocking and frequent update operations can impact datab\", \"ase performance, responsiveness, and limit \\nscalability. \\n\\nEvent Sourcing takes a different approach\", \" to capturing data. Each operation that affects data is \\npersisted to an event store. Instead of upd\", \"ating the state of a data record, we append each change to \\na sequential list of past events - simil\", \"ar to an accountant\\u2019s ledger. The Event Store becomes the \\nsystem of record for the data. It\\u2019s used \", \"to propagate various materialized views within the bounded \\ncontext of a microservice. Figure 5.8 sh\", \"ows the pattern. \\n\\n94 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fFigure 5-8. Event Sourcing \\n\\nI\", \"n the previous figure, note how each entry (in blue) for a user\\u2019s shopping cart is appended to an \\nu\", \"nderlying event store. In the adjoining materialized view, the system projects the current state by \", \"\\nreplaying all the events associated with each shopping cart. This view, or read model, is then expo\", \"sed \\nback to the UI. Events can also be integrated with external systems and applications or queried\", \" to \\ndetermine the current state of an entity. With this approach, you maintain history. You know no\", \"t only \\nthe current state of an entity, but also how you reached this state. \\n\\nMechanically speaking\", \", event sourcing simplifies the write model. There are no updates or deletes. \\nAppending each data e\", \"ntry as an immutable event minimizes contention, locking, and concurrency \\nconflicts associated with\", \" relational databases. Building read models with the materialized view pattern \\nenables you to decou\", \"ple the view from the write model and choose the best data store to optimize \\nthe needs of your appl\", \"ication UI. \\n\\nFor this pattern, consider a data store that directly supports event sourcing. Azure C\", \"osmos DB, \\nMongoDB, Cassandra, CouchDB, and RavenDB are good candidates. \\n\\nAs with all patterns and \", \"technologies, implement strategically and when needed. While event sourcing \\ncan provide increased p\", \"erformance and scalability, it comes at the expense of complexity and a \\nlearning curve. \\n\\n95 \\n\\nCHAP\", \"TER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fRelational vs. NoSQL data \\n\\nRelational and NoSQL are two\", \" types of database systems commonly implemented in cloud-native \\napps. They\\u2019re built differently, st\", \"ore data differently, and accessed differently. In this section, we\\u2019ll look \\nat both. Later in this \", \"chapter, we\\u2019ll look at an emerging database technology called NewSQL. \\n\\nRelational databases have be\", \"en a prevalent technology for decades. They\\u2019re mature, proven, and \\nwidely implemented. Competing da\", \"tabase products, tooling, and expertise abound. Relational \\ndatabases provide a store of related dat\", \"a tables. These tables have a fixed schema, use SQL \\n(Structured Query Language) to manage data, and\", \" support ACID guarantees. \\n\\nNo-SQL databases refer to high-performance, non-relational data stores. \", \"They excel in their ease-of-\\nuse, scalability, resilience, and availability characteristics. Instead\", \" of joining tables of normalized data, \\nNoSQL stores unstructured or semi-structured data, often in \", \"key-value pairs or JSON documents. No-\\nSQL databases typically don\\u2019t provide ACID guarantees beyond \", \"the scope of a single database \\npartition. High volume services that require sub second response tim\", \"e favor NoSQL datastores. \\n\\nThe impact of NoSQL technologies for distributed cloud-native systems ca\", \"n\\u2019t be overstated. The \\nproliferation of new data technologies in this space has disrupted solutions\", \" that once exclusively \\nrelied on relational databases. \\n\\nNoSQL databases include several different \", \"models for accessing and managing data, each suited to \\nspecific use cases. Figure 5-9 presents four\", \" common models. \\n\\nFigure 5-9: Data models for NoSQL databases \\n\\nModel \\n\\nCharacteristics \\n\\nDocument S\", \"tore \\n\\nKey Value Store \\n\\nWide-Column Store \\n\\nGraph Store \\n\\nData and metadata are stored hierarchical\", \"ly in \\nJSON-based documents inside the database. \\n\\nThe simplest of the NoSQL databases, data is \\nrep\", \"resented as a collection of key-value pairs. \\n\\nRelated data is stored as a set of nested-\\nkey/value \", \"pairs within a single column. \\n\\nData is stored in a graph structure as node, \\nedge, and data propert\", \"ies. \\n\\n96 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fThe CAP theorem \\n\\nAs a way to understand\", \" the differences between these types of databases, consider the CAP theorem, \\na set of principles ap\", \"plied to distributed systems that store state. Figure 5-10 shows the three \\nproperties of the CAP th\", \"eorem. \\n\\nFigure 5-10. The CAP theorem \\n\\nThe theorem states that distributed data systems will offer \", \"a trade-off between consistency, \\navailability, and partition tolerance. And, that any database can \", \"only guarantee two of the three \\nproperties: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nConsistency. Every node in the cluster re\", \"sponds with the most recent data, even if the system \\nmust block the request until all replicas upda\", \"te. If you query a \\u201cconsistent system\\u201d for an item \\nthat is currently updating, you\\u2019ll wait for that\", \" response until all replicas successfully update. \\nHowever, you\\u2019ll receive the most current data. \\n\\n\", \"Availability. Every node returns an immediate response, even if that response isn\\u2019t the most \\nrecent\", \" data. If you query an \\u201cavailable system\\u201d for an item that is updating, you\\u2019ll get the best \\npossibl\", \"e answer the service can provide at that moment. \\n\\nPartition Tolerance. Guarantees the system contin\", \"ues to operate even if a replicated data \\nnode fails or loses connectivity with other replicated dat\", \"a nodes. \\n\\nCAP theorem explains the tradeoffs associated with managing consistency and availability \", \"during a \\nnetwork partition; however tradeoffs with respect to consistency and performance also exis\", \"t with the \\nabsence of a network partition. CAP theorem is often further extended to PACELC to expla\", \"in the \\ntradeoffs more comprehensively. \\n\\nRelational databases typically provide consistency and ava\", \"ilability, but not partition tolerance. They\\u2019re \\ntypically provisioned to a single server and scale \", \"vertically by adding more resources to the machine. \\n\\n97 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n\", \" \\n \\n \\n\\fMany relational database systems support built-in replication features where copies of the pr\", \"imary \\ndatabase can be made to other secondary server instances. Write operations are made to the pr\", \"imary \\ninstance and replicated to each of the secondaries. Upon a failure, the primary instance can \", \"fail over \\nto a secondary to provide high availability. Secondaries can also be used to distribute r\", \"ead operations. \\nWhile writes operations always go against the primary replica, read operations can \", \"be routed to any of \\nthe secondaries to reduce system load. \\n\\nData can also be horizontally partitio\", \"ned across multiple nodes, such as with sharding. But, sharding \\ndramatically increases operational \", \"overhead by spitting data across many pieces that cannot easily \\ncommunicate. It can be costly and t\", \"ime consuming to manage. Relational features that include table \\njoins, transactions, and referentia\", \"l integrity require steep performance penalties in sharded \\ndeployments. \\n\\nReplication consistency a\", \"nd recovery point objectives can be tuned by configuring whether replication \\noccurs synchronously o\", \"r asynchronously. If data replicas were to lose network connectivity in a \\u201chighly \\nconsistent\\u201d or sy\", \"nchronous relational database cluster, you wouldn\\u2019t be able to write to the database. \\nThe system wo\", \"uld reject the write operation as it can\\u2019t replicate that change to the other data replica. \\nEvery d\", \"ata replica has to update before the transaction can complete. \\n\\nNoSQL databases typically support h\", \"igh availability and partition tolerance. They scale out \\nhorizontally, often across commodity serve\", \"rs. This approach provides tremendous availability, both \\nwithin and across geographical regions at \", \"a reduced cost. You partition and replicate data across \\nthese machines, or nodes, providing redunda\", \"ncy and fault tolerance. Consistency is typically tuned \\nthrough consensus protocols or quorum mecha\", \"nisms. They provide more control when navigating \\ntradeoffs between tuning synchronous versus asynch\", \"ronous replication in relational systems. \\n\\nIf data replicas were to lose connectivity in a \\u201chighly \", \"available\\u201d NoSQL database cluster, you could still \\ncomplete a write operation to the database. The \", \"database cluster would allow the write operation and \\nupdate each data replica as it becomes availab\", \"le. NoSQL databases that support multiple writable \\nreplicas can further strengthen high availabilit\", \"y by avoiding the need for failover when optimizing \\nrecovery time objective. \\n\\nModern NoSQL databas\", \"es typically implement partitioning capabilities as a feature of their system \\ndesign. Partition man\", \"agement is often built-in to the database, and routing is achieved through \\nplacement hints - often \", \"called partition keys. A flexible data models enables the NoSQL databases to \\nlower the burden of sc\", \"hema management and improve availability when deploying application \\nupdates that require data model\", \" changes. \\n\\nHigh availability and massive scalability are often more critical to the business than r\", \"elational table \\njoins and referential integrity. Developers can implement techniques and patterns s\", \"uch as Sagas, \\nCQRS, and asynchronous messaging to embrace eventual consistency. \\n\\nNowadays, care mu\", \"st be taken when considering the CAP theorem constraints. A new type of \\ndatabase, called NewSQL, ha\", \"s emerged which extends the relational database engine to support both \\nhorizontal scalability and t\", \"he scalable performance of NoSQL systems. \\n\\n98 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fConsi\", \"derations for relational vs. NoSQL systems \\n\\nBased upon specific data requirements, a cloud-native-b\", \"ased microservice can implement a relational, \\nNoSQL datastore or both. \\n\\nConsider a NoSQL datastore\", \" when: \\n\\nConsider a relational database when: \\n\\nYou have high volume workloads that require \\npredict\", \"able latency at large scale (for example, \\nlatency measured in milliseconds while \\nperforming millio\", \"ns of transactions per second) \\n\\nYour data is dynamic and frequently changes \\n\\nYour workload volume \", \"generally fits within \\nthousands of transactions per second \\n\\nYour data is highly structured and req\", \"uires \\nreferential integrity \\n\\nRelationships can be de-normalized data \\nmodels \\n\\nRelationships are e\", \"xpressed through table joins \\non normalized data models \\n\\nData retrieval is simple and expressed wit\", \"hout \\ntable joins \\n\\nData is typically replicated across geographies \\nand requires finer control over\", \" consistency, \\navailability, and performance \\n\\nYou work with complex queries and reports \\n\\nData is t\", \"ypically centralized, or can be replicated \\nregions asynchronously \\n\\nYour application will be deploy\", \"ed to commodity \\nhardware, such as with public clouds \\n\\nYour application will be deployed to large, \", \"high-\\nend hardware \\n\\nIn the next sections, we\\u2019ll explore the options available in the Azure cloud fo\", \"r storing and managing \\nyour cloud-native data. \\n\\nDatabase as a Service \\n\\nTo start, you could provis\", \"ion an Azure virtual machine and install your database of choice for each \\nservice. While you\\u2019d have\", \" full control over the environment, you\\u2019d forgo many built-in features of the \\ncloud platform. You\\u2019d\", \" also be responsible for managing the virtual machine and database for each \\nservice. This approach \", \"could quickly become time-consuming and expensive. \\n\\nInstead, cloud-native applications favor data s\", \"ervices exposed as a Database as a Service (DBaaS). \\nFully managed by a cloud vendor, these services\", \" provide built-in security, scalability, and monitoring. \\nInstead of owning the service, you simply \", \"consume it as a backing service. The provider operates the \\nresource at scale and bears the responsi\", \"bility for performance and maintenance. \\n\\nThey can be configured across cloud availability zones and\", \" regions to achieve high availability. They \\nall support just-in-time capacity and a pay-as-you-go m\", \"odel. Azure features different kinds of \\nmanaged data service options, each with specific benefits. \", \"\\n\\nWe\\u2019ll first look at relational DBaaS services available in Azure. You\\u2019ll see that Microsoft\\u2019s flag\", \"ship SQL \\nServer database is available along with several open-source options. Then, we\\u2019ll talk abou\", \"t the NoSQL \\ndata services in Azure. \\n\\n99 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fAzure rela\", \"tional databases \\n\\nFor cloud-native microservices that require relational data, Azure offers four ma\", \"naged relational \\ndatabases as a service (DBaaS) offerings, shown in Figure 5-11. \\n\\nFigure 5-11. Man\", \"aged relational databases available in Azure \\n\\nIn the previous figure, note how each sits upon a com\", \"mon DBaaS infrastructure which features key \\ncapabilities at no additional cost. \\n\\nThese features ar\", \"e especially important to organizations who provision large numbers of databases, \\nbut have limited \", \"resources to administer them. You can provision an Azure database in minutes by \\nselecting the amoun\", \"t of processing cores, memory, and underlying storage. You can scale the \\ndatabase on-the-fly and dy\", \"namically adjust resources with little to no downtime. \\n\\nAzure SQL Database \\n\\nDevelopment teams with\", \" expertise in Microsoft SQL Server should consider Azure SQL Database. It\\u2019s a \\nfully managed relatio\", \"nal database-as-a-service (DBaaS) based on the Microsoft SQL Server Database \\nEngine. The service sh\", \"ares many features found in the on-premises version of SQL Server and runs the \\nlatest stable versio\", \"n of the SQL Server Database Engine. \\n\\nFor use with a cloud-native microservice, Azure SQL Database \", \"is available with three deployment \\noptions: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n100 \\n\\nA Single Database represents a fully ma\", \"naged SQL Database running on an Azure SQL \\nDatabase server in the Azure cloud. The database is cons\", \"idered contained as it has no \\nconfiguration dependencies on the underlying database server. \\n\\nA Man\", \"aged Instance is a fully managed instance of the Microsoft SQL Server Database Engine \\nthat provides\", \" near-100% compatibility with an on-premises SQL Server. This option supports \\nlarger databases, up \", \"to 35 TB and is placed in an Azure Virtual Network for better isolation. \\n\\nCHAPTER 5 | Cloud-native \", \"data patterns \\n\\n \\n \\n \\n\\f\\u2022 \\n\\nAzure SQL Database serverless is a compute tier for a single database tha\", \"t automatically \\nscales based on workload demand. It bills only for the amount of compute used per s\", \"econd. \\nThe service is well suited for workloads with intermittent, unpredictable usage patterns, \\ni\", \"nterspersed with periods of inactivity. The serverless compute tier also automatically pauses \\ndatab\", \"ases during inactive periods so that only storage charges are billed. It automatically \\nresumes when\", \" activity returns. \\n\\nBeyond the traditional Microsoft SQL Server stack, Azure also features managed \", \"versions of three \\npopular open-source databases. \\n\\nOpen-source databases in Azure \\n\\nOpen-source rel\", \"ational databases have become a popular choice for cloud-native applications. Many \\nenterprises favo\", \"r them over commercial database products, especially for cost savings. Many \\ndevelopment teams enjoy\", \" their flexibility, community-backed development, and ecosystem of tools \\nand extensions. Open-sourc\", \"e databases can be deployed across multiple cloud providers, helping \\nminimize the concern of \\u201cvendo\", \"r lock-in.\\u201d \\n\\nDevelopers can easily self-host any open-source database on an Azure VM. While providi\", \"ng full \\ncontrol, this approach puts you on the hook for the management, monitoring, and maintenance\", \" of \\nthe database and VM. \\n\\nHowever, Microsoft continues its commitment to keeping Azure an \\u201copen pl\", \"atform\\u201d by offering \\nseveral popular open-source databases as fully managed DBaaS services. \\n\\nAzure \", \"Database for MySQL \\n\\nMySQL is an open-source relational database and a pillar for applications built\", \" on the LAMP software \\nstack. Widely chosen for read heavy workloads, it\\u2019s used by many large organi\", \"zations, including \\nFacebook, Twitter, and YouTube. The community edition is available for free, whi\", \"le the enterprise \\nedition requires a license purchase. Originally created in 1995, the product was \", \"purchased by Sun \\nMicrosystems in 2008. Oracle acquired Sun and MySQL in 2010. \\n\\nAzure Database for \", \"MySQL is a managed relational database service based on the open-source \\nMySQL Server engine. It use\", \"s the MySQL Community edition. The Azure MySQL server is the \\nadministrative point for the service. \", \"It\\u2019s the same MySQL server engine used for on-premises \\ndeployments. The engine can create a single \", \"database per server or multiple databases per server that \\nshare resources. You can continue to mana\", \"ge data using the same open-source tools without having \\nto learn new skills or manage virtual machi\", \"nes. \\n\\nAzure Database for MariaDB \\n\\nMariaDB Server is another popular open-source database server. I\", \"t was created as a fork of MySQL \\nwhen Oracle purchased Sun Microsystems, who owned MySQL. The inten\", \"t was to ensure that MariaDB \\nremained open-source. As MariaDB is a fork of MySQL, the data and tabl\", \"e definitions are compatible, \\nand the client protocols, structures, and APIs, are close-knit. \\n\\n101\", \" \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fMariaDB has a strong community and is used by many \", \"large enterprises. While Oracle continues to \\nmaintain, enhance, and support MySQL, the MariaDB foun\", \"dation manages MariaDB, allowing public \\ncontributions to the product and documentation. \\n\\nAzure Dat\", \"abase for MariaDB is a fully managed relational database as a service in the Azure cloud. The \\nservi\", \"ce is based on the MariaDB community edition server engine. It can handle mission-critical \\nworkload\", \"s with predictable performance and dynamic scalability. \\n\\nAzure Database for PostgreSQL \\n\\nPostgreSQL\", \" is an open-source relational database with over 30 years of active development. \\nPostgreSQL has a s\", \"trong reputation for reliability and data integrity. It\\u2019s feature rich, SQL compliant, \\nand consider\", \"ed more performant than MySQL - especially for workloads with complex queries and \\nheavy writes. Man\", \"y large enterprises including Apple, Red Hat, and Fujitsu have built products using \\nPostgreSQL. \\n\\nA\", \"zure Database for PostgreSQL is a fully managed relational database service, based on the open-\\nsour\", \"ce Postgres database engine. The service supports many development platforms, including C++, \\nJava, \", \"Python, Node, C#, and PHP. You can migrate PostgreSQL databases to it using the command-\\nline interf\", \"ace tool or Azure Data Migration Service. \\n\\nAzure Database for PostgreSQL is available with two depl\", \"oyment options: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nThe Single Server deployment option is a central administrative point for \", \"multiple databases \\nto which you can deploy many databases. The pricing is structured per-server bas\", \"ed upon \\ncores and storage. \\n\\nThe Hyperscale (Citus) option is powered by Citus Data technology. It \", \"enables high \\nperformance by horizontally scaling a single database across hundreds of nodes to deli\", \"ver fast \\nperformance and scale. This option allows the engine to fit more data in memory, paralleli\", \"ze \\nqueries across hundreds of nodes, and index data faster. \\n\\nNoSQL data in Azure \\n\\nCosmos DB is a \", \"fully managed, globally distributed NoSQL database service in the Azure cloud. It has \\nbeen adopted \", \"by many large companies across the world, including Coca-Cola, Skype, ExxonMobil, \\nand Liberty Mutua\", \"l. \\n\\nIf your services require fast response from anywhere in the world, high availability, or elasti\", \"c \\nscalability, Cosmos DB is a great choice. Figure 5-12 shows Cosmos DB. \\n\\n102 \\n\\nCHAPTER 5 | Cloud-\", \"native data patterns \\n\\n \\n \\n\\fFigure 5-12: Overview of Azure Cosmos DB \\n\\nThe previous figure presents \", \"many of the built-in cloud-native capabilities available in Cosmos DB. In \\nthis section, we\\u2019ll take \", \"a closer look at them. \\n\\nGlobal support \\n\\nCloud-native applications often have a global audience and\", \" require global scale. \\n\\nYou can distribute Cosmos databases across regions or around the world, pla\", \"cing data close to your \\nusers, improving response time, and reducing latency. You can add or remove\", \" a database from a \\nregion without pausing or redeploying your services. In the background, Cosmos D\", \"B transparently \\nreplicates the data to each of the configured regions. \\n\\nCosmos DB supports active/\", \"active clustering at the global level, enabling you to configure any of your \\ndatabase regions to su\", \"pport both writes and reads. \\n\\nThe Multi-region write protocol is an important feature in Cosmos DB \", \"that enables the following \\nfunctionality: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nUnlimited elastic write and read scalabilit\", \"y. \\n\\n99.999% read and write availability all around the world. \\n\\nGuaranteed reads and writes served \", \"in less than 10 milliseconds at the 99th percentile. \\n\\nWith the Cosmos DB Multi-Homing APIs, your mi\", \"croservice is automatically aware of the nearest \\nAzure region and sends requests to it. The nearest\", \" region is identified by Cosmos DB without any \\nconfiguration changes. Should a region become unavai\", \"lable, the Multi-Homing feature will \\nautomatically route requests to the next nearest available reg\", \"ion. \\n\\nMulti-model support \\n\\nWhen replatforming monolithic applications to a cloud-native architectu\", \"re, development teams \\nsometimes have to migrate open-source, NoSQL data stores. Cosmos DB can help \", \"you preserve your \\n\\n103 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\finvestment in these NoSQL \", \"datastores with its multi-model data platform. The following table shows \\nthe supported NoSQL compat\", \"ibility APIs. \\n\\nProvider \\n\\nDescription \\n\\nNoSQL API \\n\\nMongo DB API \\n\\nGremlin API \\n\\nCassandra API \\n\\nTa\", \"ble API \\n\\nPostgreSQL API \\n\\nAPI for NoSQL stores data in document format \\n\\nSupports Mongo DB APIs and\", \" JSON documents \\n\\nSupports Gremlin API with graph-based nodes \\nand edge data representations \\n\\nSuppo\", \"rts Casandra API for wide-column data \\nrepresentations \\n\\nSupports Azure Table Storage with premium \\n\", \"enhancements \\n\\nManaged service for running PostgreSQL at any \\nscale \\n\\nDevelopment teams can migrate \", \"existing Mongo, Gremlin, or Cassandra databases into Cosmos DB \\nwith minimal changes to data or code\", \". For new apps, development teams can choose among open-\\nsource options or the built-in SQL API mode\", \"l. \\n\\nInternally, Cosmos stores the data in a simple struct format made up of primitive data types. F\", \"or each \\nrequest, the database engine translates the primitive data into the model representation yo\", \"u\\u2019ve \\nselected. \\n\\nIn the previous table, note the Table API option. This API is an evolution of Azur\", \"e Table Storage. Both \\nshare the same underlying table model, but the Cosmos DB Table API adds premi\", \"um enhancements \\nnot available in the Azure Storage API. The following table contrasts the features.\", \" \\n\\nFeature \\n\\nAzure Table Storage \\n\\nAzure Cosmos DB \\n\\nLatency \\n\\nFast \\n\\nSingle-digit millisecond laten\", \"cy for reads and \\nwrites anywhere in the world \\n\\nThroughp\\nut \\n\\nGlobal \\nDistributio\\nn \\n\\nIndexing \\n\\nLi\", \"mit of 20,000 operations per table \\n\\nUnlimited operations per table \\n\\nSingle region with optional si\", \"ngle \\nsecondary read region \\n\\nTurnkey distributions to all regions with \\nautomatic failover \\n\\nAvaila\", \"ble for partition and row key \\nproperties only \\n\\nAutomatic indexing of all properties \\n\\nPricing \\n\\nOp\", \"timized for cold workloads (low \\nthroughput : storage ratio) \\n\\nOptimized for hot workloads (high \\nth\", \"roughput : storage ratio) \\n\\nMicroservices that consume Azure Table storage can easily migrate to the\", \" Cosmos DB Table API. No \\ncode changes are required. \\n\\n104 \\n\\nCHAPTER 5 | Cloud-native data patterns \", \"\\n\\n \\n \\n\\fTunable consistency \\n\\nEarlier in the Relational vs. NoSQL section, we discussed the subject o\", \"f data consistency. Data \\nconsistency refers to the integrity of your data. Cloud-native services wi\", \"th distributed data rely on \\nreplication and must make a fundamental tradeoff between read consisten\", \"cy, availability, and latency. \\n\\nMost distributed databases allow developers to choose between two c\", \"onsistency \\nmodels: strong consistency and eventual consistency. Strong consistency is the gold stan\", \"dard of data \\nprogrammability. It guarantees that a query will always return the most current data -\", \" even if the \\nsystem must incur latency waiting for an update to replicate across all database copie\", \"s. While a \\ndatabase configured for eventual consistency will return data immediately, even if that \", \"data isn\\u2019t the \\nmost current copy. The latter option enables higher availability, greater scale, and\", \" increased \\nperformance. \\n\\nAzure Cosmos DB offers five well-defined consistency models shown in Figu\", \"re 5-13. \\n\\nFigure 5-13: Cosmos DB Consistency Levels \\n\\nThese options enable you to make precise choi\", \"ces and granular tradeoffs for consistency, availability, \\nand the performance for your data. The le\", \"vels are presented in the following table. \\n\\nConsistency Level \\n\\nDescription \\n\\nEventual \\n\\nConstant P\", \"refix \\n\\nSession \\n\\nBounded Staleness \\n\\nStrong \\n\\nNo ordering guarantee for reads. Replicas will \\nevent\", \"ually converge. \\n\\nReads are still eventual, but data is returned in \\nthe ordering in which it is wri\", \"tten. \\n\\nGuarantees you can read any data written \\nduring the current session. It is the default \\ncon\", \"sistency level. \\n\\nReads trail writes by interval that you specify. \\n\\nReads are guaranteed to return \", \"most recent \\ncommitted version of an item. A client never \\nsees an uncommitted or partial read. \\n\\nIn\", \" the article Getting Behind the 9-Ball: Cosmos DB Consistency Levels Explained, Microsoft Program \\nM\", \"anager Jeremy Likness provides an excellent explanation of the five models. \\n\\nPartitioning \\n\\nAzure C\", \"osmos DB embraces automatic partitioning to scale a database to meet the performance \\nneeds of your \", \"cloud-native services. \\n\\nYou manage data in Cosmos DB data by creating databases, containers, and it\", \"ems. \\n\\n105 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fContainers live in a Cosmos DB database\", \" and represent a schema-agnostic grouping of items. Items \\nare the data that you add to the containe\", \"r. They\\u2019re represented as documents, rows, nodes, or edges. \\nAll items added to a container are auto\", \"matically indexed. \\n\\nTo partition the container, items are divided into distinct subsets called logi\", \"cal partitions. Logical \\npartitions are populated based on the value of a partition key that is asso\", \"ciated with each item in a \\ncontainer. Figure 5-14 shows two containers each with a logical partitio\", \"n based on a partition key \\nvalue. \\n\\nFigure 5-14: Cosmos DB partitioning mechanics \\n\\nNote in the pre\", \"vious figure how each item includes a partition key of either \\u2018city\\u2019 or \\u2018airport\\u2019. The key \\ndetermin\", \"es the item\\u2019s logical partition. Items with a city code are assigned to the container on the left, \\n\", \"and items with an airport code, to the container on the right. Combining the partition key value wit\", \"h \\nthe ID value creates an item\\u2019s index, which uniquely identifies the item. \\n\\nInternally, Cosmos DB\", \" automatically manages the placement of logical partitions on physical \\npartitions to satisfy the sc\", \"alability and performance needs of the container. As application throughput \\nand storage requirement\", \"s increase, Azure Cosmos DB redistributes logical partitions across a greater \\nnumber of servers. Re\", \"distribution operations are managed by Cosmos DB and invoked without \\ninterruption or downtime. \\n\\nNe\", \"wSQL databases \\n\\nNewSQL is an emerging database technology that combines the distributed scalability\", \" of NoSQL with \\nthe ACID guarantees of a relational database. NewSQL databases are important for bus\", \"iness systems \\nthat must process high-volumes of data, across distributed environments, with full tr\", \"ansactional \\nsupport and ACID compliance. While a NoSQL database can provide massive scalability, it\", \" does not \\nguarantee data consistency. Intermittent problems from inconsistent data can place a burd\", \"en on the \\ndevelopment team. Developers must construct safeguards into their microservice code to ma\", \"nage \\nproblems caused by inconsistent data. \\n\\nThe Cloud Native Computing Foundation (CNCF) features \", \"several NewSQL database projects. \\n\\n106 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n \\n\\fCockroach \", \"DB \\n\\nTiDB \\n\\nYugabyteDB \\n\\nVitess \\n\\nProject \\n\\nCharacteristics \\n\\nAn ACID-compliant, relational database\", \" that \\nscales globally. Add a new node to a cluster and \\nCockroachDB takes care of balancing the dat\", \"a \\nacross instances and geographies. It creates, \\nmanages, and distributes replicas to ensure \\nrelia\", \"bility. It\\u2019s open source and freely available. \\n\\nAn open-source database that supports Hybrid \\nTrans\", \"actional and Analytical Processing (HTAP) \\nworkloads. It is MySQL-compatible and features \\nhorizonta\", \"l scalability, strong consistency, and \\nhigh availability. TiDB acts like a MySQL server. \\nYou can c\", \"ontinue to use existing MySQL client \\nlibraries, without requiring extensive code \\nchanges to your a\", \"pplication. \\n\\nAn open source, high-performance, distributed \\nSQL database. It supports low query lat\", \"ency, \\nresilience against failures, and global data \\ndistribution. YugabyteDB is PostgreSQL-\\ncompati\", \"ble and handles scale-out RDBMS and \\ninternet-scale OLTP workloads. The product also \\nsupports NoSQL\", \" and is compatible with \\nCassandra. \\n\\nVitess is a database solution for deploying, \\nscaling, and man\", \"aging large clusters of MySQL \\ninstances. It can run in a public or private cloud \\narchitecture. Vit\", \"ess combines and extends many \\nimportant MySQL features and features both \\nvertical and horizontal s\", \"harding support. \\nOriginated by YouTube, Vitess has been serving \\nall YouTube database traffic since\", \" 2011. \\n\\nThe open-source projects in the previous figure are available from the Cloud Native Computi\", \"ng \\nFoundation. Three of the offerings are full database products, which include .NET support. The o\", \"ther, \\nVitess, is a database clustering system that horizontally scales large clusters of MySQL inst\", \"ances. \\n\\nA key design goal for NewSQL databases is to work natively in Kubernetes, taking advantage \", \"of the \\nplatform\\u2019s resiliency and scalability. \\n\\nNewSQL databases are designed to thrive in ephemera\", \"l cloud environments where underlying virtual \\nmachines can be restarted or rescheduled at a moment\\u2019\", \"s notice. The databases are designed to \\nsurvive node failures without data loss nor downtime. Cockr\", \"oachDB, for example, is able to survive a \\nmachine loss by maintaining three consistent replicas of \", \"any data across the nodes in a cluster. \\n\\nKubernetes uses a Services construct to allow a client to \", \"address a group of identical NewSQL \\ndatabases processes from a single DNS entry. By decoupling the \", \"database instances from the address \\n\\n107 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fof the ser\", \"vice with which it\\u2019s associated, we can scale without disrupting existing application instances. \\nSe\", \"nding a request to any service at a given time will always yield the same result. \\n\\nIn this scenario\", \", all database instances are equal. There are no primary or secondary relationships. \\nTechniques lik\", \"e consensus replication found in CockroachDB allow any database node to handle any \\nrequest. If the \", \"node that receives a load-balanced request has the data it needs locally, it responds \\nimmediately. \", \"If not, the node becomes a gateway and forwards the request to the appropriate nodes \\nto get the cor\", \"rect answer. From the client\\u2019s perspective, every database node is the same: They appear \\nas a singl\", \"e logical database with the consistency guarantees of a single-machine system, despite \\nhaving dozen\", \"s or even hundreds of nodes that are working behind the scenes. \\n\\nFor a detailed look at the mechani\", \"cs behind NewSQL databases, see the DASH: Four Properties of \\nKubernetes-Native Databases article. \\n\", \"\\nData migration to the cloud \\n\\nOne of the more time-consuming tasks is migrating data from one data \", \"platform to another. The \\nAzure Data Migration Service can help expedite such efforts. It can migrat\", \"e data from several external \\ndatabase sources into Azure Data platforms with minimal downtime. Targ\", \"et platforms include the \\nfollowing services: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAzure SQL Database \\n\\nAzure Datab\", \"ase for MySQL \\n\\nAzure Database for MariaDB \\n\\nAzure Database for PostgreSQL \\n\\nAzure Cosmos DB \\n\\nThe s\", \"ervice provides recommendations to guide you through the changes required to execute a \\nmigration, b\", \"oth small or large. \\n\\nCaching in a cloud-native app \\n\\nThe benefits of caching are well understood. T\", \"he technique works by temporarily copying frequently \\naccessed data from a backend data store to fas\", \"t storage that\\u2019s located closer to the application. \\nCaching is often implemented where\\u2026 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022\", \" \\n\\nData remains relatively static. \\n\\nData access is slow, especially compared to the speed of the ca\", \"che. \\n\\nData is subject to high levels of contention. \\n\\nWhy? \\n\\nAs discussed in the Microsoft caching \", \"guidance, caching can increase performance, scalability, and \\navailability for individual microservi\", \"ces and the system as a whole. It reduces the latency and \\ncontention of handling large volumes of c\", \"oncurrent requests to a data store. As data volume and the \\nnumber of users increase, the greater th\", \"e benefits of caching become. \\n\\n108 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fCaching is most \", \"effective when a client repeatedly reads data that is immutable or that changes \\ninfrequently. Examp\", \"les include reference information such as product and pricing information, or \\nshared static resourc\", \"es that are costly to construct. \\n\\nWhile microservices should be stateless, a distributed cache can \", \"support concurrent access to session \\nstate data when absolutely required. \\n\\nAlso consider caching t\", \"o avoid repetitive computations. If an operation transforms data or performs a \\ncomplicated calculat\", \"ion, cache the result for subsequent requests. \\n\\nCaching architecture \\n\\nCloud native applications ty\", \"pically implement a distributed caching architecture. The cache is hosted \\nas a cloud-based backing \", \"service, separate from the microservices. Figure 5-15 shows the architecture. \\n\\nFigure 5-15: Caching\", \" in a cloud native app \\n\\nIn the previous figure, note how the cache is independent of and shared by \", \"the microservices. In this \\nscenario, the cache is invoked by the API Gateway. As discussed in chapt\", \"er 4, the gateway serves as a \\nfront end for all incoming requests. The distributed cache increases \", \"system responsiveness by \\nreturning cached data whenever possible. Additionally, separating the cach\", \"e from the services allows \\nthe cache to scale up or out independently to meet increased traffic dem\", \"ands. \\n\\nThe previous figure presents a common caching pattern known as the cache-aside pattern. For \", \"an \\nincoming request, you first query the cache (step #1) for a response. If found, the data is retu\", \"rned \\nimmediately. If the data doesn\\u2019t exist in the cache (known as a cache miss), it\\u2019s retrieved fr\", \"om a local \\ndatabase in a downstream service (step #2). It\\u2019s then written to the cache for future re\", \"quests (step #3), \\nand returned to the caller. Care must be taken to periodically evict cached data \", \"so that the system \\nremains timely and consistent. \\n\\nAs a shared cache grows, it might prove benefic\", \"ial to partition its data across multiple nodes. Doing \\nso can help minimize contention and improve \", \"scalability. Many Caching services support the ability to \\n\\n109 \\n\\nCHAPTER 5 | Cloud-native data patt\", \"erns \\n\\n \\n \\n \\n\\fdynamically add and remove nodes and rebalance data across partitions. This approach t\", \"ypically \\ninvolves clustering. Clustering exposes a collection of federated nodes as a seamless, sin\", \"gle cache. \\nInternally, however, the data is dispersed across the nodes following a predefined distr\", \"ibution strategy \\nthat balances the load evenly. \\n\\nAzure Cache for Redis \\n\\nAzure Cache for Redis is \", \"a secure data caching and messaging broker service, fully managed by \\nMicrosoft. Consumed as a Platf\", \"orm as a Service (PaaS) offering, it provides high throughput and low-\\nlatency access to data. The s\", \"ervice is accessible to any application within or outside of Azure. \\n\\nThe Azure Cache for Redis serv\", \"ice manages access to open-source Redis servers hosted across Azure \\ndata centers. The service acts \", \"as a facade providing management, access control, and security. The \\nservice natively supports a ric\", \"h set of data structures, including strings, hashes, lists, and sets. If your \\napplication already u\", \"ses Redis, it will work as-is with Azure Cache for Redis. \\n\\nAzure Cache for Redis is more than a sim\", \"ple cache server. It can support a number of scenarios to \\nenhance a microservices architecture: \\n\\n\\u2022\", \" \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAn in-memory data store \\n\\nA distributed non-relational database \\n\\nA message broker \\n\\n\", \"A configuration or discovery server \\n\\nFor advanced scenarios, a copy of the cached data can be persi\", \"sted to disk. If a catastrophic event \\ndisables both the primary and replica caches, the cache is re\", \"constructed from the most recent \\nsnapshot. \\n\\nAzure Redis Cache is available across a number of pred\", \"efined configurations and pricing tiers. The \\nPremium tier features many enterprise-level features s\", \"uch as clustering, data persistence, geo-\\nreplication, and virtual-network isolation. \\n\\nElasticsearc\", \"h in a cloud-native app \\n\\nElasticsearch is a distributed search and analytics system that enables co\", \"mplex search capabilities \\nacross diverse types of data. It\\u2019s open source and widely popular. Consid\", \"er how the following \\ncompanies integrate Elasticsearch into their application: \\n\\n\\u2022  Wikipedia for f\", \"ull-text and incremental (search as you type) searching. \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nGitHub to index and expose over 8\", \" million code repositories. \\n\\nDocker for making its container library discoverable. \\n\\nElasticsearch \", \"is built on top of the Apache Lucene full-text search engine. Lucene provides high-\\nperformance docu\", \"ment indexing and querying. It indexes data with an inverted indexing scheme \\u2013 \\ninstead of mapping p\", \"ages to keywords, it maps keywords to pages just like a glossary at the end of a \\nbook. Lucene has p\", \"owerful query syntax capabilities and can query data by: \\n\\n110 \\n\\nCHAPTER 5 | Cloud-native data patte\", \"rns \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\nTerm (a full word) \\n\\nPrefix (starts-with word) \\n\\n\\u2022  Wildcard (using \\u201c*\\u201d or \\u201c?\\u201d \", \"filters) \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nPhrase (a sequence of text in a document) \\n\\nBoolean value (complex searches combi\", \"ning queries) \\n\\nWhile Lucene provides low-level plumbing for searching, Elasticsearch provides the s\", \"erver that sits on \\ntop of Lucene. Elasticsearch adds higher-level functionality to simplify working\", \" Lucene, including a \\nRESTful API to access Lucene\\u2019s indexing and searching functionality. It also p\", \"rovides a distributed \\ninfrastructure capable of massive scalability, fault tolerance, and high avai\", \"lability. \\n\\nFor larger cloud-native applications with complex search requirements, Elasticsearch is \", \"available as \\nmanaged service in Azure. The Microsoft Azure Marketplace features preconfigured templ\", \"ates which \\ndevelopers can use to deploy an Elasticsearch cluster on Azure. \\n\\nFrom the Microsoft Azu\", \"re Marketplace, developers can use preconfigured templates built to quickly \\ndeploy an Elasticsearch\", \" cluster on Azure. Using the Azure-managed offering, you can deploy up to 50 \\ndata nodes, 20 coordin\", \"ating nodes, and three dedicated master nodes. \\n\\nSummary \\n\\nThis chapter presented a detailed look at\", \" data in cloud-native systems. We started by contrasting data \\nstorage in monolithic applications wi\", \"th data storage patterns in cloud-native systems. We looked at \\ndata patterns implemented in cloud-n\", \"ative systems, including cross-service queries, distributed \\ntransactions, and patterns to deal with\", \" high-volume systems. We contrasted SQL with NoSQL data. \\nWe looked at data storage options availabl\", \"e in Azure that include both Microsoft-centric and open-\\nsource options. Finally, we discussed cachi\", \"ng and Elasticsearch in a cloud-native application. \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nCommand and Query Respons\", \"ibility Segregation (CQRS) pattern \\n\\nEvent Sourcing pattern \\n\\n\\u2022  Why isn\\u2019t RDBMS Partition Tolerant \", \"in CAP Theorem and why is it Available? \\n\\n\\u2022  Materialized View \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n111 \\n\\nA\", \"ll you really need to know about open source databases \\n\\nCompensating Transaction pattern \\n\\nSaga Pat\", \"tern \\n\\nSaga Patterns | How to implement business transactions using microservices \\n\\nCompensating Tra\", \"nsaction pattern \\n\\nGetting Behind the 9-Ball: Cosmos DB Consistency Levels Explained \\n\\nOn RDBMS, NoS\", \"QL and NewSQL databases. Interview with John Ryan \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\f\\u2022 \", \"\\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nSQL vs NoSQL vs NewSQL: The Full Comparison \\n\\nDASH: Four Properties of\", \" Kubernetes-Native Databases \\n\\nCockroachDB \\n\\nTiDB \\n\\nYugabyteDB \\n\\nVitess \\n\\nElasticsearch: The Definit\", \"ive Guide \\n\\nIntroduction to Apache Lucene \\n\\n112 \\n\\nCHAPTER 5 | Cloud-native data patterns \\n\\n \\n \\n\\fCHAP\", \"TER  6 \\n\\nCloud-native resiliency \\n\\nResiliency is the ability of your system to react to failure and \", \"still remain functional. It\\u2019s not about \\navoiding failure, but accepting failure and constructing yo\", \"ur cloud-native services to respond to it. \\nYou want to return to a fully functioning state quickly \", \"as possible. \\n\\nUnlike traditional monolithic applications, where everything runs together in a singl\", \"e process, cloud-\\nnative systems embrace a distributed architecture as shown in Figure 6-1: \\n\\nFigure\", \" 6-1. Distributed cloud-native environment \\n\\nIn the previous figure, each microservice and cloud-bas\", \"ed backing service execute in a separate \\nprocess, across server infrastructure, communicating via n\", \"etwork-based calls. \\n\\nOperating in this environment, a service must be sensitive to many different c\", \"hallenges: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n113 \\n\\nUnexpected network latency - the time for a service reque\", \"st to travel to the receiver and back. \\n\\nTransient faults - short-lived network connectivity errors.\", \" \\n\\nBlockage by a long-running synchronous operation. \\n\\nA host process that has crashed and is being \", \"restarted or moved. \\n\\nAn overloaded microservice that can\\u2019t respond for a short time. \\n\\nAn in-flight\", \" orchestrator operation such as a rolling upgrade or moving a service from one \\nnode to another. \\n\\nC\", \"HAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n \\n\\f\\u2022 \\n\\nHardware failures. \\n\\nCloud platforms can detect and \", \"mitigate many of these infrastructure issues. It may restart, scale out, \\nand even redistribute your\", \" service to a different node. However, to take full advantage of this built-in \\nprotection, you must\", \" design your services to react to it and thrive in this dynamic environment. \\n\\nIn the following sect\", \"ions, we\\u2019ll explore defensive techniques that your service and managed cloud \\nresources can leverage\", \" to minimize downtime and disruption. \\n\\nApplication resiliency patterns \\n\\nThe first line of defense \", \"is application resiliency. \\n\\nWhile you could invest considerable time writing your own resiliency fr\", \"amework, such products \\nalready exist. Polly is a comprehensive .NET resilience and transient-fault-\", \"handling library that allows \\ndevelopers to express resiliency policies in a fluent and thread-safe \", \"manner. Polly targets applications \\nbuilt with either .NET Framework or .NET 7. The following table \", \"describes the resiliency features, called \\npolicies, available in the Polly Library. They can be app\", \"lied individually or grouped together. \\n\\nPolicy \\n\\nExperience \\n\\nRetry \\n\\nCircuit Breaker \\n\\nTimeout \\n\\nB\", \"ulkhead \\n\\nCache \\n\\nFallback \\n\\nConfigures retry operations on designated \\noperations. \\n\\nBlocks request\", \"ed operations for a predefined \\nperiod when faults exceed a configured \\nthreshold \\n\\nPlaces limit on \", \"the duration for which a caller \\ncan wait for a response. \\n\\nConstrains actions to fixed-size resourc\", \"e pool to \\nprevent failing calls from swamping a resource. \\n\\nStores responses automatically. \\n\\nDefin\", \"es structured behavior upon a failure. \\n\\nNote how in the previous figure the resiliency policies app\", \"ly to request messages, whether coming \\nfrom an external client or back-end service. The goal is to \", \"compensate the request for a service that \\nmight be momentarily unavailable. These short-lived inter\", \"ruptions typically manifest themselves with \\nthe HTTP status codes shown in the following table. \\n\\nH\", \"TTP Status Code \\n\\nCause \\n\\n404 \\n\\n408 \\n\\n429 \\n\\n502 \\n\\n503 \\n\\n114 \\n\\nNot Found \\n\\nRequest timeout \\n\\nToo many\", \" requests (you\\u2019ve most likely been throttled) \\n\\nBad gateway \\n\\nService unavailable \\n\\nCHAPTER 6 | Clou\", \"d-native resiliency \\n\\n \\n \\n\\fHTTP Status Code \\n\\nCause \\n\\n504 \\n\\nGateway timeout \\n\\nQuestion: Would you re\", \"try an HTTP Status Code of 403 - Forbidden? No. Here, the system is \\nfunctioning properly, but infor\", \"ming the caller that they aren\\u2019t authorized to perform the requested \\noperation. Care must be taken \", \"to retry only those operations caused by failures. \\n\\nAs recommended in Chapter 1, Microsoft develope\", \"rs constructing cloud-native applications should \\ntarget the .NET platform. Version 2.1 introduced t\", \"he HTTPClientFactory library for creating HTTP Client \\ninstances for interacting with URL-based reso\", \"urces. Superseding the original HTTPClient class, the \\nfactory class supports many enhanced features\", \", one of which is tight integration with the Polly \\nresiliency library. With it, you can easily defi\", \"ne resiliency policies in the application Startup class to \\nhandle partial failures and connectivity\", \" issues. \\n\\nNext, let\\u2019s expand on retry and circuit breaker patterns. \\n\\nRetry pattern \\n\\nIn a distribu\", \"ted cloud-native environment, calls to services and cloud resources can fail because of \\ntransient (\", \"short-lived) failures, which typically correct themselves after a brief period of time. \\nImplementin\", \"g a retry strategy helps a cloud-native service mitigate these scenarios. \\n\\nThe Retry pattern enable\", \"s a service to retry a failed request operation a (configurable) number of \\ntimes with an exponentia\", \"lly increasing wait time. Figure 6-2 shows a retry in action. \\n\\nFigure 6-2. Retry pattern in action \", \"\\n\\nIn the previous figure, a retry pattern has been implemented for a request operation. It\\u2019s configu\", \"red to \\nallow up to four retries before failing with a backoff interval (wait time) starting at two \", \"seconds, which \\nexponentially doubles for each subsequent attempt. \\n\\n\\u2022 \\n\\nThe first invocation fails \", \"and returns an HTTP status code of 500. The application waits for two \\nseconds and retries the call.\", \" \\n\\n115 \\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nThe second invocation also fai\", \"ls and returns an HTTP status code of 500. The application now \\ndoubles the backoff interval to four\", \" seconds and retries the call. \\n\\nFinally, the third call succeeds. \\n\\nIn this scenario, the retry ope\", \"ration would have attempted up to four retries while doubling \\nthe backoff duration before failing t\", \"he call. \\n\\nHad the 4th retry attempt failed, a fallback policy would be invoked to gracefully handle\", \" the \\nproblem. \\n\\nIt\\u2019s important to increase the backoff period before retrying the call to allow the\", \" service time to self-\\ncorrect. It\\u2019s a best practice to implement an exponentially increasing backof\", \"f (doubling the period on \\neach retry) to allow adequate correction time. \\n\\nCircuit breaker pattern \", \"\\n\\nWhile the retry pattern can help salvage a request entangled in a partial failure, there are situa\", \"tions \\nwhere failures can be caused by unanticipated events that will require longer periods of time\", \" to \\nresolve. These faults can range in severity from a partial loss of connectivity to the complete\", \" failure of \\na service. In these situations, it\\u2019s pointless for an application to continually retry \", \"an operation that is \\nunlikely to succeed. \\n\\nTo make things worse, executing continual retry operati\", \"ons on a non-responsive service can move \\nyou into a self-imposed denial of service scenario where y\", \"ou flood your service with continual calls \\nexhausting resources such as memory, threads and databas\", \"e connections, causing failure in unrelated \\nparts of the system that use the same resources. \\n\\nIn t\", \"hese situations, it would be preferable for the operation to fail immediately and only attempt to \\ni\", \"nvoke the service if it\\u2019s likely to succeed. \\n\\nThe Circuit Breaker pattern can prevent an applicatio\", \"n from repeatedly trying to execute an operation \\nthat\\u2019s likely to fail. After a pre-defined number \", \"of failed calls, it blocks all traffic to the service. \\nPeriodically, it will allow a trial call to \", \"determine whether the fault has resolved. Figure 6-3 shows the \\nCircuit Breaker pattern in action. \\n\", \"\\n116 \\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n\\fFigure 6-3. Circuit breaker pattern in action \\n\\nIn\", \" the previous figure, a Circuit Breaker pattern has been added to the original retry pattern. Note h\", \"ow \\nafter 100 failed requests, the circuit breakers opens and no longer allows calls to the service.\", \" The \\nCheckCircuit value, set at 30 seconds, specifies how often the library allows one request to p\", \"roceed to \\nthe service. If that call succeeds, the circuit closes and the service is once again avai\", \"lable to traffic. \\n\\nKeep in mind that the intent of the Circuit Breaker pattern is different than th\", \"at of the Retry pattern. \\nThe Retry pattern enables an application to retry an operation in the expe\", \"ctation that it will succeed. \\nThe Circuit Breaker pattern prevents an application from doing an ope\", \"ration that is likely to fail. \\nTypically, an application will combine these two patterns by using t\", \"he Retry pattern to invoke an \\noperation through a circuit breaker. \\n\\nTesting for resiliency \\n\\nTesti\", \"ng for resiliency cannot always be done the same way that you test application functionality (by \\nru\", \"nning unit tests, integration tests, and so on). Instead, you must test how the end-to-end workload \", \"\\nperforms under failure conditions, which only occur intermittently. For example: inject failures by\", \" \\ncrashing processes, expired certificates, make dependent services unavailable etc. Frameworks like\", \" \\nchaos-monkey can be used for such chaos testing. \\n\\nApplication resiliency is a must for handling p\", \"roblematic requested operations. But, it\\u2019s only half of the \\nstory. Next, we cover resiliency featur\", \"es available in the Azure cloud. \\n\\nAzure platform resiliency \\n\\nBuilding a reliable application in th\", \"e cloud is different from traditional on-premises application \\ndevelopment. While historically you p\", \"urchased higher-end hardware to scale up, in a cloud \\nenvironment you scale out. Instead of trying t\", \"o prevent failures, the goal is to minimize their effects \\nand keep the system stable. \\n\\n117 \\n\\nCHAPT\", \"ER 6 | Cloud-native resiliency \\n\\n \\n \\n \\n\\fThat said, reliable cloud applications display distinct char\", \"acteristics: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nThey\\u2019re resilient, recover gracefully from problems, and continue to function\", \". \\n\\nThey\\u2019re highly available (HA) and run as designed in a healthy state with no significant \\ndownti\", \"me. \\n\\nUnderstanding how these characteristics work together - and how they affect cost - is essentia\", \"l to \\nbuilding a reliable cloud-native application. We\\u2019ll next look at ways that you can build resil\", \"iency and \\navailability into your cloud-native applications leveraging features from the Azure cloud\", \". \\n\\nDesign with resiliency \\n\\nWe\\u2019ve said resiliency enables your application to react to failure and \", \"still remain functional. The \\nwhitepaper, Resilience in Azure whitepaper, provides guidance for achi\", \"eving resilience in the Azure \\nplatform. Here are some key recommendations: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nHa\", \"rdware failure. Build redundancy into the application by deploying components across \\ndifferent faul\", \"t domains. For example, ensure that Azure VMs are placed in different racks by \\nusing Availability S\", \"ets. \\n\\nDatacenter failure. Build redundancy into the application with fault isolation zones across \\n\", \"datacenters. For example, ensure that Azure VMs are placed in different fault-isolated \\ndatacenters \", \"by using Azure Availability Zones. \\n\\nRegional failure. Replicate the data and components into anothe\", \"r region so that applications \\ncan be quickly recovered. For example, use Azure Site Recovery to rep\", \"licate Azure VMs to \\nanother Azure region. \\n\\nHeavy load. Load balance across instances to handle spi\", \"kes in usage. For example, put two or \\nmore Azure VMs behind a load balancer to distribute traffic t\", \"o all VMs. \\n\\nAccidental data deletion or corruption. Back up data so it can be restored if there\\u2019s a\", \"ny \\ndeletion or corruption. For example, use Azure Backup to periodically back up your Azure \\nVMs. \\n\", \"\\nDesign with redundancy \\n\\nFailures vary in scope of impact. A hardware failure, such as a failed dis\", \"k, can affect a single node in a \\ncluster. A failed network switch could affect an entire server rac\", \"k. Less common failures, such as loss of \\npower, could disrupt a whole datacenter. Rarely, an entire\", \" region becomes unavailable. \\n\\nRedundancy is one way to provide application resilience. The exact le\", \"vel of redundancy needed \\ndepends upon your business requirements and will affect both the cost and \", \"complexity of your \\nsystem. For example, a multi-region deployment is more expensive and more comple\", \"x to manage \\nthan a single-region deployment. You\\u2019ll need operational procedures to manage failover \", \"and failback. \\nThe additional cost and complexity might be justified for some business scenarios, bu\", \"t not others. \\n\\nTo architect redundancy, you need to identify the critical paths in your application\", \", and then \\ndetermine if there\\u2019s redundancy at each point in the path? If a subsystem should fail, w\", \"ill the \\napplication fail over to something else? Finally, you need a clear understanding of those f\", \"eatures built \\n\\n118 \\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n\\finto the Azure cloud platform that \", \"you can leverage to meet your redundancy requirements. Here are \\nrecommendations for architecting re\", \"dundancy: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nDeploy multiple instances of services. If your application depends on a \", \"single instance of a \\nservice, it creates a single point of failure. Provisioning multiple instances\", \" improves both \\nresiliency and scalability. When hosting in Azure Kubernetes Service, you can declar\", \"atively \\nconfigure redundant instances (replica sets) in the Kubernetes manifest file. The replica c\", \"ount \\nvalue can be managed programmatically, in the portal, or through autoscaling features. \\n\\nLever\", \"aging a load balancer. Load-balancing distributes your application\\u2019s requests to healthy \\nservice in\", \"stances and automatically removes unhealthy instances from rotation. When \\ndeploying to Kubernetes, \", \"load balancing can be specified in the Kubernetes manifest file in \\nthe Services section. \\n\\nPlan for\", \" multiregion deployment. If you deploy your application to a single region, and that \\nregion becomes\", \" unavailable, your application will also become unavailable. This may be \\nunacceptable under the ter\", \"ms of your application\\u2019s service level agreements. Instead, consider \\ndeploying your application and\", \" its services across multiple regions. For example, an Azure \\nKubernetes Service (AKS) cluster is de\", \"ployed to a single region. To protect your system from a \\nregional failure, you might deploy your ap\", \"plication to multiple AKS clusters across different \\nregions and use the Paired Regions feature to c\", \"oordinate platform updates and prioritize \\nrecovery efforts. \\n\\nEnable geo-replication. Geo-replicati\", \"on for services such as Azure SQL Database and Cosmos \\nDB will create secondary replicas of your dat\", \"a across multiple regions. While both services will \\nautomatically replicate data within the same re\", \"gion, geo-replication protects you against a \\nregional outage by enabling you to fail over to a seco\", \"ndary region. Another best practice for \\ngeo-replication centers around storing container images. To\", \" deploy a service in AKS, you \\nneed to store and pull the image from a repository. Azure Container R\", \"egistry integrates with \\nAKS and can securely store container images. To improve performance and ava\", \"ilability, \\nconsider geo-replicating your images to a registry in each region where you have an AKS \", \"\\ncluster. Each AKS cluster then pulls container images from the local container registry in its \\nreg\", \"ion as shown in Figure 6-4: \\n\\nFigure 6-4. Replicated resources across regions \\n\\n119 \\n\\nCHAPTER 6 | Cl\", \"oud-native resiliency \\n\\n \\n \\n \\n\\f\\u2022 \\n\\nImplement a DNS traffic load balancer. Azure Traffic Manager prov\", \"ides high-availability for \\ncritical applications by load-balancing at the DNS level. It can route t\", \"raffic to different regions \\nbased on geography, cluster response time, and even application endpoin\", \"t health. For \\nexample, Azure Traffic Manager can direct customers to the closest AKS cluster and \\na\", \"pplication instance. If you have multiple AKS clusters in different regions, use Traffic \\nManager to\", \" control how traffic flows to the applications that run in each cluster. Figure 6-5 \\nshows this scen\", \"ario. \\n\\nFigure 6-5. AKS and Azure Traffic Manager \\n\\nDesign for scalability \\n\\nThe cloud thrives on sc\", \"aling. The ability to increase/decrease system resources to address \\nincreasing/decreasing system lo\", \"ad is a key tenet of the Azure cloud. But, to effectively scale an \\napplication, you need an underst\", \"anding of the scaling features of each Azure service that you include \\nin your application. Here are\", \" recommendations for effectively implementing scaling in your system. \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n120 \\n\\nDesign for\", \" scaling. An application must be designed for scaling. To start, services should be \\nstateless so th\", \"at requests can be routed to any instance. Having stateless services also means \\nthat adding or remo\", \"ving an instance doesn\\u2019t adversely impact current users. \\n\\nPartition workloads. Decomposing domains \", \"into independent, self-contained microservices \\nenable each service to scale independently of others\", \". Typically, services will have different \\nscalability needs and requirements. Partitioning enables \", \"you to scale only what needs to be \\nscaled without the unnecessary cost of scaling an entire applica\", \"tion. \\n\\nFavor scale-out. Cloud-based applications favor scaling out resources as opposed to scaling \", \"\\nup. Scaling out (also known as horizontal scaling) involves adding more service resources to \\nan ex\", \"isting system to meet and share a desired level of performance. Scaling up (also known \\n\\nCHAPTER 6 |\", \" Cloud-native resiliency \\n\\n \\n \\n \\n\\fas vertical scaling) involves replacing existing resources with mo\", \"re powerful hardware (more \\ndisk, memory, and processing cores). Scaling out can be invoked automati\", \"cally with the \\nautoscaling features available in some Azure cloud resources. Scaling out across mul\", \"tiple \\nresources also adds redundancy to the overall system. Finally scaling up a single resource is\", \" \\ntypically more expensive than scaling out across many smaller resources. Figure 6-6 shows the \\ntwo\", \" approaches: \\n\\nFigure 6-6. Scale up versus scale out \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nScale proportionally. When sc\", \"aling a service, think in terms of resource sets. If you were to \\ndramatically scale out a specific \", \"service, what impact would that have on back-end data \\nstores, caches and dependent services? Some r\", \"esources such as Cosmos DB can scale out \\nproportionally, while many others can\\u2019t. You want to ensur\", \"e that you don\\u2019t scale out a \\nresource to a point where it will exhaust other associated resources. \", \"\\n\\nAvoid affinity. A best practice is to ensure a node doesn\\u2019t require local affinity, often referred\", \" \\nto as a sticky session. A request should be able to route to any instance. If you need to persist \", \"\\nstate, it should be saved to a distributed cache, such as Azure Redis cache. \\n\\nTake advantage of pl\", \"atform autoscaling features. Use built-in autoscaling features whenever \\npossible, rather than custo\", \"m or third-party mechanisms. Where possible, use scheduled \\nscaling rules to ensure that resources a\", \"re available without a startup delay, but add reactive \\nautoscaling to the rules as appropriate, to \", \"cope with unexpected changes in demand. For \\nmore information, see Autoscaling guidance. \\n\\nScale out\", \" aggressively. A final practice would be to scale out aggressively so that you can \\nquickly meet imm\", \"ediate spikes in traffic without losing business. And, then scale in (that is, \\nremove unneeded inst\", \"ances) conservatively to keep the system stable. A simple way to \\nimplement this is to set the cool \", \"down period, which is the time to wait between scaling \\noperations, to five minutes for adding resou\", \"rces and up to 15 minutes for removing instances. \\n\\nBuilt-in retry in services \\n\\nWe encouraged the b\", \"est practice of implementing programmatic retry operations in an earlier section. \\nKeep in mind that\", \" many Azure services and their corresponding client SDKs also include retry \\n\\n121 \\n\\nCHAPTER 6 | Clou\", \"d-native resiliency \\n\\n \\n \\n \\n\\fmechanisms. The following list summarizes retry features in the many of\", \" the Azure services that are \\ndiscussed in this book: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAzure Cosmos DB. The\", \" DocumentClient class from the client API automatically retires failed \\nattempts. The number of retr\", \"ies and maximum wait time are configurable. Exceptions thrown \\nby the client API are either requests\", \" that exceed the retry policy or non-transient errors. \\n\\nAzure Redis Cache. The Redis StackExchange \", \"client uses a connection manager class that \\nincludes retries on failed attempts. The number of retr\", \"ies, specific retry policy and wait time \\nare all configurable. \\n\\nAzure Service Bus. The Service Bus\", \" client exposes a RetryPolicy class that can be configured \\nwith a back-off interval, retry count, a\", \"nd TerminationTimeBuffer, which specifies the maximum \\ntime an operation can take. The default polic\", \"y is nine maximum retry attempts with a 30-\\nsecond backoff period between attempts. \\n\\nAzure SQL Data\", \"base. Retry support is provided when using the Entity Framework Core library. \\n\\nAzure Storage. The s\", \"torage client library support retry operations. The strategies vary across \\nAzure storage tables, bl\", \"obs, and queues. As well, alternate retries switch between primary and \\nsecondary storage services l\", \"ocations when the geo-redundancy feature is enabled. \\n\\nAzure Event Hubs. The Event Hub client librar\", \"y features a RetryPolicy property, which includes \\na configurable exponential backoff feature. \\n\\nRes\", \"ilient communications \\n\\nThroughout this book, we\\u2019ve embraced a microservice-based architectural appr\", \"oach. While such an \\narchitecture provides important benefits, it presents many challenges: \\n\\n\\u2022 \\n\\n\\u2022 \", \"\\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nOut-of-process network communication. Each microservice communicates over a networ\", \"k \\nprotocol that introduces network congestion, latency, and transient faults. \\n\\nService discovery. \", \"How do microservices discover and communicate with each other when \\nrunning across a cluster of mach\", \"ines with their own IP addresses and ports? \\n\\nResiliency. How do you manage short-lived failures and\", \" keep the system stable? \\n\\nLoad balancing. How does inbound traffic get distributed across multiple \", \"instances of a \\nmicroservice? \\n\\nSecurity. How are security concerns such as transport-level encrypti\", \"on and certificate \\nmanagement enforced? \\n\\nDistributed Monitoring. - How do you correlate and captur\", \"e traceability and monitoring for a \\nsingle request across multiple consuming microservices? \\n\\nYou c\", \"an address these concerns with different libraries and frameworks, but the implementation can \\nbe ex\", \"pensive, complex, and time-consuming. You also end up with infrastructure concerns coupled to \\nbusin\", \"ess logic. \\n\\n122 \\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n\\fService mesh \\n\\nA better approach is an\", \" evolving technology entitled Service Mesh. A service mesh is a configurable \\ninfrastructure layer w\", \"ith built-in capabilities to handle service communication and the other \\nchallenges mentioned above.\", \" It decouples these concerns by moving them into a service proxy. The \\nproxy is deployed into a sepa\", \"rate process (called a sidecar) to provide isolation from business code. \\nHowever, the sidecar is li\", \"nked to the service - it\\u2019s created with it and shares its lifecycle. Figure 6-7 \\nshows this scenario\", \". \\n\\nFigure 6-7. Service mesh with a side car \\n\\nIn the previous figure, note how the proxy intercepts\", \" and manages communication among the \\nmicroservices and the cluster. \\n\\nA service mesh is logically s\", \"plit into two disparate components: A data plane and control plane. Figure \\n6-8 shows these componen\", \"ts and their responsibilities. \\n\\n123 \\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n \\n\\fFigure 6-8. Serv\", \"ice mesh control and data plane \\n\\nOnce configured, a service mesh is highly functional. It can retri\", \"eve a corresponding pool of instances \\nfrom a service discovery endpoint. The mesh can then send a r\", \"equest to a specific instance, recording \\nthe latency and response type of the result. A mesh can ch\", \"oose the instance most likely to return a \\nfast response based on many factors, including its observ\", \"ed latency for recent requests. \\n\\nIf an instance is unresponsive or fails, the mesh will retry the r\", \"equest on another instance. If it returns \\nerrors, a mesh will evict the instance from the load-bala\", \"ncing pool and restate it after it heals. If a \\nrequest times out, a mesh can fail and then retry th\", \"e request. A mesh captures and emits metrics and \\ndistributed tracing to a centralized metrics syste\", \"m. \\n\\nIstio and Envoy \\n\\nWhile a few service mesh options currently exist, Istio is the most popular a\", \"t the time of this writing. \\nIstio is a joint venture from IBM, Google, and Lyft. It\\u2019s an open-sourc\", \"e offering that can be integrated \\ninto a new or existing distributed application. The technology pr\", \"ovides a consistent and complete \\nsolution to secure, connect, and monitor microservices. Its featur\", \"es include: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nSecure service-to-service communication in a cluster with strong i\", \"dentity-based \\nauthentication and authorization. \\n\\nAutomatic load balancing for HTTP, gRPC, WebSocke\", \"t, and TCP traffic. \\n\\nFine-grained control of traffic behavior with rich routing rules, retries, fai\", \"lovers, and fault \\ninjection. \\n\\nA pluggable policy layer and configuration API supporting access con\", \"trols, rate limits, and \\nquotas. \\n\\nAutomatic metrics, logs, and traces for all traffic within a clus\", \"ter, including cluster ingress and \\negress. \\n\\nA key component for an Istio implementation is a proxy\", \" service entitled the Envoy proxy. It runs \\nalongside each service and provides a platform-agnostic \", \"foundation for the following features: \\n\\nDynamic service discovery. \\n\\nLoad balancing. \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n124 \", \"\\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nTLS termination. \\n\\nHTTP and gRPC \", \"proxies. \\n\\nCircuit breaker resiliency. \\n\\nHealth checks. \\n\\nRolling updates with canary deployments. \\n\", \"\\nAs previously discussed, Envoy is deployed as a sidecar to each microservice in the cluster. \\n\\nInte\", \"gration with Azure Kubernetes Services \\n\\nThe Azure cloud embraces Istio and provides direct support \", \"for it within Azure Kubernetes Services. \\nThe following links can help you get started: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nIn\", \"stalling Istio in AKS \\n\\nUsing AKS and Istio \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022\", \" \\n\\nPolly \\n\\nRetry pattern \\n\\nCircuit Breaker pattern \\n\\nResilience in Azure whitepaper \\n\\nnetwork latenc\", \"y \\n\\nRedundancy \\n\\ngeo-replication \\n\\nAzure Traffic Manager \\n\\nAutoscaling guidance \\n\\nIstio \\n\\nEnvoy prox\", \"y \\n\\n125 \\n\\nCHAPTER 6 | Cloud-native resiliency \\n\\n \\n \\n\\fCHAPTER  7 \\n\\nMonitoring and health \\n\\nMicroservi\", \"ces and cloud-native applications go hand in hand with good DevOps practices. DevOps is \\nmany things\", \" to many people but perhaps one of the better definitions comes from cloud advocate \\nand DevOps evan\", \"gelist Donovan Brown: \\n\\n\\u201cDevOps is the union of people, process, and products to enable continuous d\", \"elivery of value to our \\nend users.\\u201d \\n\\nUnfortunately, with terse definitions, there\\u2019s always room to\", \" say more things. One of the key \\ncomponents of DevOps is ensuring that the applications running in \", \"production are functioning \\nproperly and efficiently. To gauge the health of the application in prod\", \"uction, it\\u2019s necessary to monitor \\nthe various logs and metrics being produced from the servers, hos\", \"ts, and the application proper. The \\nnumber of different services running in support of a cloud-nati\", \"ve application makes monitoring the \\nhealth of individual components and the application as a whole \", \"a critical challenge. \\n\\nObservability patterns \\n\\nJust as patterns have been developed to aid in the \", \"layout of code in applications, there are patterns \\nfor operating applications in a reliable way. Th\", \"ree useful patterns in maintaining applications have \\nemerged: logging, monitoring, and alerts. \\n\\nWh\", \"en to use logging \\n\\nNo matter how careful we are, applications almost always behave in unexpected wa\", \"ys in production. \\nWhen users report problems with an application, it\\u2019s useful to be able to see wha\", \"t was going on with \\nthe app when the problem occurred. One of the most tried and true ways of captu\", \"ring information \\nabout what an application is doing while it\\u2019s running is to have the application w\", \"rite down what it\\u2019s \\ndoing. This process is known as logging. Anytime failures or problems occur in \", \"production, the goal \\nshould be to reproduce the conditions under which the failures occurred, in a \", \"non-production \\nenvironment. Having good logging in place provides a roadmap for developers to follo\", \"w in order to \\nduplicate problems in an environment that can be tested and experimented with. \\n\\nChal\", \"lenges when logging with cloud-native applications \\n\\nIn traditional applications, log files are typi\", \"cally stored on the local machine. In fact, on Unix-like \\noperating systems, there\\u2019s a folder struct\", \"ure defined to hold any logs, typically under /var/log. \\n\\n126 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n\", \" \\n \\n\\fFigure 7-1. Logging to a file in a monolithic app. \\n\\nThe usefulness of logging to a flat file o\", \"n a single machine is vastly reduced in a cloud environment. \\nApplications producing logs may not ha\", \"ve access to the local disk or the local disk may be highly \\ntransient as containers are shuffled ar\", \"ound physical machines. Even simple scaling up of monolithic \\napplications across multiple nodes can\", \" make it challenging to locate the appropriate file-based log \\nfile. \\n\\nFigure 7-2. Logging to files \", \"in a scaled monolithic app. \\n\\nCloud-native applications developed using a microservices architecture\", \" also pose some challenges for \\nfile-based loggers. User requests may now span multiple services tha\", \"t are run on different machines \\n\\n127 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n \\n\\fand may include\", \" serverless functions with no access to a local file system at all. It would be very \\nchallenging to\", \" correlate the logs from a user or a session across these many services and machines. \\n\\nFigure 7-3. \", \"Logging to local files in a microservices app. \\n\\nFinally, the number of users in some cloud-native a\", \"pplications is high. Imagine that each user \\ngenerates a hundred lines of log messages when they log\", \" into an application. In isolation, that is \\nmanageable, but multiply that over 100,000 users and th\", \"e volume of logs becomes large enough that \\nspecialized tools are needed to support effective use of\", \" the logs. \\n\\nLogging in cloud-native applications \\n\\nEvery programming language has tooling that perm\", \"its writing logs, and typically the overhead for \\nwriting these logs is low. Many of the logging lib\", \"raries provide logging different kinds of criticalities, \\nwhich can be tuned at run time. For instan\", \"ce, the Serilog library is a popular structured logging library \\nfor .NET that provides the followin\", \"g logging levels: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nVerbose \\n\\nDebug \\n\\nInformation \\n\\n\\u2022  Warning \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nError \\n\\nFatal \\n\", \"\\nThese different log levels provide granularity in logging. When the application is functioning prop\", \"erly \\nin production, it may be configured to only log important messages. When the application is \\n\\n\", \"128 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n\\fmisbehaving, then the log level can be increased so\", \" more verbose logs are gathered. This balances \\nperformance against ease of debugging. \\n\\nThe high pe\", \"rformance of logging tools and the tunability of verbosity should encourage developers to \\nlog frequ\", \"ently. Many favor a pattern of logging the entry and exit of each method. This approach may \\nsound l\", \"ike overkill, but it\\u2019s infrequent that developers will wish for less logging. In fact, it\\u2019s not \\nunc\", \"ommon to perform deployments for the sole purpose of adding logging around a problematic \\nmethod. Er\", \"r on the side of too much logging and not on too little. Some tools can be used to \\nautomatically pr\", \"ovide this kind of logging. \\n\\nBecause of the challenges associated with using file-based logs in clo\", \"ud-native apps, centralized logs \\nare preferred. Logs are collected by the applications and shipped \", \"to a central logging application \\nwhich indexes and stores the logs. This class of system can ingest\", \" tens of gigabytes of logs every day. \\n\\nIt\\u2019s also helpful to follow some standard practices when bui\", \"lding logging that spans many services. \\nFor instance, generating a correlation ID at the start of a\", \" lengthy interaction, and then logging it in \\neach message that is related to that interaction, make\", \"s it easier to search for all related messages. One \\nneed only find a single message and extract the\", \" correlation ID to find all the related messages. \\nAnother example is ensuring that the log format i\", \"s the same for every service, whatever the language \\nor logging library it uses. This standardizatio\", \"n makes reading logs much easier. Figure 7-4 \\ndemonstrates how a microservices architecture can leve\", \"rage centralized logging as part of its \\nworkflow. \\n\\nFigure 7-4. Logs from various sources are inges\", \"ted into a centralized log store. \\n\\n129 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n\\fChallenges with\", \" detecting and responding to potential app health \\nissues \\n\\nSome applications aren\\u2019t mission critica\", \"l. Maybe they\\u2019re only used internally, and when a problem \\noccurs, the user can contact the team res\", \"ponsible and the application can be restarted. However, \\ncustomers often have higher expectations fo\", \"r the applications they consume. You should know when \\nproblems occur with your application before u\", \"sers do, or before users notify you. Otherwise, the first \\nyou know about a problem may be when you \", \"notice an angry deluge of social media posts deriding \\nyour application or even your organization. \\n\", \"\\nSome scenarios you may need to consider include: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nOne service in your application keep\", \"s failing and restarting, resulting in intermittent slow \\nresponses. \\n\\nAt some times of the day, you\", \"r application\\u2019s response time is slow. \\n\\nAfter a recent deployment, load on the database has tripled\", \". \\n\\nImplemented properly, monitoring can let you know about conditions that will lead to problems, \\n\", \"letting you address underlying conditions before they result in any significant user impact. \\n\\nMonit\", \"oring cloud-native apps \\n\\nSome centralized logging systems take on an additional role of collecting \", \"telemetry outside of pure \\nlogs. They can collect metrics, such as time to run a database query, ave\", \"rage response time from a \\nweb server, and even CPU load averages and memory pressure as reported by\", \" the operating system. \\nIn conjunction with the logs, these systems can provide a holistic view of t\", \"he health of nodes in the \\nsystem and the application as a whole. \\n\\nThe metric-gathering capabilitie\", \"s of the monitoring tools can also be fed manually from within the \\napplication. Business flows that\", \" are of particular interest such as new users signing up or orders being \\nplaced, may be instrumente\", \"d such that they increment a counter in the central monitoring system. \\nThis aspect unlocks the moni\", \"toring tools to not only monitor the health of the application but the \\nhealth of the business. \\n\\nQu\", \"eries can be constructed in the log aggregation tools to look for certain statistics or patterns, wh\", \"ich \\ncan then be displayed in graphical form, on custom dashboards. Frequently, teams will invest in\", \" large, \\nwall-mounted displays that rotate through the statistics related to an application. This wa\", \"y, it\\u2019s simple \\nto see the problems as they occur. \\n\\nCloud-native monitoring tools provide real-time\", \" telemetry and insight into apps regardless of whether \\nthey\\u2019re single-process monolithic applicatio\", \"ns or distributed microservice architectures. They include \\ntools that allow collection of data from\", \" the app as well as tools for querying and displaying \\ninformation about the app\\u2019s health. \\n\\nChallen\", \"ges with reacting to critical problems in cloud-native apps \\n\\nIf you need to react to problems with \", \"your application, you need some way to alert the right \\npersonnel. This is the third cloud-native ap\", \"plication observability pattern and depends on logging and \\nmonitoring. Your application needs to ha\", \"ve logging in place to allow problems to be diagnosed, and \\n\\n130 \\n\\nCHAPTER 7 | Monitoring and health\", \" \\n\\n \\n \\n\\fin some cases to feed into monitoring tools. It needs monitoring to aggregate application me\", \"trics and \\nhealth data in one place. Once this has been established, rules can be created that will \", \"trigger alerts \\nwhen certain metrics fall outside of acceptable levels. \\n\\nGenerally, alerts are laye\", \"red on top of monitoring such that certain conditions trigger appropriate \\nalerts to notify team mem\", \"bers of urgent problems. Some scenarios that may require alerts include: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nOne of your a\", \"pplication\\u2019s services is not responding after 1 minute of downtime. \\n\\nYour application is returning \", \"unsuccessful HTTP responses to more than 1% of requests. \\n\\nYour application\\u2019s average response time \", \"for key endpoints exceeds 2000 ms. \\n\\nAlerts in cloud-native apps \\n\\nYou can craft queries against the\", \" monitoring tools to look for known failure conditions. For instance, \\nqueries could search through \", \"the incoming logs for indications of HTTP status code 500, which \\nindicates a problem on a web serve\", \"r. As soon as one of these is detected, then an e-mail or an SMS \\ncould be sent to the owner of the \", \"originating service who can begin to investigate. \\n\\nTypically, though, a single 500 error isn\\u2019t enou\", \"gh to determine that a problem has occurred. It could \\nmean that a user mistyped their password or e\", \"ntered some malformed data. The alert queries can be \\ncrafted to only fire when a larger than averag\", \"e number of 500 errors are detected. \\n\\nOne of the most damaging patterns in alerting is to fire too \", \"many alerts for humans to investigate. \\nService owners will rapidly become desensitized to errors th\", \"at they\\u2019ve previously investigated and \\nfound to be benign. Then, when true errors occur, they\\u2019ll be\", \" lost in the noise of hundreds of false \\npositives. The parable of the Boy Who Cried Wolf is frequen\", \"tly told to children to warn them of this \\nvery danger. It\\u2019s important to ensure that the alerts tha\", \"t do fire are indicative of a real problem. \\n\\nLogging with Elastic Stack \\n\\nThere are many good centr\", \"alized logging tools and they vary in cost from being free, open-source \\ntools, to more expensive op\", \"tions. In many cases, the free tools are as good as or better than the paid \\nofferings. One such too\", \"l is a combination of three open-source components: Elasticsearch, Logstash, \\nand Kibana. \\n\\nCollecti\", \"vely these tools are known as the Elastic Stack or ELK stack. \\n\\nElastic Stack \\n\\nThe Elastic Stack is\", \" a powerful option for gathering information from a Kubernetes cluster. Kubernetes \\nsupports sending\", \" logs to an Elasticsearch endpoint, and for the most part, all you need to get started \\nis to set th\", \"e environment variables as shown in Figure 7-5: \\n\\nKUBE_LOGGING_DESTINATION=elasticsearch \\nKUBE_ENABL\", \"E_NODE_LOGGING=true \\n\\nFigure 7-5. Configuration variables for Kubernetes \\n\\nThis step will install El\", \"asticsearch on the cluster and target sending all the cluster logs to it. \\n\\n131 \\n\\nCHAPTER 7 | Monito\", \"ring and health \\n\\n \\n \\n\\fFigure 7-6. An example of a Kibana dashboard showing the results of a query a\", \"gainst logs that are ingested from \\nKubernetes \\n\\nWhat are the advantages of Elastic Stack? \\n\\nElastic\", \" Stack provides centralized logging in a low-cost, scalable, cloud-friendly manner. Its user \\ninterf\", \"ace streamlines data analysis so you can spend your time gleaning insights from your data \\ninstead o\", \"f fighting with a clunky interface. It supports a wide variety of inputs so as your distributed \\napp\", \"lication spans more and different kinds of services, you can expect to continue to be able to feed \\n\", \"log and metric data into the system. The Elastic Stack also supports fast searches even across large\", \" \\ndata sets, making it possible even for large applications to log detailed data and still be able t\", \"o have \\nvisibility into it in a performant fashion. \\n\\nLogstash \\n\\nThe first component is Logstash. Th\", \"is tool is used to gather log information from a large variety of \\ndifferent sources. For instance, \", \"Logstash can read logs from disk and also receive messages from \\nlogging libraries like Serilog. Log\", \"stash can do some basic filtering and expansion on the logs as they \\narrive. For instance, if your l\", \"ogs contain IP addresses then Logstash may be configured to do a \\ngeographical lookup and obtain a c\", \"ountry/region or even city of origin for that message. \\n\\nSerilog is a logging library for .NET langu\", \"ages, which allows for parameterized logging. Instead of \\ngenerating a textual log message that embe\", \"ds fields, parameters are kept separate. This library allows \\nfor more intelligent filtering and sea\", \"rching. A sample Serilog configuration for writing to Logstash \\nappears in Figure 7-7. \\n\\nvar log = n\", \"ew LoggerConfiguration() \\n         .WriteTo.Http(\\\"http://localhost:8080\\\") \\n         .CreateLogger();\", \" \\n\\nFigure 7-7. Serilog config for writing log information directly to logstash over HTTP \\n\\nLogstash \", \"would use a configuration like the one shown in Figure 7-8. \\n\\n132 \\n\\nCHAPTER 7 | Monitoring and healt\", \"h \\n\\n \\n \\n \\n\\finput { \\n    http { \\n        #default host 0.0.0.0:8080 \\n        codec => json \\n    } \\n} \", \"\\n\\noutput { \\n    elasticsearch { \\n        hosts => \\\"elasticsearch:9200\\\" \\n        index=>\\\"sales-%{+xxx\", \"x.ww}\\\" \\n    } \\n} \\n\\nFigure 7-8. A Logstash configuration for consuming logs from Serilog \\n\\nFor scenar\", \"ios where extensive log manipulation isn\\u2019t needed there\\u2019s an alternative to Logstash known \\nas Beats\", \". Beats is a family of tools that can gather a wide variety of data from logs to network data \\nand u\", \"ptime information. Many applications will use both Logstash and Beats. \\n\\nOnce the logs have been gat\", \"hered by Logstash, it needs somewhere to put them. While Logstash \\nsupports many different outputs, \", \"one of the more exciting ones is Elasticsearch. \\n\\nElasticsearch \\n\\nElasticsearch is a powerful search\", \" engine that can index logs as they arrive. It makes running queries \\nagainst the logs quick. Elasti\", \"csearch can handle huge quantities of logs and, in extreme cases, can be \\nscaled out across many nod\", \"es. \\n\\nLog messages that have been crafted to contain parameters or that have had parameters split fr\", \"om \\nthem through Logstash processing, can be queried directly as Elasticsearch preserves this inform\", \"ation. \\n\\nA query that searches for the top 10 pages visited by jill@example.com, appears in Figure 7\", \"-9. \\n\\n\\\"query\\\": { \\n    \\\"match\\\": { \\n      \\\"user\\\": \\\"jill@example.com\\\" \\n    } \\n  }, \\n  \\\"aggregations\\\": {\", \" \\n    \\\"top_10_pages\\\": { \\n      \\\"terms\\\": { \\n        \\\"field\\\": \\\"page\\\", \\n        \\\"size\\\": 10 \\n      } \\n  \", \"  } \\n  } \\n\\nFigure 7-9. An Elasticsearch query for finding top 10 pages visited by a user \\n\\nVisualizi\", \"ng information with Kibana web dashboards \\n\\nThe final component of the stack is Kibana. This tool is\", \" used to provide interactive visualizations in a \\nweb dashboard. Dashboards may be crafted even by u\", \"sers who are non-technical. Most data that is \\nresident in the Elasticsearch index, can be included \", \"in the Kibana dashboards. Individual users may \\n\\n133 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n\\fha\", \"ve different dashboard desires and Kibana enables this customization through allowing user-\\nspecific\", \" dashboards. \\n\\nInstalling Elastic Stack on Azure \\n\\nThe Elastic stack can be installed on Azure in ma\", \"ny ways. As always, it\\u2019s possible to provision virtual \\nmachines and install Elastic Stack on them d\", \"irectly. This option is preferred by some experienced users \\nas it offers the highest degree of cust\", \"omizability. Deploying on infrastructure as a service introduces \\nsignificant management overhead fo\", \"rcing those who take that path to take ownership of all the tasks \\nassociated with infrastructure as\", \" a service such as securing the machines and keeping up-to-date with \\npatches. \\n\\nAn option with less\", \" overhead is to make use of one of the many Docker containers on which the \\nElastic Stack has alread\", \"y been configured. These containers can be dropped into an existing \\nKubernetes cluster and run alon\", \"gside application code. The sebp/elk container is a well-documented \\nand tested Elastic Stack contai\", \"ner. \\n\\nAnother option is a recently announced ELK-as-a-service offering. \\n\\nReferences \\n\\n\\u2022 \\n\\nInstall \", \"Elastic Stack on Azure \\n\\nMonitoring in Azure Kubernetes Services \\n\\nThe built-in logging in Kubernete\", \"s is primitive. However, there are some great options for getting the \\nlogs out of Kubernetes and in\", \"to a place where they can be properly analyzed. If you need to monitor \\nyour AKS clusters, configuri\", \"ng Elastic Stack for Kubernetes is a great solution. \\n\\nAzure Monitor for Containers \\n\\nAzure Monitor \", \"for Containers supports consuming logs from not just Kubernetes but also from other \\norchestration e\", \"ngines such as DC/OS, Docker Swarm, and Red Hat OpenShift. \\n\\n134 \\n\\nCHAPTER 7 | Monitoring and health\", \" \\n\\n \\n \\n\\fFigure 7-10. Consuming logs from various containers \\n\\nPrometheus is a popular open source me\", \"tric monitoring solution. It is part of the Cloud Native \\nCompute Foundation. Typically, using Prome\", \"theus requires managing a Prometheus server with its \\nown store. However, Azure Monitor for Containe\", \"rs provides direct integration with Prometheus \\nmetrics endpoints, so a separate server is not requi\", \"red. \\n\\nLog and metric information is gathered not just from the containers running in the cluster bu\", \"t also \\nfrom the cluster hosts themselves. It allows correlating log information from the two making\", \" it much \\neasier to track down an error. \\n\\nInstalling the log collectors differs on Windows and Linu\", \"x clusters. But in both cases the log collection \\nis implemented as a Kubernetes DaemonSet, meaning \", \"that the log collector is run as a container on \\neach of the nodes. \\n\\nNo matter which orchestrator o\", \"r operating system is running the Azure Monitor daemon, the log \\ninformation is forwarded to the sam\", \"e Azure Monitor tools with which users are familiar. This approach \\nensures a parallel experience in\", \" environments that mix different log sources such as a hybrid \\nKubernetes/Azure Functions environmen\", \"t. \\n\\n135 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n\\fFigure 7-11. A sample dashboard showing loggin\", \"g and metric information from many running containers. \\n\\nLog.Finalize() \\n\\nLogging is one of the most\", \" overlooked and yet most important parts of deploying any application at \\nscale. As the size and com\", \"plexity of applications increase, then so does the difficulty of debugging \\nthem. Having top quality\", \" logs available makes debugging much easier and moves it from the realm of \\n\\u201cnearly impossible\\u201d to \\u201c\", \"a pleasant experience\\u201d. \\n\\nAzure Monitor \\n\\nNo other cloud provider has as mature of a cloud applicati\", \"on monitoring solution than that found in \\nAzure. Azure Monitor is an umbrella name for a collection\", \" of tools designed to provide visibility into \\nthe state of your system. It helps you understand how\", \" your cloud-native services are performing and \\nproactively identifies issues affecting them. Figure\", \" 7-12 presents a high level of view of Azure Monitor. \\n\\n136 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n\", \" \\n \\n\\fFigure 7-12. High-level view of Azure Monitor. \\n\\nGathering logs and metrics \\n\\nThe first step in\", \" any monitoring solution is to gather as much data as possible. The more data \\ngathered, the deeper \", \"the insights. Instrumenting systems has traditionally been difficult. Simple \\nNetwork Management Pro\", \"tocol (SNMP) was the gold standard protocol for collecting machine level \\ninformation, but it requir\", \"ed a great deal of knowledge and configuration. Fortunately, much of this \\nhard work has been elimin\", \"ated as the most common metrics are gathered automatically by Azure \\nMonitor. \\n\\nApplication level me\", \"trics and events aren\\u2019t possible to instrument automatically because they\\u2019re \\nspecific to the applic\", \"ation being deployed. In order to gather these metrics, there are SDKs and APIs \\navailable to direct\", \"ly report such information, such as when a customer signs up or completes an order. \\nExceptions can \", \"also be captured and reported back into Azure Monitor via Application Insights. The \\nSDKs support mo\", \"st every language found in Cloud Native Applications including Go, Python, \\nJavaScript, and the .NET\", \" languages. \\n\\nThe ultimate goal of gathering information about the state of your application is to e\", \"nsure that your \\nend users have a good experience. What better way to tell if users are experiencing\", \" issues than doing \\noutside-in web tests? These tests can be as simple as pinging your website from \", \"locations around the \\nworld or as involved as having agents log into the site and simulate user acti\", \"ons. \\n\\nReporting data \\n\\nOnce the data is gathered, it can be manipulated, summarized, and plotted in\", \"to charts, which allow \\nusers to instantly see when there are problems. These charts can be gathered\", \" into dashboards or into \\nWorkbooks, a multi-page report designed to tell a story about some aspect \", \"of the system. \\n\\n137 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n\\fNo modern application would be com\", \"plete without some artificial intelligence or machine learning. To \\nthis end, data can be passed to \", \"the various machine learning tools in Azure to allow you to extract \\ntrends and information that wou\", \"ld otherwise be hidden. \\n\\nApplication Insights provides a powerful (SQL-like) query language called \", \"Kusto that can query \\nrecords, summarize them, and even plot charts. For example, the following quer\", \"y will locate all records \\nfor the month of November 2007, group them by state, and plot the top 10 \", \"as a pie chart. \\n\\nStormEvents \\n| where StartTime >= datetime(2007-11-01) and StartTime < datetime(20\", \"07-12-01) \\n| summarize count() by State \\n| top 10 by count_ \\n| render piechart \\n\\nFigure 7-13 shows t\", \"he results of this Application Insights Query. \\n\\nFigure 7-13. Application Insights query results. \\n\\n\", \"There is a playground for experimenting with Kusto queries. Reading sample queries can also be \\ninst\", \"ructive. \\n\\nDashboards \\n\\nThere are several different dashboard technologies that may be used to surfa\", \"ce the information from \\nAzure Monitor. Perhaps the simplest is to just run queries in Application I\", \"nsights and plot the data \\ninto a chart. \\n\\n138 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n \\n\\fFigure 7\", \"-14. An example of Application Insights charts embedded in the main Azure Dashboard. \\n\\nThese charts \", \"can then be embedded in the Azure portal proper through use of the dashboard feature. \\nFor users wit\", \"h more exacting requirements, such as being able to drill down into several tiers of data, \\nAzure Mo\", \"nitor data is available to Power BI. Power BI is an industry-leading, enterprise class, business \\nin\", \"telligence tool that can aggregate data from many different data sources. \\n\\n139 \\n\\nCHAPTER 7 | Monito\", \"ring and health \\n\\n \\n \\n \\n\\fFigure 7-15. An example Power BI dashboard. \\n\\nAlerts \\n\\nSometimes, having da\", \"ta dashboards is insufficient. If nobody is awake to watch the dashboards, then \\nit can still be man\", \"y hours before a problem is addressed, or even detected. To this end, Azure Monitor \\nalso provides a\", \" top notch alerting solution. Alerts can be triggered by a wide range of conditions \\nincluding: \\n\\n\\u2022 \", \" Metric values \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nLog search queries \\n\\nActivity Log events \\n\\nHealth of the underlying\", \" Azure platform \\n\\nTests for web site availability \\n\\nWhen triggered, the alerts can perform a wide va\", \"riety of tasks. On the simple side, the alerts may just \\nsend an e-mail notification to a mailing li\", \"st or a text message to an individual. More involved alerts \\n\\n140 \\n\\nCHAPTER 7 | Monitoring and healt\", \"h \\n\\n \\n \\n \\n\\fmight trigger a workflow in a tool such as PagerDuty, which is aware of who is on call fo\", \"r a particular \\napplication. Alerts can trigger actions in Microsoft Flow unlocking near limitless p\", \"ossibilities for \\nworkflows. \\n\\nAs common causes of alerts are identified, the alerts can be enhanced\", \" with details about the common \\ncauses of the alerts and the steps to take to resolve them. Highly m\", \"ature cloud-native application \\ndeployments may opt to kick off self-healing tasks, which perform ac\", \"tions such as removing failing \\nnodes from a scale set or triggering an autoscaling activity. Eventu\", \"ally it may no longer be necessary \\nto wake up on-call personnel at 2AM to resolve a live-site issue\", \" as the system will be able to adjust \\nitself to compensate or at least limp along until somebody ar\", \"rives at work the next morning. \\n\\nAzure Monitor automatically leverages machine learning to understa\", \"nd the normal operating \\nparameters of deployed applications. This approach enables it to detect ser\", \"vices that are operating \\noutside of their normal parameters. For instance, the typical weekday traf\", \"fic on the site might be \\n10,000 requests per minute. And then, on a given week, suddenly the number\", \" of requests hits a highly \\nunusual 20,000 requests per minute. Smart Detection will notice this dev\", \"iation from the norm and \\ntrigger an alert. At the same time, the trend analysis is smart enough to \", \"avoid firing false positives \\nwhen the traffic load is expected. \\n\\nReferences \\n\\n\\u2022 \\n\\nAzure Monitor \\n\\n\", \"141 \\n\\nCHAPTER 7 | Monitoring and health \\n\\n \\n \\n\\fCHAPTER  8 \\n\\nCloud-native identity \\n\\nMost software ap\", \"plications need to have some knowledge of the user or process that is calling them. \\nThe user or pro\", \"cess interacting with an application is known as a security principal, and the process of \\nauthentic\", \"ating and authorizing these principals is known as identity management, or simply identity. \\nSimple \", \"applications may include all of their identity management within the application, but this \\napproach\", \" doesn\\u2019t scale well with many applications and many kinds of security principals. Windows \\nsupports \", \"the use of Active Directory to provide centralized authentication and authorization. \\n\\nWhile this so\", \"lution is effective within corporate networks, it isn\\u2019t designed for use by users or \\napplications t\", \"hat are outside of the AD domain. With the growth of Internet-based applications and \\nthe rise of cl\", \"oud-native apps, security models have evolved. \\n\\nIn today\\u2019s cloud-native identity model, architectur\", \"e is assumed to be distributed. Apps can be \\ndeployed anywhere and may communicate with other apps a\", \"nywhere. Clients may communicate with \\nthese apps from anywhere, and in fact, clients may consist of\", \" any combination of platforms and \\ndevices. Cloud-native identity solutions use open standards to ac\", \"hieve secure application access from \\nclients. These clients range from human users on PCs or phones\", \", to other apps hosted anywhere \\nonline, to set-top boxes and IOT devices running any software platf\", \"orm anywhere in the world. \\n\\nModern cloud-native identity solutions typically use access tokens that\", \" are issued by a secure token \\nservice/server (STS) to a security principal once their identity is d\", \"etermined. The access token, typically \\na JSON Web Token (JWT), includes claims about the security p\", \"rincipal. These claims will minimally \\ninclude the user\\u2019s identity but may also include other claims\", \" that can be used by applications to \\ndetermine the level of access to grant the principal. \\n\\nTypica\", \"lly, the STS is only responsible for authenticating the principal. Determining their level of access\", \" \\nto resources is left to other parts of the application. \\n\\nReferences \\n\\n\\u2022  Microsoft identity platf\", \"orm \\n\\nAuthentication and authorization in cloud-native \\napps \\n\\nAuthentication is the process of dete\", \"rmining the identity of a security principal. Authorization is the act \\nof granting an authenticated\", \" principal permission to perform an action or access a resource. \\nSometimes authentication is shorte\", \"ned to AuthN and authorization is shortened to AuthZ. Cloud-\\n\\n142 \\n\\nCHAPTER 8 | Cloud-native identit\", \"y \\n\\n \\n \\n\\fnative applications need to rely on open HTTP-based protocols to authenticate security prin\", \"cipals \\nsince both clients and applications could be running anywhere in the world on any platform o\", \"r device. \\nThe only common factor is HTTP. \\n\\nMany organizations still rely on local authentication s\", \"ervices like Active Directory Federation Services \\n(ADFS). While this approach has traditionally ser\", \"ved organizations well for on premises authentication \\nneeds, cloud-native applications benefit from\", \" systems designed specifically for the cloud. A recent \\n2019 United Kingdom National Cyber Security \", \"Centre (NCSC) advisory states that \\u201corganizations using \\nAzure AD as their primary authentication so\", \"urce will actually lower their risk compared to ADFS.\\u201d \\nSome reasons outlined in this analysis inclu\", \"de: \\n\\n\\u2022 \\n\\nAccess to full set of Microsoft credential protection technologies. \\n\\n\\u2022  Most organization\", \"s are already relying on Azure AD to some extent. \\n\\n\\u2022 \\n\\nDouble hashing of NTLM hashes ensures compro\", \"mise won\\u2019t allow credentials that work in \\nlocal Active Directory. \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAuthen\", \"tication basics \\n\\nAccess tokens and claims \\n\\nIt may be time to ditch your on premises authentication\", \" services \\n\\nAzure Active Directory \\n\\nMicrosoft Azure Active Directory (Azure AD) offers identity and\", \" access management as a service. \\nCustomers use it to configure and maintain who users are, what inf\", \"ormation to store about them, who \\ncan access that information, who can manage it, and what apps can\", \" access it. AAD can authenticate \\nusers for applications configured to use it, providing a single si\", \"gn-on (SSO) experience. It can be used \\non its own or be integrated with Windows AD running on premi\", \"ses. \\n\\nAzure AD is built for the cloud. It\\u2019s truly a cloud-native identity solution that uses a REST\", \"-based Graph \\nAPI and OData syntax for queries, unlike Windows AD, which uses LDAP. On premises Acti\", \"ve Directory \\ncan sync user attributes to the cloud using Identity Sync Services, allowing all authe\", \"ntication to take \\nplace in the cloud using Azure AD. Alternately, authentication can be configured \", \"via Connect to pass \\nback to local Active Directory via ADFS to be completed by Windows AD on premis\", \"es. \\n\\nAzure AD supports company branded sign-in screens, multi-factory authentication, and cloud-bas\", \"ed \\napplication proxies that are used to provide SSO for applications hosted on premises. It offers \", \"\\ndifferent kinds of security reporting and alert capabilities. \\n\\nReferences \\n\\n\\u2022  Microsoft identity \", \"platform \\n\\n143 \\n\\nCHAPTER 8 | Cloud-native identity \\n\\n \\n \\n\\fIdentityServer for cloud-native applicatio\", \"ns \\n\\nIdentityServer is an authentication server that implements OpenID Connect (OIDC) and OAuth 2.0 \", \"\\nstandards for ASP.NET Core. It\\u2019s designed to provide a common way to authenticate requests to all o\", \"f \\nyour applications, whether they\\u2019re web, native, mobile, or API endpoints. IdentityServer can be u\", \"sed to \\nimplement Single Sign-On (SSO) for multiple applications and application types. It can be us\", \"ed to \\nauthenticate actual users via sign-in forms and similar user interfaces as well as service-ba\", \"sed \\nauthentication that typically involves token issuance, verification, and renewal without any us\", \"er \\ninterface. IdentityServer is designed to be a customizable solution. Each instance is typically \", \"\\ncustomized to suit an individual organization and/or set of applications\\u2019 needs. \\n\\nCommon web app s\", \"cenarios \\n\\nTypically, applications need to support some or all of the following scenarios: \\n\\n\\u2022 \\n\\n\\u2022 \\n\", \"\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nHuman users accessing web applications with a browser. \\n\\nHuman users accessing back-end\", \" Web APIs from browser-based apps. \\n\\nHuman users on mobile/native clients accessing back-end Web API\", \"s. \\n\\nOther applications accessing back-end Web APIs (without an active user or user interface). \\n\\nAn\", \"y application may need to interact with other Web APIs, using its own identity or \\ndelegating to the\", \" user\\u2019s identity. \\n\\nFigure 8-1. Application types and scenarios. \\n\\nIn each of these scenarios, the e\", \"xposed functionality needs to be secured against unauthorized use. At \\na minimum, this typically req\", \"uires authenticating the user or principal making a request for a resource. \\nThis authentication may\", \" use one of several common protocols such as SAML2p, WS-Fed, or OpenID \\nConnect. Communicating with \", \"APIs typically uses the OAuth2 protocol and its support for security \\ntokens. Separating these criti\", \"cal cross-cutting security concerns and their implementation details from \\nthe applications themselv\", \"es ensures consistency and improves security and maintainability. \\n\\n144 \\n\\nCHAPTER 8 | Cloud-native i\", \"dentity \\n\\n \\n \\n \\n\\fOutsourcing these concerns to a dedicated product like IdentityServer helps the req\", \"uirement for every \\napplication to solve these problems itself. \\n\\nIdentityServer provides middleware\", \" that runs within an ASP.NET Core application and adds support \\nfor OpenID Connect and OAuth2 (see s\", \"upported specifications). Organizations would create their own \\nASP.NET Core app using IdentityServe\", \"r middleware to act as the STS for all of their token-based \\nsecurity protocols. The IdentityServer \", \"middleware exposes endpoints to support standard \\nfunctionality, including: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \", \"\\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAuthorize (authenticate the end user) \\n\\nToken (request a token programmatically) \\n\\nDiscove\", \"ry (metadata about the server) \\n\\nUser Info (get user information with a valid access token) \\n\\nDevice\", \" Authorization (used to start device flow authorization) \\n\\nIntrospection (token validation) \\n\\nRevoca\", \"tion (token revocation) \\n\\nEnd Session (trigger single sign-out across all apps) \\n\\nGetting started \\n\\n\", \"IdentityServer4 is available under dual license: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nRPL - lets you use the IdentityServer4 fr\", \"ee if used in open-source work \\n\\nPaid - lets you use the IdentityServer4 in a commercial scenario \\n\\n\", \"For more information about pricing, see the official product\\u2019s pricing page. \\n\\nYou can add it to you\", \"r applications using its NuGet packages. The main package is IdentityServer4, \\nwhich has been downlo\", \"aded over four million times. The base package doesn\\u2019t include any user \\ninterface code and only sup\", \"ports in-memory configuration. To use it with a database, you\\u2019ll also want \\na data provider like Ide\", \"ntityServer4.EntityFramework, which uses Entity Framework Core to store \\nconfiguration and operation\", \"al data for IdentityServer. For user interface, you can copy files from the \\nQuickstart UI repositor\", \"y into your ASP.NET Core MVC application to add support for sign in and sign \\nout using IdentityServ\", \"er middleware. \\n\\nConfiguration \\n\\nIdentityServer supports different kinds of protocols and social aut\", \"hentication providers that can be \\nconfigured as part of each custom installation. This is typically\", \" done in the ASP.NET Core application\\u2019s \\nProgram class (or in the Startup class in the ConfigureServ\", \"ices method). The configuration involves \\nspecifying the supported protocols and the paths to the se\", \"rvers and endpoints that will be used. \\nFigure 8-2 shows an example configuration taken from the Ide\", \"ntityServer4 Quickstart UI project: \\n\\npublic class Startup \\n{ \\n    public void ConfigureServices(ISe\", \"rviceCollection services) \\n    { \\n        services.AddMvc(); \\n\\n145 \\n\\nCHAPTER 8 | Cloud-native identi\", \"ty \\n\\n \\n \\n \\n\\f        // some details omitted \\n        services.AddIdentityServer(); \\n\\n          servi\", \"ces.AddAuthentication() \\n            .AddGoogle(\\\"Google\\\", options => \\n            { \\n               \", \" options.SignInScheme = \\nIdentityServerConstants.ExternalCookieAuthenticationScheme; \\n\\n             \", \"   options.ClientId = \\\"<insert here>\\\"; \\n                options.ClientSecret = \\\"<insert here>\\\"; \\n   \", \"         }) \\n            .AddOpenIdConnect(\\\"demoidsrv\\\", \\\"IdentityServer\\\", options => \\n            { \", \"\\n                options.SignInScheme = \\nIdentityServerConstants.ExternalCookieAuthenticationScheme;\", \" \\n                options.SignOutScheme = IdentityServerConstants.SignoutScheme; \\n\\n                o\", \"ptions.Authority = \\\"https://demo.identityserver.io/\\\"; \\n                options.ClientId = \\\"implicit\\\"\", \"; \\n                options.ResponseType = \\\"id_token\\\"; \\n                options.SaveTokens = true; \\n \", \"               options.CallbackPath = new PathString(\\\"/signin-idsrv\\\"); \\n                options.Sign\", \"edOutCallbackPath = new PathString(\\\"/signout-callback-idsrv\\\"); \\n                options.RemoteSignOu\", \"tPath = new PathString(\\\"/signout-idsrv\\\"); \\n\\n                options.TokenValidationParameters = new \", \"TokenValidationParameters \\n                { \\n                    NameClaimType = \\\"name\\\", \\n         \", \"           RoleClaimType = \\\"role\\\" \\n                }; \\n            }); \\n    } \\n} \\n\\nFigure 8-2. Confi\", \"guring IdentityServer. \\n\\nJavaScript clients \\n\\nMany cloud-native applications use server-side APIs an\", \"d rich client single page applications (SPAs) on \\nthe front end. IdentityServer ships a JavaScript c\", \"lient (oidc-client.js) via NPM that can be added to \\nSPAs to enable them to use IdentityServer for s\", \"ign in, sign out, and token-based authentication of \\nweb APIs. \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nIdentitySe\", \"rver documentation \\n\\nApplication types \\n\\nJavaScript OIDC client \\n\\n146 \\n\\nCHAPTER 8 | Cloud-native ide\", \"ntity \\n\\n \\n \\n \\n \\n \\n \\n\\fCHAPTER  9 \\n\\nCloud-native security \\n\\nNot a day goes by where the news doesn\\u2019t c\", \"ontain some story about a company being hacked or \\nsomehow losing their customers\\u2019 data. Even countr\", \"ies/regions aren\\u2019t immune to the problems created \\nby treating security as an afterthought. For year\", \"s, companies have treated the security of customer \\ndata and, in fact, their entire networks as some\", \"thing of a \\u201cnice to have\\u201d. Windows servers were left \\nunpatched, ancient versions of PHP kept runnin\", \"g, and MongoDB databases left wide open to the \\nworld. \\n\\nHowever, there are starting to be real-worl\", \"d consequences for not maintaining a security mindset \\nwhen building and deploying applications. Man\", \"y companies learned the hard way what can happen \\nwhen servers and desktops aren\\u2019t patched during th\", \"e 2017 outbreak of NotPetya. The cost of these \\nattacks has easily reached into the billions, with s\", \"ome estimates putting the losses from this single \\nattack at 10 billion US dollars. \\n\\nEven governmen\", \"ts aren\\u2019t immune to hacking incidents. The city of Baltimore was held ransom by \\ncriminals making it\", \" impossible for citizens to pay their bills or use city services. \\n\\nThere has also been an increase \", \"in legislation that mandates certain data protections for personal \\ndata. In Europe, GDPR has been i\", \"n effect for more than a year and, more recently, California passed \\ntheir own version called CCDA, \", \"which comes into effect January 1, 2020. The fines under GDPR can be \\nso punishing as to put compani\", \"es out of business. Google has already been fined 50 million Euros for \\nviolations, but that\\u2019s just \", \"a drop in the bucket compared with the potential fines. \\n\\nIn short, security is serious business. \\n\\n\", \"Azure security for cloud-native apps \\n\\nCloud-native applications can be both easier and more difficu\", \"lt to secure than traditional applications. \\nOn the downside, you need to secure more smaller applic\", \"ations and dedicate more energy to build \\nout the security infrastructure. The heterogeneous nature \", \"of programming languages and styles in \\nmost service deployments also means you need to pay more att\", \"ention to security bulletins from many \\ndifferent providers. \\n\\nOn the flip side, smaller services, e\", \"ach with their own data store, limit the scope of an attack. If an \\nattacker compromises one system,\", \" it\\u2019s probably more difficult for the attacker to make the jump to \\nanother system than it is in a m\", \"onolithic application. Process boundaries are strong boundaries. Also, \\nif a database backup gets ex\", \"posed, then the damage is more limited, as that database contains only a \\nsubset of data and is unli\", \"kely to contain personal data. \\n\\n147 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n\\fThreat modeling \\n\\nNo\", \" matter if the advantages outweigh the disadvantages of cloud-native applications, the same \\nholisti\", \"c security mindset must be followed. Security and secure thinking must be part of every step of \\nthe\", \" development and operations story. When planning an application ask questions like: \\n\\n\\u2022  What would \", \"be the impact of this data being lost? \\n\\n\\u2022 \\n\\nHow can we limit the damage from bad data being injecte\", \"d into this service? \\n\\n\\u2022  Who should have access to this data? \\n\\n\\u2022 \\n\\nAre there auditing policies in \", \"place around the development and release process? \\n\\nAll these questions are part of a process called\", \" threat modeling. This process tries to answer the \\nquestion of what threats there are to the system\", \", how likely the threats are, and the potential damage \\nfrom them. \\n\\nOnce the list of threats has be\", \"en established, you need to decide whether they\\u2019re worth mitigating. \\nSometimes a threat is so unlik\", \"ely and expensive to plan for that it isn\\u2019t worth spending energy on it. \\nFor instance, some state l\", \"evel actor could inject changes into the design of a process that is used by \\nmillions of devices. N\", \"ow, instead of running a certain piece of code in Ring 3, that code is run in Ring \\n0. This process \", \"allows an exploit that can bypass the hypervisor and run the attack code on the bare \\nmetal machines\", \", allowing attacks on all the virtual machines that are running on that hardware. \\n\\nThe altered proc\", \"essors are difficult to detect without a microscope and advanced knowledge of the on \\nsilicon design\", \" of that processor. This scenario is unlikely to happen and expensive to mitigate, so \\nprobably no t\", \"hreat model would recommend building exploit protection for it. \\n\\nMore likely threats, such as broke\", \"n access controls permitting Id incrementing attacks (replacing Id=2 \\nwith Id=3 in the URL) or SQL i\", \"njection, are more attractive to build protections against. The \\nmitigations for these threats are q\", \"uite reasonable to build and prevent embarrassing security holes \\nthat smear the company\\u2019s reputatio\", \"n. \\n\\nPrinciple of least privilege \\n\\nOne of the founding ideas in computer security is the Principle \", \"of Least Privilege (POLP). It\\u2019s actually a \\nfoundational idea in most any form of security be it dig\", \"ital or physical. In short, the principle is that \\nany user or process should have the smallest numb\", \"er of rights possible to execute its task. \\n\\nAs an example, think of the tellers at a bank: accessin\", \"g the safe is an uncommon activity. So, the \\naverage teller can\\u2019t open the safe themselves. To gain \", \"access, they need to escalate their request \\nthrough a bank manager, who performs additional securit\", \"y checks. \\n\\nIn a computer system, a fantastic example is the rights of a user connecting to a databa\", \"se. In many \\ncases, there\\u2019s a single user account used to both build the database structure and run \", \"the application. \\nExcept in extreme cases, the account running the application doesn\\u2019t need the abil\", \"ity to update \\nschema information. There should be several accounts that provide different levels of\", \" privilege. The \\napplication should only use the permission level that grants read and writes access\", \" to the data in the \\ntables. This kind of protection would eliminate attacks that aimed to drop data\", \"base tables or \\nintroduce malicious triggers. \\n\\n148 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n\\fAlmos\", \"t every part of building a cloud-native application can benefit from remembering the principle \\nof l\", \"east privilege. You can find it at play when setting up firewalls, network security groups, roles, a\", \"nd \\nscopes in Role-based access control (RBAC). \\n\\nPenetration testing \\n\\nAs applications become more \", \"complicated the number of attack vectors increases at an alarming rate. \\nThreat modeling is flawed i\", \"n that it tends to be executed by the same people building the system. In \\nthe same way that many de\", \"velopers have trouble envisioning user interactions and then build \\nunusable user interfaces, most d\", \"evelopers have difficulty seeing every attack vector. It\\u2019s also possible \\nthat the developers buildi\", \"ng the system aren\\u2019t well versed in attack methodologies and miss \\nsomething crucial. \\n\\nPenetration \", \"testing or \\u201cpen testing\\u201d involves bringing in external actors to attempt to attack the \\nsystem. Thes\", \"e attackers may be an external consulting company or other developers with good \\nsecurity knowledge \", \"from another part of the business. They\\u2019re given carte blanche to attempt to \\nsubvert the system. Fr\", \"equently, they\\u2019ll find extensive security holes that need to be patched. \\nSometimes the attack vecto\", \"r will be something totally unexpected like exploiting a phishing attack \\nagainst the CEO. \\n\\nAzure i\", \"tself is constantly undergoing attacks from a team of hackers inside Microsoft. Over the years, \\nthe\", \"y\\u2019ve been the first to find dozens of potentially catastrophic attack vectors, closing them before \\n\", \"they can be exploited externally. The more tempting a target, the more likely that eternal actors wi\", \"ll \\nattempt to exploit it and there are a few targets in the world more tempting than Azure. \\n\\nMonit\", \"oring \\n\\nShould an attacker attempt to penetrate an application, there should be some warning of it. \", \"\\nFrequently, attacks can be spotted by examining the logs from services. Attacks leave telltale sign\", \"s \\nthat can be spotted before they succeed. For instance, an attacker attempting to guess a password\", \" \\nwill make many requests to a login system. Monitoring around the login system can detect weird \\npa\", \"tterns that are out of line with the typical access pattern. This monitoring can be turned into an \\n\", \"alert that can, in turn, alert an operations person to activate some sort of countermeasure. A highl\", \"y \\nmature monitoring system might even take action based on these deviations proactively adding rule\", \"s \\nto block requests or throttle responses. \\n\\nSecuring the build \\n\\nOne place where security is often\", \" overlooked is around the build process. Not only should the build \\nrun security checks, such as sca\", \"nning for insecure code or checked-in credentials, but the build itself \\nshould be secure. If the bu\", \"ild server is compromised, then it provides a fantastic vector for introducing \\narbitrary code into \", \"the product. \\n\\nImagine that an attacker is looking to steal the passwords of people signing into a w\", \"eb application. \\nThey could introduce a build step that modifies the checked-out code to mirror any \", \"login request to \\nanother server. The next time code goes through the build, it\\u2019s silently updated. \", \"The source code \\nvulnerability scanning won\\u2019t catch this vulnerability as it runs before the build. \", \"Equally, nobody will \\n\\n149 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n\\fcatch it in a code review beca\", \"use the build steps live on the build server. The exploited code will go to \\nproduction where it can\", \" harvest passwords. Probably there\\u2019s no audit log of the build process \\nchanges, or at least nobody \", \"monitoring the audit. \\n\\nThis scenario is a perfect example of a seemingly low-value target that can \", \"be used to break into the \\nsystem. Once an attacker breaches the perimeter of the system, they can s\", \"tart working on finding \\nways to elevate their permissions to the point that they can cause real har\", \"m anywhere they like. \\n\\nBuilding secure code \\n\\n.NET Framework is already a quite secure framework. I\", \"t avoids some of the pitfalls of unmanaged \\ncode, such as walking off the ends of arrays. Work is ac\", \"tively done to fix security holes as they\\u2019re \\ndiscovered. There\\u2019s even a bug bounty program that pay\", \"s researchers to find issues in the framework \\nand report them instead of exploiting them. \\n\\nThere a\", \"re many ways to make .NET code more secure. Following guidelines such as the Secure coding \\nguidelin\", \"es for .NET article is a reasonable step to take to ensure that the code is secure from the \\nground \", \"up. The OWASP top 10 is another invaluable guide to build secure code. \\n\\nThe build process is a good\", \" place to put scanning tools to detect problems in source code before they \\nmake it into production.\", \" Most every project has dependencies on some other packages. A tool that \\ncan scan for outdated pack\", \"ages will catch problems in a nightly build. Even when building Docker \\nimages, it\\u2019s useful to check\", \" and make sure that the base image doesn\\u2019t have known vulnerabilities. \\nAnother thing to check is th\", \"at nobody has accidentally checked in credentials. \\n\\nBuilt-in security \\n\\nAzure is designed to balanc\", \"e usability and security for most users. Different users are going to have \\ndifferent security requi\", \"rements, so they need to fine-tune their approach to cloud security. Microsoft \\npublishes a great de\", \"al of security information in the Trust Center. This resource should be the first \\nstop for those pr\", \"ofessionals interested in understanding how the built-in attack mitigation \\ntechnologies work. \\n\\nWit\", \"hin the Azure portal, the Azure Advisor is a system that is constantly scanning an environment and \\n\", \"making recommendations. Some of these recommendations are designed to save users money, but \\nothers \", \"are designed to identify potentially insecure configurations, such as having a storage container \\nop\", \"en to the world and not protected by a Virtual Network. \\n\\nAzure network infrastructure \\n\\nIn an on-pr\", \"emises deployment environment, a great deal of energy is dedicated to setting up \\nnetworking. Settin\", \"g up routers, switches, and the such is complicated work. Networks allow certain \\nresources to talk \", \"to other resources and prevent access in some cases. A frequent network rule is to \\nrestrict access \", \"to the production environment from the development environment on the off chance \\nthat a half-develo\", \"ped piece of code runs awry and deletes a swath of data. \\n\\nOut of the box, most PaaS Azure resources\", \" have only the most basic and permissive networking setup. \\nFor instance, anybody on the Internet ca\", \"n access an app service. New SQL Server instances typically \\n\\n150 \\n\\nCHAPTER 9 | Cloud-native securit\", \"y \\n\\n \\n \\n\\fcome restricted, so that external parties can\\u2019t access them, but the IP address ranges used\", \" by Azure \\nitself are permitted through. So, while the SQL server is protected from external threats\", \", an attacker \\nonly needs to set up an Azure bridgehead from where they can launch attacks against a\", \"ll SQL \\ninstances on Azure. \\n\\nFortunately, most Azure resources can be placed into an Azure Virtual \", \"Network that allows fine-\\ngrained access control. Similar to the way that on-premises networks estab\", \"lish private networks that \\nare protected from the wider world, virtual networks are islands of priv\", \"ate IP addresses that are \\nlocated within the Azure network. \\n\\nFigure 9-1. A virtual network in Azur\", \"e. \\n\\nIn the same way that on-premises networks have a firewall governing access to the network, you \", \"can \\nestablish a similar firewall at the boundary of the virtual network. By default, all the resour\", \"ces on a \\nvirtual network can still talk to the Internet. It\\u2019s only incoming connections that requir\", \"e some form of \\nexplicit firewall exception. \\n\\nWith the network established, internal resources like\", \" storage accounts can be set up to only allow for \\naccess by resources that are also on the Virtual \", \"Network. This firewall provides an extra level of \\nsecurity, should the keys for that storage accoun\", \"t be leaked, attackers wouldn\\u2019t be able to connect to \\nit to exploit the leaked keys. This scenario \", \"is another example of the principle of least privilege. \\n\\nThe nodes in an Azure Kubernetes cluster c\", \"an participate in a virtual network just like other resources \\nthat are more native to Azure. This f\", \"unctionality is called Azure Container Networking Interface. In \\neffect, it allocates a subnet withi\", \"n the virtual network on which virtual machines and container images \\nare allocated. \\n\\nContinuing do\", \"wn the path of illustrating the principle of least privilege, not every resource within a \\nVirtual N\", \"etwork needs to talk to every other resource. For instance, in an application that provides a \\n\\n151 \", \"\\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n \\n\\fweb API over a storage account and a SQL database, it\\u2019s\", \" unlikely that the database and the storage \\naccount need to talk to one another. Any data sharing b\", \"etween them would go through the web \\napplication. So, a network security group (NSG) could be used \", \"to deny traffic between the two \\nservices. \\n\\nA policy of denying communication between resources can\", \" be annoying to implement, especially \\ncoming from a background of using Azure without traffic restr\", \"ictions. On some other clouds, the \\nconcept of network security groups is much more prevalent. For i\", \"nstance, the default policy on AWS is \\nthat resources can\\u2019t communicate among themselves until enabl\", \"ed by rules in an NSG. While slower \\nto develop this, a more restrictive environment provides a more\", \" secure default. Making use of proper \\nDevOps practices, especially using Azure Resource Manager or \", \"Terraform to manage permissions can \\nmake controlling the rules easier. \\n\\nVirtual Networks can also \", \"be useful when setting up communication between on-premises and cloud \\nresources. A virtual private \", \"network can be used to seamlessly attach the two networks together. This \\napproach allows running a \", \"virtual network without any sort of gateway for scenarios where all the \\nusers are on-site. There ar\", \"e a number of technologies that can be used to establish this network. The \\nsimplest is to use a sit\", \"e-to-site VPN that can be established between many routers and Azure. Traffic \\nis encrypted and tunn\", \"eled over the Internet at the same cost per byte as any other traffic. For \\nscenarios where more ban\", \"dwidth or more security is desirable, Azure offers a service called Express \\nRoute that uses a priva\", \"te circuit between an on-premises network and Azure. It\\u2019s more costly and \\ndifficult to establish bu\", \"t also more secure. \\n\\nRole-based access control for restricting access to Azure resources \\n\\nRBAC is \", \"a system that provides an identity to applications running in Azure. Applications can access \\nresour\", \"ces using this identity instead of or in addition to using keys or passwords. \\n\\nSecurity Principals \", \"\\n\\nThe first component in RBAC is a security principal. A security principal can be a user, group, se\", \"rvice \\nprincipal, or managed identity. \\n\\nFigure 9-2. Different types of security principals. \\n\\n\\u2022 \\n\\n\\u2022\", \" \\n\\n\\u2022 \\n\\n152 \\n\\nUser - Any user who has an account in Azure Active Directory is a user. \\n\\nGroup - A col\", \"lection of users from Azure Active Directory. As a member of a group, a user \\ntakes on the roles of \", \"that group in addition to their own. \\n\\nService principal - A security identity under which services \", \"or applications run. \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n \\n\\f\\u2022  Managed identity - An Azure Act\", \"ive Directory identity managed by Azure. Managed identities \\nare typically used when developing clou\", \"d applications that manage the credentials for \\nauthenticating to Azure services. \\n\\nThe security pri\", \"ncipal can be applied to most any resource. This aspect means that it\\u2019s possible to \\nassign a securi\", \"ty principal to a container running within Azure Kubernetes, allowing it to access secrets \\nstored i\", \"n Key Vault. An Azure Function could take on a permission allowing it to talk to an Active \\nDirector\", \"y instance to validate a JWT for a calling user. Once services are enabled with a service \\nprincipal\", \", their permissions can be managed granularly using roles and scopes. \\n\\nRoles \\n\\nA security principal\", \" can take on many roles or, using a more sartorial analogy, wear many hats. Each \\nrole defines a ser\", \"ies of permissions such as \\u201cRead messages from Azure Service Bus endpoint\\u201d. The \\neffective permissio\", \"n set of a security principal is the combination of all the permissions assigned to all \\nthe roles t\", \"hat a security principal has. Azure has a large number of built-in roles and users can define \\ntheir\", \" own roles. \\n\\nFigure 9-3. RBAC role definitions. \\n\\nBuilt into Azure are also a number of high-level \", \"roles such as Owner, Contributor, Reader, and User \\nAccount Administrator. With the Owner role, a se\", \"curity principal can access all resources and assign \\npermissions to others. A contributor has the s\", \"ame level of access to all resources but they can\\u2019t assign \\npermissions. A Reader can only view exis\", \"ting Azure resources and a User Account Administrator can \\nmanage access to Azure resources. \\n\\nMore \", \"granular built-in roles such as DNS Zone Contributor have rights limited to a single service. \\nSecur\", \"ity principals can take on any number of roles. \\n\\n153 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n \\n\\fS\", \"copes \\n\\nRoles can be applied to a restricted set of resources within Azure. For instance, applying s\", \"cope to the \\nprevious example of reading from a Service Bus queue, you can narrow the permission to \", \"a single \\nqueue: \\u201cRead messages from Azure Service Bus endpoint blah.servicebus.windows.net/queue1\\u201d \", \"\\n\\nThe scope can be as narrow as a single resource or it can be applied to an entire resource group, \", \"\\nsubscription, or even management group. \\n\\nWhen testing if a security principal has certain permissi\", \"on, the combination of role and scope are \\ntaken into account. This combination provides a powerful \", \"authorization mechanism. \\n\\nDeny \\n\\nPreviously, only \\u201callow\\u201d rules were permitted for RBAC. This behav\", \"ior made some scopes complicated \\nto build. For instance, allowing a security principal access to al\", \"l storage accounts except one required \\ngranting explicit permission to a potentially endless list o\", \"f storage accounts. Every time a new storage \\naccount was created, it would have to be added to this\", \" list of accounts. This added management \\noverhead that certainly wasn\\u2019t desirable. \\n\\nDeny rules tak\", \"e precedence over allow rules. Now representing the same \\u201callow all but one\\u201d scope \\ncould be represe\", \"nted as two rules \\u201callow all\\u201d and \\u201cdeny this one specific one\\u201d. Deny rules not only \\nease management\", \" but allow for resources that are extra secure by denying access to everybody. \\n\\nChecking access \\n\\nA\", \"s you can imagine, having a large number of roles and scopes can make figuring out the effective \\npe\", \"rmission of a service principal quite difficult. Piling deny rules on top of that, only serves to in\", \"crease \\nthe complexity. Fortunately, there\\u2019s a permissions calculator that can show the effective pe\", \"rmissions \\nfor any service principal. It\\u2019s typically found under the IAM tab in the portal, as shown\", \" in Figure 9-3. \\n\\nFigure 9-4. Permission calculator for an app service. \\n\\n154 \\n\\nCHAPTER 9 | Cloud-na\", \"tive security \\n\\n \\n \\n \\n\\fSecuring secrets \\n\\nPasswords and certificates are a common attack vector for \", \"attackers. Password-cracking hardware can \\ndo a brute-force attack and try to guess billions of pass\", \"words per second. So it\\u2019s important that the \\npasswords that are used to access resources are strong\", \", with a large variety of characters. These \\npasswords are exactly the kind of passwords that are ne\", \"ar impossible to remember. Fortunately, the \\npasswords in Azure don\\u2019t actually need to be known by a\", \"ny human. \\n\\nMany security experts suggest that using a password manager to keep your own passwords i\", \"s the \\nbest approach. While it centralizes your passwords in one location, it also allows using high\", \"ly complex \\npasswords and ensuring they\\u2019re unique for each account. The same system exists within Az\", \"ure: a \\ncentral store for secrets. \\n\\nAzure Key Vault \\n\\nAzure Key Vault provides a centralized locati\", \"on to store passwords for things such as databases, API \\nkeys, and certificates. Once a secret is en\", \"tered into the Vault, it\\u2019s never shown again and the \\ncommands to extract and view it are purposeful\", \"ly complicated. The information in the safe is \\nprotected using either software encryption or FIPS 1\", \"40-2 Level 2 validated Hardware Security \\nModules. \\n\\nAccess to the key vault is provided through RBA\", \"Cs, meaning that not just any user can access the \\ninformation in the vault. Say a web application w\", \"ishes to access the database connection string stored \\nin Azure Key Vault. To gain access, applicati\", \"ons need to run using a service principal. Under this \\nassumed role, they can read the secrets from \", \"the safe. There are a number of different security \\nsettings that can further limit the access that \", \"an application has to the vault, so that it can\\u2019t update \\nsecrets but only read them. \\n\\nAccess to th\", \"e key vault can be monitored to ensure that only the expected applications are accessing \\nthe vault.\", \" The logs can be integrated back into Azure Monitor, unlocking the ability to set up alerts \\nwhen un\", \"expected conditions are encountered. \\n\\nKubernetes \\n\\nWithin Kubernetes, there\\u2019s a similar service for\", \" maintaining small pieces of secret information. \\nKubernetes Secrets can be set via the typical kube\", \"ctl executable. \\n\\nCreating a secret is as simple as finding the base64 version of the values to be s\", \"tored: \\n\\necho -n 'admin' | base64 \\nYWRtaW4= \\necho -n '1f2d1e2e67df' | base64 \\nMWYyZDFlMmU2N2Rm \\n\\nThe\", \"n adding it to a secrets file named secret.yml for example that looks similar to the following \\nexam\", \"ple: \\n\\napiVersion: v1 \\nkind: Secret \\nmetadata: \\n  name: mysecret \\n\\n155 \\n\\nCHAPTER 9 | Cloud-native se\", \"curity \\n\\n \\n \\n\\ftype: Opaque \\ndata: \\n  username: YWRtaW4= \\n  password: MWYyZDFlMmU2N2Rm \\n\\nFinally, thi\", \"s file can be loaded into Kubernetes by running the following command: \\n\\nkubectl apply -f ./secret.y\", \"aml \\n\\nThese secrets can then be mounted into volumes or exposed to container processes through \\nenvi\", \"ronment variables. The Twelve-factor app approach to building applications suggests using the \\nlowes\", \"t common denominator to transmit settings to an application. Environment variables are the \\nlowest c\", \"ommon denominator, because they\\u2019re supported no matter the operating system or \\napplication. \\n\\nAn al\", \"ternative to use the built-in Kubernetes secrets is to access the secrets in Azure Key Vault from \\nw\", \"ithin Kubernetes. The simplest way to do this is to assign an RBAC role to the container looking to \", \"\\nload secrets. The application can then use the Azure Key Vault APIs to access the secrets. However,\", \" \\nthis approach requires modifications to the code and doesn\\u2019t follow the pattern of using environme\", \"nt \\nvariables. Instead, it\\u2019s possible to inject values into a container. This approach is actually m\", \"ore secure \\nthan using the Kubernetes secrets directly, as they can be accessed by users on the clus\", \"ter. \\n\\nEncryption in transit and at rest \\n\\nKeeping data safe is important whether it\\u2019s on disk or tr\", \"ansiting between various different services. \\nThe most effective way to keep data from leaking is to\", \" encrypt it into a format that can\\u2019t be easily read \\nby others. Azure supports a wide range of encry\", \"ption options. \\n\\nIn transit \\n\\nThere are several ways to encrypt traffic on the network in Azure. The\", \" access to Azure services is \\ntypically done over connections that use Transport Layer Security (TLS\", \"). For instance, all the \\nconnections to the Azure APIs require TLS connections. Equally, connection\", \"s to endpoints in Azure \\nstorage can be restricted to work only over TLS encrypted connections. \\n\\nTL\", \"S is a complicated protocol and simply knowing that the connection is using TLS isn\\u2019t sufficient to \", \"\\nensure security. For instance, TLS 1.0 is chronically insecure, and TLS 1.1 isn\\u2019t much better. Even\", \" within \\nthe versions of TLS, there are various settings that can make the connections easier to dec\", \"rypt. The \\nbest course of action is to check and see if the server connection is using up-to-date an\", \"d well \\nconfigured protocols. \\n\\nThis check can be done by an external service such as SSL labs\\u2019 SSL \", \"Server Test. A test run against a \\ntypical Azure endpoint, in this case a service bus endpoint, yiel\", \"ds a near perfect score of A. \\n\\nEven services like Azure SQL databases use TLS encryption to keep da\", \"ta hidden. The interesting part \\nabout encrypting the data in transit using TLS is that it isn\\u2019t pos\", \"sible, even for Microsoft, to listen in on \\nthe connection between computers running TLS. This shoul\", \"d provide comfort for companies \\nconcerned that their data may be at risk from Microsoft proper or e\", \"ven a state actor with more \\nresources than the standard attacker. \\n\\n156 \\n\\nCHAPTER 9 | Cloud-native \", \"security \\n\\n \\n \\n\\fFigure 9-5. SSL labs report showing a score of A for a Service Bus endpoint. \\n\\nWhile\", \" this level of encryption isn\\u2019t going to be sufficient for all time, it should inspire confidence th\", \"at \\nAzure TLS connections are quite secure. Azure will continue to evolve its security standards as \", \"\\nencryption improves. It\\u2019s nice to know that there\\u2019s somebody watching the security standards and \\nu\", \"pdating Azure as they improve. \\n\\nAt rest \\n\\nIn any application, there are a number of places where da\", \"ta rests on the disk. The application code \\nitself is loaded from some storage mechanism. Most appli\", \"cations also use some kind of a database \\nsuch as SQL Server, Cosmos DB, or even the amazingly price\", \"-efficient Table Storage. These databases \\nall use heavily encrypted storage to ensure that nobody o\", \"ther than the applications with proper \\npermissions can read your data. Even the system operators ca\", \"n\\u2019t read data that has been encrypted. \\nSo customers can remain confident their secret information r\", \"emains secret. \\n\\nStorage \\n\\nThe underpinning of much of Azure is the Azure Storage engine. Virtual ma\", \"chine disks are mounted \\non top of Azure Storage. Azure Kubernetes Service runs on virtual machines \", \"that, themselves, are \\nhosted on Azure Storage. Even serverless technologies, such as Azure Function\", \"s Apps and Azure \\nContainer Instances, run out of disk that is part of Azure Storage. \\n\\nIf Azure Sto\", \"rage is well encrypted, then it provides for a foundation for most everything else to also \\nbe encry\", \"pted. Azure Storage is encrypted with FIPS 140-2 compliant 256-bit AES. This is a well-\\nregarded enc\", \"ryption technology having been the subject of extensive academic scrutiny over the last \\n20 or so ye\", \"ars. At present, there\\u2019s no known practical attack that would allow someone without \\nknowledge of th\", \"e key to read data encrypted by AES. \\n\\nBy default, the keys used for encrypting Azure Storage are ma\", \"naged by Microsoft. There are extensive \\nprotections in place to ensure to prevent malicious access \", \"to these keys. However, users with \\nparticular encryption requirements can also provide their own st\", \"orage keys that are managed in Azure \\n\\n157 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n \\n\\fKey Vault. T\", \"hese keys can be revoked at any time, which would effectively render the contents of the \\nStorage ac\", \"count using them inaccessible. \\n\\nVirtual machines use encrypted storage, but it\\u2019s possible to provid\", \"e another layer of encryption by \\nusing technologies like BitLocker on Windows or DM-Crypt on Linux.\", \" These technologies mean that \\neven if the disk image was leaked off of storage, it would remain nea\", \"r impossible to read it. \\n\\nAzure SQL \\n\\nDatabases hosted on Azure SQL use a technology called Transpa\", \"rent Data Encryption (TDE) to ensure \\ndata remains encrypted. It\\u2019s enabled by default on all newly c\", \"reated SQL databases, but must be \\nenabled manually for legacy databases. TDE executes real-time enc\", \"ryption and decryption of not just \\nthe database, but also the backups and transaction logs. \\n\\nThe e\", \"ncryption parameters are stored in the master database and, on startup, are read into memory \\nfor th\", \"e remaining operations. This means that the master database must remain unencrypted. The \\nactual key\", \" is managed by Microsoft. However, users with exacting security requirements may provide \\ntheir own \", \"key in Key Vault in much the same way as is done for Azure Storage. The Key Vault provides \\nfor such\", \" services as key rotation and revocation. \\n\\nThe \\u201cTransparent\\u201d part of TDS comes from the fact that t\", \"here aren\\u2019t client changes needed to use an \\nencrypted database. While this approach provides for go\", \"od security, leaking the database password is \\nenough for users to be able to decrypt the data. Ther\", \"e\\u2019s another approach that encrypts individual \\ncolumns or tables in a database. Always Encrypted ens\", \"ures that at no point the encrypted data \\nappears in plain text inside the database. \\n\\nSetting up th\", \"is tier of encryption requires running through a wizard in SQL Server Management Studio \\nto select t\", \"he sort of encryption and where in Key Vault to store the associated keys. \\n\\n158 \\n\\nCHAPTER 9 | Cloud\", \"-native security \\n\\n \\n \\n\\fFigure 9-6. Selecting columns in a table to be encrypted using Always Encryp\", \"ted. \\n\\nClient applications that read information from these encrypted columns need to make special \\n\", \"allowances to read encrypted data. Connection strings need to be updated with Column Encryption \\nSet\", \"ting=Enabled and client credentials must be retrieved from the Key Vault. The SQL Server client \\nmus\", \"t then be primed with the column encryption keys. Once that is done, the remaining actions use \\nthe \", \"standard interfaces to SQL Client. That is, tools like Dapper and Entity Framework, which are built \", \"\\non top of SQL Client, will continue to work without changes. Always Encrypted may not yet be \\navail\", \"able for every SQL Server driver on every language. \\n\\nThe combination of TDE and Always Encrypted, b\", \"oth of which can be used with client-specific keys, \\nensures that even the most exacting encryption \", \"requirements are supported. \\n\\nCosmos DB \\n\\nCosmos DB is the newest database provided by Microsoft in \", \"Azure. It has been built from the ground \\nup with security and cryptography in mind. AES-256bit encr\", \"yption is standard for all Cosmos DB \\n\\n159 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n \\n \\n\\fdatabases an\", \"d can\\u2019t be disabled. Coupled with the TLS 1.2 requirement for communication, the entire \\nstorage sol\", \"ution is encrypted. \\n\\nFigure 9-7. The flow of data encryption within Cosmos DB. \\n\\nWhile Cosmos DB do\", \"esn\\u2019t provide for supplying customer encryption keys, there has been significant \\nwork done by the t\", \"eam to ensure it remains PCI-DSS compliant without that. Cosmos DB also doesn\\u2019t \\nsupport any sort of\", \" single column encryption similar to Azure SQL\\u2019s Always Encrypted yet. \\n\\nKeeping secure \\n\\nAzure has \", \"all the tools necessary to release a highly secure product. However, a chain is only as strong \\nas i\", \"ts weakest link. If the applications deployed on top of Azure aren\\u2019t developed with a proper \\nsecuri\", \"ty mindset and good security audits, then they become the weak link in the chain. There are \\nmany gr\", \"eat static analysis tools, encryption libraries, and security practices that can be used to ensure \\n\", \"that the software installed on Azure is as secure as Azure itself. Examples include static analysis \", \"tools, \\nencryption libraries, and security practices. \\n\\n160 \\n\\nCHAPTER 9 | Cloud-native security \\n\\n \\n\", \" \\n \\n\\fCHAPTER  10 \\n\\nDevOps \\n\\nThe favorite mantra of software consultants is to answer \\u201cIt depends\\u201d to\", \" any question posed. It isn\\u2019t \\nbecause software consultants are fond of not taking a position. It\\u2019s \", \"because there\\u2019s no one true \\nanswer to any questions in software. There\\u2019s no absolute right and wron\", \"g, but rather a balance \\nbetween opposites. \\n\\nTake, for instance, the two major schools of developin\", \"g web applications: Single Page Applications \\n(SPAs) versus server-side applications. On the one han\", \"d, the user experience tends to be better with \\nSPAs and the amount of traffic to the web server can\", \" be minimized making it possible to host them \\non something as simple as static hosting. On the othe\", \"r hand, SPAs tend to be slower to develop and \\nmore difficult to test. Which one is the right choice\", \"? Well, it depends on your situation. \\n\\nCloud-native applications aren\\u2019t immune to that same dichoto\", \"my. They have clear advantages in \\nterms of speed of development, stability, and scalability, but ma\", \"naging them can be quite a bit more \\ndifficult. \\n\\nYears ago, it wasn\\u2019t uncommon for the process of m\", \"oving an application from development to \\nproduction to take a month, or even more. Companies releas\", \"ed software on a 6-month or even every \\nyear cadence. One needs to look no further than Microsoft Wi\", \"ndows to get an idea for the cadence of \\nreleases that were acceptable before the ever-green days of\", \" Windows 10. Five years passed between \\nWindows XP and Vista, a further three between Vista and Wind\", \"ows 7. \\n\\nIt\\u2019s now fairly well established that being able to release software rapidly gives fast-mov\", \"ing companies \\na huge market advantage over their more sloth-like competitors. It\\u2019s for that reason \", \"that major \\nupdates to Windows 10 are now approximately every six months. \\n\\nThe patterns and practic\", \"es that enable faster, more reliable releases to deliver value to the business \\nare collectively kno\", \"wn as DevOps. They consist of a wide range of ideas spanning the entire software \\ndevelopment life c\", \"ycle from specifying an application all the way up to delivering and operating that \\napplication. \\n\\n\", \"DevOps emerged before microservices and it\\u2019s likely that the movement towards smaller, more fit to \\n\", \"purpose services wouldn\\u2019t have been possible without DevOps to make releasing and operating not \\njus\", \"t one but many applications in production easier. \\n\\n161 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fFigure 10-1 - D\", \"evOps and microservices. \\n\\nThrough good DevOps practices, it\\u2019s possible to realize the advantages of\", \" cloud-native applications \\nwithout suffocating under a mountain of work actually operating the appl\", \"ications. \\n\\nThere\\u2019s no golden hammer when it comes to DevOps. Nobody can sell a complete and all-\\nen\", \"compassing solution for releasing and operating high-quality applications. This is because each \\napp\", \"lication is wildly different from all others. However, there are tools that can make DevOps a far le\", \"ss \\ndaunting proposition. One of these tools is known as Azure DevOps. \\n\\nAzure DevOps \\n\\nAzure DevOps\", \" has a long pedigree. It can trace its roots back to when Team Foundation Server first \\nmoved online\", \" and through the various name changes: Visual Studio Online and Visual Studio Team \\nServices. Throug\", \"h the years, however, it has become far more than its predecessors. \\n\\nAzure DevOps is divided into f\", \"ive major components: \\n\\nFigure 10-2 - Azure DevOps. \\n\\nAzure Repos - Source code management that supp\", \"orts the venerable Team Foundation Version \\nControl (TFVC) and the industry favorite Git. Pull reque\", \"sts provide a way to enable social coding by \\nfostering discussion of changes as they\\u2019re made. \\n\\n162\", \" \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n \\n\\fAzure Boards - Provides an issue and work item tracking tool that \", \"strives to allow users to pick the \\nworkflows that work best for them. It comes with a number of pre\", \"-configured templates including \\nones to support SCRUM and Kanban styles of development. \\n\\nAzure Pip\", \"elines - A build and release management system that supports tight integration with Azure. \\nBuilds c\", \"an be run on various platforms from Windows to Linux to macOS. Build agents may be \\nprovisioned in t\", \"he cloud or on-premises. \\n\\nAzure Test Plans - No QA person will be left behind with the test managem\", \"ent and exploratory \\ntesting support offered by the Test Plans feature. \\n\\nAzure Artifacts - An artif\", \"act feed that allows companies to create their own, internal, versions of \\nNuGet, npm, and others. I\", \"t serves a double purpose of acting as a cache of upstream packages if \\nthere\\u2019s a failure of a centr\", \"alized repository. \\n\\nThe top-level organizational unit in Azure DevOps is known as a Project. Within\", \" each project the \\nvarious components, such as Azure Artifacts, can be turned on and off. Each of th\", \"ese components \\nprovides different advantages for cloud-native applications. The three most useful a\", \"re repositories, \\nboards, and pipelines. If users want to manage their source code in another reposi\", \"tory stack, such as \\nGitHub, but still take advantage of Azure Pipelines and other components, that\\u2019\", \"s perfectly possible. \\n\\nFortunately, development teams have many options when selecting a repository\", \". One of them is \\nGitHub. \\n\\nGitHub Actions \\n\\nFounded in 2009, GitHub is a widely popular web-based r\", \"epository for hosting projects, \\ndocumentation, and code. Many large tech companies, such as Apple, \", \"Amazon, Google, and \\nmainstream corporations use GitHub. GitHub uses the open-source, distributed ve\", \"rsion control system \\nnamed Git as its foundation. On top, it then adds its own set of features, inc\", \"luding defect tracking, \\nfeature and pull requests, tasks management, and wikis for each code base. \", \"\\n\\nAs GitHub evolves, it too is adding DevOps features. For example, GitHub has its own continuous \\ni\", \"ntegration/continuous delivery (CI/CD) pipeline, called GitHub Actions. GitHub Actions is a \\ncommuni\", \"ty-powered workflow automation tool. It lets DevOps teams integrate with their existing \\ntooling, mi\", \"x and match new products, and hook into their software lifecycle, including existing CI/CD \\npartners\", \".\\u201d \\n\\nGitHub has over 40 million users, making it the largest host of source code in the world. In Oc\", \"tober of \\n2018, Microsoft purchased GitHub. Microsoft has pledged that GitHub will remain an open pl\", \"atform \\nthat any developer can plug into and extend. It continues to operate as an independent compa\", \"ny. \\nGitHub offers plans for enterprise, team, professional, and free accounts. \\n\\nSource control \\n\\nO\", \"rganizing the code for a cloud-native application can be challenging. Instead of a single giant \\napp\", \"lication, the cloud-native applications tend to be made up of a web of smaller applications that \\nta\", \"lk with one another. As with all things in computing, the best arrangement of code remains an open \\n\", \"\\n163 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fquestion. There are examples of successful applications using diff\", \"erent kinds of layouts, but two \\nvariants seem to have the most popularity. \\n\\nBefore getting down in\", \"to the actual source control itself, it\\u2019s probably worth deciding on how many \\nprojects are appropri\", \"ate. Within a single project, there\\u2019s support for multiple repositories, and build \\npipelines. Board\", \"s are a little more complicated, but there too, the tasks can easily be assigned to \\nmultiple teams \", \"within a single project. It\\u2019s possible to support hundreds, even thousands of \\ndevelopers, out of a \", \"single Azure DevOps project. Doing so is likely the best approach as it provides a \\nsingle place for\", \" all developer to work out of and reduces the confusion of finding that one application \\nwhen develo\", \"pers are unsure in which project in which it resides. \\n\\nSplitting up code for microservices within t\", \"he Azure DevOps project can be slightly more challenging. \\n\\nFigure 10-3 - One vs. many repositories.\", \" \\n\\nRepository per microservice \\n\\nAt first glance, this approach seems like the most logical approach\", \" to splitting up the source code for \\nmicroservices. Each repository can contain the code needed to \", \"build the one microservice. The \\nadvantages to this approach are readily visible: \\n\\n1. \\n\\n2. \\n\\n3. \\n\\n4\", \". \\n\\n164 \\n\\nInstructions for building and maintaining the application can be added to a README file at\", \" \\nthe root of each repository. When flipping through the repositories, it\\u2019s easy to find these \\ninst\", \"ructions, reducing spin-up time for developers. \\n\\nEvery service is located in a logical place, easil\", \"y found by knowing the name of the service. \\n\\nBuilds can easily be set up such that they\\u2019re only tri\", \"ggered when a change is made to the \\nowning repository. \\n\\nThe number of changes coming into a reposi\", \"tory is limited to the small number of developers \\nworking on the project. \\n\\nCHAPTER 10 | DevOps \\n\\n \", \"\\n \\n \\n\\f5. \\n\\n6. \\n\\nSecurity is easy to set up by restricting the repositories to which developers have \", \"read and \\nwrite permissions. \\n\\nRepository level settings can be changed by the owning team with a mi\", \"nimum of discussion \\nwith others. \\n\\nOne of the key ideas behind microservices is that services shoul\", \"d be siloed and separated from each \\nother. When using Domain Driven Design to decide on the boundar\", \"ies for services the services act as \\ntransactional boundaries. Database updates shouldn\\u2019t span mult\", \"iple services. This collection of related \\ndata is referred to as a bounded context. This idea is re\", \"flected by the isolation of microservice data to \\na database separate and autonomous from the rest o\", \"f the services. It makes a great deal of sense to \\ncarry this idea all the way through to the source\", \" code. \\n\\nHowever, this approach isn\\u2019t without its issues. One of the more gnarly development problem\", \"s of our \\ntime is managing dependencies. Consider the number of files that make up the average \\nnode\", \"_modules directory. A fresh install of something like create-react-app is likely to bring with it \\nt\", \"housands of packages. The question of how to manage these dependencies is a difficult one. \\n\\nIf a de\", \"pendency is updated, then downstream packages must also update this dependency. \\nUnfortunately, that\", \" takes development work so, invariably, the node_modules directory ends up with \\nmultiple versions o\", \"f a single package, each one a dependency of some other package that is \\nversioned at a slightly dif\", \"ferent cadence. When deploying an application, which version of a \\ndependency should be used? The ve\", \"rsion that is currently in production? The version that is currently \\nin Beta but is likely to be in\", \" production by the time the consumer makes it to production? Difficult \\nproblems that aren\\u2019t resolve\", \"d by just using microservices. \\n\\nThere are libraries that are depended upon by a wide variety of pro\", \"jects. By dividing the microservices \\nup with one in each repository the internal dependencies can b\", \"est be resolved by using the internal \\nrepository, Azure Artifacts. Builds for libraries will push t\", \"heir latest versions into Azure Artifacts for \\ninternal consumption. The downstream project must sti\", \"ll be manually updated to take a dependency \\non the newly updated packages. \\n\\nAnother disadvantage p\", \"resents itself when moving code between services. Although it would be nice \\nto believe that the fir\", \"st division of an application into microservices is 100% correct, the reality is that \\nrarely we\\u2019re \", \"so prescient as to make no service division mistakes. Thus, functionality and the code that \\ndrives \", \"it will need to move from service to service: repository to repository. When leaping from one \\nrepos\", \"itory to another, the code loses its history. There are many cases, especially in the event of an \\na\", \"udit, where having full history on a piece of code is invaluable. \\n\\nThe final and most important dis\", \"advantage is coordinating changes. In a true microservices \\napplication, there should be no deployme\", \"nt dependencies between services. It should be possible to \\ndeploy services A, B, and C in any order\", \" as they have loose coupling. In reality, however, there are \\ntimes when it\\u2019s desirable to make a ch\", \"ange that crosses multiple repositories at the same time. Some \\nexamples include updating a library \", \"to close a security hole or changing a communication protocol \\nused by all services. \\n\\nTo do a cross\", \"-repository change requires a commit to each repository be made in succession. Each \\nchange in each \", \"repository will need to be pull-requested and reviewed separately. This activity can be \\ndifficult t\", \"o coordinate. \\n\\n165 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fAn alternative to using many repositories is to put\", \" all the source code together in a giant, all knowing, \\nsingle repository. \\n\\nSingle repository \\n\\nIn \", \"this approach, sometimes referred to as a monorepository, all the source code for every service is \\n\", \"put into the same repository. At first, this approach seems like a terrible idea likely to make deal\", \"ing \\nwith source code unwieldy. There are, however, some marked advantages to working this way. \\n\\nTh\", \"e first advantage is that it\\u2019s easier to manage dependencies between projects. Instead of relying on\", \" \\nsome external artifact feed, projects can directly import one another. This means that updates are\", \" \\ninstant, and conflicting versions are likely to be found at compile time on the developer\\u2019s workst\", \"ation. \\nIn effect, shifting some of the integration testing left. \\n\\nWhen moving code between project\", \"s, it\\u2019s now easier to preserve the history as the files will be \\ndetected as having been moved rathe\", \"r than being rewritten. \\n\\nAnother advantage is that wide ranging changes that cross service boundari\", \"es can be made in a \\nsingle commit. This activity reduces the overhead of having potentially dozens \", \"of changes to review \\nindividually. \\n\\nThere are many tools that can perform static analysis of code \", \"to detect insecure programming \\npractices or problematic use of APIs. In a multi-repository world, e\", \"ach repository will need to be \\niterated over to find the problems in them. The single repository al\", \"lows running the analysis all in one \\nplace. \\n\\nThere are also many disadvantages to the single repos\", \"itory approach. One of the most worrying ones \\nis that having a single repository raises security co\", \"ncerns. If the contents of a repository are leaked in \\na repository per service model, the amount of\", \" code lost is minimal. With a single repository, \\neverything the company owns could be lost. There h\", \"ave been many examples in the past of this \\nhappening and derailing entire game development efforts.\", \" Having multiple repositories exposes less \\nsurface area, which is a desirable trait in most securit\", \"y practices. \\n\\nThe size of the single repository is likely to become unmanageable rapidly. This pres\", \"ents some \\ninteresting performance implications. It may become necessary to use specialized tools su\", \"ch as Virtual \\nFile System for Git, which was originally designed to improve the experience for deve\", \"lopers on the \\nWindows team. \\n\\nFrequently the argument for using a single repository boils down to a\", \"n argument that Facebook or \\nGoogle use this method for source code arrangement. If the approach is \", \"good enough for these \\ncompanies, then, surely, it\\u2019s the correct approach for all companies. The tru\", \"th of the matter is that few \\ncompanies operate on anything like the scale of Facebook or Google. Th\", \"e problems that occur at \\nthose scales are different from those most developers will face. What is g\", \"ood for the goose may not \\nbe good for the gander. \\n\\nIn the end, either solution can be used to host\", \" the source code for microservices. However, in most \\ncases, the management, and engineering overhea\", \"d of operating in a single repository isn\\u2019t worth the \\nmeager advantages. Splitting code up over mul\", \"tiple repositories encourages better separation of \\nconcerns and encourages autonomy among developme\", \"nt teams. \\n\\n166 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fStandard directory structure \\n\\nRegardless of the single\", \" versus multiple repositories debate each service will have its own directory. \\nOne of the best opti\", \"mizations to allow developers to cross between projects quickly is to maintain a \\nstandard directory\", \" structure. \\n\\nFigure 10-4 - Standard directory structure. \\n\\nWhenever a new project is created, a tem\", \"plate that puts in place the correct structure should be used. \\nThis template can also include such \", \"useful items as a skeleton README file and an azure-\\npipelines.yml. In any microservice architecture\", \", a high degree of variance between projects makes bulk \\noperations against the services more diffic\", \"ult. \\n\\nThere are many tools that can provide templating for an entire directory, containing several \", \"source \\ncode directories. Yeoman is popular in the JavaScript world and GitHub have recently release\", \"d \\nRepository Templates, which provide much of the same functionality. \\n\\nTask management \\n\\nManaging \", \"tasks in any project can be difficult. Up front there are countless questions to be answered \\nabout \", \"the sort of workflows to set up to ensure optimal developer productivity. \\n\\nCloud-native application\", \"s tend to be smaller than traditional software products or at least they\\u2019re \\ndivided into smaller se\", \"rvices. Tracking of issues or tasks related to these services remains as important \\nas with any othe\", \"r software project. Nobody wants to lose track of some work item or explain to a \\ncustomer that thei\", \"r issue wasn\\u2019t properly logged. Boards are configured at the project level but within \\neach project,\", \" areas can be defined. These allow breaking down issues across several components. The \\nadvantage to\", \" keeping all the work for the entire application in one place is that it\\u2019s easy to move work \\nitems \", \"from one team to another as they\\u2019re understood better. \\n\\n167 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n\\fAzure De\", \"vOps comes with a number of popular templates pre-configured. In the most basic \\nconfiguration, all \", \"that is needed to know is what\\u2019s in the backlog, what people are working on, and \\nwhat\\u2019s done. It\\u2019s \", \"important to have this visibility into the process of building software, so that work \\ncan be priori\", \"tized and completed tasks reported to the customer. Of course, few software projects \\nstick to a pro\", \"cess as simple as to do, doing, and done. It doesn\\u2019t take long for people to start adding \\nsteps lik\", \"e QA or Detailed Specification to the process. \\n\\nOne of the more important parts of Agile methodolog\", \"ies is self-introspection at regular intervals. \\nThese reviews are meant to provide insight into wha\", \"t problems the team is facing and how they can \\nbe improved. Frequently, this means changing the flo\", \"w of issues and features through the \\ndevelopment process. So, it\\u2019s perfectly healthy to expand the \", \"layouts of the boards with additional \\nstages. \\n\\nThe stages in the boards aren\\u2019t the only organizati\", \"onal tool. Depending on the configuration of the \\nboard, there\\u2019s a hierarchy of work items. The most\", \" granular item that can appear on a board is a task. \\nOut of the box a task contains fields for a ti\", \"tle, description, a priority, an estimate of the amount of \\nwork remaining and the ability to link t\", \"o other work items or development items (branches, commits, \\npull requests, builds, and so forth). W\", \"ork items can be classified into different areas of the application \\nand different iterations (sprin\", \"ts) to make finding them easier. \\n\\nFigure 10-5 - Task in Azure DevOps. \\n\\nThe description field suppo\", \"rts the normal styles you\\u2019d expect (bold, italic underscore and strike \\nthrough) and the ability to \", \"insert images. This makes it a powerful tool for use when specifying work \\nor bugs. \\n\\nTasks can be r\", \"olled up into features, which define a larger unit of work. Features, in turn, can be rolled \\nup int\", \"o epics. Classifying tasks in this hierarchy makes it much easier to understand how close a large \\nf\", \"eature is to rolling out. \\n\\n168 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n\\fFigure 10-6 - Work item in Azure DevO\", \"ps. \\n\\nThere are different kinds of views into the issues in Azure Boards. Items that aren\\u2019t yet sche\", \"duled \\nappear in the backlog. From there, they can be assigned to a sprint. A sprint is a time box d\", \"uring \\nwhich it\\u2019s expected some quantity of work will be completed. This work can include tasks but \", \"also the \\nresolution of tickets. Once there, the entire sprint can be managed from the Sprint board \", \"section. This \\nview shows how work is progressing and includes a burn down chart to give an ever-upd\", \"ating \\nestimate of if the sprint will be successful. \\n\\nFigure 10-7 - Board in Azure DevOps. \\n\\nBy now\", \", it should be apparent that there\\u2019s a great deal of power in the Boards in Azure DevOps. For \\ndevel\", \"opers, there are easy views of what is being worked on. For project managers views into \\nupcoming wo\", \"rk as well as an overview of existing work. For managers, there are plenty of reports \\nabout resourc\", \"ing and capacity. Unfortunately, there\\u2019s nothing magical about cloud-native applications \\nthat elimi\", \"nate the need to track work. But if you must track work, there are a few places where the \\nexperienc\", \"e is better than in Azure DevOps. \\n\\nCI/CD pipelines \\n\\nAlmost no change in the software development l\", \"ife cycle has been so revolutionary as the advent of \\ncontinuous integration (CI) and continuous del\", \"ivery (CD). Building and running automated tests \\nagainst the source code of a project as soon as a \", \"change is checked in catches mistakes early. Prior to \\nthe advent of continuous integration builds, \", \"it wouldn\\u2019t be uncommon to pull code from the \\n\\n169 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n \\n\\frepository and \", \"find that it didn\\u2019t pass tests or couldn\\u2019t even be built. This resulted in tracking down \\nthe source\", \" of the breakage. \\n\\nTraditionally shipping software to the production environment required extensive\", \" documentation and \\na list of steps. Each one of these steps needed to be manually completed in a ve\", \"ry error prone \\nprocess. \\n\\nFigure 10-8 - Checklist. \\n\\nThe sister of continuous integration is contin\", \"uous delivery in which the freshly built packages are \\ndeployed to an environment. The manual proces\", \"s can\\u2019t scale to match the speed of development so \\nautomation becomes more important. Checklists ar\", \"e replaced by scripts that can execute the same \\ntasks faster and more accurately than any human. \\n\\n\", \"The environment to which continuous delivery delivers might be a test environment or, as is being \\nd\", \"one by many major technology companies, it could be the production environment. The latter \\nrequires\", \" an investment in high-quality tests that can give confidence that a change isn\\u2019t going to \\nbreak pr\", \"oduction for users. In the same way that continuous integration caught issues in the code \\nearly con\", \"tinuous delivery catches issues in the deployment process early. \\n\\nThe importance of automating the \", \"build and delivery process is accentuated by cloud-native \\napplications. Deployments happen more fre\", \"quently and to more environments so manually deploying \\nborders on impossible. \\n\\nAzure Builds \\n\\nAzur\", \"e DevOps provides a set of tools to make continuous integration and deployment easier than \\never. Th\", \"ese tools are located under Azure Pipelines. The first of them is Azure Builds, which is a tool \\nfor\", \" running YAML-based build definitions at scale. Users can either bring their own build machines \\n(gr\", \"eat for if the build requires a meticulously set up environment) or use a machine from a constantly \", \"\\nrefreshed pool of Azure hosted virtual machines. These hosted build agents come pre-installed with \", \"a \\n\\n170 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n\\fwide range of development tools for not just .NET development\", \" but for everything from Java to \\nPython to iPhone development. \\n\\nDevOps includes a wide range of ou\", \"t of the box build definitions that can be customized for any build. \\nThe build definitions are defi\", \"ned in a file called azure-pipelines.yml and checked into the repository so \\nthey can be versioned a\", \"long with the source code. This makes it much easier to make changes to the \\nbuild pipeline in a bra\", \"nch as the changes can be checked into just that branch. An example azure-\\npipelines.yml for buildin\", \"g an ASP.NET web application on full framework is show in Figure 10-9. \\n\\nname: $(rev:r) \\n\\nvariables:\", \" \\n  version: 9.2.0.$(Build.BuildNumber) \\n  solution: Portals.sln \\n  artifactName: drop \\n  buildPlatf\", \"orm: any cpu \\n  buildConfiguration: release \\n\\npool: \\n  name: Hosted VisualStudio \\n  demands: \\n  - ms\", \"build \\n  - visualstudio \\n  - vstest \\n\\nsteps: \\n- task: NuGetToolInstaller@0 \\n  displayName: 'Use NuGe\", \"t 4.4.1' \\n  inputs: \\n    versionSpec: 4.4.1 \\n\\n- task: NuGetCommand@2 \\n  displayName: 'NuGet restore'\", \" \\n  inputs: \\n    restoreSolution: '$(solution)' \\n\\n- task: VSBuild@1 \\n  displayName: 'Build solution'\", \" \\n  inputs: \\n    solution: '$(solution)' \\n    msbuildArgs: '-p:DeployOnBuild=true -p:WebPublishMetho\", \"d=Package -\\np:PackageAsSingleFile=true -p:SkipInvalidConfigurations=true -\\np:PackageLocation=\\\"$(buil\", \"d.artifactstagingdirectory)\\\\\\\\\\\"' \\n    platform: '$(buildPlatform)' \\n    configuration: '$(buildConfig\", \"uration)' \\n\\n- task: VSTest@2 \\n  displayName: 'Test Assemblies' \\n  inputs: \\n    testAssemblyVer2: | \\n\", \"     **\\\\$(buildConfiguration)\\\\**\\\\*test*.dll \\n     !**\\\\obj\\\\** \\n     !**\\\\*testadapter.dll \\n    platfor\", \"m: '$(buildPlatform)' \\n    configuration: '$(buildConfiguration)' \\n\\n- task: CopyFiles@2 \\n  displayNa\", \"me: 'Copy UI Test Files to: $(build.artifactstagingdirectory)' \\n  inputs: \\n\\n171 \\n\\nCHAPTER 10 | DevOp\", \"s \\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n\\f    SourceFolder: UITests \\n    TargetFolder: '$(build.artifactstagingdirector\", \"y)/uitests' \\n\\n- task: PublishBuildArtifacts@1 \\n  displayName: 'Publish Artifact' \\n  inputs: \\n    Pat\", \"htoPublish: '$(build.artifactstagingdirectory)' \\n    ArtifactName: '$(artifactName)' \\n  condition: s\", \"ucceededOrFailed() \\n\\nFigure 10-9 - A sample azure-pipelines.yml \\n\\nThis build definition uses a numbe\", \"r of built-in tasks that make creating builds as simple as building a \\nLego set (simpler than the gi\", \"ant Millennium Falcon). For instance, the NuGet task restores NuGet \\npackages, while the VSBuild tas\", \"k calls the Visual Studio build tools to perform the actual compilation. \\nThere are hundreds of diff\", \"erent tasks available in Azure DevOps, with thousands more that are \\nmaintained by the community. It\", \"\\u2019s likely that no matter what build tasks you\\u2019re looking to run, \\nsomebody has built one already. \\n\\n\", \"Builds can be triggered manually, by a check-in, on a schedule, or by the completion of another buil\", \"d. \\nIn most cases, building on every check-in is desirable. Builds can be filtered so that different\", \" builds run \\nagainst different parts of the repository or against different branches. This allows fo\", \"r scenarios like \\nrunning fast builds with reduced testing on pull requests and running a full regre\", \"ssion suite against \\nthe trunk on a nightly basis. \\n\\nThe end result of a build is a collection of fi\", \"les known as build artifacts. These artifacts can be passed \\nalong to the next step in the build pro\", \"cess or added to an Azure Artifacts feed, so they can be \\nconsumed by other builds. \\n\\nAzure DevOps r\", \"eleases \\n\\nBuilds take care of compiling the software into a shippable package, but the artifacts sti\", \"ll need to be \\npushed out to a testing environment to complete continuous delivery. For this, Azure \", \"DevOps uses a \\nseparate tool called Releases. The Releases tool makes use of the same tasks\\u2019 library\", \" that were \\navailable to the Build but introduce a concept of \\u201cstages\\u201d. A stage is an isolated envir\", \"onment into \\nwhich the package is installed. For instance, a product might make use of a development\", \", a QA, and a \\nproduction environment. Code is continuously delivered into the development environme\", \"nt where \\nautomated tests can be run against it. Once those tests pass the release moves onto the QA\", \" \\nenvironment for manual testing. Finally, the code is pushed to production where it\\u2019s visible to \\ne\", \"verybody. \\n\\nFigure 10-10 - Release pipeline \\n\\nEach stage in the build can be automatically triggered\", \" by the completion of the previous phase. In \\nmany cases, however, this isn\\u2019t desirable. Moving code\", \" into production might require approval from \\nsomebody. The Releases tool supports this by allowing \", \"approvers at each step of the release pipeline. \\nRules can be set up such that a specific person or \", \"group of people must sign off on a release before it \\n\\n172 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n \\n\\fmakes in\", \"to production. These gates allow for manual quality checks and also for compliance with any \\nregulat\", \"ory requirements related to control what goes into production. \\n\\nEverybody gets a build pipeline \\n\\nT\", \"here\\u2019s no cost to configuring many build pipelines, so it\\u2019s advantageous to have at least one build \", \"\\npipeline per microservice. Ideally, microservices are independently deployable to any environment s\", \"o \\nhaving each one able to be released via its own pipeline without releasing a mass of unrelated co\", \"de is \\nperfect. Each pipeline can have its own set of approvals allowing for variations in build pro\", \"cess for \\neach service. \\n\\nVersioning releases \\n\\nOne drawback to using the Releases functionality is \", \"that it can\\u2019t be defined in a checked-in azure-\\npipelines.yml file. There are many reasons you might\", \" want to do that from having per-branch release \\ndefinitions to including a release skeleton in your\", \" project template. Fortunately, work is ongoing to \\nshift some of the stages support into the Build \", \"component. This will be known as multi-stage build \\nand the first version is available now! \\n\\nFeatur\", \"e flags \\n\\nIn chapter 1, we affirmed that cloud native is much about speed and agility. Users expect \", \"rapid \\nresponsiveness, innovative features, and zero downtime. Feature flags are a modern deployment\", \" \\ntechnique that helps increase agility for cloud-native applications. They enable you to deploy new\", \" \\nfeatures into a production environment, but restrict their availability. With the flick of a switc\", \"h, you can \\nactivate a new feature for specific users without restarting the app or deploying new co\", \"de. They \\nseparate the release of new features from their code deployment. \\n\\nFeature flags are built\", \" upon conditional logic that control visibility of functionality for users at run \\ntime. In modern c\", \"loud-native systems, it\\u2019s common to deploy new features into production early, but \\ntest them with a\", \" limited audience. As confidence increases, the feature can be incrementally rolled out \\nto wider au\", \"diences. \\n\\nOther use cases for feature flags include: \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nRestrict premium functionali\", \"ty to specific customer groups willing to pay higher subscription \\nfees. \\n\\nStabilize a system by qui\", \"ckly deactivating a problem feature, avoiding the risks of a rollback or \\nimmediate hotfix. \\n\\nDisabl\", \"e an optional feature with high resource consumption during peak usage periods. \\n\\nConduct experiment\", \"al feature releases to small user segments to validate feasibility and \\npopularity. \\n\\nFeature flags \", \"also promote trunk-based development. It\\u2019s a source-control branching model where \\ndevelopers collab\", \"orate on features in a single branch. The approach minimizes the risk and complexity \\nof merging lar\", \"ge numbers of long-running feature branches. Features are unavailable until activated. \\n\\n173 \\n\\nCHAPT\", \"ER 10 | DevOps \\n\\n \\n \\n\\fImplementing feature flags \\n\\nAt its core, a feature flag is a reference to a s\", \"imple decision object. It returns a Boolean state of on or \\noff. The flag typically wraps a block of\", \" code that encapsulates a feature capability. The state of the flag \\ndetermines whether that code bl\", \"ock executes for a given user. Figure 10-11 shows the \\nimplementation. \\n\\nif (featureFlag) { \\n    // \", \"Run this code block if the featureFlag value is true \\n} else { \\n    // Run this code block if the fe\", \"atureFlag value is false \\n} \\n\\nFigure 10-11 - Simple feature flag implementation. \\n\\nNote how this app\", \"roach separates the decision logic from the feature code. \\n\\nIn chapter 1, we discussed the Twelve-Fa\", \"ctor App. The guidance recommended keeping configuration \\nsettings external from application executa\", \"ble code. When needed, settings can be read in from the \\nexternal source. Feature flag configuration\", \" values should also be independent from their codebase. By \\nexternalizing flag configuration in a se\", \"parate repository, you can change flag state without modifying \\nand redeploying the application. \\n\\nA\", \"zure App Configuration provides a centralized repository for feature flags. With it, you define \\ndif\", \"ferent kinds of feature flags and manipulate their states quickly and confidently. You add the App \\n\", \"Configuration client libraries to your application to enable feature flag functionality. Various \\npr\", \"ogramming language frameworks are supported. \\n\\nFeature flags can be easily implemented in an ASP.NET\", \" Core service. Installing the .NET Feature \\nManagement libraries and App Configuration provider enab\", \"le you to declaratively add feature flags to \\nyour code. They enable FeatureGate attributes so that \", \"you don\\u2019t have to manually write if statements \\nacross your codebase. \\n\\nOnce configured in your Star\", \"tup class, you can add feature flag functionality at the controller, action, \\nor middleware level. F\", \"igure 10-12 presents controller and action implementation: \\n\\n[FeatureGate(MyFeatureFlags.FeatureA)] \", \"\\npublic class ProductController : Controller \\n{ \\n    ... \\n} \\n\\n[FeatureGate(MyFeatureFlags.FeatureA)]\", \" \\npublic IActionResult UpdateProductStatus() \\n{ \\n    return ObjectResult(ProductDto); \\n} \\n\\nFigure 10\", \"-12 - Feature flag implementation in a controller and action. \\n\\nIf a feature flag is disabled, the u\", \"ser will receive a 404 (Not Found) status code with no response body. \\n\\nFeature flags can also be in\", \"jected directly into C# classes. Figure 10-13 shows feature flag injection: \\n\\n174 \\n\\nCHAPTER 10 | Dev\", \"Ops \\n\\n \\n \\n \\n \\n\\fpublic class ProductController : Controller \\n{ \\n    private readonly IFeatureManager \", \"_featureManager; \\n\\n    public ProductController(IFeatureManager featureManager) \\n    { \\n        _fea\", \"tureManager = featureManager; \\n    } \\n} \\n\\nFigure 10-13 - Feature flag injection into a class. \\n\\nThe \", \"Feature Management libraries manage the feature flag lifecycle behind the scenes. For example, \\nto m\", \"inimize high numbers of calls to the configuration store, the libraries cache flag states for a \\nspe\", \"cified duration. They can guarantee the immutability of flag states during a request call. They also\", \" \\noffer a Point-in-time snapshot. You can reconstruct the history of any key-value and provide its p\", \"ast \\nvalue at any moment within the previous seven days. \\n\\nInfrastructure as code \\n\\nCloud-native sys\", \"tems embrace microservices, containers, and modern system design to achieve speed \\nand agility. They\", \" provide automated build and release stages to ensure consistent and quality code. \\nBut, that\\u2019s only\", \" part of the story. How do you provision the cloud environments upon which these \\nsystems run? \\n\\nMod\", \"ern cloud-native applications embrace the widely accepted practice of Infrastructure as Code, or \\nIa\", \"C. With IaC, you automate platform provisioning. You essentially apply software engineering \\npractic\", \"es such as testing and versioning to your DevOps practices. Your infrastructure and \\ndeployments are\", \" automated, consistent, and repeatable. Just as continuous delivery automated the \\ntraditional model\", \" of manual deployments, Infrastructure as Code (IaC) is evolving how application \\nenvironments are m\", \"anaged. \\n\\nTools like Azure Resource Manager (ARM), Terraform, and the Azure Command Line Interface (\", \"CLI) \\nenable you to declaratively script the cloud infrastructure you require. \\n\\nAzure Resource Mana\", \"ger templates \\n\\nARM stands for Azure Resource Manager. It\\u2019s an API provisioning engine that is built\", \" into Azure and \\nexposed as an API service. ARM enables you to deploy, update, delete, and manage th\", \"e resources \\ncontained in Azure resource group in a single, coordinated operation. You provide the e\", \"ngine with a \\nJSON-based template that specifies the resources you require and their configuration. \", \"ARM \\nautomatically orchestrates the deployment in the correct order respecting dependencies. The eng\", \"ine \\nensures idempotency. If a desired resource already exists with the same configuration, provisio\", \"ning \\nwill be ignored. \\n\\nAzure Resource Manager templates are a JSON-based language for defining var\", \"ious resources in \\nAzure. The basic schema looks something like Figure 10-14. \\n\\n{ \\n  \\\"$schema\\\": \\\"htt\", \"ps://schema.management.azure.com/schemas/2015-01-\\n\\n175 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n\\f01/deploymentT\", \"emplate.json#\\\", \\n  \\\"contentVersion\\\": \\\"\\\", \\n  \\\"apiProfile\\\": \\\"\\\", \\n  \\\"parameters\\\": {  }, \\n  \\\"variables\\\":\", \" {  }, \\n  \\\"functions\\\": [  ], \\n  \\\"resources\\\": [  ], \\n  \\\"outputs\\\": {  } \\n} \\n\\nFigure 10-14 - The schema\", \" for a Resource Manager template \\n\\nWithin this template, one might define a storage container inside\", \" the resources section like so: \\n\\n\\\"resources\\\": [ \\n    { \\n      \\\"type\\\": \\\"Microsoft.Storage/storageAcc\", \"ounts\\\", \\n      \\\"name\\\": \\\"[variables('storageAccountName')]\\\", \\n      \\\"location\\\": \\\"[parameters('locatio\", \"n')]\\\", \\n      \\\"apiVersion\\\": \\\"2018-07-01\\\", \\n      \\\"sku\\\": { \\n        \\\"name\\\": \\\"[parameters('storageAcco\", \"untType')]\\\" \\n      }, \\n      \\\"kind\\\": \\\"StorageV2\\\", \\n      \\\"properties\\\": {} \\n    } \\n  ], \\n\\nFigure 10-1\", \"5 - An example of a storage account defined in a Resource Manager template \\n\\nAn ARM template can be \", \"parameterized with dynamic environment and configuration information. \\nDoing so enables it to be reu\", \"sed to define different environments, such as development, QA, or \\nproduction. Normally, the templat\", \"e creates all resources within a single Azure resource group. It\\u2019s \\npossible to define multiple reso\", \"urce groups in a single Resource Manager template, if needed. You \\ncan delete all resources in an en\", \"vironment by deleting the resource group itself. Cost analysis can also \\nbe run at the resource grou\", \"p level, allowing for quick accounting of how much each environment is \\ncosting. \\n\\nThere are many ex\", \"amples of ARM templates available in the Azure Quickstart Templates project on \\nGitHub. They can hel\", \"p accelerate creating a new template or modifying an existing one. \\n\\nResource Manager templates can \", \"be run in many of ways. Perhaps the simplest way is to simply paste \\nthem into the Azure portal. For\", \" experimental deployments, this method can be quick. They can also be \\nrun as part of a build or rel\", \"ease process in Azure DevOps. There are tasks that will leverage \\nconnections into Azure to run the \", \"templates. Changes to Resource Manager templates are applied \\nincrementally, meaning that to add a n\", \"ew resource requires just adding it to the template. The tooling \\nwill reconcile differences between\", \" the current resources and those defined in the template. Resources \\nwill then be created or altered\", \" so they match what is defined in the template. \\n\\nTerraform \\n\\nCloud-native applications are often co\", \"nstructed to be cloud agnostic. Being so means the application \\nisn\\u2019t tightly coupled to a particula\", \"r cloud vendor and can be deployed to any public cloud. \\n\\n176 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fTerraform\", \" is a commercial templating tool that can provision cloud-native applications across all the \\nmajor \", \"cloud players: Azure, Google Cloud Platform, AWS, and AliCloud. Instead of using JSON as the \\ntempla\", \"te definition language, it uses the slightly more terse HCL (Hashicorp Configuration Language). \\n\\nAn\", \" example Terraform file that does the same as the previous Resource Manager template (Figure 10-\\n15)\", \" is shown in Figure 10-16: \\n\\nprovider \\\"azurerm\\\" { \\n  version = \\\"=1.28.0\\\" \\n} \\n\\nresource \\\"azurerm_reso\", \"urce_group\\\" \\\"testrg\\\" { \\n  name     = \\\"production\\\" \\n  location = \\\"West US\\\" \\n} \\n\\nresource \\\"azurerm_sto\", \"rage_account\\\" \\\"testsa\\\" { \\n  name                     = \\\"${var.storageAccountName}\\\" \\n  resource_group\", \"_name      = \\\"${azurerm_resource_group.testrg.name}\\\" \\n  location                 = \\\"${var.region}\\\" \\n\", \"  account_tier             = \\\"${var.tier}\\\" \\n  account_replication_type = \\\"${var.replicationType}\\\" \\n\\n\", \"} \\n\\nFigure 10-16 - An example of a Resource Manager template \\n\\nTerraform also provides intuitive err\", \"or messages for problem templates. There\\u2019s also a handy validate \\ntask that can be used in the build\", \" phase to catch template errors early. \\n\\nAs with Resource Manager templates, command-line tools are \", \"available to deploy Terraform \\ntemplates. There are also community-created tasks in Azure Pipelines \", \"that can validate and apply \\nTerraform templates. \\n\\nSometimes Terraform and ARM templates output mea\", \"ningful values, such as a connection string to a \\nnewly created database. This information can be ca\", \"ptured in the build pipeline and used in \\nsubsequent tasks. \\n\\nAzure CLI Scripts and Tasks \\n\\nFinally,\", \" you can leverage Azure CLI to declaratively script your cloud infrastructure. Azure CLI scripts \\nca\", \"n be created, found, and shared to provision and configure almost any Azure resource. The CLI is \\nsi\", \"mple to use with a gentle learning curve. Scripts are executed within either PowerShell or Bash. \\nTh\", \"ey\\u2019re also straightforward to debug, especially when compared with ARM templates. \\n\\nAzure CLI script\", \"s work well when you need to tear down and redeploy your infrastructure. Updating \\nan existing envir\", \"onment can be tricky. Many CLI commands aren\\u2019t idempotent. That means they\\u2019ll \\nrecreate the resource\", \" each time they\\u2019re run, even if the resource already exists. It\\u2019s always possible to \\nadd code that \", \"checks for the existence of each resource before creating it. But, doing so, your script \\ncan become\", \" bloated and difficult to manage. \\n\\nThese scripts can also be embedded in Azure DevOps pipelines as \", \"Azure CLI tasks. Executing the \\npipeline invokes the script. \\n\\n177 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n \\n \\n \", \"\\n\\fFigure 10-17 shows a YAML snippet that lists the version of Azure CLI and the details of the \\nsubs\", \"cription. Note how Azure CLI commands are included in an inline script. \\n\\n- task: AzureCLI@2 \\n  disp\", \"layName: Azure CLI \\n  inputs: \\n    azureSubscription: <Name of the Azure Resource Manager service co\", \"nnection> \\n    scriptType: ps \\n    scriptLocation: inlineScript \\n    inlineScript: | \\n      az --ver\", \"sion \\n      az account show \\n\\nFigure 10-17 - Azure CLI script \\n\\nIn the article, What is Infrastructu\", \"re as Code, Author Sam Guckenheimer describes how, \\u201cTeams who \\nimplement IaC can deliver stable envi\", \"ronments rapidly and at scale. Teams avoid manual \\nconfiguration of environments and enforce consist\", \"ency by representing the desired state of their \\nenvironments via code. Infrastructure deployments w\", \"ith IaC are repeatable and prevent runtime issues \\ncaused by configuration drift or missing dependen\", \"cies. DevOps teams can work together with a \\nunified set of practices and tools to deliver applicati\", \"ons and their supporting infrastructure rapidly, \\nreliably, and at scale.\\u201d \\n\\nCloud Native Applicatio\", \"n Bundles \\n\\nA key property of cloud-native applications is that they leverage the capabilities of th\", \"e cloud to speed \\nup development. This design often means that a full application uses different kin\", \"ds of technologies. \\nApplications may be shipped in Docker containers, some services may use Azure F\", \"unctions, while \\nother parts may run directly on virtual machines allocated on large metal servers w\", \"ith hardware GPU \\nacceleration. No two cloud-native applications are the same, so it\\u2019s been difficul\", \"t to provide a single \\nmechanism for shipping them. \\n\\nThe Docker containers may run on Kubernetes us\", \"ing a Helm Chart for deployment. The Azure \\nFunctions may be allocated using Terraform templates. Fi\", \"nally, the virtual machines may be allocated \\nusing Terraform but built out using Ansible. This is a\", \" large variety of technologies and there has been \\nno way to package them all together into a reason\", \"able package. Until now. \\n\\nCloud Native Application Bundles (CNABs) are a joint effort by many commu\", \"nity-minded companies \\nsuch as Microsoft, Docker, and HashiCorp to develop a specification to packag\", \"e distributed \\napplications. \\n\\nThe effort was announced in December of 2018, so there\\u2019s still a fair\", \" bit of work to do to expose the \\neffort to the greater community. However, there\\u2019s already an open \", \"specification and a reference \\nimplementation known as Duffle. This tool, which was written in Go, i\", \"s a joint effort between Docker \\nand Microsoft. \\n\\nThe CNABs can contain different kinds of installat\", \"ion technologies. This aspect allows things like Helm \\nCharts, Terraform templates, and Ansible Play\", \"books to coexist in the same package. Once built, the \\npackages are self-contained and portable; the\", \"y can be installed from a USB stick. The packages are \\ncryptographically signed to ensure they origi\", \"nate from the party they claim. \\n\\n178 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fThe core of a CNAB is a file call\", \"ed bundle.json. This file defines the contents of the bundle, be they \\nTerraform or images or anythi\", \"ng else. Figure 11-9 defines a CNAB that invokes some Terraform. \\nNotice, however, that it actually \", \"defines an invocation image that is used to invoke the Terraform. \\nWhen packaged up, the Docker file\", \" that is located in the cnab directory is built into a Docker image, \\nwhich will be included in the \", \"bundle. Having Terraform installed inside a Docker container in the \\nbundle means that users don\\u2019t n\", \"eed to have Terraform installed on their machine to run the bundling. \\n\\n{ \\n    \\\"name\\\": \\\"terraform\\\", \", \"\\n    \\\"version\\\": \\\"0.1.0\\\", \\n    \\\"schemaVersion\\\": \\\"v1.0.0-WD\\\", \\n    \\\"parameters\\\": { \\n        \\\"backend\\\":\", \" { \\n            \\\"type\\\": \\\"boolean\\\", \\n            \\\"defaultValue\\\": false, \\n            \\\"destination\\\": {\", \" \\n                \\\"env\\\": \\\"TF_VAR_backend\\\" \\n            } \\n        } \\n    }, \\n    \\\"invocationImages\\\":\", \" [ \\n        { \\n        \\\"imageType\\\": \\\"docker\\\", \\n        \\\"image\\\": \\\"cnab/terraform:latest\\\" \\n        } \\n\", \"    ], \\n    \\\"credentials\\\": { \\n        \\\"tenant_id\\\": { \\n            \\\"env\\\": \\\"TF_VAR_tenant_id\\\" \\n       \", \" }, \\n        \\\"client_id\\\": { \\n            \\\"env\\\": \\\"TF_VAR_client_id\\\" \\n        }, \\n        \\\"client_secr\", \"et\\\": { \\n            \\\"env\\\": \\\"TF_VAR_client_secret\\\" \\n        }, \\n        \\\"subscription_id\\\": { \\n       \", \"     \\\"env\\\": \\\"TF_VAR_subscription_id\\\" \\n        }, \\n        \\\"ssh_authorized_key\\\": { \\n            \\\"env\\\"\", \": \\\"TF_VAR_ssh_authorized_key\\\" \\n        } \\n    }, \\n    \\\"actions\\\": { \\n        \\\"status\\\": { \\n           \", \" \\\"modifies\\\": true \\n        } \\n    } \\n} \\n\\nFigure 10-18 - An example Terraform file \\n\\nThe bundle.json \", \"also defines a set of parameters that are passed down into the Terraform. \\nParameterization of the b\", \"undle allows for installation in various different environments. \\n\\nThe CNAB format is also flexible,\", \" allowing it to be used against any cloud. It can even be used against \\non-premises solutions such a\", \"s OpenStack. \\n\\n179 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fDevOps Decisions \\n\\nThere are so many great tools in \", \"the DevOps space these days and even more fantastic books and \\npapers on how to succeed. A favorite \", \"book to get started on the DevOps journey is The Phoenix \\nProject, which follows the transformation \", \"of a fictional company from NoOps to DevOps. One thing is \\nfor certain: DevOps is no longer a \\u201cnice \", \"to have\\u201d when deploying complex, Cloud Native Applications. \\nIt\\u2019s a requirement and should be planne\", \"d for and resourced at the start of any project. \\n\\nReferences \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nAzure DevOps \\n\\nAzure\", \" Resource Manager \\n\\nTerraform \\n\\nAzure CLI \\n\\n180 \\n\\nCHAPTER 10 | DevOps \\n\\n \\n \\n\\fCHAPTER  11 \\n\\nSummary: \", \"Architecting \\ncloud-native apps \\n\\nIn summary, here are important conclusions from this guide: \\n\\n\\u2022 \\n\\n\", \"\\u2022 \\n\\n\\u2022 \\n\\nCloud-native is about designing modern applications that embrace rapid change, large scale, \", \"\\nand resilience, in modern, dynamic environments such as public, private, and hybrid clouds. \\n\\nThe C\", \"loud Native Computing Foundation (CNCF) is an influential open-source consortium \\nof over 300 major \", \"corporations. It\\u2019s responsible for driving the adoption of cloud-native \\ncomputing across technology\", \" and cloud stacks. \\n\\nCNCF guidelines recommend that cloud-native applications embrace six important \", \"pillars as \\nshown in Figure 11-1: \\n\\nFigure 11-1. Cloud-native foundational pillars \\n\\n\\u2022 \\n\\nThese cloud\", \"-native pillars include: \\n\\n\\u2013 \\n\\nThe cloud and its underlying service model \\n\\n\\u2013  Modern design princip\", \"les \\n\\n\\u2013  Microservices \\n\\n\\u2013 \\n\\n\\u2013 \\n\\n\\u2013 \\n\\nContainerization and container orchestration \\n\\nCloud-based back\", \"ing services, such as databases and message brokers \\n\\nAutomation, including Infrastructure as Code a\", \"nd code deployment \\n\\n181 \\n\\nCHAPTER 11 | Summary: Architecting cloud-native apps \\n\\n \\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\", \"\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\n\\u2022 \\n\\nKubernetes is the hosting environment of choice for most cloud-native appli\", \"cations. Smaller, \\nsimple services are sometimes hosted in serverless platforms, such as Azure Funct\", \"ions. Among \\nmany key automation features, both environments provide automatic scaling to handle \\nfl\", \"uctuating workload volumes. \\n\\nService communication becomes a significant design decision when const\", \"ructing a cloud-\\nnative application. Applications typically expose an API gateway to manage front-en\", \"d client \\ncommunication. Then backend microservices strive to communicate with each other \\nimplement\", \"ing asynchronous communication patterns, when possible. \\n\\ngRPC is a modern, high-performance framewo\", \"rk that evolves the age-old remote procedure \\ncall (RPC) protocol. Cloud-native applications often e\", \"mbrace gRPC to streamline messaging \\nbetween back-end services. gRPC uses HTTP/2 for its transport p\", \"rotocol. It can be up to 8x \\nfaster than JSON serialization with message sizes 60-80% smaller. gRPC \", \"is open source and \\nmanaged by the Cloud Native Computing Foundation (CNCF). \\n\\nDistributed data is a\", \" model often implemented by cloud-native applications. Applications \\nsegregate business functionalit\", \"y into small, independent microservices. Each microservice \\nencapsulates its own dependencies, data,\", \" and state. The classic shared database model \\nevolves into one of many smaller databases, each alig\", \"ning with a microservice. When the \\nsmoke clears, we emerge with a design that exposes a database-pe\", \"r-microservice model. \\n\\nNo-SQL databases refer to high-performance, non-relational data stores. They\", \" excel in their \\nease-of-use, scalability, resilience, and availability characteristics. High volume\", \" services that \\nrequire sub second response time favor NoSQL datastores. The proliferation of NoSQL \", \"\\ntechnologies for distributed cloud-native systems can\\u2019t be overstated. \\n\\nNewSQL is an emerging data\", \"base technology that combines the distributed scalability of \\nNoSQL and the ACID guarantees of a rel\", \"ational database. NewSQL databases target business \\nsystems that must process high-volumes of data, \", \"across distributed environments, with full \\ntransactional/ACID compliance. The Cloud Native Computin\", \"g Foundation (CNCF) features \\nseveral NewSQL database projects. \\n\\nResiliency is the ability of your \", \"system to react to failure and still remain functional. Cloud-\\nnative systems embrace distributed ar\", \"chitecture where failure is inevitable. Applications must \\nbe constructed to respond elegantly to fa\", \"ilure and quickly return to a fully functioning state. \\n\\nService meshes are a configurable infrastru\", \"cture layer with built-in capabilities to handle \\nservice communication and other cross-cutting chal\", \"lenges. They decouple cross-cutting \\nresponsibilities from your business code. These responsibilitie\", \"s move into a service proxy. \\nReferred to as the Sidecar pattern, the proxy is deployed into a separ\", \"ate process to provide \\nisolation from your business code. \\n\\nObservability is a key design considera\", \"tion for cloud-native applications. As services are \\ndistributed across a cluster of nodes, centrali\", \"zed logging, monitoring, and alerts, become \\nmandatory. Azure Monitor is a collection of cloud-based\", \" tools designed to provide visibility \\ninto the state of your system. \\n\\n182 \\n\\nCHAPTER 11 | Summary: \", \"Architecting cloud-native apps \\n\\n \\n \\n\\f\\u2022 \\n\\n\\u2022 \\n\\nInfrastructure as Code is a widely accepted practice t\", \"hat automates platform provisioning. \\nYour infrastructure and deployments are automated, consistent,\", \" and repeatable. Tools like \\nAzure Resource Manager, Terraform, and the Azure CLI, enable you to dec\", \"laratively script the \\ncloud infrastructure you require. \\n\\nCode automation is a requirement for clou\", \"d-native applications. Modern CI/CD systems help \\nfulfill this principle. They provide separate buil\", \"d and deployment steps that help ensure \\nconsistent and quality code. The build stage transforms the\", \" code into a binary artifact. The \\nrelease stage picks up the binary artifact, applies external envi\", \"ronment configuration, and \\ndeploys it to a specified environment. Azure DevOps and GitHub are full-\", \"featured DevOps \\nenvironments. \\n\\n183 \\n\\nCHAPTER 11 | Summary: Architecting cloud-native apps \\n\\n \\n \\n\\f\"]"