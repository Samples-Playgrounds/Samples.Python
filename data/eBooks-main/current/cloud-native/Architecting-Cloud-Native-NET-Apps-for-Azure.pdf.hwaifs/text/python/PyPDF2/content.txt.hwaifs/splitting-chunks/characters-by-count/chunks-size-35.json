"[\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEDITION v1.0.3  \\nRefer changelog  for th\", \"e book updates and community contributions.  \\nPUBLISHED BY  \\nMicrosoft Developer Division, .NET, and\", \" Visual Studio product teams  \\nA division of Microsoft Corporation  \\nOne Microsoft Way  \\nRedmond, Wa\", \"shington 98052 -6399  \\nCopyright \\u00a9 2023 by Microsoft Corporation  \\nAll rights reserved. No part of t\", \"he contents of this book may be reproduced or transmitted in any \\nform or by any means without the w\", \"ritten permission of the publisher.  \\nThis book is provided \\u201cas -is\\u201d and expresses the author\\u2019s view\", \"s and opinions. The views, opinions, and \\ninformation expressed in this book, including URL and othe\", \"r Internet website references, may change \\nwithout notice.  \\nSome examples depicted herein are provi\", \"ded for illustration only and are fictitious. No real association \\nor connection is intended or shou\", \"ld be inferred.  \\nMicrosoft and the trademarks listed at https://www.microsoft.com  on the \\u201cTrademar\", \"ks\\u201d webpage are \\ntrademarks of the Microsoft group of companies.  \\nMac and macOS are trademarks of A\", \"pple Inc.  \\nThe Docker whale logo is a registered trademark of Docker, Inc.  Used by permission.  \\nA\", \"ll other marks and logos are property of their respective owners.  \\nAuthors:  \\nRob Vettor , Principa\", \"l MTC (Microsoft Technology Center) Architect for Cloud App Innovation - \\nthinkingincloudnative.com \", \", Microsoft  \\nSteve \\u201cardalis\\u201d Smith , Software Architect and Trainer - Ardalis.com  \\nParticipants an\", \"d Reviewers:  \\nCesar De la Torre , Principal Program Manager, .NET team, Microsoft  \\nNish Anil , Sen\", \"ior Program Manager, .NET team, Microsoft  \\nJeremy Likness , Senior Program Manager, .NET team, Micr\", \"osoft  \\nCecil Phillip , Senior Cloud Advocate, Microsoft  \\nSumit Ghosh , Principal Consultant at Neu\", \"desic  \\nEditors:  \\nMaira Wenzel , Program Manager, .NET team, Microsoft   \\nDavid Pine , Senior Conte\", \"nt Developer, .NET docs, Microsoft  \\nVersion  \\nThis guide has been written to cover .NET 7  version \", \"along with many additional updates related to \\nthe same \\u201cwave\\u201d of technologies (that is, Azure and a\", \"dditional third -party technologies) coinciding in \\ntime with the .NET 7 release.  \\nWho should use t\", \"his guide  \\nThe audience for this guide is mainly developers, development leads, and architects who \", \"are \\ninterested in learning how to build applications designed for the cloud.  \\nA secondary audience\", \" is technical decision -makers who plan to choose whether to build their \\napplications using a cloud\", \" -native approach.  \\nHow you can use this guide  \\nThis guide begins by defining cloud native and int\", \"roducing a reference application built using cloud -\\nnative principles and technologies. Beyond thes\", \"e first two chapters, the rest of the book is broken up \\ninto specific chapters focused on topics co\", \"mmon to m ost cloud -native applications. You can jump to \\nany of these chapters to learn about clou\", \"d -native approaches to:  \\n\\u2022 Data and data access  \\n\\u2022 Communication patterns  \\n\\u2022 Scaling and scalabi\", \"lity  \\n\\u2022 Application resiliency  \\n\\u2022 Monitoring and health  \\n\\u2022 Identity and security  \\n\\u2022 DevOps  \\nThi\", \"s guide is available both in PDF form and online. Feel free to forward this document or links to its\", \" \\nonline version to your team to help ensure common understanding of these topics. Most of these \\nto\", \"pics benefit from a consistent understanding of the underlying principles and patterns, as  well as \", \"\\nthe trade -offs involved in decisions related to these topics. Our goal with this document is to eq\", \"uip \\nteams and their leaders with the information they need to make well -informed decisions for the\", \"ir \\napplications\\u2019 architecture, development, and hos ting.  \\ni Contents   \\nContents  \\nIntroduction t\", \"o cloud -native applications  ................................ ................................ ....\", \"........  1 \\nCloud -native computing  ................................ .............................\", \"... ................................ ................................ ..................  3 \\nWhat is\", \" Cloud Native?  ................................ ................................ ..................\", \".............. ................................ ............................  4 \\nThe pillars of clou\", \"d native  ................................ ................................ ........................\", \"........ ................................ ................  5 \\nThe cloud  ..........................\", \"...... ................................ ................................ ...........................\", \"..... ................................ ................  5 \\nModern design  .........................\", \"....... ................................ ................................ ..........................\", \"...... ................................ ..... 6 \\nMicroservices  ................................ ...\", \"............................. ................................ ................................ ....\", \"............................ ........ 9 \\nContainers  ................................ ..............\", \".................. ................................ ................................ ...............\", \"................. ...........  12 \\nBacking services  ................................ ..............\", \".................. ................................ ................................ ...............\", \".................  15 \\nAutomation  ................................ ................................\", \" ................................ ................................ ................................ \", \"......... 17 \\nCandidate apps for cloud native  ................................ ....................\", \"............ ................................ ................................ ..... 19 \\nModernizing\", \" legacy apps  ................................ ................................ ....................\", \"............ ................................ ..............  19 \\nSummary  .........................\", \"....... ................................ ................................ ..........................\", \"...... ................................ ..............  21 \\nIntroducing eShopOnContainers reference \", \"app  ................................ ................................  22 \\nFeatures and requirement\", \"s  ................................ ................................ ...............................\", \". ................................ ...............  23 \\nOverview of the code  ......................\", \".......... ................................ ................................ .......................\", \"......... ..........................  25 \\nUnderstanding microservices  .............................\", \"... ................................ ................................ ..............................\", \".. ...........  27 \\nMapping eShopOnContainers to Azure Services  ................................ ..\", \".............................. ................................ ..... 27 \\nContainer orchestration an\", \"d clustering  ................................ ................................ ....................\", \"............ ...................  28 \\nAPI Gateway  ................................ ................\", \"................ ................................ ................................ .................\", \"............... ....... 28 \\nData  ................................ ................................ \", \"................................ ................................ ................................ .\", \"......................  29 \\nEvent Bus  ................................ ............................\", \".... ................................ ................................ .............................\", \"... .............  30 \\nResiliency  ................................ ................................\", \" ................................ ................................ ................................ \", \".............  30 \\nDeploying eShopOnContainers to Azure  ................................ ..........\", \"...................... ................................ ....................  30 \\nAzure Kubernetes S\", \"ervice  ................................ ................................ ..........................\", \"...... ................................ .............  30 \\nDeploying to Azure Kubernetes Service usi\", \"ng Helm  ................................ ................................ .........................\", \"  30 \\nAzure Functions and Logic Apps (Serverless)  ................................ ................\", \"................ ................................ ....... 32 \\nCentralized configuration  ...........\", \"..................... ................................ ................................ ............\", \".................... ..................  33  \\nii Contents  Azure App Configuration  ................\", \"................ ................................ ................................ .................\", \"............... ..............  33 \\nAzure Key Vault  ................................ ..............\", \".................. ................................ ................................ ...............\", \"................. . 34 \\nConfiguration in eShop  ................................ ...................\", \"............. ................................ ................................ ..................  \", \"34 \\nReferences  ................................ ................................ ..................\", \".............. ................................ ................................ ...........  34 \\nSc\", \"aling cloud -native applications  ................................ ................................ \", \"........................  36 \\nLeveraging containers and orchestrators  .............................\", \"... ................................ ................................ ....................  36 \\nChal\", \"lenges with monolithic deployments  ................................ ...............................\", \". ................................ ..............  36 \\nWhat are the benefits of containers and orche\", \"strators?  ................................ ................................ ..................  38 \", \"\\nWhat are the scaling benefits?  ................................ ................................ .\", \"............................... ................................ .... 40 \\nWhat scenarios are ideal f\", \"or containers and orchestrators? ................................ ................................ .\", \"..........  42 \\nWhen should you avoid using containers and orchestrators?  .........................\", \"....... ................................ ....... 42 \\nDevelopment resources  ........................\", \"........ ................................ ................................ .........................\", \"....... .................  42 \\nLeveraging serverless functions  ................................ ...\", \"............................. ................................ ................................ ....\", \".. 46 \\nWhat is serverless?  ................................ ................................ ......\", \".......................... ................................ ...........................  47 \\nWhat ch\", \"allenges are solved by serverless?  ................................ ...............................\", \". ................................ ............  47 \\nWhat is the difference between a microservice a\", \"nd a serverless function?  ................................ .............  47 \\nWhat scenarios are ap\", \"propriate for serverless?  ................................ ................................ .......\", \"......................... ... 47 \\nWhen should you avoid serverless?  ...............................\", \". ................................ ................................ ..........................  48 \\n\", \"Combining containers and serverless approaches  ................................ ...................\", \"............. ................................ .. 49 \\nWhen does it make sense to use containers with\", \" serverless?  ................................ ................................ ........ 49 \\nWhen sh\", \"ould you avoid using containers with Azure Functions?  ................................ ............\", \"....................  49 \\nHow to combine serverless and Docker containers  .........................\", \"....... ................................ ...........................  49 \\nHow to combine serverless \", \"and Kubernetes with KEDA  ................................ ................................ ........\", \"..........  50 \\nDeploying containers in Azure  ................................ ....................\", \"............ ................................ ................................ ........ 50 \\nAzure Co\", \"ntainer Registry  ................................ ................................ ................\", \"................ ................................ ..............  50 \\nACR Tasks  ...................\", \"............. ................................ ................................ ....................\", \"............ ................................ ............  52 \\nAzure Kubernetes Service  ..........\", \"...................... ................................ ................................ ...........\", \"..................... .............  52 \\nAzure Bridge to Kubernetes  ...............................\", \". ................................ ................................ ................................\", \" ......... 53 \\nScaling containers and serverless applications  ................................ ....\", \"............................ ................................ ......... 53 \\nThe simple solution: sca\", \"ling up  ................................ ................................ .........................\", \"....... ................................ .. 53 \\nScaling out cloud -native apps  ....................\", \"............ ................................ ................................ .....................\", \"........... .... 54 \\nOther container deployment options  ................................ ..........\", \"...................... ................................ ...........................  55  \\niii Conten\", \"ts  When does it make sense to deploy to App Service for Containers?  ..............................\", \".. .........................  55 \\nHow to deploy to App Service for Containers  .....................\", \"........... ................................ ................................ ...... 55 \\nWhen does i\", \"t make sense to deploy to Azure Container Instances?  ................................ .............\", \".............  55 \\nHow to deploy an app to Azure Container Instances  ..............................\", \".. ................................ ........................  55 \\nReferences  ......................\", \".......... ................................ ................................ .......................\", \"......... ................................ ...........  56 \\nCloud -native communication patterns  ..\", \".............................. ................................ ...............  58 \\nCommunication c\", \"onsiderations  ................................ ................................ ...................\", \"............. ................................ ...... 58 \\nFront -end client communication  .........\", \"....................... ................................ ................................ ..........\", \"...................... .... 60 \\nSimple Gateways  ................................ ..................\", \".............. ................................ ................................ ...................\", \"...........  62 \\nAzure Application Gateway  ................................ .......................\", \"......... ................................ ................................ ..........  63 \\nAzure AP\", \"I Management  ................................ ................................ ....................\", \"............ ................................ .................  63 \\nReal-time communication  ......\", \".......................... ................................ ................................ .......\", \"......................... ............  66 \\nService -to-service communication  .....................\", \"........... ................................ ................................ ......................\", \"..........  67 \\nQueries  ................................ ................................ .........\", \"....................... ................................ ................................ ..........\", \".......  68 \\nCommands  ................................ ................................ ...........\", \"..................... ................................ ................................ ..........  \", \"71 \\nEvents  ................................ ................................ ......................\", \".......... ................................ ................................ ....................  7\", \"4 \\ngRPC  ................................ ................................ .........................\", \"....... ................................ ................................ ..........................\", \".  80 \\nWhat is gRPC?  ................................ ................................ ............\", \".................... ................................ ................................ ... 80 \\ngRPC \", \"Benefits  ................................ ................................ ........................\", \"........ ................................ ................................ .... 80 \\nProtocol Buffers\", \"  ................................ ................................ ................................\", \" ................................ ................................  81 \\ngRPC support in .NET  ......\", \".......................... ................................ ................................ .......\", \"......................... .....................  81 \\ngRPC usage  ................................ ..\", \".............................. ................................ ................................ ...\", \"............................. ......... 82 \\ngRPC implementation  ................................ ..\", \".............................. ................................ ................................ ...\", \".................  83 \\nLooking ahead  ................................ .............................\", \"... ................................ ................................ ..............................\", \".. ... 85 \\nService Mesh communication infrastructure  ................................ .............\", \"................... ................................ .............  85 \\nSummary  ...................\", \"............. ................................ ................................ ....................\", \"............ ................................ ..............  86 \\nCloud -native data patterns  .....\", \"........................... ................................ ................................ .. 88 \", \"\\nDatabase -per-microservice, why?  ................................ ................................\", \" ................................ ................................ .. 89 \\nCross -service queries  ..\", \".............................. ................................ ................................ ...\", \"............................. ...........................  90 \\nDistributed transactions  ...........\", \"..................... ................................ ................................ ............\", \".................... .....................  91 \\nHigh volume data  ................................ .\", \"............................... ................................ ................................ ..\", \".............................. . 93 \\nCQRS  ................................ ........................\", \"........ ................................ ................................ .........................\", \"....... .....................  93  \\niv Contents  Event sourcing  ................................ ..\", \".............................. ................................ ................................ ...\", \"............................. ... 94 \\nRelational vs.  NoSQL data  ................................ .\", \"............................... ................................ ................................ ..\", \"...............  96 \\nThe CAP theorem  ................................ .............................\", \"... ................................ ................................ ............................. \", \" 97 \\nConsiderations for relational vs.  NoSQL systems  ................................ ............\", \".................... ................................  99 \\nDatabase as a Service  ..................\", \".............. ................................ ................................ ...................\", \"............. .....................  99 \\nAzure relational databases  ...............................\", \". ................................ ................................ ................................\", \" ......... 100 \\nAzure SQL Database ................................ ................................\", \" ................................ ................................ ......................  100 \\nOpen\", \" -source databases in Azure  ................................ ................................ .....\", \"........................... .............................  101 \\nNoSQL data in Azure  ...............\", \"................. ................................ ................................ ................\", \"................ ....................  102 \\nNewSQL databases  ................................ .....\", \"........................... ................................ ................................ ......\", \"..................  106 \\nData migration to the cloud  ................................ .............\", \"................... ................................ ................................ ...... 108 \\nCa\", \"ching in a cloud -native app  ................................ ................................ ....\", \"............................ ................................ ....... 108 \\nWhy?  ...................\", \"............. ................................ ................................ ....................\", \"............ ................................ ....................  108 \\nCaching architecture  .....\", \"........................... ................................ ................................ ......\", \".......................... .....................  109 \\nAzure Cache for Redis  ......................\", \".......... ................................ ................................ .......................\", \"......... ..................  110 \\nElasticsearch in a cloud -native app  ...........................\", \"..... ................................ ................................ ............................\", \".  110 \\nSummary  ................................ ................................ .................\", \"............... ................................ ................................ ............  111 \", \"\\nCloud -native resiliency  ................................ ................................ .......\", \"......................... ....... 113 \\nApplication resiliency patterns  ............................\", \".... ................................ ................................ .............................\", \"... ...... 114 \\nCircuit breaker pattern  ................................ ..........................\", \"...... ................................ ................................ .................  116 \\nTes\", \"ting for resiliency  ................................ ................................ .............\", \"................... ................................ .....................  117 \\nAzure platform resi\", \"liency  ................................ ................................ ..........................\", \"...... ................................ .................  117 \\nDesign with resiliency  ............\", \".................... ................................ ................................ .............\", \"................... ...................  118 \\nDesign with redundancy ...............................\", \". ................................ ................................ ................................\", \" ..............  118 \\nDesign for scalability  ................................ .....................\", \"........... ................................ ................................ ..................... \", \" 120 \\nBuilt-in retry in services  ................................ ................................ \", \"................................ ................................ ...............  121 \\nResilient co\", \"mmunications  ................................ ................................ ....................\", \"............ ................................ ................  122 \\nService mesh  .................\", \"............... ................................ ................................ ..................\", \".............. ................................ .... 123 \\nIstio and Envoy  .........................\", \"....... ................................ ................................ ..........................\", \"...... ................................  124 \\nIntegration with Azure Kubernetes Services  ..........\", \"...................... ................................ ................................ ....... 125\", \" \\nMonitoring and health  ................................ ................................ .........\", \"....................... ........  126 \\nObservability patterns  ................................ ....\", \"............................ ................................ ................................ .....\", \"..................  126  \\nv Contents  When to use logging  ................................ ........\", \"........................ ................................ ................................ .........\", \"...........  126 \\nChallenges with detecting and responding to potential app health issues  .........\", \"....................... ...........  130 \\nChallenges with reacting to critical problems in cloud -na\", \"tive apps  ................................ ..........................  130 \\nLogging with Elastic St\", \"ack  ................................ ................................ .............................\", \"... ................................ ...............  131 \\nElastic Stack  ..........................\", \"...... ................................ ................................ ...........................\", \"..... ................................ ...... 131 \\nWhat are the advantages of Elastic Stack?  ......\", \".......................... ................................ ................................ .......\", \"...  132 \\nLogstash  ................................ ................................ ..............\", \".................. ................................ ................................ .............  \", \"132 \\nElasticsearch  ................................ ................................ ..............\", \".................. ................................ ................................ ..... 133 \\nVisu\", \"alizing information with Kibana web dashboards  ................................ ...................\", \"............. ....................  133 \\nInstalling Elastic Stack on Azure  ........................\", \"........ ................................ ................................ .........................\", \"......  134 \\nReferences  ................................ ................................ .........\", \"....................... ................................ ................................ ......... \", \"134 \\nMonitoring in Azure Kubernetes Services  ................................ .....................\", \"........... ................................ .................  134 \\nAzure Monitor for Containers  .\", \"............................... ................................ ................................ ..\", \".............................. ... 134 \\nLog.Finalize()  ................................ ...........\", \"..................... ................................ ................................ ............\", \".................... .... 136 \\nAzure Monitor  ................................ .....................\", \"........... ................................ ................................ ......................\", \".......... ...... 136 \\nGathering logs and metrics  ................................ ................\", \"................ ................................ ................................ ........ 137 \\nRep\", \"orting data  ................................ ................................ .....................\", \"........... ................................ ................................  137 \\nDashboards  ....\", \"............................ ................................ ................................ .....\", \"........................... ................................ ....... 138 \\nAlerts  ..................\", \".............. ................................ ................................ ...................\", \"............. ................................ ...................  140 \\nReferences  ...............\", \"................. ................................ ................................ ................\", \"................ ................................ ......... 141 \\nCloud -native identity  ...........\", \"..................... ................................ ................................ ..........  \", \"142 \\nReferences  ................................ ................................ .................\", \"............... ................................ ................................ .............  142\", \" \\nAuthentication and authorization in cloud -native apps  ................................ .........\", \"....................... .....................  142 \\nReferences  ................................ ...\", \"............................. ................................ ................................ ....\", \"............................ ......... 143 \\nAzure Active Directory  ................................\", \" ................................ ................................ ................................ \", \"......................  143 \\nReferences  ................................ ..........................\", \"...... ................................ ................................ ...........................\", \"..... ......... 143 \\nIdentityServer for cloud -native applications  ................................\", \" ................................ ................................ ............  144 \\nCommon web app\", \" scenarios  ................................ ................................ ......................\", \".......... ................................ ..... 144 \\nGetting started  ............................\", \".... ................................ ................................ .............................\", \"... ................................  145 \\nConfiguration  ................................ .........\", \"....................... ................................ ................................ ..........\", \"...................... ... 145 \\nJavaScript clients  ................................ ...............\", \"................. ................................ ................................ ................\", \"............  146 \\nReferences  ................................ ................................ ...\", \"............................. ................................ ................................ ....\", \"..... 146  \\nvi Contents  Cloud -native security  ................................ ..................\", \".............. ................................ ..........  147 \\nAzure security for cloud -native ap\", \"ps  ................................ ................................ ..............................\", \".. ..........................  147 \\nThreat modeling  ................................ ..............\", \".................. ................................ ................................ ...............\", \"..............  148 \\nPrinciple of least privilege  ................................ ................\", \"................ ................................ ................................ ...........  148 \", \"\\nPenetration testing  ................................ ................................ ............\", \".................... ................................ ........................  149 \\nMonitoring  ...\", \"............................. ................................ ................................ ....\", \"............................ ................................ ........ 149 \\nSecuring the build  ....\", \"............................ ................................ ................................ .....\", \"........................... ..........................  149 \\nBuilding secure code  .................\", \"............... ................................ ................................ ..................\", \".............. ....................  150 \\nBuilt-in security  ................................ ......\", \".......................... ................................ ................................ .......\", \"........................  150 \\nAzure network infrastructure  ................................ ......\", \".......................... ................................ ................................ ..... 1\", \"50 \\nRole-based access control for restricting access to Azure resources ............................\", \".... ........................  152 \\nSecurity Principals  ................................ ..........\", \"...................... ................................ ................................ ...........\", \"...............  152 \\nRoles  ................................ ................................ .....\", \"........................... ................................ ................................ ......\", \"..............  153 \\nScopes  ................................ ................................ .....\", \"........................... ................................ ................................ ......\", \"..........  154 \\nDeny  ................................ ................................ ...........\", \"..................... ................................ ................................ ............\", \"........  154 \\nChecking access  ................................ ................................ ..\", \".............................. ................................ ..............................  154 \", \"\\nSecuring secrets  ................................ ................................ ...............\", \"................. ................................ ..............................  155 \\nAzure Key Va\", \"ult  ................................ ................................ .............................\", \"... ................................ ...............................  155 \\nKubernetes  .............\", \"................... ................................ ................................ ..............\", \".................. ................................ ........ 155 \\nEncryption in transit and at rest \", \" ................................ ................................ ................................ \", \"...............................  156 \\nKeeping secure  ................................ .............\", \"................... ................................ ................................ ..............\", \"..................  160 \\nDevOps  ................................ ................................ .\", \"............................... ................................ . 161 \\nAzure DevOps  ..............\", \".................. ................................ ................................ ...............\", \"................. ................................ ....... 162 \\nGitHub Actions  ....................\", \"............ ................................ ................................ .....................\", \"........... ................................ ..... 163 \\nSource control  ............................\", \".... ................................ ................................ .............................\", \"... ................................ ...... 163 \\nRepository per microservice  ......................\", \".......... ................................ ................................ .......................\", \"......... ...... 164 \\nSingle repository  ................................ ..........................\", \"...... ................................ ................................ ...........................\", \".  166 \\nStandard directory structure  ................................ .............................\", \"... ................................ ................................ ...... 167 \\nTask management  .\", \"............................... ................................ ................................ ..\", \".............................. ..............................  167 \\nCI/CD pipelines  ...............\", \"................. ................................ ................................ ................\", \"................ ................................ .... 169 \\nAzure Builds  ..........................\", \"...... ................................ ................................ ...........................\", \"..... ................................ ...... 170 \\nAzure DevOps releases  ..........................\", \"...... ................................ ................................ ...........................\", \"..... ................  172  \\nvii Contents  Everybody gets a build pipeline  .......................\", \"......... ................................ ................................ ........................\", \".......  173 \\nVersioning releases  ................................ ................................\", \" ................................ ................................ ........................  173 \\nFe\", \"ature flags  ................................ ................................ .....................\", \"........... ................................ ................................ ......... 173 \\nImpleme\", \"nting feature flags  ................................ ................................ .............\", \"................... ................................ ........ 174 \\nInfrastructure as code  .........\", \"....................... ................................ ................................ ..........\", \"...................... .......................  175 \\nAzure Resource Manager templates  .............\", \"................... ................................ ................................ ..............\", \"........  175 \\nTerraform  ................................ ................................ ........\", \"........................ ................................ ................................ .........\", \"..  176 \\nAzure CLI Scripts and Tasks ................................ ..............................\", \".. ................................ ................................ ........ 177 \\nCloud Native Appl\", \"ication Bundles  ................................ ................................ .................\", \"............... ...............................  178 \\nDevOps Decisions  ............................\", \".... ................................ ................................ .............................\", \"... ..........................  180 \\nReferences  ................................ ..................\", \".............. ................................ ................................ ...................\", \"............. ......... 180 \\nSummary: Architecting cloud -native apps  .............................\", \"... ................................ ....... 181  \\n1 CHAPTER 1 | Introduction to cloud -native appli\", \"cations  \\n CHAPTER  1 \\nIntroduction to cloud -\\nnative applications  \\nAnother day, at the office, wor\", \"king on \\u201cthe next big thing.\\u201d  \\nYour cellphone rings. It\\u2019s your friendly recruiter - the one who cal\", \"ls daily with exciting new \\nopportunities.  \\nBut this time it\\u2019s different: Start -up, equity, and pl\", \"enty of funding.  \\nThe mention of the cloud, microservices, and cutting -edge technology pushes you \", \"over the edge.  \\nFast forward a few weeks and you\\u2019re now a new employee in a design session architec\", \"ting a major \\neCommerce application. You\\u2019re going to compete with the leading eCommerce sites.  \\nHow\", \" will you build it?  \\nIf you follow the guidance from past 15 years, you\\u2019ll most likely build the sy\", \"stem shown in Figure 1.1.  \\n \\nFigure 1 -1. Traditional monolithic design  \\nYou construct a large cor\", \"e application containing all of your domain logic. It includes modules such as \\nIdentity, Catalog, O\", \"rdering, and more. They directly communicate with each other within a single \\nserver process. The mo\", \"dules share a large relational database. The core exposes functionality via an \\nHTML interface and a\", \" mobile app.  \\nCongratulations! You just created a monolithic application.  \\nNot all is bad. Monolit\", \"hs offer some distinct advantages. For example, they\\u2019re straightforward to\\u2026  \\n \\n2 CHAPTER 1 | Introd\", \"uction to cloud -native applications  \\n \\u2022 build  \\n\\u2022 test \\n\\u2022 deploy  \\n\\u2022 troubleshoot  \\n\\u2022 vertically s\", \"cale  \\nMany successful apps that exist today were created as monoliths. The app is a hit and continu\", \"es to \\nevolve, iteration after iteration, adding more functionality.  \\nAt some point, however, you b\", \"egin to feel uncomfortable. You find yourself losing control of the \\napplication. As time goes on, t\", \"he feeling becomes more intense, and you eventually enter a state \\nknown as the Fear Cycle:  \\n\\u2022 The \", \"app has become so overwhelmingly complicated that no single person understands it.  \\n\\u2022 You fear maki\", \"ng changes - each change has unintended and costly side effects.  \\n\\u2022 New features/fixes become trick\", \"y, time-consuming, and expensive to implement.  \\n\\u2022 Each release becomes as small as possible and req\", \"uires a full deployment of the entire \\napplication.  \\n\\u2022 One unstable component can crash the entire \", \"system.  \\n\\u2022 New technologies and frameworks aren\\u2019t an option.  \\n\\u2022 It\\u2019s difficult to implement agile \", \"delivery methodologies.  \\n\\u2022 Architectural erosion sets in as the code base deteriorates with never -\", \"ending \\u201cquick fixes.\\u201d  \\n\\u2022 Finally, the consultants  come in and tell you to rewrite it.  \\nSound fami\", \"liar?  \\nMany organizations have addressed this monolithic fear cycle by adopting a cloud -native app\", \"roach to \\nbuilding systems. Figure 1 -2 shows the same system built applying cloud -native technique\", \"s and \\npractices.   \\n3 CHAPTER 1 | Introduction to cloud -native applications  \\n  \\nFigure 1 -2. Clou\", \"d -native design  \\nNote how the application is decomposed across a set of small isolated microservic\", \"es. Each service is \\nself-contained and encapsulates its own code, data, and dependencies. Each is d\", \"eployed in a software \\ncontainer and managed by a container orchestrator. Ins tead of a large relati\", \"onal database, each \\nservice owns it own datastore, the type of which vary based upon the data needs\", \". Note how some \\nservices depend on a relational database, but other on NoSQL databases. One service\", \" stores its state \\nin a distributed c ache. Note how all traffic routes through an API Gateway servi\", \"ce that is responsible \\nfor routing traffic to the core back -end services and enforcing many cross \", \"-cutting concerns. Most \\nimportantly, the application takes full advantage of the scalability, ava i\", \"lability, and resiliency features \\nfound in modern cloud platforms.  \\nCloud -native computing  \\nHmm\\u2026\", \" We just used the term, Cloud Native . Your first thought might be, \\u201cWhat exactly does that \\nmean?\\u201d \", \"Another industry buzzword concocted by software vendors to market more stuff?\\u201d  \\nFortunately it\\u2019s fa\", \"r different, and hopefully this book will help convince you.  \\nWithin a short time, cloud native has\", \" become a driving trend in the software industry. It\\u2019s a new way \\nto construct large, complex system\", \"s. The approach takes full advantage of modern software \\ndevelopment practices, technologies, and cl\", \"oud infrastructure. C loud native changes the way you \\ndesign, implement, deploy, and operationalize\", \" systems.  \\nUnlike the continuous hype that drives our industry, cloud native is for-real. Consider \", \"the Cloud Native \\nComputing Foundation  (CNCF), a consortium of over 400 major corporations. Its cha\", \"rter is to make \\n \\n4 CHAPTER 1 | Introduction to cloud -native applications  \\n cloud -native computi\", \"ng ubiquitous across technology and cloud stacks. As one of the most influential \\nopen -source group\", \"s, it hosts many of the fastest -growing open source -projects in GitHub. These \\nprojects include Ku\", \"bernetes , Prometheus , Helm , Envoy , and gRPC . \\nThe CNCF fosters an ecosystem of open -source and\", \" vendor -neutrality. Following that lead, this book \\npresents cloud -native principles, patterns, an\", \"d best practices that are technology agnostic. At the \\nsame time, we discuss the services and infras\", \"tructure ava ilable in the Microsoft Azure cloud for \\nconstructing cloud -native systems.  \\nSo, what\", \" exactly is Cloud Native? Sit back, relax, and let us help you explore this new world.  \\nWhat is Clo\", \"ud Native?  \\nStop what you\\u2019re doing and ask your colleagues to define the term \\u201cCloud Native\\u201d. There\", \"\\u2019s a good \\nchance you\\u2019ll get several different answers.  \\nLet\\u2019s start with a simple definition:  \\nCl\", \"oud -native architecture and technologies are an approach to designing, constructing, and operating \", \"\\nworkloads that are built in the cloud and take full advantage of the cloud computing model.  \\nThe C\", \"loud Native Computing Foundation  provides the official definition : \\nCloud -native technologies emp\", \"ower organizations to build and run scalable applications in modern, \\ndynamic environments such as p\", \"ublic, private, and hybrid clouds. Containers, service meshes, \\nmicroservices, immutable infrastruct\", \"ure, and declarative APIs exe mplify this approach.  \\nThese techniques enable loosely coupled system\", \"s that are resilient, manageable, and observable. \\nCombined with robust automation, they allow engin\", \"eers to make high -impact changes frequently and \\npredictably with minimal toil.  \\nCloud native is a\", \"bout speed  and agility . Business systems are evolving from enabling business \\ncapabilities to weap\", \"ons of strategic transformation that accelerate business velocity and growth. It\\u2019s \\nimperative to ge\", \"t new ideas to market immediately.  \\nAt the same time, business systems have also become increasingl\", \"y complex with users demanding \\nmore. They expect rapid responsiveness, innovative features, and zer\", \"o downtime. Performance \\nproblems, recurring errors, and the inability to move fast are no long er a\", \"cceptable. Your users will visit \\nyour competitor. Cloud -native systems are designed to embrace rap\", \"id change, large scale, and \\nresilience.  \\nHere are some companies who have implemented cloud -nativ\", \"e techniques. Think about the speed, \\nagility, and scalability they\\u2019ve achieved.  \\nCompany  Experien\", \"ce  \\nNetflix  Has 600+ services in production. Deploys 100 \\ntimes per day.  \\nUber  Has 1,000+ servic\", \"es in production. Deploys \\nseveral thousand times each week.   \\n5 CHAPTER 1 | Introduction to cloud \", \"-native applications  \\n Company  Experience  \\nWeChat  Has 3,000+ services in production. Deploys \\n1,\", \"000 times a day.  \\nAs you can see, Netflix, Uber, and, WeChat expose cloud -native systems that cons\", \"ist of many \\nindependent services. This architectural style enables them to rapidly respond to marke\", \"t conditions. \\nThey instantaneously update small areas of a live, complex appl ication, without a fu\", \"ll redeployment. \\nThey individually scale services as needed.  \\nThe pillars of cloud native  \\nThe sp\", \"eed and agility of cloud native derive from many factors. Foremost is cloud infrastructure . But \\nth\", \"ere\\u2019s more: Five other foundational pillars shown in Figure 1 -3 also provide the bedrock for cloud \", \"-\\nnative systems.  \\n \\nFigure 1 -3. Cloud -native foundational pillars  \\nLet\\u2019s take some time to bett\", \"er understand the significance of each pillar.  \\nThe cloud  \\nCloud -native systems take full advanta\", \"ge of the cloud service model.  \\nDesigned to thrive in a dynamic, virtualized cloud environment, the\", \"se systems make extensive use of \\nPlatform as a Service (PaaS)  compute infrastructure and managed s\", \"ervices. They treat the underlying \\ninfrastructure as disposable  - provisioned in minutes and resiz\", \"ed, scaled, or destroyed on demand \\u2013 \\nvia automation.  \\nConsider the widely accepted DevOps concept \", \"of Pets vs.  Cattle . In a traditional data center, servers \\nare treated as Pets: a physical machine\", \", given a meaningful name, and cared  for. You scale by adding \\nmore resources to the same machine (\", \"scaling up). If the server becomes sick, you nurse it back to \\nhealth. Should the server become unav\", \"ailable, everyone notices.  \\nThe Cattle  service model is different. You provision each instance as \", \"a virtual machine or container. \\nThey\\u2019re identical and assigned a system identifier such as Service \", \"-01, Service -02, and so on. You scale \\nby creating more of them (scaling out). When one becomes una\", \" vailable, nobody notices.  \\n \\n6 CHAPTER 1 | Introduction to cloud -native applications  \\n The cattl\", \"e model embraces immutable infrastructure . Servers aren\\u2019t repaired or modified. If one fails or \\nre\", \"quires updating, it\\u2019s destroyed and a new one is provisioned \\u2013 all done via automation.  \\nCloud -nat\", \"ive systems embrace the Cattle service model. They continue to run as the infrastructure \\nscales in \", \"or out with no regard to the machines upon which they\\u2019re running.  \\nThe Azure cloud platform support\", \"s this type of highly elastic infrastructure with automatic scaling, \\nself-healing, and monitoring c\", \"apabilities.  \\nModern design  \\nHow would you design a cloud -native app? What would your architectur\", \"e look like? To what \\nprinciples, patterns, and best practices would you adhere? What infrastructure\", \" and operational \\nconcerns would be important?  \\nThe Twelve -Factor Application  \\nA widely accepted \", \"methodology for constructing cloud -based applications is the Twelve -Factor \\nApplication . It descr\", \"ibes a set of principles and practices that developers follow to construct \\napplications optimized f\", \"or modern cloud environments. Special attention is given to portability across \\nenvironments and dec\", \"larative automation.  \\nWhile applicable to any web -based application, many practitioners consider T\", \"welve -Factor a solid \\nfoundation for building cloud -native apps. Systems built upon these principl\", \"es can deploy and scale \\nrapidly and add features to react quickly to market changes . \\nThe followin\", \"g table highlights the Twelve -Factor methodology:  \\nFactor  Explanation  \\n1 - Code Base  A single c\", \"ode base for each microservice, stored \\nin its own repository. Tracked with version \\ncontrol, it can\", \" deploy to multiple environments \\n(QA, Staging, Production).  \\n2 - Dependencies  Each microservice i\", \"solates and packages its own \\ndependencies, embracing changes without \\nimpacting the entire system. \", \" \\n3 - Configurations  Configuration information is moved out of the \\nmicroservice and externalized t\", \"hrough a \\nconfiguration management tool outside of the \\ncode. The same deployment can propagate \\nacr\", \"oss environments with the correct \\nconfiguration applied.  \\n4 - Backing Services  Ancillary resource\", \"s (data stores, caches, \\nmessage brokers) should be exposed via an \\naddressable URL. Doing so decoup\", \"les the \\nresource from the application, enabling it to be \\ninterchangeable.   \\n7 CHAPTER 1 | Introdu\", \"ction to cloud -native applications  \\n Factor  Explanation  \\n5 - Build, Release, Run  Each release m\", \"ust enforce a strict separation \\nacross the build, release, and run stages. Each \\nshould be tagged w\", \"ith a unique ID and support \\nthe ability to roll back. Modern CI/CD systems \\nhelp fulfill this princ\", \"iple.  \\n6 - Processes  Each microservice should execute in its own \\nprocess, isolated from other run\", \"ning services. \\nExternalize required state to a backing service \\nsuch as a distributed cache or data\", \" store.  \\n7 - Port Binding  Each microservice should be self -contained with \\nits interfaces and fun\", \"ctionality exposed on its \\nown port. Doing so provides isolation from \\nother microservices.  \\n8 - Co\", \"ncurrency  When capacity needs to increase, scale out \\nservices horizontally across multiple identic\", \"al \\nprocesses (copies) as opposed to scaling -up a \\nsingle large instance on the most powerful \\nmach\", \"ine available. Develop the application to be \\nconcurrent making scaling o ut in cloud \\nenvironments \", \"seamless.  \\n9 - Disposability  Service instances should be disposable. Favor \\nfast startup to increa\", \"se scalability opportunities \\nand graceful shutdowns to leave the system in a \\ncorrect state. Docker\", \" containers along with an \\norchestrator inherently satisfy this requirement.  \\n10 - Dev/Prod Parity \", \" Keep environments across the application \\nlifecycle as similar as possible, avoiding costly \\nshortc\", \"uts. Here, the adoption of containers can \\ngreatly contribute by promoting the same \\nexecution envir\", \"onment.  \\n11 - Logging  Treat logs generated by microservices as event \\nstreams. Process them with a\", \"n event \\naggregator. Propagate log data to data -\\nmining/log management tools like Azure \\nMonitor or\", \" Splunk and eventually to long -term \\narchival.  \\n12 - Admin Processes  Run administrative/managemen\", \"t tasks, such as \\ndata cleanup or computing analytics, as one -off \\nprocesses. Use independent tools\", \" to invoke \\nthese tasks from the production environment, \\nbut separately from the application.   \\n8 \", \"CHAPTER 1 | Introduction to cloud -native applications  \\n In the book,  Beyond the Twelve -Factor Ap\", \"p , author Kevin Hoffman details each of the original 12 \\nfactors (written in 2011). Additionally, h\", \"e discusses three extra factors that reflect today\\u2019s modern \\ncloud application design.  \\nNew Factor \", \" Explanation  \\n13 - API First  Make everything a service. Assume your code \\nwill be consumed by a fr\", \"ont -end client, gateway, \\nor another service.  \\n14 - Telemetry  On a workstation, you have deep vis\", \"ibility into \\nyour application and its behavior. In the cloud, \\nyou don\\u2019t. Make sure your design inc\", \"ludes the \\ncollection of monitoring, domain -specific, and \\nhealth/system data.  \\n15 - Authenticatio\", \"n/ Authorization  Implement identity from the start. Consider \\nRBAC (role -based access control)  fe\", \"atures \\navailable in public clouds.  \\nWe\\u2019ll refer to many of the 12+ factors in this chapter and thr\", \"oughout the book.  \\nAzure Well -Architected Framework  \\nDesigning and deploying cloud -based workloa\", \"ds can be challenging, especially when implementing \\ncloud -native architecture. Microsoft provides \", \"industry standard best practices to help you and your \\nteam deliver robust cloud solutions.  \\nThe Mi\", \"crosoft Well -Architected Framework  provides a set of guiding tenets that can be used to \\nimprove t\", \"he quality of a cloud -native workload. The framework consists of five pillars of architecture \\nexce\", \"llence:  \\nTenets  Description  \\nCost management  Focus on generating incremental value early. \\nApply\", \" Build -Measure -Learn  principles to \\naccelerate time to market while avoiding \\ncapital -intensive \", \"solutions. Using a pay -as-you-\\ngo strategy, invest as you scale out, rather than \\ndelivering a larg\", \"e investment up front.  \\nOperational excellence  Automate the environment and operations to \\nincreas\", \"e speed and reduce human error. Roll \\nproblem updates back or forward quickly. \\nImplement monitoring\", \" and diagnostics from the \\nstart.  \\nPerformance efficiency  Efficiently meet demands placed on your \", \"\\nworkloads. Favor horizontal scaling (scaling out) \\nand design it into your systems. Continually \\nco\", \"nduct performance and load testing to \\nidentify potential bottlenecks.   \\n9 CHAPTER 1 | Introduction\", \" to cloud -native applications  \\n Tenets  Description  \\nReliability  Build workloads that are both r\", \"esilient and \\navailable. Resiliency enables workloads to \\nrecover from failures and continue functio\", \"ning. \\nAvailability ensures users access to your \\nworkload at all times. Design applications to \\nexp\", \"ect failures and recover from them.  \\nSecurity  Implement security across the entire lifecycle of \\na\", \"n application, from design and implementation \\nto deployment and operations. Pay close \\nattention to\", \" identity management, infrastructure \\naccess, application security, and data \\nsovereignty and encryp\", \"tion.  \\nTo get started, Microsoft provides a set of online assessments  to help you assess your curr\", \"ent cloud \\nworkloads against the five well -architected pillars.  \\nMicroservices  \\nCloud -native sys\", \"tems embrace microservices, a popular architectural style for constructing modern \\napplications.  \\nB\", \"uilt as a distributed set of small, independent services that interact through a shared fabric, \\nmic\", \"roservices share the following characteristics:  \\n\\u2022 Each implements a specific business capability w\", \"ithin a larger domain context.  \\n\\u2022 Each is developed autonomously and can be deployed independently.\", \"  \\n\\u2022 Each is self -contained encapsulating its own data storage technology, dependencies, and \\nprogr\", \"amming platform.  \\n\\u2022 Each runs in its own process and communicates with others using standard commun\", \"ication \\nprotocols such as HTTP/HTTPS, gRPC, WebSockets, or AMQP . \\n\\u2022 They compose together to form \", \"an application.  \\nFigure 1 -4 contrasts a monolithic application approach with a microservices appro\", \"ach. Note how the \\nmonolith is composed of a layered architecture, which executes in a single proces\", \"s. It typically \\nconsumes a relational database. The microservice approach, h owever, segregates fun\", \"ctionality into \\nindependent services, each with its own logic, state, and data. Each microservice h\", \"osts its own \\ndatastore.   \\n10 CHAPTER 1 | Introduction to cloud -native applications  \\n  \\nFigure 1 \", \"-4. Monolithic versus microservices architecture  \\nNote how microservices promote the Processes  pri\", \"nciple from the Twelve -Factor Application , \\ndiscussed earlier in the chapter.  \\nFactor #6 specifie\", \"s \\u201cEach microservice should execute in its own process, isolated from other running \\nservices.\\u201d  \\nWh\", \"y microservices?  \\nMicroservices provide agility.  \\nEarlier in the chapter, we compared an eCommerce\", \" application built as a monolith to that with \\nmicroservices. In the example, we saw some clear bene\", \"fits:  \\n\\u2022 Each microservice has an autonomous lifecycle and can evolve independently and deploy \\nfre\", \"quently. You don\\u2019t have to wait for a quarterly release to deploy a new feature or update. \\nYou can \", \"update a small area of a live application with less risk of disrupting th e entire system. \\nThe upda\", \"te can be made without a full redeployment of the application.  \\n\\u2022 Each microservice can scale indep\", \"endently. Instead of scaling the entire application as a single \\nunit, you scale out only those serv\", \"ices that require more processing power to meet desired \\nperformance levels and service -level agree\", \"ments. Fine -grained scalin g provides for greater \\ncontrol of your system and helps reduce overall \", \"costs as you scale portions of your system, \\nnot everything.  \\nAn excellent reference guide for unde\", \"rstanding microservices is .NET Microservices: Architecture for \\nContainerized .NET Applications . T\", \"he book deep dives into microservices design and architecture. It\\u2019s \\na companion for a full-stack mi\", \"croservice reference architecture  available as a free download from \\nMicrosoft.  \\nDeveloping micros\", \"ervices  \\nMicroservices can be created upon any modern development platform.  \\n \\n11 CHAPTER 1 | Intr\", \"oduction to cloud -native applications  \\n The Microsoft .NET platform is an excellent choice. Free a\", \"nd open source, it has many built -in features \\nthat simplify microservice development. .NET is cros\", \"s -platform. Applications can be built and run on \\nWindows, macOS, and most flavors of Linux.  \\n.NET\", \" is highly performant and has scored well in comparison to Node.js and other competing \\nplatforms. I\", \"nterestingly, TechEmpower  conducted an extensive set of performance benchmarks  across \\nmany web ap\", \"plication platforms and frameworks. .NET scored in the top 10 - well above Node.js and \\nother compet\", \"ing platforms.  \\n.NET is maintained by Microsoft and the .NET community on GitHub.  \\nMicroservice ch\", \"allenges  \\nWhile distributed cloud -native microservices can provide immense agility and speed, they\", \" present \\nmany challenges:  \\nCommunication  \\nHow will front -end client applications communicate wit\", \"h backed -end core microservices? Will you \\nallow direct communication? Or, might you abstract the b\", \"ack -end microservices with a gateway \\nfacade that provides flexibility, control, and security?  \\nHo\", \"w will back -end core microservices communicate with each other? Will you allow direct HTTP calls \\nt\", \"hat can increase coupling and impact performance and agility? Or might you consider decoupled \\nmessa\", \"ging with queue and topic technologies?  \\nCommunication is covered in the Cloud -native communicatio\", \"n patterns  chapter.  \\nResiliency  \\nA microservices architecture moves your system from in -process \", \"to out -of-process network \\ncommunication. In a distributed architecture, what happens when Service \", \"B isn\\u2019t responding to a \\nnetwork call from Service A? Or, what happens when Service C becomes tempor\", \"arily unavailable and \\nother services calling it become blocked?  \\nResiliency is covered in the Clou\", \"d -native resiliency  chapter.  \\nDistributed Data  \\nBy design, each microservice encapsulates its ow\", \"n data, exposing operations via its public interface. If \\nso, how do you query data or implement a t\", \"ransaction across multiple services?  \\nDistributed data is covered in the Cloud -native data pattern\", \"s  chapter.  \\nSecrets  \\nHow will your microservices securely store and manage secrets and sensitive \", \"configuration data?  \\nSecrets are covered in detail Cloud -native security .  \\n12 CHAPTER 1 | Introd\", \"uction to cloud -native applications  \\n Manage Complexity with Dapr  \\nDapr  is a distributed, open -\", \"source application runtime. Through an architecture of pluggable \\ncomponents, it dramatically simpli\", \"fies the plumbing  behind distributed applications. It provides a \\ndynamic glue  that binds your app\", \"lication with pre -built infrastructure capabilities and components \\nfrom the Dapr runtime. Figure 1\", \" -5 shows Dapr from 20,000 feet.  \\n \\nFigure 1 -5. Dapr at 20,000 feet.  \\nIn the top row of the figur\", \"e, note how Dapr provides language -specific SDKs  for popular development \\nplatforms. Dapr v1 inclu\", \"des support for .NET, Go, Node.js, Python, PHP, Java, and JavaScript.  \\nWhile language -specific SDK\", \"s enhance the developer experience, Dapr is platform agnostic. Under the \\nhood, Dapr\\u2019s programming m\", \"odel exposes capabilities through standard HTTP/gRPC communication \\nprotocols. Any programming platf\", \"orm can call Dapr via its nativ e HTTP and gRPC APIs.  \\nThe blue boxes across the center of the figu\", \"re represent the Dapr building blocks. Each exposes pre -\\nbuilt plumbing code for a distributed appl\", \"ication capability that your application can consume.  \\nThe components row represents a large set of\", \" pre -defined infrastructure components that your \\napplication can consume. Think of components as i\", \"nfrastructure code you don\\u2019t have to write.  \\nThe bottom row highlights the portability of Dapr and \", \"the diverse environments across which it can \\nrun. \\nMicrosoft features a free ebook Dapr for .NET De\", \"velopers  for learning Dapr.  \\nLooking ahead, Dapr has the potential to have a profound impact on cl\", \"oud -native application \\ndevelopment.  \\nContainers  \\nIt\\u2019s natural to hear the term container  mentio\", \"ned in any cloud native  conversation. In the book, Cloud \\nNative Patterns , author Cornelia Davis o\", \"bserves that, \\u201cContainers are a great enabler of cloud -native \\n \\n13 CHAPTER 1 | Introduction to clo\", \"ud -native applications  \\n software.\\u201d The Cloud Native Computing Foundation places microservice cont\", \"ainerization as the first \\nstep in their Cloud -Native Trail Map  - guidance for enterprises beginni\", \"ng their cloud -native journey.  \\nContainerizing a microservice is simple and straightforward. The c\", \"ode, its dependencies, and runtime \\nare packaged into a binary called a container image . Images are\", \" stored in a container registry, which \\nacts as a repository or library for images. A registry can b\", \"e located on your development computer, in \\nyour data center, or in a public cloud. Docker itself ma\", \"intains a public registry via Docker Hub . The \\nAzure cloud features a private container registry  t\", \"o store container images close to the cloud \\napplications that will run them.  \\nWhen an application \", \"starts or scales, you transform the container image into a running container \\ninstance. The instance\", \" runs on any computer that has a container runtime  engine installed. You can \\nhave as many instance\", \"s of the containerized service as needed.  \\nFigure 1 -6 shows three different microservices, each in\", \" its own container, all running on a single host.  \\n \\nFigure 1 -6. Multiple containers running on a \", \"container host  \\nNote how each container maintains its own set of dependencies and runtime, which ca\", \"n be different \\nfrom one another. Here, we see different versions of the Product microservice runnin\", \"g on the same \\nhost. Each container shares a slice of the underlying host operating system, memory, \", \"and processor, \\nbut is isolated from one another.  \\nNote how well the container model embraces the D\", \"ependencies  principle from the Twelve -Factor \\nApplication . \\nFactor #2 specifies that \\u201cEach micros\", \"ervice isolates and packages its own dependencies, embracing \\nchanges without impacting the entire s\", \"ystem.\\u201d  \\nContainers support both Linux and Windows workloads. The Azure cloud openly embraces both.\", \" \\nInterestingly, it\\u2019s Linux, not Windows Server, that has become the more popular operating system i\", \"n \\nAzure.  \\n \\n14 CHAPTER 1 | Introduction to cloud -native applications  \\n While several container v\", \"endors exist, Docker  has captured the lion\\u2019s share of the market. The \\ncompany has been driving the\", \" software container movement. It has become the de facto standard for \\npackaging, deploying, and run\", \"ning cloud -native applications.  \\nWhy containers?  \\nContainers provide portability and guarantee co\", \"nsistency across environments. By encapsulating \\neverything into a single package, you isolate  the \", \"microservice and its dependencies from the \\nunderlying infrastructure.  \\nYou can deploy the containe\", \"r in any environment that hosts the Docker runtime engine. Containerized \\nworkloads also eliminate t\", \"he expense of pre -configuring each environment with frameworks, software \\nlibraries, and runtime en\", \"gines.  \\nBy sharing the underlying operating system and host resources, a container has a much small\", \"er \\nfootprint than a full virtual machine. The smaller size increases the density , or number of \\nmi\", \"croservices, that a given host can run at one time.  \\nContainer orchestration  \\nWhile tools such as \", \"Docker create images and run containers, you also need tools to manage them. \\nContainer management i\", \"s done with a special software program called a container orchestrator . \\nWhen operating at scale wi\", \"th many independent running containers, orchestration is essential.  \\nFigure 1 -7 shows management t\", \"asks that container orchestrators automate.  \\n \\nFigure 1 -7. What container orchestrators do  \\nThe f\", \"ollowing table describes common orchestration tasks.  \\nTasks  Explanation  \\nScheduling  Automaticall\", \"y provision container instances.  \\nAffinity/anti -affinity  Provision containers nearby or far apart\", \" from \\neach other, helping availability and \\nperformance.  \\nHealth monitoring  Automatically detect \", \"and correct failures.  \\nFailover  Automatically reprovision a failed instance to a \\nhealthy machine.\", \"  \\n \\n15 CHAPTER 1 | Introduction to cloud -native applications  \\n Tasks  Explanation  \\nScaling  Auto\", \"matically add or remove a container \\ninstance to meet demand.  \\nNetworking  Manage a networking over\", \"lay for container \\ncommunication.  \\nService Discovery  Enable containers to locate each other.  \\nRol\", \"ling Upgrades  Coordinate incremental upgrades with zero \\ndowntime deployment. Automatically roll ba\", \"ck \\nproblematic changes.  \\nNote how container orchestrators embrace the Disposability  and Concurren\", \"cy  principles from the \\nTwelve -Factor Application . \\nFactor #9 specifies that \\u201cService instances s\", \"hould be disposable, favoring fast startups to increase \\nscalability opportunities and graceful shut\", \"downs to leave the system in a correct state.\\u201d  Docker \\ncontainers along with an orchestrator inhere\", \"ntly satisfy this requirement.\\u201d  \\nFactor #8 specifies that \\u201cServices scale out across a large number\", \" of small identical processes (copies) as \\nopposed to scaling -up a single large instance on the mos\", \"t powerful machine available.\\u201d  \\nWhile several container orchestrators exist, Kubernetes  has become\", \" the de facto standard for the \\ncloud -native world. It\\u2019s a portable, extensible, open -source platf\", \"orm for managing containerized \\nworkloads.  \\nYou could host your own instance of Kubernetes, but the\", \"n you\\u2019d be responsible for provisioning and \\nmanaging its resources - which can be complex. The Azur\", \"e cloud features Kubernetes as a managed \\nservice. Both Azure Kubernetes Service (AKS)  and Azure Re\", \"d Hat OpenShift (ARO)  enable you to fully \\nleverage the features and power of Kubernetes as a manag\", \"ed service, without having to install and \\nmaintain it.  \\nContainer orchestration is covered in deta\", \"il in Scaling Cloud -Native Applications . \\nBacking services  \\nCloud -native systems depend upon man\", \"y different ancillary resources, such as data stores, message \\nbrokers, monitoring, and identity ser\", \"vices. These services are known as backing services . \\nFigure 1 -8 shows many common backing service\", \"s that cloud -native systems consume.   \\n16 CHAPTER 1 | Introduction to cloud -native applications  \", \"\\n  \\nFigure 1 -8. Common backing services  \\nYou could host your own backing services, but then you\\u2019d \", \"be responsible for licensing, provisioning, \\nand managing those resources.  \\nCloud providers offer a\", \" rich assortment of managed backing services.  Instead of owning the service, \\nyou simply consume it\", \". The cloud provider operates the resource at scale and bears the responsibility \\nfor performance, s\", \"ecurity, and maintenance. Monitoring, redundancy, and availability are built into the \\nservice. Prov\", \"iders  guarantee service level performance and fully support their managed services - \\nopen a ticket\", \" and they fix your issue.  \\nCloud -native systems favor managed backing services from cloud vendors.\", \" The savings in time and \\nlabor can be significant. The operational risk of hosting your own and exp\", \"eriencing trouble can get \\nexpensive fast.  \\nA best practice is to treat a backing service as an att\", \"ached resource , dynamically bound to a \\nmicroservice with configuration information (a URL and cred\", \"entials) stored in an external \\nconfiguration. This guidance is spelled out in the Twelve -Factor Ap\", \"plication , discussed earlier in the \\nchapter.  \\nFactor #4  specifies that backing services \\u201cshould \", \"be exposed via an addressable URL. Doing so \\ndecouples the resource from the application, enabling i\", \"t to be interchangeable.\\u201d  \\nFactor #3  specifies that \\u201cConfiguration information is moved out of the\", \" microservice and externalized \\nthrough a configuration management tool outside of the code.\\u201d  \\nWith\", \" this pattern, a backing service can be attached and detached without code changes. You might \\npromo\", \"te a microservice from QA to a staging environment. You update the microservice \\nconfiguration to po\", \"int to the backing services in staging and inject the settings into your container \\nthrough an envir\", \"onment variable.  \\n \\n17 CHAPTER 1 | Introduction to cloud -native applications  \\n Cloud vendors prov\", \"ide APIs for you to communicate with their proprietary backing services. These \\nlibraries encapsulat\", \"e the proprietary plumbing and complexity. However, communicating directly with \\nthese APIs will tig\", \"htly couple your code to that specific b acking service. It\\u2019s a widely accepted practice \\nto insulat\", \"e the implementation details of the vendor API. Introduce an intermediation layer, or \\nintermediate \", \"API, exposing generic operations to your service code and wrap the vendor code inside \\nit. This loos\", \" e coupling enables you to swap out one backing service for another or move your code to \\na differen\", \"t cloud environment without having to make changes to the mainline service code. Dapr, \\ndiscussed ea\", \"rlier, follows this model with its set of prebuilt building blocks . \\nOn a final thought, backing se\", \"rvices also promote the Statelessness  principle from the Twelve -Factor \\nApplication , discussed ea\", \"rlier in the chapter.  \\nFactor #6  specifies that, \\u201cEach microservice should execute in its own proc\", \"ess, isolated from other \\nrunning services. Externalize required state to a backing service such as \", \"a distributed cache or data \\nstore.\\u201d  \\nBacking services are discussed in Cloud -native data patterns\", \"  and Cloud -native communication \\npatterns . \\nAutomation  \\nAs you\\u2019ve seen, cloud -native systems em\", \"brace microservices, containers, and modern system design \\nto achieve speed and agility. But, that\\u2019s\", \" only part of the story. How do you provision the cloud \\nenvironments upon which these systems run? \", \"How do you rapidly deploy app features and updates? \\nHow do you round out the fu ll picture?  \\nEnter\", \" the widely accepted practice of Infrastructure as Code , or IaC.  \\nWith IaC, you automate platform \", \"provisioning and application deployment. You essentially apply \\nsoftware engineering practices such \", \"as testing and versioning to your DevOps practices. Your \\ninfrastructure and deployments are automat\", \"ed, consistent, and repeat able.  \\nAutomating infrastructure  \\nTools like Azure Resource Manager , A\", \"zure Bicep , Terraform  from HashiCorp, and the Azure CLI , enable \\nyou to declaratively script the \", \"cloud infrastructure you require. Resource names, locations, capacities, \\nand secrets are parameteri\", \"zed and dynamic. The script is versioned and checked into source control \\nas an artifact of your pro\", \"ject. You invoke the script to provision a consistent and repeatable \\ninfrastructure across system e\", \"nvironments, such as QA, staging, and production.  \\nUnder the hood, IaC is idempotent, meaning that \", \"you can run the same script over and over without \\nside effects. If the team needs to make a change,\", \" they edit and rerun the script. Only the updated \\nresources are affected.  \\nIn the article, What is\", \" Infrastructure as Code , Author Sam Guckenheimer describes how, \\u201cTeams who \\nimplement IaC can deliv\", \"er stable environments rapidly and at scale. They avoid manual configuration \\nof environments and en\", \"force consistency by representing the desired state of their environments via \\ncode . Infrastructure\", \" deployments with IaC are repeatable and prevent runtime issues caused by \\nconfiguration drift or mi\", \"ssing dependencies. DevOps teams can work together with a unified set of  \\n18 CHAPTER 1 | Introducti\", \"on to cloud -native applications  \\n practices and tools to deliver applications and their supporting\", \" infrastructure rapidly, reliably, and at \\nscale.\\u201d  \\nAutomating deployments  \\nThe Twelve -Factor App\", \"lication , discussed earlier, calls for separate steps when transforming \\ncompleted code into a runn\", \"ing application.  \\nFactor #5  specifies that \\u201cEach release must enforce a strict separation across t\", \"he build, release and run \\nstages. Each should be tagged with a unique ID and support the ability to\", \" roll back.\\u201d  \\nModern CI/CD systems help fulfill this principle. They provide separate build and del\", \"ivery steps that \\nhelp ensure consistent and quality code that\\u2019s readily available to users.  \\nFigur\", \"e 1 -9 shows the separation across the deployment process.  \\n \\nFigure 1 -9. Deployment steps in a CI\", \"/CD Pipeline  \\nIn the previous figure, pay special attention to separation of tasks:  \\n1. The develo\", \"per constructs a feature in their development environment, iterating through what \\nis called the \\u201cin\", \"ner loop\\u201d of code, run, and debug.  \\n2. When complete, that code is pushed  into a code repository, \", \"such as GitHub, Azure DevOps, or \\nBitBucket.  \\n3. The push triggers a build stage that transforms th\", \"e code into a binary artifact. The work is \\nimplemented with a Continuous Integration (CI)  pipeline\", \". It automatically builds, tests, and \\npackages the application.  \\n4. The release stage picks up the\", \" binary artifact, applies external application and environment \\nconfiguration information, and produ\", \"ces an immutable release. The release is deployed to a \\nspecified environment. The work is implement\", \"ed with a Continuous Delivery (CD)  pipeline. \\nEach release should be identifiable. You can say, \\u201cTh\", \"is deployment is running Release 2.1.1 of \\nthe application.\\u201d  \\n \\n19 CHAPTER 1 | Introduction to clou\", \"d -native applications  \\n 5. Finally, the released feature is run in the target execution environmen\", \"t. Releases are \\nimmutable meaning that any change must create a new release.  \\nApplying these pract\", \"ices, organizations have radically evolved how they ship software. Many have \\nmoved from quarterly r\", \"eleases to on -demand updates. The goal is to catch problems early in the \\ndevelopment cycle when th\", \"ey\\u2019re less expensive to fix. The longer  the duration between integrations, \\nthe more expensive prob\", \"lems become to resolve. With consistency in the integration process, teams \\ncan commit code changes \", \"more frequently, leading to better collaboration and software quality.  \\nInfrastructure as code and \", \"deployment automation, along with GitHub and Azure DevOps are \\ndiscussed in detail in DevOps . \\nCand\", \"idate apps for cloud native  \\nThink about the apps your organization needs to build. Then, look at t\", \"he existing apps in your \\nportfolio. How many of them warrant a cloud -native architecture? All of t\", \"hem? Perhaps some?  \\nApplying cost/benefit analysis, there\\u2019s a good chance some wouldn\\u2019t support the\", \" effort. The cost of \\nbecoming cloud native would far exceed the business value of the application. \", \" \\nWhat type of application might be a candidate for cloud native?  \\n\\u2022 Strategic enterprise systems t\", \"hat need to constantly evolve business capabilities/features  \\n\\u2022 An application that requires a high\", \" release velocity - with high confidence  \\n\\u2022 A system where individual features must release without\", \"  a full redeployment of the entire \\nsystem  \\n\\u2022 An application developed by teams with expertise in \", \"different technology stacks  \\n\\u2022 An application with components that must scale independently  \\nSmall\", \"er, less impactful line -of-business applications might fare well with a simple monolithic \\narchitec\", \"ture hosted in a Cloud PaaS environment.  \\nThen there are legacy systems. While we\\u2019d all like to bui\", \"ld new applications, we\\u2019re often responsible \\nfor modernizing legacy workloads that are critical to \", \"the business.  \\nModernizing legacy apps  \\nThe free Microsoft e -book Modernize existing .NET applica\", \"tions with Azure cloud and Windows \\nContainers  provides guidance about migrating on -premises workl\", \"oads into cloud. Figure 1 -10 shows \\nthat there isn\\u2019t a single, one -size-fits-all strategy for mode\", \"rnizing legacy applications.   \\n20 CHAPTER 1 | Introduction to cloud -native applications  \\n  \\nFigur\", \"e 1 -10. Strategies for migrating legacy workloads  \\nMonolithic apps that are non -critical might be\", \"nefit from a quick lift-and-shift  (Cloud Infrastructure -\\nReady ) migration. Here, the on -premises\", \" workload is rehosted to a cloud -based VM, without changes. \\nThis approach uses the IaaS (Infrastru\", \"cture as a Service) model . Azure includes several tools such as \\nAzure Migrate , Azure Site Recover\", \"y , and Azure Database Migration Service  to help streamline the \\nmove. While this strategy can yiel\", \"d some cost savings, such applications typically weren\\u2019t designed to \\nunlock and leverage the benefi\", \"ts of cloud computing.  \\nLegacy apps that are critical to the business often benefit from an enhance\", \"d Cloud Optimized  \\nmigration. This approach includes deployment optimizations that enable key cloud\", \" services - without \\nchanging the core architecture of the application. For example, you might conta\", \"inerize  the application \\nand deploy it to a container orchestrator, like Azure Kubernetes Services \", \", discussed later in this book. \\nOnce in the cloud, the application can consume cloud backing servic\", \"es such as databases, message \\nqueues, monitoring, and distributed caching.  \\nFinally, monolithic ap\", \"ps that provide strategic enterprise functions might best benefit from a Cloud -\\nNative  approach, t\", \"he subject of this book. This approach provides agility and velocity. But, it comes at \\na cost of re\", \"platforming, rearchitecting, and rewriting code. Over time, a legacy application could be \\ndecompose\", \"d into microservices, containerized, and ultim ately replatformed  into a cloud -native \\narchitectur\", \"e.  \\nIf you and your team believe a cloud -native approach is appropriate, it behooves you to ration\", \"alize \\nthe decision with your organization. What exactly is the business problem that a cloud -nativ\", \"e \\napproach will solve? How would it align with business needs?  \\n\\u2022 Rapid releases of features with \", \"increased confidence?  \\n\\u2022 Fine-grained scalability - more efficient usage of resources?  \\n \\n21 CHAPT\", \"ER 1 | Introduction to cloud -native applications  \\n \\u2022 Improved system resiliency?  \\n\\u2022 Improved syst\", \"em performance?  \\n\\u2022 More visibility into operations?  \\n\\u2022 Blend development platforms and data stores\", \" to arrive at the best tool for the job?  \\n\\u2022 Future -proof application investment?  \\nThe right migra\", \"tion strategy depends on organizational priorities and the systems you\\u2019re targeting. \\nFor many, it m\", \"ay be more cost effective to cloud -optimize a monolithic application or add coarse -\\ngrained servic\", \"es to an N -Tier app. In these cases, you can still make full use of cloud PaaS capabilities \\nlike t\", \"he ones offered by Azure App Service.  \\nSummary  \\nIn this chapter, we introduced cloud -native compu\", \"ting. We provided a definition along with the key \\ncapabilities that drive a cloud -native applicati\", \"on. We looked at the types of applications that might \\njustify this investment and effort.  \\nWith th\", \"e introduction behind, we now dive into a much more detailed look at cloud native.  \\nReferences  \\n\\u2022 \", \"Cloud Native Computing Foundation  \\n\\u2022 .NET Microservices: Architecture for Containerized .NET applic\", \"ations  \\n\\u2022 Microsoft Azure Well -Architected Framework  \\n\\u2022 Modernize existing .NET applications with\", \" Azure cloud and Windows Containers  \\n\\u2022 Cloud Native Patterns by Cornelia Davis  \\n\\u2022 Cloud native app\", \"lications: Ship faster, reduce risk, and grow your business  \\n\\u2022 Dapr for .NET Developers  \\n\\u2022 Dapr do\", \"cuments  \\n\\u2022 Beyond the Twelve -Factor Application  \\n\\u2022 What is Infrastructure as Code  \\n\\u2022 Uber Engine\", \"ering\\u2019s Micro Deploy: Deploying Daily with Confidence  \\n\\u2022 How Netflix Deploys Code  \\n\\u2022 Overload Cont\", \"rol for Scaling WeChat Microservices   \\n22 CHAPTER 2 | Introducing eShopOnContainers reference app  \", \"\\n CHAPTER  2 \\nIntroducing \\neShopOnContainers \\nreference app  \\nMicrosoft, in partnership with leading\", \" community experts, has produced a full -featured cloud -native \\nmicroservices reference application\", \", eShopOnContainers. This application is built to showcase using \\n.NET and Docker, and optionally Az\", \"ure, Kubernetes, and V isual Studio, to build an online storefront.  \\n \\nFigure 2 -1. eShopOnContaine\", \"rs Sample App Screenshot.  \\n \\n23 CHAPTER 2 | Introducing eShopOnContainers reference app  \\n Before s\", \"tarting this chapter, we recommend that you download the eShopOnContainers reference \\napplication . \", \"If you do so, it should be easier for you to follow along with the information presented.  \\nFeatures\", \" and requirements  \\nLet\\u2019s start with a review of the application\\u2019s features and requirements. The eS\", \"hopOnContainers \\napplication represents an online store that sells various physical products like t \", \"-shirts and coffee \\nmugs. If you\\u2019ve bought anything online before, the experience of using the store\", \" should be r elatively \\nfamiliar. Here are some of the basic features the store implements:  \\n\\u2022 List\", \" catalog items  \\n\\u2022 Filter items by type  \\n\\u2022 Filter items by brand  \\n\\u2022 Add items to the shopping bask\", \"et  \\n\\u2022 Edit or remove items from the basket  \\n\\u2022 Checkout  \\n\\u2022 Register an account  \\n\\u2022 Sign in  \\n\\u2022 Sig\", \"n out \\n\\u2022 Review orders  \\nThe application also has the following non -functional requirements:  \\n\\u2022 It\", \" needs to be highly available and it must scale automatically to meet increased traffic (and \\nscale \", \"back down once traffic subsides).  \\n\\u2022 It should provide easy -to-use monitoring of its health and di\", \"agnostic logs to help \\ntroubleshoot any issues it encounters.  \\n\\u2022 It should support an agile develop\", \"ment process, including support for continuous integration \\nand deployment (CI/CD).  \\n\\u2022 In addition \", \"to the two web front ends (traditional and Single Page Application), the \\napplication must also supp\", \"ort mobile client apps running different kinds of operating \\nsystems.  \\n\\u2022 It should support cross -p\", \"latform hosting and cross -platform development.   \\n24 CHAPTER 2 | Introducing eShopOnContainers ref\", \"erence app  \\n  \\nFigure 2 -2. eShopOnContainers reference application development architecture.  \\nThe\", \" eShopOnContainers application is accessible from web or mobile clients that access the \\napplication\", \" over HTTPS targeting either the ASP.NET Core MVC server application or an appropriate \\nAPI Gateway.\", \" API Gateways offer several advantages, such as decoupl ing back -end services from \\nindividual fron\", \"t -end clients and providing better security. The application also makes use of a related \\npattern k\", \"nown as Backends -for-Frontends (BFF), which recommends creating separate API gateways \\nfor each fro\", \"nt -end client. The  reference architecture demonstrates breaking up the API gateways \\nbased on whet\", \"her the request is coming from a web or mobile client.  \\nThe application\\u2019s functionality is broken u\", \"p into many distinct microservices. There are services \\nresponsible for authentication and identity,\", \" listing items from the product catalog, managing users\\u2019 \\nshopping baskets, and placing orders. Each\", \" of these separ ate services has its own persistent storage. \\nThere\\u2019s no single primary data store w\", \"ith which all services interact. Instead, coordination and \\ncommunication between the services is do\", \"ne on an as -needed basis and by using a message bus.  \\nEach of the different microservices is desig\", \"ned differently, based on their individual requirements. This \\naspect means their technology stack m\", \"ay differ, although they\\u2019re all built using .NET and designed for \\nthe cloud. Simpler services provi\", \"de basic Creat e-Read -Update -Delete (CRUD) access to the underlying \\ndata stores, while more advan\", \"ced services use Domain -Driven Design approaches and patterns to \\nmanage business complexity.  \\n \\n2\", \"5 CHAPTER 2 | Introducing eShopOnContainers reference app  \\n  \\nFigure 2 -3. Different kinds of micro\", \"services.  \\nOverview of the code  \\nBecause it uses microservices, the eShopOnContainers app includes\", \" quite a few separate projects and \\nsolutions in its GitHub repository. In addition to separate solu\", \"tions and executable files, the various \\nservices are designed to run inside their own contai ners, \", \"both during local development and at run \\ntime in production. Figure 2 -4 shows the full Visual Stud\", \"io solution, in which the various different \\nprojects are organized.  \\n \\n26 CHAPTER 2 | Introducing \", \"eShopOnContainers reference app  \\n  \\n \\n27 CHAPTER 2 | Introducing eShopOnContainers reference app  \\n\", \" Figure 2 -4. Projects in Visual Studio solution.  \\nThe code is organized to support the different m\", \"icroservices, and within each microservice, the code is \\nbroken up into domain logic, infrastructure\", \" concerns, and user interface or service endpoint. In many \\ncases, each service\\u2019s dependencies can b\", \"e fulfilled by Azure services in production, and alternative \\noptions for local development. Let\\u2019s e\", \"xamine how the application\\u2019s requirements map to Azure \\nservices. \\nUnderstanding microservices  \\nThi\", \"s book focuses on cloud -native applications built using Azure technology. To learn more about \\nmicr\", \"oservices best practices and how to architect microservice -based applications, read the \\ncompanion \", \"book, .NET Microservices: Architecture for Containerized .NET Applications . \\nMapping eShopOnContain\", \"ers to Azure Services  \\nAlthough not required, Azure is well -suited to supporting the eShopOnContai\", \"ners because the project \\nwas built to be a cloud -native application. The application is built with\", \" .NET, so it can run on Linux or \\nWindows containers depending on the Docker host. T he application \", \"is made up of multiple \\nautonomous microservices, each with its own data. The different microservice\", \"s showcase different \\napproaches, ranging from simple CRUD operations to more complex DDD and CQRS p\", \"atterns. \\nMicroservices communicate with cli ents over HTTP and with one another via message -based \", \"\\ncommunication. The application supports multiple platforms for clients as well, since it adopts HTT\", \"P \\nas a standard communication protocol and includes ASP.NET Core and Xamarin mobile apps that run \\n\", \"on An droid, iOS, and Windows platforms.  \\nThe application\\u2019s architecture is shown in Figure 2 -5. O\", \"n the left are the client apps, broken up into \\nmobile, traditional Web, and Web Single Page Applica\", \"tion (SPA) flavors. On the right are the server -\\nside components that make up the system, each of w\", \"hic h can be hosted in Docker containers and \\nKubernetes clusters. The traditional web app is powere\", \"d by the ASP.NET Core MVC application shown \\nin yellow. This app and the mobile and web SPA applicat\", \"ions communicate with the individual \\nmicroservices through one  or more API gateways. The API gatew\", \"ays follow the \\u201cbackends for front \\nends\\u201d (BFF) pattern, meaning that each gateway is designed to su\", \"pport a given front -end client. The \\nindividual microservices are listed to the right of the API ga\", \"teways and include both  business logic \\nand some kind of persistence store. The different services \", \"make use of SQL Server databases, Redis \\ncache instances, and MongoDB/CosmosDB stores. On the far ri\", \"ght is the system\\u2019s Event Bus, which is \\nused for communication between the microse rvices.   \\n28 CH\", \"APTER 2 | Introducing eShopOnContainers reference app  \\n  \\nFigure 2 -5. The eShopOnContainers Archit\", \"ecture.  \\nThe server -side components of this architecture all map easily to Azure services.  \\nConta\", \"iner orchestration and clustering  \\nThe application\\u2019s container -hosted services, from ASP.NET Core \", \"MVC apps to individual Catalog and \\nOrdering microservices, can be hosted and managed in Azure Kuber\", \"netes Service (AKS). The \\napplication can run locally on Docker and Kubernetes, and the same co ntai\", \"ners can then be deployed \\nto staging and production environments hosted in AKS. This process can be\", \" automated as we\\u2019ll see in \\nthe next section.  \\nAKS provides management services for individual clus\", \"ters of containers. The application will deploy \\nseparate containers for each microservice in the AK\", \"S cluster, as shown in the architecture diagram \\nabove. This approach allows each individual service\", \" to sc ale independently according to its resource \\ndemands. Each microservice can also be deployed \", \"independently, and ideally such deployments \\nshould incur zero system downtime.  \\nAPI Gateway  \\nThe \", \"eShopOnContainers application has multiple front -end clients and multiple different back -end \\nserv\", \"ices. There\\u2019s no one -to-one correspondence between the client applications and the microservices \\nt\", \"hat support them. In such a scenario, there may be a grea t deal of complexity when writing client \\n\", \"software to interface with the various back -end services in a secure manner. Each client would need\", \" to \\naddress this complexity on its own, resulting in duplication and many places in which to make u\", \"pdates \\nas service s change or new policies are implemented.  \\nAzure API Management (APIM) helps org\", \"anizations publish APIs in a consistent, manageable fashion. \\nAPIM consists of three components: the\", \" API Gateway, and administration portal (the Azure portal), \\nand a developer portal.  \\n \\n29 CHAPTER \", \"2 | Introducing eShopOnContainers reference app  \\n The API Gateway accepts API calls and routes them\", \" to the appropriate back -end API. It can also \\nprovide additional services like verification of API\", \" keys or JWT tokens and API transformation on the \\nfly without code modifications (for instance, to \", \"accommodat e clients expecting an older interface).  \\nThe Azure portal is where you define the API s\", \"chema and package different APIs into products. You \\nalso configure user access, view reports, and c\", \"onfigure policies for quotas or transformations.  \\nThe developer portal serves as the main resource \", \"for developers. It provides developers with API \\ndocumentation, an interactive test console, and rep\", \"orts on their own usage. Developers also use the \\nportal to create and manage their own accounts, in\", \"cluding subsc ription and API key support.  \\nUsing APIM, applications can expose several different g\", \"roups of services, each providing a back end \\nfor a particular front -end client. APIM is recommende\", \"d for complex scenarios. For simpler needs, the \\nlightweight API Gateway Ocelot can be used. The eSh\", \"opOn Containers app uses Ocelot because of its \\nsimplicity and because it can be deployed into the s\", \"ame application environment as the application \\nitself. Learn more about eShopOnContainers, APIM, an\", \"d Ocelot.  \\nAnother option if your application is using AKS is to deploy the Azure Gateway Ingress C\", \"ontroller as a \\npod within your AKS cluster. This approach allows your cluster to integrate with an \", \"Azure Application \\nGateway, allowing the gateway to load -balance traff ic to the AKS pods. Learn mo\", \"re about the Azure \\nGateway Ingress Controller for AKS . \\nData  \\nThe various back -end services used\", \" by eShopOnContainers have different storage requirements. \\nSeveral microservices use SQL Server dat\", \"abases. The Basket microservice leverages a Redis cache for \\nits persistence. The Locations microser\", \"vice expects a MongoDB A PI for its data. Azure supports each \\nof these data formats.  \\nFor SQL Serv\", \"er database support, Azure has products for everything from single databases up to \\nhighly scalable \", \"SQL Database elastic pools. Individual microservices can be configured to \\ncommunicate with their ow\", \"n individual SQL Server databases quickly and easily. These databases can \\nbe scaled as needed to su\", \"pport each separate microservice according to its needs.  \\nThe eShopOnContainers application stores \", \"the user\\u2019s current shopping basket between requests. This \\naspect is managed by the Basket microserv\", \"ice that stores the data in a Redis cache. In development, \\nthis cache can be deployed in a containe\", \"r, while in produ ction it can utilize Azure Cache for Redis. \\nAzure Cache for Redis is a fully mana\", \"ged service offering high performance and reliability without the \\nneed to deploy and manage Redis i\", \"nstances or containers on your own.  \\nThe Locations microservice uses a MongoDB NoSQL database for i\", \"ts persistence. During \\ndevelopment, the database can be deployed in its own container, while in pro\", \"duction the service can \\nleverage Azure Cosmos DB\\u2019s API for MongoDB . One of the benefits of Azure C\", \"osmos DB is its ability \\nto leverage multiple different communication protocols, including a SQL API\", \" and common NoSQL \\nAPIs including MongoDB, Cassandra, Gremlin, and Azure Table Storage. Azure Cosmos\", \" DB offers a \\nfully managed  and globally distributed database as a service that can scale to meet t\", \"he needs of the \\nservices that use it.  \\nDistributed data in cloud -native applications is covered i\", \"n more detail in chapter 5 .  \\n30 CHAPTER 2 | Introducing eShopOnContainers reference app  \\n Event B\", \"us  \\nThe application uses events to communicate changes between different services. This functionali\", \"ty can \\nbe implemented with various implementations, and locally the eShopOnContainers application u\", \"ses \\nRabbitMQ . When hosted in Azure, the application would leverage Azure Service Bus  for its mess\", \"aging. \\nAzure Service Bus is a fully managed integration message broker that allows applications and\", \" services \\nto communicate with one another in a decoupled, reliable, asynchronous manner. Azure Serv\", \"ice Bus \\nsupports individual queues as well as  separate topics  to support publisher -subscriber sc\", \"enarios. The \\neShopOnContainers application would leverage topics with Azure Service Bus to support \", \"distributing \\nmessages from one microservice to any other microservice that needed to react to a giv\", \"en message.  \\nResiliency  \\nOnce deployed to production, the eShopOnContainers application would be a\", \"ble to take advantage \\nof several Azure services available to improve its resiliency. The applicatio\", \"n publishes health checks, \\nwhich can be integrated with Application Insights to prov ide reporting \", \"and alerts based on the app\\u2019s \\navailability. Azure resources also provide diagnostic logs that can b\", \"e used to identify and correct bugs \\nand performance issues. Resource logs provide detailed informat\", \"ion on when and how different Azure \\nresource s are used by the application. You\\u2019ll learn more about\", \" cloud -native resiliency features in \\nchapter 6 . \\nDeploying eShopOnContainers to Azure  \\nThe eShop\", \"OnContainers application can be deployed to various Azure platforms. The recommended \\napproach is to\", \" deploy the application to Azure Kubernetes Services (AKS). Helm, a Kubernetes \\ndeployment tool, is \", \"available to reduce deployment complexity. Optionally, developers may \\nimplement Azure Dev Spaces fo\", \"r Kubernetes to s treamline their development process.  \\nAzure Kubernetes Service  \\nTo host eShop in\", \" AKS, the first step is to create an AKS cluster. To do so, you might use the Azure \\nportal, which w\", \"ill walk you through the required steps. You could also create a cluster from the Azure \\nCLI, taking\", \" care to enable Role -Based Access Control (RBAC) and application routing. The \\neShopOnContainers\\u2019 d\", \"ocumentation details the steps for creating your own AKS cluster. Once created, \\nyou can access and \", \"manage the cluster from the Kubernetes dashboard.  \\nYou can now deploy the eShop application to the \", \"cluster using Helm.  \\nDeploying to Azure Kubernetes Service using Helm  \\nHelm is an application pack\", \"age manager tool that works directly with Kubernetes. It helps you define, \\ninstall, and upgrade Kub\", \"ernetes applications. While simple apps can be deployed to AKS with custom \\nCLI scripts or simple de\", \"ployment files, complex apps ca n contain many Kubernetes objects and benefit \\nfrom Helm.  \\nUsing He\", \"lm, applications include text -based configuration files, called Helm charts, which declaratively \\nd\", \"escribe the application and configuration in Helm packages. Charts use standard YAML -formatted  \\n31\", \" CHAPTER 2 | Introducing eShopOnContainers reference app  \\n files to describe a related set of Kuber\", \"netes resources. They\\u2019re versioned alongside the application \\ncode they describe. Helm Charts range \", \"from simple to complex depending on the requirements of the \\ninstallation they describe.  \\nHelm is c\", \"omposed of a command -line client tool, which consumes helm charts and launches \\ncommands to a serve\", \"r component named, Tiller. Tiller communicates with the Kubernetes API to \\nensure the correct provis\", \"ioning of your containerized workloads. Helm is ma intained by the Cloud -\\nnative Computing Foundati\", \"on.  \\nThe following yaml file presents a Helm template:  \\napiVersion : v1 \\nkind: Service \\nmetadata :\", \" \\n  name: {{ .Values.app.svc.marketing }} \\n  labels: \\n    app: {{ template \\\"marketing -api.name\\\"  . \", \"}} \\n    chart: {{ template \\\"marketing -api.chart\\\"  . }} \\n    release: {{ .Release.Name }} \\n    herit\", \"age : {{ .Release.Service }} \\nspec: \\n  type: {{ .Values.service.type }} \\n  ports: \\n    - port: {{ .V\", \"alues.service.port }} \\n      targetPort : http \\n      protocol : TCP \\n      name: http \\n  selector :\", \" \\n    app: {{ template \\\"marketing -api.name\\\"  . }} \\n    release: {{ .Release.Name }} \\nNote how the t\", \"emplate describes a dynamic set of key/value pairs. When the template is invoked, \\nvalues that enclo\", \"sed in curly braces are pulled in from other yaml -based configuration files.  \\nYou\\u2019ll find the eSho\", \"pOnContainers helm charts in the /k8s/helm folder. Figure 2 -6 shows how the \\ndifferent components o\", \"f the application are organized into a folder structure used by helm to define \\nand managed deployme\", \"nts.   \\n32 CHAPTER 2 | Introducing eShopOnContainers reference app  \\n  \\nFigure 2 -6. The eShopOnCont\", \"ainers helm folder.  \\nEach individual component is installed using a helm install command. eShop inc\", \"ludes a \\u201cdeploy all\\u201d \\nscript that loops through and installs the components using their respective h\", \"elm charts. The result is \\na repeatable process, versioned with the application in source control, t\", \"hat anyone on the team can \\ndeploy to an AKS cluster with a one -line script command.  \\nNote that ve\", \"rsion 3 of Helm officially removes the need for the Tiller server component. More \\ninformation on th\", \"is enhancement can be found here. \\nAzure Functions and Logic Apps (Serverless)  \\nThe eShopOnContaine\", \"rs sample includes support for tracking online marketing campaigns. An Azure \\nFunction is used to tr\", \"ack marketing campaign details for a given campaign ID. Rather than creating a \\n \\n33 CHAPTER 2 | Int\", \"roducing eShopOnContainers reference app  \\n full microservice, a single Azure Function is simpler an\", \"d sufficient. Azure Functions have a simple build \\nand deployment model, especially when configured \", \"to run in Kubernetes. Deploying the function is \\nscripted using Azure Resource Manager (ARM) templat\", \"es  and the Azure CLI. This campaign service \\nisn\\u2019t customer -facing and invokes a single operation,\", \" making it a great candidate for Azure Functions. \\nThe function requires minimal configuration, incl\", \"uding a database connection string data and image \\nbase URI set tings. You configure Azure Functions\", \" in the Azure portal.  \\nCentralized configuration  \\nUnlike a monolithic app in which everything runs\", \" within a single instance, a cloud -native application \\nconsists of independent services distributed\", \" across virtual machines, containers, and geographic \\nregions. Managing configuration settings for d\", \"ozens of in terdependent services can be challenging. \\nDuplicate copies of configuration settings ac\", \"ross different locations are error prone and difficult to \\nmanage. Centralized configuration is a cr\", \"itical requirement for distributed cloud -native applications.  \\nAs discussed in Chapter 1 , the Twe\", \"lve -Factor App recommendations require strict separation between \\ncode and configuration. Configura\", \"tion must be stored externally from the application and read -in as \\nneeded. Storing configuration v\", \"alues as constants or literal values in code is a v iolation. The same \\nconfiguration values are oft\", \"en be used by many services in the same application. Additionally, we \\nmust support the same values \", \"across multiple environments, such as dev, testing, and production. The \\nbest practice is store them\", \" in a centr alized configuration store.  \\nThe Azure cloud presents several great options.  \\nAzure Ap\", \"p Configuration  \\nAzure App Configuration  is a fully managed Azure service that stores non -secret \", \"configuration \\nsettings in a secure, centralized location. Stored values can be shared among multipl\", \"e services and \\napplications.  \\nThe service is simple to use and provides several benefits:  \\n\\u2022 Flex\", \"ible key/value representations and mappings  \\n\\u2022 Tagging with Azure labels  \\n\\u2022 Dedicated UI for manag\", \"ement  \\n\\u2022 Encryption of sensitive information  \\n\\u2022 Querying and batch retrieval  \\nAzure App Configura\", \"tion maintains changes made to key -value settings for seven days. The point -in-\\ntime snapshot feat\", \"ure enables you to reconstruct the history of a setting and even rollback for a failed \\ndeployment. \", \" \\nApp Configuration automatically caches each setting to avoid excessive calls to the configuration \", \"\\nstore. The refresh operation waits until the cached value of a setting expires to update that setti\", \"ng, \\neven when its value changes in the configuration store.  The default cache expiration time is 3\", \"0 \\nseconds. You can override the expiration time.   \\n34 CHAPTER 2 | Introducing eShopOnContainers re\", \"ference app  \\n App Configuration encrypts all configuration values in transit and at rest. Key names\", \" and labels are \\nused as indexes for retrieving configuration data and aren\\u2019t encrypted.  \\nAlthough \", \"App Configuration provides hardened security, Azure Key Vault is still the best place for \\nstoring a\", \"pplication secrets. Key Vault provides hardware -level encryption, granular access policies, and \\nma\", \"nagement operations such as certificate rotation. You can create App Configuration values that \\nrefe\", \"rence secrets stored in a Key Vault.  \\nAzure Key Vault  \\nKey Vault is a managed service for securely\", \" storing and accessing secrets. A secret is anything that you \\nwant to tightly control access to, su\", \"ch as API keys, passwords, or certificates. A vault is a logical group \\nof secrets.  \\nKey Vault grea\", \"tly reduces the chances that secrets may be accidentally leaked. When using Key Vault, \\napplication \", \"developers no longer need to store security information in their application. This practice \\nelimina\", \"tes the need to store this information insid e your code. For example, an application may need \\nto c\", \"onnect to a database. Instead of storing the connection string in the app\\u2019s code, you can store it \\n\", \"securely in Key Vault.  \\nYour applications can securely access the information they need by using UR\", \"Is. These URIs allow the \\napplications to retrieve specific versions of a secret. There\\u2019s no need to\", \" write custom code to protect \\nany of the secret information stored in Key Vault.  \\nAccess to Key Va\", \"ult requires proper caller authentication and authorization. Typically, each cloud -\\nnative microser\", \"vice uses a ClientId/ClientSecret combination. It\\u2019s important to keep these credentials \\noutside sou\", \"rce control. A best practice is to set them  in the application\\u2019s environment. Direct access to \\nKey\", \" Vault from AKS can be achieved using Key Vault FlexVolume . \\nConfiguration in eShop  \\nThe eShopOnCo\", \"ntainers application includes local application settings files with each microservice. \\nThese files \", \"are checked into source control, but don\\u2019t include production secrets such as connection \\nstrings or\", \" API keys. In production, individual settings  may be overwritten with per -service environment \\nvar\", \"iables. Injecting secrets in environment variables is a common practice for hosted applications, but\", \" \\ndoesn\\u2019t provide a central configuration store. To support centralized management of configuration \", \"\\nsettings, each microservice includes a setting to toggle between its use of local settings or Azure\", \" Key \\nVault settings.  \\nReferences  \\n\\u2022 The eShopOnContainers Architecture  \\n\\u2022 Orchestrating microser\", \"vices and multi -container applications for high scalability and \\navailability  \\n\\u2022 Azure API Managem\", \"ent  \\n\\u2022 Azure SQL Database Overview  \\n\\u2022 Azure Cache for Redis  \\n\\u2022 Azure Cosmos DB\\u2019s API for MongoDB \", \"  \\n35 CHAPTER 2 | Introducing eShopOnContainers reference app  \\n \\u2022 Azure Service Bus  \\n\\u2022 Azure Monit\", \"or overview  \\n\\u2022 eShopOnContainers: Create Kubernetes cluster in AKS  \\n\\u2022 eShopOnContainers: Azure Dev\", \" Spaces  \\n\\u2022 Azure Dev Spaces   \\n36 CHAPTER 3 | Scaling cloud -native applications  \\n CHAPTER  3 \\nSca\", \"ling cloud -native \\napplications  \\nOne of the most -often touted advantages of moving to a cloud hos\", \"ting environment is scalability. \\nScalability, or the ability for an application to accept additiona\", \"l user load without compromising \\nperformance for each user. It\\u2019s most often achieved by break ing u\", \"p an application into small pieces \\nthat can each be given whatever resources they require. Cloud ve\", \"ndors enable massive scalability \\nanytime and anywhere in the world.  \\nIn this chapter, we discuss t\", \"echnologies that enable cloud -native applications to scale to meet user \\ndemand. These technologies\", \" include:  \\n\\u2022 Containers  \\n\\u2022 Orchestrators  \\n\\u2022 Serverless computing  \\nLeveraging containers and orch\", \"estrators  \\nContainers and orchestrators are designed to solve problems common to monolithic deploym\", \"ent \\napproaches.  \\nChallenges with monolithic deployments  \\nTraditionally, most applications have be\", \"en deployed as a single unit. Such applications are referred to \\nas a monolith. This general approac\", \"h of deploying applications as single units even if they\\u2019re \\ncomposed of multiple modules or assembl\", \"ies is known as mo nolithic architecture, as shown in Figure \\n3-1.  \\n37 CHAPTER 3 | Scaling cloud -n\", \"ative applications  \\n  \\nFigure 3 -1. Monolithic architecture.  \\nAlthough they have the benefit of si\", \"mplicity, monolithic architectures face many challenges:  \\nDeployment  \\nAdditionally, they require a\", \" restart of the application, which may temporarily impact availability if \\nzero-downtime techniques \", \"are not applied while deploying.  \\nScaling  \\nA monolithic application is hosted entirely on a single\", \" machine instance, often requiring high -\\ncapability hardware. If any part of the monolith requires \", \"scaling, another copy of the entire application \\nmust be deployed to another machine. With a monolit\", \"h, you can\\u2019t scale application components \\nindividually - it\\u2019s all or nothing. Scalin g components t\", \"hat don\\u2019t require scaling results in inefficient and \\ncostly resource usage.  \\nEnvironment  \\nMonolit\", \"hic applications are typically deployed to a hosting environment with a pre -installed operating \\nsy\", \"stem, runtime, and library dependencies. This environment may not match that upon which the \\napplica\", \"tion was developed or tested. Inconsistencies across  application environments are a common \\nsource \", \"of problems for monolithic deployments.  \\nCoupling  \\nA monolithic application is likely to experienc\", \"e high coupling across its functional components. \\nWithout hard boundaries, system changes often res\", \"ult in unintended and costly side effects. New \\nfeatures/fixes become tricky, time -consuming, and e\", \"xpensive to implement. Updates require extensive \\ntesting. Coupling also makes it difficult to refac\", \"tor components or swap in alternative \\nimplementations. Even when constructed with a strict separati\", \"on of concerns, architectural erosion \\nsets in as the monolithic code b ase deteriorates with never \", \"-ending \\u201cspecial cases.\\u201d  \\n \\n38 CHAPTER 3 | Scaling cloud -native applications  \\n Platform lock -in \", \"\\nA monolithic application is constructed with a single technology stack. While offering uniformity, \", \"this \\ncommitment can become a barrier to innovation. New features and components will be built using\", \" \\nthe application\\u2019s current stack - even when more modern t echnologies may be a better choice. A \\nl\", \"onger -term risk is your technology stack becoming outdated and obsolete. Rearchitecting an entire \\n\", \"application to a new, more modern platform is at best expensive and risky.  \\nWhat are the benefits o\", \"f containers and orchestrators?  \\nWe introduced containers in Chapter 1. We highlighted how the Clou\", \"d Native Computing Foundation \\n(CNCF) ranks containerization as the first step in their Cloud -Nativ\", \"e Trail Map  - guidance for \\nenterprises beginning their cloud -native journey. In this section, we \", \"discuss the benefits of containers.  \\nDocker is the most popular container management platform. It w\", \"orks with containers on both Linux or \\nWindows. Containers provide separate but reproducible applica\", \"tion environments that run the same \\nway on any system. This aspect makes them perfect for devel opi\", \"ng and hosting cloud -native services. \\nContainers are isolated from one another. Two containers on \", \"the same host hardware can have \\ndifferent versions of software, without causing conflicts.  \\nContai\", \"ners are defined by simple text -based files that become project artifacts and are checked into \\nsou\", \"rce control. While full servers and virtual machines require manual effort to update, containers are\", \" \\neasily version -controlled. Apps built to run in con tainers can be developed, tested, and deploye\", \"d \\nusing automated tools as part of a build pipeline.  \\nContainers are immutable. Once you define a \", \"container, you can recreate and run it exactly the same \\nway. This immutability lends itself to comp\", \"onent -based design. If some parts of an application evolve \\ndifferently than others, why redeploy t\", \"he entire app w hen you can just deploy the parts that change \\nmost frequently? Different features a\", \"nd cross -cutting concerns of an app can be broken up into \\nseparate units. Figure 3 -2 shows how a \", \"monolithic app can take advantage of containers and \\nmicroservices by delegat ing certain features o\", \"r functionality. The remaining functionality in the app \\nitself has also been containerized.   \\n39 C\", \"HAPTER 3 | Scaling cloud -native applications  \\n  \\nFigure 3 -2. Decomposing a monolithic app to embr\", \"ace microservices.  \\nEach cloud -native service is built and deployed in a separate container. Each \", \"can update as needed. \\nIndividual services can be hosted on nodes with resources appropriate to each\", \" service. The \\nenvironment each service runs in is immutable, shared across dev,  test, and producti\", \"on environments, \\nand easily versioned. Coupling between different areas of the application occurs e\", \"xplicitly as calls or \\nmessages between services, not compile -time dependencies within the monolith\", \". You can also choose \\nthe technology tha t best suites a given capability without requiring changes\", \" to the rest of the app.  \\nContainerized services require automated management. It wouldn\\u2019t be feasi\", \"ble to manually administer \\na large set of independently deployed containers. For example, consider \", \"the following tasks:  \\n\\u2022 How will container instances be provisioned across a cluster of many machin\", \"es?  \\n\\u2022 Once deployed, how will containers discover and communicate with each other?  \\n\\u2022 How can con\", \"tainers scale in or out on -demand?  \\n\\u2022 How do you monitor the health of each container?  \\n\\u2022 How do \", \"you protect a container against hardware and software failures?  \\n\\u2022 How do upgrade containers for a \", \"live application with zero downtime?  \\nContainer orchestrators address and automate these and other \", \"concerns.  \\nIn the cloud -native eco -system, Kubernetes has become the de facto container orchestra\", \"tor. It\\u2019s an \\nopen -source platform managed by the Cloud Native Computing Foundation (CNCF). Kuberne\", \"tes \\nautomates the deployment, scaling, and operational concerns of conta inerized workloads across \", \"a \\nmachine cluster. However, installing and managing Kubernetes is notoriously complex.  \\n \\n40 CHAPT\", \"ER 3 | Scaling cloud -native applications  \\n A much better approach is to leverage Kubernetes as a m\", \"anaged service from a cloud vendor. The \\nAzure cloud features a fully managed Kubernetes platform en\", \"titled Azure Kubernetes Service (AKS) . \\nAKS abstracts the complexity and operational overhead of ma\", \"naging Kubernetes. You consume \\nKubernetes as a cloud service; Microsoft takes responsibility for ma\", \"naging and supporting it. AKS also \\ntightly integrates with other Azure services and dev tools.  \\nAK\", \"S is a cluster -based technology. A pool of federated virtual machines, or nodes, is deployed to the\", \" \\nAzure cloud. Together they form a highly available environment, or cluster. The cluster appears as\", \" a \\nseamless, single entity to your cloud -native applicat ion. Under the hood, AKS deploys your \\nco\", \"ntainerized services across these nodes following a predefined strategy that evenly distributes the \", \"\\nload.  \\nWhat are the scaling benefits?  \\nServices built on containers can leverage scaling benefits\", \" provided by orchestration tools like \\nKubernetes. By design containers only know about themselves. \", \"Once you have multiple containers \\nthat need to work together, you should organize them at a higher \", \"level. Organizing large numbers of \\ncontainers and their shared dependencies, such as network config\", \"uration, is where orchestration tools \\ncome in to save the day! Kubernetes creates an abstraction la\", \"yer over groups of containers and \\norganizes them into pods. Pods run on worker machines referred to\", \" as nodes . This organized structure \\nis referred to as a cluster . Figure 3 -3 shows the different \", \"components of a Kubernetes cluster.  \\n \\nFigure 3 -3. Kubernetes cluster components.  \\nScaling contai\", \"nerized workloads is a key feature of container orchestrators. AKS supports automatic \\nscaling acros\", \"s two dimensions: Container instances and compute nodes. Together they give AKS the \\nability to quic\", \"kly and efficiently respond to spikes in demand and add additional resources. We \\ndiscuss scaling in\", \" AKS la ter in this chapter.  \\n \\n41 CHAPTER 3 | Scaling cloud -native applications  \\n Declarative ve\", \"rsus imperative  \\nKubernetes supports both declarative and imperative configuration. The imperative \", \"approach involves \\nrunning various commands that tell Kubernetes what to do each step of the way. Ru\", \"n this image. \\nDelete this pod. Expose this port. With the declarative appro ach, you create a confi\", \"guration file, called \\na manifest, to describe what you want instead of what to do. Kubernetes reads\", \" the manifest and \\ntransforms your desired end state into actual end state.  \\nImperative commands ar\", \"e great for learning and interactive experimentation. However, you\\u2019ll want to \\ndeclaratively create \", \"Kubernetes manifest files to embrace an infrastructure as code approach, \\nproviding for reliable and\", \" repeatable deployments. The manifes t file becomes a project artifact and is \\nused in your CI/CD pi\", \"peline for automating Kubernetes deployments.  \\nIf you\\u2019ve already configured your cluster using impe\", \"rative commands, you can export a declarative \\nmanifest by using kubectl get svc SERVICENAME -o yaml\", \" > service.yaml. This command produces a \\nmanifest similar to one shown below:  \\napiVersion : v1 \\nki\", \"nd: Service \\nmetadata : \\n  creationTimestamp : \\\"2019-09-13T13:58:47Z\\\"  \\n  labels: \\n    component : a\", \"piserver  \\n    provider : kubernetes  \\n  name: kubernetes  \\n  namespace : default \\n  resourceVersion\", \" : \\\"153\\\" \\n  selfLink : /api/v1/namespaces/default/services/kubernetes  \\n  uid: 9b1fac62 -d62e-11e9-8\", \"968-00155d38010d  \\nspec: \\n  clusterIP : 10.96.0.1  \\n  ports: \\n  - name: https \\n    port: 443 \\n    pr\", \"otocol : TCP \\n    targetPort : 6443 \\n  sessionAffinity : None \\n  type: ClusterIP  \\nstatus: \\n  loadBa\", \"lancer : {} \\nWhen using declarative configuration, you can preview the changes that will be made bef\", \"ore \\ncommitting them by using kubectl diff -f FOLDERNAME against the folder where your configuration\", \" \\nfiles are located. Once you\\u2019re sure you want to apply the changes, ru n kubectl apply -f FOLDERNAM\", \"E. \\nAdd -R to recursively process a folder hierarchy.  \\nYou can also use declarative configuration w\", \"ith other Kubernetes features, one of which being \\ndeployments. Declarative deployments help manage \", \"releases, updates, and scaling. They instruct the \\nKubernetes deployment controller on how to deploy\", \" new changes, scale out load, or roll back to a \\nprevious revision. If a cluster is unstable, a decl\", \"arative deployment will automatically return the cluster \\nback to a desired state. For example, if a\", \" node should crash, the deployment mechanism will redeploy \\na replacement  to achieve your desired s\", \"tate   \\n42 CHAPTER 3 | Scaling cloud -native applications  \\n Using declarative configuration allows \", \"infrastructure to be represented as code that can be checked in \\nand versioned alongside the applica\", \"tion code. It provides improved change control and better \\nsupport for continuous deployment using a\", \" build and deploy p ipeline.  \\nWhat scenarios are ideal for containers and orchestrators?  \\nThe foll\", \"owing scenarios are ideal for using containers and orchestrators.  \\nApplications requiring high upti\", \"me and scalability  \\nIndividual applications that have high uptime and scalability requirements are \", \"ideal candidates for \\ncloud -native architectures using microservices, containers, and orchestrators\", \". They can be developed \\nin containers, tested across versioned environments, an d deployed into pro\", \"duction with zero \\ndowntime. The use of Kubernetes clusters ensures such apps can also scale on dema\", \"nd and recover \\nautomatically from node failures.  \\nLarge numbers of applications  \\nOrganizations th\", \"at deploy and maintain large numbers of applications benefit from containers and \\norchestrators. The\", \" up front effort of setting up containerized environments and Kubernetes clusters is \\nprimarily a fi\", \"xed cost. Deploying, maintaining, and updating individual applications has a cost that \\nvaries with \", \"the number of applications. Beyond a few applications, the complexity of maintaining \\ncustom applica\", \"tions manually exceeds the cost of implementing a solution using containers and \\norchestrators.  \\nWh\", \"en should you avoid using containers and orchestrators?  \\nIf you\\u2019re unable to build your application\", \" following the Twelve -Factor App principles, you should \\nconsider avoiding containers and orchestra\", \"tors. In these cases, consider a VM -based hosting platform, \\nor possibly some hybrid system. With i\", \"t, you can always spin off certain pieces of functionality into \\nseparate containers or even serverl\", \"ess functions.  \\nDevelopment resources  \\nThis section shows a short list of development resources th\", \"at may help you get started using \\ncontainers and orchestrators for your next application. If you\\u2019re\", \" looking for guidance on how to \\ndesign your cloud -native microservices architecture app, read this\", \"  book\\u2019s companion, .NET \\nMicroservices: Architecture for Containerized .NET Applications . \\nLocal K\", \"ubernetes Development  \\nKubernetes deployments provide great value in production environments, but c\", \"an also run locally on \\nyour development machine. While you may work on individual microservices ind\", \"ependently, there \\nmay be times when you\\u2019ll need to run the entire system locally - just as it will \", \"run when deployed to \\nproduction. There are several tools that can help: Minikube and Docker Desktop\", \". Visual Studio also \\nprovides tooling for Docker development.   \\n43 CHAPTER 3 | Scaling cloud -nati\", \"ve applications  \\n Minikube  \\nWhat is Minikube? The Minikube project says \\u201cMinikube implements a loc\", \"al Kubernetes cluster on \\nmacOS, Linux, and Windows.\\u201d Its primary goals are \\u201cto be the best tool for\", \" local Kubernetes \\napplication development and to support all Kubernetes features that f it.\\u201d Instal\", \"ling Minikube is \\nseparate from Docker, but Minikube supports different hypervisors than Docker Desk\", \"top supports. \\nThe following Kubernetes features are currently supported by Minikube:  \\n\\u2022 DNS \\n\\u2022 Nod\", \"ePorts  \\n\\u2022 ConfigMaps and secrets  \\n\\u2022 Dashboards  \\n\\u2022 Container runtimes: Docker, rkt, CRI -O, and co\", \"ntainerd  \\n\\u2022 Enabling Container Network Interface (CNI)  \\n\\u2022 Ingress  \\nAfter installing Minikube, you\", \" can quickly start using it by running the minikube start command, which \\ndownloads an image and sta\", \"rt the local Kubernetes cluster. Once the cluster is started, you interact \\nwith it using the standa\", \"rd Kubernetes kubectl comman ds. \\nDocker Desktop  \\nYou can also work with Kubernetes directly from D\", \"ocker Desktop on Windows. It is your only option if \\nyou\\u2019re using Windows Containers, and is a great\", \" choice for non -Windows containers as well. Figure 3 -\\n4 shows how to enable local Kubernetes suppo\", \"rt when run ning Docker Desktop.  \\n \\nFigure 3 -4. Configuring Kubernetes in Docker Desktop.  \\n \\n44 C\", \"HAPTER 3 | Scaling cloud -native applications  \\n Docker Desktop is the most popular tool for configu\", \"ring and running containerized apps locally. \\nWhen you work with Docker Desktop, you can develop loc\", \"ally against the exact same set of Docker \\ncontainer images that you\\u2019ll deploy to production. Docker\", \" Deskto p is designed to \\u201cbuild, test, and \\nship\\u201d containerized apps locally. It supports both Linux\", \" and Windows containers. Once you push your \\nimages to an image registry, like Azure Container Regis\", \"try or Docker Hub, AKS can pull and deploy \\nthem to production.  \\nVisual Studio Docker Tooling  \\nVis\", \"ual Studio supports Docker development for web -based applications. When you create a new \\nASP.NET C\", \"ore application, you have an option to configure it with Docker support, as shown in Figure \\n3-5. \\n \", \"\\nFigure 3 -5. Visual Studio Enable Docker Support  \\nWhen this option is selected, the project is cre\", \"ated with a Dockerfile in its root, which can be used to \\nbuild and host the app in a Docker contain\", \"er. An example Dockerfile is shown in Figure 3 -6. \\nFROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base\", \" \\nWORKDIR /app \\nEXPOSE 80 \\nEXPOSE 443 \\n \\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build \\nWORKDIR /sr\", \"c \\nCOPY [\\\"eShopWeb/eShopWeb.csproj\\\" , \\\"eShopWeb/\\\" ] \\nRUN dotnet restore \\\"eShopWeb/eShopWeb.csproj\\\"  \", \"\\nCOPY . . \\nWORKDIR \\\"/src/eShopWeb\\\"  \\nRUN dotnet build \\\"eShopWeb.csproj\\\"  -c Release -o /app/build  \\n\", \" \\nFROM build AS publish \\nRUN dotnet publish \\\"eShopWeb.csproj\\\"  -c Release -o /app/publish  \\n \\n \\n45 C\", \"HAPTER 3 | Scaling cloud -native applications  \\n FROM base AS final \\nWORKDIR /app \\nCOPY --from=publi\", \"sh  /app/publish .  \\nENTRYPOINT  [\\\"dotnet\\\" , \\\"eShopWeb.dll\\\" ] \\nFigure 3 -6. Visual Studio generated \", \"Dockerfile  \\nOnce support is added, you can run your application in a Docker container in Visual Stu\", \"dio. Figure 3 -7 \\nshows the different run options available from a new ASP.NET Core project created \", \"with Docker \\nsupport added.  \\n \\nFigure 3 -7. Visual Studio Docker Run Options  \\nAlso, at any time yo\", \"u can add Docker support to an existing ASP.NET Core application. From the \\nVisual Studio Solution E\", \"xplorer, right -click on the project and select Add > Docker Support , as shown \\nin Figure 3 -8. \\n \\n\", \"46 CHAPTER 3 | Scaling cloud -native applications  \\n  \\nFigure 3 -8. Adding Docker support to Visual \", \"Studio  \\nVisual Studio Code Docker Tooling  \\nThere are many extensions available for Visual Studio C\", \"ode that support Docker development.  \\nMicrosoft provides the Docker for Visual Studio Code extensio\", \"n . This extension simplifies the process \\nof adding container support to applications. It scaffolds\", \" required files, builds Docker images, and \\nenables you to debug your app inside a container. The ex\", \"tension features a visual explorer that makes \\nit easy to tak e actions on containers and images suc\", \"h as start, stop, inspect, remove, and more. The \\nextension also supports Docker Compose enabling yo\", \"u to manage multiple running containers as a \\nsingle unit.  \\nLeveraging serverless functions  \\nIn th\", \"e spectrum from managing physical machines to leveraging cloud capabilities, serverless lives at \\nth\", \"e extreme end. Your only responsibility is your code, and you only pay when your code runs. Azure \\nF\", \"unctions provides a way to build serverless capabilit ies into your cloud -native applications.  \\n \\n\", \"47 CHAPTER 3 | Scaling cloud -native applications  \\n What is serverless?  \\nServerless is a relativel\", \"y new service model of cloud computing. It doesn\\u2019t mean that servers are \\noptional - your code still\", \" runs on a server somewhere. The distinction is that the application team no \\nlonger concerns itself\", \" with managing server infrastruct ure. Instead, the cloud vendor own this \\nresponsibility. The devel\", \"opment team increases its productivity by delivering business solutions to \\ncustomers, not plumbing.\", \"  \\nServerless computing uses event -triggered stateless containers to host your services. They can s\", \"cale \\nout and in to meet demand as -needed. Serverless platforms like Azure Functions have tight \\nin\", \"tegration with other Azure services like queues, events, and st orage.  \\nWhat challenges are solved \", \"by serverless?  \\nServerless platforms address many time -consuming and expensive concerns:  \\n\\u2022 Purch\", \"asing machines and software licenses  \\n\\u2022 Housing, securing, configuring, and maintaining the machine\", \"s and their networking, power, \\nand A/C requirements  \\n\\u2022 Patching and upgrading operating systems an\", \"d software  \\n\\u2022 Configuring web servers or machine services to host application software  \\n\\u2022 Configur\", \"ing application software within its platform  \\nMany companies allocate large budgets to support hard\", \"ware infrastructure concerns. Moving to the \\ncloud can help reduce these costs; shifting application\", \"s to serverless can help eliminate them.  \\nWhat is the difference between a microservice and a serve\", \"rless \\nfunction?  \\nTypically, a microservice encapsulates a business capability, such as a shopping \", \"cart for an online \\neCommerce site. It exposes multiple operations that enable a user to manage thei\", \"r shopping \\nexperience. A function, however, is a small, lightweight block of  code that executes a \", \"single -purpose \\noperation in response to an event. Microservices are typically constructed to respo\", \"nd to requests, \\noften from an interface. Requests can be HTTP Rest - or gRPC -based. Serverless ser\", \"vices respond to \\nevents. Its event -driven architecture is ideal for processing short -running, bac\", \"kground tasks.  \\nWhat scenarios are appropriate for serverless?  \\nServerless exposes individual shor\", \"t -running functions that are invoked in response to a trigger. This \\nmakes them ideal for processin\", \"g background tasks.  \\nAn application might need to send an email as a step in a workflow. Instead of\", \" sending the \\nnotification as part of a microservice request, place the message details onto a queue\", \". An Azure \\nFunction can dequeue the message and asynchronously send the email. Doing so could impro\", \"ve the \\nperformance and scalability of the microservice. Queue -based load leveling  can be implemen\", \"ted to \\navoid bottlenecks related to sending the emails. Additionally, this stand -alone service cou\", \"ld be reused \\nas a utility across many different applications.   \\n48 CHAPTER 3 | Scaling cloud -nati\", \"ve applications  \\n Asynchronous messaging from queues and topics is a common pattern to trigger serv\", \"erless functions. \\nHowever, Azure Functions can be triggered by other events, such as changes to Azu\", \"re Blob Storage. A \\nservice that supports image uploads could have an Azure F unction responsible fo\", \"r optimizing the \\nimage size. The function could be triggered directly by inserts into Azure Blob St\", \"orage, keeping \\ncomplexity out of the microservice operations.  \\nMany services have long -running pr\", \"ocesses as part of their workflows. Often these tasks are done as \\npart of the user\\u2019s interaction wi\", \"th the application. These tasks can force the user to wait, negatively \\nimpacting their experience. \", \"Serverless computing pro vides a great way to move slower tasks outside \\nof the user interaction loo\", \"p. These tasks can scale with demand without requiring the entire \\napplication to scale.  \\nWhen shou\", \"ld you avoid serverless?  \\nServerless solutions provision and scale on demand. When a new instance i\", \"s invoked, cold starts are a \\ncommon issue. A cold start is the period of time it takes to provision\", \" this instance. Normally, this delay \\nmight be a few seconds, but can be longer depen ding on variou\", \"s factors. Once provisioned, a single \\ninstance is kept alive as long as it receives periodic reques\", \"ts. But, if a service is called less frequently, \\nAzure may remove it from memory and require a cold\", \" start when reinvoked. Cold starts are also \\nrequired when a function scales out to a new instance. \", \" \\nFigure 3 -9 shows a cold -start pattern. Note the extra steps required when the app is cold.  \\n \\nF\", \"igure 3 -9. Cold start versus warm start.  \\nTo avoid cold starts entirely, you might switch from a c\", \"onsumption plan to a dedicated plan . You can \\nalso configure one or more pre-warmed instances  with\", \" the premium plan upgrade. In these cases, \\nwhen you need to add another instance, it\\u2019s already up a\", \"nd ready to go. These options can help \\nmitigate the cold start issue associated with serverless com\", \"puting.  \\n \\n49 CHAPTER 3 | Scaling cloud -native applications  \\n Cloud providers bill for serverless\", \" based on compute execution time and consumed memory. Long \\nrunning operations or high memory consum\", \"ption workloads aren\\u2019t always the best candidates for \\nserverless. Serverless functions favor small \", \"chunks of work that can  complete quickly. Most serverless \\nplatforms require individual functions t\", \"o complete within a few minutes. Azure Functions defaults to a \\n5-minute time -out duration, which c\", \"an be configured up to 10 minutes. The Azure Functions premium \\nplan can mitigate th is issue as wel\", \"l, defaulting time -outs to 30 minutes with an unbounded higher \\nlimit that can be configured. Compu\", \"te time isn\\u2019t calendar time. More advanced functions using the \\nAzure Durable Functions framework  m\", \"ay pause execution over a course of several days. The billing is \\nbased on actual execution time - w\", \"hen the function wakes up and resumes processing.  \\nFinally, leveraging Azure Functions for applicat\", \"ion tasks adds complexity. It\\u2019s wise to first architect \\nyour application with a modular, loosely co\", \"upled design. Then, identify if there are benefits serverless \\nwould offer that justify the addition\", \"al complex ity. \\nCombining containers and serverless approaches  \\nCloud -native applications typical\", \"ly implement services leveraging containers and orchestration. There \\nare often opportunities to exp\", \"ose some of the application\\u2019s services as Azure Functions. However, \\nwith a cloud -native app deploy\", \"ed to Kubernetes, it would  be nice to leverage Azure Functions within \\nthis same toolset. Fortunate\", \"ly, you can wrap Azure Functions inside Docker containers and deploy \\nthem using the same processes \", \"and tools as the rest of your Kubernetes -based app.  \\nWhen does it make sense to use containers wit\", \"h serverless?  \\nYour Azure Function has no knowledge of the platform on which it\\u2019s deployed. For som\", \"e scenarios, \\nyou may have specific requirements and need to customize the environment on which your\", \" function \\ncode will run. You\\u2019ll need a custom image that supports depende ncies or a configuration \", \"not \\nsupported by the default image. In these cases, it makes sense to deploy your function in a cus\", \"tom \\nDocker container.  \\nWhen should you avoid using containers with Azure Functions?  \\nIf you want \", \"to use consumption billing, you can\\u2019t run your function in a container. What\\u2019s more, if you \\ndeploy \", \"your function to a Kubernetes cluster, you\\u2019ll no longer benefit from the built -in scaling \\nprovided\", \" by Azure Functions. You\\u2019ll need to use Kuberne tes\\u2019 scaling features, described earlier in this \\nch\", \"apter.  \\nHow to combine serverless and Docker containers  \\nTo wrap an Azure Function in a Docker con\", \"tainer, install the Azure Functions Core Tools  and then run \\nthe following command:  \\nfunc init Pro\", \"jectName --worker-runtime dotnet --docker \\nWhen the project is created, it will include a Dockerfile\", \" and the worker runtime configured to dotnet. \\nNow, you can create and test your function locally. B\", \"uild and run it using the docker build and docker  \\n50 CHAPTER 3 | Scaling cloud -native application\", \"s  \\n run commands. For detailed steps to get started building Azure Functions with Docker support, s\", \"ee \\nthe Create a function on Linux using a custom image  tutorial.  \\nHow to combine serverless and K\", \"ubernetes with KEDA  \\nIn this chapter, you\\u2019ve seen that the Azure Functions\\u2019 platform automatically \", \"scales out to meet \\ndemand. When deploying containerized functions to AKS, however, you lose the bui\", \"lt -in scaling \\nfunctionality. To the rescue comes Kubernetes -based Event Driven (KEDA) . It enable\", \"s fine -grained \\nautoscaling for event -driven Kubernetes workloads, including containerized functio\", \"ns.  \\nKEDA provides event -driven scaling functionality to the Functions\\u2019 runtime in a Docker contai\", \"ner. \\nKEDA can scale from zero instances (when no events are occurring) out to n instances, based on\", \" load. \\nIt enables autoscaling by exposing custom metrics to the Kubernetes autoscaler (Horizontal P\", \"od \\nAutoscaler). Using Functions containers with KEDA makes it possible to replicate serverless func\", \"tion \\ncapabilities in any Kubernetes cluster.  \\nIt\\u2019s worth noting that the KEDA project is now manag\", \"ed by the Cloud Native Computing Foundation \\n(CNCF).  \\nDeploying containers in Azure  \\nWe\\u2019ve discuss\", \"ed containers in this chapter and in chapter 1. We\\u2019ve seen that containers provide many \\nbenefits to\", \" cloud -native applications, including portability. In the Azure cloud, you can deploy the \\nsame con\", \"tainerized services across staging and product ion environments. Azure provides several \\noptions for\", \" hosting these containerized workloads:  \\n\\u2022 Azure Kubernetes Services (AKS)  \\n\\u2022 Azure Container Inst\", \"ance (ACI)  \\n\\u2022 Azure Web Apps for Containers  \\nAzure Container Registry  \\nWhen containerizing a micr\", \"oservice, you first build a container \\u201cimage.\\u201d The image is a binary \\nrepresentation of the service \", \"code, dependencies, and runtime. While you can manually create an \\nimage using the Docker Build comm\", \"and from the Docker API, a better approach is to create it as part \\nof an automated build process.  \", \"\\nOnce created, container images are stored in container registries. They enable you to build, store,\", \" and \\nmanage container images. There are many registries available, both public and private. Azure \\n\", \"Container Registry (ACR) is a fully managed container regis try service in the Azure cloud. It persi\", \"sts \\nyour images inside the Azure network, reducing the time to deploy them to Azure container hosts\", \". \\nYou can also secure them using the same security and identity procedures that you use for other \\n\", \"Azure resources.  \\nYou create an Azure Container Registry using the Azure portal , Azure CLI , or Po\", \"werShell tools . \\nCreating a registry in Azure is simple. It requires an Azure subscription, resourc\", \"e group, and a unique  \\n51 CHAPTER 3 | Scaling cloud -native applications  \\n name. Figure 3 -10 show\", \"s the basic options for creating a registry, which will be hosted at \\nregistryname.azurecr.io.  \\n \\nF\", \"igure 3 -10. Create container registry  \\nOnce you\\u2019ve created the registry, you\\u2019ll need to authentica\", \"te with it before you can use it. Typically, \\nyou\\u2019ll log into the registry using the Azure CLI comma\", \"nd:  \\naz acr login --name *registryname*  \\nOnce authenticated, you can use docker commands to push c\", \"ontainer images to it. Before you can do \\nso, however, you must tag your image with the fully qualif\", \"ied name (URL) of your ACR login server. It \\nwill have the format registryname .azurecr.io.  \\ndocker\", \" tag mycontainer myregistry.azurecr.io/mycontainer:v1  \\nAfter you\\u2019ve tagged the image, you use the d\", \"ocker push command to push the image to your ACR \\ninstance.  \\ndocker push myregistry.azurecr.io/myco\", \"ntainer:v1  \\n \\n52 CHAPTER 3 | Scaling cloud -native applications  \\n After you push an image to the r\", \"egistry, it\\u2019s a good idea to remove the image from your local Docker \\nenvironment, using this comman\", \"d:  \\ndocker rmi myregistry.azurecr.io/mycontainer:v1  \\nAs a best practice, you shouldn\\u2019t manually pu\", \"sh images to a container registry. Instead, use a build \\npipeline defined in a tool like GitHub or A\", \"zure DevOps. Learn more in the Cloud -Native DevOps \\nchapter . \\nACR Tasks  \\nACR Tasks  is a set of f\", \"eatures available from the Azure Container Registry. It extends your inner -loop \\ndevelopment cycle \", \" by building and managing container images in the Azure cloud. Instead of \\ninvoking a docker build a\", \"nd docker push locally on your development machine, they\\u2019re automatically \\nhandled by ACR Tasks in t\", \"he cloud.  \\nThe following AZ CLI command both builds a container image and pushes it to ACR:  \\n# cre\", \"ate a container registry  \\naz acr create --resource -group myResourceGroup --name myContainerRegistr\", \"y008 --sku \\nBasic \\n \\n# build container image in ACR and push it into your container registry  \\naz ac\", \"r build --image sample/hello -world:v1  --registry myContain erRegistry008 --file \\nDockerfile .  \\nAs\", \" you can see from the previous command block, there\\u2019s no need to install Docker Desktop on your \\ndev\", \"elopment machine. Additionally, you can configure ACR Task triggers to rebuild containers images \\non\", \" both source code and base image updates.  \\nAzure Kubernetes Service  \\nWe discussed Azure Kubernetes\", \" Service (AKS) at length in this chapter. We\\u2019ve seen that it\\u2019s the de \\nfacto container orchestrator \", \"managing containerized cloud -native applications.  \\nOnce you deploy an image to a registry, such as\", \" ACR, you can configure AKS to automatically pull and \\ndeploy it. With a CI/CD pipeline in place, yo\", \"u might configure a canary release  strategy to minimize \\nthe risk involved when rapidly deploying u\", \"pdates. The new version of the app is initially configured in \\nproduction with no traffic routed to \", \"it. Then, the system will route a small percentage of users to the \\nnewly deployed version. As  the \", \"team gains confidence in the new version, it can roll out more \\ninstances and retire the old. AKS ea\", \"sily supports this style of deployment.  \\nAs with most resources in Azure, you can create an Azure K\", \"ubernetes Service cluster using the portal, \\ncommand -line, or automation tools like Helm or Terrafo\", \"rm. To get started with a new cluster, you \\nneed to provide the following information:  \\n\\u2022 Azure sub\", \"scription  \\n\\u2022 Resource group  \\n\\u2022 Kubernetes cluster name  \\n\\u2022 Region   \\n53 CHAPTER 3 | Scaling cloud \", \"-native applications  \\n \\u2022 Kubernetes version  \\n\\u2022 DNS name prefix  \\n\\u2022 Node size  \\n\\u2022 Node count  \\nThis\", \" information is sufficient to get started. As part of the creation process in the Azure portal, you \", \"can \\nalso configure options for the following features of your cluster:  \\n\\u2022 Scale  \\n\\u2022 Authentication\", \"  \\n\\u2022 Networking  \\n\\u2022 Monitoring  \\n\\u2022 Tags  \\nThis quickstart walks through deploying an AKS cluster usi\", \"ng the Azure portal . \\nAzure Bridge to Kubernetes  \\nCloud -native applications can grow large and co\", \"mplex, requiring significant compute resources to \\nrun. In these scenarios, the entire application c\", \"an\\u2019t be hosted on a development machine (especially a \\nlaptop). Azure Bridge to Kubernetes  addresse\", \"s the shortcoming. It enables developers to work with a \\nlocal version of their service while hostin\", \"g the entire application in an AKS development cluster.  \\nWhen ready, developers test their changes \", \"locally while running against the full application in the AKS \\ncluster - without replicating depende\", \"ncies. Under the hood, the bridge merges code from the local \\nmachine with services in AKS. Develope\", \"rs can rapidly i terate and debug code directly in Kubernetes \\nusing Visual Studio or Visual Studio \", \"Code.  \\nGabe Monroy, former VP of Product Management at Microsoft, describes it well:  \\nImagine you\\u2019\", \"re a new employee trying to fix a bug in a complex microservices application consisting \\nof dozens o\", \"f components, each with their own configuration and backing services. To get started, you \\nmust conf\", \"igure your local development environment so th at it can mimic production including setting \\nup your\", \" IDE, building tool chain, containerized service dependencies, a local Kubernetes environment, \\nmock\", \"s for backing services, and more. With all the time involved setting up your development \\nenvironmen\", \"t, fix ing that first bug could take days! Or you could just use Bridge to Kubernetes and \\nAKS. \\nSca\", \"ling containers and serverless applications  \\nThere  are two ways to scale an application: up or out\", \". The former refers to adding capacity to a single \\nresource, while the latter refers to adding more\", \" resources to increase capacity.  \\nThe simple solution: scaling up  \\nUpgrading an existing host serv\", \"er with increased CPU, memory, disk I/O speed, and network I/O \\nspeed is known as scaling up . Scali\", \"ng up a cloud -native application involves choosing more capable  \\n54 CHAPTER 3 | Scaling cloud -nat\", \"ive applications  \\n resources from the cloud vendor. For example, you can create a new node pool wit\", \"h larger VMs in \\nyour Kubernetes cluster. Then, migrate your containerized services to the new pool.\", \"  \\nServerless apps scale up by choosing the premium Functions plan  or premium instance sizes from a\", \" \\ndedicated app service plan.  \\nScaling out cloud -native apps  \\nCloud -native applications often ex\", \"perience large fluctuations in demand and require scale on a \\nmoment\\u2019s notice. They favor scaling ou\", \"t. Scaling out is done horizontally by adding additional \\nmachines (called nodes) or application ins\", \"tances to an existing cl uster. In Kubernetes, you can scale \\nmanually by adjusting configuration se\", \"ttings for the app (for example, scaling a node pool ), or \\nthrough autoscaling.  \\nAKS clusters can \", \"autoscale in one of two ways:  \\nFirst, the Horizontal Pod Autoscaler  monitors resource demand and a\", \"utomatically scales your POD \\nreplicas to meet it. When traffic increases, additional replicas are a\", \"utomatically provisioned to scale \\nout your services. Likewise, when demand decreases, they\\u2019re remov\", \"ed to scale -in your service s. You \\ndefine the metric on which to scale, for example, CPU usage. Yo\", \"u can also specify the minimum and \\nmaximum number of replicas to run. AKS monitors that metric and \", \"scales accordingly.  \\nNext, the AKS Cluster Autoscaler  feature enables you to automatically scale c\", \"ompute nodes across a \\nKubernetes cluster to meet demand. With it, you can automatically add new VMs\", \" to the underlying \\nAzure Virtual Machine Scale Set whenever more compute capacity of is required. I\", \"t also remove s \\nnodes when no longer required.  \\nFigure 3 -11 shows the relationship between these \", \"two scaling services.  \\n \\nFigure 3 -11. Scaling out an App Service plan.  \\n \\n55 CHAPTER 3 | Scaling \", \"cloud -native applications  \\n Working together, both ensure an optimal number of container instances\", \" and compute nodes to \\nsupport fluctuating demand. The horizontal pod autoscaler optimizes the numbe\", \"r of pods required. \\nThe cluster autoscaler optimizes the number of nodes required.  \\nScaling Azure \", \"Functions  \\nAzure Functions automatically scale out upon demand. Server resources are dynamically al\", \"located and \\nremoved based on the number of triggered events. You\\u2019re only charged for compute resour\", \"ces \\nconsumed when your functions run. Billing is based upon the numbe r of executions, execution ti\", \"me, \\nand memory used.  \\nWhile the default consumption plan provides an economical and scalable solut\", \"ion for most apps, the \\npremium option allows developers flexibility for custom Azure Functions requ\", \"irements. Upgrading to \\nthe premium plan provides control over instance sizes, pre -warmed instances\", \" (to avoid cold start \\ndelays), and dedicated VMs.  \\nOther container deployment options  \\nAside from\", \" Azure Kubernetes Service (AKS), you can also deploy containers to Azure App Service for \\nContainers\", \" and Azure Container Instances.  \\nWhen does it make sense to deploy to App Service for Containers?  \", \"\\nSimple production applications that don\\u2019t require orchestration are well suited to Azure App Servic\", \"e \\nfor Containers.  \\nHow to deploy to App Service for Containers  \\nTo deploy to Azure App Service fo\", \"r Containers , you\\u2019ll need an Azure Container Registry (ACR) instance \\nand credentials to access it.\", \" Push your container image to the ACR repository so that your Azure App \\nService can pull it when ne\", \"eded. Once complete, you can configure the app for Continuous \\nDeploymen t. Doing so will automatica\", \"lly deploy updates whenever the image changes in ACR.  \\nWhen does it make sense to deploy to Azure C\", \"ontainer Instances?  \\nAzure Container Instances (ACI)  enables you to run Docker containers in a man\", \"aged, serverless cloud \\nenvironment, without having to set up virtual machines or clusters. It\\u2019s a g\", \"reat solution for short -\\nrunning workloads that can run in an isolated container. Consider ACI for \", \"simple servic es, testing \\nscenarios, task automation, and build jobs. ACI spins -up a container ins\", \"tance, performs the task, and \\nthen spins it down.  \\nHow to deploy an app to Azure Container Instanc\", \"es  \\nTo deploy to Azure Container Instances (ACI) , you need an Azure Container Registry (ACR) and \\n\", \"credentials for accessing it. Once you push your container image to the repository, it\\u2019s available t\", \"o pull \\ninto ACI. You can work with ACI using the Azure portal or command -line interface. ACR provi\", \"des tight  \\nintegration with ACI. Figure 3 -12 shows how to push an individual container image to AC\", \"R.   \\n56 CHAPTER 3 | Scaling cloud -native applications  \\n  \\nFigure 3 -12. Azure Container Registry \", \"Run Instance  \\nCreating an instance in ACI can be done quickly. Specify the image registry, Azure re\", \"source group \\ninformation, the amount of memory to allocate, and the port on which to listen. This q\", \"uickstart shows \\nhow to deploy a container instance to ACI using the Azure portal . \\nOnce the deploy\", \"ment completes, find the newly deployed container\\u2019s IP address and communicate \\nwith it over the por\", \"t you specified.  \\nAzure Container Instances offers the fastest way to run simple container workload\", \"s in Azure. You don\\u2019t \\nneed to configure an app service, orchestrator, or virtual machine. For scena\", \"rios where you require full \\ncontainer orchestration, service discovery, auto matic scaling, or coor\", \"dinated upgrades, we \\nrecommend Azure Kubernetes Service (AKS).  \\nReferences  \\n\\u2022 What is Kubernetes?\", \"  \\n\\u2022 Installing Kubernetes with Minikube  \\n\\u2022 MiniKube vs Docker Desktop  \\n\\u2022 Visual Studio Tools for \", \"Docker  \\n \\n57 CHAPTER 3 | Scaling cloud -native applications  \\n \\u2022 Understanding serverless cold star\", \"t  \\n\\u2022 Pre-warmed Azure Functions instances  \\n\\u2022 Create a function on Linux using a custom image  \\n\\u2022 R\", \"un Azure Functions in a Docker Container  \\n\\u2022 Create a function on Linux using a custom image  \\n\\u2022 Azu\", \"re Functions with Kubernetes Event Driven Autoscaling  \\n\\u2022 Canary Release  \\n\\u2022 Azure Dev Spaces with V\", \"S Code  \\n\\u2022 Azure Dev Spaces with Visual Studio  \\n\\u2022 AKS Multiple Node Pools  \\n\\u2022 AKS Cluster Autoscale\", \"r  \\n\\u2022 Tutorial: Scale applications in AKS  \\n\\u2022 Azure Functions scale and hosting  \\n\\u2022 Azure Container \", \"Instances Docs  \\n\\u2022 Deploy Container Instance from ACR   \\n58 CHAPTER 4 | Cloud -native communication \", \"patterns  \\n CHAPTER  4 \\nCloud -native \\ncommunication patterns  \\nWhen constructing a cloud -native sy\", \"stem, communication becomes a significant design decision. How \\ndoes a front -end client application\", \" communicate with a back -end microservice? How do back -end \\nmicroservices communicate with each ot\", \"her? What are the principl es, patterns, and best practices to \\nconsider when implementing communica\", \"tion in cloud -native applications?  \\nCommunication considerations  \\nIn a monolithic application, co\", \"mmunication is straightforward. The code modules execute together in \\nthe same executable space (pro\", \"cess) on a server. This approach can have performance advantages as \\neverything runs together in sha\", \"red memory, but results in tightly coupled code that becomes difficult \\nto maintain, evolve, and sca\", \"le.  \\nCloud -native systems implement a microservice -based architecture with many small, independen\", \"t \\nmicroservices. Each microservice executes in a separate process and typically runs inside a conta\", \"iner \\nthat is deployed to a cluster . \\nA cluster groups a pool of virtual machines together to form \", \"a highly available environment. They\\u2019re \\nmanaged with an orchestration tool, which is responsible fo\", \"r deploying and managing the \\ncontainerized microservices. Figure 4 -1 shows a Kubernetes  cluster d\", \"eployed into the Azure cloud \\nwith the fully managed Azure Kubernetes Services .  \\n59 CHAPTER 4 | Cl\", \"oud -native communication patterns  \\n  \\nFigure 4 -1. A Kubernetes cluster in Azure  \\nAcross the clus\", \"ter, microservices communicate with each other through APIs and messaging \\ntechnologies.  \\nWhile the\", \"y provide many benefits, microservices are no free lunch. Local in -process method calls \\nbetween co\", \"mponents are now replaced with network calls. Each microservice must communicate over \\na network pro\", \"tocol, which adds complexity to your system:  \\n\\u2022 Network congestion, latency, and transient faults a\", \"re a constant concern.  \\n\\u2022 Resiliency (that is, retrying failed requests) is essential.  \\n\\u2022 Some cal\", \"ls must be idempotent  as to keep consistent state.  \\n\\u2022 Each microservice must authenticate and auth\", \"orize calls.  \\n\\u2022 Each message must be serialized and then deserialized - which can be expensive.  \\n\\u2022\", \" Message encryption/decryption becomes important.  \\nThe book .NET Microservices: Architecture for Co\", \"ntainerized .NET Applications , available for free from \\nMicrosoft, provides an in -depth coverage o\", \"f communication patterns for microservice applications. In \\nthis chapter, we provide a high -level o\", \"verview of these patterns along with implementation options \\navailable in the Azure cloud.  \\nIn this\", \" chapter, we\\u2019ll first address communication between front -end applications and back -end \\nmicroserv\", \"ices. We\\u2019ll then look at back -end microservices communicate with each other. We\\u2019ll explore \\n \\n60 CH\", \"APTER 4 | Cloud -native communication patterns  \\n the up and gRPC communication technology. Finally,\", \" we\\u2019ll look new innovative communication \\npatterns using service mesh technology. We\\u2019ll also see how\", \" the Azure cloud provides different kinds \\nof backing services  to support cloud -native communicati\", \"on.  \\nFront -end client communication  \\nIn a cloud -native system, front -end clients (mobile, web, \", \"and desktop applications) require a \\ncommunication channel to interact with independent back -end mi\", \"croservices.  \\nWhat are the options?  \\nTo keep things simple, a front -end client could directly com\", \"municate  with the back -end microservices, \\nshown in Figure 4 -2. \\n \\nFigure 4 -2. Direct client to \", \"service communication  \\nWith this approach, each microservice has a public endpoint that is accessib\", \"le by front -end clients. In \\na production environment, you\\u2019d place a load balancer in front of the \", \"microservices, routing traffic \\nproportionately.  \\nWhile simple to implement, direct client communic\", \"ation would be acceptable only for simple \\nmicroservice applications. This pattern tightly couples f\", \"ront -end clients to core back -end services, \\nopening the door for many problems, including:  \\n\\u2022 Cl\", \"ient susceptibility to back -end service refactoring.  \\n\\u2022 A wider attack surface as core back -end s\", \"ervices are directly exposed.  \\n\\u2022 Duplication of cross -cutting concerns across each microservice.  \", \"\\n\\u2022 Overly complex client code - clients must keep track of multiple endpoints and handle failures \\ni\", \"n a resilient way.  \\n \\n61 CHAPTER 4 | Cloud -native communication patterns  \\n Instead, a widely acce\", \"pted cloud design pattern is to implement an API Gateway Service  between the \\nfront -end applicatio\", \"ns and back -end services. The pattern is shown in Figure 4 -3. \\n \\nFigure 4 -3. API gateway pattern \", \" \\nIn the previous figure, note how the API Gateway service abstracts the back -end core microservice\", \"s. \\nImplemented as a web API, it acts as a reverse proxy , routing incoming traffic to the internal \", \"\\nmicroservices.  \\nThe gateway insulates the client from internal service partitioning and refactorin\", \"g. If you change a \\nback -end service, you accommodate for it in the gateway without breaking the cl\", \"ient. It\\u2019s also your \\nfirst line of defense for cross -cutting concerns, such as identity, caching, \", \"resiliency, metering, and \\nthrottling. Many of these cross -cutting concerns can be off -loaded from\", \" the back -end core services to \\nthe gateway, simplifying the back -end services.  \\nCare must be tak\", \"en to keep the API Gateway simple and fast. Typically, business logic is kept out of \\nthe gateway. A\", \" complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger \\nsystems often\", \" expose multiple API Gateways segmented by  client type (mobile, web, desktop) or \\nback -end functio\", \"nality. The Backend for Frontends  pattern provides direction for implementing \\nmultiple gateways. T\", \"he pattern is shown in Figure 4 -4. \\n \\n62 CHAPTER 4 | Cloud -native communication patterns  \\n  \\nFigu\", \"re 4 -4. Backend for frontend pattern  \\nNote in the previous figure how incoming traffic is sent to \", \"a specific API gateway - based upon client \\ntype: web, mobile, or desktop app. This approach makes s\", \"ense as the capabilities of each device differ \\nsignificantly across form factor, performance, and  \", \"display limitations. Typically mobile applications \\nexpose less functionality than a browser or desk\", \"top applications. Each gateway can be optimized to \\nmatch the capabilities and functionality of the \", \"corresponding device.  \\nSimple Gateways  \\nTo start, you could build your own API Gateway service. A \", \"quick search of GitHub will provide many \\nexamples.  \\nFor simple .NET cloud -native applications, yo\", \"u might consider the Ocelot Gateway . Open source and \\ncreated for .NET microservices, it\\u2019s lightwei\", \"ght, fast, scalable. Like any API Gateway, its primary \\nfunctionality is to forward incoming HTTP re\", \"quests to downstream services. Additionally, it supports a \\nwide variety of capabilities that a re c\", \"onfigurable in a .NET middleware pipeline.  \\nYARP  (Yet Another Reverse proxy) is another open sourc\", \"e reverse proxy led by a group of Microsoft \\nproduct teams. Downloadable as a NuGet package, YARP pl\", \"ugs into the ASP.NET framework as \\nmiddleware and is highly customizable. You\\u2019ll find YARP well-docu\", \"mented  with various usage \\nexamples.  \\nFor enterprise cloud -native applications, there are several\", \" managed Azure services that can help \\njump -start your efforts.  \\n \\n63 CHAPTER 4 | Cloud -native co\", \"mmunication patterns  \\n Azure Application Gateway  \\nFor simple gateway requirements, you may conside\", \"r Azure Application Gateway . Available as an Azure \\nPaaS service , it includes basic gateway featur\", \"es such as URL routing, SSL termination, and a Web \\nApplication Firewall. The service supports Layer\", \" -7 load balancing  capabilities. With Layer 7, you can \\nroute requests based on the actual content \", \"of an HTTP message, not just low -level TCP network \\npackets.  \\nThroughout this book, we evangelize \", \"hosting cloud -native systems in Kubernetes . A container \\norchestrator, Kubernetes automates the de\", \"ployment, scaling, and operational concerns of \\ncontainerized workloads. Azure Application Gateway c\", \"an be configured as an API gateway for Azure \\nKubernetes Service  cluster.  \\nThe Application Gateway\", \" Ingress Controller  enables Azure Application Gateway to work directly with \\nAzure Kubernetes Servi\", \"ce . Figure 4.5 shows the architecture.  \\n \\nFigure 4 -5. Application Gateway Ingress Controller  \\nKu\", \"bernetes includes a built -in feature that supports HTTP (Level 7) load balancing, called Ingress . \", \"\\nIngress defines a set of rules for how microservice instances inside AKS can be exposed to the outs\", \"ide \\nworld. In the previous image, the ingress controller interprets the ingress rules configured fo\", \"r the \\ncluster and automatically configures the Azure App lication Gateway. Based on those rules, th\", \"e \\nApplication Gateway routes traffic to microservices running inside AKS. The ingress controller li\", \"stens \\nfor changes to ingress rules and makes the appropriate changes to the Azure Application Gatew\", \"ay.  \\nAzure API Management  \\nFor moderate to large -scale cloud -native systems, you may consider Az\", \"ure API Management . It\\u2019s a \\ncloud -based service that not only solves your API Gateway needs, but p\", \"rovides a full -featured \\ndeveloper and administrative experience. API Management is shown in Figure\", \" 4 -6. \\n \\n64 CHAPTER 4 | Cloud -native communication patterns  \\n  \\nFigure 4 -6. Azure API Management\", \"  \\nTo start, API Management exposes a gateway server that allows controlled access to back -end \\nser\", \"vices based upon configurable rules and policies. These services can be in the Azure cloud, your \\non\", \"-prem data center, or other public clouds. API keys and JWT to kens determine who can do what. All \\n\", \"traffic is logged for analytical purposes.  \\nFor developers, API Management offers a developer porta\", \"l that provides access to services, \\ndocumentation, and sample code for invoking them. Developers ca\", \"n use Swagger/Open API to \\ninspect service endpoints and analyze their usage. The service works acro\", \"ss the major development \\nplatforms: .NET, Java, Golang, and more.  \\nThe publisher portal exposes a \", \"management dashboard where administrators expose APIs and \\nmanage their behavior. Service access can\", \" be granted, service health monitored, and service telemetry \\ngathered. Administrators apply policie\", \"s  to each endpoint to affect behavior. Policies  are pre -built \\nstatements that execute sequential\", \"ly for each service call. Policies are configured for an inbound call, \\noutbound call, or invoked up\", \"on an error. Policies can be applied at different service scopes as to \\nenable deterministic orderin\", \"g when co mbining policies. The product ships with a large number of \\nprebuilt policies . \\nHere are \", \"examples of how policies can affect the behavior of your cloud -native services:  \\n\\u2022 Restrict servic\", \"e access.  \\n\\u2022 Enforce authentication.  \\n \\n\\u2022 Throttle calls from a single source, if necessary.  \\n\\u2022 E\", \"nable caching.  \\n\\u2022 Block calls from specific IP addresses.  \\n \\n65 CHAPTER 4 | Cloud -native communic\", \"ation patterns  \\n \\u2022 Control the flow of the service.  \\n\\u2022 Convert requests from SOAP to REST or betwe\", \"en different data formats, such as from XML to \\nJSON.  \\nAzure API Management can expose back -end se\", \"rvices that are hosted anywhere \\u2013 in the cloud or your \\ndata center. For legacy services that you ma\", \"y expose in your cloud -native systems, it supports both \\nREST and SOAP APIs. Even other Azure servi\", \"ces can be expos ed through API Management. You could \\nplace a managed API on top of an Azure backin\", \"g service like Azure Service Bus  or Azure Logic Apps . \\nAzure API Management doesn\\u2019t include built \", \"-in load -balancing support and should be used in \\nconjunction with a load -balancing service.  \\nAzu\", \"re API Management is available across four different tiers : \\n\\u2022 Developer  \\n\\u2022 Basic  \\n\\u2022 Standard  \\n\\u2022\", \" Premium  \\nThe Developer tier is meant for non -production workloads and evaluation. The other tiers\", \" offer \\nprogressively more power, features, and higher service level agreements (SLAs). The Premium \", \"tier \\nprovides Azure Virtual Network  and multi -region support . All tiers have a fixed price per h\", \"our.  \\nThe Azure cloud also offers a serverless tier  for Azure API Management. Referred to as the \\n\", \"consumption pricing tier , the service is a variant of API Management designed around the serverless\", \" \\ncomputing model. Unlike the \\u201cpre -allocated\\u201d pricing tiers previously shown, the consumption tier \", \"\\nprovides instant provisioning and pay -per-action pricing.  \\nIt enables API Gateway features for th\", \"e following use cases:  \\n\\u2022 Microservices implemented using serverless technologies such as Azure Fun\", \"ctions  and Azure \\nLogic Apps . \\n\\u2022 Azure backing service resources such as Service Bus queues and to\", \"pics, Azure storage, and \\nothers.  \\n\\u2022 Microservices where traffic has occasional large spikes but re\", \"mains low most the time.  \\nThe consumption tier uses the same underlying service API Management comp\", \"onents, but employs \\nan entirely different architecture based on dynamically allocated resources. It\", \" aligns perfectly with the \\nserverless computing model:  \\n\\u2022 No infrastructure to manage.  \\n\\u2022 No idle\", \" capacity.  \\n\\u2022 High -availability.  \\n\\u2022 Automatic scaling.  \\n\\u2022 Cost is based on actual usage.  \\nThe n\", \"ew consumption tier is a great choice for cloud -native systems that expose serverless resources \\nas\", \" APIs.   \\n66 CHAPTER 4 | Cloud -native communication patterns  \\n Real-time communication  \\nReal-time\", \", or push, communication is another option for front -end applications that communicate \\nwith back -\", \"end cloud -native systems over HTTP. Applications, such as financial -tickers, online \\neducation, ga\", \"ming, and job -progress updates, require instantaneous , real -time responses from the \\nback -end. W\", \"ith normal HTTP communication, there\\u2019s no way for the client to know when new data is \\navailable. Th\", \"e client must continually poll or send requests to the server. With real-time \\ncommunication, the se\", \"rver can push ne w data to the client at any time.  \\nReal-time systems are often characterized by hi\", \"gh -frequency data flows and large numbers of \\nconcurrent client connections. Manually implementing \", \"real -time connectivity can quickly become \\ncomplex, requiring non -trivial infrastructure to ensure\", \" scalability a nd reliable messaging to connected \\nclients. You could find yourself managing an inst\", \"ance of Azure Redis Cache and a set of load \\nbalancers configured with sticky sessions for client af\", \"finity.  \\nAzure SignalR Service  is a fully managed Azure service that simplifies real -time communi\", \"cation for \\nyour cloud -native applications. Technical implementation details like capacity provisio\", \"ning, scaling, \\nand persistent connections are abstracted away. They\\u2019re handled for you with  a 99.9\", \"% service -level \\nagreement. You focus on application features, not infrastructure plumbing.  \\nOnce \", \"enabled, a cloud -based HTTP service can push content updates directly to connected clients, \\ninclud\", \"ing browser, mobile and desktop applications. Clients are updated without the need to poll the \\nserv\", \"er. Azure SignalR abstracts the transport technologies  that create real -time connectivity, includi\", \"ng \\nWebSockets, Server -Side Events, and Long Polling. Developers focus on sending messages to all o\", \"r \\nspecific subsets of connected clients.  \\nFigure 4 -7 shows a set of HTTP Clients connecting to a \", \"Cloud -native application with Azure SignalR \\nenabled.   \\n67 CHAPTER 4 | Cloud -native communication\", \" patterns  \\n  \\nFigure 4 -7. Azure SignalR  \\nAnother advantage of Azure SignalR Service comes with im\", \"plementing Serverless cloud -native \\nservices. Perhaps your code is executed on demand with Azure Fu\", \"nctions triggers. This scenario can \\nbe tricky because your code doesn\\u2019t maintain long connections w\", \"ith clients. Azure SignalR Service can \\nhandle this situation since the service already manages conn\", \"ections for you.  \\nAzure SignalR Service closely integrates with other Azure services, such as Azure\", \" SQL Database, \\nService Bus, or Redis Cache, opening up many possibilities for your cloud -native ap\", \"plications.  \\nService -to-service communication  \\nMoving from the front -end client, we now address \", \"back -end microservices communicate with each \\nother.  \\nWhen constructing a cloud -native applicatio\", \"n, you\\u2019ll want to be sensitive to how back -end services \\ncommunicate with each other. Ideally, the \", \"less inter -service communication, the better. However, \\navoidance isn\\u2019t always possible as back -en\", \"d services often rely on one another to complete an \\noperation.  \\nThere are several widely accepted \", \"approaches to implementing cross -service communication. The type \\nof communication interaction  wil\", \"l often determine the best approach.  \\nConsider the following interaction types:  \\n\\u2022 Query  \\u2013 when a\", \" calling microservice requires a response from a called microservice, such as, \\n\\u201cHey, give me the bu\", \"yer information for a given customer Id.\\u201d  \\n \\n68 CHAPTER 4 | Cloud -native communication patterns  \\n\", \" \\u2022 Command  \\u2013 when the calling microservice needs another microservice to execute an action \\nbut doe\", \"sn\\u2019t require a response, such as, \\u201cHey, just ship this order.\\u201d  \\n\\u2022 Event  \\u2013 when a microservice, cal\", \"led the publisher, raises an event that state has changed or an \\naction has occurred. Other microser\", \"vices, called subscribers, who are interested, can react to \\nthe event appropriately. The publisher \", \"and the subscribers aren\\u2019t awar e of each other.  \\nMicroservice systems typically use a combination \", \"of these interaction types when executing \\noperations that require cross -service interaction. Let\\u2019s\", \" take a close look at each and how you might \\nimplement them.  \\nQueries  \\nMany times, one microservi\", \"ce might need to query  another, requiring an immediate response to \\ncomplete an operation. A shoppi\", \"ng basket microservice may need product information and a price to \\nadd an item to its basket. There\", \" are many approaches for implementing query operations.  \\nRequest/Response Messaging  \\nOne option fo\", \"r implementing this scenario is for the calling back -end microservice to make direct \\nHTTP requests\", \" to the microservices it needs to query, shown in Figure 4 -8. \\n \\nFigure 4 -8. Direct HTTP communica\", \"tion  \\nWhile direct HTTP calls between microservices are relatively simple to implement, care should\", \" be \\ntaken to minimize this practice. To start, these calls are always synchronous  and will block t\", \"he \\noperation until a result is returned or the request times outs. What were once self -contained, \", \"\\nindependent services, able to evolve independently and deploy frequently, now become coupled to \\nea\", \"ch other. As coupling among microservices i ncrease, their architectural benefits diminish.  \\n \\n69 C\", \"HAPTER 4 | Cloud -native communication patterns  \\n Executing an infrequent request that makes a sing\", \"le direct HTTP call to another microservice might be \\nacceptable for some systems. However, high -vo\", \"lume calls that invoke direct HTTP calls to multiple \\nmicroservices aren\\u2019t advisable. They can incre\", \"ase latenc y and negatively impact the performance, \\nscalability, and availability of your system. E\", \"ven worse, a long series of direct HTTP communication can \\nlead to deep and complex chains of synchr\", \"onous microservices calls, shown in Figure 4 -9: \\n \\nFigure 4 -9. Chaining HTTP queries  \\nYou can cer\", \"tainly imagine the risk in the design shown in the previous image. What happens if Step \\n#3 fails? O\", \"r Step #8 fails? How do you recover? What if Step #6 is slow because the underlying service \\nis busy\", \"? How do you continue? Even if all works correc tly, think of the latency this call would incur, \\nwh\", \"ich is the sum of the latency of each step.  \\nThe large degree of coupling in the previous image sug\", \"gests the services weren\\u2019t optimally modeled. \\nIt would behoove the team to revisit their design.  \\n\", \"Materialized View pattern  \\nA popular option for removing microservice coupling is the Materialized \", \"View pattern . With this \\npattern, a microservice stores its own local, denormalized copy of data th\", \"at\\u2019s owned by other services. \\nInstead of the Shopping Basket microservice querying the Product Cata\", \"log and Pricing microservices, \\nit maintains its own local copy of that data. This pattern eliminate\", \"s unnecessary coupling and \\nimproves reliability and response time. The entire operation executes in\", \"side a single process. We \\nexplore this pattern and other data concerns in Chapter 5.  \\nService Aggr\", \"egator Pattern  \\nAnother option for eliminating microservice -to-microservice coupling is an Aggrega\", \"tor microservice , \\nshown in purple in Figure 4 -10. \\n \\n70 CHAPTER 4 | Cloud -native communication p\", \"atterns  \\n  \\nFigure 4 -10. Aggregator microservice  \\nThe pattern isolates an operation that makes ca\", \"lls to multiple back -end microservices, centralizing its \\nlogic into a specialized microservice. Th\", \"e purple checkout aggregator microservice in the previous \\nfigure orchestrates the workflow for the \", \"Checkout operation. It includes calls to several back -end \\nmicroservices in a sequenced order. Data\", \" from the workflow is aggregate d and returned to the caller. \\nWhile it still implements direct HTTP\", \" calls, the aggregator microservice reduces direct dependencies \\namong back -end microservices.  \\nRe\", \"quest/Reply Pattern  \\nAnother approach for decoupling synchronous HTTP messages is a Request -Reply \", \"Pattern , which uses \\nqueuing communication. Communication using a queue is always a one -way channe\", \"l, with a producer \\nsending the message and consumer receiving it. With this pattern, both a request\", \" queue and response \\nqueue are implemented, shown in Figure 4 -11. \\n \\n71 CHAPTER 4 | Cloud -native c\", \"ommunication patterns  \\n  \\nFigure 4 -11. Request -reply pattern  \\nHere, the message producer creates\", \" a query -based message that contains a unique correlation ID and \\nplaces it into a request queue. T\", \"he consuming service dequeues the messages, processes it and places \\nthe response into the response \", \"queue with the same correlation ID. The producer service dequeues \\nthe message, matches it with the \", \"correlation ID and continues processing. We cover queues in detail \\nin the next section.  \\nCommands \", \" \\nAnother type of communication interaction is a command . A microservice may need another \\nmicroser\", \"vice to perform an action. The Ordering microservice may need the Shipping microservice to \\ncreate a\", \" shipment for an approved order. In Figure 4 -12, one microservice, called a Producer, sends a \\nmess\", \"age to another mi croservice, the Consumer, commanding it to do something.  \\n \\n72 CHAPTER 4 | Cloud \", \"-native communication patterns  \\n  \\nFigure 4 -12. Command interaction with a queue  \\nMost often, the\", \" Producer doesn\\u2019t require a response and can fire-and-forget  the message. If a reply is \\nneeded, th\", \"e Consumer sends a separate message back to Producer on another channel. A command \\nmessage is best \", \"sent asynchronously with a message queue. supported by a lightweight message \\nbroker. In the previou\", \"s diagram, note how a queue separates and decouples both services.  \\nA message queue is an intermedi\", \"ary construct through which a producer and consumer pass a \\nmessage. Queues implement an asynchronou\", \"s, point -to-point messaging pattern. The Producer \\nknows where a command needs to be sent and route\", \"s appropriately. The queue g uarantees that a \\nmessage is processed by exactly one of the consumer i\", \"nstances that are reading from the channel. In \\nthis scenario, either the producer or consumer servi\", \"ce can scale out without affecting the other. As \\nwell, technologies can be disparate on  each side,\", \" meaning that we might have a Java microservice \\ncalling a Golang  microservice.  \\nIn chapter 1, we \", \"talked about backing services . Backing services are ancillary resources upon which \\ncloud -native s\", \"ystems depend. Message queues are backing services. The Azure cloud supports two \\ntypes of message q\", \"ueues that your cloud -native systems can consume to implement command \\nmessaging: Azure St orage Qu\", \"eues and Azure Service Bus Queues.  \\nAzure Storage Queues  \\nAzure storage queues offer a simple queu\", \"eing infrastructure that is fast, affordable, and backed by \\nAzure storage accounts.  \\nAzure Storage\", \" Queues  feature a REST -based queuing mechanism with reliable and persistent \\nmessaging. They provi\", \"de a minimal feature set, but are inexpensive and store millions of messages. \\nTheir capacity ranges\", \" up to 500 TB. A single message can be up to 64 KB in size.  \\nYou can access messages from anywhere \", \"in the world via authenticated calls using HTTP or HTTPS. \\nStorage queues can scale out to large num\", \"bers of concurrent clients to handle traffic spikes.  \\n \\n73 CHAPTER 4 | Cloud -native communication \", \"patterns  \\n That said, there are limitations with the service:  \\n\\u2022 Message order isn\\u2019t guaranteed.  \", \"\\n\\u2022 A message can only persist for seven days before it\\u2019s automatically removed.  \\n\\u2022 Support for stat\", \"e management, duplicate detection, or transactions isn\\u2019t available.  \\nFigure 4 -13 shows the hierarc\", \"hy of an Azure Storage Queue.  \\n \\nFigure 4 -13. Storage queue hierarchy  \\nIn the previous figure, no\", \"te how storage queues store their messages in the underlying Azure Storage \\naccount.  \\nFor developer\", \"s, Microsoft provides several client and server -side libraries for Storage queue \\nprocessing. Most \", \"major platforms are supported including .NET, Java, JavaScript, Ruby, Python, and \\nGo. Developers sh\", \"ould never communicate directly with these lib raries. Doing so will tightly couple \\nyour microservi\", \"ce code to the Azure Storage Queue service. It\\u2019s a better practice to insulate the \\nimplementation d\", \"etails of the API. Introduce an intermediation layer, or intermediate API, that exposes \\ngeneric ope\", \"ration s and encapsulates the concrete library. This loose coupling enables you to swap out \\none que\", \"uing service for another without having to make changes to the mainline service code.  \\nAzure Storag\", \"e queues are an economical option to implement command messaging in your cloud -\\nnative applications\", \". Especially when a queue size will exceed 80 GB, or a simple feature set is \\nacceptable. You only p\", \"ay for the storage of the messages; there are n o fixed hourly charges.  \\nAzure Service Bus Queues  \", \"\\nFor more complex messaging requirements, consider Azure Service Bus queues.  \\nSitting atop a robust\", \" message infrastructure, Azure Service Bus  supports a brokered messaging model . \\nMessages are reli\", \"ably stored in a broker (the queue) until received by the consumer. The queue \\nguarantees First -In/\", \"First -Out (FIFO) message delivery, respecting the order in which messages were \\nadded to the queue.\", \"  \\nThe size of a message can be much larger, up to 256 KB. Messages are persisted in the queue for a\", \"n \\nunlimited period of time. Service Bus supports not only HTTP -based calls, but also provides full\", \" \\n \\n74 CHAPTER 4 | Cloud -native communication patterns  \\n support for the AMQP protocol . AMQP is a\", \"n open -standard across vendors that supports a binary \\nprotocol and higher degrees of reliability. \", \" \\nService Bus provides a rich set of features, including transaction support  and a duplicate detect\", \"ion \\nfeature . The queue guarantees \\u201cat most once delivery\\u201d per message. It automatically discards a\", \" \\nmessage that has already been sent. If a producer is in doubt, it can resend the same message, and\", \" \\nService Bus guarantees that only one copy will be processed. Duplicat e detection frees you from \\n\", \"having to build additional infrastructure plumbing.  \\nTwo more enterprise features are partitioning \", \"and sessions. A conventional Service Bus queue is \\nhandled by a single message broker and stored in \", \"a single message store. But, Service Bus Partitioning  \\nspreads the queue across multiple message br\", \"okers and message stores. The overall throughput is no \\nlonger limited by the performance of a singl\", \"e message broker or messaging store. A temporary \\noutage of a messaging store doesn\\u2019t render a parti\", \"tioned queue unavailable.  \\nService Bus Sessions  provide a way to group -related messages. Imagine \", \"a workflow scenario where \\nmessages must be processed together and the operation completed at the en\", \"d. To take advantage, \\nsessions must be explicitly enabled for the queue and each related messaged m\", \"ust cont ain the same \\nsession ID.  \\nHowever, there are some important caveats: Service Bus queues s\", \"ize is limited to 80 GB, which is much \\nsmaller than what\\u2019s available from store queues. Additionall\", \"y, Service Bus queues incur a base cost \\nand charge per operation.  \\nFigure 4 -14 outlines the high \", \"-level architecture of a Service Bus queue.  \\n \\nFigure 4 -14. Service Bus queue  \\nIn the previous fi\", \"gure, note the point -to-point relationship. Two instances of the same provider are \\nenqueuing messa\", \"ges into a single Service Bus queue. Each message is consumed by only one of three \\nconsumer instanc\", \"es on the right. Next, we discuss how to implement messaging where different \\nconsumers may all be i\", \"nterested the same message.  \\nEvents  \\nMessage queuing is an effective way to implement communicatio\", \"n where a producer can \\nasynchronously send a consumer a message. However, what happens when many di\", \"fferent consumers  \\n \\n75 CHAPTER 4 | Cloud -native communication patterns  \\n are interested in the s\", \"ame message? A dedicated message queue for each consumer wouldn\\u2019t scale \\nwell and would become diffi\", \"cult to manage.  \\nTo address this scenario, we move to the third type of message interaction, the ev\", \"ent . One \\nmicroservice announces that an action had occurred. Other microservices, if interested, r\", \"eact to the \\naction, or event. This is also known as the event -driven architectural style . \\nEventi\", \"ng is a two -step process. For a given state change, a microservice publishes an event to a \\nmessage\", \" broker, making it available to any other interested microservice. The interested microservice \\nis n\", \"otified by subscribing to the event in the message br oker. You use the Publish/Subscribe  pattern \\n\", \"to implement event -based communication . \\nFigure 4 -15 shows a shopping basket microservice publish\", \"ing an event with two other microservices \\nsubscribing to it.  \\n \\nFigure 4 -15. Event -Driven messag\", \"ing  \\nNote the event bus  component that sits in the middle of the communication channel. It\\u2019s a cus\", \"tom \\nclass that encapsulates the message broker and decouples it from the underlying application. Th\", \"e \\nordering and inventory microservices independently operate the event with no kno wledge of each \\n\", \"other, nor the shopping basket microservice. When the registered event is published to the event bus\", \", \\nthey act upon it.  \\nWith eventing, we move from queuing technology to topics . A topic  is simila\", \"r to a queue, but supports \\na one -to-many messaging pattern. One microservice publishes a message. \", \"Multiple subscribing \\nmicroservices can choose to receive and act upon that message. Figure 4 -16 sh\", \"ows a topic \\narchitecture.  \\n \\n76 CHAPTER 4 | Cloud -native communication patterns  \\n  \\nFigure 4 -16\", \". Topic architecture  \\nIn the previous figure, publishers send messages to the topic. At the end, su\", \"bscribers receive \\nmessages from subscriptions. In the middle, the topic forwards messages to subscr\", \"iptions based on a \\nset of rules, shown in dark blue boxes. Rules act as a filter that forward speci\", \"fic messages to a \\nsubscription. Here, a \\u201cGetPrice\\u201d event would be sent to the price and logging sub\", \"scriptions as the \\nlogging subsc ription has chosen to receive all messages. A \\u201cGetInformation\\u201d even\", \"t would be sent to \\nthe information and logging subscriptions.  \\nThe Azure cloud supports two differ\", \"ent topic services: Azure Service Bus Topics and Azure EventGrid.  \\nAzure Service Bus Topics  \\nSitti\", \"ng on top of the same robust brokered message model of Azure Service Bus queues are Azure \\nService B\", \"us Topics . A topic can receive messages from multiple independent publishers and send \\nmessages to \", \"up to 2,000 subscribers. Subscriptions can be dynamically added or removed at run time \\nwithout stop\", \"ping the system or recreating the topic.  \\nMany advanced features from Azure Service Bus queues are \", \"also available for topics, including \\nDuplicate Detection  and Transaction support . By default, Ser\", \"vice Bus topics are handled by a single \\nmessage broker and stored in a single message store. But, S\", \"ervice Bus Partitioning  scales a topic by \\nspreading it across many message brokers and message sto\", \"res.  \\nScheduled Message Delivery  tags a message with a specific time for processing. The message w\", \"on\\u2019t \\nappear in the topic before that time. Message Deferral  enables you to defer a retrieval of a \", \"message \\nto a later time. Both are commonly used in workflow processing scenarios where operations a\", \"re \\nprocessed in a particular order. You can postpone processing of received messages until prior wo\", \"rk \\nhas been complet ed. \\nService Bus topics are a robust and proven technology for enabling publish\", \"/subscribe communication \\nin your cloud -native systems.  \\nAzure Event Grid  \\nWhile Azure Service Bu\", \"s is a battle -tested messaging broker with a full set of enterprise features, \\nAzure Event Grid  is\", \" the new kid on the block.  \\n \\n77 CHAPTER 4 | Cloud -native communication patterns  \\n At first glanc\", \"e, Event Grid may look like just another topic -based messaging system. However, it\\u2019s \\ndifferent in \", \"many ways. Focused on event -driven workloads, it enables real -time event processing, \\ndeep Azure i\", \"ntegration, and an open -platform - all on serverless infrastructure. It\\u2019s designed for \\ncontemporar\", \"y cloud -native and serverless applications  \\nAs a centralized eventing backplane , or pipe, Event G\", \"rid reacts to events inside Azure resources and \\nfrom your own services.  \\nEvent notifications are p\", \"ublished to an Event Grid Topic, which, in turn, routes each event to a \\nsubscription. Subscribers m\", \"ap to subscriptions and consume the events. Like Service Bus, Event Grid \\nsupports a filtered subscr\", \"iber model  where a subscription sets rule for the events it wishes to receive. \\nEvent Grid provides\", \" fast throughput with a guarantee of 10 million events per second enabling near \\nreal-time delivery \", \"- far more than what Azure Service Bus can generate.  \\nA sweet spot for Event Grid is its deep integ\", \"ration into the fabric of Azure infrastructure. An Azure \\nresource, such as Cosmos DB, can publish b\", \"uilt -in events directly to other interested Azure resources - \\nwithout the need for custom code. Ev\", \"ent Grid can p ublish events from an Azure Subscription, \\nResource Group, or Service, giving develop\", \"ers fine -grained control over the lifecycle of cloud \\nresources. However, Event Grid isn\\u2019t limited \", \"to Azure. It\\u2019s an open platform that can consume custom \\nHTTP events publis hed from applications or\", \" third -party services and route events to external \\nsubscribers.  \\nWhen publishing and subscribing \", \"to native events from Azure resources, no coding is required. With \\nsimple configuration, you can in\", \"tegrate events from one Azure resource to another leveraging built -in \\nplumbing for Topics and Subs\", \"criptions. Figure 4 -17 show s the anatomy of Event Grid.  \\n \\nFigure 4 -17. Event Grid anatomy  \\n \\n7\", \"8 CHAPTER 4 | Cloud -native communication patterns  \\n A major difference between EventGrid and Servi\", \"ce Bus is the underlying message exchange pattern . \\nService Bus implements an older style pull mode\", \"l  in which the downstream subscriber actively polls \\nthe topic subscription for new messages. On th\", \"e upside, this approach gives the subscriber full control \\nof the pace at which it processes message\", \"s. It controls when and how many messages to process at \\nany given time. Unread messages remain in t\", \"he subscription until processed. A significant \\nshortcoming is the latency between the time the even\", \"t is generated and the polling operation that \\npulls that message to th e subscriber for processing.\", \" Also, the overhead of constant polling for the \\nnext event consumes resources and money.  \\nEventGri\", \"d, however, is different. It implements a push model  in which events are sent to the \\nEventHandlers\", \" as received, giving near real -time event delivery. It also reduces cost as the service is \\ntrigger\", \"ed only when it\\u2019s needed to consume an event \\u2013 not continually as with polling. That said, an \\nevent\", \" handler must  handle the incoming load and provide throttling mechanisms to protect itself \\nfrom be\", \"coming overwhelmed. Many Azure services that consume these events, such as Azure \\nFunctions and Logi\", \"c Apps provi de automatic autoscaling capabilities to handle increased loads.  \\nEvent Grid is a full\", \"y managed serverless cloud service. It dynamically scales based on your traffic and \\ncharges you onl\", \"y for your actual usage, not pre -purchased capacity. The first 100,000 operations per \\nmonth are fr\", \"ee \\u2013 operations being defined as event ingress (incoming event notifications), \\nsubscription deliver\", \"y attempts, management calls, and filtering by subject. With 99.99% availability, \\nEventGrid guarant\", \"ees the delivery of an event within a 24 -hour period, with built -in retry functionality \\nfor unsuc\", \" cessful delivery. Undelivered messages can be moved to a \\u201cdead -letter\\u201d queue for resolution. \\nUnli\", \"ke Azure Service Bus, Event Grid is tuned for fast performance and doesn\\u2019t support features like \\nor\", \"dered messaging, transactions, and sessions.  \\nStreaming messages in the Azure cloud  \\nAzure Service\", \" Bus and Event Grid provide great support for applications that expose single, discrete \\nevents like\", \" a new document has been inserted into a Cosmos DB. But, what if your cloud -native \\nsystem needs to\", \" process a stream of related events ? Event streams  are more complex. They\\u2019re typically \\ntime-order\", \"ed, interrelated, and must be processed as a group.  \\nAzure Event Hub  is a data streaming platform \", \"and event ingestion service that collects, transforms, \\nand stores events. It\\u2019s fine -tuned to captu\", \"re streaming data, such as continuous event notifications \\nemitted from a telemetry context. The ser\", \"vice is highly scalable and c an store and process millions of \\nevents per second . Shown in Figure \", \"4 -18, it\\u2019s often a front door for an event pipeline, decoupling \\ningest stream from event consumpti\", \"on.   \\n79 CHAPTER 4 | Cloud -native communication patterns  \\n  \\nFigure 4 -18. Azure Event Hub  \\nEven\", \"t Hub supports low latency and configurable time retention. Unlike queues and topics, Event \\nHubs ke\", \"ep event data after it\\u2019s been read by a consumer. This feature enables other data analytic \\nservices\", \", both internal and external, to replay the data for f urther analysis. Events stored in event hub \\n\", \"are only deleted upon expiration of the retention period, which is one day by default, but \\nconfigur\", \"able.  \\nEvent Hub supports common event publishing protocols including HTTPS and AMQP. It also suppo\", \"rts \\nKafka 1.0. Existing Kafka applications can communicate with Event Hub  using the Kafka protocol\", \" \\nproviding an alternative to managing large Kafka clusters. Many open -source cloud -native systems\", \" \\nembrace Kafka.  \\nEvent Hubs implements message streaming through a partitioned consumer model  in \", \"which each \\nconsumer only reads a specific subset, or partition, of the message stream. This pattern\", \" enables \\ntremendous horizontal scale for event processing and provides other stream -focused featur\", \"es that are \\nunavailable in queues and topics. A partiti on is an ordered sequence of events that is\", \" held in an event \\nhub. As newer events arrive, they\\u2019re added to the end of this sequence.  Figure 4\", \" -19 shows partitioning \\nin an Event Hub.  \\n \\nFigure 4 -19. Event Hub partitioning  \\nInstead of read\", \"ing from the same resource, each consumer group reads across a subset, or partition, \\nof the message\", \" stream.  \\n \\n80 CHAPTER 4 | Cloud -native communication patterns  \\n For cloud -native applications t\", \"hat must stream large numbers of events, Azure Event Hub can be a \\nrobust and affordable solution.  \", \"\\ngRPC  \\nSo far in this book, we\\u2019ve focused on REST -based  communication. We\\u2019ve seen that REST is a \", \"flexible \\narchitectural style that defines CRUD -based operations against entity resources. Clients \", \"interact with \\nresources across HTTP with a request/response communication model. While REST is wide\", \"ly \\nimplemented, a  newer communication technology, gRPC, has gained tremendous momentum across \\nthe\", \" cloud -native community.  \\nWhat is gRPC?  \\ngRPC is a modern, high -performance framework that evolv\", \"es the age -old remote procedure call (RPC)  \\nprotocol. At the application level, gRPC streamlines m\", \"essaging between clients and back -end services. \\nOriginating from Google, gRPC is open source and p\", \"art of the Cloud Native Computing Foundation \\n(CNCF)  ecosystem of cloud -native offerings. CNCF con\", \"siders gRPC an incubating project . Incubating \\nmeans end users are using the technology in producti\", \"on applications, and the project has a healthy \\nnumber of contributors.  \\nA typical gRPC client app \", \"will expose a local, in -process function that implements a business \\noperation. Under the covers, t\", \"hat local function invokes another function on a remote machine. What \\nappears to be a local call es\", \"sentially becomes a transparent o ut-of-process call to a remote service. \\nThe RPC plumbing abstract\", \"s the point -to-point networking communication, serialization, and \\nexecution between computers.  \\nI\", \"n cloud -native applications, developers often work across programming languages, frameworks, and \\nt\", \"echnologies. This interoperability  complicates message contracts and the plumbing required for \\ncro\", \"ss -platform communication. gRPC provides a \\u201cuniform horizontal layer\\u201d that abstracts these \\nconcern\", \"s. Developers code in their native platform focused on business functionality, while gRPC \\nhand les \", \"communication plumbing.  \\ngRPC offers comprehensive support across most popular development stacks, \", \"including Java, \\nJavaScript, C#, Go, Swift, and NodeJS.  \\ngRPC Benefits  \\ngRPC uses HTTP/2 for its t\", \"ransport protocol. While compatible with HTTP 1.1, HTTP/2 features many \\nadvanced capabilities:  \\n\\u2022 \", \"A binary framing protocol for data transport - unlike HTTP 1.1, which is text based.  \\n\\u2022 Multiplexin\", \"g support for sending multiple parallel requests over the same connection - HTTP \\n1.1 limits process\", \"ing to one request/response message at a time.  \\n\\u2022 Bidirectional full -duplex communication for send\", \"ing both client requests and server responses \\nsimultaneously.  \\n\\u2022 Built-in streaming enabling reque\", \"sts and responses to asynchronously stream large data sets.  \\n\\u2022 Header compression that reduces netw\", \"ork usage.   \\n81 CHAPTER 4 | Cloud -native communication patterns  \\n gRPC is lightweight and highly \", \"performant. It can be up to 8x faster than JSON serialization with \\nmessages 60 -80% smaller. In Mic\", \"rosoft Windows Communication Foundation (WCF)  parlance, gRPC \\nperformance exceeds the speed and eff\", \"iciency of the highly optimized NetTCP bindings . Unlike \\nNetTCP, which favors the Microsoft stack, \", \"gRPC is cross -platform.  \\nProtocol Buffers  \\ngRPC embraces an open -source technology called Protoc\", \"ol Buffers . They provide a highly efficient \\nand platform -neutral serialization format for seriali\", \"zing structured messages that services send to \\neach other. Using a cross -platform Interface Defini\", \"tion Language (IDL), developers define a service \\ncontract for each micr oservice. The contract, imp\", \"lemented as a text -based .proto file, describes the \\nmethods, inputs, and outputs for each service.\", \" The same contract file can be used for gRPC clients and \\nservices built on different development pl\", \"atforms.  \\nUsing the proto file, the Protobuf compiler, protoc, generates both client and service co\", \"de for your \\ntarget platform. The code includes the following components:  \\n\\u2022 Strongly typed objects\", \", shared by the client and service, that represent the service operations \\nand data elements for a m\", \"essage.  \\n\\u2022 A strongly typed base class with the required network plumbing that the remote gRPC serv\", \"ice \\ncan inherit and extend.  \\n\\u2022 A client stub that contains the required plumbing to invoke the rem\", \"ote gRPC service.  \\nAt run time, each message is serialized as a standard Protobuf representation an\", \"d exchanged between \\nthe client and remote service. Unlike JSON or XML, Protobuf messages are serial\", \"ized as compiled \\nbinary bytes.  \\nThe book, gRPC for WCF Developers , available from the Microsoft A\", \"rchitecture site, provides in -depth \\ncoverage of gRPC and Protocol Buffers.  \\ngRPC support in .NET \", \" \\ngRPC is integrated into .NET Core 3.0 SDK and later. The following tools support it:  \\n\\u2022 Visual St\", \"udio 2022 with the ASP.NET and web development workload installed  \\n\\u2022 Visual Studio Code  \\n\\u2022 The dot\", \"net CLI  \\nThe SDK includes tooling for endpoint routing, built -in IoC, and logging. The open -sourc\", \"e Kestrel web \\nserver supports HTTP/2 connections. Figure 4 -20 shows a Visual Studio 2022 template \", \"that scaffolds a \\nskeleton project for a gRPC service. Note how .NET fu lly supports Windows, Linux,\", \" and macOS.   \\n82 CHAPTER 4 | Cloud -native communication patterns  \\n  \\nFigure 4 -20. gRPC support i\", \"n Visual Studio 2022  \\nFigure 4 -21 shows the skeleton gRPC service generated from the built -in sca\", \"ffolding included in \\nVisual Studio 2022.  \\n \\nFigure 4 -21. gRPC project in Visual Studio 2022  \\nIn \", \"the previous figure, note the proto description file and service code. As you\\u2019ll see shortly, Visual\", \" \\nStudio generates additional configuration in both the Startup class and underlying project file.  \", \"\\ngRPC usage  \\nFavor gRPC for the following scenarios:  \\n\\u2022 Synchronous backend microservice -to-micro\", \"service communication where an immediate \\nresponse is required to continue processing.  \\n\\u2022 Polyglot \", \"environments that need to support mixed programming platforms.  \\n\\u2022 Low latency and high throughput c\", \"ommunication where performance is critical.  \\n\\u2022 Point -to-point real -time communication - gRPC can \", \"push messages in real time without \\npolling and has excellent support for bi -directional streaming.\", \"  \\n \\n83 CHAPTER 4 | Cloud -native communication patterns  \\n \\u2022 Network constrained environments \\u2013 bin\", \"ary gRPC messages are always smaller than an \\nequivalent text -based JSON message.  \\nAt the time, of\", \" this writing, gRPC is primarily used with backend services. Modern browsers can\\u2019t \\nprovide the leve\", \"l of HTTP/2 control required to support a front -end gRPC client. That said, there\\u2019s \\nsupport for gR\", \"PC -Web with .NET  that enables gRPC communication from browser -based apps built \\nwith JavaScript o\", \"r Blazor WebAssembly technologies. gRPC -Web enables an ASP.NET Core gRPC app \\nto support gRPC featu\", \"res in browser apps:  \\n\\u2022 Strongly typed, code -generated clients  \\n\\u2022 Compact Protobuf messages  \\n\\u2022 S\", \"erver streaming  \\ngRPC implementation  \\nThe microservice reference architecture, eShop on Containers\", \" , from Microsoft, shows how to \\nimplement gRPC services in .NET applications. Figure 4 -22 presents\", \" the back -end architecture.  \\n \\nFigure 4 -22. Backend architecture for eShop on Containers  \\nIn the\", \" previous figure, note how eShop embraces the Backend for Frontends pattern  (BFF) by \\nexposing mult\", \"iple API gateways. We discussed the BFF pattern earlier in this chapter. Pay close \\n \\n84 CHAPTER 4 |\", \" Cloud -native communication patterns  \\n attention to the Aggregator microservice (in gray) that sit\", \"s between the Web -Shopping API Gateway \\nand backend Shopping microservices. The Aggregator receives\", \" a single request from a client, \\ndispatches it to various microservices, aggregates the results, an\", \" d sends them back to the requesting \\nclient. Such operations typically require synchronous communic\", \"ation as to produce an immediate \\nresponse. In eShop, backend calls from the Aggregator are performe\", \"d using gRPC as shown in Figure \\n4-23. \\n \\nFigure 4 -23. gRPC in eShop on Containers  \\ngRPC communica\", \"tion requires both client and server components. In the previous figure, note how \\nthe Shopping Aggr\", \"egator implements a gRPC client. The client makes synchronous gRPC calls (in red) \\nto backend micros\", \"ervices, each of which implement a gRPC serv er. Both the client and server take \\nadvantage of the b\", \"uilt -in gRPC plumbing from the .NET SDK. Client -side stubs  provide the plumbing \\nto invoke remote\", \" gRPC calls. Server -side components provide gRPC plumbing that custom service \\nclasses can inherit \", \"and consume.  \\nMicroservices that expose both a RESTful API and gRPC communication require multiple \", \"endpoints to \\nmanage traffic. You would open an endpoint that listens for HTTP traffic for the RESTf\", \"ul calls and \\nanother for gRPC calls. The gRPC endpoint must be configure d for the HTTP/2 protocol \", \"that is \\nrequired for gRPC communication.  \\nWhile we strive to decouple microservices with asynchron\", \"ous communication patterns, some \\noperations require direct calls. gRPC should be the primary choice\", \" for direct synchronous \\ncommunication between microservices. Its high -performance communication pr\", \"oto col, based on \\nHTTP/2 and protocol buffers, make it a perfect choice.  \\n \\n85 CHAPTER 4 | Cloud -\", \"native communication patterns  \\n Looking ahead  \\nLooking ahead, gRPC will continue to gain traction \", \"for cloud -native systems. The performance \\nbenefits and ease of development are compelling. However\", \", REST will likely be around for a long time. \\nIt excels for publicly exposed APIs and for backward \", \"compatib ility reasons.  \\nService Mesh communication infrastructure  \\nThroughout this chapter, we\\u2019ve\", \" explored the challenges of microservice communication. We said that \\ndevelopment teams need to be s\", \"ensitive to how back -end services communicate with each other. \\nIdeally, the less inter -service co\", \"mmunication, the better. However, avoidance isn\\u2019t always possible as \\nback -end services often rely \", \"on one another to complete operations.  \\nWe explored different approaches for implementing synchrono\", \"us HTTP communication and \\nasynchronous messaging. In each of the cases, the developer is burdened w\", \"ith implementing \\ncommunication code. Communication code is complex and time intensive. Incorrect d \", \"ecisions can \\nlead to significant performance issues.  \\nA more modern approach to microservice commu\", \"nication centers around a new and rapidly evolving \\ntechnology entitled Service Mesh . A service mes\", \"h  is a configurable infrastructure layer with built -in \\ncapabilities to handle service -to-service\", \" communication, resiliency, and many cross -cutting concerns. \\nIt moves the responsibility for these\", \" concerns out of the microservices and into service mesh layer.  \\nCommunication is abstracted away f\", \"rom your microservices.  \\nA key component of a service mesh is a proxy. In a cloud -native applicati\", \"on, an instance of a proxy is \\ntypically colocated with each microservice. While they execute in sep\", \"arate processes, the two are \\nclosely linked and share the same lifecycle. This patte rn, known as t\", \"he Sidecar pattern , and is shown in \\nFigure 4 -24. \\n \\nFigure 4 -24. Service mesh with a side car  \\n\", \" \\n86 CHAPTER 4 | Cloud -native communication patterns  \\n Note in the previous figure how messages ar\", \"e intercepted by a proxy that runs alongside each \\nmicroservice. Each proxy can be configured with t\", \"raffic rules specific to the microservice. It \\nunderstands messages and can route them across your s\", \"ervices and the  outside world.  \\nAlong with managing service -to-service communication, the Service\", \" Mesh provides support for \\nservice discovery and load balancing.  \\nOnce configured, a service mesh \", \"is highly functional. The mesh retrieves a corresponding pool of \\ninstances from a service discovery\", \" endpoint. It sends a request to a specific service instance, recording \\nthe latency and response ty\", \"pe of the result. It choos es the instance most likely to return a fast \\nresponse based on different\", \" factors, including the observed latency for recent requests.  \\nA service mesh manages traffic, comm\", \"unication, and networking concerns at the application level. It \\nunderstands messages and requests. \", \"A service mesh typically integrates with a container orchestrator. \\nKubernetes supports an extensibl\", \"e architecture in whic h a service mesh can be added.  \\nIn chapter 6, we deep -dive into Service Mes\", \"h technologies including a discussion on its architecture \\nand available open -source implementation\", \"s.  \\nSummary  \\nIn this chapter, we discussed cloud -native communication patterns. We started by exa\", \"mining how \\nfront -end clients communicate with back -end microservices. Along the way, we talked ab\", \"out API \\nGateway platforms and real -time communication. We then looked at ho w microservices commun\", \"icate \\nwith other back -end services. We looked at both synchronous HTTP communication and \\nasynchro\", \"nous messaging across services. We covered gRPC, an upcoming technology in the cloud -\\nnative world.\", \" Finally, we introduced a new and rapid ly evolving technology entitled Service Mesh that \\ncan strea\", \"mline microservice communication.  \\nSpecial emphasis was on managed Azure services that can help imp\", \"lement communication in cloud -\\nnative systems:  \\n\\u2022 Azure Application Gateway  \\n\\u2022 Azure API Manageme\", \"nt  \\n\\u2022 Azure SignalR Service  \\n\\u2022 Azure Storage Queues  \\n\\u2022 Azure Service Bus  \\n\\u2022 Azure Event Grid  \\n\\u2022\", \" Azure Event Hub  \\nWe next move to distributed data in cloud -native systems and the benefits and ch\", \"allenges that it \\npresents.  \\nReferences  \\n\\u2022 .NET Microservices: Architecture for Containerized .NET\", \" applications  \\n\\u2022 Designing Interservice Communication for Microservices  \\n\\u2022 Azure SignalR Service, \", \"a fully managed service to add real -time functionality   \\n87 CHAPTER 4 | Cloud -native communicatio\", \"n patterns  \\n \\u2022 Azure API Gateway Ingress Controller  \\n\\u2022 gRPC Documentation  \\n\\u2022 gRPC for WCF Develop\", \"ers  \\n\\u2022 Comparing gRPC Services with HTTP APIs  \\n\\u2022 Building gRPC Services with .NET video   \\n88 CHAP\", \"TER 5 | Cloud -native data patterns  \\n CHAPTER  5 \\nCloud -native data patterns  \\nAs we\\u2019ve seen throu\", \"ghout this book, a cloud -native approach changes the way you design, deploy, \\nand manage applicatio\", \"ns. It also changes the way you manage and store data.  \\nFigure 5 -1 contrasts the differences.  \\n \\n\", \"Figure 5 -1. Data management in cloud -native applications  \\nExperienced developers will easily reco\", \"gnize the architecture on the left -side of figure 5 -1. In this \\nmonolithic application , business \", \"service components collocate together in a shared services tier, \\nsharing data from a single relatio\", \"nal database.  \\nIn many ways, a single database keeps data management simple. Querying data across m\", \"ultiple tables \\nis straightforward. Changes to data update together or they all rollback. ACID trans\", \"actions  guarantee \\nstrong and immediate consistency.  \\nDesigning for cloud -native, we take a diffe\", \"rent approach. On the right -side of Figure 5 -1, note how \\nbusiness functionality segregates into s\", \"mall, independent microservices. Each microservice \\nencapsulates a specific business capability and \", \"its own data. The  monolithic database decomposes \\n \\n89 CHAPTER 5 | Cloud -native data patterns  \\n i\", \"nto a distributed data model with many smaller databases, each aligning with a microservice. When \\nt\", \"he smoke clears, we emerge with a design that exposes a database per microservice . \\nDatabase -per-m\", \"icroservice, why?  \\nThis database per microservice provides many benefits, especially for systems th\", \"at must evolve rapidly \\nand support massive scale. With this model\\u2026  \\n\\u2022 Domain data is encapsulated \", \"within the service  \\n\\u2022 Data schema can evolve without directly impacting other services  \\n\\u2022 Each dat\", \"a store can independently scale  \\n\\u2022 A data store failure in one service won\\u2019t directly impact other \", \"services  \\nSegregating data also enables each microservice to implement the data store type that is \", \"best \\noptimized for its workload, storage needs, and read/write patterns. Choices include relational\", \", \\ndocument, key -value, and even graph -based data stores.  \\nFigure 5 -2 presents the principle of \", \"polyglot persistence in a cloud -native system.  \\n \\nFigure 5 -2. Polyglot data persistence  \\nNote in\", \" the previous figure how each microservice supports a different type of data store.  \\n\\u2022 The product \", \"catalog microservice consumes a relational database to accommodate the rich \\nrelational structure of\", \" its underlying data.  \\n\\u2022 The shopping cart microservice consumes a distributed cache that supports \", \"its simple, key -\\nvalue data store.  \\n\\u2022 The ordering microservice consumes both a NoSql document dat\", \"abase for write operations \\nalong with a highly denormalized key/value store to accommodate high -vo\", \"lumes of read \\noperations.  \\n \\n90 CHAPTER 5 | Cloud -native data patterns  \\n While relational databa\", \"ses remain relevant for microservices with complex data, NoSQL databases \\nhave gained considerable p\", \"opularity. They provide massive scale and high availability. Their \\nschemaless nature allows develop\", \"ers to move away from an architectu re of typed data classes and \\nORMs that make change expensive an\", \"d time -consuming. We cover NoSQL databases later in this \\nchapter.  \\nWhile encapsulating data into \", \"separate microservices can increase agility, performance, and scalability, \\nit also presents many ch\", \"allenges. In the next section, we discuss these challenges along with patterns \\nand practices to hel\", \"p overcome them.  \\nCross -service queries  \\nWhile microservices are independent and focus on specifi\", \"c functional capabilities, like inventory, \\nshipping, or ordering, they frequently require integrati\", \"on with other microservices. Often the \\nintegration involves one microservice querying  another for \", \"data. Figure 5 -3 shows the scenario.  \\n \\nFigure 5 -3. Querying across microservices  \\nIn the preced\", \"ing figure, we see a shopping basket microservice that adds an item to a user\\u2019s shopping \\nbasket. Wh\", \"ile the data store for this microservice contains basket and line item data, it doesn\\u2019t \\nmaintain pr\", \"oduct or pricing data. Instead, those data it ems are owned by the catalog and pricing \\nmicroservice\", \"s. This aspect presents a problem. How can the shopping basket microservice add a \\nproduct to the us\", \"er\\u2019s shopping basket when it doesn\\u2019t have product nor pricing data in its database?  \\nOne option dis\", \"cussed in Chapter 4 is a direct HTTP call  from the shopping basket to the catalog and \\npricing micr\", \"oservices. However, in chapter 4, we said synchronous HTTP calls couple  microservices \\ntogether, re\", \"ducing their autonomy and diminishing their architectural benefits.  \\nWe could also implement a requ\", \"est -reply pattern with separate inbound and outbound queues for \\neach service. However, this patter\", \"n is complicated and requires plumbing to correlate request and \\nresponse messages. While it does de\", \"couple the backend microservice calls, the calling service must \\nstill synchronously wait for the ca\", \"ll to complete. Network congestion, transient faults, or an \\noverloaded microservice and can result \", \"in long -running and even failed operations.  \\n \\n91 CHAPTER 5 | Cloud -native data patterns  \\n Inste\", \"ad, a widely accepted pattern for removing cross -service dependencies is the Materialized View \\nPat\", \"tern , shown in Figure 5 -4. \\n \\nFigure 5 -4. Materialized View Pattern  \\nWith this pattern, you plac\", \"e a local data table (known as a read model ) in the shopping basket service. \\nThis table contains a\", \" denormalized copy of the data needed from the product and pricing \\nmicroservices. Copying the data \", \"directly into the shopping basket microservice eliminates the need for \\nexpensive cross -service cal\", \"ls. With the data local to the service, you improve the service\\u2019s response \\ntime and reliability. Ad\", \"ditionally, having its own copy of the data makes the shopping basket service \\nmore resilien t. If t\", \"he catalog service should become unavailable, it wouldn\\u2019t directly impact the \\nshopping basket servi\", \"ce. The shopping basket can continue operating with the data from its own \\nstore.  \\nThe catch with t\", \"his approach is that you now have duplicate data in your system. However, \\nstrategically  duplicatin\", \"g data in cloud -native systems is an established practice and not considered an \\nanti-pattern, or b\", \"ad practice. Keep in mind that one and only one service  can own a data set and have \\nauthority over\", \" it. You\\u2019ll need to synchronize the read models when the system of record is updated. \\nSynchronizati\", \"on is typically implemented via asynchronous messaging with a publish/subscribe \\npattern , as shown \", \"in Figure 5.4.  \\nDistributed transactions  \\nWhile querying data across microservices is difficult, i\", \"mplementing a transaction across several \\nmicroservices is even more complex. The inherent challenge\", \" of maintaining data consistency across \\nindependent data sources in different microservices can\\u2019t b\", \"e understated. The lack of distributed \\ntransactions in cloud -native applications means that you mu\", \"st manage distributed transactions \\nprogrammatically. You move from a world of immediate consistency\", \"  to that of eventual consistency . \\nFigure 5 -5 shows the problem.  \\n \\n92 CHAPTER 5 | Cloud -native\", \" data patterns  \\n  \\nFigure 5 -5. Implementing a transaction across microservices  \\nIn the preceding \", \"figure, five independent microservices participate in a distributed transaction that \\ncreates an ord\", \"er. Each microservice maintains its own data store and implements a local transaction \\nfor its store\", \". To create the order, the local transact ion for each individual microservice must succeed, \\nor all\", \" must abort and roll back the operation. While built -in transactional support is available inside \\n\", \"each of the microservices, there\\u2019s no support for a distributed transaction that would span across a\", \"l l \\nfive services to keep data consistent.  \\nInstead, you must construct this distributed transacti\", \"on programmatically . \\nA popular pattern for adding distributed transactional support is the Saga pa\", \"ttern. It\\u2019s implemented \\nby grouping local transactions together programmatically and sequentially i\", \"nvoking each one. If any \\nof the local transactions fail, the Saga aborts the ope ration and invokes\", \" a set of compensating \\ntransactions . The compensating transactions undo the changes made by the pr\", \"eceding local \\ntransactions and restore data consistency. Figure 5 -6 shows a failed transaction wit\", \"h the Saga pattern.  \\n \\nFigure 5 -6. Rolling back a transaction  \\n \\n93 CHAPTER 5 | Cloud -native dat\", \"a patterns  \\n In the previous figure, the Update Inventory  operation has failed in the Inventory mi\", \"croservice. The \\nSaga invokes a set of compensating transactions (in red) to adjust the inventory co\", \"unts, cancel the \\npayment and the order, and return the data for each microservice back to a consist\", \"ent state.  \\nSaga patterns are typically choreographed as a series of related events, or orchestrate\", \"d as a set of \\nrelated commands. In Chapter 4, we discussed the service aggregator pattern that woul\", \"d be the \\nfoundation for an orchestrated saga implementation. We also discus sed eventing along with\", \" Azure \\nService Bus and Azure Event Grid topics that would be a foundation for a choreographed saga \", \"\\nimplementation.  \\nHigh volume data  \\nLarge cloud -native applications often support high -volume da\", \"ta requirements. In these scenarios, \\ntraditional data storage techniques can cause bottlenecks. For\", \" complex systems that deploy on a large \\nscale, both Command and Query Responsibility Segregation (C\", \"QRS) and Event Sourcing may improve \\napplication performance.  \\nCQRS  \\nCQRS , is an architectural pa\", \"ttern that can help maximize performance, scalability, and security. The \\npattern separates operatio\", \"ns that read data from those operations that write data.  \\nFor normal scenarios, the same entity mod\", \"el and data repository object are used for both read and \\nwrite operations.  \\nHowever, a high volume\", \" data scenario can benefit from separate models and data tables for reads \\nand writes. To improve pe\", \"rformance, the read operation could query against a highly denormalized \\nrepresentation of the data \", \"to avoid expensive repetitive table joins and table locks. The write  \\noperation, known as a command\", \" , would update against a fully normalized representation of the data \\nthat would guarantee consiste\", \"ncy. You then need to implement a mechanism to keep both \\nrepresentations in sync. Typically, whe ne\", \"ver the write table is modified, it publishes an event that \\nreplicates the modification to the read\", \" table.  \\nFigure 5 -7 shows an implementation of the CQRS pattern.  \\n \\nFigure 5 -7. CQRS implementat\", \"ion  \\n \\n94 CHAPTER 5 | Cloud -native data patterns  \\n In the previous figure, separate command and q\", \"uery models are implemented. Each data write \\noperation is saved to the write store and then propaga\", \"ted to the read store. Pay close attention to \\nhow the data propagation process operates on the prin\", \"ciple of eventual consistency . The read model \\neventually synchronizes with the write model, but th\", \"ere may be some lag in the process. We discuss \\neventual consistency in the next section.  \\nThis sep\", \"aration enables reads and writes to scale independently. Read operations use a schema \\noptimized for\", \" queries, while the writes use a schema optimized for updates. Read queries go against \\ndenormalized\", \" data, while complex business logic can be applied to the write model. As well, you \\nmight impose ti\", \"ghter security on write operations than those exposing reads.  \\nImplementing CQRS can improve applic\", \"ation performance for cloud -native services. However, it does \\nresult in a more complex design. App\", \"ly this principle carefully and strategically to those sections of \\nyour cloud -native application t\", \"hat will benefit from it . For more on CQRS, see the Microsoft book .NET \\nMicroservices: Architectur\", \"e for Containerized .NET Applications . \\nEvent sourcing  \\nAnother approach to optimizing high volume\", \" data scenarios involves Event Sourcing . \\nA system typically stores the current state of a data ent\", \"ity. If a user changes their phone number, for \\nexample, the customer record is updated with the new\", \" number. We always know the current state of a \\ndata entity, but each update overwrites the previous\", \" state.  \\nIn most cases, this model works fine. In high volume systems, however, overhead from trans\", \"actional \\nlocking and frequent update operations can impact database performance, responsiveness, an\", \"d limit \\nscalability.  \\nEvent Sourcing takes a different approach to capturing data. Each operation \", \"that affects data is \\npersisted to an event store. Instead of updating the state of a data record, w\", \"e append each change to \\na sequential list of past events - similar to an accounta nt\\u2019s ledger. The \", \"Event Store becomes the \\nsystem of record for the data. It\\u2019s used to propagate various materialized \", \"views within the bounded \\ncontext of a microservice. Figure 5.8 shows the pattern.   \\n95 CHAPTER 5 |\", \" Cloud -native data patterns  \\n  \\nFigure 5 -8. Event Sourcing  \\nIn the previous figure, note how eac\", \"h entry (in blue) for a user\\u2019s shopping cart is appended to an \\nunderlying event store. In the adjoi\", \"ning materialized view, the system projects the current state by \\nreplaying all the events associate\", \"d with each shopping cart. This view, or read model, is then exposed \\nback to the UI. Events can als\", \"o be in tegrated with external systems and applications or queried to \\ndetermine the current state o\", \"f an entity. With this approach, you maintain history. You know not only \\nthe current state of an en\", \"tity, but also how you reached this state.  \\nMechanically speaking, event sourcing simplifies the wr\", \"ite model. There are no updates or deletes. \\nAppending each data entry as an immutable event minimiz\", \"es contention, locking, and concurrency \\nconflicts associated with relational databases. Building re\", \"ad models with the materialized view pattern \\nenables you to decouple the view from the write model \", \"and choose the best data store to optimize \\nthe needs of your application UI.  \\nFor this pattern, co\", \"nsider a data store that directly supports event sourcing. Azure Cosmos DB, \\nMongoDB, Cassandra, Cou\", \"chDB, and RavenDB are good candidates.  \\nAs with all patterns and technologies, implement strategica\", \"lly and when needed. While event sourcing \\ncan provide increased performance and scalability, it com\", \"es at the expense of complexity and a \\nlearning curve.  \\n \\n96 CHAPTER 5 | Cloud -native data pattern\", \"s  \\n Relational vs.  NoSQL data  \\nRelational and NoSQL are two types of database systems commonly im\", \"plemented in cloud -native \\napps. They\\u2019re built differently, store data differently, and accessed di\", \"fferently. In this section, we\\u2019ll look \\nat both. Later in this chapter, we\\u2019ll look at an emer ging d\", \"atabase technology called NewSQL . \\nRelational databases  have been a prevalent technology for decad\", \"es. They\\u2019re mature, proven, and \\nwidely implemented. Competing database products, tooling, and exper\", \"tise abound. Relational \\ndatabases provide a store of related data tables. These tables have a fixed\", \" schema, use SQ L \\n(Structured Query Language) to manage data, and support ACID guarantees.  \\nNo-SQL\", \" databases  refer to high -performance, non -relational data stores. They excel in their ease -of-\\nu\", \"se, scalability, resilience, and availability characteristics. Instead of joining tables of normaliz\", \"ed data, \\nNoSQL stores unstructured or semi -structured data, often in ke y-value pairs or JSON docu\", \"ments. No -\\nSQL databases typically don\\u2019t provide ACID guarantees beyond the scope of a single datab\", \"ase \\npartition. High volume services that require sub second response time favor NoSQL datastores.  \", \"\\nThe impact of NoSQL  technologies for distributed cloud -native systems can\\u2019t be overstated. The \\np\", \"roliferation of new data technologies in this space has disrupted solutions that once exclusively \\nr\", \"elied on relational databases.  \\nNoSQL databases include several different models for accessing and \", \"managing data, each suited to \\nspecific use cases. Figure 5 -9 presents four common models.  \\n \\nFigu\", \"re 5 -9: Data models for NoSQL databases  \\nModel  Characteristics  \\nDocument Store  Data and metadat\", \"a are stored hierarchically in \\nJSON -based documents inside the database.  \\nKey Value Store  The si\", \"mplest of the NoSQL databases, data is \\nrepresented as a collection of key -value pairs.  \\nWide -Col\", \"umn Store  Related data is stored as a set of nested -\\nkey/value pairs within a single column.  \\nGra\", \"ph Store  Data is stored in a graph structure as node, \\nedge, and data properties.  \\n \\n97 CHAPTER 5 \", \"| Cloud -native data patterns  \\n The CAP theorem  \\nAs a way to understand the differences between th\", \"ese types of databases, consider the CAP theorem, \\na set of principles applied to distributed system\", \"s that store state. Figure 5 -10 shows the three \\nproperties of the CAP theorem.  \\n \\nFigure 5 -10. T\", \"he CAP theorem  \\nThe theorem states that distributed data systems will offer a trade -off between co\", \"nsistency, \\navailability, and partition tolerance. And, that any database can only guarantee two of \", \"the three \\nproperties:  \\n\\u2022 Consistency.  Every node in the cluster responds with the most recent dat\", \"a, even if the system \\nmust block the request until all replicas update. If you query a \\u201cconsistent \", \"system\\u201d for an item \\nthat is currently updating, you\\u2019ll wait for that response until all replicas su\", \"ccessfully update. \\nHowever, you\\u2019ll receive the most current data.  \\n\\u2022 Availability.  Every node ret\", \"urns an immediate response, even if that response isn\\u2019t the most \\nrecent data. If you query an \\u201cavai\", \"lable system\\u201d for an item that is updating, you\\u2019ll get the best \\npossible answer the service can pro\", \"vide at that moment.  \\n\\u2022 Partition Tolerance.  Guarantees the system continues to operate even if a \", \"replicated data \\nnode fails or loses connectivity with other replicated data nodes.  \\nCAP theorem ex\", \"plains the tradeoffs associated with managing consistency and availability during a \\nnetwork partiti\", \"on; however tradeoffs with respect to consistency and performance also exist with the \\nabsence of a \", \"network partition. CAP theorem is often furt her extended to PACELC  to explain the \\ntradeoffs more \", \"comprehensively.  \\nRelational databases typically provide consistency and availability, but not part\", \"ition tolerance. They\\u2019re \\ntypically provisioned to a single server and scale vertically by adding mo\", \"re resources to the machine.  \\n \\n98 CHAPTER 5 | Cloud -native data patterns  \\n Many relational datab\", \"ase systems support built -in replication features where copies of the primary \\ndatabase can be made\", \" to other secondary server instances. Write operations are made to the primary \\ninstance and replica\", \"ted to each of the secondaries. Upon a failure, the primary instance can fail over \\nto a secondary t\", \"o provide high availability. Secondaries can also be used to distribute read operations. \\nWhile writ\", \"es operations always go against the primary replica , read operations can be routed to any of \\nthe s\", \"econdaries to reduce system load.  \\nData can also be horizontally partitioned across multiple nodes,\", \" such as with sharding . But, sharding \\ndramatically increases operational overhead by spitting data\", \" across many pieces that cannot easily \\ncommunicate. It can be costly and time consuming to manage. \", \"Relational features that include table \\njoins, transactions, and referential integ rity require stee\", \"p performance penalties in sharded \\ndeployments.  \\nReplication consistency and recovery point object\", \"ives can be tuned by configuring whether replication \\noccurs synchronously or asynchronously. If dat\", \"a replicas were to lose network connectivity in a \\u201chighly \\nconsistent\\u201d or synchronous relational dat\", \"abase clu ster, you wouldn\\u2019t be able to write to the database. \\nThe system would reject the write op\", \"eration as it can\\u2019t replicate that change to the other data replica. \\nEvery data replica has to upda\", \"te before the transaction can complete.  \\nNoSQL databases typically support high availability and pa\", \"rtition tolerance. They scale out \\nhorizontally, often across commodity servers. This approach provi\", \"des tremendous availability, both \\nwithin and across geographical regions at a reduced cost. You par\", \" tition and replicate data across \\nthese machines, or nodes, providing redundancy and fault toleranc\", \"e. Consistency is typically tuned \\nthrough consensus protocols or quorum mechanisms. They provide mo\", \"re control when navigating \\ntradeoffs between tuning synchro nous versus asynchronous replication in\", \" relational systems.  \\nIf data replicas were to lose connectivity in a \\u201chighly available\\u201d NoSQL data\", \"base cluster, you could still \\ncomplete a write operation to the database. The database cluster woul\", \"d allow the write operation and \\nupdate each data replica as it becomes available . NoSQL databases \", \"that support multiple writable \\nreplicas can further strengthen high availability by avoiding the ne\", \"ed for failover when optimizing \\nrecovery time objective.  \\nModern NoSQL databases typically impleme\", \"nt partitioning capabilities as a feature of their system \\ndesign. Partition management is often bui\", \"lt -in to the database, and routing is achieved through \\nplacement hints - often called partition ke\", \"ys. A flexible data  models enables the NoSQL databases to \\nlower the burden of schema management an\", \"d improve availability when deploying application \\nupdates that require data model changes.  \\nHigh a\", \"vailability and massive scalability are often more critical to the business than relational table \\nj\", \"oins and referential integrity. Developers can implement techniques and patterns such as Sagas, \\nCQR\", \"S, and asynchronous messaging to embrace eventual co nsistency.  \\nNowadays, care must be taken when \", \"considering the CAP theorem constraints. A new type of \\ndatabase, called NewSQL, has emerged which e\", \"xtends the relational database engine to support both \\nhorizontal scalability and the scalable perfo\", \"rmance of NoSQL systems.   \\n99 CHAPTER 5 | Cloud -native data patterns  \\n Considerations for relatio\", \"nal vs.  NoSQL systems  \\nBased upon specific data requirements, a cloud -native -based microservice \", \"can implement a relational, \\nNoSQL datastore or both.  \\nConsider a NoSQL datastore when:  Consider a\", \" relational database when:  \\nYou have high volume workloads that require \\npredictable latency at lar\", \"ge scale (for example, \\nlatency measured in milliseconds while \\nperforming millions of transactions \", \"per second)  Your workload volume generally fits within \\nthousands of transactions per second  \\nYour\", \" data is dynamic and frequently changes  Your data is highly structured and requires \\nreferential in\", \"tegrity  \\nRelationships can be de -normalized data \\nmodels  Relationships are expressed through tabl\", \"e joins \\non normalized data models  \\nData retrieval is simple and expressed without \\ntable joins  Yo\", \"u work with complex queries and reports  \\nData is typically replicated across geographies \\nand requi\", \"res finer control over consistency, \\navailability, and performance  Data is typically centralized, o\", \"r can be replicated \\nregions asynchronously  \\nYour application will be deployed to commodity \\nhardwa\", \"re, such as with public clouds  Your application will be deployed to large, high -\\nend hardware  \\nIn\", \" the next sections, we\\u2019ll explore the options available in the Azure cloud for storing and managing \", \"\\nyour cloud -native data.  \\nDatabase as a Service  \\nTo start, you could provision an Azure virtual m\", \"achine and install your database of choice for each \\nservice. While you\\u2019d have full control over the\", \" environment, you\\u2019d forgo many built -in features of the \\ncloud platform. You\\u2019d also be responsible \", \"for managin g the virtual machine and database for each \\nservice. This approach could quickly become\", \" time -consuming and expensive.  \\nInstead, cloud -native applications favor data services exposed as\", \" a Database as a Service (DBaaS). \\nFully managed by a cloud vendor, these services provide built -in\", \" security, scalability, and monitoring. \\nInstead of owning the service, you simply consume it as a b\", \"acking service . The provider operates the \\nresource at scale and bears the responsibility for perfo\", \"rmance and maintenance.  \\nThey can be configured across cloud availability zones and regions to achi\", \"eve high availability. They \\nall support just -in-time capacity and a pay -as-you-go model. Azure fe\", \"atures different kinds of \\nmanaged data service options, each with specific benefits.  \\nWe\\u2019ll first \", \"look at relational DBaaS services available in Azure. You\\u2019ll see that Microsoft\\u2019s flagship SQL \\nServ\", \"er database is available along with several open -source options. Then, we\\u2019ll talk about the NoSQL \\n\", \"data services in Azure.   \\n100 CHAPTER 5 | Cloud -native data patterns  \\n Azure relational databases\", \"  \\nFor cloud -native microservices that require relational data, Azure offers four managed relationa\", \"l \\ndatabases as a service (DBaaS) offerings, shown in Figure 5 -11. \\n \\nFigure 5 -11. Managed relatio\", \"nal databases available in Azure  \\nIn the previous figure, note how each sits upon a common DBaaS in\", \"frastructure which features key \\ncapabilities at no additional cost.  \\nThese features are especially\", \" important to organizations who provision large numbers of databases, \\nbut have limited resources to\", \" administer them. You can provision an Azure database in minutes by \\nselecting the amount of process\", \"ing cores, memory, and underlying storage. You c an scale the \\ndatabase on -the-fly and dynamically \", \"adjust resources with little to no downtime.  \\nAzure SQL Database  \\nDevelopment teams with expertise\", \" in Microsoft SQL Server should consider Azure SQL Database . It\\u2019s a \\nfully managed relational datab\", \"ase -as-a-service (DBaaS) based on the Microsoft SQL Server Database \\nEngine. The service shares man\", \"y features found in the on -premises version of SQL Server and runs the \\nlatest stable version of th\", \"e SQL Server Database  Engine.  \\nFor use with a cloud -native microservice, Azure SQL Database is av\", \"ailable with three deployment \\noptions:  \\n\\u2022 A Single Database  represents a fully managed SQL Databa\", \"se running on an Azure SQL \\nDatabase server  in the Azure cloud. The database is considered containe\", \"d  as it has no \\nconfiguration dependencies on the underlying database server.  \\n\\u2022 A Managed Instanc\", \"e  is a fully managed instance of the Microsoft SQL Server Database Engine \\nthat provides near -100%\", \" compatibility with an on -premises SQL Server. This option supports \\nlarger databases, up to 35 TB \", \"and is placed in an Azure Virtual Network  for better isolation.  \\n \\n101 CHAPTER 5 | Cloud -native d\", \"ata patterns  \\n \\u2022 Azure SQL Database serverless  is a compute tier for a single database that automa\", \"tically \\nscales based on workload demand. It bills only for the amount of compute used per second. \\n\", \"The service is well suited for workloads with intermittent, unpredictable usage patterns, \\nintersper\", \"sed wit h periods of inactivity. The serverless compute tier also automatically pauses \\ndatabases du\", \"ring inactive periods so that only storage charges are billed. It automatically \\nresumes when activi\", \"ty returns.  \\nBeyond the traditional Microsoft SQL Server stack, Azure also features managed version\", \"s of three \\npopular open -source databases.  \\nOpen -source databases in Azure  \\nOpen -source relatio\", \"nal databases have become a popular choice for cloud -native applications. Many \\nenterprises favor t\", \"hem over commercial database products, especially for cost savings. Many \\ndevelopment teams enjoy th\", \"eir flexibility, community -backed develo pment, and ecosystem of tools \\nand extensions. Open -sourc\", \"e databases can be deployed across multiple cloud providers, helping \\nminimize the concern of \\u201cvendo\", \"r lock -in.\\u201d \\nDevelopers can easily self -host any open -source database on an Azure VM. While provi\", \"ding full \\ncontrol, this approach puts you on the hook for the management, monitoring, and maintenan\", \"ce of \\nthe database and VM.  \\nHowever, Microsoft continues its commitment to keeping Azure an \\u201copen \", \"platform\\u201d by offering \\nseveral popular open -source databases as fully managed  DBaaS services.  \\nAz\", \"ure Database for MySQL  \\nMySQL  is an open -source  relational database and a pillar for application\", \"s built on the LAMP software \\nstack . Widely chosen for read heavy  workloads, it\\u2019s used by many lar\", \"ge organizations, including \\nFacebook, Twitter, and YouTube. The community edition is available for \", \"free, while the enterprise \\nedition requires a license purchase. Originally created in 1995, the pro\", \"duct was purchased by Sun  \\nMicrosystems in 2008. Oracle acquired Sun and MySQL in 2010.  \\nAzure Dat\", \"abase for MySQL  is a managed relational database service based on the open -source \\nMySQL Server en\", \"gine. It uses the MySQL Community edition. The Azure MySQL server is the \\nadministrative point for t\", \"he service. It\\u2019s the same MySQL server engine used for on -premises \\ndeployme nts. The engine can cr\", \"eate a single database per server or multiple databases per server that \\nshare resources. You can co\", \"ntinue to manage data using the same open -source tools without having \\nto learn new skills or manag\", \"e virtual machines.  \\nAzure Database for MariaDB  \\nMariaDB  Server  is another popular open -source \", \"database server. It was created as a fork of MySQL \\nwhen Oracle purchased Sun Microsystems, who owne\", \"d MySQL. The intent was to ensure that MariaDB \\nremained open -source. As MariaDB is a fork of MySQL\", \", the data and table definitions are compatible, \\nand the client protocols, structures, and APIs, ar\", \"e  close -knit.  \\n102 CHAPTER 5 | Cloud -native data patterns  \\n MariaDB has a strong community and \", \"is used by many large enterprises. While Oracle continues to \\nmaintain, enhance, and support MySQL, \", \"the MariaDB foundation manages MariaDB, allowing public \\ncontributions to the product and documentat\", \"ion.  \\nAzure Database for MariaDB  is a fully managed relational database as a service in the Azure \", \"cloud. The \\nservice is based on the  MariaDB community edition server engine. It can handle mission \", \"-critical \\nworkloads with predictable performance and dynamic scalability.  \\nAzure Database for Post\", \"greSQL  \\nPostgreSQL  is an open -source relational database with over 30 years of active development\", \". \\nPostgreSQL  has a strong reputation for reliability and data integrity. It\\u2019s feature rich, SQL co\", \"mpliant, \\nand considered more performant than MySQL - especially for workloads w ith complex queries\", \" and \\nheavy writes. Many large enterprises including Apple, Red Hat, and Fujitsu have built products\", \" using \\nPostgreSQL.  \\nAzure Database for PostgreSQL  is a fully managed relational database service,\", \" based on the open -\\nsource Postgres database engine. The service supports many development platform\", \"s, including C++, \\nJava, Python, Node, C#, and PHP. You can migrate PostgreSQL databases to it using\", \" the comman d-\\nline interface tool or Azure Data Migration Service.  \\nAzure Database for PostgreSQL \", \"is available with two deployment options:  \\n\\u2022 The Single Server  deployment option is a central admi\", \"nistrative point for multiple databases \\nto which you can deploy many databases. The pricing is stru\", \"ctured per -server based upon \\ncores and storage.  \\n\\u2022 The Hyperscale (Citus) option  is powered by  \", \"Citus Data  technology. It enables high \\nperformance by horizontally scaling  a single database acro\", \"ss hundreds of nodes to deliver fast \\nperformance and scale. This option allows the engine to fit mo\", \"re data in memory, parallelize \\nqueries across hundreds of nodes, and index data faster.  \\nNoSQL dat\", \"a in Azure  \\nCosmos DB is a fully managed, globally distributed NoSQL database service in the Azure \", \"cloud. It has \\nbeen adopted by many large companies across the world, including Coca -Cola, Skype, E\", \"xxonMobil, \\nand Liberty Mutual.  \\nIf your services require fast response from anywhere in the world,\", \" high availability, or elastic \\nscalability, Cosmos DB is a great choice. Figure 5 -12 shows Cosmos \", \"DB.   \\n103 CHAPTER 5 | Cloud -native data patterns  \\n  \\nFigure 5 -12: Overview of Azure Cosmos DB  \\n\", \"The previous figure presents many of the built -in cloud -native capabilities available in Cosmos DB\", \". In \\nthis section, we\\u2019ll take a closer look at them.  \\nGlobal support  \\nCloud -native applications \", \"often have a global audience and require global scale.  \\nYou can distribute Cosmos databases across \", \"regions or around the world, placing data close to your \\nusers, improving response time, and reducin\", \"g latency. You can add or remove a database from a \\nregion without pausing or redeploying your servi\", \"ces. In the background, Cosmos DB transparently \\nreplicates the data to each of the configured regio\", \"ns.  \\nCosmos DB supports active/active  clustering at the global level, enabling you to configure an\", \"y of your \\ndatabase regions to support both writes and reads . \\nThe Multi -region write  protocol is\", \" an important feature in Cosmos DB that enables the following \\nfunctionality:  \\n\\u2022 Unlimited elastic \", \"write and read scalability.  \\n\\u2022 99.999% read and write availability all around the world.  \\n\\u2022 Guaran\", \"teed reads and writes served in less than 10 milliseconds at the 99th percentile.  \\nWith the Cosmos \", \"DB Multi -Homing APIs , your microservice is automatically aware of the nearest \\nAzure region and se\", \"nds requests to it. The nearest region is identified by Cosmos DB without any \\nconfiguration changes\", \". Should a region become unavailable, the Multi -Homing feature will \\nautomatically route requests t\", \"o the next nearest available region.  \\nMulti -model support  \\nWhen replatforming monolithic applicat\", \"ions to a cloud -native architecture, development teams \\nsometimes have to migrate open -source, NoS\", \"QL data stores. Cosmos DB can help you preserve your \\n \\n104 CHAPTER 5 | Cloud -native data patterns \", \" \\n investment in these NoSQL datastores with its multi -model  data platform. The following table sh\", \"ows \\nthe supported NoSQL compatibility APIs . \\nProvider  Description  \\nNoSQL API  API for NoSQL stor\", \"es data in document format  \\nMongo DB API  Supports Mongo DB APIs and JSON documents  \\nGremlin API  \", \"Supports Gremlin API with graph -based nodes \\nand edge data representations  \\nCassandra API  Support\", \"s Casandra API for wide -column data \\nrepresentations  \\nTable API  Supports Azure Table Storage with\", \" premium \\nenhancements  \\nPostgreSQL API  Managed service for running PostgreSQL at any \\nscale  \\nDeve\", \"lopment teams can migrate existing Mongo, Gremlin, or Cassandra databases into Cosmos DB \\nwith minim\", \"al changes to data or code. For new apps, development teams can choose among open -\\nsource options o\", \"r the built -in SQL API model.  \\nInternally, Cosmos stores the data in a simple struct format made u\", \"p of primitive data types. For each \\nrequest, the database engine translates the primitive data into\", \" the model representation you\\u2019ve \\nselected.  \\nIn the previous table, note the Table API  option. Thi\", \"s API is an evolution of Azure Table Storage. Both \\nshare the same underlying table model, but the C\", \"osmos DB Table API adds premium enhancements \\nnot available in the Azure Storage API. The following \", \"table contrasts the features.  \\nFeature  Azure Table Storage  Azure Cosmos DB  \\nLatency  Fast Single\", \" -digit millisecond latency for reads and \\nwrites anywhere in the world  \\nThroughp\\nut Limit of 20,00\", \"0 operations per table  Unlimited operations per table  \\nGlobal \\nDistributio\\nn Single region with op\", \"tional single \\nsecondary read region  Turnkey distributions to all regions with \\nautomatic failover \", \" \\nIndexing  Available for partition and row key \\nproperties only  Automatic indexing of all properti\", \"es  \\nPricing  Optimized for cold workloads (low \\nthroughput : storage ratio)  Optimized for hot work\", \"loads (high \\nthroughput : storage ratio)  \\nMicroservices that consume Azure Table storage can easily\", \" migrate to the Cosmos DB Table API. No \\ncode changes are required.   \\n105 CHAPTER 5 | Cloud -native\", \" data patterns  \\n Tunable consistency  \\nEarlier in the Relational vs.  NoSQL  section, we discussed \", \"the subject of data consistency . Data \\nconsistency refers to the integrity  of your data. Cloud -na\", \"tive services with distributed data rely on \\nreplication and must make a fundamental tradeoff betwee\", \"n read consistency, availability, and latency.  \\nMost distributed databases allow developers to choo\", \"se between two consistency \\nmodels:  strong  consistency and  eventual  consistency. Strong consiste\", \"ncy  is the gold standard of data \\nprogrammability. It guarantees that a query will always return th\", \"e most current data - even if the \\nsystem must incur latency waiting for an update to replicate acro\", \"ss all database copies. While a \\ndatabase configured for eventual consistency  will return data imme\", \"diately, even if that data isn\\u2019t the \\nmost current copy. The latter opti on enables higher availabil\", \"ity, greater scale, and increased \\nperformance.  \\nAzure Cosmos DB offers five well -defined consiste\", \"ncy models  shown in Figure 5 -13. \\n \\nFigure 5 -13: Cosmos DB Consistency Levels  \\nThese options ena\", \"ble you to make precise choices and granular tradeoffs for consistency, availability, \\nand the perfo\", \"rmance for your data. The levels are presented in the following table.  \\nConsistency Level  Descript\", \"ion  \\nEventual  No ordering guarantee for reads. Replicas will \\neventually converge.  \\nConstant Pref\", \"ix  Reads are still eventual, but data is returned in \\nthe ordering in which it is written.  \\nSessio\", \"n  Guarantees you can read any data written \\nduring the current session. It is the default \\nconsiste\", \"ncy level.  \\nBounded Staleness  Reads trail writes by interval that you specify.  \\nStrong  Reads are\", \" guaranteed to return most recent \\ncommitted version of an item. A client never \\nsees an uncommitted\", \" or partial read.  \\nIn the article Getting Behind the 9 -Ball: Cosmos DB Consistency Levels Explaine\", \"d , Microsoft Program \\nManager Jeremy Likness provides an excellent explanation of the five models. \", \" \\nPartitioning  \\nAzure Cosmos DB embraces automatic partitioning  to scale a database to meet the pe\", \"rformance \\nneeds of your cloud -native services.  \\nYou manage data in Cosmos DB data by creating dat\", \"abases, containers, and items.  \\n \\n106 CHAPTER 5 | Cloud -native data patterns  \\n Containers live in\", \" a Cosmos DB database and represent a schema -agnostic grouping of items. Items \\nare the data that y\", \"ou add to the container. They\\u2019re represented as documents, rows, nodes, or edges. \\nAll items added t\", \"o a container are automatically indexed.  \\nTo partition the container, items are divided into distin\", \"ct subsets called  logical partitions. Logical \\npartitions are populated based on the value of a  pa\", \"rtition key  that is associated with each item in a \\ncontainer. Figure 5 -14 shows two containers ea\", \"ch wi th a logical partition based on a partition key \\nvalue.  \\n \\nFigure 5 -14: Cosmos DB partitioni\", \"ng mechanics  \\nNote in the previous figure how each item includes a partition key of either \\u2018city\\u2019 o\", \"r \\u2018airport\\u2019. The key \\ndetermines the item\\u2019s logical partition. Items with a city code are assigned t\", \"o the container on the left, \\nand items with an airport code, to the cont ainer on the right. Combin\", \"ing the partition key value with \\nthe ID value creates an item\\u2019s  index, which uniquely identifies t\", \"he item.  \\nInternally, Cosmos DB automatically manages the placement of logical partitions  on physi\", \"cal \\npartitions to satisfy the scalability and performance needs of the container. As application th\", \"roughput \\nand storage requirements increase, Azure Cosmos DB redistributes logical partitions across\", \" a greater \\nnumber of servers. Redistribution oper ations are managed by Cosmos DB and invoked witho\", \"ut \\ninterruption or downtime.  \\nNewSQL databases  \\nNewSQL  is an emerging database technology that c\", \"ombines the distributed scalability of  NoSQL  with \\nthe ACID  guarantees of a relational database. \", \"NewSQL databases are important for business systems \\nthat must process high -volumes of data, across\", \" distributed environ ments, with full transactional \\nsupport and ACID compliance. While a NoSQL data\", \"base can provide massive scalability, it does not \\nguarantee data consistency. Intermittent problems\", \" from inconsistent data can place a burden on the \\ndevelopment team. Devel opers must construct safe\", \"guards into their microservice code to manage \\nproblems caused by inconsistent data.  \\nThe Cloud Nat\", \"ive Computing Foundation (CNCF) features several NewSQL database projects.  \\n \\n107 CHAPTER 5 | Cloud\", \" -native data patterns  \\n Project  Characteristics  \\nCockroach DB  An ACID -compliant, relational da\", \"tabase that \\nscales globally. Add a new node to a cluster and \\nCockroachDB takes care of balancing t\", \"he data \\nacross instances and geographies. It creates, \\nmanages, and distributes replicas to ensure \", \"\\nreliability. It\\u2019s open sourc e and freely available.  \\nTiDB An open -source database that supports \", \"Hybrid \\nTransactional and Analytical Processing (HTAP) \\nworkloads. It is MySQL -compatible and featu\", \"res \\nhorizontal scalability, strong consistency, and \\nhigh availability. TiDB acts like a MySQL serv\", \"er. \\nYou can continue to  use existing MySQL client \\nlibraries, without requiring extensive code \\nch\", \"anges to your application.  \\nYugabyteDB  An open source, high -performance, distributed \\nSQL databas\", \"e. It supports low query latency, \\nresilience against failures, and global data \\ndistribution. Yugab\", \"yteDB is PostgreSQL -\\ncompatible and handles scale -out RDBMS and \\ninternet -scale OLTP workloads. T\", \"he product also \\nsupports NoSQL and is compatible with \\nCassandra.  \\nVitess  Vitess is a database so\", \"lution for deploying, \\nscaling, and managing large clusters of MySQL \\ninstances. It can run in a pub\", \"lic or private cloud \\narchitecture. Vitess combines and extends many \\nimportant MySQL features and f\", \"eatures both \\nvertical and horizontal sharding support. \\nOriginated by YouTube, Vitess has been serv\", \"ing \\nall YouTube database traffic since 2011.  \\nThe open -source projects in the previous figure are\", \" available from the Cloud Native Computing \\nFoundation. Three of the offerings are full database pro\", \"ducts, which include .NET support. The other, \\nVitess, is a database clustering system that horizont\", \"ally sc ales large clusters of MySQL instances.  \\nA key design goal for NewSQL databases is to work \", \"natively in Kubernetes, taking advantage of the \\nplatform\\u2019s resiliency and scalability.  \\nNewSQL dat\", \"abases are designed to thrive in ephemeral cloud environments where underlying virtual \\nmachines can\", \" be restarted or rescheduled at a moment\\u2019s notice. The databases are designed to \\nsurvive node failu\", \"res without data loss nor downtime. CockroachDB,  for example, is able to survive a \\nmachine loss by\", \" maintaining three consistent replicas of any data across the nodes in a cluster.  \\nKubernetes uses \", \"a Services construct  to allow a client to address a group of identical NewSQL \\ndatabases processes \", \"from a single DNS entry. By decoupling the database instances from the address  \\n108 CHAPTER 5 | Clo\", \"ud -native data patterns  \\n of the service with which it\\u2019s associated, we can scale without disrupti\", \"ng existing application instances. \\nSending a request to any service at a given time will always yie\", \"ld the same result.  \\nIn this scenario, all database instances are equal. There are no primary or se\", \"condary relationships. \\nTechniques like consensus replication  found in CockroachDB allow any databa\", \"se node to handle any \\nrequest. If the node that receives a load -balanced request has the data it n\", \"eeds locally, it responds \\nimmediately. If not, the node becomes a gateway and forwards the request \", \"to the appropriate no des \\nto get the correct answer. From the client\\u2019s perspective, every database \", \"node is the same: They appear \\nas a single  logical  database with the consistency guarantees of a s\", \"ingle -machine system, despite \\nhaving dozens or even hundreds of nodes that are working behind the \", \"scenes.  \\nFor a detailed look at the mechanics behind NewSQL databases, see the DASH: Four Propertie\", \"s of \\nKubernetes -Native Databases  article.  \\nData migration to the cloud  \\nOne of the more time -c\", \"onsuming tasks is migrating data from one data platform to another. The \\nAzure Data Migration Servic\", \"e  can help expedite such efforts. It can migrate data from several external \\ndatabase sources into \", \"Azure Data platforms with minimal downtime. Target platforms include the \\nfollowing services:  \\n\\u2022 Az\", \"ure SQL Database  \\n\\u2022 Azure Database for MySQL  \\n\\u2022 Azure Database for MariaDB  \\n\\u2022 Azure Database for \", \"PostgreSQL  \\n\\u2022 Azure Cosmos DB  \\nThe service provides recommendations to guide you through the chang\", \"es required to execute a \\nmigration, both small or large.  \\nCaching in a cloud -native app  \\nThe ben\", \"efits of caching are well understood. The technique works by temporarily copying frequently \\naccesse\", \"d data from a backend data store to fast storage  that\\u2019s located closer to the application. \\nCaching\", \" is often implemented where\\u2026  \\n\\u2022 Data remains relatively static.  \\n\\u2022 Data access is slow, especially\", \" compared to the speed of the cache.  \\n\\u2022 Data is subject to high levels of contention.  \\nWhy?  \\nAs d\", \"iscussed in the Microsoft caching guidance , caching can increase performance, scalability, and \\nava\", \"ilability for individual microservices and the system as a whole. It reduces the latency and \\nconten\", \"tion of handling large volumes of concurrent requests to a data store. As data volume and the \\nnumbe\", \"r of users increase, the greater the benefits of caching become.   \\n109 CHAPTER 5 | Cloud -native da\", \"ta patterns  \\n Caching is most effective when a client repeatedly reads data that is immutable or th\", \"at changes \\ninfrequently. Examples include reference information such as product and pricing informa\", \"tion, or \\nshared static resources that are costly to construct.  \\nWhile microservices should be stat\", \"eless, a distributed cache can support concurrent access to session \\nstate data when absolutely requ\", \"ired.  \\nAlso consider caching to avoid repetitive computations. If an operation transforms data or p\", \"erforms a \\ncomplicated calculation, cache the result for subsequent requests.  \\nCaching architecture\", \"  \\nCloud native applications typically implement a distributed caching architecture. The cache is ho\", \"sted \\nas a cloud -based backing service , separate from the microservices. Figure 5 -15 shows the ar\", \"chitecture.  \\n \\nFigure 5 -15: Caching in a cloud native app  \\nIn the previous figure, note how the c\", \"ache is independent of and shared by the microservices. In this \\nscenario, the cache is invoked by t\", \"he API Gateway . As discussed in chapter 4, the gateway serves as a \\nfront end for all incoming requ\", \"ests. The distributed cache increases system responsiveness by \\nreturning cached data whenever possi\", \"ble. Additionally, separating the cache from the services allows \\nthe cach e to scale up or out inde\", \"pendently to meet increased traffic demands.  \\nThe previous figure presents a common caching pattern\", \" known as the cache -aside pattern . For an \\nincoming request, you first query the cache (step #1) f\", \"or a response. If found, the data is returned \\nimmediately. If the data doesn\\u2019t exist in the cache (\", \"known as a cache miss ), it\\u2019s retrieved from a local \\ndatabase in a downstream service (step #2). It\", \"\\u2019s then written to the cache for future requests (step #3), \\nand returned to the caller. Care must b\", \"e taken to periodically evict cached data so that the system \\nremains timely and consistent.  \\nAs a \", \"shared cache grows, it might prove beneficial to partition its data across multiple nodes. Doing \\nso\", \" can help minimize contention and improve scalability. Many Caching services support the ability to \", \"\\n \\n110 CHAPTER 5 | Cloud -native data patterns  \\n dynamically add and remove nodes and rebalance dat\", \"a across partitions. This approach typically \\ninvolves clustering. Clustering exposes a collection o\", \"f federated nodes as a seamless, single cache. \\nInternally, however, the data is dispersed across th\", \"e nodes following a predefined distribution strategy \\nthat balances the load evenly.  \\nAzure Cache f\", \"or Redis  \\nAzure Cache for Redis  is a secure data caching and messaging broker service, fully manag\", \"ed by \\nMicrosoft. Consumed as a Platform as a Service (PaaS) offering, it provides high throughput a\", \"nd low -\\nlatency access to data. The service is accessible to any application within or outs ide of \", \"Azure.  \\nThe Azure Cache for Redis service manages access to open -source Redis servers hosted acros\", \"s Azure \\ndata centers. The service acts as a facade providing management, access control, and securi\", \"ty. The \\nservice natively supports a rich set of data structures, in cluding strings, hashes, lists,\", \" and sets. If your \\napplication already uses Redis, it will work as -is with Azure Cache for Redis. \", \" \\nAzure Cache for Redis is more than a simple cache server. It can support a number of scenarios to \", \"\\nenhance a microservices architecture:  \\n\\u2022 An in -memory data store  \\n\\u2022 A distributed non -relationa\", \"l database  \\n\\u2022 A message broker  \\n\\u2022 A configuration or discovery server  \\nFor advanced scenarios, a \", \"copy of the cached data can be persisted to disk . If a catastrophic event \\ndisables both the primar\", \"y and replica caches, the cache is reconstructed from the most recent \\nsnapshot.  \\nAzure Redis Cache\", \" is available across a number of predefined configurations and pricing tiers. The \\nPremium tier  fea\", \"tures many enterprise -level features such as clustering, data persistence, geo -\\nreplication, and v\", \"irtual -network isolation.  \\nElasticsearch in a cloud -native app  \\nElasticsearch is a distributed s\", \"earch and analytics system that enables complex search capabilities \\nacross diverse types of data. I\", \"t\\u2019s open source and widely popular. Consider how the following \\ncompanies integrate Elasticsearch in\", \"to their application:  \\n\\u2022 Wikipedia  for full -text and incremental (search as you type) searching. \", \" \\n\\u2022 GitHub  to index and expose over 8 million code repositories.  \\n \\n\\u2022 Docker  for making its conta\", \"iner library discoverable.  \\nElasticsearch is built on top of the Apache Lucene  full-text search en\", \"gine. Lucene provides high -\\nperformance document indexing and querying. It indexes data with an inv\", \"erted indexing scheme \\u2013 \\ninstead of mapping pages to keywords, it maps keywords to pages just like a\", \" glossary at the end of a \\nbook. Lucene has powerful query syntax capabilities and can query data by\", \":   \\n111 CHAPTER 5 | Cloud -native data patterns  \\n \\u2022 Term (a full word)  \\n\\u2022 Prefix (starts -with wo\", \"rd)  \\n\\u2022 Wildcard (using \\u201c*\\u201d or \\u201c?\\u201d filters)  \\n\\u2022 Phrase (a sequence of text in a document)  \\n\\u2022 Boolea\", \"n value (complex searches combining queries)  \\nWhile Lucene provides low -level plumbing for searchi\", \"ng, Elasticsearch provides the server that sits on \\ntop of Lucene. Elasticsearch adds higher -level \", \"functionality to simplify working Lucene, including a \\nRESTful API to access Lucene\\u2019s indexing and s\", \"earchin g functionality. It also provides a distributed \\ninfrastructure capable of massive scalabili\", \"ty, fault tolerance, and high availability.  \\nFor larger cloud -native applications with complex sea\", \"rch requirements, Elasticsearch is available as \\nmanaged service in Azure. The Microsoft Azure Marke\", \"tplace features preconfigured templates which \\ndevelopers can use to deploy an Elasticsearch cluster\", \" on A zure.  \\nFrom the Microsoft Azure Marketplace, developers can use preconfigured templates built\", \" to quickly \\ndeploy an Elasticsearch cluster on Azure. Using the Azure -managed offering, you can de\", \"ploy up to 50 \\ndata nodes, 20 coordinating nodes, and three dedicated mas ter nodes.  \\nSummary  \\nThi\", \"s chapter presented a detailed look at data in cloud -native systems. We started by contrasting data\", \" \\nstorage in monolithic applications with data storage patterns in cloud -native systems. We looked \", \"at \\ndata patterns implemented in cloud -native systems, in cluding cross -service queries, distribut\", \"ed \\ntransactions, and patterns to deal with high -volume systems. We contrasted SQL with NoSQL data.\", \" \\nWe looked at data storage options available in Azure that include both Microsoft -centric and open\", \" -\\nsource options. Fin ally, we discussed caching and Elasticsearch in a cloud -native application. \", \" \\nReferences  \\n\\u2022 Command and Query Responsibility Segregation (CQRS) pattern  \\n\\u2022 Event Sourcing patt\", \"ern  \\n\\u2022 Why isn\\u2019t RDBMS Partition Tolerant in CAP Theorem and why is it Available?  \\n\\u2022 Materialized \", \"View  \\n\\u2022 All you really need to know about open source databases  \\n\\u2022 Compensating Transaction patter\", \"n  \\n\\u2022 Saga Pattern  \\n\\u2022 Saga Patterns | How to implement business transactions using microservices  \\n\", \"\\u2022 Compensating Transaction pattern  \\n\\u2022 Getting Behind the 9 -Ball: Cosmos DB Consistency Levels Expl\", \"ained  \\n\\u2022 On RDBMS, NoSQL and NewSQL databases. Interview with John Ryan   \\n112 CHAPTER 5 | Cloud -n\", \"ative data patterns  \\n \\u2022 SQL vs NoSQL vs NewSQL: The Full Comparison  \\n\\u2022 DASH: Four Properties of Ku\", \"bernetes -Native Databases  \\n\\u2022 CockroachDB  \\n\\u2022 TiDB \\n\\u2022 YugabyteDB  \\n\\u2022 Vitess  \\n\\u2022 Elasticsearch: The \", \"Definitive Guide  \\n\\u2022 Introduction to Apache Lucene   \\n113 CHAPTER 6 | Cloud -native resiliency  \\n CH\", \"APTER  6 \\nCloud -native resiliency  \\nResiliency is the ability of your system to react to failure an\", \"d still remain functional. It\\u2019s not about \\navoiding failure, but accepting failure and constructing \", \"your cloud -native services to respond to it. \\nYou want to return to a fully functioning state quick\", \"ly as possible.  \\nUnlike traditional monolithic applications, where everything runs together in a si\", \"ngle process, cloud -\\nnative systems embrace a distributed architecture as shown in Figure 6 -1: \\n \\n\", \"Figure 6 -1. Distributed cloud -native environment  \\nIn the previous figure, each microservice and c\", \"loud -based backing service  execute in a separate \\nprocess, across server infrastructure, communica\", \"ting via network -based calls.  \\nOperating in this environment, a service must be sensitive to many \", \"different challenges:  \\n\\u2022 Unexpected network latency - the time for a service request to travel to t\", \"he receiver and back.  \\n\\u2022 Transient faults  - short -lived network connectivity errors.  \\n\\u2022 Blockage\", \" by a long -running synchronous operation.  \\n\\u2022 A host process that has crashed and is being restarte\", \"d or moved.  \\n\\u2022 An overloaded microservice that can\\u2019t respond for a short time.  \\n\\u2022 An in -flight or\", \"chestrator operation such as a rolling upgrade or moving a service from one \\nnode to another.  \\n \\n11\", \"4 CHAPTER 6 | Cloud -native resiliency  \\n \\u2022 Hardware failures.  \\nCloud platforms can detect and miti\", \"gate many of these infrastructure issues. It may restart, scale out, \\nand even redistribute your ser\", \"vice to a different node. However, to take full advantage of this built -in \\nprotection, you must de\", \"sign your services to re act to it and thrive in this dynamic environment.  \\nIn the following sectio\", \"ns, we\\u2019ll explore defensive techniques that your service and managed cloud \\nresources can leverage t\", \"o minimize downtime and disruption.  \\nApplication resiliency patterns  \\nThe first line of defense is\", \" application resiliency.  \\nWhile you could invest considerable time writing your own resiliency fram\", \"ework, such products \\nalready exist. Polly  is a comprehensive .NET resilience and transient -fault-\", \"handling library that allows \\ndevelopers to express resiliency policies in a fluent and thread -safe\", \" manner. Polly targets applications \\nbuilt with either .NET Framework or .NET 7. The following table\", \" de scribes the resiliency features, called \\npolicies, available in the Polly Library. They can be a\", \"pplied individually or grouped together.  \\nPolicy  Experience  \\nRetry  Configures retry operations o\", \"n designated \\noperations.  \\nCircuit Breaker  Blocks requested operations for a predefined \\nperiod wh\", \"en faults exceed a configured \\nthreshold  \\nTimeout  Places limit on the duration for which a caller \", \"\\ncan wait for a response.  \\nBulkhead  Constrains actions to fixed -size resource pool to \\nprevent fa\", \"iling calls from swamping a resource.  \\nCache  Stores responses automatically.  \\nFallback  Defines s\", \"tructured behavior upon a failure.  \\nNote how in the previous figure the resiliency policies apply t\", \"o request messages, whether coming \\nfrom an external client or back -end service. The goal is to com\", \"pensate the request for a service that \\nmight be momentarily unavailable. These short -lived inte rr\", \"uptions typically manifest themselves with \\nthe HTTP status codes shown in the following table.  \\nHT\", \"TP Status Code  Cause  \\n404 Not Found  \\n408 Request timeout  \\n429 Too many requests (you\\u2019ve most lik\", \"ely been throttled)  \\n502 Bad gateway  \\n503 Service unavailable   \\n115 CHAPTER 6 | Cloud -native res\", \"iliency  \\n HTTP Status Code  Cause  \\n504 Gateway timeout  \\nQuestion: Would you retry an HTTP Status \", \"Code of 403 - Forbidden? No. Here, the system is \\nfunctioning properly, but informing the caller tha\", \"t they aren\\u2019t authorized to perform the requested \\noperation. Care must be taken to retry only those\", \" operations caused by failures.  \\nAs recommended in Chapter 1, Microsoft developers constructing clo\", \"ud -native applications should \\ntarget the .NET platform. Version 2.1 introduced the HTTPClientFacto\", \"ry  library for creating HTTP Client \\ninstances for interacting with URL -based resources. Supersedi\", \"ng the original HTTPClient class, the \\nfactory class supports many enhanced features, one of which i\", \"s tight integration  with the Polly \\nresiliency library. With it, you can easily define resiliency p\", \"olicies in the application Startup class to \\nhandle partial failures and connectivity issues.  \\nNext\", \", let\\u2019s expand on retry and circuit breaker patterns.  \\nRetry pattern  \\nIn a distributed cloud -nati\", \"ve environment, calls to services and cloud resources can fail because of \\ntransient (short -lived) \", \"failures, which typically correct themselves after a brief period of time. \\nImplementing a retry str\", \"ategy helps a cloud -native service mitigate these scenarios.  \\nThe Retry pattern  enables a service\", \" to retry a failed request operation a (configurable) number of \\ntimes with an exponentially increas\", \"ing wait time. Figure 6 -2 shows a retry in action.  \\n \\nFigure 6 -2. Retry pattern in action  \\nIn th\", \"e previous figure, a retry pattern has been implemented for a request operation. It\\u2019s configured to \", \"\\nallow up to four retries before failing with a backoff interval (wait time) starting at two seconds\", \", which \\nexponentially doubles for each subsequent at tempt.  \\n\\u2022 The first invocation fails and retu\", \"rns an HTTP status code of 500. The application waits for two \\nseconds and retries the call.  \\n \\n116\", \" CHAPTER 6 | Cloud -native resiliency  \\n \\u2022 The second invocation also fails and returns an HTTP stat\", \"us code of 500. The application now \\ndoubles the backoff interval to four seconds and retries the ca\", \"ll.  \\n\\u2022 Finally, the third call succeeds.  \\n\\u2022 In this scenario, the retry operation would have attem\", \"pted up to four retries while doubling \\nthe backoff duration before failing the call.  \\n\\u2022 Had the 4t\", \"h retry attempt failed, a fallback policy would be invoked to gracefully handle the \\nproblem.  \\nIt\\u2019s\", \" important to increase the backoff period before retrying the call to allow the service time to self\", \" -\\ncorrect. It\\u2019s a best practice to implement an exponentially increasing backoff (doubling the peri\", \"od on \\neach retry) to allow adequate correction time.  \\nCircuit breaker pattern  \\nWhile the retry pa\", \"ttern can help salvage a request entangled in a partial failure, there are situations \\nwhere failure\", \"s can be caused by unanticipated events that will require longer periods of time to \\nresolve. These \", \"faults can range in severity from a part ial loss of connectivity to the complete failure of \\na serv\", \"ice. In these situations, it\\u2019s pointless for an application to continually retry an operation that i\", \"s \\nunlikely to succeed.  \\nTo make things worse, executing continual retry operations on a non -respo\", \"nsive service can move \\nyou into a self -imposed denial of service scenario where you flood your ser\", \"vice with continual calls \\nexhausting resources such as memory, threads and database c onnections, c\", \"ausing failure in unrelated \\nparts of the system that use the same resources.  \\nIn these situations,\", \" it would be preferable for the operation to fail immediately and only attempt to \\ninvoke the servic\", \"e if it\\u2019s likely to succeed.  \\nThe Circuit Breaker pattern  can prevent an application from repeated\", \"ly trying to execute an operation \\nthat\\u2019s likely to fail. After a pre -defined number of failed call\", \"s, it blocks all traffic to the service. \\nPeriodically, it will allow a trial call to determine whet\", \"her the fault has resolved. Figure 6 -3 shows the \\nCircuit Breaker pattern in action.   \\n117 CHAPTER\", \" 6 | Cloud -native resiliency  \\n  \\nFigure 6 -3. Circuit breaker pattern in action  \\nIn the previous \", \"figure, a Circuit Breaker pattern has been added to the original retry pattern. Note how \\nafter 100 \", \"failed requests, the circuit breakers opens and no longer allows calls to the service. The \\nCheckCir\", \"cuit value, set at 30 seconds, specifies h ow often the library allows one request to proceed to \\nth\", \"e service. If that call succeeds, the circuit closes and the service is once again available to traf\", \"fic.  \\nKeep in mind that the intent of the Circuit Breaker pattern is different  than that of the Re\", \"try pattern. \\nThe Retry pattern enables an application to retry an operation in the expectation that\", \" it will succeed. \\nThe Circuit Breaker pattern prevents an application from doing an operation that \", \"is likely to fail. \\nTypically, an appli cation will combine  these two patterns by using the Retry p\", \"attern to invoke an \\noperation through a circuit breaker.  \\nTesting for resiliency  \\nTesting for res\", \"iliency cannot always be done the same way that you test application functionality (by \\nrunning unit\", \" tests, integration tests, and so on). Instead, you must test how the end -to-end workload \\nperforms\", \" under failure conditions, which only occur intermittently. For example: inject failures by \\ncrashin\", \"g processes, expired certificates, make dependent services unavailable etc. Frameworks like \\nchaos -\", \"monkey  can be used for such chaos testing.  \\nApplication resiliency is a must for handling problema\", \"tic requested operations. But, it\\u2019s only half of the \\nstory. Next, we cover resiliency features avai\", \"lable in the Azure cloud.  \\nAzure platform resiliency  \\nBuilding a reliable application in the cloud\", \" is different from traditional on -premises application \\ndevelopment. While historically you purchas\", \"ed higher -end hardware to scale up, in a cloud \\nenvironment you scale out. Instead of trying to pre\", \"vent failures, t he goal is to minimize their effects \\nand keep the system stable.  \\n \\n118 CHAPTER 6\", \" | Cloud -native resiliency  \\n That said, reliable cloud applications display distinct characteristi\", \"cs:  \\n\\u2022 They\\u2019re resilient, recover gracefully from problems, and continue to function.  \\n\\u2022 They\\u2019re h\", \"ighly available (HA) and run as designed in a healthy state with no significant \\ndowntime.  \\nUnderst\", \"anding how these characteristics work together - and how they affect cost - is essential to \\nbuildin\", \"g a reliable cloud -native application. We\\u2019ll next look at ways that you can build resiliency and \\na\", \"vailability into your cloud -native applications leveraging features from the  Azure cloud.  \\nDesign\", \" with resiliency  \\nWe\\u2019ve said resiliency enables your application to react to failure and still rema\", \"in functional. The \\nwhitepaper, Resilience in Azure whitepaper , provides guidance for achieving res\", \"ilience in the Azure \\nplatform. Here are some key recommendations:  \\n\\u2022 Hardware failure.  Build redu\", \"ndancy into the application by deploying components across \\ndifferent fault domains. For example, en\", \"sure that Azure VMs are placed in different racks by \\nusing Availability Sets.  \\n\\u2022 Datacenter failur\", \"e.  Build redundancy into the application with fault isolation zones across \\ndatacenters. For exampl\", \"e, ensure that Azure VMs are placed in different fault -isolated \\ndatacenters by using Azure Availab\", \"ility Zones.  \\n\\u2022 Regional failure.  Replicate the data and components into another region so that ap\", \"plications \\ncan be quickly recovered. For example, use Azure Site Recovery to replicate Azure VMs to\", \" \\nanother Azure region.  \\n\\u2022 Heavy load.  Load balance across instances to handle spikes in usage. Fo\", \"r example, put two or \\nmore Azure VMs behind a load balancer to distribute traffic to all VMs.  \\n\\u2022 A\", \"ccidental data deletion or corruption.  Back up data so it can be restored if there\\u2019s any \\ndeletion \", \"or corruption. For example, use Azure Backup to periodically back up your Azure \\nVMs.  \\nDesign with \", \"redundancy  \\nFailures vary in scope of impact. A hardware failure, such as a failed disk, can affect\", \" a single node in a \\ncluster. A failed network switch could affect an entire server rack. Less commo\", \"n failures, such as loss of \\npower, could disrupt a whole datacenter. R arely, an entire region beco\", \"mes unavailable.  \\nRedundancy  is one way to provide application resilience. The exact level of redu\", \"ndancy needed \\ndepends upon your business requirements and will affect both the cost and complexity \", \"of your \\nsystem. For example, a multi -region deployment is more expensive and more compl ex to mana\", \"ge \\nthan a single -region deployment. You\\u2019ll need operational procedures to manage failover and fail\", \"back. \\nThe additional cost and complexity might be justified for some business scenarios, but not ot\", \"hers.  \\nTo architect redundancy, you need to identify the critical paths in your application, and th\", \"en \\ndetermine if there\\u2019s redundancy at each point in the path? If a subsystem should fail, will the \", \"\\napplication fail over to something else? Finally, you need a clea r understanding of those features\", \" built  \\n119 CHAPTER 6 | Cloud -native resiliency  \\n into the Azure cloud platform that you can leve\", \"rage to meet your redundancy requirements. Here are \\nrecommendations for architecting redundancy:  \\n\", \"\\u2022 Deploy multiple instances of services.  If your application depends on a single instance of a \\nser\", \"vice, it creates a single point of failure. Provisioning multiple instances improves both \\nresilienc\", \"y and scalability. When hosting in Azure Kubernetes Service, you can declaratively \\nconfigure redund\", \" ant instances (replica sets) in the Kubernetes manifest file. The replica count \\nvalue can be manag\", \"ed programmatically, in the portal, or through autoscaling features.  \\n\\u2022 Leveraging a load balancer.\", \"  Load -balancing distributes your application\\u2019s requests to healthy \\nservice instances and automati\", \"cally removes unhealthy instances from rotation. When \\ndeploying to Kubernetes, load balancing can b\", \"e specified in the Kubernetes manifest file in \\nthe Services section.  \\n\\u2022 Plan for multiregion deplo\", \"yment.  If you deploy your application to a single region, and that \\nregion becomes unavailable, you\", \"r application will also become unavailable. This may be \\nunacceptable under the terms of your applic\", \"ation\\u2019s service level agreements. Instead, consider \\ndeploying you r application and its services ac\", \"ross multiple regions. For example, an Azure \\nKubernetes Service (AKS) cluster is deployed to a sing\", \"le region. To protect your system from a \\nregional failure, you might deploy your application  to mu\", \"ltiple AKS clusters across different \\nregions and use the Paired Regions  feature to coordinate plat\", \"form updates and prioritize \\nrecovery efforts.  \\n\\u2022 Enable geo-replication . Geo-replication for serv\", \"ices such as Azure SQL Database and Cosmos \\nDB will create secondary replicas of your data across mu\", \"ltiple regions. While both services will \\nautomatically replicate data within the same region, geo -\", \"replication protects you against a  \\nregional outage by enabling you to fail over to a secondary reg\", \"ion. Another best practice for \\ngeo-replication centers around storing container images. To deploy a\", \" service in AKS, you \\nneed to store and pull the image from a repository. Azure Container Regi stry \", \"integrates with \\nAKS and can securely store container images. To improve performance and availabilit\", \"y, \\nconsider geo -replicating your images to a registry in each region where you have an AKS \\ncluste\", \"r. Each AKS cluster then pulls container images from th e local container registry in its \\nregion as\", \" shown in Figure 6 -4: \\n \\nFigure 6 -4. Replicated resources across regions  \\n \\n120 CHAPTER 6 | Cloud\", \" -native resiliency  \\n \\u2022 Implement a DNS traffic load balancer.  Azure Traffic Manager  provides hig\", \"h -availability for \\ncritical applications by load -balancing at the DNS level. It can route traffic\", \" to different regions \\nbased on geography, cluster response time, and even application endpoint heal\", \"th. For \\nexample, Azure Traffic Manager can dire ct customers to the closest AKS cluster and \\napplic\", \"ation instance. If you have multiple AKS clusters in different regions, use Traffic \\nManager to cont\", \"rol how traffic flows to the applications that run in each cluster. Figure 6 -5 \\nshows this scenario\", \".  \\n \\nFigure 6 -5. AKS and Azure Traffic Manager  \\nDesign for scalability  \\nThe cloud thrives on sca\", \"ling. The ability to increase/decrease system resources to address \\nincreasing/decreasing system loa\", \"d is a key tenet of the Azure cloud. But, to effectively scale an \\napplication, you need an understa\", \"nding of the scaling features of e ach Azure service that you include \\nin your application. Here are\", \" recommendations for effectively implementing scaling in your system.  \\n\\u2022 Design for scaling.  An ap\", \"plication must be designed for scaling. To start, services should be \\nstateless so that requests can\", \" be routed to any instance. Having stateless services also means \\nthat adding or removing an instanc\", \"e doesn\\u2019t adversely impact current users.  \\n\\u2022 Partition workloads . Decomposing domains into indepen\", \"dent, self-contained microservices \\nenable each service to scale independently of others. Typically,\", \" services will have different \\nscalability needs and requirements. Partitioning enables you to scale\", \" only what needs to be \\nscaled without the unnecessary cost of sc aling an entire application.  \\n\\u2022 F\", \"avor scale -out. Cloud -based applications favor scaling out resources as opposed to scaling \\nup. Sc\", \"aling out (also known as horizontal scaling) involves adding more service resources to \\nan existing \", \"system to meet and share a desired level of performance. Scaling up (also k nown \\n \\n121 CHAPTER 6 | \", \"Cloud -native resiliency  \\n as vertical scaling) involves replacing existing resources with more pow\", \"erful hardware (more \\ndisk, memory, and processing cores). Scaling out can be invoked automatically \", \"with the \\nautoscaling features available in some Azure cloud resources. Scaling out ac ross multiple\", \" \\nresources also adds redundancy to the overall system. Finally scaling up a single resource is \\ntyp\", \"ically more expensive than scaling out across many smaller resources. Figure 6 -6 shows the \\ntwo app\", \"roaches:  \\n \\nFigure 6 -6. Scale up versus scale out  \\n\\u2022 Scale proportionally.  When scaling a servic\", \"e, think in terms of resource sets . If you were to \\ndramatically scale out a specific service, what\", \" impact would that have on back -end data \\nstores, caches and dependent services? Some resources suc\", \"h as Cosmos DB can scale out \\nproportionally, while many others can\\u2019t. You want to ensure that you d\", \"on\\u2019t scale out a \\nresource to a point where it will exhaust other associated resources.  \\n\\u2022 Avoid af\", \"finity.  A best practice is to ensure a node doesn\\u2019t require local affinity, often referred \\nto as a\", \" sticky session . A request should be able to route to any instance. If you need to persist \\nstate, \", \"it should be saved to a distributed cache, such as Azure Redis cache . \\n\\u2022 Take advantage of platform\", \" autoscaling features.  Use built -in autoscaling features whenever \\npossible, rather than custom or\", \" third -party mechanisms. Where possible, use scheduled \\nscaling rules to ensure that resources are \", \"available without a startup delay, but add reactive \\nautoscaling to the rules as app ropriate, to co\", \"pe with unexpected changes in demand. For \\nmore information, see Autoscaling guidance . \\n\\u2022 Scale out\", \" aggressively.  A final practice would be to scale out aggressively so that you can \\nquickly meet im\", \"mediate spikes in traffic without losing business. And, then scale in (that is, \\nremove unneeded ins\", \"tances) conservatively to keep the system stable. A simple way to \\nimpleme nt this is to set the coo\", \"l down period, which is the time to wait between scaling \\noperations, to five minutes for adding res\", \"ources and up to 15 minutes for removing instances.  \\nBuilt -in retry in services  \\nWe encouraged th\", \"e best practice of implementing programmatic retry operations in an earlier section. \\nKeep in mind t\", \"hat many Azure services and their corresponding client SDKs also include retry \\n \\n122 CHAPTER 6 | Cl\", \"oud -native resiliency  \\n mechanisms. The following list summarizes retry features in the many of th\", \"e Azure services that are \\ndiscussed in this book:  \\n\\u2022 Azure Cosmos DB.  The DocumentClient  class f\", \"rom the client API automatically retires failed \\nattempts. The number of retries and maximum wait ti\", \"me are configurable. Exceptions thrown \\nby the client API are either requests that exceed the retry \", \"policy or non -transient errors.  \\n\\u2022 Azure Redis Cache.  The Redis StackExchange client uses a conne\", \"ction manager class that \\nincludes retries on failed attempts. The number of retries, specific retry\", \" policy and wait time \\nare all configurable.  \\n\\u2022 Azure Service Bus.  The Service Bus client exposes \", \"a RetryPolicy class  that can be configured \\nwith a back -off interval, retry count, and Termination\", \"TimeBuffer , which specifies the maximum \\ntime an operation can take. The default policy is nine max\", \"imum retry attempts with a 30 -\\nsecond backoff period between attempts.  \\n\\u2022 Azure SQL Database.  Ret\", \"ry support is provided when using the Entity Framework Core  library.  \\n\\u2022 Azure Storage.  The storag\", \"e client library support retry operations. The strategies vary across \\nAzure storage tables, blobs, \", \"and queues. As well, alternate retries switch between primary and \\nsecondary storage services locati\", \"ons when the geo -redundancy feature is enabled.  \\n\\u2022 Azure Event Hubs.  The Event Hub client library\", \" features a RetryPolicy property, which includes \\na configurable exponential backoff feature.  \\nResi\", \"lient communications  \\nThroughout this book, we\\u2019ve embraced a microservice -based architectural appr\", \"oach. While such an \\narchitecture provides important benefits, it presents many challenges:  \\n\\u2022 Out-\", \"of-process network communication.  Each microservice communicates over a network \\nprotocol that intr\", \"oduces network congestion, latency, and transient faults.  \\n\\u2022 Service discovery.  How do microservic\", \"es discover and communicate with each other when \\nrunning across a cluster of machines with their ow\", \"n IP addresses and ports?  \\n\\u2022 Resiliency.  How do you manage short -lived failures and keep the syst\", \"em stable?  \\n\\u2022 Load balancing.  How does inbound traffic get distributed across multiple instances o\", \"f a \\nmicroservice?  \\n\\u2022 Security.  How are security concerns such as transport -level encryption and \", \"certificate \\nmanagement enforced?  \\n\\u2022 Distributed Monitoring.  - How do you correlate and capture tr\", \"aceability and monitoring for a \\nsingle request across multiple consuming microservices?  \\nYou can a\", \"ddress these concerns with different libraries and frameworks, but the implementation can \\nbe expens\", \"ive, complex, and time -consuming. You also end up with infrastructure concerns coupled to \\nbusiness\", \" logic.   \\n123 CHAPTER 6 | Cloud -native resiliency  \\n Service mesh  \\nA better approach is an evolvi\", \"ng technology entitled Service Mesh . A service mesh  is a configurable \\ninfrastructure layer with b\", \"uilt -in capabilities to handle service communication and the other \\nchallenges mentioned above. It \", \"decouples these concerns by moving them into a service proxy. The \\nproxy is deployed into a separate\", \" process (cal led a sidecar ) to provide isolation from business code. \\nHowever, the sidecar is link\", \"ed to the service - it\\u2019s created with it and shares its lifecycle. Figure 6 -7 \\nshows this scenario.\", \"  \\n \\nFigure 6 -7. Service mesh with a side car  \\nIn the previous figure, note how the proxy intercep\", \"ts and manages communication among the \\nmicroservices and the cluster.  \\nA service mesh is logically\", \" split into two disparate components: A data plane  and control plane . Figure \\n6-8 shows these comp\", \"onents and their responsibilities.  \\n \\n124 CHAPTER 6 | Cloud -native resiliency  \\n  \\nFigure 6 -8. Se\", \"rvice mesh control and data plane  \\nOnce configured, a service mesh is highly functional. It can ret\", \"rieve a corresponding pool of instances \\nfrom a service discovery endpoint. The mesh can then send a\", \" request to a specific instance, recording \\nthe latency and response type of the result. A mes h can\", \" choose the instance most likely to return a \\nfast response based on many factors, including its obs\", \"erved latency for recent requests.  \\nIf an instance is unresponsive or fails, the mesh will retry th\", \"e request on another instance. If it returns \\nerrors, a mesh will evict the instance from the load -\", \"balancing pool and restate it after it heals. If a \\nrequest times out, a mesh can fail and then retr\", \"y the request. A mesh captures and emits metrics and \\ndistributed tracing to a centralized metrics s\", \"ystem.  \\nIstio and Envoy  \\nWhile a few service mesh options currently exist, Istio is the most popul\", \"ar at the time of this writing. \\nIstio is a joint venture from IBM, Google, and Lyft. It\\u2019s an open -\", \"source offering that can be integrated \\ninto a new or existing distributed application. The technolo\", \"gy provides a consistent and complete \\nsolution to secure, connect, and monitor microservices. Its f\", \"eatures include:  \\n\\u2022 Secure service -to-service communication in a cluster with strong identity -bas\", \"ed \\nauthentication and authorization.  \\n\\u2022 Automatic load balancing for HTTP, gRPC , WebSocket, and T\", \"CP traffic.  \\n\\u2022 Fine-grained control of traffic behavior with rich routing rules, retries, failovers\", \", and fault \\ninjection.  \\n\\u2022 A pluggable policy layer and configuration API supporting access control\", \"s, rate limits, and \\nquotas.  \\n\\u2022 Automatic metrics, logs, and traces for all traffic within a cluste\", \"r, including cluster ingress and \\negress.  \\nA key component for an Istio implementation is a proxy s\", \"ervice entitled the Envoy proxy . It runs \\nalongside each service and provides a platform -agnostic \", \"foundation for the following features:  \\n\\u2022 Dynamic service discovery.  \\n\\u2022 Load balancing.  \\n \\n125 CH\", \"APTER 6 | Cloud -native resiliency  \\n \\u2022 TLS termination.  \\n\\u2022 HTTP and gRPC proxies.  \\n\\u2022 Circuit brea\", \"ker resiliency.  \\n\\u2022 Health checks.  \\n\\u2022 Rolling updates with canary  deployments.  \\nAs previously dis\", \"cussed, Envoy is deployed as a sidecar to each microservice in the cluster.  \\nIntegration with Azure\", \" Kubernetes Services  \\nThe Azure cloud embraces Istio and provides direct support for it within Azur\", \"e Kubernetes Services. \\nThe following links can help you get started:  \\n\\u2022 Installing Istio in AKS  \\n\", \"\\u2022 Using AKS and Istio  \\nReferences  \\n\\u2022 Polly  \\n\\u2022 Retry pattern  \\n\\u2022 Circuit Breaker pattern  \\n\\u2022 Resil\", \"ience in Azure whitepaper  \\n\\u2022 network latency  \\n\\u2022 Redundancy  \\n\\u2022 geo-replication  \\n\\u2022 Azure Traffic M\", \"anager  \\n\\u2022 Autoscaling guidance  \\n\\u2022 Istio \\n\\u2022 Envoy proxy   \\n126 CHAPTER 7 | Monitoring and health  \\n\", \" CHAPTER  7 \\nMonitoring and health  \\nMicroservices and cloud -native applications go hand in hand wi\", \"th good DevOps practices. DevOps is \\nmany things to many people but perhaps one of the better defini\", \"tions comes from cloud advocate \\nand DevOps evangelist Donovan Brown:  \\n\\u201cDevOps is the union of peop\", \"le, process, and products to enable continuous delivery of value to our \\nend users.\\u201d  \\nUnfortunately\", \", with terse definitions, there\\u2019s always room to say more things. One of the key \\ncomponents of DevO\", \"ps is ensuring that the applications running in production are functioning \\nproperly and efficiently\", \". To gauge the health of the application in p roduction, it\\u2019s necessary to monitor \\nthe various logs\", \" and metrics being produced from the servers, hosts, and the application proper. The \\nnumber of diff\", \"erent services running in support of a cloud -native application makes monitoring the \\nhealth of ind\", \"ividua l components and the application as a whole a critical challenge.  \\nObservability patterns  \\n\", \"Just as patterns have been developed to aid in the layout of code in applications, there are pattern\", \"s \\nfor operating applications in a reliable way. Three useful patterns in maintaining applications h\", \"ave \\nemerged: logging , monitoring , and alerts . \\nWhen to use logging  \\nNo matter how careful we ar\", \"e, applications almost always behave in unexpected ways in production. \\nWhen users report problems w\", \"ith an application, it\\u2019s useful to be able to see what was going on with \\nthe app when the problem o\", \"ccurred. One of the most tried and true ways of capturing information \\nabout what an application is \", \"doing while it\\u2019s running is to have the application write down what it\\u2019s \\ndoing. This process is kno\", \"wn as logging. Anytime failures or problems occur in production, the goal \\nshould be to re produce t\", \"he conditions under which the failures occurred, in a non -production \\nenvironment. Having good logg\", \"ing in place provides a roadmap for developers to follow in order to \\nduplicate problems in an envir\", \"onment that can be tested and experimented with.  \\nChallenges when logging with cloud -native applic\", \"ations  \\nIn traditional applications, log files are typically stored on the local machine. In fact, \", \"on Unix -like \\noperating systems, there\\u2019s a folder structure defined to hold any logs, typically und\", \"er /var/log.   \\n127 CHAPTER 7 | Monitoring and health  \\n  \\nFigure 7 -1. Logging to a file in a monol\", \"ithic app.  \\nThe usefulness of logging to a flat file on a single machine is vastly reduced in a clo\", \"ud environment. \\nApplications producing logs may not have access to the local disk or the local disk\", \" may be highly \\ntransient as containers are shuffled around physical ma chines. Even simple scaling \", \"up of monolithic \\napplications across multiple nodes can make it challenging to locate the appropria\", \"te file -based log \\nfile. \\n \\nFigure 7 -2. Logging to files in a scaled monolithic app.  \\nCloud -nati\", \"ve applications developed using a microservices architecture also pose some challenges for \\nfile-bas\", \"ed loggers. User requests may now span multiple services that are run on different machines \\n \\n128 C\", \"HAPTER 7 | Monitoring and health  \\n and may include serverless functions with no access to a local f\", \"ile system at all. It would be very \\nchallenging to correlate the logs from a user or a session acro\", \"ss these many services and machines.  \\n \\nFigure 7 -3. Logging to local files in a microservices app.\", \"  \\nFinally, the number of users in some cloud -native applications is high. Imagine that each user \\n\", \"generates a hundred lines of log messages when they log into an application. In isolation, that is \\n\", \"manageable, but multiply that over 100,000 users and the volum e of logs becomes large enough that \\n\", \"specialized tools are needed to support effective use of the logs.  \\nLogging in cloud -native applic\", \"ations  \\nEvery programming language has tooling that permits writing logs, and typically the overhea\", \"d for \\nwriting these logs is low. Many of the logging libraries provide logging different kinds of c\", \"riticalities, \\nwhich can be tuned at run time. For instance, the Serilog library  is a popular struc\", \"tured logging library \\nfor .NET that provides the following logging levels:  \\n\\u2022 Verbose  \\n\\u2022 Debug  \\n\", \"\\u2022 Information  \\n\\u2022 Warning  \\n\\u2022 Error  \\n\\u2022 Fatal  \\nThese different log levels provide granularity in lo\", \"gging. When the application is functioning properly \\nin production, it may be configured to only log\", \" important messages. When the application is \\n \\n129 CHAPTER 7 | Monitoring and health  \\n misbehaving\", \", then the log level can be increased so more verbose logs are gathered. This balances \\nperformance \", \"against ease of debugging.  \\nThe high performance of logging tools and the tunability of verbosity s\", \"hould encourage developers to \\nlog frequently. Many favor a pattern of logging the entry and exit of\", \" each method. This approach may \\nsound like overkill, but it\\u2019s infrequent that develope rs will wish\", \" for less logging. In fact, it\\u2019s not \\nuncommon to perform deployments for the sole purpose of adding\", \" logging around a problematic \\nmethod. Err on the side of too much logging and not on too little. So\", \"me tools can be used to \\nautomatically provide this kind of logging.  \\nBecause of the challenges ass\", \"ociated with using file -based logs in cloud -native apps, centralized logs \\nare preferred. Logs are\", \" collected by the applications and shipped to a central logging application \\nwhich indexes and store\", \"s the logs. This class of syste m can ingest tens of gigabytes of logs every day.  \\nIt\\u2019s also helpfu\", \"l to follow some standard practices when building logging that spans many services. \\nFor instance, g\", \"enerating a correlation ID  at the start of a lengthy interaction, and then logging it in \\neach mess\", \"age that is related to that interaction, makes it easier to search for all related messages. One \\nne\", \"ed only find a single message and extract the correlation ID to find all the related messages. \\nAnot\", \"her example is ensuring that the log format is the same for every service, whatever the language \\nor\", \" logging library it uses. This standardization makes reading logs much easier. Figure 7 -4 \\ndemonstr\", \"ates how a microservices architecture can lev erage centralized logging as part of its \\nworkflow.  \\n\", \" \\nFigure 7 -4. Logs from various sources are ingested into a centralized log store.  \\n \\n130 CHAPTER \", \"7 | Monitoring and health  \\n Challenges with detecting and responding to potential app health \\nissue\", \"s  \\nSome applications aren\\u2019t mission critical. Maybe they\\u2019re only used internally, and when a proble\", \"m \\noccurs, the user can contact the team responsible and the application can be restarted. However, \", \"\\ncustomers often have higher expectations for the applications  they consume. You should know when \\n\", \"problems occur with your application before  users do, or before users notify you. Otherwise, the fi\", \"rst \\nyou know about a problem may be when you notice an angry deluge of social media posts deriding \", \"\\nyour application or ev en your organization.  \\nSome scenarios you may need to consider include:  \\n\\u2022\", \" One service in your application keeps failing and restarting, resulting in intermittent slow \\nrespo\", \"nses.  \\n\\u2022 At some times of the day, your application\\u2019s response time is slow.  \\n\\u2022 After a recent dep\", \"loyment, load on the database has tripled.  \\nImplemented properly, monitoring can let you know about\", \" conditions that will lead to problems, \\nletting you address underlying conditions before they resul\", \"t in any significant user impact.  \\nMonitoring cloud -native apps  \\nSome centralized logging systems\", \" take on an additional role of collecting telemetry outside of pure \\nlogs. They can collect metrics,\", \" such as time to run a database query, average response time from a \\nweb server, and even CPU load a\", \"verages and memory pressur e as reported by the operating system. \\nIn conjunction with the logs, the\", \"se systems can provide a holistic view of the health of nodes in the \\nsystem and the application as \", \"a whole.  \\nThe metric -gathering capabilities of the monitoring tools can also be fed manually from \", \"within the \\napplication. Business flows that are of particular interest such as new users signing up\", \" or orders being \\nplaced, may be instrumented such that they increment  a counter in the central mon\", \"itoring system. \\nThis aspect unlocks the monitoring tools to not only monitor the health of the appl\", \"ication but the \\nhealth of the business.  \\nQueries can be constructed in the log aggregation tools t\", \"o look for certain statistics or patterns, which \\ncan then be displayed in graphical form, on custom\", \" dashboards. Frequently, teams will invest in large, \\nwall-mounted displays that rotate through the \", \"s tatistics related to an application. This way, it\\u2019s simple \\nto see the problems as they occur.  \\nC\", \"loud -native monitoring tools provide real -time telemetry and insight into apps regardless of wheth\", \"er \\nthey\\u2019re single -process monolithic applications or distributed microservice architectures. They \", \"include \\ntools that allow collection of data from the app as  well as tools for querying and display\", \"ing \\ninformation about the app\\u2019s health.  \\nChallenges with reacting to critical problems in cloud -n\", \"ative apps  \\nIf you need to react to problems with your application, you need some way to alert the \", \"right \\npersonnel. This is the third cloud -native application observability pattern and depends on l\", \"ogging and \\nmonitoring. Your application needs to have logging in place to allow problems to be diag\", \"nosed, and  \\n131 CHAPTER 7 | Monitoring and health  \\n in some cases to feed into monitoring tools. I\", \"t needs monitoring to aggregate application metrics and \\nhealth data in one place. Once this has bee\", \"n established, rules can be created that will trigger alerts \\nwhen certain metrics fall outside of a\", \"cceptable le vels. \\nGenerally, alerts are layered on top of monitoring such that certain conditions \", \"trigger appropriate \\nalerts to notify team members of urgent problems. Some scenarios that may requi\", \"re alerts include:  \\n\\u2022 One of your application\\u2019s services is not responding after 1 minute of downti\", \"me.  \\n\\u2022 Your application is returning unsuccessful HTTP responses to more than 1% of requests.  \\n\\u2022 Y\", \"our application\\u2019s average response time for key endpoints exceeds 2000 ms.  \\nAlerts in cloud -native\", \" apps  \\nYou can craft queries against the monitoring tools to look for known failure conditions. For\", \" instance, \\nqueries could search through the incoming logs for indications of HTTP status code 500, \", \"which \\nindicates a problem on a web server. As soon as one of thes e is detected, then an e -mail or\", \" an SMS \\ncould be sent to the owner of the originating service who can begin to investigate.  \\nTypic\", \"ally, though, a single 500 error isn\\u2019t enough to determine that a problem has occurred. It could \\nme\", \"an that a user mistyped their password or entered some malformed data. The alert queries can be \\ncra\", \"fted to only fire when a larger than average number o f 500 errors are detected.  \\nOne of the most d\", \"amaging patterns in alerting is to fire too many alerts for humans to investigate. \\nService owners w\", \"ill rapidly become desensitized to errors that they\\u2019ve previously investigated and \\nfound to be beni\", \"gn. Then, when true errors occur, they\\u2019ll  be lost in the noise of hundreds of false \\npositives. The\", \" parable of the Boy Who Cried Wolf  is frequently told to children to warn them of this \\nvery danger\", \". It\\u2019s important to ensure that the alerts that do fire are indicative of a real problem.  \\nLogging \", \"with Elastic Stack  \\nThere are many good centralized logging tools and they vary in cost from being \", \"free, open -source \\ntools, to more expensive options. In many cases, the free tools are as good as o\", \"r better than the paid \\nofferings. One such tool is a combination of three open -source components: \", \"Elasticsearch, Logstash, \\nand Kibana.  \\nCollectively these tools are known as the Elastic Stack or E\", \"LK stack.  \\nElastic Stack  \\nThe Elastic Stack is a powerful option for gathering information from a \", \"Kubernetes cluster. Kubernetes \\nsupports sending logs to an Elasticsearch endpoint, and for the most\", \" part , all you need to get started \\nis to set the environment variables as shown in Figure 7 -5: \\nK\", \"UBE_LOGGING_DESTINATION=elasticsearch  \\nKUBE_ENABLE_NODE_LOGGING=true  \\nFigure 7 -5. Configuration v\", \"ariables for Kubernetes  \\nThis step will install Elasticsearch on the cluster and target sending all\", \" the cluster logs to it.   \\n132 CHAPTER 7 | Monitoring and health  \\n  \\nFigure 7 -6. An example of a \", \"Kibana dashboard showing the results of a query against logs that are ingested from \\nKubernetes  \\nWh\", \"at are the advantages of Elastic Stack?  \\nElastic Stack provides centralized logging in a low -cost,\", \" scalable, cloud -friendly manner. Its user \\ninterface streamlines data analysis so you can spend yo\", \"ur time gleaning insights from your data \\ninstead of fighting with a clunky interface. It supports a\", \" w ide variety of inputs so as your distributed \\napplication spans more and different kinds of servi\", \"ces, you can expect to continue to be able to feed \\nlog and metric data into the system. The Elastic\", \" Stack also supports fast searches even across large \\ndata set s, making it possible even for large \", \"applications to log detailed data and still be able to have \\nvisibility into it in a performant fash\", \"ion.  \\nLogstash  \\nThe first component is Logstash . This tool is used to gather log information from\", \" a large variety of \\ndifferent sources. For instance, Logstash can read logs from disk and also rece\", \"ive messages from \\nlogging libraries like Serilog . Logstash can do some basic filtering and expansi\", \"on on the logs as they \\narrive. For instance, if your logs contain IP addresses then Logstash may be\", \" configured to do a \\ngeographical lookup and obtain a country/region or even city of origin for that\", \" message . \\nSerilog is a logging library for .NET languages, which allows for parameterized logging.\", \" Instead of \\ngenerating a textual log message that embeds fields, parameters are kept separate. This\", \" library allows \\nfor more intelligent filtering and searching. A sampl e Serilog configuration for w\", \"riting to Logstash \\nappears in Figure 7 -7. \\nvar log = new LoggerConfiguration () \\n         .WriteTo\", \".Http(\\\"http://localhost:8080\\\" ) \\n         .CreateLogger (); \\nFigure 7 -7. Serilog config for writing\", \" log information directly to logstash over HTTP  \\nLogstash would use a configuration like the one sh\", \"own in Figure 7 -8. \\n \\n133 CHAPTER 7 | Monitoring and health  \\n input { \\n    http { \\n        #defaul\", \"t host 0.0.0.0:8080  \\n        codec => json  \\n    } \\n} \\n \\noutput {  \\n    elasticsearch {  \\n        h\", \"osts => \\\"elasticsearch:9200\\\"  \\n        index=>\\\"sales -%{+xxxx.ww}\\\"  \\n    } \\n} \\nFigure 7 -8. A Logsta\", \"sh configuration for consuming logs from Serilog  \\nFor scenarios where extensive log manipulation is\", \"n\\u2019t needed there\\u2019s an alternative to Logstash known \\nas Beats . Beats is a family of tools that can \", \"gather a wide variety of data from logs to network data \\nand uptime information. Many applications w\", \"ill use both Logstash and Beats.  \\nOnce the logs have been gathered by Logstash, it needs somewhere \", \"to put them. While Logstash \\nsupports many different outputs, one of the more exciting ones is Elast\", \"icsearch.  \\nElasticsearch  \\nElasticsearch is a powerful search engine that can index logs as they ar\", \"rive. It makes running queries \\nagainst the logs quick. Elasticsearch can handle huge quantities of \", \"logs and, in extreme cases, can be \\nscaled out across many nodes.  \\nLog messages that have been craf\", \"ted to contain parameters or that have had parameters split from \\nthem through Logstash processing, \", \"can be queried directly as Elasticsearch preserves this information.  \\nA query that searches for the\", \" top 10 pages visited by jill@example.com, appears in Figure 7 -9. \\n\\\"query\\\":  { \\n    \\\"match\\\": { \\n   \", \"   \\\"user\\\": \\\"jill@example.com\\\"  \\n    } \\n  }, \\n  \\\"aggregations\\\":  { \\n    \\\"top_10_pages\\\" : { \\n      \\\"te\", \"rms\\\": { \\n        \\\"field\\\": \\\"page\\\", \\n        \\\"size\\\": 10 \\n      } \\n    } \\n  } \\nFigure 7 -9. An Elastics\", \"earch query for finding top 10 pages visited by a user  \\nVisualizing information with Kibana web das\", \"hboards  \\nThe final component of the stack is Kibana. This tool is used to provide interactive visua\", \"lizations in a \\nweb dashboard. Dashboards may be crafted even by users who are non -technical. Most \", \"data that is \\nresident in the Elasticsearch index, can be included in  the Kibana dashboards. Indivi\", \"dual users may  \\n134 CHAPTER 7 | Monitoring and health  \\n have different dashboard desires and Kiban\", \"a enables this customization through allowing user -\\nspecific dashboards.  \\nInstalling Elastic Stack\", \" on Azure  \\nThe Elastic stack can be installed on Azure in many ways. As always, it\\u2019s possible to pr\", \"ovision virtual \\nmachines and install Elastic Stack on them directly . This option is preferred by s\", \"ome experienced users \\nas it offers the highest degree of customizability. Deploying on infrastructu\", \"re as a service introduces \\nsignificant management overhead forcing those who take that path to take\", \" ownership of all the tasks  \\nassociated with infrastructure as a service such as securing the machi\", \"nes and keeping up -to-date with \\npatches.  \\nAn option with less overhead is to make use of one of t\", \"he many Docker containers on which the \\nElastic Stack has already been configured. These containers \", \"can be dropped into an existing \\nKubernetes cluster and run alongside application code. The sebp/elk\", \"  container is a well -documented \\nand tested Elastic Stack container.  \\nAnother option is a recentl\", \"y announced ELK -as-a-service offering . \\nReferences  \\n\\u2022 Install Elastic Stack on Azure  \\nMonitoring\", \" in Azure Kubernetes Services  \\nThe built -in logging in Kubernetes is primitive. However, there are\", \" some great options for getting the \\nlogs out of Kubernetes and into a place where they can be prope\", \"rly analyzed. If you need to monitor \\nyour AKS clusters, configuring Elastic Stack for Kube rnetes i\", \"s a great solution.  \\nAzure Monitor for Containers  \\nAzure Monitor for Containers  supports consumin\", \"g logs from not just Kubernetes but also from other \\norchestration engines such as DC/OS, Docker Swa\", \"rm, and Red Hat OpenShift.   \\n135 CHAPTER 7 | Monitoring and health  \\n  \\nFigure 7 -10. Consuming log\", \"s from various containers  \\nPrometheus  is a popular open source metric monitoring solution. It is p\", \"art of the Cloud Native \\nCompute Foundation. Typically, using Prometheus requires managing a Prometh\", \"eus server with its \\nown store. However, Azure Monitor for Containers provides direct integration wi\", \"th Prometheus \\nmetrics endpoints , so a separate server is not required.  \\nLog and metric informatio\", \"n is gathered not just from the containers running in the cluster but also \\nfrom the cluster hosts t\", \"hemselves. It allows correlating log information from the two making it much \\neasier to track down a\", \"n error.  \\nInstalling the log collectors differs on Windows  and Linux  clusters. But in both cases \", \"the log collection \\nis implemented as a Kubernetes DaemonSet , meaning that the log collector is run\", \" as a container on \\neach of the nodes.  \\nNo matter which orchestrator or operating system is running\", \" the Azure Monitor daemon, the log \\ninformation is forwarded to the same Azure Monitor tools with wh\", \"ich users are familiar. This approach \\nensures a parallel experience in environments that mix differ\", \"ent log sources such as a hybrid \\nKubernetes/Azure Functions environment.  \\n \\n136 CHAPTER 7 | Monito\", \"ring and health  \\n  \\nFigure 7 -11. A sample dashboard showing logging and metric information from ma\", \"ny running containers.  \\nLog.Finalize()  \\nLogging is one of the most overlooked and yet most importa\", \"nt parts of deploying any application at \\nscale. As the size and complexity of applications increase\", \", then so does the difficulty of debugging \\nthem. Having top quality logs available makes debugging \", \"much easier and moves it from the realm of \\n\\u201cnearly impossible\\u201d to \\u201ca pleasant experience\\u201d.  \\nAzure \", \"Monitor  \\nNo other cloud provider has as mature of a cloud application monitoring solution than that\", \" found in \\nAzure. Azure Monitor is an umbrella name for a collection of tools designed to provide vi\", \"sibility into \\nthe state of your system. It helps you understand how  your cloud -native services ar\", \"e performing and \\nproactively identifies issues affecting them. Figure 7 -12 presents a high level o\", \"f view of Azure Monitor.  \\n \\n137 CHAPTER 7 | Monitoring and health  \\n  \\nFigure 7 -12. High -level vi\", \"ew of Azure Monitor.  \\nGathering logs and metrics  \\nThe first step in any monitoring solution is to \", \"gather as much data as possible. The more data \\ngathered, the deeper the insights. Instrumenting sys\", \"tems has traditionally been difficult. Simple \\nNetwork Management Protocol (SNMP) was the gold stand\", \"ard protoc ol for collecting machine level \\ninformation, but it required a great deal of knowledge a\", \"nd configuration. Fortunately, much of this \\nhard work has been eliminated as the most common metric\", \"s are gathered automatically by Azure \\nMonitor.  \\nApplication level metrics and events aren\\u2019t possib\", \"le to instrument automatically because they\\u2019re \\nspecific to the application being deployed. In order\", \" to gather these metrics, there are SDKs and APIs \\navailable  to directly report such information, s\", \"uch as when a customer signs up or completes an order. \\nExceptions can also be captured and reported\", \" back into Azure Monitor via Application Insights. The \\nSDKs support most every language found in Cl\", \"oud Native Applicat ions including Go, Python, \\nJavaScript, and the .NET languages.  \\nThe ultimate g\", \"oal of gathering information about the state of your application is to ensure that your \\nend users h\", \"ave a good experience. What better way to tell if users are experiencing issues than doing \\noutside \", \"-in web tests ? These tests can be as simple as pinging your website from locations around the \\nworl\", \"d or as involved as having agents log into the site and simulate user actions.  \\nReporting data  \\nOn\", \"ce the data is gathered, it can be manipulated, summarized, and plotted into charts, which allow \\nus\", \"ers to instantly see when there are problems. These charts can be gathered into dashboards or into \\n\", \"Workbooks, a multi -page report designed to tell a story about some aspect of the system.  \\n \\n138 CH\", \"APTER 7 | Monitoring and health  \\n No modern application would be complete without some artificial i\", \"ntelligence or machine learning. To \\nthis end, data can be passed  to the various machine learning t\", \"ools in Azure to allow you to extract \\ntrends and information that would otherwise be hidden.  \\nAppl\", \"ication Insights provides a powerful (SQL -like) query language called Kusto  that can query \\nrecord\", \"s, summarize them, and even plot charts. For example, the following query will locate all records \\nf\", \"or the month of November 2007, group them by state, and plot the top 10 as a pie chart.  \\nStormEvent\", \"s  \\n| where StartTime >= datetime(2007 -11-01) and StartTime < datetime(2007 -12-01) \\n| summarize co\", \"unt() by State  \\n| top 10 by count_  \\n| render piechart  \\nFigure 7 -13 shows the results of this App\", \"lication Insights Query.  \\n \\nFigure 7 -13. Application Insights query results.  \\nThere is a playgrou\", \"nd for experimenting with Kusto  queries. Reading sample queries  can also be \\ninstructive.  \\nDashbo\", \"ards  \\nThere are several different dashboard technologies that may be used to surface the informatio\", \"n from \\nAzure Monitor. Perhaps the simplest is to just run queries in Application Insights and plot \", \"the data \\ninto a chart . \\n \\n139 CHAPTER 7 | Monitoring and health  \\n  \\nFigure 7 -14. An example of A\", \"pplication Insights charts embedded in the main Azure Dashboard.  \\nThese charts can then be embedded\", \" in the Azure portal proper through use of the dashboard feature. \\nFor users with more exacting requ\", \"irements, such as being able to drill down into several tiers of data, \\nAzure Monitor data is availa\", \"ble to Power BI . Power BI is an industry -leading, enterprise class, business \\nintelligence tool th\", \"at can aggregate data from many different data sources.  \\n \\n140 CHAPTER 7 | Monitoring and health  \\n\", \"  \\nFigure 7 -15. An example Power BI dashboard.  \\nAlerts  \\nSometimes, having data dashboards is insu\", \"fficient. If nobody is awake to watch the dashboards, then \\nit can still be many hours before a prob\", \"lem is addressed, or even detected. To this end, Azure Monitor \\nalso provides a top notch alerting s\", \"olution . Alerts can be triggered by a wide range of conditions \\nincluding:  \\n\\u2022 Metric values  \\n\\u2022 Lo\", \"g search queries  \\n\\u2022 Activity Log events  \\n\\u2022 Health of the underlying Azure platform  \\n\\u2022 Tests for w\", \"eb site availability  \\nWhen triggered, the alerts can perform a wide variety of tasks. On the simple\", \" side, the alerts may just \\nsend an e -mail notification to a mailing list or a text message to an i\", \"ndividual. More involved alerts \\n \\n141 CHAPTER 7 | Monitoring and health  \\n might trigger a workflow\", \" in a tool such as PagerDuty, which is aware of who is on call for a particular \\napplication. Alerts\", \" can trigger actions in Microsoft Flow  unlocking near limitless possibilities for \\nworkflows.  \\nAs \", \"common causes of alerts are identified, the alerts can be enhanced with details about the common \\nca\", \"uses of the alerts and the steps to take to resolve them. Highly mature cloud -native application \\nd\", \"eployments may opt to kick off self -healing tasks, which perform actions such as removing failing \\n\", \"nodes from a scale set or triggering an autoscaling activity. Eventually it may no longer be necessa\", \"ry \\nto wake up on -call personnel at 2AM to resolve a live -site issue as the system will be able to\", \" adjust \\nitself to compensate or at least limp along until somebody arrives at work the next morning\", \".  \\nAzure Monitor automatically leverages machine learning to understand the normal operating \\nparam\", \"eters of deployed applications. This approach enables it to detect services that are operating \\nouts\", \"ide of their normal parameters. For instance, the typical week day traffic on the site might be \\n10,\", \"000 requests per minute. And then, on a given week, suddenly the number of requests hits a highly \\nu\", \"nusual 20,000 requests per minute. Smart Detection  will notice this deviation from the norm and \\ntr\", \"igger an alert. At the same time, the trend analysis is smart enough to avoid firing false positives\", \" \\nwhen the traffic load is expected.  \\nReferences  \\n\\u2022 Azure Monitor   \\n142 CHAPTER 8 | Cloud -native\", \" identity  \\n CHAPTER  8 \\nCloud -native identity  \\nMost software applications need to have some knowl\", \"edge of the user or process that is calling them. \\nThe user or process interacting with an applicati\", \"on is known as a security principal, and the process of \\nauthenticating and authorizing these princi\", \"pals is known as identity management, or simply identity . \\nSimple applications may include all of t\", \"heir identity management within the application, but this \\napproach doesn\\u2019t scale well with many app\", \"lications and many kinds of security principals. Windows \\nsupports the use of Active Directory to pr\", \"ovide centralized authentication and authorization.  \\nWhile this solution is effective within corpor\", \"ate networks, it isn\\u2019t designed for use by users or \\napplications that are outside of the AD domain.\", \" With the growth of Internet -based applications and \\nthe rise of cloud -native apps, security model\", \"s have evolved . \\nIn today\\u2019s cloud -native identity model, architecture is assumed to be distributed\", \". Apps can be \\ndeployed anywhere and may communicate with other apps anywhere. Clients may communica\", \"te with \\nthese apps from anywhere, and in fact, clients may consist of any co mbination of platforms\", \" and \\ndevices. Cloud -native identity solutions use open standards to achieve secure application acc\", \"ess from \\nclients. These clients range from human users on PCs or phones, to other apps hosted anywh\", \"ere \\nonline, to set -top boxes and IOT devices running any software platform anywhere in the world. \", \" \\nModern cloud -native identity solutions typically use access tokens that are issued by a secure to\", \"ken \\nservice/server (STS) to a security principal once their identity is determined. The access toke\", \"n, typically \\na JSON Web Token (JWT), includes claims  about the security principal. These claims wi\", \"ll minimally \\ninclude the user\\u2019s identity but may also include other claims that can be used by appl\", \"ications to \\ndetermine the level of access to grant the principal.  \\nTypically, the STS is only resp\", \"onsible for authenticating the principal. Determining their level of access \\nto resources is left to\", \" other parts of the application.  \\nReferences  \\n\\u2022 Microsoft identity platform  \\nAuthentication and a\", \"uthorization in cloud -native \\napps \\nAuthentication  is the process of determining the identity of a\", \" security principal. Authorization  is the act \\nof granting an authenticated principal permission to\", \" perform an action or access a resource. \\nSometimes authentication is shortened to AuthN and authori\", \"zation is shortened to AuthZ. Cloud - \\n143 CHAPTER 8 | Cloud -native identity  \\n native applications\", \" need to rely on open HTTP -based protocols to authenticate security principals \\nsince both clients \", \"and applications could be running anywhere in the world on any platform or device. \\nThe only common \", \"factor is HTTP.  \\nMany organizations still rely on local authentication services like Active Directo\", \"ry Federation Services \\n(ADFS). While this approach has traditionally served organizations well for \", \"on premises authentication \\nneeds, cloud -native applications benefit from sy stems designed specifi\", \"cally for the cloud. A recent \\n2019 United Kingdom National Cyber Security Centre (NCSC) advisory st\", \"ates that \\u201corganizations using \\nAzure AD as their primary authentication source will actually lower \", \"their risk compared to ADFS.\\u201d \\nSome r easons outlined in this analysis  include:  \\n\\u2022 Access to full \", \"set of Microsoft credential protection technologies.  \\n\\u2022 Most organizations are already relying on A\", \"zure AD to some extent.  \\n\\u2022 Double hashing of NTLM hashes ensures compromise won\\u2019t allow credentials\", \" that work in \\nlocal Active Directory.  \\nReferences  \\n\\u2022 Authentication basics  \\n\\u2022 Access tokens and \", \"claims  \\n\\u2022 It may be time to ditch your on premises authentication services  \\nAzure Active Directory\", \"  \\nMicrosoft Azure Active Directory (Azure AD) offers identity and access management as a service. \\n\", \"Customers use it to configure and maintain who users are, what information to store about them, who \", \"\\ncan access that information, who can manage it, and what apps can access it. AAD can authenticate \\n\", \"users for applications configured  to use it, providing a single sign -on (SSO) experience. It can b\", \"e used \\non its own or be integrated with Windows AD running on premises.  \\nAzure AD is built for the\", \" cloud. It\\u2019s truly a cloud -native identity solution that uses a REST -based Graph \\nAPI and OData sy\", \"ntax for queries, unlike Windows AD, which uses LDAP. On premises Active Directory \\ncan sync user at\", \"tributes to the cloud using Identit y Sync Services, allowing all authentication to take \\nplace in t\", \"he cloud using Azure AD. Alternately, authentication can be configured via Connect to pass \\nback to \", \"local Active Directory via ADFS to be completed by Windows AD on premises.  \\nAzure AD supports compa\", \"ny branded sign -in screens, multi -factory authentication, and cloud -based \\napplication proxies th\", \"at are used to provide SSO for applications hosted on premises. It offers \\ndifferent kinds of securi\", \"ty reporting and alert capabilities.  \\nReferences  \\n\\u2022 Microsoft identity platform   \\n144 CHAPTER 8 |\", \" Cloud -native identity  \\n IdentityServer for cloud -native applications  \\nIdentityServer is an auth\", \"entication server that implements OpenID Connect (OIDC) and OAuth 2.0 \\nstandards for ASP.NET Core. I\", \"t\\u2019s designed to provide a common way to authenticate requests to all of \\nyour applications, whether \", \"they\\u2019re web, native, mobile, or API endpoints. IdentityServer can be used to \\nimplement Single Sign \", \"-On (SSO) for multiple applications and application types. It can be used to \\nauthenticate actual us\", \"ers via sign -in forms and simila r user interfaces as well as service -based \\nauthentication that t\", \"ypically involves token issuance, verification, and renewal without any user \\ninterface. IdentitySer\", \"ver is designed to be a customizable solution. Each instance is typically \\ncustomized to suit an ind\", \"ividual organization and/or set of applications\\u2019 needs.  \\nCommon web app scenarios  \\nTypically, appl\", \"ications need to support some or all of the following scenarios:  \\n\\u2022 Human users accessing web appli\", \"cations with a browser.  \\n\\u2022 Human users accessing back -end Web APIs from browser -based apps.  \\n\\u2022 H\", \"uman users on mobile/native clients accessing back -end Web APIs.  \\n\\u2022 Other applications accessing b\", \"ack -end Web APIs (without an active user or user interface).  \\n\\u2022 Any application may need to intera\", \"ct with other Web APIs, using its own identity or \\ndelegating to the user\\u2019s identity.  \\n \\nFigure 8 -\", \"1. Application types and scenarios.  \\nIn each of these scenarios, the exposed functionality needs to\", \" be secured against unauthorized use. At \\na minimum, this typically requires authenticating the user\", \" or principal making a request for a resource. \\nThis authentication may use one of several commo n p\", \"rotocols such as SAML2p, WS -Fed, or OpenID \\nConnect. Communicating with APIs typically uses the OAu\", \"th2 protocol and its support for security \\ntokens. Separating these critical cross -cutting security\", \" concerns and their implementation details from \\nthe appli cations themselves ensures consistency an\", \"d improves security and maintainability. \\n \\n145 CHAPTER 8 | Cloud -native identity  \\n Outsourcing th\", \"ese concerns to a dedicated product like IdentityServer helps the requirement for every \\napplication\", \" to solve these problems itself.  \\nIdentityServer provides middleware that runs within an ASP.NET Co\", \"re application and adds support \\nfor OpenID Connect and OAuth2 (see supported specifications ). Orga\", \"nizations would create their own \\nASP.NET Core app using IdentityServer middleware to act as the STS\", \" for all of their token -based \\nsecurity protocols. The IdentityServer middleware exposes endpoints \", \"to support standard \\nfunctionality, including:  \\n\\u2022 Authorize (authenticate the end user)  \\n\\u2022 Token (\", \"request a token programmatically)  \\n\\u2022 Discovery (metadata about the server)  \\n\\u2022 User Info (get user \", \"information with a valid access token)  \\n\\u2022 Device Authorization (used to start device flow authoriza\", \"tion)  \\n\\u2022 Introspection (token validation)  \\n\\u2022 Revocation (token revocation)  \\n\\u2022 End Session (trigge\", \"r single sign -out across all apps)  \\nGetting started  \\nIdentityServer4 is available under dual lice\", \"nse:  \\n\\u2022 RPL - lets you use the IdentityServer4 free if used in open -source work  \\n\\u2022 Paid - lets yo\", \"u use the IdentityServer4 in a commercial scenario  \\nFor more information about pricing, see the off\", \"icial product\\u2019s pricing page . \\nYou can add it to your applications using its NuGet packages. The ma\", \"in package is IdentityServer4 , \\nwhich has been downloaded over four million times. The base package\", \" doesn\\u2019t include any user \\ninterface code and only supports in -memory configuration. To use it with\", \" a database, you\\u2019ll also want \\na data provider like IdentityServer4.EntityFramework , which uses Ent\", \"ity Framework Core to store \\nconfiguration and operational data for IdentityServer. For user interfa\", \"ce, you can copy files from the \\nQuickstart UI repository  into your ASP.NET Core MVC application to\", \" add support for sign in and sign \\nout using IdentityServer middleware.  \\nConfiguration  \\nIdentitySe\", \"rver supports different kinds of protocols and social authentication providers that can be \\nconfigur\", \"ed as part of each custom installation. This is typically done in the ASP.NET Core application\\u2019s \\nPr\", \"ogram class (or in the Startup class in the Conf igureServices method). The configuration involves \\n\", \"specifying the supported protocols and the paths to the servers and endpoints that will be used. \\nFi\", \"gure 8 -2 shows an example configuration taken from the IdentityServer4 Quickstart UI project:  \\npub\", \"lic class Startup \\n{ \\n    public void ConfigureServices (IServiceCollection services ) \\n    { \\n     \", \"   services .AddMvc(); \\n  \\n146 CHAPTER 8 | Cloud -native identity  \\n         // some details omitted\", \"  \\n        services .AddIdentityServer (); \\n \\n          services .AddAuthentication () \\n            \", \".AddGoogle (\\\"Google\\\" , options => \\n            { \\n                options.SignInScheme  = \\nIdentityS\", \"erverConstants .ExternalCookieAuthenticationScheme ; \\n \\n                options.ClientId  = \\\"<insert\", \" here>\\\" ; \\n                options.ClientSecret  = \\\"<insert here>\\\" ; \\n            }) \\n            .A\", \"ddOpenIdConnect (\\\"demoidsrv\\\" , \\\"IdentityServer\\\" , options => \\n            { \\n                options\", \".SignInScheme  = \\nIdentityServerConstants .ExternalCookieAuthenticationScheme ; \\n                opt\", \"ions.SignOutScheme  = IdentityServerConstants .SignoutScheme ; \\n \\n                options.Authority \", \" = \\\"https://demo.identityserver.io/\\\" ; \\n                options.ClientId  = \\\"implicit\\\" ; \\n          \", \"      options.ResponseType  = \\\"id_token\\\" ; \\n                options.SaveTokens  = true; \\n           \", \"     options.CallbackPath  = new PathString (\\\"/signin -idsrv\\\"); \\n                options.SignedOutCa\", \"llbackPath  = new PathString (\\\"/signout -callback -idsrv\\\"); \\n                options.RemoteSignOutPa\", \"th  = new PathString (\\\"/signout -idsrv\\\"); \\n \\n                options.TokenValidationParameters  = ne\", \"w TokenValidationParameters  \\n                { \\n                    NameClaimType = \\\"name\\\", \\n      \", \"              RoleClaimType = \\\"role\\\" \\n                }; \\n            }); \\n    } \\n} \\nFigure 8 -2. Co\", \"nfiguring IdentityServer.  \\nJavaScript clients  \\nMany cloud -native applications use server -side AP\", \"Is and rich client single page applications (SPAs) on \\nthe front end. IdentityServer ships a JavaScr\", \"ipt client  (oidc -client.js) via NPM that can be added to \\nSPAs to enable them to use IdentityServe\", \"r for sign in, sign out, and token -based authentication of \\nweb APIs.  \\nReferences  \\n\\u2022 IdentityServ\", \"er documentation  \\n\\u2022 Application types  \\n\\u2022 JavaScript OIDC client   \\n147 CHAPTER 9 | Cloud -native s\", \"ecurity  \\n CHAPTER  9 \\nCloud -native security  \\nNot a day goes by where the news doesn\\u2019t contain som\", \"e story about a company being hacked or \\nsomehow losing their customers\\u2019 data. Even countries/region\", \"s aren\\u2019t immune to the problems created \\nby treating security as an afterthought. For years, compani\", \"es hav e treated the security of customer \\ndata and, in fact, their entire networks as something of \", \"a \\u201cnice to have\\u201d. Windows servers were left \\nunpatched, ancient versions of PHP kept running, and Mo\", \"ngoDB databases left wide open to the \\nworld.  \\nHowever, there are starting to be real -world conseq\", \"uences for not maintaining a security mindset \\nwhen building and deploying applications. Many compan\", \"ies learned the hard way what can happen \\nwhen servers and desktops aren\\u2019t patched during the 2017 o\", \"utbreak  of NotPetya . The cost of these \\nattacks has easily reached into the billions, with some es\", \"timates putting the losses from this single \\nattack at 10 billion US dollars.  \\nEven governments are\", \"n\\u2019t immune to hacking incidents. The city of Baltimore was held ransom by \\ncriminals  making it impo\", \"ssible for citizens to pay their bills or use city services.  \\nThere has also been an increase in le\", \"gislation that mandates certain data protections for personal \\ndata. In Europe, GDPR has been in eff\", \"ect for more than a year and, more recently, California passed \\ntheir own version called CCDA, which\", \" comes into effect Ja nuary 1, 2020. The fines under GDPR can be \\nso punishing as to put companies o\", \"ut of business. Google has already been fined 50 million Euros for \\nviolations, but that\\u2019s just a dr\", \"op in the bucket compared with the potential fines.  \\nIn short, security is serious business.  \\nAzur\", \"e security for cloud -native apps  \\nCloud -native applications can be both easier and more difficult\", \" to secure than traditional applications. \\nOn the downside, you need to secure more smaller applicat\", \"ions and dedicate more energy to build \\nout the security infrastructure. The heterogeneous natu re o\", \"f programming languages and styles in \\nmost service deployments also means you need to pay more atte\", \"ntion to security bulletins from many \\ndifferent providers.  \\nOn the flip side, smaller services, ea\", \"ch with their own data store, limit the scope of an attack. If an \\nattacker compromises one system, \", \"it\\u2019s probably more difficult for the attacker to make the jump to \\nanother system than it is in a mo\", \"nolithic application . Process boundaries are strong boundaries. Also, \\nif a database backup gets ex\", \"posed, then the damage is more limited, as that database contains only a \\nsubset of data and is unli\", \"kely to contain personal data.   \\n148 CHAPTER 9 | Cloud -native security  \\n Threat modeling  \\nNo mat\", \"ter if the advantages outweigh the disadvantages of cloud -native applications, the same \\nholistic s\", \"ecurity mindset must be followed. Security and secure thinking must be part of every step of \\nthe de\", \"velopment and operations story. When planning an appl ication ask questions like:  \\n\\u2022 What would be \", \"the impact of this data being lost?  \\n\\u2022 How can we limit the damage from bad data being injected int\", \"o this service?  \\n\\u2022 Who should have access to this data?  \\n\\u2022 Are there auditing policies in place ar\", \"ound the development and release process?  \\nAll these questions are part of a process called threat \", \"modeling . This process tries to answer the \\nquestion of what threats there are to the system, how l\", \"ikely the threats are, and the potential damage \\nfrom them.  \\nOnce the list of threats has been esta\", \"blished, you need to decide whether they\\u2019re worth mitigating. \\nSometimes a threat is so unlikely and\", \" expensive to plan for that it isn\\u2019t worth spending energy on it. \\nFor instance, some state level ac\", \"tor could inject cha nges into the design of a process that is used by \\nmillions of devices. Now, in\", \"stead of running a certain piece of code in Ring 3 , that code is run in Ring \\n0. This process allow\", \"s an exploit that can bypass the hypervisor and run the attack code on the bare \\nmetal machines, all\", \"owing attacks on all the virtual machines that are running on that hardware.  \\nThe altered processor\", \"s are difficult to detect without a microscope and advanced knowledge of the on \\nsilicon design of t\", \"hat processor. This scenario is unlikely to happen and expensive to mitigate, so \\nprobably no threat\", \" model would recommend building explo it protection for it.  \\nMore likely threats, such as broken ac\", \"cess controls permitting Id incrementing attacks (replacing Id=2 \\nwith Id=3 in the URL) or SQL injec\", \"tion, are more attractive to build protections against. The \\nmitigations for these threats are quite\", \" reasonable to build  and prevent embarrassing security holes \\nthat smear the company\\u2019s reputation. \", \" \\nPrinciple of least privilege  \\nOne of the founding ideas in computer security is the Principle of \", \"Least Privilege (POLP). It\\u2019s actually a \\nfoundational idea in most any form of security be it digita\", \"l or physical. In short, the principle is that \\nany user or process should have the smalles t number\", \" of rights possible to execute its task.  \\nAs an example, think of the tellers at a bank: accessing \", \"the safe is an uncommon activity. So, the \\naverage teller can\\u2019t open the safe themselves. To gain ac\", \"cess, they need to escalate their request \\nthrough a bank manager, who performs additional security \", \"c hecks.  \\nIn a computer system, a fantastic example is the rights of a user connecting to a databas\", \"e. In many \\ncases, there\\u2019s a single user account used to both build the database structure and run t\", \"he application. \\nExcept in extreme cases, the account running the app lication doesn\\u2019t need the abil\", \"ity to update \\nschema information. There should be several accounts that provide different levels of\", \" privilege. The \\napplication should only use the permission level that grants read and writes access\", \" to the data in the \\ntables. This kind of protection would eliminate attacks that aimed to drop data\", \"base tables or \\nintroduce malicious triggers.   \\n149 CHAPTER 9 | Cloud -native security  \\n Almost ev\", \"ery part of building a cloud -native application can benefit from remembering the principle \\nof leas\", \"t privilege. You can find it at play when setting up firewalls, network security groups, roles, and \", \"\\nscopes in Role -based access control (RBAC).  \\nPenetration testing  \\nAs applications become more co\", \"mplicated the number of attack vectors increases at an alarming rate. \\nThreat modeling is flawed in \", \"that it tends to be executed by the same people building the system. In \\nthe same way that many deve\", \"lopers have trouble envision ing user interactions and then build \\nunusable user interfaces, most de\", \"velopers have difficulty seeing every attack vector. It\\u2019s also possible \\nthat the developers buildin\", \"g the system aren\\u2019t well versed in attack methodologies and miss \\nsomething crucial.  \\nPenetration t\", \"esting or \\u201cpen testing\\u201d involves bringing in external actors to attempt to attack the \\nsystem. These\", \" attackers may be an external consulting company or other developers with good \\nsecurity knowledge f\", \"rom another part of the business. They\\u2019re giv en carte blanche to attempt to \\nsubvert the system. Fr\", \"equently, they\\u2019ll find extensive security holes that need to be patched. \\nSometimes the attack vecto\", \"r will be something totally unexpected like exploiting a phishing attack \\nagainst the CEO.  \\nAzure i\", \"tself is constantly undergoing attacks from a team of hackers inside Microsoft . Over the years, \\nth\", \"ey\\u2019ve been the first to find dozens of potentially catastrophic attack vectors, closing them before \", \"\\nthey can be exploited externally. The more tempting a target, the more likely that eternal actors w\", \"ill \\nattempt to exploit it and there a re a few targets in the world more tempting than Azure.  \\nMon\", \"itoring  \\nShould an attacker attempt to penetrate an application, there should be some warning of it\", \". \\nFrequently, attacks can be spotted by examining the logs from services. Attacks leave telltale si\", \"gns \\nthat can be spotted before they succeed. For instance, an attac ker attempting to guess a passw\", \"ord \\nwill make many requests to a login system. Monitoring around the login system can detect weird \", \"\\npatterns that are out of line with the typical access pattern. This monitoring can be turned into a\", \"n \\nalert that can, in turn, alert an operations person to activate some sort of countermeasure. A hi\", \"ghly \\nmature monitoring system might even take action based on these deviations proactively adding r\", \"ules \\nto block requests or throttle responses.  \\nSecuring the build  \\nOne place where security is of\", \"ten overlooked is around the build process. Not only should the build \\nrun security checks, such as \", \"scanning for insecure code or checked -in credentials, but the build itself \\nshould be secure. If th\", \"e build server is compromised , then it provides a fantastic vector for introducing \\narbitrary code \", \"into the product.  \\nImagine that an attacker is looking to steal the passwords of people signing int\", \"o a web application. \\nThey could introduce a build step that modifies the checked -out code to mirro\", \"r any login request to \\nanother server. The next time code goes through the bui ld, it\\u2019s silently up\", \"dated. The source code \\nvulnerability scanning won\\u2019t catch this vulnerability as it runs before the \", \"build. Equally, nobody will  \\n150 CHAPTER 9 | Cloud -native security  \\n catch it in a code review be\", \"cause the build steps live on the build server. The exploited code will go to \\nproduction where it c\", \"an harvest passwords. Probably there\\u2019s no audit log of the build process \\nchanges, or at least nobod\", \"y monitoring the audit.  \\nThis scenario is a perfect example of a seemingly low -value target that c\", \"an be used to break into the \\nsystem. Once an attacker breaches the perimeter of the system, they ca\", \"n start working on finding \\nways to elevate their permissions to the point that they can cause real \", \"harm anywhere they like.  \\nBuilding secure code  \\n.NET Framework is already a quite secure framework\", \". It avoids some of the pitfalls of unmanaged \\ncode, such as walking off the ends of arrays. Work is\", \" actively done to fix security holes as they\\u2019re \\ndiscovered. There\\u2019s even a bug bounty program  that\", \" pays researchers to find issues in the framework \\nand report them instead of exploiting them.  \\nThe\", \"re are many ways to make .NET code more secure. Following guidelines such as the Secure coding \\nguid\", \"elines for .NET  article is a reasonable step to take to ensure that the code is secure from the \\ngr\", \"ound up. The OWASP top 10  is another invaluable guide to build secure code.  \\nThe build process is \", \"a good place to put scanning tools to detect problems in source code before they \\nmake it into produ\", \"ction. Most every project has dependencies on some other packages. A tool that \\ncan scan for outdate\", \"d packages will catch problems in a nightly build. Even when buil ding Docker \\nimages, it\\u2019s useful t\", \"o check and make sure that the base image doesn\\u2019t have known vulnerabilities. \\nAnother thing to chec\", \"k is that nobody has accidentally checked in credentials.  \\nBuilt -in security  \\nAzure is designed t\", \"o balance usability and security for most users. Different users are going to have \\ndifferent securi\", \"ty requirements, so they need to fine -tune their approach to cloud security. Microsoft \\npublishes a\", \" great deal of security information in t he Trust Center . This resource should be the first \\nstop f\", \"or those professionals interested in understanding how the built -in attack mitigation \\ntechnologies\", \" work.  \\nWithin the Azure portal, the Azure Advisor  is a system that is constantly scanning an envi\", \"ronment and \\nmaking recommendations. Some of these recommendations are designed to save users money,\", \" but \\nothers are designed to identify potentially insecure configurations, such as having a storage \", \"container \\nopen to the world and not protected by a Virtual Network.  \\nAzure network infrastructure \", \" \\nIn an on -premises deployment environment, a great deal of energy is dedicated to setting up \\nnetw\", \"orking. Setting up routers, switches, and the such is complicated work. Networks allow certain \\nreso\", \"urces to talk to other resources and prevent access in some c ases. A frequent network rule is to \\nr\", \"estrict access to the production environment from the development environment on the off chance \\ntha\", \"t a half -developed piece of code runs awry and deletes a swath of data.  \\nOut of the box, most PaaS\", \" Azure resources have only the most basic and permissive networking setup. \\nFor instance, anybody on\", \" the Internet can access an app service. New SQL Server instances typically  \\n151 CHAPTER 9 | Cloud \", \"-native security  \\n come restricted, so that external parties can\\u2019t access them, but the IP address \", \"ranges used by Azure \\nitself are permitted through. So, while the SQL server is protected from exter\", \"nal threats, an attacker \\nonly needs to set up an Azure bridgehead from where they can launch attack\", \"s against all SQL \\ninstances on Azure.  \\nFortunately, most Azure resources can be placed into an Azu\", \"re Virtual Network that allows fine -\\ngrained access control. Similar to the way that on -premises n\", \"etworks establish private networks that \\nare protected from the wider world, virtual networks are is\", \"l ands of private IP addresses that are \\nlocated within the Azure network.  \\n \\nFigure 9 -1. A virtua\", \"l network in Azure.  \\nIn the same way that on -premises networks have a firewall governing access to\", \" the network, you can \\nestablish a similar firewall at the boundary of the virtual network. By defau\", \"lt, all the resources on a \\nvirtual network can still talk to the Internet. It\\u2019s only incoming conne\", \"ctions that require some form of \\nexplicit firewall exception.  \\nWith the network established, inter\", \"nal resources like storage accounts can be set up to only allow for \\naccess by resources that are al\", \"so on the Virtual Network. This firewall provides an extra level of \\nsecurity, should the keys for t\", \"hat storage account be leaked, attackers wouldn\\u2019t be able to connect to \\nit to exploit the leaked ke\", \"ys. This scenario is another example of the principle of least privilege.  \\nThe nodes in an Azure Ku\", \"bernetes cluster can participate in a virtual network just like other resources \\nthat are more nativ\", \"e to Azure. This functionality is called Azure Container Networking Interface . In \\neffect, it alloc\", \"ates a subnet within the virtual network on which virtual machines and container images \\nare allocat\", \"ed.  \\nContinuing down the path of illustrating the principle of least privilege, not every resource \", \"within a \\nVirtual Network needs to talk to every other resource. For instance, in an application tha\", \"t provides a \\n \\n152 CHAPTER 9 | Cloud -native security  \\n web API over a storage account and a SQL d\", \"atabase, it\\u2019s unlikely that the database and the storage \\naccount need to talk to one another. Any d\", \"ata sharing between them would go through the web \\napplication. So, a network security group (NSG)  \", \"could be used to deny traffic between the two \\nservices.  \\nA policy of denying communication between\", \" resources can be annoying to implement, especially \\ncoming from a background of using Azure without\", \" traffic restrictions. On some other clouds, the \\nconcept of network security groups is much more pr\", \"evalent. For inst ance, the default policy on AWS is \\nthat resources can\\u2019t communicate among themsel\", \"ves until enabled by rules in an NSG. While slower \\nto develop this, a more restrictive environment \", \"provides a more secure default. Making use of proper \\nDevOps practices, espec ially using Azure Reso\", \"urce Manager or Terraform  to manage permissions can \\nmake controlling the rules easier.  \\nVirtual N\", \"etworks can also be useful when setting up communication between on -premises and cloud \\nresources. \", \"A virtual private network can be used to seamlessly attach the two networks together. This \\napproach\", \" allows running a virtual network without any sort  of gateway for scenarios where all the \\nusers ar\", \"e on -site. There are a number of technologies that can be used to establish this network. The \\nsimp\", \"lest is to use a site-to-site VPN  that can be established between many routers and Azure. Traffic \\n\", \"is encrypted and tunneled over the Internet at the same cost per byte as any other traffic. For \\nsce\", \"narios where more bandwidth or more security is desirable, Azure offers a service called Express \\nRo\", \"ute  that uses a private circuit between an on -premises network and Azure. It\\u2019s more costly and \\ndi\", \"fficult to establish but also more secure.  \\nRole-based access control for restricting access to Azu\", \"re resources  \\nRBAC is a system that provides an identity to applications running in Azure. Applicat\", \"ions can access \\nresources using this identity instead of or in addition to using keys or passwords.\", \"  \\nSecurity Principals  \\nThe first component in RBAC is a security principal. A security principal c\", \"an be a user, group, service \\nprincipal, or managed identity.  \\n \\nFigure 9 -2. Different types of se\", \"curity principals.  \\n\\u2022 User - Any user who has an account in Azure Active Directory is a user.  \\n\\u2022 G\", \"roup - A collection of users from Azure Active Directory. As a member of a group, a user \\ntakes on t\", \"he roles of that group in addition to their own.  \\n\\u2022 Service principal - A security identity under w\", \"hich services or applications run.  \\n \\n153 CHAPTER 9 | Cloud -native security  \\n \\u2022 Managed identity \", \"- An Azure Active Directory identity managed by Azure. Managed identities \\nare typically used when d\", \"eveloping cloud applications that manage the credentials for \\nauthenticating to Azure services.  \\nTh\", \"e security principal can be applied to most any resource. This aspect means that it\\u2019s possible to \\na\", \"ssign a security principal to a container running within Azure Kubernetes, allowing it to access sec\", \"rets \\nstored in Key Vault. An Azure Function could take on a permission allowing it to talk to an Ac\", \"tive \\nDirectory instance to validate a JWT for a calling user. Once services are enabled with a serv\", \"ice \\nprincipal, their permissions can be managed granularly using roles and scopes.  \\nRoles  \\nA secu\", \"rity principal can take on many roles or, using a more sartorial analogy, wear many hats. Each \\nrole\", \" defines a series of permissions such as \\u201cRead messages from Azure Service Bus endpoint\\u201d. The \\neffec\", \"tive permission set of a security principal is the c ombination of all the permissions assigned to a\", \"ll \\nthe roles that a security principal has. Azure has a large number of built -in roles and users c\", \"an define \\ntheir own roles.  \\n \\nFigure 9 -3. RBAC role definitions.  \\nBuilt into Azure are also a nu\", \"mber of high -level roles such as Owner, Contributor, Reader, and User \\nAccount Administrator. With \", \"the Owner role, a security principal can access all resources and assign \\npermissions to others. A c\", \"ontributor has the same level  of access to all resources but they can\\u2019t assign \\npermissions. A Read\", \"er can only view existing Azure resources and a User Account Administrator can \\nmanage access to Azu\", \"re resources.  \\nMore granular built -in roles such as DNS Zone Contributor  have rights limited to a\", \" single service. \\nSecurity principals can take on any number of roles.  \\n \\n154 CHAPTER 9 | Cloud -na\", \"tive security  \\n Scopes  \\nRoles can be applied to a restricted set of resources within Azure. For in\", \"stance, applying scope to the \\nprevious example of reading from a Service Bus queue, you can narrow \", \"the permission to a single \\nqueue: \\u201cRead messages from Azure Service Bus endpoint bl ah.servicebus.w\", \"indows.net/queue1\\u201d  \\nThe scope can be as narrow as a single resource or it can be applied to an enti\", \"re resource group, \\nsubscription, or even management group.  \\nWhen testing if a security principal h\", \"as certain permission, the combination of role and scope are \\ntaken into account. This combination p\", \"rovides a powerful authorization mechanism.  \\nDeny  \\nPreviously, only \\u201callow\\u201d rules were permitted f\", \"or RBAC. This behavior made some scopes complicated \\nto build. For instance, allowing a security pri\", \"ncipal access to all storage accounts except one required \\ngranting explicit permission to a potenti\", \"ally endles s list of storage accounts. Every time a new storage \\naccount was created, it would have\", \" to be added to this list of accounts. This added management \\noverhead that certainly wasn\\u2019t desirab\", \"le.  \\nDeny rules take precedence over allow rules. Now representing the same \\u201callow all but one\\u201d sco\", \"pe \\ncould be represented as two rules \\u201callow all\\u201d and \\u201cdeny this one specific one\\u201d. Deny rules not o\", \"nly \\nease management but allow for resources that are extra secu re by denying access to everybody. \", \" \\nChecking access  \\nAs you can imagine, having a large number of roles and scopes can make figuring \", \"out the effective \\npermission of a service principal quite difficult. Piling deny rules on top of th\", \"at, only serves to increase \\nthe complexity. Fortunately, there\\u2019s a permissions calculator  that can\", \" show the effective permissions \\nfor any service principal. It\\u2019s typically found under the IAM tab i\", \"n the portal, as shown in Figure 9 -3. \\n \\nFigure 9 -4. Permission calculator for an app service.  \\n \", \"\\n155 CHAPTER 9 | Cloud -native security  \\n Securing secrets  \\nPasswords and certificates are a commo\", \"n attack vector for attackers. Password -cracking hardware can \\ndo a brute -force attack and try to \", \"guess billions of passwords per second. So it\\u2019s important that the \\npasswords that are used to acces\", \"s resources are strong , with a large variety of characters. These \\npasswords are exactly the kind o\", \"f passwords that are near impossible to remember. Fortunately, the \\npasswords in Azure don\\u2019t actuall\", \"y need to be known by any human.  \\nMany security experts suggest  that using a password manager to k\", \"eep your own passwords is the \\nbest approach. While it centralizes your passwords in one location, i\", \"t also allows using highly complex \\npasswords and ensuring they\\u2019re unique for each account. The same\", \" system exists within A zure: a \\ncentral store for secrets.  \\nAzure Key Vault  \\nAzure Key Vault prov\", \"ides a centralized location to store passwords for things such as databases, API \\nkeys, and certific\", \"ates. Once a secret is entered into the Vault, it\\u2019s never shown again and the \\ncommands to extract a\", \"nd view it are purposefully complicate d. The information in the safe is \\nprotected using either sof\", \"tware encryption or FIPS 140 -2 Level 2 validated Hardware Security \\nModules.  \\nAccess to the key va\", \"ult is provided through RBACs, meaning that not just any user can access the \\ninformation in the vau\", \"lt. Say a web application wishes to access the database connection string stored \\nin Azure Key Vault\", \". To gain access, applications need to  run using a service principal. Under this \\nassumed role, the\", \"y can read the secrets from the safe. There are a number of different security \\nsettings that can fu\", \"rther limit the access that an application has to the vault, so that it can\\u2019t update \\nsecrets but on\", \"ly read them.  \\nAccess to the key vault can be monitored to ensure that only the expected applicatio\", \"ns are accessing \\nthe vault. The logs can be integrated back into Azure Monitor, unlocking the abili\", \"ty to set up alerts \\nwhen unexpected conditions are encountered.  \\nKubernetes  \\nWithin Kubernetes, t\", \"here\\u2019s a similar service for maintaining small pieces of secret information. \\nKubernetes Secrets can\", \" be set via the typical kubectl executable.  \\nCreating a secret is as simple as finding the base64 v\", \"ersion of the values to be stored:  \\necho -n 'admin' | base64  \\nYWRtaW4=  \\necho -n '1f2d1e2e67df' | \", \"base64  \\nMWYyZDFlMmU2N2Rm  \\nThen adding it to a secrets file named secret.yml for example that looks\", \" similar to the following \\nexample:  \\napiVersion : v1 \\nkind: Secret \\nmetadata : \\n  name: mysecret   \", \"\\n156 CHAPTER 9 | Cloud -native security  \\n type: Opaque \\ndata: \\n  username : YWRtaW4=  \\n  password :\", \" MWYyZDFlMmU2N2Rm  \\nFinally,  this file can be loaded into Kubernetes by running the following comma\", \"nd:  \\nkubectl apply -f ./secret.yaml  \\nThese secrets can then be mounted into volumes or exposed to \", \"container processes through \\nenvironment variables. The Twelve -factor app  approach to building app\", \"lications suggests using the \\nlowest common denominator to transmit settings to an application. Envi\", \"ronment variables are the \\nlowest common denominator, because they\\u2019re supported no matter the operat\", \"ing system or \\napplication.  \\nAn alternative to use the built -in Kubernetes secrets is to access th\", \"e secrets in Azure Key Vault from \\nwithin Kubernetes. The simplest way to do this is to assign an RB\", \"AC role to the container looking to \\nload secrets. The application can then use the Azure  Key Vault\", \" APIs to access the secrets. However, \\nthis approach requires modifications to the code and doesn\\u2019t \", \"follow the pattern of using environment \\nvariables. Instead, it\\u2019s possible to inject values into a c\", \"ontainer. This approach is actually more secure \\nthan using the Kubernetes secrets directly, as they\", \" can be accessed by users on the cluster.  \\nEncryption in transit and at rest  \\nKeeping data safe is\", \" important whether it\\u2019s on disk or transiting between various different services. \\nThe most effectiv\", \"e way to keep data from leaking is to encrypt it into a format that can\\u2019t be easily read \\nby others.\", \" Azure supports a wide range of encryp tion options.  \\nIn transit  \\nThere are several ways to encryp\", \"t traffic on the network in Azure. The access to Azure services is \\ntypically done over connections \", \"that use Transport Layer Security (TLS). For instance, all the \\nconnections to the Azure APIs requir\", \"e TLS connections. Equally, connections to endpoints in Azure \\nstorage can be restricted to work onl\", \"y over TLS encrypted connections.  \\nTLS is a complicated protocol and simply knowing that the connec\", \"tion is using TLS isn\\u2019t sufficient to \\nensure security. For instance, TLS 1.0 is chronically insecur\", \"e, and TLS 1.1 isn\\u2019t much better. Even within \\nthe versions of TLS, there are various settings  that\", \" can make the connections easier to decrypt. The \\nbest course of action is to check and see if the s\", \"erver connection is using up -to-date and well \\nconfigured protocols.  \\nThis check can be done by an\", \" external service such as SSL labs\\u2019 SSL Server Test. A test run against a \\ntypical Azure endpoint, i\", \"n this case a service bus endpoint, yields a near perfect score of A.  \\nEven services like Azure SQL\", \" databases use TLS encryption to keep data hidden. The interesting part \\nabout encrypting the data i\", \"n transit using TLS is that it isn\\u2019t possible, even for Microsoft, to listen in on \\nthe connection b\", \"etween computers running TLS. This should provide comfort for companies \\nconcerned that their data m\", \"ay be at risk from Microsoft proper or even a state actor with more \\nresources than the standard att\", \"acker.   \\n157 CHAPTER 9 | Cloud -native security  \\n  \\nFigure 9 -5. SSL labs report showing a score o\", \"f A for a Service Bus endpoint.  \\nWhile this level of encryption isn\\u2019t going to be sufficient for al\", \"l time, it should inspire confidence that \\nAzure TLS connections are quite secure. Azure will contin\", \"ue to evolve its security standards as \\nencryption improves. It\\u2019s nice to know that there\\u2019s somebody\", \" watching the security standards and \\nupdating Azure as they improve.  \\nAt rest  \\nIn any application\", \", there are a number of places where data rests on the disk. The application code \\nitself is loaded \", \"from some storage mechanism. Most applications also use some kind of a database \\nsuch as SQL Server,\", \" Cosmos DB, or even the amazingly price -efficient Table Storage. These databases \\nall use heavily e\", \"ncrypted storage to ensure that nobody other than the applications with proper \\npermissions can read\", \" your data. Even the system operators can\\u2019t read data that has been encrypted. \\nSo customers can rem\", \" ain confident their secret information remains secret.  \\nStorage  \\nThe underpinning of much of Azur\", \"e is the Azure Storage engine. Virtual machine disks are mounted \\non top of Azure Storage. Azure Kub\", \"ernetes Service runs on virtual machines that, themselves, are \\nhosted on Azure Storage. Even server\", \"less technologies, such as  Azure Functions Apps and Azure \\nContainer Instances, run out of disk tha\", \"t is part of Azure Storage.  \\nIf Azure Storage is well encrypted, then it provides for a foundation \", \"for most everything else to also \\nbe encrypted. Azure Storage is encrypted  with FIPS 140 -2 complia\", \"nt 256-bit AES . This is a well -\\nregarded encryption technology having been the subject of extensiv\", \"e academic scrutiny over the last \\n20 or so years. At present, there\\u2019s no known practical attack tha\", \"t would allow someone without \\nknowledge of the key to read data encrypted by AES.  \\nBy default, the\", \" keys used for encrypting Azure Storage are managed by Microsoft. There are extensive \\nprotections i\", \"n place to ensure to prevent malicious access to these keys. However, users with \\nparticular encrypt\", \"ion requirements can also provide their own storage keys  that are managed in Azure \\n \\n158 CHAPTER 9\", \" | Cloud -native security  \\n Key Vault. These keys can be revoked at any time, which would effective\", \"ly render the contents of the \\nStorage account using them inaccessible.  \\nVirtual machines use encry\", \"pted storage, but it\\u2019s possible to provide another layer of encryption by \\nusing technologies like B\", \"itLocker on Windows or DM -Crypt on Linux. These technologies mean that \\neven if the disk image was \", \"leaked off of storage, it would remain near impossible to read it.  \\nAzure SQL  \\nDatabases hosted on\", \" Azure SQL use a technology called Transparent Data Encryption (TDE)  to ensure \\ndata remains encryp\", \"ted. It\\u2019s enabled by default on all newly created SQL databases, but must be \\nenabled manually for l\", \"egacy databases. TDE executes real -time encryption and decryption of not just \\nthe database, but al\", \"so the backups and transactio n logs.  \\nThe encryption parameters are stored in the master database \", \"and, on startup, are read into memory \\nfor the remaining operations. This means that the master data\", \"base must remain unencrypted. The \\nactual key is managed by Microsoft. However, users with exactin g\", \" security requirements may provide \\ntheir own key in Key Vault in much the same way as is done for A\", \"zure Storage. The Key Vault provides \\nfor such services as key rotation and revocation.  \\nThe \\u201cTrans\", \"parent\\u201d part of TDS comes from the fact that there aren\\u2019t client changes needed to use an \\nencrypted\", \" database. While this approach provides for good security, leaking the database password is \\nenough \", \"for users to be able to decrypt the data. There\\u2019 s another approach that encrypts individual \\ncolumn\", \"s or tables in a database. Always Encrypted  ensures that at no point the encrypted data \\nappears in\", \" plain text inside the database.  \\nSetting up this tier of encryption requires running through a wiz\", \"ard in SQL Server Management Studio \\nto select the sort of encryption and where in Key Vault to stor\", \"e the associated keys.   \\n159 CHAPTER 9 | Cloud -native security  \\n  \\nFigure 9 -6. Selecting columns\", \" in a table to be encrypted using Always Encrypted.  \\nClient applications that read information from\", \" these encrypted columns need to make special \\nallowances to read encrypted data. Connection strings\", \" need to be updated with Column Encryption \\nSetting=Enabled and client credentials must be retrieved\", \" from the Ke y Vault. The SQL Server client \\nmust then be primed with the column encryption keys. On\", \"ce that is done, the remaining actions use \\nthe standard interfaces to SQL Client. That is, tools li\", \"ke Dapper and Entity Framework, which are built \\non top of SQL Client, w ill continue to work withou\", \"t changes. Always Encrypted may not yet be \\navailable for every SQL Server driver on every language.\", \"  \\nThe combination of TDE and Always Encrypted, both of which can be used with client -specific keys\", \", \\nensures that even the most exacting encryption requirements are supported.  \\nCosmos DB  \\nCosmos D\", \"B is the newest database provided by Microsoft in Azure. It has been built from the ground \\nup with \", \"security and cryptography in mind. AES -256bit encryption is standard for all Cosmos DB \\n \\n160 CHAPT\", \"ER 9 | Cloud -native security  \\n databases and can\\u2019t be disabled. Coupled with the TLS 1.2 requireme\", \"nt for communication, the entire \\nstorage solution is encrypted.  \\n \\nFigure 9 -7. The flow of data e\", \"ncryption within Cosmos DB.  \\nWhile Cosmos DB doesn\\u2019t provide for supplying customer encryption keys\", \", there has been significant \\nwork done by the team to ensure it remains PCI -DSS compliant without \", \"that. Cosmos DB also doesn\\u2019t \\nsupport any sort of single column encryption similar to Azu re SQL\\u2019s A\", \"lways Encrypted yet.  \\nKeeping secure  \\nAzure has all the tools necessary to release a highly secure\", \" product. However, a chain is only as strong \\nas its weakest link. If the applications deployed on t\", \"op of Azure aren\\u2019t developed with a proper \\nsecurity mindset and good security audits, then they b e\", \"come the weak link in the chain. There are \\nmany great static analysis tools, encryption libraries, \", \"and security practices that can be used to ensure \\nthat the software installed on Azure is as secure\", \" as Azure itself. Examples include static analysis tools , \\nencryption libraries , and security prac\", \"tices . \\n \\n161 CHAPTER 10 | DevOps  \\n CHAPTER  10 \\nDevOps  \\nThe favorite mantra of software consulta\", \"nts is to answer \\u201cIt depends\\u201d to any question posed. It isn\\u2019t \\nbecause software consultants are fond\", \" of not taking a position. It\\u2019s because there\\u2019s no one true \\nanswer to any questions in software. Th\", \"ere\\u2019s no absolute right and wrong, but rather a balance \\nbetween opposites.  \\nTake, for instance, th\", \"e two major schools of developing web applications: Single Page Applications \\n(SPAs) versus server -\", \"side applications. On the one hand, the user experience tends to be better with \\nSPAs and the amount\", \" of traffic to the web server can be minimized making it possible to host them \\non something as simp\", \"le as static hosting. On the other hand, SPAs tend to be slower to develop and \\nmore difficult to te\", \"st. Which one is the right choice? Well, it depends on your situation.  \\nCloud -native applications \", \"aren\\u2019t immune to that same dichotomy. They have clear advantages in \\nterms of speed of development, \", \"stability, and scalability, but managing them can be quite a bit more \\ndifficult.  \\nYears ago, it wa\", \"sn\\u2019t uncommon for the process of moving an application from development to \\nproduction to take a mon\", \"th, or even more. Companies released software on a 6 -month or even every \\nyear cadence. One needs t\", \"o look no further than Microsoft Windows to  get an idea for the cadence of \\nreleases that were acce\", \"ptable before the ever -green days of Windows 10. Five years passed between \\nWindows XP and Vista, a\", \" further three between Vista and Windows 7.  \\nIt\\u2019s now fairly well established that being able to re\", \"lease software rapidly gives fast -moving companies \\na huge market advantage over their more sloth -\", \"like competitors. It\\u2019s for that reason that major \\nupdates to Windows 10 are now approximately every\", \" six m onths.  \\nThe patterns and practices that enable faster, more reliable releases to deliver val\", \"ue to the business \\nare collectively known as DevOps. They consist of a wide range of ideas spanning\", \" the entire software \\ndevelopment life cycle from specifying an application all the way up to delive\", \"ring and operating that \\napplication.  \\nDevOps emerged before microservices and it\\u2019s likely that the\", \" movement towards smaller, more fit to \\npurpose services wouldn\\u2019t have been possible without DevOps \", \"to make releasing and operating not \\njust one but many applications in production easier.   \\n162 CHA\", \"PTER 10 | DevOps  \\n  \\nFigure 10 -1 - DevOps and microservices.  \\nThrough good DevOps practices, it\\u2019s\", \" possible to realize the advantages of cloud -native applications \\nwithout suffocating under a mount\", \"ain of work actually operating the applications.  \\nThere\\u2019s no golden hammer when it comes to DevOps.\", \" Nobody can sell a complete and all -\\nencompassing solution for releasing and operating high -qualit\", \"y applications. This is because each \\napplication is wildly different from all others. However, ther\", \"e are tools  that can make DevOps a far less \\ndaunting proposition. One of these tools is known as A\", \"zure DevOps.  \\nAzure DevOps  \\nAzure DevOps has a long pedigree. It can trace its roots back to when \", \"Team Foundation Server first \\nmoved online and through the various name changes: Visual Studio Onlin\", \"e and Visual Studio Team \\nServices. Through the years, however, it has become far more t han its pre\", \"decessors.  \\nAzure DevOps is divided into five major components:  \\n \\nFigure 10 -2 - Azure DevOps.  \\n\", \"Azure Repos  - Source code management that supports the venerable Team Foundation Version \\nControl (\", \"TFVC) and the industry favorite Git. Pull requests provide a way to enable social coding by \\nfosteri\", \"ng discussion of changes as they\\u2019re made.  \\n \\n163 CHAPTER 10 | DevOps  \\n Azure Boards  - Provides an\", \" issue and work item tracking tool that strives to allow users to pick the \\nworkflows that work best\", \" for them. It comes with a number of pre -configured templates including \\nones to support SCRUM and \", \"Kanban styles of development.  \\nAzure Pipelines  - A build and release management system that suppor\", \"ts tight integration with Azure. \\nBuilds can be run on various platforms from Windows to Linux to ma\", \"cOS. Build agents may be \\nprovisioned in the cloud or on -premises.  \\nAzure Test Plans  - No QA pers\", \"on will be left behind with the test management and exploratory \\ntesting support offered by the Test\", \" Plans feature.  \\nAzure Artifacts  - An artifact feed that allows companies to create their own, int\", \"ernal, versions of \\nNuGet, npm, and others. It serves a double purpose of acting as a cache of upstr\", \"eam packages if \\nthere\\u2019s a failure of a centralized repository.  \\nThe top -level organizational unit\", \" in Azure DevOps is known as a Project. Within each project the \\nvarious components, such as Azure A\", \"rtifacts, can be turned on and off. Each of these components \\nprovides different advantages for clou\", \"d -native applications. Th e three most useful are repositories, \\nboards, and pipelines. If users wa\", \"nt to manage their source code in another repository stack, such as \\nGitHub, but still take advantag\", \"e of Azure Pipelines and other components, that\\u2019s perfectly possible.  \\nFortunately, development tea\", \"ms have many options when selecting a repository. One of them is \\nGitHub.  \\nGitHub Actions  \\nFounded\", \" in 2009, GitHub is a widely popular web -based repository for hosting projects, \\ndocumentation, and\", \" code. Many large tech companies, such as Apple, Amazon, Google, and \\nmainstream corporations use Gi\", \"tHub. GitHub uses the open -source, distributed versi on control system \\nnamed Git as its foundation\", \". On top, it then adds its own set of features, including defect tracking, \\nfeature and pull request\", \"s, tasks management, and wikis for each code base.  \\nAs GitHub evolves, it too is adding DevOps feat\", \"ures. For example, GitHub has its own continuous \\nintegration/continuous delivery (CI/CD) pipeline, \", \"called GitHub Actions. GitHub Actions is a \\ncommunity -powered workflow automation tool. It lets Dev\", \"Ops teams in tegrate with their existing \\ntooling, mix and match new products, and hook into their s\", \"oftware lifecycle, including existing CI/CD \\npartners.\\u201d  \\nGitHub has over 40 million users, making i\", \"t the largest host of source code in the world. In October of \\n2018, Microsoft purchased GitHub. Mic\", \"rosoft has pledged that GitHub will remain an open platform  \\nthat any developer can plug into and e\", \"xtend. It continues to operate as an independent company. \\nGitHub offers plans for enterprise, team,\", \" professional, and free accounts.  \\nSource control  \\nOrganizing the code for a cloud -native applica\", \"tion can be challenging. Instead of a single giant \\napplication, the cloud -native applications tend\", \" to be made up of a web of smaller applications that \\ntalk with one another. As with all things in c\", \"omputing, the  best arrangement of code remains an open  \\n164 CHAPTER 10 | DevOps  \\n question. There\", \" are examples of successful applications using different kinds of layouts, but two \\nvariants seem to\", \" have the most popularity.  \\nBefore getting down into the actual source control itself, it\\u2019s probabl\", \"y worth deciding on how many \\nprojects are appropriate. Within a single project, there\\u2019s support for\", \" multiple repositories, and build \\npipelines. Boards are a little more complicated, but  there too, \", \"the tasks can easily be assigned to \\nmultiple teams within a single project. It\\u2019s possible to suppor\", \"t hundreds, even thousands of \\ndevelopers, out of a single Azure DevOps project. Doing so is likely \", \"the best approach as it provides a \\nsingle plac e for all developer to work out of and reduces the c\", \"onfusion of finding that one application \\nwhen developers are unsure in which project in which it re\", \"sides.  \\nSplitting up code for microservices within the Azure DevOps project can be slightly more ch\", \"allenging.  \\n \\nFigure 10 -3 - One vs.  many repositories.  \\nRepository per microservice  \\nAt first g\", \"lance, this approach seems like the most logical approach to splitting up the source code for \\nmicro\", \"services. Each repository can contain the code needed to build the one microservice. The \\nadvantages\", \" to this approach are readily visible:  \\n1. Instructions for building and maintaining the applicatio\", \"n can be added to a README file at \\nthe root of each repository. When flipping through the repositor\", \"ies, it\\u2019s easy to find these \\ninstructions, reducing spin -up time for developers.  \\n2. Every servic\", \"e is located in a logical place, easily found by knowing the name of the service.  \\n3. Builds can ea\", \"sily be set up such that they\\u2019re only triggered when a change is made to the \\nowning repository.  \\n4\", \". The number of changes coming into a repository is limited to the small number of developers \\nworki\", \"ng on the project.  \\n \\n165 CHAPTER 10 | DevOps  \\n 5. Security is easy to set up by restricting the r\", \"epositories to which developers have read and \\nwrite permissions.  \\n6. Repository level settings can\", \" be changed by the owning team with a minimum of discussion \\nwith others.  \\nOne of the key ideas beh\", \"ind microservices is that services should be siloed and separated from each \\nother. When using Domai\", \"n Driven Design to decide on the boundaries for services the services act as \\ntransactional boundari\", \"es. Database updates shouldn\\u2019t spa n multiple services. This collection of related \\ndata is referred\", \" to as a bounded context. This idea is reflected by the isolation of microservice data to \\na databas\", \"e separate and autonomous from the rest of the services. It makes a great deal of sense to \\ncarry th\", \"is idea all the way through to the source code.  \\nHowever, this approach isn\\u2019t without its issues. O\", \"ne of the more gnarly development problems of our \\ntime is managing dependencies. Consider the numbe\", \"r of files that make up the average \\nnode_modules directory. A fresh install of something like creat\", \"e -react -app is likely to bring with it \\nthousands of packages. The question of how to manage these\", \" dependencies is a difficult one.  \\nIf a dependency is updated, then downstream packages must also u\", \"pdate this dependency. \\nUnfortunately, that takes development work so, invariably, the node_modules \", \"directory ends up with \\nmultiple versions of a single package, each one a dependency of some o ther \", \"package that is \\nversioned at a slightly different cadence. When deploying an application, which ver\", \"sion of a \\ndependency should be used? The version that is currently in production? The version that \", \"is currently \\nin Beta but is likely to be in productio n by the time the consumer makes it to produc\", \"tion? Difficult \\nproblems that aren\\u2019t resolved by just using microservices.  \\nThere are libraries th\", \"at are depended upon by a wide variety of projects. By dividing the microservices \\nup with one in ea\", \"ch repository the internal dependencies can best be resolved by using the internal \\nrepository, Azur\", \"e Artifacts. Builds for libraries wi ll push their latest versions into Azure Artifacts for \\ninterna\", \"l consumption. The downstream project must still be manually updated to take a dependency \\non the ne\", \"wly updated packages.  \\nAnother disadvantage presents itself when moving code between services. Alth\", \"ough it would be nice \\nto believe that the first division of an application into microservices is 10\", \"0% correct, the reality is that \\nrarely we\\u2019re so prescient as to make no service d ivision mistakes.\", \" Thus, functionality and the code that \\ndrives it will need to move from service to service: reposit\", \"ory to repository. When leaping from one \\nrepository to another, the code loses its history. There a\", \"re many cases, especially in the event of  an \\naudit, where having full history on a piece of code i\", \"s invaluable.  \\nThe final and most important disadvantage is coordinating changes. In a true microse\", \"rvices \\napplication, there should be no deployment dependencies between services. It should be possi\", \"ble to \\ndeploy services A, B, and C in any order as they have loose coupli ng. In reality, however, \", \"there are \\ntimes when it\\u2019s desirable to make a change that crosses multiple repositories at the same\", \" time. Some \\nexamples include updating a library to close a security hole or changing a communicatio\", \"n protocol \\nused by all services.  \\nTo do a cross -repository change requires a commit to each repos\", \"itory be made in succession. Each \\nchange in each repository will need to be pull -requested and rev\", \"iewed separately. This activity can be \\ndifficult to coordinate.   \\n166 CHAPTER 10 | DevOps  \\n An al\", \"ternative to using many repositories is to put all the source code together in a giant, all knowing,\", \" \\nsingle repository.  \\nSingle repository  \\nIn this approach, sometimes referred to as a monoreposito\", \"ry , all the source code for every service is \\nput into the same repository. At first, this approach\", \" seems like a terrible idea likely to make dealing \\nwith source code unwieldy. There are, however, s\", \"ome marked advantages to working this way.  \\nThe first advantage is that it\\u2019s easier to manage depen\", \"dencies between projects. Instead of relying on \\nsome external artifact feed, projects can directly \", \"import one another. This means that updates are \\ninstant, and conflicting versions are likely to be \", \"fou nd at compile time on the developer\\u2019s workstation. \\nIn effect, shifting some of the integration \", \"testing left.  \\nWhen moving code between projects, it\\u2019s now easier to preserve the history as the fi\", \"les will be \\ndetected as having been moved rather than being rewritten.  \\nAnother advantage is that \", \"wide ranging changes that cross service boundaries can be made in a \\nsingle commit. This activity re\", \"duces the overhead of having potentially dozens of changes to review \\nindividually.  \\nThere are many\", \" tools that can perform static analysis of code to detect insecure programming \\npractices or problem\", \"atic use of APIs. In a multi -repository world, each repository will need to be \\niterated over to fi\", \"nd the problems in them. The single repositor y allows running the analysis all in one \\nplace.  \\nThe\", \"re are also many disadvantages to the single repository approach. One of the most worrying ones \\nis \", \"that having a single repository raises security concerns. If the contents of a repository are leaked\", \" in \\na repository per service model, the amount of code  lost is minimal. With a single repository, \", \"\\neverything the company owns could be lost. There have been many examples in the past of this \\nhappe\", \"ning and derailing entire game development efforts. Having multiple repositories exposes less \\nsurfa\", \"ce area, which is a desirable trait in most security practices.  \\nThe size of the single repository \", \"is likely to become unmanageable rapidly. This presents some \\ninteresting performance implications. \", \"It may become necessary to use specialized tools such as Virtual \\nFile System for Git , which was or\", \"iginally designed to improve the experience for developers on the \\nWindows team.  \\nFrequently the ar\", \"gument for using a single repository boils down to an argument that Facebook or \\nGoogle use this met\", \"hod for source code arrangement. If the approach is good enough for these \\ncompanies, then, surely, \", \"it\\u2019s the correct approach for all compani es. The truth of the matter is that few \\ncompanies operate\", \" on anything like the scale of Facebook or Google. The problems that occur at \\nthose scales are diff\", \"erent from those most developers will face. What is good for the goose may not \\nbe good for the gand\", \"e r. \\nIn the end, either solution can be used to host the source code for microservices. However, in\", \" most \\ncases, the management, and engineering overhead of operating in a single repository isn\\u2019t wor\", \"th the \\nmeager advantages. Splitting code up over multiple repos itories encourages better separatio\", \"n of \\nconcerns and encourages autonomy among development teams.   \\n167 CHAPTER 10 | DevOps  \\n Standa\", \"rd directory structure  \\nRegardless of the single versus multiple repositories debate each service w\", \"ill have its own directory. \\nOne of the best optimizations to allow developers to cross between proj\", \"ects quickly is to maintain a \\nstandard directory structure.  \\n \\nFigure 10 -4 - Standard directory s\", \"tructure.  \\nWhenever a new project is created, a template that puts in place the correct structure s\", \"hould be used. \\nThis template can also include such useful items as a skeleton README file and an az\", \"ure -\\npipelines.yml. In any microservice architecture, a high degree of variance between projects ma\", \"kes bulk \\noperations against the services more difficult.  \\nThere are many tools that can provide te\", \"mplating for an entire directory, containing several source \\ncode directories. Yeoman  is popular in\", \" the JavaScript world and GitHub have recently released \\nRepository Templates , which provide much o\", \"f the same functionality.  \\nTask management  \\nManaging tasks in any project can be difficult. Up fro\", \"nt there are countless questions to be answered \\nabout the sort of workflows to set up to ensure opt\", \"imal developer productivity.  \\nCloud -native applications tend to be smaller than traditional softwa\", \"re products or at least they\\u2019re \\ndivided into smaller services. Tracking of issues or tasks related \", \"to these services remains as important \\nas with any other software project. Nobody wants t o lose tr\", \"ack of some work item or explain to a \\ncustomer that their issue wasn\\u2019t properly logged. Boards are \", \"configured at the project level but within \\neach project, areas can be defined. These allow breaking\", \" down issues across several components. The \\nadvan tage to keeping all the work for the entire appli\", \"cation in one place is that it\\u2019s easy to move work \\nitems from one team to another as they\\u2019re unders\", \"tood better.  \\n \\n168 CHAPTER 10 | DevOps  \\n Azure DevOps comes with a number of popular templates pr\", \"e -configured. In the most basic \\nconfiguration, all that is needed to know is what\\u2019s in the backlog\", \", what people are working on, and \\nwhat\\u2019s done. It\\u2019s important to have this visibility into the proc\", \"ess  of building software, so that work \\ncan be prioritized and completed tasks reported to the cust\", \"omer. Of course, few software projects \\nstick to a process as simple as to do, doing, and done. It d\", \"oesn\\u2019t take long for people to start adding \\nsteps like QA or D etailed Specification to the process\", \".  \\nOne of the more important parts of Agile methodologies is self -introspection at regular interva\", \"ls. \\nThese reviews are meant to provide insight into what problems the team is facing and how they c\", \"an \\nbe improved. Frequently, this means changing the flow of is sues and features through the \\ndevel\", \"opment process. So, it\\u2019s perfectly healthy to expand the layouts of the boards with additional \\nstag\", \"es.  \\nThe stages in the boards aren\\u2019t the only organizational tool. Depending on the configuration o\", \"f the \\nboard, there\\u2019s a hierarchy of work items. The most granular item that can appear on a board i\", \"s a task. \\nOut of the box a task contains fields for a title, de scription, a priority, an estimate \", \"of the amount of \\nwork remaining and the ability to link to other work items or development items (b\", \"ranches, commits, \\npull requests, builds, and so forth). Work items can be classified into different\", \" areas of the applicati on \\nand different iterations (sprints) to make finding them easier.  \\n \\nFigu\", \"re 10 -5 - Task in Azure DevOps.  \\nThe description field supports the normal styles you\\u2019d expect (bo\", \"ld, italic underscore and strike \\nthrough) and the ability to insert images. This makes it a powerfu\", \"l tool for use when specifying work \\nor bugs.  \\nTasks can be rolled up into features, which define a\", \" larger unit of work. Features, in turn, can be rolled \\nup into epics . Classifying tasks in this hi\", \"erarchy makes it much easier to understand how close a large \\nfeature is to rolling out.  \\n \\n169 CHA\", \"PTER 10 | DevOps  \\n  \\nFigure 10 -6 - Work item in Azure DevOps.  \\nThere are different kinds of views\", \" into the issues in Azure Boards. Items that aren\\u2019t yet scheduled \\nappear in the backlog. From there\", \", they can be assigned to a sprint. A sprint is a time box during \\nwhich it\\u2019s expected some quantity\", \" of work will be complet ed. This work can include tasks but also the \\nresolution of tickets. Once t\", \"here, the entire sprint can be managed from the Sprint board section. This \\nview shows how work is p\", \"rogressing and includes a burn down chart to give an ever -updating \\nestimate of if t he sprint will\", \" be successful.  \\n \\nFigure 10 -7 - Board in Azure DevOps.  \\nBy now, it should be apparent that there\", \"\\u2019s a great deal of power in the Boards in Azure DevOps. For \\ndevelopers, there are easy views of wha\", \"t is being worked on. For project managers views into \\nupcoming work as well as an overview of exist\", \"ing work. For mana gers, there are plenty of reports \\nabout resourcing and capacity. Unfortunately, \", \"there\\u2019s nothing magical about cloud -native applications \\nthat eliminate the need to track work. But\", \" if you must track work, there are a few places where the \\nexperience is better  than in Azure DevOp\", \"s.  \\nCI/CD pipelines  \\nAlmost no change in the software development life cycle has been so revolutio\", \"nary as the advent of \\ncontinuous integration (CI) and continuous delivery (CD). Building and runnin\", \"g automated tests \\nagainst the source code of a project as soon as a change is ch ecked in catches m\", \"istakes early. Prior to \\nthe advent of continuous integration builds, it wouldn\\u2019t be uncommon to pul\", \"l code from the \\n \\n170 CHAPTER 10 | DevOps  \\n repository and find that it didn\\u2019t pass tests or could\", \"n\\u2019t even be built. This resulted in tracking down \\nthe source of the breakage.  \\nTraditionally shipp\", \"ing software to the production environment required extensive documentation and \\na list of steps. Ea\", \"ch one of these steps needed to be manually completed in a very error prone \\nprocess.  \\n \\nFigure 10 \", \"-8 - Checklist.  \\nThe sister of continuous integration is continuous delivery in which the freshly b\", \"uilt packages are \\ndeployed to an environment. The manual process can\\u2019t scale to match the speed of \", \"development so \\nautomation becomes more important. Checklists are replaced b y scripts that can exec\", \"ute the same \\ntasks faster and more accurately than any human.  \\nThe environment to which continuous\", \" delivery delivers might be a test environment or, as is being \\ndone by many major technology compan\", \"ies, it could be the production environment. The latter \\nrequires an investment in high -quality tes\", \"ts that can give confide nce that a change isn\\u2019t going to \\nbreak production for users. In the same w\", \"ay that continuous integration caught issues in the code \\nearly continuous delivery catches issues i\", \"n the deployment process early.  \\nThe importance of automating the build and delivery process is acc\", \"entuated by cloud -native \\napplications. Deployments happen more frequently and to more environments\", \" so manually deploying \\nborders on impossible.  \\nAzure Builds  \\nAzure DevOps provides a set of tools\", \" to make continuous integration and deployment easier than \\never. These tools are located under Azur\", \"e Pipelines. The first of them is Azure Builds, which is a tool \\nfor running YAML -based build defin\", \"itions at scale. Users can either bring their own build machines \\n(great for if the build requires a\", \" meticulously set up environment) or use a machine from a constantly \\nrefreshed pool of Azure hosted\", \" virtual machines. These hosted build agents come pre -installed with a \\n \\n171 CHAPTER 10 | DevOps  \", \"\\n wide range of development tools for not just .NET development but for everything from Java to \\nPyt\", \"hon to iPhone development.  \\nDevOps includes a wide range of out of the box build definitions that c\", \"an be customized for any build. \\nThe build definitions are defined in a file called azure -pipelines\", \".yml and checked into the repository so \\nthey can be versioned along with the source cod e. This mak\", \"es it much easier to make changes to the \\nbuild pipeline in a branch as the changes can be checked i\", \"nto just that branch. An example azure -\\npipelines.yml for building an ASP.NET web application on fu\", \"ll framework is show in Figure 10 -9. \\nname: $(rev:r)  \\n \\nvariables : \\n  version: 9.2.0.$(Build.Buil\", \"dNumber)  \\n  solution : Portals.sln  \\n  artifactName : drop \\n  buildPlatform : any cpu \\n  buildConfi\", \"guration : release \\n \\npool: \\n  name: Hosted VisualStudio  \\n  demands: \\n  - msbuild \\n  - visualstudio\", \"  \\n  - vstest \\n \\nsteps: \\n- task: NuGetToolInstaller@0  \\n  displayName : 'Use NuGet 4.4.1'  \\n  inputs\", \": \\n    versionSpec : 4.4.1 \\n \\n- task: NuGetCommand@2  \\n  displayName : 'NuGet restore'  \\n  inputs: \\n\", \"    restoreSolution : '$(solution)'  \\n \\n- task: VSBuild@1  \\n  displayName : 'Build solution'  \\n  inp\", \"uts: \\n    solution : '$(solution)'  \\n    msbuildArgs : '-p:DeployOnBuild=true -p:WebPublishMethod=Pa\", \"ckage -\\np:PackageAsSingleFile=true -p:SkipInvalidConfigurations=true -\\np:PackageLocation=\\\"$(build.ar\", \"tifactstagingdirectory) \\\\\\\\\\\"' \\n    platform : '$(buildPlatform)'  \\n    configuration : '$(buildConfig\", \"uration)'  \\n \\n- task: VSTest@2  \\n  displayName : 'Test Assemblies'  \\n  inputs: \\n    testAssemblyVer2\", \" : | \\n     **\\\\$(buildConfiguration) \\\\**\\\\*test*.dll  \\n     !**\\\\obj\\\\** \\n     !**\\\\*testadapter.dll  \\n  \", \"  platform : '$(buildPlatform)'  \\n    configuration : '$(buildConfiguration)'  \\n \\n- task: CopyFiles@\", \"2  \\n  displayName : 'Copy UI Test Files to: $(build.artifactstagingdirectory)'  \\n  inputs:  \\n172 CHA\", \"PTER 10 | DevOps  \\n     SourceFolder : UITests \\n    TargetFolder : '$(build.artifactstagingdirectory\", \")/uitests'  \\n \\n- task: PublishBuildArtifacts@1  \\n  displayName : 'Publish Artifact'  \\n  inputs: \\n   \", \" PathtoPublish : '$(build.artifactstagingdirectory)'  \\n    ArtifactName : '$(artifactName)'  \\n  cond\", \"ition : succeededOrFailed()  \\nFigure 10 -9 - A sample azure -pipelines.yml  \\nThis build definition u\", \"ses a number of built -in tasks that make creating builds as simple as building a \\nLego set (simpler\", \" than the giant Millennium Falcon). For instance, the NuGet task restores NuGet \\npackages, while the\", \" VSBuild task calls the Visual Studi o build tools to perform the actual compilation. \\nThere are hun\", \"dreds of different tasks available in Azure DevOps, with thousands more that are \\nmaintained by the \", \"community. It\\u2019s likely that no matter what build tasks you\\u2019re looking to run, \\nsomebody has buil t o\", \"ne already.  \\nBuilds can be triggered manually, by a check -in, on a schedule, or by the completion \", \"of another build. \\nIn most cases, building on every check -in is desirable. Builds can be filtered s\", \"o that different builds run \\nagainst different parts of the repository or against different branches\", \". This allows for scenarios like \\nrunning fast builds with reduced testing on pull requests and runn\", \"ing a full regression suite against \\nthe trunk on a nightly basis.  \\nThe end result of a build is a \", \"collection of files known as build artifacts. These artifacts can be passed \\nalong to the next step \", \"in the build process or added to an Azure Artifacts feed, so they can be \\nconsumed by other builds. \", \" \\nAzure DevOps releases  \\nBuilds take care of compiling the software into a shippable package, but t\", \"he artifacts still need to be \\npushed out to a testing environment to complete continuous delivery. \", \"For this, Azure DevOps uses a \\nseparate tool called Releases. The Releases tool make s use of the sa\", \"me tasks\\u2019 library that were \\navailable to the Build but introduce a concept of \\u201cstages\\u201d. A stage is \", \"an isolated environment into \\nwhich the package is installed. For instance, a product might make use\", \" of a development, a QA, and a \\nproduction e nvironment. Code is continuously delivered into the dev\", \"elopment environment where \\nautomated tests can be run against it. Once those tests pass the release\", \" moves onto the QA \\nenvironment for manual testing. Finally, the code is pushed to production where \", \"it\\u2019 s visible to \\neverybody.  \\n \\nFigure 10 -10 - Release pipeline  \\nEach stage in the build can be a\", \"utomatically triggered by the completion of the previous phase. In \\nmany cases, however, this isn\\u2019t \", \"desirable. Moving code into production might require approval from \\nsomebody. The Releases tool supp\", \"orts this by allowing appr overs at each step of the release pipeline. \\nRules can be set up such tha\", \"t a specific person or group of people must sign off on a release before it \\n \\n173 CHAPTER 10 | DevO\", \"ps  \\n makes into production. These gates allow for manual quality checks and also for compliance wit\", \"h any \\nregulatory requirements related to control what goes into production.  \\nEverybody gets a buil\", \"d pipeline  \\nThere\\u2019s no cost to configuring many build pipelines, so it\\u2019s advantageous to have at le\", \"ast one build \\npipeline per microservice. Ideally, microservices are independently deployable to any\", \" environment so \\nhaving each one able to be released via its own pipeli ne without releasing a mass \", \"of unrelated code is \\nperfect. Each pipeline can have its own set of approvals allowing for variatio\", \"ns in build process for \\neach service.  \\nVersioning releases  \\nOne drawback to using the Releases fu\", \"nctionality is that it can\\u2019t be defined in a checked -in azure -\\npipelines.yml file. There are many \", \"reasons you might want to do that from having per -branch release \\ndefinitions to including a releas\", \"e skeleton in your proje ct template. Fortunately, work is ongoing to \\nshift some of the stages supp\", \"ort into the Build component. This will be known as multi -stage build \\nand the first version is ava\", \"ilable now ! \\nFeature flags  \\nIn chapter 1, we affirmed that cloud native is much about speed and ag\", \"ility. Users expect rapid \\nresponsiveness, innovative features, and zero downtime. Feature flags are\", \" a modern deployment \\ntechnique that helps increase agility for cloud -native applications. They ena\", \"ble  you to deploy new \\nfeatures into a production environment, but restrict their availability. Wit\", \"h the flick of a switch, you can \\nactivate a new feature for specific users without restarting the a\", \"pp or deploying new code. They \\nseparate the release of new fea tures from their code deployment.  \\n\", \"Feature flags are built upon conditional logic that control visibility of functionality for users at\", \" run \\ntime. In modern cloud -native systems, it\\u2019s common to deploy new features into production earl\", \"y, but \\ntest them with a limited audience. As confidence in creases, the feature can be incrementall\", \"y rolled out \\nto wider audiences.  \\nOther use cases for feature flags include:  \\n\\u2022 Restrict premium \", \"functionality to specific customer groups willing to pay higher subscription \\nfees. \\n\\u2022 Stabilize a s\", \"ystem by quickly deactivating a problem feature, avoiding the risks of a rollback or \\nimmediate hotf\", \"ix.  \\n\\u2022 Disable an optional feature with high resource consumption during peak usage periods.  \\n\\u2022 Co\", \"nduct experimental feature releases to small user segments to validate feasibility and \\npopularity. \", \" \\nFeature flags also promote trunk -based development. It\\u2019s a source -control branching model where \", \"\\ndevelopers collaborate on features in a single branch. The approach minimizes the risk and complexi\", \"ty \\nof merging large numbers of long -running feature branches.  Features are unavailable until acti\", \"vated.   \\n174 CHAPTER 10 | DevOps  \\n Implementing feature flags  \\nAt its core, a feature flag is a r\", \"eference to a simple decision object. It returns a Boolean state of on or \\noff. The flag typically w\", \"raps a block of code that encapsulates a feature capability. The state of the flag \\ndetermines wheth\", \"er that code block executes for a given user. Figure 10 -11 shows the \\nimplementation.  \\nif (feature\", \"Flag ) { \\n    // Run this code block if the featureFlag value is true  \\n} else { \\n    // Run this co\", \"de block if the featureFlag value is false  \\n} \\nFigure 10 -11 - Simple feature flag implementation. \", \" \\nNote how this approach separates the decision logic from the feature code.  \\nIn chapter 1, we disc\", \"ussed the Twelve -Factor App. The guidance recommended keeping configuration \\nsettings external from\", \" application executable code. When needed, settings can be read in from the \\nexternal source. Featur\", \"e flag configuration values should als o be independent from their codebase. By \\nexternalizing flag \", \"configuration in a separate repository, you can change flag state without modifying \\nand redeploying\", \" the application.  \\nAzure App Configuration  provides a centralized repository for feature flags. Wi\", \"th it, you define \\ndifferent kinds of feature flags and manipulate their states quickly and confiden\", \"tly. You add the App \\nConfiguration client libraries to your application to enable feature flag func\", \"ti onality. Various \\nprogramming language frameworks are supported.  \\nFeature flags can be easily im\", \"plemented in an ASP.NET Core service . Installing the .NET Feature \\nManagement libraries and App Con\", \"figuration provider enable you to declaratively add feature flags to \\nyour code. They enable Feature\", \"Gate attributes so that you don\\u2019t have to manually write if statements \\nacross your codebase.  \\nOnce\", \" configured in your Startup class, you can add feature flag functionality at the controller, action,\", \" \\nor middleware level. Figure 10 -12 presents controller and action implementation:  \\n[FeatureGate (\", \"MyFeatureFlags .FeatureA )] \\npublic class ProductController : Controller  \\n{ \\n    ... \\n} \\n \\n \\n[Featu\", \"reGate (MyFeatureFlags .FeatureA )] \\npublic IActionResult UpdateProductStatus () \\n{ \\n    return Obje\", \"ctResult (ProductDto ); \\n} \\nFigure 10 -12 - Feature flag implementation in a controller and action. \", \" \\nIf a feature flag is disabled, the user will receive a 404 (Not Found) status code with no respons\", \"e body.  \\nFeature flags can also be injected directly into C# classes. Figure 10 -13 shows feature f\", \"lag injection:   \\n175 CHAPTER 10 | DevOps  \\n public class ProductController : Controller  \\n{ \\n    pr\", \"ivate readonly  IFeatureManager _featureManager ; \\n \\n    public ProductController (IFeatureManager f\", \"eatureManager ) \\n    { \\n        _featureManager = featureManager ; \\n    } \\n} \\nFigure 10 -13 - Featur\", \"e flag injection into a class.  \\nThe Feature Management libraries manage the feature flag lifecycle \", \"behind the scenes. For example, \\nto minimize high numbers of calls to the configuration store, the l\", \"ibraries cache flag states for a \\nspecified duration. They can guarantee the immutability of flag st\", \"ates during a request ca ll. They also \\noffer a Point -in-time snapshot. You can reconstruct the his\", \"tory of any key -value and provide its past \\nvalue at any moment within the previous seven days.  \\nI\", \"nfrastructure as code  \\nCloud -native systems embrace microservices, containers, and modern system d\", \"esign to achieve speed \\nand agility. They provide automated build and release stages to ensure consi\", \"stent and quality code. \\nBut, that\\u2019s only part of the story. How do you provision t he cloud environ\", \"ments upon which these \\nsystems run?  \\nModern cloud -native applications embrace the widely accepted\", \" practice of Infrastructure as Code , or \\nIaC. With IaC, you automate platform provisioning. You ess\", \"entially apply software engineering \\npractices such as testing and versioning to your DevOps practic\", \"es. Your infrastructure and \\ndeployments are automated, consistent, and repeatable. Just as con tinu\", \"ous delivery automated the \\ntraditional model of manual deployments, Infrastructure as Code (IaC) is\", \" evolving how application \\nenvironments are managed.  \\nTools like Azure Resource Manager (ARM), Terr\", \"aform, and the Azure Command Line Interface (CLI) \\nenable you to declaratively script the cloud infr\", \"astructure you require.  \\nAzure Resource Manager templates  \\nARM stands for Azure Resource Manager .\", \" It\\u2019s an API provisioning engine that is built into Azure and \\nexposed as an API service. ARM enable\", \"s you to deploy, update, delete, and manage the resources \\ncontained in Azure resource group in a si\", \"ngle, coordinated operation. You provide the engine with a \\nJSON -based template that specifies the \", \"resources you require and their configuration. ARM \\nautomatically orchestrates the deployment in the\", \" correct order respecting dependencies. The engine \\nensures idempotency. If a desired resource alrea\", \"dy exists with th e same configuration, provisioning \\nwill be ignored.  \\nAzure Resource Manager temp\", \"lates are a JSON -based language for defining various resources in \\nAzure. The basic schema looks so\", \"mething like Figure 10 -14. \\n{ \\n  \\\"$schema\\\" : \\\"https://schema.management.azure.com/schemas/2015 -01-\", \" \\n176 CHAPTER 10 | DevOps  \\n 01/deploymentTemplate.json#\\\" , \\n  \\\"contentVersion\\\" : \\\"\\\", \\n  \\\"apiProfile\", \"\\\" : \\\"\\\", \\n  \\\"parameters\\\" : {  }, \\n  \\\"variables\\\" : {  }, \\n  \\\"functions\\\" : [  ], \\n  \\\"resources\\\" : [  ],\", \" \\n  \\\"outputs\\\" : {  } \\n} \\nFigure 10 -14 - The schema for a Resource Manager template  \\nWithin this te\", \"mplate, one might define a storage container inside the resources section like so:  \\n\\\"resources\\\":  [\", \" \\n    { \\n      \\\"type\\\": \\\"Microsoft.Storage/storageAccounts\\\" , \\n      \\\"name\\\": \\\"[variables('storageAcco\", \"untName')]\\\" , \\n      \\\"location\\\" : \\\"[parameters('location')]\\\" , \\n      \\\"apiVersion\\\" : \\\"2018-07-01\\\", \\n\", \"      \\\"sku\\\": { \\n        \\\"name\\\": \\\"[parameters('storageAccountType')]\\\"  \\n      }, \\n      \\\"kind\\\": \\\"Stor\", \"ageV2\\\" , \\n      \\\"properties\\\" : {} \\n    } \\n  ], \\nFigure 10 -15 - An example of a storage account defi\", \"ned in a Resource Manager template  \\nAn ARM template can be parameterized with dynamic environment a\", \"nd configuration information. \\nDoing so enables it to be reused to define different environments, su\", \"ch as development, QA, or \\nproduction. Normally, the template creates all resources within a si ngle\", \" Azure resource group. It\\u2019s \\npossible to define multiple resource groups in a single Resource Manage\", \"r template, if needed. You \\ncan delete all resources in an environment by deleting the resource grou\", \"p itself. Cost analysis can also \\nbe run at the resourc e group level, allowing for quick accounting\", \" of how much each environment is \\ncosting.  \\nThere are many examples of ARM templates available in t\", \"he Azure Quickstart Templates  project on \\nGitHub. They can help accelerate creating a new template \", \"or modifying an existing one.  \\nResource Manager templates can be run in many of ways. Perhaps the s\", \"implest way is to simply paste \\nthem into the Azure portal. For experimental deployments, this metho\", \"d can be quick. They can also be \\nrun as part of a build or release process in Azure DevOp s. There \", \"are tasks that will leverage \\nconnections into Azure to run the templates. Changes to Resource Manag\", \"er templates are applied \\nincrementally, meaning that to add a new resource requires just adding it \", \"to the template. The tooling \\nwill reconcile diffe rences between the current resources and those de\", \"fined in the template. Resources \\nwill then be created or altered so they match what is defined in t\", \"he template.  \\nTerraform  \\nCloud -native applications are often constructed to be cloud agnostic. Be\", \"ing so means the application \\nisn\\u2019t tightly coupled to a particular cloud vendor and can be deployed\", \" to any public cloud.   \\n177 CHAPTER 10 | DevOps  \\n Terraform  is a commercial templating tool that \", \"can provision cloud -native applications across all the \\nmajor cloud players: Azure, Google Cloud Pl\", \"atform, AWS, and AliCloud. Instead of using JSON as the \\ntemplate definition language, it uses the s\", \"lightly more terse HC L (Hashicorp Configuration Language).  \\nAn example Terraform file that does th\", \"e same as the previous Resource Manager template (Figure 10 -\\n15) is shown in Figure 10 -16: \\nprovid\", \"er \\\"azurerm\\\" {  \\n  version = \\\"=1.28.0\\\"  \\n} \\n \\nresource \\\"azurerm_resource_group\\\" \\\"testrg\\\" {  \\n  name \", \"    = \\\"production\\\"  \\n  location = \\\"West US\\\"  \\n} \\n \\nresource \\\"azurerm_storage_account\\\" \\\"testsa\\\" {  \\n \", \" name                     = \\\"${var.storageAccountName}\\\"  \\n  resource _group_name      = \\\"${azurerm_r\", \"esource_group.testrg.name}\\\"  \\n  location                 = \\\"${var.region}\\\"  \\n  account_tier         \", \"    = \\\"${var.tier}\\\"  \\n  account_replication_type = \\\"${var.replicationType}\\\"  \\n \\n} \\nFigure 10 -16 - A\", \"n example of a Resource Manager template  \\nTerraform also provides intuitive error messages for prob\", \"lem templates. There\\u2019s also a handy validate \\ntask that can be used in the build phase to catch temp\", \"late errors early.  \\nAs with Resource Manager templates, command -line tools are available to deploy\", \" Terraform \\ntemplates. There are also community -created tasks in Azure Pipelines that can validate \", \"and apply \\nTerraform templates.  \\nSometimes Terraform and ARM templates output meaningful values, su\", \"ch as a connection string to a \\nnewly created database. This information can be captured in the buil\", \"d pipeline and used in \\nsubsequent tasks.  \\nAzure CLI Scripts and Tasks  \\nFinally, you can leverage \", \"Azure CLI  to declaratively script your cloud infrastructure. Azure CLI scripts \\ncan be created, fou\", \"nd, and shared to provision and configure almost any Azure resource. The CLI is \\nsimple to use with \", \"a gentle learning curve. Scripts are executed within either PowerShe ll or Bash. \\nThey\\u2019re also strai\", \"ghtforward to debug, especially when compared with ARM templates.  \\nAzure CLI scripts work well when\", \" you need to tear down and redeploy your infrastructure. Updating \\nan existing environment can be tr\", \"icky. Many CLI commands aren\\u2019t idempotent. That means they\\u2019ll \\nrecreate the resource each time they\\u2019\", \"re run, even if the resour ce already exists. It\\u2019s always possible to \\nadd code that checks for the \", \"existence of each resource before creating it. But, doing so, your script \\ncan become bloated and di\", \"fficult to manage.  \\nThese scripts can also be embedded in Azure DevOps pipelines as Azure CLI tasks\", \". Executing the \\npipeline invokes the script.   \\n178 CHAPTER 10 | DevOps  \\n Figure 10 -17 shows a YA\", \"ML snippet that lists the version of Azure CLI and the details of the \\nsubscription. Note how Azure \", \"CLI commands are included in an inline script.  \\n- task: AzureCLI@2  \\n  displayName : Azure CLI  \\n  \", \"inputs: \\n    azureSubscription : <Name of the Azure Resource Manager service connection>  \\n    scrip\", \"tType : ps \\n    scriptLocation : inlineScript  \\n    inlineScript : | \\n      az --version \\n      az a\", \"ccount show  \\nFigure 10 -17 - Azure CLI script  \\nIn the article, What is Infrastructure as Code , Au\", \"thor Sam Guckenheimer describes how, \\u201cTeams who \\nimplement IaC can deliver stable environments rapid\", \"ly and at scale. Teams avoid manual \\nconfiguration of environments and enforce consistency by repres\", \"enting the desired state of their \\nenvironments via cod e. Infrastructure deployments with IaC are r\", \"epeatable and prevent runtime issues \\ncaused by configuration drift or missing dependencies. DevOps \", \"teams can work together with a \\nunified set of practices and tools to deliver applications and their\", \" supporting in frastructure rapidly, \\nreliably, and at scale.\\u201d  \\nCloud Native Application Bundles  \\n\", \"A key property of cloud -native applications is that they leverage the capabilities of the cloud to \", \"speed \\nup development. This design often means that a full application uses different kinds of techn\", \"ologies. \\nApplications may be shipped in Docker containers,  some services may use Azure Functions, \", \"while \\nother parts may run directly on virtual machines allocated on large metal servers with hardwa\", \"re GPU \\nacceleration. No two cloud -native applications are the same, so it\\u2019s been difficult to prov\", \"ide a single \\nmechan ism for shipping them.  \\nThe Docker containers may run on Kubernetes using a He\", \"lm Chart for deployment. The Azure \\nFunctions may be allocated using Terraform templates. Finally, t\", \"he virtual machines may be allocated \\nusing Terraform but built out using Ansible. This is a large v\", \"arie ty of technologies and there has been \\nno way to package them all together into a reasonable pa\", \"ckage. Until now.  \\nCloud Native Application Bundles (CNABs) are a joint effort by many community -m\", \"inded companies \\nsuch as Microsoft, Docker, and HashiCorp to develop a specification to package dist\", \"ributed \\napplications.  \\nThe effort was announced in December of 2018, so there\\u2019s still a fair bit o\", \"f work to do to expose the \\neffort to the greater community. However, there\\u2019s already an open specif\", \"ication  and a reference \\nimplementation known as Duffle . This tool, which was written in Go, is a \", \"joint effort between Docker \\nand Microsoft.  \\nThe CNABs can contain different kinds of installation \", \"technologies. This aspect allows things like Helm \\nCharts, Terraform templates, and Ansible Playbook\", \"s to coexist in the same package. Once built, the \\npackages are self -contained and portable; they c\", \"an be  installed from a USB stick. The packages are \\ncryptographically signed to ensure they origina\", \"te from the party they claim.   \\n179 CHAPTER 10 | DevOps  \\n The core of a CNAB is a file called bund\", \"le.json. This file defines the contents of the bundle, be they \\nTerraform or images or anything else\", \". Figure 11 -9 defines a CNAB that invokes some Terraform. \\nNotice, however, that it actually define\", \"s an invocation ima ge that is used to invoke the Terraform. \\nWhen packaged up, the Docker file that\", \" is located in the cnab  directory is built into a Docker image, \\nwhich will be included in the bund\", \"le. Having Terraform installed inside a Docker container in the \\nbundle means th at users don\\u2019t need\", \" to have Terraform installed on their machine to run the bundling.  \\n{ \\n    \\\"name\\\": \\\"terraform\\\" , \\n \", \"   \\\"version\\\" : \\\"0.1.0\\\", \\n    \\\"schemaVersion\\\" : \\\"v1.0.0-WD\\\", \\n    \\\"parameters\\\" : { \\n        \\\"backend\\\"\", \" : { \\n            \\\"type\\\": \\\"boolean\\\" , \\n            \\\"defaultValue\\\" : false, \\n            \\\"destination\", \"\\\" : { \\n                \\\"env\\\": \\\"TF_VAR_backend\\\"  \\n            } \\n        } \\n    }, \\n    \\\"invocationIm\", \"ages\\\" : [ \\n        { \\n        \\\"imageType\\\" : \\\"docker\\\" , \\n        \\\"image\\\": \\\"cnab/terraform:latest\\\"  \\n \", \"       } \\n    ], \\n    \\\"credentials\\\" : { \\n        \\\"tenant_id\\\" : { \\n            \\\"env\\\": \\\"TF_VAR_tenant_\", \"id\\\"  \\n        }, \\n        \\\"client_id\\\" : { \\n            \\\"env\\\": \\\"TF_VAR_client_id\\\"  \\n        }, \\n     \", \"   \\\"client_secret\\\" : { \\n            \\\"env\\\": \\\"TF_VAR_client_secret\\\"  \\n        }, \\n        \\\"subscriptio\", \"n_id\\\" : { \\n            \\\"env\\\": \\\"TF_VAR_subscription_id\\\"  \\n        }, \\n        \\\"ssh_authorized_key\\\" : \", \"{ \\n            \\\"env\\\": \\\"TF_VAR_ssh_authorized_key\\\"  \\n        } \\n    }, \\n    \\\"actions\\\" : { \\n        \\\"s\", \"tatus\\\" : { \\n            \\\"modifies\\\" : true \\n        } \\n    } \\n} \\nFigure 10 -18 - An example Terraform\", \" file  \\nThe bundle.json also defines a set of parameters that are passed down into the Terraform. \\nP\", \"arameterization of the bundle allows for installation in various different environments.  \\nThe CNAB \", \"format is also flexible, allowing it to be used against any cloud. It can even be used against \\non-p\", \"remises solutions such as OpenStack .  \\n180 CHAPTER 10 | DevOps  \\n DevOps Decisions  \\nThere are so m\", \"any great tools in the DevOps space these days and even more fantastic books and \\npapers on how to s\", \"ucceed. A favorite book to get started on the DevOps journey is The Phoenix \\nProject , which follows\", \" the transformation of a fictional company from NoOps to DevOps. One thing is \\nfor certain: DevOps i\", \"s no longer a \\u201cnice to have\\u201d when deploying complex, Cloud Native Applications. \\nIt\\u2019s a requirement \", \"and should be planned for and resourced at the start of any project.  \\nReferences  \\n\\u2022 Azure DevOps  \", \"\\n\\u2022 Azure Resource Manager  \\n\\u2022 Terraform  \\n\\u2022 Azure CLI   \\n181 CHAPTER 11 | Summary: Architecting clou\", \"d -native apps  \\n CHAPTER  11 \\nSummary: Architecting \\ncloud -native apps  \\nIn summary, here are impo\", \"rtant conclusions from this guide:  \\n\\u2022 Cloud -native  is about designing modern applications that em\", \"brace rapid change, large scale, \\nand resilience, in modern, dynamic environments such as public, pr\", \"ivate, and hybrid clouds.  \\n\\u2022 The Cloud Native Computing Foundation  (CNCF)  is an influential open \", \"-source consortium \\nof over 300 major corporations. It\\u2019s responsible for driving the adoption of clo\", \"ud -native \\ncomputing across technology and cloud stacks.  \\n\\u2022 CNCF guidelines  recommend that cloud \", \"-native applications embrace six important pillars as \\nshown in Figure 11 -1: \\n \\nFigure 11 -1. Cloud\", \" -native foundational pillars  \\n\\u2022 These cloud -native pillars include:  \\n\\u2013 The cloud and its underly\", \"ing service model  \\n\\u2013 Modern design principles  \\n\\u2013 Microservices  \\n\\u2013 Containerization and container \", \"orchestration  \\n\\u2013 Cloud -based backing services, such as databases and message brokers  \\n\\u2013 Automatio\", \"n, including Infrastructure as Code and code deployment  \\n \\n182 CHAPTER 11 | Summary: Architecting c\", \"loud -native apps  \\n \\u2022 Kubernetes  is the hosting environment of choice for most cloud -native appli\", \"cations. Smaller, \\nsimple services are sometimes hosted in serverless platforms, such as Azure Funct\", \"ions. Among \\nmany key automation features, both environments provide automatic scaling to han dle \\nf\", \"luctuating workload volumes.  \\n\\u2022 Service communication  becomes a significant design decision when c\", \"onstructing a cloud -\\nnative application. Applications typically expose an API gateway to manage fro\", \"nt -end client \\ncommunication. Then backend microservices strive to communicate with each other \\nimp\", \"lementing asynch ronous communication patterns, when possible.  \\n\\u2022 gRPC  is a modern, high -performa\", \"nce framework that evolves the age -old remote procedure \\ncall (RPC) protocol. Cloud -native applica\", \"tions often embrace gRPC to streamline messaging \\nbetween back -end services. gRPC uses HTTP/2 for i\", \"ts transport protocol. It can be u p to 8x \\nfaster than JSON serialization with message sizes 60 -80\", \"% smaller. gRPC is open source and \\nmanaged by the Cloud Native Computing Foundation (CNCF).  \\n\\u2022 Dis\", \"tributed data  is a model often implemented by cloud -native applications. Applications \\nsegregate b\", \"usiness functionality into small, independent microservices. Each microservice \\nencapsulates its own\", \" dependencies, data, and state. The classic shared database model \\nevolves  into one of many smaller\", \" databases, each aligning with a microservice. When the \\nsmoke clears, we emerge with a design that \", \"exposes a database -per-microservice model.  \\n\\u2022 No-SQL databases  refer to high -performance, non -r\", \"elational data stores. They excel in their \\nease-of-use, scalability, resilience, and availability c\", \"haracteristics. High volume services that \\nrequire sub second response time favor NoSQL datastores. \", \"The proliferation of NoSQ L \\ntechnologies for distributed cloud -native systems can\\u2019t be overstated.\", \"  \\n\\u2022 NewSQL  is an emerging database technology that combines the distributed scalability of \\nNoSQL \", \"and the ACID guarantees of a relational database. NewSQL databases target business \\nsystems that mus\", \"t process high -volumes of data, across distributed environments, with full \\ntransactional/ACID comp\", \"liance. The Cloud Native Computing Foundation (CNCF) features \\nseveral NewSQL database projects.  \\n\\u2022\", \" Resiliency  is the ability of your system to react to failure and still remain functional. Cloud -\\n\", \"native systems embrace distributed architecture where failure is inevitable. Applications must \\nbe c\", \"onstructed to respond elegantly to failure and quickly return to a full y functioning state.  \\n\\u2022 Ser\", \"vice meshes  are a configurable infrastructure layer with built -in capabilities to handle \\nservice \", \"communication and other cross -cutting challenges. They decouple cross -cutting \\nresponsibilities fr\", \"om your business code. These responsibilities move into a service proxy. \\nReferred to as the Sidecar\", \" pattern, the proxy is deployed into a separate process to provide \\nisolation from your business cod\", \"e.  \\n\\u2022 Observability  is a key design consideration for cloud -native applications. As services are \", \"\\ndistributed across a cluster of nodes, centralized logging, monitoring, and alerts, become \\nmandato\", \"ry. Azure Monitor is a collection of cloud -based tools designed to provide visib ility \\ninto the st\", \"ate of your system.   \\n183 CHAPTER 11 | Summary: Architecting cloud -native apps  \\n \\u2022 Infrastructure\", \" as Code  is a widely accepted practice that automates platform provisioning. \\nYour infrastructure a\", \"nd deployments are automated, consistent, and repeatable. Tools like \\nAzure Resource Manager, Terraf\", \"orm, and the Azure CLI, enable you to declaratively script the \\ncloud infrastructure you require.  \\n\", \"\\u2022 Code automation  is a requirement for cloud -native applications. Modern CI/CD systems help \\nfulfi\", \"ll this principle. They provide separate build and deployment steps that help ensure \\nconsistent and\", \" quality code. The build stage transforms the code into a binary artifact. Th e \\nrelease stage picks\", \" up the binary artifact, applies external environment configuration, and \\ndeploys it to a specified \", \"environment. Azure DevOps and GitHub are full -featured DevOps \\nenvironments.  \"]"