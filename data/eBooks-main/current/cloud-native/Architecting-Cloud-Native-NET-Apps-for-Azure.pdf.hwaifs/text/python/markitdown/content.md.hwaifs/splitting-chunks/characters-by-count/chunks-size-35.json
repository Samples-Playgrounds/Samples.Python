"[\"\\n\\n\\fEDITION v1.0.3\\n\\nRefer changelog for the book updates and community contributions.\\n\\nPUBLISHED BY\\n\\n\", \"Microsoft Developer Division, .NET, and Visual Studio product teams\\n\\nA division of Microsoft Corpora\", \"tion\\n\\nOne Microsoft Way\\n\\nRedmond, Washington 98052-6399\\n\\nCopyright \\u00a9 2023 by Microsoft Corporation\\n\\n\", \"All rights reserved. No part of the contents of this book may be reproduced or transmitted in any\\nfo\", \"rm or by any means without the written permission of the publisher.\\n\\nThis book is provided \\u201cas-is\\u201d a\", \"nd expresses the author\\u2019s views and opinions. The views, opinions, and\\ninformation expressed in this\", \" book, including URL and other Internet website references, may change\\nwithout notice.\\n\\nSome example\", \"s depicted herein are provided for illustration only and are fictitious. No real association\\nor conn\", \"ection is intended or should be inferred.\\n\\nMicrosoft and the trademarks listed at https://www.micros\", \"oft.com on the \\u201cTrademarks\\u201d webpage are\\ntrademarks of the Microsoft group of companies.\\n\\nMac and mac\", \"OS are trademarks of Apple Inc.\\n\\nThe Docker whale logo is a registered trademark of Docker, Inc. Use\", \"d by permission.\\n\\nAll other marks and logos are property of their respective owners.\\n\\nAuthors:\\n\\nRob \", \"Vettor, Principal MTC (Microsoft Technology Center) Architect for Cloud App Innovation -\\nthinkinginc\", \"loudnative.com, Microsoft\\n\\nSteve \\u201cardalis\\u201d Smith, Software Architect and Trainer - Ardalis.com\\n\\nPart\", \"icipants and Reviewers:\\n\\nCesar De la Torre, Principal Program Manager, .NET team, Microsoft\\n\\nNish An\", \"il, Senior Program Manager, .NET team, Microsoft\\n\\nJeremy Likness, Senior Program Manager, .NET team,\", \" Microsoft\\n\\nCecil Phillip, Senior Cloud Advocate, Microsoft\\n\\nSumit Ghosh, Principal Consultant at Ne\", \"udesic\\n\\nEditors:\\n\\nMaira Wenzel, Program Manager, .NET team, Microsoft\\n\\n\\fDavid Pine, Senior Content D\", \"eveloper, .NET docs, Microsoft\\n\\nVersion\\n\\nThis guide has been written to cover .NET 7 version along w\", \"ith many additional updates related to\\nthe same \\u201cwave\\u201d of technologies (that is, Azure and additiona\", \"l third-party technologies) coinciding in\\ntime with the .NET 7 release.\\n\\nWho should use this guide\\n\\n\", \"The audience for this guide is mainly developers, development leads, and architects who are\\ninterest\", \"ed in learning how to build applications designed for the cloud.\\n\\nA secondary audience is technical \", \"decision-makers who plan to choose whether to build their\\napplications using a cloud-native approach\", \".\\n\\nHow you can use this guide\\n\\nThis guide begins by defining cloud native and introducing a referenc\", \"e application built using cloud-\\nnative principles and technologies. Beyond these first two chapters\", \", the rest of the book is broken up\\ninto specific chapters focused on topics common to most cloud-na\", \"tive applications. You can jump to\\nany of these chapters to learn about cloud-native approaches to:\\n\", \"\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nData and data access\\n\\nCommunication patterns\\n\\nScaling and scalability\\n\\nApplication resi\", \"liency\\n\\n\\u2022  Monitoring and health\\n\\n\\u2022\\n\\n\\u2022\\n\\nIdentity and security\\n\\nDevOps\\n\\nThis guide is available both \", \"in PDF form and online. Feel free to forward this document or links to its\\nonline version to your te\", \"am to help ensure common understanding of these topics. Most of these\\ntopics benefit from a consiste\", \"nt understanding of the underlying principles and patterns, as well as\\nthe trade-offs involved in de\", \"cisions related to these topics. Our goal with this document is to equip\\nteams and their leaders wit\", \"h the information they need to make well-informed decisions for their\\napplications\\u2019 architecture, de\", \"velopment, and hosting.\\n\\n\\fContents\\n\\nIntroduction to cloud-native applications ......................\", \"...................................................... 1\\n\\nCloud-native computing ...................\", \"....................................................................................................\", \"........................... 3\\n\\nWhat is Cloud Native? ...............................................\", \"....................................................................................................\", \"......... 4\\n\\nThe pillars of cloud native ...........................................................\", \"..................................................................................... 5\\n\\nThe cloud .\", \"....................................................................................................\", \"........................................................................... 5\\n\\nModern design .......\", \"....................................................................................................\", \".......................................................... 6\\n\\nMicroservices ........................\", \"....................................................................................................\", \"............................................ 9\\n\\nContainers .........................................\", \"....................................................................................................\", \".............................. 12\\n\\nBacking services ................................................\", \"....................................................................................................\", \"............ 15\\n\\nAutomation ........................................................................\", \"................................................................................................. 17\", \"\\n\\nCandidate apps for cloud native ..................................................................\", \"................................................................... 19\\n\\nModernizing legacy apps ....\", \"....................................................................................................\", \"...................................... 19\\n\\nSummary .................................................\", \"....................................................................................................\", \"......................... 21\\n\\nIntroducing eShopOnContainers reference app ..........................\", \"...................................... 22\\n\\nFeatures and requirements ...............................\", \"....................................................................................................\", \"............ 23\\n\\nOverview of the code ..............................................................\", \"............................................................................................ 25\\n\\nUnd\", \"erstanding microservices ...........................................................................\", \"................................................................ 27\\n\\nMapping eShopOnContainers to Az\", \"ure Services .......................................................................................\", \".............. 27\\n\\nContainer orchestration and clustering ..........................................\", \"......................................................................... 28\\n\\nAPI Gateway ..........\", \"....................................................................................................\", \"......................................................... 28\\n\\nData .................................\", \"....................................................................................................\", \".................................................. 29\\n\\nEvent Bus ...................................\", \"....................................................................................................\", \"...................................... 30\\n\\nResiliency ..............................................\", \"....................................................................................................\", \"........................... 30\\n\\nDeploying eShopOnContainers to Azure ...............................\", \"..................................................................................... 30\\n\\nAzure Kube\", \"rnetes Service .....................................................................................\", \"........................................................ 30\\n\\nDeploying to Azure Kubernetes Service u\", \"sing Helm ......................................................................................... \", \"30\\n\\nAzure Functions and Logic Apps (Serverless) ....................................................\", \"................................................... 32\\n\\nCentralized configuration ..................\", \"....................................................................................................\", \"............................ 33\\n\\ni\\n\\nContents\\n\\n\\fAzure App Configuration .............................\", \"....................................................................................................\", \"............. 33\\n\\nAzure Key Vault ..................................................................\", \"............................................................................................... 34\\n\\n\", \"Configuration in eShop .............................................................................\", \"..................................................................... 34\\n\\nReferences ...............\", \"....................................................................................................\", \"........................................................ 34\\n\\nScaling cloud-native applications .....\", \"................................................................................... 36\\n\\nLeveraging c\", \"ontainers and orchestrators ........................................................................\", \"............................................ 36\\n\\nChallenges with monolithic deployments ............\", \".................................................................................................. 3\", \"6\\n\\nWhat are the benefits of containers and orchestrators? ..........................................\", \"........................................ 38\\n\\nWhat are the scaling benefits? ........................\", \"....................................................................................................\", \"........ 40\\n\\nWhat scenarios are ideal for containers and orchestrators?.............................\", \".............................................. 42\\n\\nWhen should you avoid using containers and orches\", \"trators? ....................................................................... 42\\n\\nDevelopment res\", \"ources .............................................................................................\", \".................................................... 42\\n\\nLeveraging serverless functions ...........\", \"....................................................................................................\", \"....................... 46\\n\\nWhat is serverless? ....................................................\", \"....................................................................................................\", \"... 47\\n\\nWhat challenges are solved by serverless? ..................................................\", \".......................................................... 47\\n\\nWhat is the difference between a micr\", \"oservice and a serverless function? ............................................. 47\\n\\nWhat scenarios\", \" are appropriate for serverless? ...................................................................\", \"................................ 47\\n\\nWhen should you avoid serverless? .............................\", \"............................................................................................. 48\\n\\nCo\", \"mbining containers and serverless approaches .......................................................\", \"........................................... 49\\n\\nWhen does it make sense to use containers with serve\", \"rless? ........................................................................ 49\\n\\nWhen should you \", \"avoid using containers with Azure Functions? .......................................................\", \"......... 49\\n\\nHow to combine serverless and Docker containers ......................................\", \"..................................................... 49\\n\\nHow to combine serverless and Kubernetes w\", \"ith KEDA .................................................................................. 50\\n\\nDepl\", \"oying containers in Azure ..........................................................................\", \".............................................................. 50\\n\\nAzure Container Registry ........\", \"....................................................................................................\", \".................................. 50\\n\\nACR Tasks ...................................................\", \"....................................................................................................\", \"..................... 52\\n\\nAzure Kubernetes Service .................................................\", \"............................................................................................ 52\\n\\nAzu\", \"re Bridge to Kubernetes ............................................................................\", \"............................................................. 53\\n\\nScaling containers and serverless \", \"applications .......................................................................................\", \".................. 53\\n\\nThe simple solution: scaling up .............................................\", \"..................................................................................... 53\\n\\nScaling ou\", \"t cloud-native apps ................................................................................\", \".................................................... 54\\n\\nOther container deployment options ........\", \"....................................................................................................\", \"............... 55\\n\\nii\\n\\nContents\\n\\n\\fWhen does it make sense to deploy to App Service for Containers? \", \"......................................................... 55\\n\\nHow to deploy to App Service for Conta\", \"iners ..............................................................................................\", \"........ 55\\n\\nWhen does it make sense to deploy to Azure Container Instances? .......................\", \"................................... 55\\n\\nHow to deploy an app to Azure Container Instances ..........\", \".............................................................................. 55\\n\\nReferences ......\", \"....................................................................................................\", \"................................................................. 56\\n\\nCloud-native communication pat\", \"terns ............................................................................... 58\\n\\nCommunicat\", \"ion considerations .................................................................................\", \"..................................................... 58\\n\\nFront-end client communication ...........\", \"....................................................................................................\", \"..................... 60\\n\\nSimple Gateways ..........................................................\", \"....................................................................................................\", \" 62\\n\\nAzure Application Gateway .....................................................................\", \"..................................................................... 63\\n\\nAzure API Management .....\", \"....................................................................................................\", \"........................................ 63\\n\\nReal-time communication ...............................\", \"....................................................................................................\", \"......... 66\\n\\nService-to-service communication .....................................................\", \"........................................................................... 67\\n\\nQueries ............\", \"....................................................................................................\", \"................................................................. 68\\n\\nCommands .....................\", \"....................................................................................................\", \"................................................. 71\\n\\nEvents .......................................\", \"....................................................................................................\", \"......................................... 74\\n\\ngRPC .................................................\", \"....................................................................................................\", \"...................................... 80\\n\\nWhat is gRPC? ...........................................\", \"....................................................................................................\", \".................... 80\\n\\ngRPC Benefits .............................................................\", \"....................................................................................................\", \"... 80\\n\\nProtocol Buffers ...........................................................................\", \"..................................................................................... 81\\n\\ngRPC suppo\", \"rt in .NET .........................................................................................\", \"............................................................ 81\\n\\ngRPC usage ........................\", \"....................................................................................................\", \"............................................. 82\\n\\ngRPC implementation ..............................\", \"....................................................................................................\", \".................. 83\\n\\nLooking ahead ...............................................................\", \"....................................................................................................\", \" 85\\n\\nService Mesh communication infrastructure .....................................................\", \"........................................................ 85\\n\\nSummary ...............................\", \"....................................................................................................\", \"........................................... 86\\n\\nCloud-native data patterns .........................\", \"......................................................................... 88\\n\\nDatabase-per-microserv\", \"ice, why? ..........................................................................................\", \"........................................ 89\\n\\nCross-service queries .................................\", \"....................................................................................................\", \"...................... 90\\n\\nDistributed transactions ................................................\", \"....................................................................................................\", \". 91\\n\\nHigh volume data .............................................................................\", \".................................................................................... 93\\n\\nCQRS ......\", \"....................................................................................................\", \"........................................................................... 93\\n\\niii\\n\\nContents\\n\\n\\fEven\", \"t sourcing .........................................................................................\", \".......................................................................... 94\\n\\nRelational vs. NoSQL \", \"data ...............................................................................................\", \".................................................. 96\\n\\nThe CAP theorem .............................\", \"....................................................................................................\", \"............................ 97\\n\\nConsiderations for relational vs. NoSQL systems ...................\", \"............................................................................. 99\\n\\nDatabase as a Serv\", \"ice ................................................................................................\", \"..................................................... 99\\n\\nAzure relational databases ...............\", \"....................................................................................................\", \"...................... 100\\n\\nAzure SQL Database......................................................\", \"................................................................................................ 100\", \"\\n\\nOpen-source databases in Azure ...................................................................\", \".......................................................... 101\\n\\nNoSQL data in Azure ................\", \"....................................................................................................\", \"................................ 102\\n\\nNewSQL databases .............................................\", \"....................................................................................................\", \"....... 106\\n\\nData migration to the cloud ...........................................................\", \"........................................................................... 108\\n\\nCaching in a cloud-\", \"native app .........................................................................................\", \".............................................. 108\\n\\nWhy? ...........................................\", \"....................................................................................................\", \"..................................... 108\\n\\nCaching architecture ....................................\", \"....................................................................................................\", \"............. 109\\n\\nAzure Cache for Redis ...........................................................\", \"....................................................................................... 110\\n\\nElastic\", \"search in a cloud-native app .......................................................................\", \"...................................................... 110\\n\\nSummary ................................\", \"....................................................................................................\", \"........................................ 111\\n\\nCloud-native resiliency ..............................\", \"......................................................................... 113\\n\\nApplication resilienc\", \"y patterns .........................................................................................\", \"............................................. 114\\n\\nCircuit breaker pattern .........................\", \"....................................................................................................\", \".................... 116\\n\\nTesting for resiliency ...................................................\", \".................................................................................................. 1\", \"17\\n\\nAzure platform resiliency ......................................................................\", \"........................................................................... 117\\n\\nDesign with resilie\", \"ncy ................................................................................................\", \"................................................... 118\\n\\nDesign with redundancy.....................\", \"....................................................................................................\", \"..................... 118\\n\\nDesign for scalability ..................................................\", \"................................................................................................... \", \"120\\n\\nBuilt-in retry in services ....................................................................\", \"........................................................................... 121\\n\\nResilient communica\", \"tions ..............................................................................................\", \".................................................. 122\\n\\nService mesh ...............................\", \"....................................................................................................\", \"................................. 123\\n\\nIstio and Envoy .............................................\", \"....................................................................................................\", \"............... 124\\n\\nIntegration with Azure Kubernetes Services ....................................\", \"................................................................... 125\\n\\nMonitoring and health .....\", \"................................................................................................... \", \"126\\n\\nObservability patterns ........................................................................\", \"............................................................................... 126\\n\\niv\\n\\nContents\\n\\n\\f\", \"When to use logging ................................................................................\", \".................................................................... 126\\n\\nChallenges with detecting \", \"and responding to potential app health issues ........................................... 130\\n\\nChall\", \"enges with reacting to critical problems in cloud-native apps ......................................\", \".................... 130\\n\\nLogging with Elastic Stack ...............................................\", \"................................................................................................ 131\", \"\\n\\nElastic Stack ....................................................................................\", \".................................................................................. 131\\n\\nWhat are the\", \" advantages of Elastic Stack? ......................................................................\", \".................................... 132\\n\\nLogstash .................................................\", \"....................................................................................................\", \"........................ 132\\n\\nElasticsearch ........................................................\", \"....................................................................................................\", \"......... 133\\n\\nVisualizing information with Kibana web dashboards ..................................\", \".................................................. 133\\n\\nInstalling Elastic Stack on Azure ..........\", \"....................................................................................................\", \"................. 134\\n\\nReferences ..................................................................\", \"....................................................................................................\", \"... 134\\n\\nMonitoring in Azure Kubernetes Services ...................................................\", \".............................................................. 134\\n\\nAzure Monitor for Containers ...\", \"....................................................................................................\", \"............................ 134\\n\\nLog.Finalize() ...................................................\", \"....................................................................................................\", \"............. 136\\n\\nAzure Monitor ...................................................................\", \"................................................................................................... \", \"136\\n\\nGathering logs and metrics ....................................................................\", \".................................................................... 137\\n\\nReporting data ...........\", \"....................................................................................................\", \"................................................. 137\\n\\nDashboards ..................................\", \"....................................................................................................\", \"................................. 138\\n\\nAlerts ......................................................\", \"....................................................................................................\", \"......................... 140\\n\\nReferences ..........................................................\", \"....................................................................................................\", \"........... 141\\n\\nCloud-native identity .............................................................\", \"............................................. 142\\n\\nReferences ......................................\", \"....................................................................................................\", \"................................... 142\\n\\nAuthentication and authorization in cloud-native apps .....\", \"................................................................................ 142\\n\\nReferences ...\", \"....................................................................................................\", \".................................................................. 143\\n\\nAzure Active Directory .....\", \"....................................................................................................\", \"............................................. 143\\n\\nReferences ......................................\", \"....................................................................................................\", \"............................... 143\\n\\nIdentityServer for cloud-native applications ..................\", \".......................................................................................... 144\\n\\nComm\", \"on web app scenarios ...............................................................................\", \"...................................................... 144\\n\\nGetting started ........................\", \"....................................................................................................\", \".................................... 145\\n\\nConfiguration ............................................\", \"....................................................................................................\", \"................... 145\\n\\nJavaScript clients ........................................................\", \"....................................................................................................\", \" 146\\n\\nReferences ...................................................................................\", \"...................................................................................... 146\\n\\nv\\n\\nConte\", \"nts\\n\\n\\fCloud-native security ........................................................................\", \".................................. 147\\n\\nAzure security for cloud-native apps .......................\", \"................................................................................................... \", \"147\\n\\nThreat modeling ...............................................................................\", \".............................................................................. 148\\n\\nPrinciple of lea\", \"st privilege .......................................................................................\", \".................................................... 148\\n\\nPenetration testing ......................\", \"....................................................................................................\", \".............................. 149\\n\\nMonitoring .....................................................\", \"....................................................................................................\", \"............... 149\\n\\nSecuring the build ............................................................\", \".............................................................................................. 149\\n\\n\", \"Building secure code ...............................................................................\", \"..................................................................... 150\\n\\nBuilt-in security .......\", \"....................................................................................................\", \".................................................... 150\\n\\nAzure network infrastructure .............\", \"....................................................................................................\", \".................... 150\\n\\nRole-based access control for restricting access to Azure resources.......\", \"................................................. 152\\n\\nSecurity Principals .........................\", \"....................................................................................................\", \"............................. 152\\n\\nRoles ...........................................................\", \"....................................................................................................\", \"..................... 153\\n\\nScopes ..................................................................\", \"....................................................................................................\", \".......... 154\\n\\nDeny ...............................................................................\", \"....................................................................................................\", \". 154\\n\\nChecking access .............................................................................\", \"................................................................................. 154\\n\\nSecuring secr\", \"ets ................................................................................................\", \".............................................................. 155\\n\\nAzure Key Vault ................\", \"....................................................................................................\", \"........................................... 155\\n\\nKubernetes ........................................\", \"....................................................................................................\", \"............................ 155\\n\\nEncryption in transit and at rest ................................\", \"............................................................................................... 156\\n\", \"\\nKeeping secure ....................................................................................\", \"............................................................................ 160\\n\\nDevOps ...........\", \"....................................................................................................\", \".................. 161\\n\\nAzure DevOps ...............................................................\", \"....................................................................................................\", \".... 162\\n\\nGitHub Actions ...........................................................................\", \".......................................................................................... 163\\n\\nSour\", \"ce control .........................................................................................\", \"............................................................................. 163\\n\\nRepository per mi\", \"croservice .........................................................................................\", \"............................................. 164\\n\\nSingle repository ...............................\", \"....................................................................................................\", \"......................... 166\\n\\nStandard directory structure ........................................\", \".............................................................................................. 167\\n\\n\", \"Task management ....................................................................................\", \".......................................................................... 167\\n\\nCI/CD pipelines ....\", \"....................................................................................................\", \"............................................................ 169\\n\\nAzure Builds .....................\", \"....................................................................................................\", \"............................................. 170\\n\\nAzure DevOps releases ...........................\", \"....................................................................................................\", \"................. 172\\n\\nvi\\n\\nContents\\n\\n\\fEverybody gets a build pipeline ..............................\", \"................................................................................................. 17\", \"3\\n\\nVersioning releases .............................................................................\", \"........................................................................... 173\\n\\nFeature flags .....\", \"....................................................................................................\", \"................................................................ 173\\n\\nImplementing feature flags ...\", \"....................................................................................................\", \"................................. 174\\n\\nInfrastructure as code ......................................\", \"....................................................................................................\", \"............. 175\\n\\nAzure Resource Manager templates ................................................\", \"...................................................................... 175\\n\\nTerraform ..............\", \"....................................................................................................\", \"......................................................... 176\\n\\nAzure CLI Scripts and Tasks..........\", \"....................................................................................................\", \".......................... 177\\n\\nCloud Native Application Bundles ...................................\", \"............................................................................................ 178\\n\\nDe\", \"vOps Decisions .....................................................................................\", \"..................................................................... 180\\n\\nReferences ..............\", \"....................................................................................................\", \"....................................................... 180\\n\\nSummary: Architecting cloud-native apps\", \" ....................................................................... 181\\n\\nvii\\n\\nContents\\n\\n\\fCHAPTE\", \"R  1\\n\\nIntroduction to cloud-\\nnative applications\\n\\nAnother day, at the office, working on \\u201cthe next b\", \"ig thing.\\u201d\\n\\nYour cellphone rings. It\\u2019s your friendly recruiter - the one who calls daily with exciti\", \"ng new\\nopportunities.\\n\\nBut this time it\\u2019s different: Start-up, equity, and plenty of funding.\\n\\nThe m\", \"ention of the cloud, microservices, and cutting-edge technology pushes you over the edge.\\n\\nFast forw\", \"ard a few weeks and you\\u2019re now a new employee in a design session architecting a major\\neCommerce app\", \"lication. You\\u2019re going to compete with the leading eCommerce sites.\\n\\nHow will you build it?\\n\\nIf you \", \"follow the guidance from past 15 years, you\\u2019ll most likely build the system shown in Figure 1.1.\\n\\nFi\", \"gure 1-1. Traditional monolithic design\\n\\nYou construct a large core application containing all of yo\", \"ur domain logic. It includes modules such as\\nIdentity, Catalog, Ordering, and more. They directly co\", \"mmunicate with each other within a single\\nserver process. The modules share a large relational datab\", \"ase. The core exposes functionality via an\\nHTML interface and a mobile app.\\n\\nCongratulations! You ju\", \"st created a monolithic application.\\n\\nNot all is bad. Monoliths offer some distinct advantages. For \", \"example, they\\u2019re straightforward to\\u2026\\n\\n1\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\f\\u2022\\n\\n\", \"\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nbuild\\n\\ntest\\n\\ndeploy\\n\\ntroubleshoot\\n\\nvertically scale\\n\\nMany successful apps that exist tod\", \"ay were created as monoliths. The app is a hit and continues to\\nevolve, iteration after iteration, a\", \"dding more functionality.\\n\\nAt some point, however, you begin to feel uncomfortable. You find yoursel\", \"f losing control of the\\napplication. As time goes on, the feeling becomes more intense, and you even\", \"tually enter a state\\nknown as the Fear Cycle:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nThe app has become so over\", \"whelmingly complicated that no single person understands it.\\n\\nYou fear making changes - each change \", \"has unintended and costly side effects.\\n\\nNew features/fixes become tricky, time-consuming, and expen\", \"sive to implement.\\n\\nEach release becomes as small as possible and requires a full deployment of the \", \"entire\\napplication.\\n\\nOne unstable component can crash the entire system.\\n\\nNew technologies and frame\", \"works aren\\u2019t an option.\\n\\nIt\\u2019s difficult to implement agile delivery methodologies.\\n\\nArchitectural er\", \"osion sets in as the code base deteriorates with never-ending \\u201cquick fixes.\\u201d\\n\\nFinally, the consultan\", \"ts come in and tell you to rewrite it.\\n\\nSound familiar?\\n\\nMany organizations have addressed this mono\", \"lithic fear cycle by adopting a cloud-native approach to\\nbuilding systems. Figure 1-2 shows the same\", \" system built applying cloud-native techniques and\\npractices.\\n\\n2\\n\\nCHAPTER 1 | Introduction to cloud-\", \"native applications\\n\\n\\fFigure 1-2. Cloud-native design\\n\\nNote how the application is decomposed across\", \" a set of small isolated microservices. Each service is\\nself-contained and encapsulates its own code\", \", data, and dependencies. Each is deployed in a software\\ncontainer and managed by a container orches\", \"trator. Instead of a large relational database, each\\nservice owns it own datastore, the type of whic\", \"h vary based upon the data needs. Note how some\\nservices depend on a relational database, but other \", \"on NoSQL databases. One service stores its state\\nin a distributed cache. Note how all traffic routes\", \" through an API Gateway service that is responsible\\nfor routing traffic to the core back-end service\", \"s and enforcing many cross-cutting concerns. Most\\nimportantly, the application takes full advantage \", \"of the scalability, availability, and resiliency features\\nfound in modern cloud platforms.\\n\\nCloud-na\", \"tive computing\\n\\nHmm\\u2026 We just used the term, Cloud Native. Your first thought might be, \\u201cWhat exactly\", \" does that\\nmean?\\u201d Another industry buzzword concocted by software vendors to market more stuff?\\u201d\\n\\nFo\", \"rtunately it\\u2019s far different, and hopefully this book will help convince you.\\n\\nWithin a short time, \", \"cloud native has become a driving trend in the software industry. It\\u2019s a new way\\nto construct large,\", \" complex systems. The approach takes full advantage of modern software\\ndevelopment practices, techno\", \"logies, and cloud infrastructure. Cloud native changes the way you\\ndesign, implement, deploy, and op\", \"erationalize systems.\\n\\nUnlike the continuous hype that drives our industry, cloud native is for-real\", \". Consider the Cloud Native\\nComputing Foundation (CNCF), a consortium of over 400 major corporations\", \". Its charter is to make\\n\\n3\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fcloud-native co\", \"mputing ubiquitous across technology and cloud stacks. As one of the most influential\\nopen-source gr\", \"oups, it hosts many of the fastest-growing open source-projects in GitHub. These\\nprojects include Ku\", \"bernetes, Prometheus, Helm, Envoy, and gRPC.\\n\\nThe CNCF fosters an ecosystem of open-source and vendo\", \"r-neutrality. Following that lead, this book\\npresents cloud-native principles, patterns, and best pr\", \"actices that are technology agnostic. At the\\nsame time, we discuss the services and infrastructure a\", \"vailable in the Microsoft Azure cloud for\\nconstructing cloud-native systems.\\n\\nSo, what exactly is Cl\", \"oud Native? Sit back, relax, and let us help you explore this new world.\\n\\nWhat is Cloud Native?\\n\\nSto\", \"p what you\\u2019re doing and ask your colleagues to define the term \\u201cCloud Native\\u201d. There\\u2019s a good\\nchance\", \" you\\u2019ll get several different answers.\\n\\nLet\\u2019s start with a simple definition:\\n\\nCloud-native architec\", \"ture and technologies are an approach to designing, constructing, and operating\\nworkloads that are b\", \"uilt in the cloud and take full advantage of the cloud computing model.\\n\\nThe Cloud Native Computing \", \"Foundation provides the official definition:\\n\\nCloud-native technologies empower organizations to bui\", \"ld and run scalable applications in modern,\\ndynamic environments such as public, private, and hybrid\", \" clouds. Containers, service meshes,\\nmicroservices, immutable infrastructure, and declarative APIs e\", \"xemplify this approach.\\n\\nThese techniques enable loosely coupled systems that are resilient, managea\", \"ble, and observable.\\nCombined with robust automation, they allow engineers to make high-impact chang\", \"es frequently and\\npredictably with minimal toil.\\n\\nCloud native is about speed and agility. Business \", \"systems are evolving from enabling business\\ncapabilities to weapons of strategic transformation that\", \" accelerate business velocity and growth. It\\u2019s\\nimperative to get new ideas to market immediately.\\n\\nA\", \"t the same time, business systems have also become increasingly complex with users demanding\\nmore. T\", \"hey expect rapid responsiveness, innovative features, and zero downtime. Performance\\nproblems, recur\", \"ring errors, and the inability to move fast are no longer acceptable. Your users will visit\\nyour com\", \"petitor. Cloud-native systems are designed to embrace rapid change, large scale, and\\nresilience.\\n\\nHe\", \"re are some companies who have implemented cloud-native techniques. Think about the speed,\\nagility, \", \"and scalability they\\u2019ve achieved.\\n\\nCompany\\n\\nExperience\\n\\nNetflix\\n\\nUber\\n\\n4\\n\\nHas 600+ services in produ\", \"ction. Deploys 100\\ntimes per day.\\n\\nHas 1,000+ services in production. Deploys\\nseveral thousand times\", \" each week.\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fCompany\\n\\nExperience\\n\\nWeChat\\n\\nHa\", \"s 3,000+ services in production. Deploys\\n1,000 times a day.\\n\\nAs you can see, Netflix, Uber, and, WeC\", \"hat expose cloud-native systems that consist of many\\nindependent services. This architectural style \", \"enables them to rapidly respond to market conditions.\\nThey instantaneously update small areas of a l\", \"ive, complex application, without a full redeployment.\\nThey individually scale services as needed.\\n\\n\", \"The pillars of cloud native\\n\\nThe speed and agility of cloud native derive from many factors. Foremos\", \"t is cloud infrastructure. But\\nthere\\u2019s more: Five other foundational pillars shown in Figure 1-3 als\", \"o provide the bedrock for cloud-\\nnative systems.\\n\\nFigure 1-3. Cloud-native foundational pillars\\n\\nLet\", \"\\u2019s take some time to better understand the significance of each pillar.\\n\\nThe cloud\\n\\nCloud-native sys\", \"tems take full advantage of the cloud service model.\\n\\nDesigned to thrive in a dynamic, virtualized c\", \"loud environment, these systems make extensive use of\\nPlatform as a Service (PaaS) compute infrastru\", \"cture and managed services. They treat the underlying\\ninfrastructure as disposable - provisioned in \", \"minutes and resized, scaled, or destroyed on demand \\u2013\\nvia automation.\\n\\nConsider the widely accepted \", \"DevOps concept of Pets vs. Cattle. In a traditional data center, servers\\nare treated as Pets: a phys\", \"ical machine, given a meaningful name, and cared for. You scale by adding\\nmore resources to the same\", \" machine (scaling up). If the server becomes sick, you nurse it back to\\nhealth. Should the server be\", \"come unavailable, everyone notices.\\n\\nThe Cattle service model is different. You provision each insta\", \"nce as a virtual machine or container.\\nThey\\u2019re identical and assigned a system identifier such as Se\", \"rvice-01, Service-02, and so on. You scale\\nby creating more of them (scaling out). When one becomes \", \"unavailable, nobody notices.\\n\\n5\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fThe cattle \", \"model embraces immutable infrastructure. Servers aren\\u2019t repaired or modified. If one fails or\\nrequir\", \"es updating, it\\u2019s destroyed and a new one is provisioned \\u2013 all done via automation.\\n\\nCloud-native sy\", \"stems embrace the Cattle service model. They continue to run as the infrastructure\\nscales in or out \", \"with no regard to the machines upon which they\\u2019re running.\\n\\nThe Azure cloud platform supports this t\", \"ype of highly elastic infrastructure with automatic scaling,\\nself-healing, and monitoring capabiliti\", \"es.\\n\\nModern design\\n\\nHow would you design a cloud-native app? What would your architecture look like?\", \" To what\\nprinciples, patterns, and best practices would you adhere? What infrastructure and operatio\", \"nal\\nconcerns would be important?\\n\\nThe Twelve-Factor Application\\n\\nA widely accepted methodology for c\", \"onstructing cloud-based applications is the Twelve-Factor\\nApplication. It describes a set of princip\", \"les and practices that developers follow to construct\\napplications optimized for modern cloud enviro\", \"nments. Special attention is given to portability across\\nenvironments and declarative automation.\\n\\nW\", \"hile applicable to any web-based application, many practitioners consider Twelve-Factor a solid\\nfoun\", \"dation for building cloud-native apps. Systems built upon these principles can deploy and scale\\nrapi\", \"dly and add features to react quickly to market changes.\\n\\nThe following table highlights the Twelve-\", \"Factor methodology:\\n\\nFactor\\n\\nExplanation\\n\\n1 - Code Base\\n\\n2 - Dependencies\\n\\n3 - Configurations\\n\\n4 - B\", \"acking Services\\n\\nA single code base for each microservice, stored\\nin its own repository. Tracked wit\", \"h version\\ncontrol, it can deploy to multiple environments\\n(QA, Staging, Production).\\n\\nEach microserv\", \"ice isolates and packages its own\\ndependencies, embracing changes without\\nimpacting the entire syste\", \"m.\\n\\nConfiguration information is moved out of the\\nmicroservice and externalized through a\\nconfigurat\", \"ion management tool outside of the\\ncode. The same deployment can propagate\\nacross environments with \", \"the correct\\nconfiguration applied.\\n\\nAncillary resources (data stores, caches,\\nmessage brokers) shoul\", \"d be exposed via an\\naddressable URL. Doing so decouples the\\nresource from the application, enabling \", \"it to be\\ninterchangeable.\\n\\n6\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fFactor\\n\\nExplan\", \"ation\\n\\n5 - Build, Release, Run\\n\\n6 - Processes\\n\\n7 - Port Binding\\n\\n8 - Concurrency\\n\\n9 - Disposability\\n\", \"\\n10 - Dev/Prod Parity\\n\\n11 - Logging\\n\\n12 - Admin Processes\\n\\nEach release must enforce a strict separa\", \"tion\\nacross the build, release, and run stages. Each\\nshould be tagged with a unique ID and support\\nt\", \"he ability to roll back. Modern CI/CD systems\\nhelp fulfill this principle.\\n\\nEach microservice should\", \" execute in its own\\nprocess, isolated from other running services.\\nExternalize required state to a b\", \"acking service\\nsuch as a distributed cache or data store.\\n\\nEach microservice should be self-containe\", \"d with\\nits interfaces and functionality exposed on its\\nown port. Doing so provides isolation from\\not\", \"her microservices.\\n\\nWhen capacity needs to increase, scale out\\nservices horizontally across multiple\", \" identical\\nprocesses (copies) as opposed to scaling-up a\\nsingle large instance on the most powerful\\n\", \"machine available. Develop the application to be\\nconcurrent making scaling out in cloud\\nenvironments\", \" seamless.\\n\\nService instances should be disposable. Favor\\nfast startup to increase scalability oppor\", \"tunities\\nand graceful shutdowns to leave the system in a\\ncorrect state. Docker containers along with\", \" an\\norchestrator inherently satisfy this requirement.\\n\\nKeep environments across the application\\nlife\", \"cycle as similar as possible, avoiding costly\\nshortcuts. Here, the adoption of containers can\\ngreatl\", \"y contribute by promoting the same\\nexecution environment.\\n\\nTreat logs generated by microservices as \", \"event\\nstreams. Process them with an event\\naggregator. Propagate log data to data-\\nmining/log managem\", \"ent tools like Azure\\nMonitor or Splunk and eventually to long-term\\narchival.\\n\\nRun administrative/man\", \"agement tasks, such as\\ndata cleanup or computing analytics, as one-off\\nprocesses. Use independent to\", \"ols to invoke\\nthese tasks from the production environment,\\nbut separately from the application.\\n\\n7\\n\\n\", \"CHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fIn the book, Beyond the Twelve-Factor App, a\", \"uthor Kevin Hoffman details each of the original 12\\nfactors (written in 2011). Additionally, he disc\", \"usses three extra factors that reflect today\\u2019s modern\\ncloud application design.\\n\\nNew Factor\\n\\nExplana\", \"tion\\n\\n13 - API First\\n\\n14 - Telemetry\\n\\n15 - Authentication/ Authorization\\n\\nMake everything a service.\", \" Assume your code\\nwill be consumed by a front-end client, gateway,\\nor another service.\\n\\nOn a worksta\", \"tion, you have deep visibility into\\nyour application and its behavior. In the cloud,\\nyou don\\u2019t. Make\", \" sure your design includes the\\ncollection of monitoring, domain-specific, and\\nhealth/system data.\\n\\nI\", \"mplement identity from the start. Consider\\nRBAC (role-based access control) features\\navailable in pu\", \"blic clouds.\\n\\nWe\\u2019ll refer to many of the 12+ factors in this chapter and throughout the book.\\n\\nAzure\", \" Well-Architected Framework\\n\\nDesigning and deploying cloud-based workloads can be challenging, espec\", \"ially when implementing\\ncloud-native architecture. Microsoft provides industry standard best practic\", \"es to help you and your\\nteam deliver robust cloud solutions.\\n\\nThe Microsoft Well-Architected Framewo\", \"rk provides a set of guiding tenets that can be used to\\nimprove the quality of a cloud-native worklo\", \"ad. The framework consists of five pillars of architecture\\nexcellence:\\n\\nTenets\\n\\nDescription\\n\\nCost ma\", \"nagement\\n\\nOperational excellence\\n\\nPerformance efficiency\\n\\nFocus on generating incremental value earl\", \"y.\\nApply Build-Measure-Learn principles to\\naccelerate time to market while avoiding\\ncapital-intensiv\", \"e solutions. Using a pay-as-you-\\ngo strategy, invest as you scale out, rather than\\ndelivering a larg\", \"e investment up front.\\n\\nAutomate the environment and operations to\\nincrease speed and reduce human e\", \"rror. Roll\\nproblem updates back or forward quickly.\\nImplement monitoring and diagnostics from the\\nst\", \"art.\\n\\nEfficiently meet demands placed on your\\nworkloads. Favor horizontal scaling (scaling out)\\nand \", \"design it into your systems. Continually\\nconduct performance and load testing to\\nidentify potential \", \"bottlenecks.\\n\\n8\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fReliability\\n\\nSecurity\\n\\nTene\", \"ts\\n\\nDescription\\n\\nBuild workloads that are both resilient and\\navailable. Resiliency enables workloads\", \" to\\nrecover from failures and continue functioning.\\nAvailability ensures users access to your\\nworklo\", \"ad at all times. Design applications to\\nexpect failures and recover from them.\\n\\nImplement security a\", \"cross the entire lifecycle of\\nan application, from design and implementation\\nto deployment and opera\", \"tions. Pay close\\nattention to identity management, infrastructure\\naccess, application security, and \", \"data\\nsovereignty and encryption.\\n\\nTo get started, Microsoft provides a set of online assessments to \", \"help you assess your current cloud\\nworkloads against the five well-architected pillars.\\n\\nMicroservic\", \"es\\n\\nCloud-native systems embrace microservices, a popular architectural style for constructing moder\", \"n\\napplications.\\n\\nBuilt as a distributed set of small, independent services that interact through a s\", \"hared fabric,\\nmicroservices share the following characteristics:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nEach implements a spec\", \"ific business capability within a larger domain context.\\n\\nEach is developed autonomously and can be \", \"deployed independently.\\n\\nEach is self-contained encapsulating its own data storage technology, depen\", \"dencies, and\\nprogramming platform.\\n\\nEach runs in its own process and communicates with others using \", \"standard communication\\nprotocols such as HTTP/HTTPS, gRPC, WebSockets, or AMQP.\\n\\n\\u2022\\n\\nThey compose tog\", \"ether to form an application.\\n\\nFigure 1-4 contrasts a monolithic application approach with a microse\", \"rvices approach. Note how the\\nmonolith is composed of a layered architecture, which executes in a si\", \"ngle process. It typically\\nconsumes a relational database. The microservice approach, however, segre\", \"gates functionality into\\nindependent services, each with its own logic, state, and data. Each micros\", \"ervice hosts its own\\ndatastore.\\n\\n9\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fFigure 1\", \"-4. Monolithic versus microservices architecture\\n\\nNote how microservices promote the Processes princ\", \"iple from the Twelve-Factor Application,\\ndiscussed earlier in the chapter.\\n\\nFactor #6 specifies \\u201cEac\", \"h microservice should execute in its own process, isolated from other running\\nservices.\\u201d\\n\\nWhy micros\", \"ervices?\\n\\nMicroservices provide agility.\\n\\nEarlier in the chapter, we compared an eCommerce applicati\", \"on built as a monolith to that with\\nmicroservices. In the example, we saw some clear benefits:\\n\\n\\u2022\\n\\n\\u2022\", \"\\n\\nEach microservice has an autonomous lifecycle and can evolve independently and deploy\\nfrequently. \", \"You don\\u2019t have to wait for a quarterly release to deploy a new feature or update.\\nYou can update a s\", \"mall area of a live application with less risk of disrupting the entire system.\\nThe update can be ma\", \"de without a full redeployment of the application.\\n\\nEach microservice can scale independently. Inste\", \"ad of scaling the entire application as a single\\nunit, you scale out only those services that requir\", \"e more processing power to meet desired\\nperformance levels and service-level agreements. Fine-graine\", \"d scaling provides for greater\\ncontrol of your system and helps reduce overall costs as you scale po\", \"rtions of your system,\\nnot everything.\\n\\nAn excellent reference guide for understanding microservices\", \" is .NET Microservices: Architecture for\\nContainerized .NET Applications. The book deep dives into m\", \"icroservices design and architecture. It\\u2019s\\na companion for a full-stack microservice reference archi\", \"tecture available as a free download from\\nMicrosoft.\\n\\nDeveloping microservices\\n\\nMicroservices can be\", \" created upon any modern development platform.\\n\\n10\\n\\nCHAPTER 1 | Introduction to cloud-native applica\", \"tions\\n\\n\\fThe Microsoft .NET platform is an excellent choice. Free and open source, it has many built-\", \"in features\\nthat simplify microservice development. .NET is cross-platform. Applications can be buil\", \"t and run on\\nWindows, macOS, and most flavors of Linux.\\n\\n.NET is highly performant and has scored we\", \"ll in comparison to Node.js and other competing\\nplatforms. Interestingly, TechEmpower conducted an e\", \"xtensive set of performance benchmarks across\\nmany web application platforms and frameworks. .NET sc\", \"ored in the top 10 - well above Node.js and\\nother competing platforms.\\n\\n.NET is maintained by Micros\", \"oft and the .NET community on GitHub.\\n\\nMicroservice challenges\\n\\nWhile distributed cloud-native micro\", \"services can provide immense agility and speed, they present\\nmany challenges:\\n\\nCommunication\\n\\nHow wi\", \"ll front-end client applications communicate with backed-end core microservices? Will you\\nallow dire\", \"ct communication? Or, might you abstract the back-end microservices with a gateway\\nfacade that provi\", \"des flexibility, control, and security?\\n\\nHow will back-end core microservices communicate with each \", \"other? Will you allow direct HTTP calls\\nthat can increase coupling and impact performance and agilit\", \"y? Or might you consider decoupled\\nmessaging with queue and topic technologies?\\n\\nCommunication is co\", \"vered in the Cloud-native communication patterns chapter.\\n\\nResiliency\\n\\nA microservices architecture \", \"moves your system from in-process to out-of-process network\\ncommunication. In a distributed architec\", \"ture, what happens when Service B isn\\u2019t responding to a\\nnetwork call from Service A? Or, what happen\", \"s when Service C becomes temporarily unavailable and\\nother services calling it become blocked?\\n\\nResi\", \"liency is covered in the Cloud-native resiliency chapter.\\n\\nDistributed Data\\n\\nBy design, each microse\", \"rvice encapsulates its own data, exposing operations via its public interface. If\\nso, how do you que\", \"ry data or implement a transaction across multiple services?\\n\\nDistributed data is covered in the Clo\", \"ud-native data patterns chapter.\\n\\nSecrets\\n\\nHow will your microservices securely store and manage sec\", \"rets and sensitive configuration data?\\n\\nSecrets are covered in detail Cloud-native security.\\n\\n11\\n\\nCH\", \"APTER 1 | Introduction to cloud-native applications\\n\\n\\fManage Complexity with Dapr\\n\\nDapr is a distrib\", \"uted, open-source application runtime. Through an architecture of pluggable\\ncomponents, it dramatica\", \"lly simplifies the plumbing behind distributed applications. It provides a\\ndynamic glue that binds y\", \"our application with pre-built infrastructure capabilities and components\\nfrom the Dapr runtime. Fig\", \"ure 1-5 shows Dapr from 20,000 feet.\\n\\nFigure 1-5. Dapr at 20,000 feet.\\n\\nIn the top row of the figure\", \", note how Dapr provides language-specific SDKs for popular development\\nplatforms. Dapr v1 includes \", \"support for .NET, Go, Node.js, Python, PHP, Java, and JavaScript.\\n\\nWhile language-specific SDKs enha\", \"nce the developer experience, Dapr is platform agnostic. Under the\\nhood, Dapr\\u2019s programming model ex\", \"poses capabilities through standard HTTP/gRPC communication\\nprotocols. Any programming platform can \", \"call Dapr via its native HTTP and gRPC APIs.\\n\\nThe blue boxes across the center of the figure represe\", \"nt the Dapr building blocks. Each exposes pre-\\nbuilt plumbing code for a distributed application cap\", \"ability that your application can consume.\\n\\nThe components row represents a large set of pre-defined\", \" infrastructure components that your\\napplication can consume. Think of components as infrastructure \", \"code you don\\u2019t have to write.\\n\\nThe bottom row highlights the portability of Dapr and the diverse env\", \"ironments across which it can\\nrun.\\n\\nMicrosoft features a free ebook Dapr for .NET Developers for lea\", \"rning Dapr.\\n\\nLooking ahead, Dapr has the potential to have a profound impact on cloud-native applica\", \"tion\\ndevelopment.\\n\\nContainers\\n\\nIt\\u2019s natural to hear the term container mentioned in any cloud native\", \" conversation. In the book, Cloud\\nNative Patterns, author Cornelia Davis observes that, \\u201cContainers \", \"are a great enabler of cloud-native\\n\\n12\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fsof\", \"tware.\\u201d The Cloud Native Computing Foundation places microservice containerization as the first\\nstep\", \" in their Cloud-Native Trail Map - guidance for enterprises beginning their cloud-native journey.\\n\\nC\", \"ontainerizing a microservice is simple and straightforward. The code, its dependencies, and runtime\\n\", \"are packaged into a binary called a container image. Images are stored in a container registry, whic\", \"h\\nacts as a repository or library for images. A registry can be located on your development computer\", \", in\\nyour data center, or in a public cloud. Docker itself maintains a public registry via Docker Hu\", \"b. The\\nAzure cloud features a private container registry to store container images close to the clou\", \"d\\napplications that will run them.\\n\\nWhen an application starts or scales, you transform the containe\", \"r image into a running container\\ninstance. The instance runs on any computer that has a container ru\", \"ntime engine installed. You can\\nhave as many instances of the containerized service as needed.\\n\\nFigu\", \"re 1-6 shows three different microservices, each in its own container, all running on a single host.\", \"\\n\\nFigure 1-6. Multiple containers running on a container host\\n\\nNote how each container maintains its\", \" own set of dependencies and runtime, which can be different\\nfrom one another. Here, we see differen\", \"t versions of the Product microservice running on the same\\nhost. Each container shares a slice of th\", \"e underlying host operating system, memory, and processor,\\nbut is isolated from one another.\\n\\nNote h\", \"ow well the container model embraces the Dependencies principle from the Twelve-Factor\\nApplication.\\n\", \"\\nFactor #2 specifies that \\u201cEach microservice isolates and packages its own dependencies, embracing\\nc\", \"hanges without impacting the entire system.\\u201d\\n\\nContainers support both Linux and Windows workloads. T\", \"he Azure cloud openly embraces both.\\nInterestingly, it\\u2019s Linux, not Windows Server, that has become \", \"the more popular operating system in\\nAzure.\\n\\n13\\n\\nCHAPTER 1 | Introduction to cloud-native applicatio\", \"ns\\n\\n\\fWhile several container vendors exist, Docker has captured the lion\\u2019s share of the market. The\\n\", \"company has been driving the software container movement. It has become the de facto standard for\\npa\", \"ckaging, deploying, and running cloud-native applications.\\n\\nWhy containers?\\n\\nContainers provide port\", \"ability and guarantee consistency across environments. By encapsulating\\neverything into a single pac\", \"kage, you isolate the microservice and its dependencies from the\\nunderlying infrastructure.\\n\\nYou can\", \" deploy the container in any environment that hosts the Docker runtime engine. Containerized\\nworkloa\", \"ds also eliminate the expense of pre-configuring each environment with frameworks, software\\nlibrarie\", \"s, and runtime engines.\\n\\nBy sharing the underlying operating system and host resources, a container \", \"has a much smaller\\nfootprint than a full virtual machine. The smaller size increases the density, or\", \" number of\\nmicroservices, that a given host can run at one time.\\n\\nContainer orchestration\\n\\nWhile too\", \"ls such as Docker create images and run containers, you also need tools to manage them.\\nContainer ma\", \"nagement is done with a special software program called a container orchestrator.\\nWhen operating at \", \"scale with many independent running containers, orchestration is essential.\\n\\nFigure 1-7 shows manage\", \"ment tasks that container orchestrators automate.\\n\\nFigure 1-7. What container orchestrators do\\n\\nThe \", \"following table describes common orchestration tasks.\\n\\nTasks\\n\\nExplanation\\n\\nScheduling\\n\\nAffinity/anti\", \"-affinity\\n\\nHealth monitoring\\n\\nFailover\\n\\nAutomatically provision container instances.\\n\\nProvision cont\", \"ainers nearby or far apart from\\neach other, helping availability and\\nperformance.\\n\\nAutomatically det\", \"ect and correct failures.\\n\\nAutomatically reprovision a failed instance to a\\nhealthy machine.\\n\\n14\\n\\nCH\", \"APTER 1 | Introduction to cloud-native applications\\n\\n\\fScaling\\n\\nNetworking\\n\\nService Discovery\\n\\nRollin\", \"g Upgrades\\n\\nTasks\\n\\nExplanation\\n\\nAutomatically add or remove a container\\ninstance to meet demand.\\n\\nMa\", \"nage a networking overlay for container\\ncommunication.\\n\\nEnable containers to locate each other.\\n\\nCoo\", \"rdinate incremental upgrades with zero\\ndowntime deployment. Automatically roll back\\nproblematic chan\", \"ges.\\n\\nNote how container orchestrators embrace the Disposability and Concurrency principles from the\", \"\\nTwelve-Factor Application.\\n\\nFactor #9 specifies that \\u201cService instances should be disposable, favor\", \"ing fast startups to increase\\nscalability opportunities and graceful shutdowns to leave the system i\", \"n a correct state.\\u201d Docker\\ncontainers along with an orchestrator inherently satisfy this requirement\", \".\\u201d\\n\\nFactor #8 specifies that \\u201cServices scale out across a large number of small identical processes \", \"(copies) as\\nopposed to scaling-up a single large instance on the most powerful machine available.\\u201d\\n\\n\", \"While several container orchestrators exist, Kubernetes has become the de facto standard for the\\nclo\", \"ud-native world. It\\u2019s a portable, extensible, open-source platform for managing containerized\\nworklo\", \"ads.\\n\\nYou could host your own instance of Kubernetes, but then you\\u2019d be responsible for provisioning\", \" and\\nmanaging its resources - which can be complex. The Azure cloud features Kubernetes as a managed\", \"\\nservice. Both Azure Kubernetes Service (AKS) and Azure Red Hat OpenShift (ARO) enable you to fully\\n\", \"leverage the features and power of Kubernetes as a managed service, without having to install and\\nma\", \"intain it.\\n\\nContainer orchestration is covered in detail in Scaling Cloud-Native Applications.\\n\\nBack\", \"ing services\\n\\nCloud-native systems depend upon many different ancillary resources, such as data stor\", \"es, message\\nbrokers, monitoring, and identity services. These services are known as backing services\", \".\\n\\nFigure 1-8 shows many common backing services that cloud-native systems consume.\\n\\n15\\n\\nCHAPTER 1 |\", \" Introduction to cloud-native applications\\n\\n\\fFigure 1-8. Common backing services\\n\\nYou could host you\", \"r own backing services, but then you\\u2019d be responsible for licensing, provisioning,\\nand managing thos\", \"e resources.\\n\\nCloud providers offer a rich assortment of managed backing services. Instead of owning\", \" the service,\\nyou simply consume it. The cloud provider operates the resource at scale and bears the\", \" responsibility\\nfor performance, security, and maintenance. Monitoring, redundancy, and availability\", \" are built into the\\nservice. Providers guarantee service level performance and fully support their m\", \"anaged services -\\nopen a ticket and they fix your issue.\\n\\nCloud-native systems favor managed backing\", \" services from cloud vendors. The savings in time and\\nlabor can be significant. The operational risk\", \" of hosting your own and experiencing trouble can get\\nexpensive fast.\\n\\nA best practice is to treat a\", \" backing service as an attached resource, dynamically bound to a\\nmicroservice with configuration inf\", \"ormation (a URL and credentials) stored in an external\\nconfiguration. This guidance is spelled out i\", \"n the Twelve-Factor Application, discussed earlier in the\\nchapter.\\n\\nFactor #4 specifies that backing\", \" services \\u201cshould be exposed via an addressable URL. Doing so\\ndecouples the resource from the applic\", \"ation, enabling it to be interchangeable.\\u201d\\n\\nFactor #3 specifies that \\u201cConfiguration information is m\", \"oved out of the microservice and externalized\\nthrough a configuration management tool outside of the\", \" code.\\u201d\\n\\nWith this pattern, a backing service can be attached and detached without code changes. You\", \" might\\npromote a microservice from QA to a staging environment. You update the microservice\\nconfigur\", \"ation to point to the backing services in staging and inject the settings into your container\\nthroug\", \"h an environment variable.\\n\\n16\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fCloud vendor\", \"s provide APIs for you to communicate with their proprietary backing services. These\\nlibraries encap\", \"sulate the proprietary plumbing and complexity. However, communicating directly with\\nthese APIs will\", \" tightly couple your code to that specific backing service. It\\u2019s a widely accepted practice\\nto insul\", \"ate the implementation details of the vendor API. Introduce an intermediation layer, or\\nintermediate\", \" API, exposing generic operations to your service code and wrap the vendor code inside\\nit. This loos\", \"e coupling enables you to swap out one backing service for another or move your code to\\na different \", \"cloud environment without having to make changes to the mainline service code. Dapr,\\ndiscussed earli\", \"er, follows this model with its set of prebuilt building blocks.\\n\\nOn a final thought, backing servic\", \"es also promote the Statelessness principle from the Twelve-Factor\\nApplication, discussed earlier in\", \" the chapter.\\n\\nFactor #6 specifies that, \\u201cEach microservice should execute in its own process, isola\", \"ted from other\\nrunning services. Externalize required state to a backing service such as a distribut\", \"ed cache or data\\nstore.\\u201d\\n\\nBacking services are discussed in Cloud-native data patterns and Cloud-nat\", \"ive communication\\npatterns.\\n\\nAutomation\\n\\nAs you\\u2019ve seen, cloud-native systems embrace microservices,\", \" containers, and modern system design\\nto achieve speed and agility. But, that\\u2019s only part of the sto\", \"ry. How do you provision the cloud\\nenvironments upon which these systems run? How do you rapidly dep\", \"loy app features and updates?\\nHow do you round out the full picture?\\n\\nEnter the widely accepted prac\", \"tice of Infrastructure as Code, or IaC.\\n\\nWith IaC, you automate platform provisioning and applicatio\", \"n deployment. You essentially apply\\nsoftware engineering practices such as testing and versioning to\", \" your DevOps practices. Your\\ninfrastructure and deployments are automated, consistent, and repeatabl\", \"e.\\n\\nAutomating infrastructure\\n\\nTools like Azure Resource Manager, Azure Bicep, Terraform from HashiC\", \"orp, and the Azure CLI, enable\\nyou to declaratively script the cloud infrastructure you require. Res\", \"ource names, locations, capacities,\\nand secrets are parameterized and dynamic. The script is version\", \"ed and checked into source control\\nas an artifact of your project. You invoke the script to provisio\", \"n a consistent and repeatable\\ninfrastructure across system environments, such as QA, staging, and pr\", \"oduction.\\n\\nUnder the hood, IaC is idempotent, meaning that you can run the same script over and over\", \" without\\nside effects. If the team needs to make a change, they edit and rerun the script. Only the \", \"updated\\nresources are affected.\\n\\nIn the article, What is Infrastructure as Code, Author Sam Guckenhe\", \"imer describes how, \\u201cTeams who\\nimplement IaC can deliver stable environments rapidly and at scale. T\", \"hey avoid manual configuration\\nof environments and enforce consistency by representing the desired s\", \"tate of their environments via\\ncode. Infrastructure deployments with IaC are repeatable and prevent \", \"runtime issues caused by\\nconfiguration drift or missing dependencies. DevOps teams can work together\", \" with a unified set of\\n\\n17\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fpractices and to\", \"ols to deliver applications and their supporting infrastructure rapidly, reliably, and at\\nscale.\\u201d\\n\\nA\", \"utomating deployments\\n\\nThe Twelve-Factor Application, discussed earlier, calls for separate steps wh\", \"en transforming\\ncompleted code into a running application.\\n\\nFactor #5 specifies that \\u201cEach release m\", \"ust enforce a strict separation across the build, release and run\\nstages. Each should be tagged with\", \" a unique ID and support the ability to roll back.\\u201d\\n\\nModern CI/CD systems help fulfill this principl\", \"e. They provide separate build and delivery steps that\\nhelp ensure consistent and quality code that\\u2019\", \"s readily available to users.\\n\\nFigure 1-9 shows the separation across the deployment process.\\n\\nFigur\", \"e 1-9. Deployment steps in a CI/CD Pipeline\\n\\nIn the previous figure, pay special attention to separa\", \"tion of tasks:\\n\\n1.\\n\\nThe developer constructs a feature in their development environment, iterating t\", \"hrough what\\nis called the \\u201cinner loop\\u201d of code, run, and debug.\\n\\n2.  When complete, that code is pus\", \"hed into a code repository, such as GitHub, Azure DevOps, or\\n\\nBitBucket.\\n\\n3.\\n\\n4.\\n\\nThe push triggers \", \"a build stage that transforms the code into a binary artifact. The work is\\nimplemented with a Contin\", \"uous Integration (CI) pipeline. It automatically builds, tests, and\\npackages the application.\\n\\nThe r\", \"elease stage picks up the binary artifact, applies external application and environment\\nconfiguratio\", \"n information, and produces an immutable release. The release is deployed to a\\nspecified environment\", \". The work is implemented with a Continuous Delivery (CD) pipeline.\\nEach release should be identifia\", \"ble. You can say, \\u201cThis deployment is running Release 2.1.1 of\\nthe application.\\u201d\\n\\n18\\n\\nCHAPTER 1 | In\", \"troduction to cloud-native applications\\n\\n\\f5.\\n\\nFinally, the released feature is run in the target exe\", \"cution environment. Releases are\\nimmutable meaning that any change must create a new release.\\n\\nApply\", \"ing these practices, organizations have radically evolved how they ship software. Many have\\nmoved fr\", \"om quarterly releases to on-demand updates. The goal is to catch problems early in the\\ndevelopment c\", \"ycle when they\\u2019re less expensive to fix. The longer the duration between integrations,\\nthe more expe\", \"nsive problems become to resolve. With consistency in the integration process, teams\\ncan commit code\", \" changes more frequently, leading to better collaboration and software quality.\\n\\nInfrastructure as c\", \"ode and deployment automation, along with GitHub and Azure DevOps are\\ndiscussed in detail in DevOps.\", \"\\n\\nCandidate apps for cloud native\\n\\nThink about the apps your organization needs to build. Then, look\", \" at the existing apps in your\\nportfolio. How many of them warrant a cloud-native architecture? All o\", \"f them? Perhaps some?\\n\\nApplying cost/benefit analysis, there\\u2019s a good chance some wouldn\\u2019t support t\", \"he effort. The cost of\\nbecoming cloud native would far exceed the business value of the application.\", \"\\n\\nWhat type of application might be a candidate for cloud native?\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nStrategic enterpri\", \"se systems that need to constantly evolve business capabilities/features\\n\\nAn application that requir\", \"es a high release velocity - with high confidence\\n\\nA system where individual features must release w\", \"ithout a full redeployment of the entire\\nsystem\\n\\nAn application developed by teams with expertise in\", \" different technology stacks\\n\\nAn application with components that must scale independently\\n\\nSmaller,\", \" less impactful line-of-business applications might fare well with a simple monolithic\\narchitecture \", \"hosted in a Cloud PaaS environment.\\n\\nThen there are legacy systems. While we\\u2019d all like to build new\", \" applications, we\\u2019re often responsible\\nfor modernizing legacy workloads that are critical to the bus\", \"iness.\\n\\nModernizing legacy apps\\n\\nThe free Microsoft e-book Modernize existing .NET applications with\", \" Azure cloud and Windows\\nContainers provides guidance about migrating on-premises workloads into clo\", \"ud. Figure 1-10 shows\\nthat there isn\\u2019t a single, one-size-fits-all strategy for modernizing legacy a\", \"pplications.\\n\\n19\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\fFigure 1-10. Strategies fo\", \"r migrating legacy workloads\\n\\nMonolithic apps that are non-critical might benefit from a quick lift-\", \"and-shift (Cloud Infrastructure-\\nReady) migration. Here, the on-premises workload is rehosted to a c\", \"loud-based VM, without changes.\\nThis approach uses the IaaS (Infrastructure as a Service) model. Azu\", \"re includes several tools such as\\nAzure Migrate, Azure Site Recovery, and Azure Database Migration S\", \"ervice to help streamline the\\nmove. While this strategy can yield some cost savings, such applicatio\", \"ns typically weren\\u2019t designed to\\nunlock and leverage the benefits of cloud computing.\\n\\nLegacy apps t\", \"hat are critical to the business often benefit from an enhanced Cloud Optimized\\nmigration. This appr\", \"oach includes deployment optimizations that enable key cloud services - without\\nchanging the core ar\", \"chitecture of the application. For example, you might containerize the application\\nand deploy it to \", \"a container orchestrator, like Azure Kubernetes Services, discussed later in this book.\\nOnce in the \", \"cloud, the application can consume cloud backing services such as databases, message\\nqueues, monitor\", \"ing, and distributed caching.\\n\\nFinally, monolithic apps that provide strategic enterprise functions \", \"might best benefit from a Cloud-\\nNative approach, the subject of this book. This approach provides a\", \"gility and velocity. But, it comes at\\na cost of replatforming, rearchitecting, and rewriting code. O\", \"ver time, a legacy application could be\\ndecomposed into microservices, containerized, and ultimately\", \" replatformed into a cloud-native\\narchitecture.\\n\\nIf you and your team believe a cloud-native approac\", \"h is appropriate, it behooves you to rationalize\\nthe decision with your organization. What exactly i\", \"s the business problem that a cloud-native\\napproach will solve? How would it align with business nee\", \"ds?\\n\\n\\u2022\\n\\n\\u2022\\n\\n20\\n\\nRapid releases of features with increased confidence?\\n\\nFine-grained scalability - mor\", \"e efficient usage of resources?\\n\\nCHAPTER 1 | Introduction to cloud-native applications\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\nImpro\", \"ved system resiliency?\\n\\nImproved system performance?\\n\\n\\u2022  More visibility into operations?\\n\\n\\u2022\\n\\n\\u2022\\n\\nBle\", \"nd development platforms and data stores to arrive at the best tool for the job?\\n\\nFuture-proof appli\", \"cation investment?\\n\\nThe right migration strategy depends on organizational priorities and the system\", \"s you\\u2019re targeting.\\nFor many, it may be more cost effective to cloud-optimize a monolithic applicati\", \"on or add coarse-\\ngrained services to an N-Tier app. In these cases, you can still make full use of \", \"cloud PaaS capabilities\\nlike the ones offered by Azure App Service.\\n\\nSummary\\n\\nIn this chapter, we in\", \"troduced cloud-native computing. We provided a definition along with the key\\ncapabilities that drive\", \" a cloud-native application. We looked at the types of applications that might\\njustify this investme\", \"nt and effort.\\n\\nWith the introduction behind, we now dive into a much more detailed look at cloud na\", \"tive.\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\nCloud Native Computing Foundation\\n\\n.NET Microservices: Architecture for Con\", \"tainerized .NET applications\\n\\n\\u2022  Microsoft Azure Well-Architected Framework\\n\\n\\u2022  Modernize existing .\", \"NET applications with Azure cloud and Windows Containers\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nCloud Native Patterns by Co\", \"rnelia Davis\\n\\nCloud native applications: Ship faster, reduce risk, and grow your business\\n\\nDapr for \", \".NET Developers\\n\\nDapr documents\\n\\nBeyond the Twelve-Factor Application\\n\\n\\u2022  What is Infrastructure as \", \"Code\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n21\\n\\nUber Engineering\\u2019s Micro Deploy: Deploying Daily with Confidence\\n\\nHow Netflix Dep\", \"loys Code\\n\\nOverload Control for Scaling WeChat Microservices\\n\\nCHAPTER 1 | Introduction to cloud-nati\", \"ve applications\\n\\n\\fCHAPTER  2\\n\\nIntroducing\\neShopOnContainers\\nreference app\\n\\nMicrosoft, in partnership\", \" with leading community experts, has produced a full-featured cloud-native\\nmicroservices reference a\", \"pplication, eShopOnContainers. This application is built to showcase using\\n.NET and Docker, and opti\", \"onally Azure, Kubernetes, and Visual Studio, to build an online storefront.\\n\\nFigure 2-1. eShopOnCont\", \"ainers Sample App Screenshot.\\n\\n22\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\fBefore \", \"starting this chapter, we recommend that you download the eShopOnContainers reference\\napplication. I\", \"f you do so, it should be easier for you to follow along with the information presented.\\n\\nFeatures a\", \"nd requirements\\n\\nLet\\u2019s start with a review of the application\\u2019s features and requirements. The eShop\", \"OnContainers\\napplication represents an online store that sells various physical products like t-shir\", \"ts and coffee\\nmugs. If you\\u2019ve bought anything online before, the experience of using the store shoul\", \"d be relatively\\nfamiliar. Here are some of the basic features the store implements:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\", \"\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nList catalog items\\n\\nFilter items by type\\n\\nFilter items by brand\\n\\nAdd items to the sho\", \"pping basket\\n\\nEdit or remove items from the basket\\n\\nCheckout\\n\\nRegister an account\\n\\nSign in\\n\\nSign out\", \"\\n\\nReview orders\\n\\nThe application also has the following non-functional requirements:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nIt\", \" needs to be highly available and it must scale automatically to meet increased traffic (and\\nscale b\", \"ack down once traffic subsides).\\n\\nIt should provide easy-to-use monitoring of its health and diagnos\", \"tic logs to help\\ntroubleshoot any issues it encounters.\\n\\nIt should support an agile development proc\", \"ess, including support for continuous integration\\nand deployment (CI/CD).\\n\\nIn addition to the two we\", \"b front ends (traditional and Single Page Application), the\\napplication must also support mobile cli\", \"ent apps running different kinds of operating\\nsystems.\\n\\n\\u2022\\n\\nIt should support cross-platform hosting \", \"and cross-platform development.\\n\\n23\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\fFigur\", \"e 2-2. eShopOnContainers reference application development architecture.\\n\\nThe eShopOnContainers appl\", \"ication is accessible from web or mobile clients that access the\\napplication over HTTPS targeting ei\", \"ther the ASP.NET Core MVC server application or an appropriate\\nAPI Gateway. API Gateways offer sever\", \"al advantages, such as decoupling back-end services from\\nindividual front-end clients and providing \", \"better security. The application also makes use of a related\\npattern known as Backends-for-Frontends\", \" (BFF), which recommends creating separate API gateways\\nfor each front-end client. The reference arc\", \"hitecture demonstrates breaking up the API gateways\\nbased on whether the request is coming from a we\", \"b or mobile client.\\n\\nThe application\\u2019s functionality is broken up into many distinct microservices. \", \"There are services\\nresponsible for authentication and identity, listing items from the product catal\", \"og, managing users\\u2019\\nshopping baskets, and placing orders. Each of these separate services has its ow\", \"n persistent storage.\\nThere\\u2019s no single primary data store with which all services interact. Instead\", \", coordination and\\ncommunication between the services is done on an as-needed basis and by using a m\", \"essage bus.\\n\\nEach of the different microservices is designed differently, based on their individual \", \"requirements. This\\naspect means their technology stack may differ, although they\\u2019re all built using \", \".NET and designed for\\nthe cloud. Simpler services provide basic Create-Read-Update-Delete (CRUD) acc\", \"ess to the underlying\\ndata stores, while more advanced services use Domain-Driven Design approaches \", \"and patterns to\\nmanage business complexity.\\n\\n24\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference\", \" app\\n\\n\\fFigure 2-3. Different kinds of microservices.\\n\\nOverview of the code\\n\\nBecause it uses microser\", \"vices, the eShopOnContainers app includes quite a few separate projects and\\nsolutions in its GitHub \", \"repository. In addition to separate solutions and executable files, the various\\nservices are designe\", \"d to run inside their own containers, both during local development and at run\\ntime in production. F\", \"igure 2-4 shows the full Visual Studio solution, in which the various different\\nprojects are organiz\", \"ed.\\n\\n25\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\f26\\n\\nCHAPTER 2 | Introducing eShop\", \"OnContainers reference app\\n\\n\\fFigure 2-4. Projects in Visual Studio solution.\\n\\nThe code is organized \", \"to support the different microservices, and within each microservice, the code is\\nbroken up into dom\", \"ain logic, infrastructure concerns, and user interface or service endpoint. In many\\ncases, each serv\", \"ice\\u2019s dependencies can be fulfilled by Azure services in production, and alternative\\noptions for loc\", \"al development. Let\\u2019s examine how the application\\u2019s requirements map to Azure\\nservices.\\n\\nUnderstandi\", \"ng microservices\\n\\nThis book focuses on cloud-native applications built using Azure technology. To le\", \"arn more about\\nmicroservices best practices and how to architect microservice-based applications, re\", \"ad the\\ncompanion book, .NET Microservices: Architecture for Containerized .NET Applications.\\n\\nMappin\", \"g eShopOnContainers to Azure Services\\n\\nAlthough not required, Azure is well-suited to supporting the\", \" eShopOnContainers because the project\\nwas built to be a cloud-native application. The application i\", \"s built with .NET, so it can run on Linux or\\nWindows containers depending on the Docker host. The ap\", \"plication is made up of multiple\\nautonomous microservices, each with its own data. The different mic\", \"roservices showcase different\\napproaches, ranging from simple CRUD operations to more complex DDD an\", \"d CQRS patterns.\\nMicroservices communicate with clients over HTTP and with one another via message-b\", \"ased\\ncommunication. The application supports multiple platforms for clients as well, since it adopts\", \" HTTP\\nas a standard communication protocol and includes ASP.NET Core and Xamarin mobile apps that ru\", \"n\\non Android, iOS, and Windows platforms.\\n\\nThe application\\u2019s architecture is shown in Figure 2-5. On\", \" the left are the client apps, broken up into\\nmobile, traditional Web, and Web Single Page Applicati\", \"on (SPA) flavors. On the right are the server-\\nside components that make up the system, each of whic\", \"h can be hosted in Docker containers and\\nKubernetes clusters. The traditional web app is powered by \", \"the ASP.NET Core MVC application shown\\nin yellow. This app and the mobile and web SPA applications c\", \"ommunicate with the individual\\nmicroservices through one or more API gateways. The API gateways foll\", \"ow the \\u201cbackends for front\\nends\\u201d (BFF) pattern, meaning that each gateway is designed to support a g\", \"iven front-end client. The\\nindividual microservices are listed to the right of the API gateways and \", \"include both business logic\\nand some kind of persistence store. The different services make use of S\", \"QL Server databases, Redis\\ncache instances, and MongoDB/CosmosDB stores. On the far right is the sys\", \"tem\\u2019s Event Bus, which is\\nused for communication between the microservices.\\n\\n27\\n\\nCHAPTER 2 | Introdu\", \"cing eShopOnContainers reference app\\n\\n\\fFigure 2-5. The eShopOnContainers Architecture.\\n\\nThe server-s\", \"ide components of this architecture all map easily to Azure services.\\n\\nContainer orchestration and c\", \"lustering\\n\\nThe application\\u2019s container-hosted services, from ASP.NET Core MVC apps to individual Cat\", \"alog and\\nOrdering microservices, can be hosted and managed in Azure Kubernetes Service (AKS). The\\nap\", \"plication can run locally on Docker and Kubernetes, and the same containers can then be deployed\\nto \", \"staging and production environments hosted in AKS. This process can be automated as we\\u2019ll see in\\nthe\", \" next section.\\n\\nAKS provides management services for individual clusters of containers. The applicat\", \"ion will deploy\\nseparate containers for each microservice in the AKS cluster, as shown in the archit\", \"ecture diagram\\nabove. This approach allows each individual service to scale independently according \", \"to its resource\\ndemands. Each microservice can also be deployed independently, and ideally such depl\", \"oyments\\nshould incur zero system downtime.\\n\\nAPI Gateway\\n\\nThe eShopOnContainers application has multi\", \"ple front-end clients and multiple different back-end\\nservices. There\\u2019s no one-to-one correspondence\", \" between the client applications and the microservices\\nthat support them. In such a scenario, there \", \"may be a great deal of complexity when writing client\\nsoftware to interface with the various back-en\", \"d services in a secure manner. Each client would need to\\naddress this complexity on its own, resulti\", \"ng in duplication and many places in which to make updates\\nas services change or new policies are im\", \"plemented.\\n\\nAzure API Management (APIM) helps organizations publish APIs in a consistent, manageable\", \" fashion.\\nAPIM consists of three components: the API Gateway, and administration portal (the Azure p\", \"ortal),\\nand a developer portal.\\n\\n28\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\fThe A\", \"PI Gateway accepts API calls and routes them to the appropriate back-end API. It can also\\nprovide ad\", \"ditional services like verification of API keys or JWT tokens and API transformation on the\\nfly with\", \"out code modifications (for instance, to accommodate clients expecting an older interface).\\n\\nThe Azu\", \"re portal is where you define the API schema and package different APIs into products. You\\nalso conf\", \"igure user access, view reports, and configure policies for quotas or transformations.\\n\\nThe develope\", \"r portal serves as the main resource for developers. It provides developers with API\\ndocumentation, \", \"an interactive test console, and reports on their own usage. Developers also use the\\nportal to creat\", \"e and manage their own accounts, including subscription and API key support.\\n\\nUsing APIM, applicatio\", \"ns can expose several different groups of services, each providing a back end\\nfor a particular front\", \"-end client. APIM is recommended for complex scenarios. For simpler needs, the\\nlightweight API Gatew\", \"ay Ocelot can be used. The eShopOnContainers app uses Ocelot because of its\\nsimplicity and because i\", \"t can be deployed into the same application environment as the application\\nitself. Learn more about \", \"eShopOnContainers, APIM, and Ocelot.\\n\\nAnother option if your application is using AKS is to deploy t\", \"he Azure Gateway Ingress Controller as a\\npod within your AKS cluster. This approach allows your clus\", \"ter to integrate with an Azure Application\\nGateway, allowing the gateway to load-balance traffic to \", \"the AKS pods. Learn more about the Azure\\nGateway Ingress Controller for AKS.\\n\\nData\\n\\nThe various back\", \"-end services used by eShopOnContainers have different storage requirements.\\nSeveral microservices u\", \"se SQL Server databases. The Basket microservice leverages a Redis cache for\\nits persistence. The Lo\", \"cations microservice expects a MongoDB API for its data. Azure supports each\\nof these data formats.\\n\", \"\\nFor SQL Server database support, Azure has products for everything from single databases up to\\nhigh\", \"ly scalable SQL Database elastic pools. Individual microservices can be configured to\\ncommunicate wi\", \"th their own individual SQL Server databases quickly and easily. These databases can\\nbe scaled as ne\", \"eded to support each separate microservice according to its needs.\\n\\nThe eShopOnContainers applicatio\", \"n stores the user\\u2019s current shopping basket between requests. This\\naspect is managed by the Basket m\", \"icroservice that stores the data in a Redis cache. In development,\\nthis cache can be deployed in a c\", \"ontainer, while in production it can utilize Azure Cache for Redis.\\nAzure Cache for Redis is a fully\", \" managed service offering high performance and reliability without the\\nneed to deploy and manage Red\", \"is instances or containers on your own.\\n\\nThe Locations microservice uses a MongoDB NoSQL database fo\", \"r its persistence. During\\ndevelopment, the database can be deployed in its own container, while in p\", \"roduction the service can\\nleverage Azure Cosmos DB\\u2019s API for MongoDB. One of the benefits of Azure C\", \"osmos DB is its ability\\nto leverage multiple different communication protocols, including a SQL API \", \"and common NoSQL\\nAPIs including MongoDB, Cassandra, Gremlin, and Azure Table Storage. Azure Cosmos D\", \"B offers a\\nfully managed and globally distributed database as a service that can scale to meet the n\", \"eeds of the\\nservices that use it.\\n\\nDistributed data in cloud-native applications is covered in more \", \"detail in chapter 5.\\n\\n29\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\fEvent Bus\\n\\nThe a\", \"pplication uses events to communicate changes between different services. This functionality can\\nbe \", \"implemented with various implementations, and locally the eShopOnContainers application uses\\nRabbitM\", \"Q. When hosted in Azure, the application would leverage Azure Service Bus for its messaging.\\nAzure S\", \"ervice Bus is a fully managed integration message broker that allows applications and services\\nto co\", \"mmunicate with one another in a decoupled, reliable, asynchronous manner. Azure Service Bus\\nsupports\", \" individual queues as well as separate topics to support publisher-subscriber scenarios. The\\neShopOn\", \"Containers application would leverage topics with Azure Service Bus to support distributing\\nmessages\", \" from one microservice to any other microservice that needed to react to a given message.\\n\\nResilienc\", \"y\\n\\nOnce deployed to production, the eShopOnContainers application would be able to take advantage\\nof\", \" several Azure services available to improve its resiliency. The application publishes health checks\", \",\\nwhich can be integrated with Application Insights to provide reporting and alerts based on the app\", \"\\u2019s\\navailability. Azure resources also provide diagnostic logs that can be used to identify and corre\", \"ct bugs\\nand performance issues. Resource logs provide detailed information on when and how different\", \" Azure\\nresources are used by the application. You\\u2019ll learn more about cloud-native resiliency featur\", \"es in\\nchapter 6.\\n\\nDeploying eShopOnContainers to Azure\\n\\nThe eShopOnContainers application can be dep\", \"loyed to various Azure platforms. The recommended\\napproach is to deploy the application to Azure Kub\", \"ernetes Services (AKS). Helm, a Kubernetes\\ndeployment tool, is available to reduce deployment comple\", \"xity. Optionally, developers may\\nimplement Azure Dev Spaces for Kubernetes to streamline their devel\", \"opment process.\\n\\nAzure Kubernetes Service\\n\\nTo host eShop in AKS, the first step is to create an AKS \", \"cluster. To do so, you might use the Azure\\nportal, which will walk you through the required steps. Y\", \"ou could also create a cluster from the Azure\\nCLI, taking care to enable Role-Based Access Control (\", \"RBAC) and application routing. The\\neShopOnContainers\\u2019 documentation details the steps for creating y\", \"our own AKS cluster. Once created,\\nyou can access and manage the cluster from the Kubernetes dashboa\", \"rd.\\n\\nYou can now deploy the eShop application to the cluster using Helm.\\n\\nDeploying to Azure Kuberne\", \"tes Service using Helm\\n\\nHelm is an application package manager tool that works directly with Kuberne\", \"tes. It helps you define,\\ninstall, and upgrade Kubernetes applications. While simple apps can be dep\", \"loyed to AKS with custom\\nCLI scripts or simple deployment files, complex apps can contain many Kuber\", \"netes objects and benefit\\nfrom Helm.\\n\\nUsing Helm, applications include text-based configuration file\", \"s, called Helm charts, which declaratively\\ndescribe the application and configuration in Helm packag\", \"es. Charts use standard YAML-formatted\\n\\n30\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\", \"\\n\\ffiles to describe a related set of Kubernetes resources. They\\u2019re versioned alongside the applicati\", \"on\\ncode they describe. Helm Charts range from simple to complex depending on the requirements of the\", \"\\ninstallation they describe.\\n\\nHelm is composed of a command-line client tool, which consumes helm ch\", \"arts and launches\\ncommands to a server component named, Tiller. Tiller communicates with the Kuberne\", \"tes API to\\nensure the correct provisioning of your containerized workloads. Helm is maintained by th\", \"e Cloud-\\nnative Computing Foundation.\\n\\nThe following yaml file presents a Helm template:\\n\\napiVersion\", \": v1\\nkind: Service\\nmetadata:\\n  name: {{ .Values.app.svc.marketing }}\\n  labels:\\n    app: {{ template \", \"\\\"marketing-api.name\\\" . }}\\n    chart: {{ template \\\"marketing-api.chart\\\" . }}\\n    release: {{ .Release\", \".Name }}\\n    heritage: {{ .Release.Service }}\\nspec:\\n  type: {{ .Values.service.type }}\\n  ports:\\n    \", \"- port: {{ .Values.service.port }}\\n      targetPort: http\\n      protocol: TCP\\n      name: http\\n  sel\", \"ector:\\n    app: {{ template \\\"marketing-api.name\\\" . }}\\n    release: {{ .Release.Name }}\\n\\nNote how the\", \" template describes a dynamic set of key/value pairs. When the template is invoked,\\nvalues that encl\", \"osed in curly braces are pulled in from other yaml-based configuration files.\\n\\nYou\\u2019ll find the eShop\", \"OnContainers helm charts in the /k8s/helm folder. Figure 2-6 shows how the\\ndifferent components of t\", \"he application are organized into a folder structure used by helm to define\\nand managed deployments.\", \"\\n\\n31\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\fFigure 2-6. The eShopOnContainers he\", \"lm folder.\\n\\nEach individual component is installed using a helm install command. eShop includes a \\u201cd\", \"eploy all\\u201d\\nscript that loops through and installs the components using their respective helm charts.\", \" The result is\\na repeatable process, versioned with the application in source control, that anyone o\", \"n the team can\\ndeploy to an AKS cluster with a one-line script command.\\n\\nNote that version 3 of Helm\", \" officially removes the need for the Tiller server component. More\\ninformation on this enhancement c\", \"an be found here.\\n\\nAzure Functions and Logic Apps (Serverless)\\n\\nThe eShopOnContainers sample include\", \"s support for tracking online marketing campaigns. An Azure\\nFunction is used to track marketing camp\", \"aign details for a given campaign ID. Rather than creating a\\n\\n32\\n\\nCHAPTER 2 | Introducing eShopOnCon\", \"tainers reference app\\n\\n\\ffull microservice, a single Azure Function is simpler and sufficient. Azure \", \"Functions have a simple build\\nand deployment model, especially when configured to run in Kubernetes.\", \" Deploying the function is\\nscripted using Azure Resource Manager (ARM) templates and the Azure CLI. \", \"This campaign service\\nisn\\u2019t customer-facing and invokes a single operation, making it a great candid\", \"ate for Azure Functions.\\nThe function requires minimal configuration, including a database connectio\", \"n string data and image\\nbase URI settings. You configure Azure Functions in the Azure portal.\\n\\nCentr\", \"alized configuration\\n\\nUnlike a monolithic app in which everything runs within a single instance, a c\", \"loud-native application\\nconsists of independent services distributed across virtual machines, contai\", \"ners, and geographic\\nregions. Managing configuration settings for dozens of interdependent services \", \"can be challenging.\\nDuplicate copies of configuration settings across different locations are error \", \"prone and difficult to\\nmanage. Centralized configuration is a critical requirement for distributed c\", \"loud-native applications.\\n\\nAs discussed in Chapter 1, the Twelve-Factor App recommendations require \", \"strict separation between\\ncode and configuration. Configuration must be stored externally from the a\", \"pplication and read-in as\\nneeded. Storing configuration values as constants or literal values in cod\", \"e is a violation. The same\\nconfiguration values are often be used by many services in the same appli\", \"cation. Additionally, we\\nmust support the same values across multiple environments, such as dev, tes\", \"ting, and production. The\\nbest practice is store them in a centralized configuration store.\\n\\nThe Azu\", \"re cloud presents several great options.\\n\\nAzure App Configuration\\n\\nAzure App Configuration is a full\", \"y managed Azure service that stores non-secret configuration\\nsettings in a secure, centralized locat\", \"ion. Stored values can be shared among multiple services and\\napplications.\\n\\nThe service is simple to\", \" use and provides several benefits:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nFlexible key/value representations and mappings\\n\", \"\\nTagging with Azure labels\\n\\nDedicated UI for management\\n\\nEncryption of sensitive information\\n\\nQueryi\", \"ng and batch retrieval\\n\\nAzure App Configuration maintains changes made to key-value settings for sev\", \"en days. The point-in-\\ntime snapshot feature enables you to reconstruct the history of a setting and\", \" even rollback for a failed\\ndeployment.\\n\\nApp Configuration automatically caches each setting to avoi\", \"d excessive calls to the configuration\\nstore. The refresh operation waits until the cached value of \", \"a setting expires to update that setting,\\neven when its value changes in the configuration store. Th\", \"e default cache expiration time is 30\\nseconds. You can override the expiration time.\\n\\n33\\n\\nCHAPTER 2 \", \"| Introducing eShopOnContainers reference app\\n\\n\\fApp Configuration encrypts all configuration values \", \"in transit and at rest. Key names and labels are\\nused as indexes for retrieving configuration data a\", \"nd aren\\u2019t encrypted.\\n\\nAlthough App Configuration provides hardened security, Azure Key Vault is stil\", \"l the best place for\\nstoring application secrets. Key Vault provides hardware-level encryption, gran\", \"ular access policies, and\\nmanagement operations such as certificate rotation. You can create App Con\", \"figuration values that\\nreference secrets stored in a Key Vault.\\n\\nAzure Key Vault\\n\\nKey Vault is a man\", \"aged service for securely storing and accessing secrets. A secret is anything that you\\nwant to tight\", \"ly control access to, such as API keys, passwords, or certificates. A vault is a logical group\\nof se\", \"crets.\\n\\nKey Vault greatly reduces the chances that secrets may be accidentally leaked. When using Ke\", \"y Vault,\\napplication developers no longer need to store security information in their application. T\", \"his practice\\neliminates the need to store this information inside your code. For example, an applica\", \"tion may need\\nto connect to a database. Instead of storing the connection string in the app\\u2019s code, \", \"you can store it\\nsecurely in Key Vault.\\n\\nYour applications can securely access the information they \", \"need by using URIs. These URIs allow the\\napplications to retrieve specific versions of a secret. The\", \"re\\u2019s no need to write custom code to protect\\nany of the secret information stored in Key Vault.\\n\\nAcc\", \"ess to Key Vault requires proper caller authentication and authorization. Typically, each cloud-\\nnat\", \"ive microservice uses a ClientId/ClientSecret combination. It\\u2019s important to keep these credentials\\n\", \"outside source control. A best practice is to set them in the application\\u2019s environment. Direct acce\", \"ss to\\nKey Vault from AKS can be achieved using Key Vault FlexVolume.\\n\\nConfiguration in eShop\\n\\nThe eS\", \"hopOnContainers application includes local application settings files with each microservice.\\nThese \", \"files are checked into source control, but don\\u2019t include production secrets such as connection\\nstrin\", \"gs or API keys. In production, individual settings may be overwritten with per-service environment\\nv\", \"ariables. Injecting secrets in environment variables is a common practice for hosted applications, b\", \"ut\\ndoesn\\u2019t provide a central configuration store. To support centralized management of configuration\", \"\\nsettings, each microservice includes a setting to toggle between its use of local settings or Azure\", \" Key\\nVault settings.\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n34\\n\\nThe eShopOnContainers Architecture\\n\\nOrchestr\", \"ating microservices and multi-container applications for high scalability and\\navailability\\n\\nAzure AP\", \"I Management\\n\\nAzure SQL Database Overview\\n\\nAzure Cache for Redis\\n\\nAzure Cosmos DB\\u2019s API for MongoDB\\n\", \"\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure Service Bus\\n\\nAzure M\", \"onitor overview\\n\\neShopOnContainers: Create Kubernetes cluster in AKS\\n\\neShopOnContainers: Azure Dev S\", \"paces\\n\\nAzure Dev Spaces\\n\\n35\\n\\nCHAPTER 2 | Introducing eShopOnContainers reference app\\n\\n\\fCHAPTER  3\\n\\nS\", \"caling cloud-native\\napplications\\n\\nOne of the most-often touted advantages of moving to a cloud hosti\", \"ng environment is scalability.\\nScalability, or the ability for an application to accept additional u\", \"ser load without compromising\\nperformance for each user. It\\u2019s most often achieved by breaking up an \", \"application into small pieces\\nthat can each be given whatever resources they require. Cloud vendors \", \"enable massive scalability\\nanytime and anywhere in the world.\\n\\nIn this chapter, we discuss technolog\", \"ies that enable cloud-native applications to scale to meet user\\ndemand. These technologies include:\\n\", \"\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nContainers\\n\\nOrchestrators\\n\\nServerless computing\\n\\nLeveraging containers and orchestrators\\n\\n\", \"Containers and orchestrators are designed to solve problems common to monolithic deployment\\napproach\", \"es.\\n\\nChallenges with monolithic deployments\\n\\nTraditionally, most applications have been deployed as \", \"a single unit. Such applications are referred to\\nas a monolith. This general approach of deploying a\", \"pplications as single units even if they\\u2019re\\ncomposed of multiple modules or assemblies is known as m\", \"onolithic architecture, as shown in Figure\\n3-1.\\n\\n36\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\", \"\\fFigure 3-1. Monolithic architecture.\\n\\nAlthough they have the benefit of simplicity, monolithic arch\", \"itectures face many challenges:\\n\\nDeployment\\n\\nAdditionally, they require a restart of the application\", \", which may temporarily impact availability if\\nzero-downtime techniques are not applied while deploy\", \"ing.\\n\\nScaling\\n\\nA monolithic application is hosted entirely on a single machine instance, often requi\", \"ring high-\\ncapability hardware. If any part of the monolith requires scaling, another copy of the en\", \"tire application\\nmust be deployed to another machine. With a monolith, you can\\u2019t scale application c\", \"omponents\\nindividually - it\\u2019s all or nothing. Scaling components that don\\u2019t require scaling results \", \"in inefficient and\\ncostly resource usage.\\n\\nEnvironment\\n\\nMonolithic applications are typically deploy\", \"ed to a hosting environment with a pre-installed operating\\nsystem, runtime, and library dependencies\", \". This environment may not match that upon which the\\napplication was developed or tested. Inconsiste\", \"ncies across application environments are a common\\nsource of problems for monolithic deployments.\\n\\nC\", \"oupling\\n\\nA monolithic application is likely to experience high coupling across its functional compon\", \"ents.\\nWithout hard boundaries, system changes often result in unintended and costly side effects. Ne\", \"w\\nfeatures/fixes become tricky, time-consuming, and expensive to implement. Updates require extensiv\", \"e\\ntesting. Coupling also makes it difficult to refactor components or swap in alternative\\nimplementa\", \"tions. Even when constructed with a strict separation of concerns, architectural erosion\\nsets in as \", \"the monolithic code base deteriorates with never-ending \\u201cspecial cases.\\u201d\\n\\n37\\n\\nCHAPTER 3 | Scaling cl\", \"oud-native applications\\n\\n\\fPlatform lock-in\\n\\nA monolithic application is constructed with a single te\", \"chnology stack. While offering uniformity, this\\ncommitment can become a barrier to innovation. New f\", \"eatures and components will be built using\\nthe application\\u2019s current stack - even when more modern t\", \"echnologies may be a better choice. A\\nlonger-term risk is your technology stack becoming outdated an\", \"d obsolete. Rearchitecting an entire\\napplication to a new, more modern platform is at best expensive\", \" and risky.\\n\\nWhat are the benefits of containers and orchestrators?\\n\\nWe introduced containers in Cha\", \"pter 1. We highlighted how the Cloud Native Computing Foundation\\n(CNCF) ranks containerization as th\", \"e first step in their Cloud-Native Trail Map - guidance for\\nenterprises beginning their cloud-native\", \" journey. In this section, we discuss the benefits of containers.\\n\\nDocker is the most popular contai\", \"ner management platform. It works with containers on both Linux or\\nWindows. Containers provide separ\", \"ate but reproducible application environments that run the same\\nway on any system. This aspect makes\", \" them perfect for developing and hosting cloud-native services.\\nContainers are isolated from one ano\", \"ther. Two containers on the same host hardware can have\\ndifferent versions of software, without caus\", \"ing conflicts.\\n\\nContainers are defined by simple text-based files that become project artifacts and \", \"are checked into\\nsource control. While full servers and virtual machines require manual effort to up\", \"date, containers are\\neasily version-controlled. Apps built to run in containers can be developed, te\", \"sted, and deployed\\nusing automated tools as part of a build pipeline.\\n\\nContainers are immutable. Onc\", \"e you define a container, you can recreate and run it exactly the same\\nway. This immutability lends \", \"itself to component-based design. If some parts of an application evolve\\ndifferently than others, wh\", \"y redeploy the entire app when you can just deploy the parts that change\\nmost frequently? Different \", \"features and cross-cutting concerns of an app can be broken up into\\nseparate units. Figure 3-2 shows\", \" how a monolithic app can take advantage of containers and\\nmicroservices by delegating certain featu\", \"res or functionality. The remaining functionality in the app\\nitself has also been containerized.\\n\\n38\", \"\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fFigure 3-2. Decomposing a monolithic app to embrac\", \"e microservices.\\n\\nEach cloud-native service is built and deployed in a separate container. Each can \", \"update as needed.\\nIndividual services can be hosted on nodes with resources appropriate to each serv\", \"ice. The\\nenvironment each service runs in is immutable, shared across dev, test, and production envi\", \"ronments,\\nand easily versioned. Coupling between different areas of the application occurs explicitl\", \"y as calls or\\nmessages between services, not compile-time dependencies within the monolith. You can \", \"also choose\\nthe technology that best suites a given capability without requiring changes to the rest\", \" of the app.\\n\\nContainerized services require automated management. It wouldn\\u2019t be feasible to manual\", \"ly administer\\na large set of independently deployed containers. For example, consider the following \", \"tasks:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nHow will container instances be provisioned across a cluster of many machi\", \"nes?\\n\\nOnce deployed, how will containers discover and communicate with each other?\\n\\nHow can containe\", \"rs scale in or out on-demand?\\n\\nHow do you monitor the health of each container?\\n\\nHow do you protect \", \"a container against hardware and software failures?\\n\\nHow do upgrade containers for a live applicatio\", \"n with zero downtime?\\n\\nContainer orchestrators address and automate these and other concerns.\\n\\nIn th\", \"e cloud-native eco-system, Kubernetes has become the de facto container orchestrator. It\\u2019s an\\nopen-s\", \"ource platform managed by the Cloud Native Computing Foundation (CNCF). Kubernetes\\nautomates the dep\", \"loyment, scaling, and operational concerns of containerized workloads across a\\nmachine cluster. Howe\", \"ver, installing and managing Kubernetes is notoriously complex.\\n\\n39\\n\\nCHAPTER 3 | Scaling cloud-nativ\", \"e applications\\n\\n\\fA much better approach is to leverage Kubernetes as a managed service from a cloud \", \"vendor. The\\nAzure cloud features a fully managed Kubernetes platform entitled Azure Kubernetes Servi\", \"ce (AKS).\\nAKS abstracts the complexity and operational overhead of managing Kubernetes. You consume\\n\", \"Kubernetes as a cloud service; Microsoft takes responsibility for managing and supporting it. AKS al\", \"so\\ntightly integrates with other Azure services and dev tools.\\n\\nAKS is a cluster-based technology. A\", \" pool of federated virtual machines, or nodes, is deployed to the\\nAzure cloud. Together they form a \", \"highly available environment, or cluster. The cluster appears as a\\nseamless, single entity to your c\", \"loud-native application. Under the hood, AKS deploys your\\ncontainerized services across these nodes \", \"following a predefined strategy that evenly distributes the\\nload.\\n\\nWhat are the scaling benefits?\\n\\nS\", \"ervices built on containers can leverage scaling benefits provided by orchestration tools like\\nKuber\", \"netes. By design containers only know about themselves. Once you have multiple containers\\nthat need \", \"to work together, you should organize them at a higher level. Organizing large numbers of\\ncontainers\", \" and their shared dependencies, such as network configuration, is where orchestration tools\\ncome in \", \"to save the day! Kubernetes creates an abstraction layer over groups of containers and\\norganizes the\", \"m into pods. Pods run on worker machines referred to as nodes. This organized structure\\nis referred \", \"to as a cluster. Figure 3-3 shows the different components of a Kubernetes cluster.\\n\\nFigure 3-3. Kub\", \"ernetes cluster components.\\n\\nScaling containerized workloads is a key feature of container orchestra\", \"tors. AKS supports automatic\\nscaling across two dimensions: Container instances and compute nodes. T\", \"ogether they give AKS the\\nability to quickly and efficiently respond to spikes in demand and add add\", \"itional resources. We\\ndiscuss scaling in AKS later in this chapter.\\n\\n40\\n\\nCHAPTER 3 | Scaling cloud-n\", \"ative applications\\n\\n\\fDeclarative versus imperative\\n\\nKubernetes supports both declarative and imperat\", \"ive configuration. The imperative approach involves\\nrunning various commands that tell Kubernetes wh\", \"at to do each step of the way. Run this image.\\nDelete this pod. Expose this port. With the declarati\", \"ve approach, you create a configuration file, called\\na manifest, to describe what you want instead o\", \"f what to do. Kubernetes reads the manifest and\\ntransforms your desired end state into actual end st\", \"ate.\\n\\nImperative commands are great for learning and interactive experimentation. However, you\\u2019ll wa\", \"nt to\\ndeclaratively create Kubernetes manifest files to embrace an infrastructure as code approach,\\n\", \"providing for reliable and repeatable deployments. The manifest file becomes a project artifact and \", \"is\\nused in your CI/CD pipeline for automating Kubernetes deployments.\\n\\nIf you\\u2019ve already configured \", \"your cluster using imperative commands, you can export a declarative\\nmanifest by using kubectl get s\", \"vc SERVICENAME -o yaml > service.yaml. This command produces a\\nmanifest similar to one shown below:\\n\", \"\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  creationTimestamp: \\\"2019-09-13T13:58:47Z\\\"\\n  labels:\\n    co\", \"mponent: apiserver\\n    provider: kubernetes\\n  name: kubernetes\\n  namespace: default\\n  resourceVersio\", \"n: \\\"153\\\"\\n  selfLink: /api/v1/namespaces/default/services/kubernetes\\n  uid: 9b1fac62-d62e-11e9-8968-0\", \"0155d38010d\\nspec:\\n  clusterIP: 10.96.0.1\\n  ports:\\n  - name: https\\n    port: 443\\n    protocol: TCP\\n  \", \"  targetPort: 6443\\n  sessionAffinity: None\\n  type: ClusterIP\\nstatus:\\n  loadBalancer: {}\\n\\nWhen using \", \"declarative configuration, you can preview the changes that will be made before\\ncommitting them by u\", \"sing kubectl diff -f FOLDERNAME against the folder where your configuration\\nfiles are located. Once \", \"you\\u2019re sure you want to apply the changes, run kubectl apply -f FOLDERNAME.\\nAdd -R to recursively pr\", \"ocess a folder hierarchy.\\n\\nYou can also use declarative configuration with other Kubernetes features\", \", one of which being\\ndeployments. Declarative deployments help manage releases, updates, and scaling\", \". They instruct the\\nKubernetes deployment controller on how to deploy new changes, scale out load, o\", \"r roll back to a\\nprevious revision. If a cluster is unstable, a declarative deployment will automati\", \"cally return the cluster\\nback to a desired state. For example, if a node should crash, the deploymen\", \"t mechanism will redeploy\\na replacement to achieve your desired state\\n\\n41\\n\\nCHAPTER 3 | Scaling cloud\", \"-native applications\\n\\n\\fUsing declarative configuration allows infrastructure to be represented as co\", \"de that can be checked in\\nand versioned alongside the application code. It provides improved change \", \"control and better\\nsupport for continuous deployment using a build and deploy pipeline.\\n\\nWhat scenar\", \"ios are ideal for containers and orchestrators?\\n\\nThe following scenarios are ideal for using contain\", \"ers and orchestrators.\\n\\nApplications requiring high uptime and scalability\\n\\nIndividual applications \", \"that have high uptime and scalability requirements are ideal candidates for\\ncloud-native architectur\", \"es using microservices, containers, and orchestrators. They can be developed\\nin containers, tested a\", \"cross versioned environments, and deployed into production with zero\\ndowntime. The use of Kubernetes\", \" clusters ensures such apps can also scale on demand and recover\\nautomatically from node failures.\\n\\n\", \"Large numbers of applications\\n\\nOrganizations that deploy and maintain large numbers of applications \", \"benefit from containers and\\norchestrators. The up front effort of setting up containerized environme\", \"nts and Kubernetes clusters is\\nprimarily a fixed cost. Deploying, maintaining, and updating individu\", \"al applications has a cost that\\nvaries with the number of applications. Beyond a few applications, t\", \"he complexity of maintaining\\ncustom applications manually exceeds the cost of implementing a solutio\", \"n using containers and\\norchestrators.\\n\\nWhen should you avoid using containers and orchestrators?\\n\\nIf\", \" you\\u2019re unable to build your application following the Twelve-Factor App principles, you should\\ncons\", \"ider avoiding containers and orchestrators. In these cases, consider a VM-based hosting platform,\\nor\", \" possibly some hybrid system. With it, you can always spin off certain pieces of functionality into\\n\", \"separate containers or even serverless functions.\\n\\nDevelopment resources\\n\\nThis section shows a short\", \" list of development resources that may help you get started using\\ncontainers and orchestrators for \", \"your next application. If you\\u2019re looking for guidance on how to\\ndesign your cloud-native microservic\", \"es architecture app, read this book\\u2019s companion, .NET\\nMicroservices: Architecture for Containerized \", \".NET Applications.\\n\\nLocal Kubernetes Development\\n\\nKubernetes deployments provide great value in prod\", \"uction environments, but can also run locally on\\nyour development machine. While you may work on ind\", \"ividual microservices independently, there\\nmay be times when you\\u2019ll need to run the entire system lo\", \"cally - just as it will run when deployed to\\nproduction. There are several tools that can help: Mini\", \"kube and Docker Desktop. Visual Studio also\\nprovides tooling for Docker development.\\n\\n42\\n\\nCHAPTER 3 \", \"| Scaling cloud-native applications\\n\\n\\fMinikube\\n\\nWhat is Minikube? The Minikube project says \\u201cMinikub\", \"e implements a local Kubernetes cluster on\\nmacOS, Linux, and Windows.\\u201d Its primary goals are \\u201cto be \", \"the best tool for local Kubernetes\\napplication development and to support all Kubernetes features th\", \"at fit.\\u201d Installing Minikube is\\nseparate from Docker, but Minikube supports different hypervisors th\", \"an Docker Desktop supports.\\nThe following Kubernetes features are currently supported by Minikube:\\n\\n\", \"\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nDNS\\n\\nNodePorts\\n\\nConfigMaps and secrets\\n\\nDashboards\\n\\nContainer runtimes: Docker,\", \" rkt, CRI-O, and containerd\\n\\nEnabling Container Network Interface (CNI)\\n\\nIngress\\n\\nAfter installing M\", \"inikube, you can quickly start using it by running the minikube start command, which\\ndownloads an im\", \"age and start the local Kubernetes cluster. Once the cluster is started, you interact\\nwith it using \", \"the standard Kubernetes kubectl commands.\\n\\nDocker Desktop\\n\\nYou can also work with Kubernetes directl\", \"y from Docker Desktop on Windows. It is your only option if\\nyou\\u2019re using Windows Containers, and is \", \"a great choice for non-Windows containers as well. Figure 3-\\n4 shows how to enable local Kubernetes \", \"support when running Docker Desktop.\\n\\nFigure 3-4. Configuring Kubernetes in Docker Desktop.\\n\\n43\\n\\nCHA\", \"PTER 3 | Scaling cloud-native applications\\n\\n\\fDocker Desktop is the most popular tool for configuring\", \" and running containerized apps locally.\\nWhen you work with Docker Desktop, you can develop locally \", \"against the exact same set of Docker\\ncontainer images that you\\u2019ll deploy to production. Docker Deskt\", \"op is designed to \\u201cbuild, test, and\\nship\\u201d containerized apps locally. It supports both Linux and Win\", \"dows containers. Once you push your\\nimages to an image registry, like Azure Container Registry or Do\", \"cker Hub, AKS can pull and deploy\\nthem to production.\\n\\nVisual Studio Docker Tooling\\n\\nVisual Studio s\", \"upports Docker development for web-based applications. When you create a new\\nASP.NET Core applicatio\", \"n, you have an option to configure it with Docker support, as shown in Figure\\n3-5.\\n\\nFigure 3-5. Visu\", \"al Studio Enable Docker Support\\n\\nWhen this option is selected, the project is created with a Dockerf\", \"ile in its root, which can be used to\\nbuild and host the app in a Docker container. An example Docke\", \"rfile is shown in Figure 3-6.\\n\\nFROM mcr.microsoft.com/dotnet/aspnet:7.0 AS base\\nWORKDIR /app\\nEXPOSE \", \"80\\nEXPOSE 443\\n\\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build\\nWORKDIR /src\\nCOPY [\\\"eShopWeb/eShopWeb.\", \"csproj\\\", \\\"eShopWeb/\\\"]\\nRUN dotnet restore \\\"eShopWeb/eShopWeb.csproj\\\"\\nCOPY . .\\nWORKDIR \\\"/src/eShopWeb\\\"\", \"\\nRUN dotnet build \\\"eShopWeb.csproj\\\" -c Release -o /app/build\\n\\nFROM build AS publish\\nRUN dotnet publi\", \"sh \\\"eShopWeb.csproj\\\" -c Release -o /app/publish\\n\\n44\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\", \"\\fFROM base AS final\\nWORKDIR /app\\nCOPY --from=publish /app/publish .\\nENTRYPOINT [\\\"dotnet\\\", \\\"eShopWeb.\", \"dll\\\"]\\n\\nFigure 3-6. Visual Studio generated Dockerfile\\n\\nOnce support is added, you can run your appli\", \"cation in a Docker container in Visual Studio. Figure 3-7\\nshows the different run options available \", \"from a new ASP.NET Core project created with Docker\\nsupport added.\\n\\nFigure 3-7. Visual Studio Docker\", \" Run Options\\n\\nAlso, at any time you can add Docker support to an existing ASP.NET Core application. \", \"From the\\nVisual Studio Solution Explorer, right-click on the project and select Add > Docker Support\", \", as shown\\nin Figure 3-8.\\n\\n45\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fFigure 3-8. Adding Do\", \"cker support to Visual Studio\\n\\nVisual Studio Code Docker Tooling\\n\\nThere are many extensions availabl\", \"e for Visual Studio Code that support Docker development.\\n\\nMicrosoft provides the Docker for Visual \", \"Studio Code extension. This extension simplifies the process\\nof adding container support to applicat\", \"ions. It scaffolds required files, builds Docker images, and\\nenables you to debug your app inside a \", \"container. The extension features a visual explorer that makes\\nit easy to take actions on containers\", \" and images such as start, stop, inspect, remove, and more. The\\nextension also supports Docker Compo\", \"se enabling you to manage multiple running containers as a\\nsingle unit.\\n\\nLeveraging serverless funct\", \"ions\\n\\nIn the spectrum from managing physical machines to leveraging cloud capabilities, serverless l\", \"ives at\\nthe extreme end. Your only responsibility is your code, and you only pay when your code runs\", \". Azure\\nFunctions provides a way to build serverless capabilities into your cloud-native application\", \"s.\\n\\n46\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fWhat is serverless?\\n\\nServerless is a relativ\", \"ely new service model of cloud computing. It doesn\\u2019t mean that servers are\\noptional - your code stil\", \"l runs on a server somewhere. The distinction is that the application team no\\nlonger concerns itself\", \" with managing server infrastructure. Instead, the cloud vendor own this\\nresponsibility. The develop\", \"ment team increases its productivity by delivering business solutions to\\ncustomers, not plumbing.\\n\\nS\", \"erverless computing uses event-triggered stateless containers to host your services. They can scale\\n\", \"out and in to meet demand as-needed. Serverless platforms like Azure Functions have tight\\nintegratio\", \"n with other Azure services like queues, events, and storage.\\n\\nWhat challenges are solved by serverl\", \"ess?\\n\\nServerless platforms address many time-consuming and expensive concerns:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nPurch\", \"asing machines and software licenses\\n\\nHousing, securing, configuring, and maintaining the machines a\", \"nd their networking, power,\\nand A/C requirements\\n\\nPatching and upgrading operating systems and softw\", \"are\\n\\nConfiguring web servers or machine services to host application software\\n\\nConfiguring applicati\", \"on software within its platform\\n\\nMany companies allocate large budgets to support hardware infrastru\", \"cture concerns. Moving to the\\ncloud can help reduce these costs; shifting applications to serverless\", \" can help eliminate them.\\n\\nWhat is the difference between a microservice and a serverless\\nfunction?\\n\", \"\\nTypically, a microservice encapsulates a business capability, such as a shopping cart for an online\", \"\\neCommerce site. It exposes multiple operations that enable a user to manage their shopping\\nexperien\", \"ce. A function, however, is a small, lightweight block of code that executes a single-purpose\\noperat\", \"ion in response to an event. Microservices are typically constructed to respond to requests,\\noften f\", \"rom an interface. Requests can be HTTP Rest- or gRPC-based. Serverless services respond to\\nevents. I\", \"ts event-driven architecture is ideal for processing short-running, background tasks.\\n\\nWhat scenario\", \"s are appropriate for serverless?\\n\\nServerless exposes individual short-running functions that are in\", \"voked in response to a trigger. This\\nmakes them ideal for processing background tasks.\\n\\nAn applicati\", \"on might need to send an email as a step in a workflow. Instead of sending the\\nnotification as part \", \"of a microservice request, place the message details onto a queue. An Azure\\nFunction can dequeue the\", \" message and asynchronously send the email. Doing so could improve the\\nperformance and scalability o\", \"f the microservice. Queue-based load leveling can be implemented to\\navoid bottlenecks related to sen\", \"ding the emails. Additionally, this stand-alone service could be reused\\nas a utility across many dif\", \"ferent applications.\\n\\n47\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fAsynchronous messaging fro\", \"m queues and topics is a common pattern to trigger serverless functions.\\nHowever, Azure Functions ca\", \"n be triggered by other events, such as changes to Azure Blob Storage. A\\nservice that supports image\", \" uploads could have an Azure Function responsible for optimizing the\\nimage size. The function could \", \"be triggered directly by inserts into Azure Blob Storage, keeping\\ncomplexity out of the microservice\", \" operations.\\n\\nMany services have long-running processes as part of their workflows. Often these task\", \"s are done as\\npart of the user\\u2019s interaction with the application. These tasks can force the user to\", \" wait, negatively\\nimpacting their experience. Serverless computing provides a great way to move slow\", \"er tasks outside\\nof the user interaction loop. These tasks can scale with demand without requiring t\", \"he entire\\napplication to scale.\\n\\nWhen should you avoid serverless?\\n\\nServerless solutions provision a\", \"nd scale on demand. When a new instance is invoked, cold starts are a\\ncommon issue. A cold start is \", \"the period of time it takes to provision this instance. Normally, this delay\\nmight be a few seconds,\", \" but can be longer depending on various factors. Once provisioned, a single\\ninstance is kept alive a\", \"s long as it receives periodic requests. But, if a service is called less frequently,\\nAzure may remo\", \"ve it from memory and require a cold start when reinvoked. Cold starts are also\\nrequired when a func\", \"tion scales out to a new instance.\\n\\nFigure 3-9 shows a cold-start pattern. Note the extra steps requ\", \"ired when the app is cold.\\n\\nFigure 3-9. Cold start versus warm start.\\n\\nTo avoid cold starts entirely\", \", you might switch from a consumption plan to a dedicated plan. You can\\nalso configure one or more p\", \"re-warmed instances with the premium plan upgrade. In these cases,\\nwhen you need to add another inst\", \"ance, it\\u2019s already up and ready to go. These options can help\\nmitigate the cold start issue associat\", \"ed with serverless computing.\\n\\n48\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fCloud providers b\", \"ill for serverless based on compute execution time and consumed memory. Long\\nrunning operations or h\", \"igh memory consumption workloads aren\\u2019t always the best candidates for\\nserverless. Serverless functi\", \"ons favor small chunks of work that can complete quickly. Most serverless\\nplatforms require individu\", \"al functions to complete within a few minutes. Azure Functions defaults to a\\n5-minute time-out durat\", \"ion, which can be configured up to 10 minutes. The Azure Functions premium\\nplan can mitigate this is\", \"sue as well, defaulting time-outs to 30 minutes with an unbounded higher\\nlimit that can be configure\", \"d. Compute time isn\\u2019t calendar time. More advanced functions using the\\nAzure Durable Functions frame\", \"work may pause execution over a course of several days. The billing is\\nbased on actual execution tim\", \"e - when the function wakes up and resumes processing.\\n\\nFinally, leveraging Azure Functions for appl\", \"ication tasks adds complexity. It\\u2019s wise to first architect\\nyour application with a modular, loosely\", \" coupled design. Then, identify if there are benefits serverless\\nwould offer that justify the additi\", \"onal complexity.\\n\\nCombining containers and serverless approaches\\n\\nCloud-native applications typicall\", \"y implement services leveraging containers and orchestration. There\\nare often opportunities to expos\", \"e some of the application\\u2019s services as Azure Functions. However,\\nwith a cloud-native app deployed t\", \"o Kubernetes, it would be nice to leverage Azure Functions within\\nthis same toolset. Fortunately, yo\", \"u can wrap Azure Functions inside Docker containers and deploy\\nthem using the same processes and too\", \"ls as the rest of your Kubernetes-based app.\\n\\nWhen does it make sense to use containers with serverl\", \"ess?\\n\\nYour Azure Function has no knowledge of the platform on which it\\u2019s deployed. For some scenario\", \"s,\\nyou may have specific requirements and need to customize the environment on which your function\\nc\", \"ode will run. You\\u2019ll need a custom image that supports dependencies or a configuration not\\nsupported\", \" by the default image. In these cases, it makes sense to deploy your function in a custom\\nDocker con\", \"tainer.\\n\\nWhen should you avoid using containers with Azure Functions?\\n\\nIf you want to use consumptio\", \"n billing, you can\\u2019t run your function in a container. What\\u2019s more, if you\\ndeploy your function to a\", \" Kubernetes cluster, you\\u2019ll no longer benefit from the built-in scaling\\nprovided by Azure Functions.\", \" You\\u2019ll need to use Kubernetes\\u2019 scaling features, described earlier in this\\nchapter.\\n\\nHow to combine\", \" serverless and Docker containers\\n\\nTo wrap an Azure Function in a Docker container, install the Azur\", \"e Functions Core Tools and then run\\nthe following command:\\n\\nfunc init ProjectName --worker-runtime d\", \"otnet --docker\\n\\nWhen the project is created, it will include a Dockerfile and the worker runtime con\", \"figured to dotnet.\\nNow, you can create and test your function locally. Build and run it using the do\", \"cker build and docker\\n\\n49\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\frun commands. For detaile\", \"d steps to get started building Azure Functions with Docker support, see\\nthe Create a function on Li\", \"nux using a custom image tutorial.\\n\\nHow to combine serverless and Kubernetes with KEDA\\n\\nIn this chap\", \"ter, you\\u2019ve seen that the Azure Functions\\u2019 platform automatically scales out to meet\\ndemand. When de\", \"ploying containerized functions to AKS, however, you lose the built-in scaling\\nfunctionality. To the\", \" rescue comes Kubernetes-based Event Driven (KEDA). It enables fine-grained\\nautoscaling for event-dr\", \"iven Kubernetes workloads, including containerized functions.\\n\\nKEDA provides event-driven scaling fu\", \"nctionality to the Functions\\u2019 runtime in a Docker container.\\nKEDA can scale from zero instances (whe\", \"n no events are occurring) out to n instances, based on load.\\nIt enables autoscaling by exposing cus\", \"tom metrics to the Kubernetes autoscaler (Horizontal Pod\\nAutoscaler). Using Functions containers wit\", \"h KEDA makes it possible to replicate serverless function\\ncapabilities in any Kubernetes cluster.\\n\\nI\", \"t\\u2019s worth noting that the KEDA project is now managed by the Cloud Native Computing Foundation\\n(CNCF\", \").\\n\\nDeploying containers in Azure\\n\\nWe\\u2019ve discussed containers in this chapter and in chapter 1. We\\u2019v\", \"e seen that containers provide many\\nbenefits to cloud-native applications, including portability. In\", \" the Azure cloud, you can deploy the\\nsame containerized services across staging and production envir\", \"onments. Azure provides several\\noptions for hosting these containerized workloads:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure K\", \"ubernetes Services (AKS)\\n\\nAzure Container Instance (ACI)\\n\\nAzure Web Apps for Containers\\n\\nAzure Conta\", \"iner Registry\\n\\nWhen containerizing a microservice, you first build a container \\u201cimage.\\u201d The image is\", \" a binary\\nrepresentation of the service code, dependencies, and runtime. While you can manually crea\", \"te an\\nimage using the Docker Build command from the Docker API, a better approach is to create it as\", \" part\\nof an automated build process.\\n\\nOnce created, container images are stored in container registr\", \"ies. They enable you to build, store, and\\nmanage container images. There are many registries availab\", \"le, both public and private. Azure\\nContainer Registry (ACR) is a fully managed container registry se\", \"rvice in the Azure cloud. It persists\\nyour images inside the Azure network, reducing the time to dep\", \"loy them to Azure container hosts.\\nYou can also secure them using the same security and identity pro\", \"cedures that you use for other\\nAzure resources.\\n\\nYou create an Azure Container Registry using the Az\", \"ure portal, Azure CLI, or PowerShell tools.\\nCreating a registry in Azure is simple. It requires an A\", \"zure subscription, resource group, and a unique\\n\\n50\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\", \"\\fname. Figure 3-10 shows the basic options for creating a registry, which will be hosted at\\nregistry\", \"name.azurecr.io.\\n\\nFigure 3-10. Create container registry\\n\\nOnce you\\u2019ve created the registry, you\\u2019ll n\", \"eed to authenticate with it before you can use it. Typically,\\nyou\\u2019ll log into the registry using the\", \" Azure CLI command:\\n\\naz acr login --name *registryname*\\n\\nOnce authenticated, you can use docker comm\", \"ands to push container images to it. Before you can do\\nso, however, you must tag your image with the\", \" fully qualified name (URL) of your ACR login server. It\\nwill have the format registryname.azurecr.i\", \"o.\\n\\ndocker tag mycontainer myregistry.azurecr.io/mycontainer:v1\\n\\nAfter you\\u2019ve tagged the image, you \", \"use the docker push command to push the image to your ACR\\ninstance.\\n\\ndocker push myregistry.azurecr.\", \"io/mycontainer:v1\\n\\n51\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fAfter you push an image to th\", \"e registry, it\\u2019s a good idea to remove the image from your local Docker\\nenvironment, using this comm\", \"and:\\n\\ndocker rmi myregistry.azurecr.io/mycontainer:v1\\n\\nAs a best practice, you shouldn\\u2019t manually pu\", \"sh images to a container registry. Instead, use a build\\npipeline defined in a tool like GitHub or Az\", \"ure DevOps. Learn more in the Cloud-Native DevOps\\nchapter.\\n\\nACR Tasks\\n\\nACR Tasks is a set of feature\", \"s available from the Azure Container Registry. It extends your inner-loop\\ndevelopment cycle by build\", \"ing and managing container images in the Azure cloud. Instead of\\ninvoking a docker build and docker \", \"push locally on your development machine, they\\u2019re automatically\\nhandled by ACR Tasks in the cloud.\\n\\n\", \"The following AZ CLI command both builds a container image and pushes it to ACR:\\n\\n# create a contain\", \"er registry\\naz acr create --resource-group myResourceGroup --name myContainerRegistry008 --sku\\nBasic\", \"\\n\\n# build container image in ACR and push it into your container registry\\naz acr build --image sampl\", \"e/hello-world:v1  --registry myContainerRegistry008 --file\\nDockerfile .\\n\\nAs you can see from the pre\", \"vious command block, there\\u2019s no need to install Docker Desktop on your\\ndevelopment machine. Addition\", \"ally, you can configure ACR Task triggers to rebuild containers images\\non both source code and base \", \"image updates.\\n\\nAzure Kubernetes Service\\n\\nWe discussed Azure Kubernetes Service (AKS) at length in t\", \"his chapter. We\\u2019ve seen that it\\u2019s the de\\nfacto container orchestrator managing containerized cloud-n\", \"ative applications.\\n\\nOnce you deploy an image to a registry, such as ACR, you can configure AKS to a\", \"utomatically pull and\\ndeploy it. With a CI/CD pipeline in place, you might configure a canary releas\", \"e strategy to minimize\\nthe risk involved when rapidly deploying updates. The new version of the app \", \"is initially configured in\\nproduction with no traffic routed to it. Then, the system will route a sm\", \"all percentage of users to the\\nnewly deployed version. As the team gains confidence in the new versi\", \"on, it can roll out more\\ninstances and retire the old. AKS easily supports this style of deployment.\", \"\\n\\nAs with most resources in Azure, you can create an Azure Kubernetes Service cluster using the port\", \"al,\\ncommand-line, or automation tools like Helm or Terraform. To get started with a new cluster, you\", \"\\nneed to provide the following information:\\n\\nAzure subscription\\n\\nResource group\\n\\nKubernetes cluster \", \"name\\n\\nRegion\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n52\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nKubernetes\", \" version\\n\\nDNS name prefix\\n\\nNode size\\n\\nNode count\\n\\nThis information is sufficient to get started. As \", \"part of the creation process in the Azure portal, you can\\nalso configure options for the following f\", \"eatures of your cluster:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nScale\\n\\nAuthentication\\n\\nNetworking\\n\\n\\u2022  Monitoring\\n\\n\\u2022\\n\\nTags\\n\\nThis q\", \"uickstart walks through deploying an AKS cluster using the Azure portal.\\n\\nAzure Bridge to Kubernetes\", \"\\n\\nCloud-native applications can grow large and complex, requiring significant compute resources to\\nr\", \"un. In these scenarios, the entire application can\\u2019t be hosted on a development machine (especially \", \"a\\nlaptop). Azure Bridge to Kubernetes addresses the shortcoming. It enables developers to work with \", \"a\\nlocal version of their service while hosting the entire application in an AKS development cluster.\", \"\\n\\nWhen ready, developers test their changes locally while running against the full application in th\", \"e AKS\\ncluster - without replicating dependencies. Under the hood, the bridge merges code from the lo\", \"cal\\nmachine with services in AKS. Developers can rapidly iterate and debug code directly in Kubernet\", \"es\\nusing Visual Studio or Visual Studio Code.\\n\\nGabe Monroy, former VP of Product Management at Micro\", \"soft, describes it well:\\n\\nImagine you\\u2019re a new employee trying to fix a bug in a complex microservic\", \"es application consisting\\nof dozens of components, each with their own configuration and backing ser\", \"vices. To get started, you\\nmust configure your local development environment so that it can mimic pr\", \"oduction including setting\\nup your IDE, building tool chain, containerized service dependencies, a l\", \"ocal Kubernetes environment,\\nmocks for backing services, and more. With all the time involved settin\", \"g up your development\\nenvironment, fixing that first bug could take days! Or you could just use Brid\", \"ge to Kubernetes and\\nAKS.\\n\\nScaling containers and serverless applications\\n\\nThere are two ways to sca\", \"le an application: up or out. The former refers to adding capacity to a single\\nresource, while the l\", \"atter refers to adding more resources to increase capacity.\\n\\nThe simple solution: scaling up\\n\\nUpgrad\", \"ing an existing host server with increased CPU, memory, disk I/O speed, and network I/O\\nspeed is kno\", \"wn as scaling up. Scaling up a cloud-native application involves choosing more capable\\n\\n53\\n\\nCHAPTER \", \"3 | Scaling cloud-native applications\\n\\n\\fresources from the cloud vendor. For example, you can create\", \" a new node pool with larger VMs in\\nyour Kubernetes cluster. Then, migrate your containerized servic\", \"es to the new pool.\\n\\nServerless apps scale up by choosing the premium Functions plan or premium inst\", \"ance sizes from a\\ndedicated app service plan.\\n\\nScaling out cloud-native apps\\n\\nCloud-native applicati\", \"ons often experience large fluctuations in demand and require scale on a\\nmoment\\u2019s notice. They favor\", \" scaling out. Scaling out is done horizontally by adding additional\\nmachines (called nodes) or appli\", \"cation instances to an existing cluster. In Kubernetes, you can scale\\nmanually by adjusting configur\", \"ation settings for the app (for example, scaling a node pool), or\\nthrough autoscaling.\\n\\nAKS clusters\", \" can autoscale in one of two ways:\\n\\nFirst, the Horizontal Pod Autoscaler monitors resource demand an\", \"d automatically scales your POD\\nreplicas to meet it. When traffic increases, additional replicas are\", \" automatically provisioned to scale\\nout your services. Likewise, when demand decreases, they\\u2019re remo\", \"ved to scale-in your services. You\\ndefine the metric on which to scale, for example, CPU usage. You \", \"can also specify the minimum and\\nmaximum number of replicas to run. AKS monitors that metric and sca\", \"les accordingly.\\n\\nNext, the AKS Cluster Autoscaler feature enables you to automatically scale comput\", \"e nodes across a\\nKubernetes cluster to meet demand. With it, you can automatically add new VMs to th\", \"e underlying\\nAzure Virtual Machine Scale Set whenever more compute capacity of is required. It also \", \"removes\\nnodes when no longer required.\\n\\nFigure 3-11 shows the relationship between these two scaling\", \" services.\\n\\nFigure 3-11. Scaling out an App Service plan.\\n\\n54\\n\\nCHAPTER 3 | Scaling cloud-native appl\", \"ications\\n\\n\\fWorking together, both ensure an optimal number of container instances and compute nodes \", \"to\\nsupport fluctuating demand. The horizontal pod autoscaler optimizes the number of pods required.\\n\", \"The cluster autoscaler optimizes the number of nodes required.\\n\\nScaling Azure Functions\\n\\nAzure Funct\", \"ions automatically scale out upon demand. Server resources are dynamically allocated and\\nremoved bas\", \"ed on the number of triggered events. You\\u2019re only charged for compute resources\\nconsumed when your f\", \"unctions run. Billing is based upon the number of executions, execution time,\\nand memory used.\\n\\nWhil\", \"e the default consumption plan provides an economical and scalable solution for most apps, the\\npremi\", \"um option allows developers flexibility for custom Azure Functions requirements. Upgrading to\\nthe pr\", \"emium plan provides control over instance sizes, pre-warmed instances (to avoid cold start\\ndelays), \", \"and dedicated VMs.\\n\\nOther container deployment options\\n\\nAside from Azure Kubernetes Service (AKS), y\", \"ou can also deploy containers to Azure App Service for\\nContainers and Azure Container Instances.\\n\\nWh\", \"en does it make sense to deploy to App Service for Containers?\\n\\nSimple production applications that \", \"don\\u2019t require orchestration are well suited to Azure App Service\\nfor Containers.\\n\\nHow to deploy to A\", \"pp Service for Containers\\n\\nTo deploy to Azure App Service for Containers, you\\u2019ll need an Azure Conta\", \"iner Registry (ACR) instance\\nand credentials to access it. Push your container image to the ACR repo\", \"sitory so that your Azure App\\nService can pull it when needed. Once complete, you can configure the \", \"app for Continuous\\nDeployment. Doing so will automatically deploy updates whenever the image changes\", \" in ACR.\\n\\nWhen does it make sense to deploy to Azure Container Instances?\\n\\nAzure Container Instances\", \" (ACI) enables you to run Docker containers in a managed, serverless cloud\\nenvironment, without havi\", \"ng to set up virtual machines or clusters. It\\u2019s a great solution for short-\\nrunning workloads that c\", \"an run in an isolated container. Consider ACI for simple services, testing\\nscenarios, task automatio\", \"n, and build jobs. ACI spins-up a container instance, performs the task, and\\nthen spins it down.\\n\\nHo\", \"w to deploy an app to Azure Container Instances\\n\\nTo deploy to Azure Container Instances (ACI), you n\", \"eed an Azure Container Registry (ACR) and\\ncredentials for accessing it. Once you push your container\", \" image to the repository, it\\u2019s available to pull\\ninto ACI. You can work with ACI using the Azure por\", \"tal or command-line interface. ACR provides tight\\nintegration with ACI. Figure 3-12 shows how to pus\", \"h an individual container image to ACR.\\n\\n55\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\fFigure \", \"3-12. Azure Container Registry Run Instance\\n\\nCreating an instance in ACI can be done quickly. Specif\", \"y the image registry, Azure resource group\\ninformation, the amount of memory to allocate, and the po\", \"rt on which to listen. This quickstart shows\\nhow to deploy a container instance to ACI using the Azu\", \"re portal.\\n\\nOnce the deployment completes, find the newly deployed container\\u2019s IP address and commun\", \"icate\\nwith it over the port you specified.\\n\\nAzure Container Instances offers the fastest way to run \", \"simple container workloads in Azure. You don\\u2019t\\nneed to configure an app service, orchestrator, or vi\", \"rtual machine. For scenarios where you require full\\ncontainer orchestration, service discovery, auto\", \"matic scaling, or coordinated upgrades, we\\nrecommend Azure Kubernetes Service (AKS).\\n\\nReferences\\n\\n\\u2022 \", \" What is Kubernetes?\\n\\n\\u2022\\n\\nInstalling Kubernetes with Minikube\\n\\n\\u2022  MiniKube vs Docker Desktop\\n\\n\\u2022\\n\\nVisu\", \"al Studio Tools for Docker\\n\\n56\\n\\nCHAPTER 3 | Scaling cloud-native applications\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\", \"\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nUnderstanding serverless cold start\\n\\nPre-warmed Azure Functions instances\\n\\n\", \"Create a function on Linux using a custom image\\n\\nRun Azure Functions in a Docker Container\\n\\nCreate a\", \" function on Linux using a custom image\\n\\nAzure Functions with Kubernetes Event Driven Autoscaling\\n\\nC\", \"anary Release\\n\\nAzure Dev Spaces with VS Code\\n\\nAzure Dev Spaces with Visual Studio\\n\\nAKS Multiple Node\", \" Pools\\n\\nAKS Cluster Autoscaler\\n\\nTutorial: Scale applications in AKS\\n\\nAzure Functions scale and hosti\", \"ng\\n\\nAzure Container Instances Docs\\n\\nDeploy Container Instance from ACR\\n\\n57\\n\\nCHAPTER 3 | Scaling clou\", \"d-native applications\\n\\n\\fCHAPTER  4\\n\\nCloud-native\\ncommunication patterns\\n\\nWhen constructing a cloud-n\", \"ative system, communication becomes a significant design decision. How\\ndoes a front-end client appli\", \"cation communicate with a back-end microservice? How do back-end\\nmicroservices communicate with each\", \" other? What are the principles, patterns, and best practices to\\nconsider when implementing communic\", \"ation in cloud-native applications?\\n\\nCommunication considerations\\n\\nIn a monolithic application, comm\", \"unication is straightforward. The code modules execute together in\\nthe same executable space (proces\", \"s) on a server. This approach can have performance advantages as\\neverything runs together in shared \", \"memory, but results in tightly coupled code that becomes difficult\\nto maintain, evolve, and scale.\\n\\n\", \"Cloud-native systems implement a microservice-based architecture with many small, independent\\nmicros\", \"ervices. Each microservice executes in a separate process and typically runs inside a container\\nthat\", \" is deployed to a cluster.\\n\\nA cluster groups a pool of virtual machines together to form a highly av\", \"ailable environment. They\\u2019re\\nmanaged with an orchestration tool, which is responsible for deploying \", \"and managing the\\ncontainerized microservices. Figure 4-1 shows a Kubernetes cluster deployed into th\", \"e Azure cloud\\nwith the fully managed Azure Kubernetes Services.\\n\\n58\\n\\nCHAPTER 4 | Cloud-native commun\", \"ication patterns\\n\\n\\fFigure 4-1. A Kubernetes cluster in Azure\\n\\nAcross the cluster, microservices comm\", \"unicate with each other through APIs and messaging\\ntechnologies.\\n\\nWhile they provide many benefits, \", \"microservices are no free lunch. Local in-process method calls\\nbetween components are now replaced w\", \"ith network calls. Each microservice must communicate over\\na network protocol, which adds complexity\", \" to your system:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nNetwork congestion, latency, and transient faults are a constant co\", \"ncern.\\n\\nResiliency (that is, retrying failed requests) is essential.\\n\\nSome calls must be idempotent \", \"as to keep consistent state.\\n\\nEach microservice must authenticate and authorize calls.\\n\\nEach message\", \" must be serialized and then deserialized - which can be expensive.\\n\\n\\u2022  Message encryption/decryptio\", \"n becomes important.\\n\\nThe book .NET Microservices: Architecture for Containerized .NET Applications,\", \" available for free from\\nMicrosoft, provides an in-depth coverage of communication patterns for micr\", \"oservice applications. In\\nthis chapter, we provide a high-level overview of these patterns along wit\", \"h implementation options\\navailable in the Azure cloud.\\n\\nIn this chapter, we\\u2019ll first address communi\", \"cation between front-end applications and back-end\\nmicroservices. We\\u2019ll then look at back-end micros\", \"ervices communicate with each other. We\\u2019ll explore\\n\\n59\\n\\nCHAPTER 4 | Cloud-native communication patte\", \"rns\\n\\n\\fthe up and gRPC communication technology. Finally, we\\u2019ll look new innovative communication\\npat\", \"terns using service mesh technology. We\\u2019ll also see how the Azure cloud provides different kinds\\nof \", \"backing services to support cloud-native communication.\\n\\nFront-end client communication\\n\\nIn a cloud-\", \"native system, front-end clients (mobile, web, and desktop applications) require a\\ncommunication cha\", \"nnel to interact with independent back-end microservices.\\n\\nWhat are the options?\\n\\nTo keep things sim\", \"ple, a front-end client could directly communicate with the back-end microservices,\\nshown in Figure \", \"4-2.\\n\\nFigure 4-2. Direct client to service communication\\n\\nWith this approach, each microservice has \", \"a public endpoint that is accessible by front-end clients. In\\na production environment, you\\u2019d place \", \"a load balancer in front of the microservices, routing traffic\\nproportionately.\\n\\nWhile simple to imp\", \"lement, direct client communication would be acceptable only for simple\\nmicroservice applications. T\", \"his pattern tightly couples front-end clients to core back-end services,\\nopening the door for many p\", \"roblems, including:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n60\\n\\nClient susceptibility to back-end service refactoring.\\n\\nA wider\", \" attack surface as core back-end services are directly exposed.\\n\\nDuplication of cross-cutting concer\", \"ns across each microservice.\\n\\nOverly complex client code - clients must keep track of multiple endpo\", \"ints and handle failures\\nin a resilient way.\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fInst\", \"ead, a widely accepted cloud design pattern is to implement an API Gateway Service between the\\nfront\", \"-end applications and back-end services. The pattern is shown in Figure 4-3.\\n\\nFigure 4-3. API gatewa\", \"y pattern\\n\\nIn the previous figure, note how the API Gateway service abstracts the back-end core micr\", \"oservices.\\nImplemented as a web API, it acts as a reverse proxy, routing incoming traffic to the int\", \"ernal\\nmicroservices.\\n\\nThe gateway insulates the client from internal service partitioning and refact\", \"oring. If you change a\\nback-end service, you accommodate for it in the gateway without breaking the \", \"client. It\\u2019s also your\\nfirst line of defense for cross-cutting concerns, such as identity, caching, \", \"resiliency, metering, and\\nthrottling. Many of these cross-cutting concerns can be off-loaded from th\", \"e back-end core services to\\nthe gateway, simplifying the back-end services.\\n\\nCare must be taken to k\", \"eep the API Gateway simple and fast. Typically, business logic is kept out of\\nthe gateway. A complex\", \" gateway risks becoming a bottleneck and eventually a monolith itself. Larger\\nsystems often expose m\", \"ultiple API Gateways segmented by client type (mobile, web, desktop) or\\nback-end functionality. The \", \"Backend for Frontends pattern provides direction for implementing\\nmultiple gateways. The pattern is \", \"shown in Figure 4-4.\\n\\n61\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-4. Backend for \", \"frontend pattern\\n\\nNote in the previous figure how incoming traffic is sent to a specific API gateway\", \" - based upon client\\ntype: web, mobile, or desktop app. This approach makes sense as the capabilitie\", \"s of each device differ\\nsignificantly across form factor, performance, and display limitations. Typi\", \"cally mobile applications\\nexpose less functionality than a browser or desktop applications. Each gat\", \"eway can be optimized to\\nmatch the capabilities and functionality of the corresponding device.\\n\\nSimp\", \"le Gateways\\n\\nTo start, you could build your own API Gateway service. A quick search of GitHub will p\", \"rovide many\\nexamples.\\n\\nFor simple .NET cloud-native applications, you might consider the Ocelot Gate\", \"way. Open source and\\ncreated for .NET microservices, it\\u2019s lightweight, fast, scalable. Like any API \", \"Gateway, its primary\\nfunctionality is to forward incoming HTTP requests to downstream services. Addi\", \"tionally, it supports a\\nwide variety of capabilities that are configurable in a .NET middleware pipe\", \"line.\\n\\nYARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Micro\", \"soft\\nproduct teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as\\nmiddle\", \"ware and is highly customizable. You\\u2019ll find YARP well-documented with various usage\\nexamples.\\n\\nFor \", \"enterprise cloud-native applications, there are several managed Azure services that can help\\njump-st\", \"art your efforts.\\n\\n62\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fAzure Application Gateway\\n\\n\", \"For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure\\nP\", \"aaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web\\nAppl\", \"ication Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can\\nro\", \"ute requests based on the actual content of an HTTP message, not just low-level TCP network\\npackets.\", \"\\n\\nThroughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container\\norches\", \"trator, Kubernetes automates the deployment, scaling, and operational concerns of\\ncontainerized work\", \"loads. Azure Application Gateway can be configured as an API gateway for Azure\\nKubernetes Service cl\", \"uster.\\n\\nThe Application Gateway Ingress Controller enables Azure Application Gateway to work directl\", \"y with\\nAzure Kubernetes Service. Figure 4.5 shows the architecture.\\n\\nFigure 4-5. Application Gateway\", \" Ingress Controller\\n\\nKubernetes includes a built-in feature that supports HTTP (Level 7) load balanc\", \"ing, called Ingress.\\nIngress defines a set of rules for how microservice instances inside AKS can be\", \" exposed to the outside\\nworld. In the previous image, the ingress controller interprets the ingress \", \"rules configured for the\\ncluster and automatically configures the Azure Application Gateway. Based o\", \"n those rules, the\\nApplication Gateway routes traffic to microservices running inside AKS. The ingre\", \"ss controller listens\\nfor changes to ingress rules and makes the appropriate changes to the Azure Ap\", \"plication Gateway.\\n\\nAzure API Management\\n\\nFor moderate to large-scale cloud-native systems, you may \", \"consider Azure API Management. It\\u2019s a\\ncloud-based service that not only solves your API Gateway need\", \"s, but provides a full-featured\\ndeveloper and administrative experience. API Management is shown in \", \"Figure 4-6.\\n\\n63\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-6. Azure API Management\\n\", \"\\nTo start, API Management exposes a gateway server that allows controlled access to back-end\\nservice\", \"s based upon configurable rules and policies. These services can be in the Azure cloud, your\\non-prem\", \" data center, or other public clouds. API keys and JWT tokens determine who can do what. All\\ntraffic\", \" is logged for analytical purposes.\\n\\nFor developers, API Management offers a developer portal that p\", \"rovides access to services,\\ndocumentation, and sample code for invoking them. Developers can use Swa\", \"gger/Open API to\\ninspect service endpoints and analyze their usage. The service works across the maj\", \"or development\\nplatforms: .NET, Java, Golang, and more.\\n\\nThe publisher portal exposes a management d\", \"ashboard where administrators expose APIs and\\nmanage their behavior. Service access can be granted, \", \"service health monitored, and service telemetry\\ngathered. Administrators apply policies to each endp\", \"oint to affect behavior. Policies are pre-built\\nstatements that execute sequentially for each servic\", \"e call. Policies are configured for an inbound call,\\noutbound call, or invoked upon an error. Polici\", \"es can be applied at different service scopes as to\\nenable deterministic ordering when combining pol\", \"icies. The product ships with a large number of\\nprebuilt policies.\\n\\nHere are examples of how policie\", \"s can affect the behavior of your cloud-native services:\\n\\nRestrict service access.\\n\\nEnforce authenti\", \"cation.\\n\\nThrottle calls from a single source, if necessary.\\n\\nEnable caching.\\n\\nBlock calls from speci\", \"fic IP addresses.\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n64\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\nContro\", \"l the flow of the service.\\n\\nConvert requests from SOAP to REST or between different data formats, su\", \"ch as from XML to\\nJSON.\\n\\nAzure API Management can expose back-end services that are hosted anywhere \", \"\\u2013 in the cloud or your\\ndata center. For legacy services that you may expose in your cloud-native sys\", \"tems, it supports both\\nREST and SOAP APIs. Even other Azure services can be exposed through API Mana\", \"gement. You could\\nplace a managed API on top of an Azure backing service like Azure Service Bus or A\", \"zure Logic Apps.\\nAzure API Management doesn\\u2019t include built-in load-balancing support and should be \", \"used in\\nconjunction with a load-balancing service.\\n\\nAzure API Management is available across four di\", \"fferent tiers:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nDeveloper\\n\\nBasic\\n\\nStandard\\n\\nPremium\\n\\nThe Developer tier is meant for non\", \"-production workloads and evaluation. The other tiers offer\\nprogressively more power, features, and \", \"higher service level agreements (SLAs). The Premium tier\\nprovides Azure Virtual Network and multi-re\", \"gion support. All tiers have a fixed price per hour.\\n\\nThe Azure cloud also offers a serverless tier \", \"for Azure API Management. Referred to as the\\nconsumption pricing tier, the service is a variant of A\", \"PI Management designed around the serverless\\ncomputing model. Unlike the \\u201cpre-allocated\\u201d pricing tie\", \"rs previously shown, the consumption tier\\nprovides instant provisioning and pay-per-action pricing.\\n\", \"\\nIt enables API Gateway features for the following use cases:\\n\\n\\u2022  Microservices implemented using se\", \"rverless technologies such as Azure Functions and Azure\\n\\nLogic Apps.\\n\\n\\u2022\\n\\nAzure backing service resou\", \"rces such as Service Bus queues and topics, Azure storage, and\\nothers.\\n\\n\\u2022  Microservices where traff\", \"ic has occasional large spikes but remains low most the time.\\n\\nThe consumption tier uses the same un\", \"derlying service API Management components, but employs\\nan entirely different architecture based on \", \"dynamically allocated resources. It aligns perfectly with the\\nserverless computing model:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\", \"\\u2022\\n\\n\\u2022\\n\\nNo infrastructure to manage.\\n\\nNo idle capacity.\\n\\nHigh-availability.\\n\\nAutomatic scaling.\\n\\nCost \", \"is based on actual usage.\\n\\nThe new consumption tier is a great choice for cloud-native systems that \", \"expose serverless resources\\nas APIs.\\n\\n65\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fReal-tim\", \"e communication\\n\\nReal-time, or push, communication is another option for front-end applications that\", \" communicate\\nwith back-end cloud-native systems over HTTP. Applications, such as financial-tickers, \", \"online\\neducation, gaming, and job-progress updates, require instantaneous, real-time responses from \", \"the\\nback-end. With normal HTTP communication, there\\u2019s no way for the client to know when new data is\", \"\\navailable. The client must continually poll or send requests to the server. With real-time\\ncommunic\", \"ation, the server can push new data to the client at any time.\\n\\nReal-time systems are often characte\", \"rized by high-frequency data flows and large numbers of\\nconcurrent client connections. Manually impl\", \"ementing real-time connectivity can quickly become\\ncomplex, requiring non-trivial infrastructure to \", \"ensure scalability and reliable messaging to connected\\nclients. You could find yourself managing an \", \"instance of Azure Redis Cache and a set of load\\nbalancers configured with sticky sessions for client\", \" affinity.\\n\\nAzure SignalR Service is a fully managed Azure service that simplifies real-time communi\", \"cation for\\nyour cloud-native applications. Technical implementation details like capacity provisioni\", \"ng, scaling,\\nand persistent connections are abstracted away. They\\u2019re handled for you with a 99.9% se\", \"rvice-level\\nagreement. You focus on application features, not infrastructure plumbing.\\n\\nOnce enabled\", \", a cloud-based HTTP service can push content updates directly to connected clients,\\nincluding brows\", \"er, mobile and desktop applications. Clients are updated without the need to poll the\\nserver. Azure \", \"SignalR abstracts the transport technologies that create real-time connectivity, including\\nWebSocket\", \"s, Server-Side Events, and Long Polling. Developers focus on sending messages to all or\\nspecific sub\", \"sets of connected clients.\\n\\nFigure 4-7 shows a set of HTTP Clients connecting to a Cloud-native appl\", \"ication with Azure SignalR\\nenabled.\\n\\n66\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-\", \"7. Azure SignalR\\n\\nAnother advantage of Azure SignalR Service comes with implementing Serverless clou\", \"d-native\\nservices. Perhaps your code is executed on demand with Azure Functions triggers. This scena\", \"rio can\\nbe tricky because your code doesn\\u2019t maintain long connections with clients. Azure SignalR Se\", \"rvice can\\nhandle this situation since the service already manages connections for you.\\n\\nAzure Signal\", \"R Service closely integrates with other Azure services, such as Azure SQL Database,\\nService Bus, or \", \"Redis Cache, opening up many possibilities for your cloud-native applications.\\n\\nService-to-service c\", \"ommunication\\n\\nMoving from the front-end client, we now address back-end microservices communicate wi\", \"th each\\nother.\\n\\nWhen constructing a cloud-native application, you\\u2019ll want to be sensitive to how bac\", \"k-end services\\ncommunicate with each other. Ideally, the less inter-service communication, the bette\", \"r. However,\\navoidance isn\\u2019t always possible as back-end services often rely on one another to comple\", \"te an\\noperation.\\n\\nThere are several widely accepted approaches to implementing cross-service communi\", \"cation. The type\\nof communication interaction will often determine the best approach.\\n\\nConsider the \", \"following interaction types:\\n\\n\\u2022\\n\\nQuery \\u2013 when a calling microservice requires a response from a call\", \"ed microservice, such as,\\n\\u201cHey, give me the buyer information for a given customer Id.\\u201d\\n\\n67\\n\\nCHAPTER\", \" 4 | Cloud-native communication patterns\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\nCommand \\u2013 when the calling microservice needs anoth\", \"er microservice to execute an action\\nbut doesn\\u2019t require a response, such as, \\u201cHey, just ship this o\", \"rder.\\u201d\\n\\nEvent \\u2013 when a microservice, called the publisher, raises an event that state has changed or\", \" an\\naction has occurred. Other microservices, called subscribers, who are interested, can react to\\nt\", \"he event appropriately. The publisher and the subscribers aren\\u2019t aware of each other.\\n\\nMicroservice \", \"systems typically use a combination of these interaction types when executing\\noperations that requir\", \"e cross-service interaction. Let\\u2019s take a close look at each and how you might\\nimplement them.\\n\\nQuer\", \"ies\\n\\nMany times, one microservice might need to query another, requiring an immediate response to\\nco\", \"mplete an operation. A shopping basket microservice may need product information and a price to\\nadd \", \"an item to its basket. There are many approaches for implementing query operations.\\n\\nRequest/Respons\", \"e Messaging\\n\\nOne option for implementing this scenario is for the calling back-end microservice to m\", \"ake direct\\nHTTP requests to the microservices it needs to query, shown in Figure 4-8.\\n\\nFigure 4-8. D\", \"irect HTTP communication\\n\\nWhile direct HTTP calls between microservices are relatively simple to imp\", \"lement, care should be\\ntaken to minimize this practice. To start, these calls are always synchronous\", \" and will block the\\noperation until a result is returned or the request times outs. What were once s\", \"elf-contained,\\nindependent services, able to evolve independently and deploy frequently, now become \", \"coupled to\\neach other. As coupling among microservices increase, their architectural benefits dimini\", \"sh.\\n\\n68\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fExecuting an infrequent request that make\", \"s a single direct HTTP call to another microservice might be\\nacceptable for some systems. However, h\", \"igh-volume calls that invoke direct HTTP calls to multiple\\nmicroservices aren\\u2019t advisable. They can \", \"increase latency and negatively impact the performance,\\nscalability, and availability of your system\", \". Even worse, a long series of direct HTTP communication can\\nlead to deep and complex chains of sync\", \"hronous microservices calls, shown in Figure 4-9:\\n\\nFigure 4-9. Chaining HTTP queries\\n\\nYou can certai\", \"nly imagine the risk in the design shown in the previous image. What happens if Step\\n#3 fails? Or St\", \"ep #8 fails? How do you recover? What if Step #6 is slow because the underlying service\\nis busy? How\", \" do you continue? Even if all works correctly, think of the latency this call would incur,\\nwhich is \", \"the sum of the latency of each step.\\n\\nThe large degree of coupling in the previous image suggests th\", \"e services weren\\u2019t optimally modeled.\\nIt would behoove the team to revisit their design.\\n\\nMaterializ\", \"ed View pattern\\n\\nA popular option for removing microservice coupling is the Materialized View patter\", \"n. With this\\npattern, a microservice stores its own local, denormalized copy of data that\\u2019s owned by\", \" other services.\\nInstead of the Shopping Basket microservice querying the Product Catalog and Pricin\", \"g microservices,\\nit maintains its own local copy of that data. This pattern eliminates unnecessary c\", \"oupling and\\nimproves reliability and response time. The entire operation executes inside a single pr\", \"ocess. We\\nexplore this pattern and other data concerns in Chapter 5.\\n\\nService Aggregator Pattern\\n\\nAn\", \"other option for eliminating microservice-to-microservice coupling is an Aggregator microservice,\\nsh\", \"own in purple in Figure 4-10.\\n\\n69\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-10. Ag\", \"gregator microservice\\n\\nThe pattern isolates an operation that makes calls to multiple back-end micro\", \"services, centralizing its\\nlogic into a specialized microservice. The purple checkout aggregator mic\", \"roservice in the previous\\nfigure orchestrates the workflow for the Checkout operation. It includes c\", \"alls to several back-end\\nmicroservices in a sequenced order. Data from the workflow is aggregated an\", \"d returned to the caller.\\nWhile it still implements direct HTTP calls, the aggregator microservice r\", \"educes direct dependencies\\namong back-end microservices.\\n\\nRequest/Reply Pattern\\n\\nAnother approach fo\", \"r decoupling synchronous HTTP messages is a Request-Reply Pattern, which uses\\nqueuing communication.\", \" Communication using a queue is always a one-way channel, with a producer\\nsending the message and co\", \"nsumer receiving it. With this pattern, both a request queue and response\\nqueue are implemented, sho\", \"wn in Figure 4-11.\\n\\n70\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-11. Request-reply\", \" pattern\\n\\nHere, the message producer creates a query-based message that contains a unique correlatio\", \"n ID and\\nplaces it into a request queue. The consuming service dequeues the messages, processes it a\", \"nd places\\nthe response into the response queue with the same correlation ID. The producer service de\", \"queues\\nthe message, matches it with the correlation ID and continues processing. We cover queues in \", \"detail\\nin the next section.\\n\\nCommands\\n\\nAnother type of communication interaction is a command. A mic\", \"roservice may need another\\nmicroservice to perform an action. The Ordering microservice may need the\", \" Shipping microservice to\\ncreate a shipment for an approved order. In Figure 4-12, one microservice,\", \" called a Producer, sends a\\nmessage to another microservice, the Consumer, commanding it to do somet\", \"hing.\\n\\n71\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-12. Command interaction with a\", \" queue\\n\\nMost often, the Producer doesn\\u2019t require a response and can fire-and-forget the message. If \", \"a reply is\\nneeded, the Consumer sends a separate message back to Producer on another channel. A comm\", \"and\\nmessage is best sent asynchronously with a message queue. supported by a lightweight message\\nbro\", \"ker. In the previous diagram, note how a queue separates and decouples both services.\\n\\nA message que\", \"ue is an intermediary construct through which a producer and consumer pass a\\nmessage. Queues impleme\", \"nt an asynchronous, point-to-point messaging pattern. The Producer\\nknows where a command needs to be\", \" sent and routes appropriately. The queue guarantees that a\\nmessage is processed by exactly one of t\", \"he consumer instances that are reading from the channel. In\\nthis scenario, either the producer or co\", \"nsumer service can scale out without affecting the other. As\\nwell, technologies can be disparate on \", \"each side, meaning that we might have a Java microservice\\ncalling a Golang microservice.\\n\\nIn chapter\", \" 1, we talked about backing services. Backing services are ancillary resources upon which\\ncloud-nati\", \"ve systems depend. Message queues are backing services. The Azure cloud supports two\\ntypes of messag\", \"e queues that your cloud-native systems can consume to implement command\\nmessaging: Azure Storage Qu\", \"eues and Azure Service Bus Queues.\\n\\nAzure Storage Queues\\n\\nAzure storage queues offer a simple queuei\", \"ng infrastructure that is fast, affordable, and backed by\\nAzure storage accounts.\\n\\nAzure Storage Que\", \"ues feature a REST-based queuing mechanism with reliable and persistent\\nmessaging. They provide a mi\", \"nimal feature set, but are inexpensive and store millions of messages.\\nTheir capacity ranges up to 5\", \"00 TB. A single message can be up to 64 KB in size.\\n\\nYou can access messages from anywhere in the wo\", \"rld via authenticated calls using HTTP or HTTPS.\\nStorage queues can scale out to large numbers of co\", \"ncurrent clients to handle traffic spikes.\\n\\n72\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fTh\", \"at said, there are limitations with the service:\\n\\n\\u2022  Message order isn\\u2019t guaranteed.\\n\\n\\u2022\\n\\n\\u2022\\n\\nA messag\", \"e can only persist for seven days before it\\u2019s automatically removed.\\n\\nSupport for state management, \", \"duplicate detection, or transactions isn\\u2019t available.\\n\\nFigure 4-13 shows the hierarchy of an Azure S\", \"torage Queue.\\n\\nFigure 4-13. Storage queue hierarchy\\n\\nIn the previous figure, note how storage queues\", \" store their messages in the underlying Azure Storage\\naccount.\\n\\nFor developers, Microsoft provides s\", \"everal client and server-side libraries for Storage queue\\nprocessing. Most major platforms are suppo\", \"rted including .NET, Java, JavaScript, Ruby, Python, and\\nGo. Developers should never communicate dir\", \"ectly with these libraries. Doing so will tightly couple\\nyour microservice code to the Azure Storage\", \" Queue service. It\\u2019s a better practice to insulate the\\nimplementation details of the API. Introduce \", \"an intermediation layer, or intermediate API, that exposes\\ngeneric operations and encapsulates the c\", \"oncrete library. This loose coupling enables you to swap out\\none queuing service for another without\", \" having to make changes to the mainline service code.\\n\\nAzure Storage queues are an economical option\", \" to implement command messaging in your cloud-\\nnative applications. Especially when a queue size wil\", \"l exceed 80 GB, or a simple feature set is\\nacceptable. You only pay for the storage of the messages;\", \" there are no fixed hourly charges.\\n\\nAzure Service Bus Queues\\n\\nFor more complex messaging requiremen\", \"ts, consider Azure Service Bus queues.\\n\\nSitting atop a robust message infrastructure, Azure Service \", \"Bus supports a brokered messaging model.\\nMessages are reliably stored in a broker (the queue) until \", \"received by the consumer. The queue\\nguarantees First-In/First-Out (FIFO) message delivery, respectin\", \"g the order in which messages were\\nadded to the queue.\\n\\nThe size of a message can be much larger, up\", \" to 256 KB. Messages are persisted in the queue for an\\nunlimited period of time. Service Bus support\", \"s not only HTTP-based calls, but also provides full\\n\\n73\\n\\nCHAPTER 4 | Cloud-native communication patt\", \"erns\\n\\n\\fsupport for the AMQP protocol. AMQP is an open-standard across vendors that supports a binary\", \"\\nprotocol and higher degrees of reliability.\\n\\nService Bus provides a rich set of features, including\", \" transaction support and a duplicate detection\\nfeature. The queue guarantees \\u201cat most once delivery\\u201d\", \" per message. It automatically discards a\\nmessage that has already been sent. If a producer is in do\", \"ubt, it can resend the same message, and\\nService Bus guarantees that only one copy will be processed\", \". Duplicate detection frees you from\\nhaving to build additional infrastructure plumbing.\\n\\nTwo more e\", \"nterprise features are partitioning and sessions. A conventional Service Bus queue is\\nhandled by a s\", \"ingle message broker and stored in a single message store. But, Service Bus Partitioning\\nspreads the\", \" queue across multiple message brokers and message stores. The overall throughput is no\\nlonger limit\", \"ed by the performance of a single message broker or messaging store. A temporary\\noutage of a messagi\", \"ng store doesn\\u2019t render a partitioned queue unavailable.\\n\\nService Bus Sessions provide a way to grou\", \"p-related messages. Imagine a workflow scenario where\\nmessages must be processed together and the op\", \"eration completed at the end. To take advantage,\\nsessions must be explicitly enabled for the queue a\", \"nd each related messaged must contain the same\\nsession ID.\\n\\nHowever, there are some important caveat\", \"s: Service Bus queues size is limited to 80 GB, which is much\\nsmaller than what\\u2019s available from sto\", \"re queues. Additionally, Service Bus queues incur a base cost\\nand charge per operation.\\n\\nFigure 4-14\", \" outlines the high-level architecture of a Service Bus queue.\\n\\nFigure 4-14. Service Bus queue\\n\\nIn th\", \"e previous figure, note the point-to-point relationship. Two instances of the same provider are\\nenqu\", \"euing messages into a single Service Bus queue. Each message is consumed by only one of three\\nconsum\", \"er instances on the right. Next, we discuss how to implement messaging where different\\nconsumers may\", \" all be interested the same message.\\n\\nEvents\\n\\nMessage queuing is an effective way to implement commu\", \"nication where a producer can\\nasynchronously send a consumer a message. However, what happens when m\", \"any different consumers\\n\\n74\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fare interested in the\", \" same message? A dedicated message queue for each consumer wouldn\\u2019t scale\\nwell and would become diff\", \"icult to manage.\\n\\nTo address this scenario, we move to the third type of message interaction, the ev\", \"ent. One\\nmicroservice announces that an action had occurred. Other microservices, if interested, rea\", \"ct to the\\naction, or event. This is also known as the event-driven architectural style.\\n\\nEventing is\", \" a two-step process. For a given state change, a microservice publishes an event to a\\nmessage broker\", \", making it available to any other interested microservice. The interested microservice\\nis notified \", \"by subscribing to the event in the message broker. You use the Publish/Subscribe pattern\\nto implemen\", \"t event-based communication.\\n\\nFigure 4-15 shows a shopping basket microservice publishing an event w\", \"ith two other microservices\\nsubscribing to it.\\n\\nFigure 4-15. Event-Driven messaging\\n\\nNote the event \", \"bus component that sits in the middle of the communication channel. It\\u2019s a custom\\nclass that encapsu\", \"lates the message broker and decouples it from the underlying application. The\\nordering and inventor\", \"y microservices independently operate the event with no knowledge of each\\nother, nor the shopping ba\", \"sket microservice. When the registered event is published to the event bus,\\nthey act upon it.\\n\\nWith \", \"eventing, we move from queuing technology to topics. A topic is similar to a queue, but supports\\na o\", \"ne-to-many messaging pattern. One microservice publishes a message. Multiple subscribing\\nmicroservic\", \"es can choose to receive and act upon that message. Figure 4-16 shows a topic\\narchitecture.\\n\\n75\\n\\nCHA\", \"PTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-16. Topic architecture\\n\\nIn the previous figu\", \"re, publishers send messages to the topic. At the end, subscribers receive\\nmessages from subscriptio\", \"ns. In the middle, the topic forwards messages to subscriptions based on a\\nset of rules, shown in da\", \"rk blue boxes. Rules act as a filter that forward specific messages to a\\nsubscription. Here, a \\u201cGetP\", \"rice\\u201d event would be sent to the price and logging subscriptions as the\\nlogging subscription has cho\", \"sen to receive all messages. A \\u201cGetInformation\\u201d event would be sent to\\nthe information and logging s\", \"ubscriptions.\\n\\nThe Azure cloud supports two different topic services: Azure Service Bus Topics and A\", \"zure EventGrid.\\n\\nAzure Service Bus Topics\\n\\nSitting on top of the same robust brokered message model \", \"of Azure Service Bus queues are Azure\\nService Bus Topics. A topic can receive messages from multiple\", \" independent publishers and send\\nmessages to up to 2,000 subscribers. Subscriptions can be dynamical\", \"ly added or removed at run time\\nwithout stopping the system or recreating the topic.\\n\\nMany advanced \", \"features from Azure Service Bus queues are also available for topics, including\\nDuplicate Detection \", \"and Transaction support. By default, Service Bus topics are handled by a single\\nmessage broker and s\", \"tored in a single message store. But, Service Bus Partitioning scales a topic by\\nspreading it across\", \" many message brokers and message stores.\\n\\nScheduled Message Delivery tags a message with a specific\", \" time for processing. The message won\\u2019t\\nappear in the topic before that time. Message Deferral enabl\", \"es you to defer a retrieval of a message\\nto a later time. Both are commonly used in workflow process\", \"ing scenarios where operations are\\nprocessed in a particular order. You can postpone processing of r\", \"eceived messages until prior work\\nhas been completed.\\n\\nService Bus topics are a robust and proven te\", \"chnology for enabling publish/subscribe communication\\nin your cloud-native systems.\\n\\nAzure Event Gri\", \"d\\n\\nWhile Azure Service Bus is a battle-tested messaging broker with a full set of enterprise feature\", \"s,\\nAzure Event Grid is the new kid on the block.\\n\\n76\\n\\nCHAPTER 4 | Cloud-native communication pattern\", \"s\\n\\n\\fAt first glance, Event Grid may look like just another topic-based messaging system. However, it\", \"\\u2019s\\ndifferent in many ways. Focused on event-driven workloads, it enables real-time event processing,\", \"\\ndeep Azure integration, and an open-platform - all on serverless infrastructure. It\\u2019s designed for\\n\", \"contemporary cloud-native and serverless applications\\n\\nAs a centralized eventing backplane, or pipe,\", \" Event Grid reacts to events inside Azure resources and\\nfrom your own services.\\n\\nEvent notifications\", \" are published to an Event Grid Topic, which, in turn, routes each event to a\\nsubscription. Subscrib\", \"ers map to subscriptions and consume the events. Like Service Bus, Event Grid\\nsupports a filtered su\", \"bscriber model where a subscription sets rule for the events it wishes to receive.\\nEvent Grid provid\", \"es fast throughput with a guarantee of 10 million events per second enabling near\\nreal-time delivery\", \" - far more than what Azure Service Bus can generate.\\n\\nA sweet spot for Event Grid is its deep integ\", \"ration into the fabric of Azure infrastructure. An Azure\\nresource, such as Cosmos DB, can publish bu\", \"ilt-in events directly to other interested Azure resources -\\nwithout the need for custom code. Event\", \" Grid can publish events from an Azure Subscription,\\nResource Group, or Service, giving developers f\", \"ine-grained control over the lifecycle of cloud\\nresources. However, Event Grid isn\\u2019t limited to Azur\", \"e. It\\u2019s an open platform that can consume custom\\nHTTP events published from applications or third-pa\", \"rty services and route events to external\\nsubscribers.\\n\\nWhen publishing and subscribing to native ev\", \"ents from Azure resources, no coding is required. With\\nsimple configuration, you can integrate event\", \"s from one Azure resource to another leveraging built-in\\nplumbing for Topics and Subscriptions. Figu\", \"re 4-17 shows the anatomy of Event Grid.\\n\\nFigure 4-17. Event Grid anatomy\\n\\n77\\n\\nCHAPTER 4 | Cloud-nat\", \"ive communication patterns\\n\\n\\fA major difference between EventGrid and Service Bus is the underlying \", \"message exchange pattern.\\n\\nService Bus implements an older style pull model in which the downstream \", \"subscriber actively polls\\nthe topic subscription for new messages. On the upside, this approach give\", \"s the subscriber full control\\nof the pace at which it processes messages. It controls when and how m\", \"any messages to process at\\nany given time. Unread messages remain in the subscription until processe\", \"d. A significant\\nshortcoming is the latency between the time the event is generated and the polling \", \"operation that\\npulls that message to the subscriber for processing. Also, the overhead of constant p\", \"olling for the\\nnext event consumes resources and money.\\n\\nEventGrid, however, is different. It implem\", \"ents a push model in which events are sent to the\\nEventHandlers as received, giving near real-time e\", \"vent delivery. It also reduces cost as the service is\\ntriggered only when it\\u2019s needed to consume an \", \"event \\u2013 not continually as with polling. That said, an\\nevent handler must handle the incoming load a\", \"nd provide throttling mechanisms to protect itself\\nfrom becoming overwhelmed. Many Azure services th\", \"at consume these events, such as Azure\\nFunctions and Logic Apps provide automatic autoscaling capabi\", \"lities to handle increased loads.\\n\\nEvent Grid is a fully managed serverless cloud service. It dynami\", \"cally scales based on your traffic and\\ncharges you only for your actual usage, not pre-purchased cap\", \"acity. The first 100,000 operations per\\nmonth are free \\u2013 operations being defined as event ingress (\", \"incoming event notifications),\\nsubscription delivery attempts, management calls, and filtering by su\", \"bject. With 99.99% availability,\\nEventGrid guarantees the delivery of an event within a 24-hour peri\", \"od, with built-in retry functionality\\nfor unsuccessful delivery. Undelivered messages can be moved t\", \"o a \\u201cdead-letter\\u201d queue for resolution.\\nUnlike Azure Service Bus, Event Grid is tuned for fast perfo\", \"rmance and doesn\\u2019t support features like\\nordered messaging, transactions, and sessions.\\n\\nStreaming m\", \"essages in the Azure cloud\\n\\nAzure Service Bus and Event Grid provide great support for applications \", \"that expose single, discrete\\nevents like a new document has been inserted into a Cosmos DB. But, wha\", \"t if your cloud-native\\nsystem needs to process a stream of related events? Event streams are more co\", \"mplex. They\\u2019re typically\\ntime-ordered, interrelated, and must be processed as a group.\\n\\nAzure Event \", \"Hub is a data streaming platform and event ingestion service that collects, transforms,\\nand stores e\", \"vents. It\\u2019s fine-tuned to capture streaming data, such as continuous event notifications\\nemitted fro\", \"m a telemetry context. The service is highly scalable and can store and process millions of\\nevents p\", \"er second. Shown in Figure 4-18, it\\u2019s often a front door for an event pipeline, decoupling\\ningest st\", \"ream from event consumption.\\n\\n78\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-18. Azu\", \"re Event Hub\\n\\nEvent Hub supports low latency and configurable time retention. Unlike queues and topi\", \"cs, Event\\nHubs keep event data after it\\u2019s been read by a consumer. This feature enables other data a\", \"nalytic\\nservices, both internal and external, to replay the data for further analysis. Events stored\", \" in event hub\\nare only deleted upon expiration of the retention period, which is one day by default,\", \" but\\nconfigurable.\\n\\nEvent Hub supports common event publishing protocols including HTTPS and AMQP. I\", \"t also supports\\nKafka 1.0. Existing Kafka applications can communicate with Event Hub using the Kafk\", \"a protocol\\nproviding an alternative to managing large Kafka clusters. Many open-source cloud-native \", \"systems\\nembrace Kafka.\\n\\nEvent Hubs implements message streaming through a partitioned consumer model\", \" in which each\\nconsumer only reads a specific subset, or partition, of the message stream. This patt\", \"ern enables\\ntremendous horizontal scale for event processing and provides other stream-focused featu\", \"res that are\\nunavailable in queues and topics. A partition is an ordered sequence of events that is \", \"held in an event\\nhub. As newer events arrive, they\\u2019re added to the end of this sequence. Figure 4-19\", \" shows partitioning\\nin an Event Hub.\\n\\nFigure 4-19. Event Hub partitioning\\n\\nInstead of reading from t\", \"he same resource, each consumer group reads across a subset, or partition,\\nof the message stream.\\n\\n7\", \"9\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFor cloud-native applications that must stream \", \"large numbers of events, Azure Event Hub can be a\\nrobust and affordable solution.\\n\\ngRPC\\n\\nSo far in t\", \"his book, we\\u2019ve focused on REST-based communication. We\\u2019ve seen that REST is a flexible\\narchitectura\", \"l style that defines CRUD-based operations against entity resources. Clients interact with\\nresources\", \" across HTTP with a request/response communication model. While REST is widely\\nimplemented, a newer \", \"communication technology, gRPC, has gained tremendous momentum across\\nthe cloud-native community.\\n\\nW\", \"hat is gRPC?\\n\\ngRPC is a modern, high-performance framework that evolves the age-old remote procedure\", \" call (RPC)\\nprotocol. At the application level, gRPC streamlines messaging between clients and back-\", \"end services.\\nOriginating from Google, gRPC is open source and part of the Cloud Native Computing Fo\", \"undation\\n(CNCF) ecosystem of cloud-native offerings. CNCF considers gRPC an incubating project. Incu\", \"bating\\nmeans end users are using the technology in production applications, and the project has a he\", \"althy\\nnumber of contributors.\\n\\nA typical gRPC client app will expose a local, in-process function th\", \"at implements a business\\noperation. Under the covers, that local function invokes another function o\", \"n a remote machine. What\\nappears to be a local call essentially becomes a transparent out-of-process\", \" call to a remote service.\\nThe RPC plumbing abstracts the point-to-point networking communication, s\", \"erialization, and\\nexecution between computers.\\n\\nIn cloud-native applications, developers often work \", \"across programming languages, frameworks, and\\ntechnologies. This interoperability complicates messag\", \"e contracts and the plumbing required for\\ncross-platform communication. gRPC provides a \\u201cuniform hor\", \"izontal layer\\u201d that abstracts these\\nconcerns. Developers code in their native platform focused on bu\", \"siness functionality, while gRPC\\nhandles communication plumbing.\\n\\ngRPC offers comprehensive support \", \"across most popular development stacks, including Java,\\nJavaScript, C#, Go, Swift, and NodeJS.\\n\\ngRPC\", \" Benefits\\n\\ngRPC uses HTTP/2 for its transport protocol. While compatible with HTTP 1.1, HTTP/2 featu\", \"res many\\nadvanced capabilities:\\n\\n\\u2022\\n\\nA binary framing protocol for data transport - unlike HTTP 1.1, \", \"which is text based.\\n\\n\\u2022  Multiplexing support for sending multiple parallel requests over the same c\", \"onnection - HTTP\\n\\n1.1 limits processing to one request/response message at a time.\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n80\\n\\nBid\", \"irectional full-duplex communication for sending both client requests and server responses\\nsimultane\", \"ously.\\n\\nBuilt-in streaming enabling requests and responses to asynchronously stream large data sets.\", \"\\n\\nHeader compression that reduces network usage.\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\f\", \"gRPC is lightweight and highly performant. It can be up to 8x faster than JSON serialization with\\nme\", \"ssages 60-80% smaller. In Microsoft Windows Communication Foundation (WCF) parlance, gRPC\\nperformanc\", \"e exceeds the speed and efficiency of the highly optimized NetTCP bindings. Unlike\\nNetTCP, which fav\", \"ors the Microsoft stack, gRPC is cross-platform.\\n\\nProtocol Buffers\\n\\ngRPC embraces an open-source tec\", \"hnology called Protocol Buffers. They provide a highly efficient\\nand platform-neutral serialization \", \"format for serializing structured messages that services send to\\neach other. Using a cross-platform \", \"Interface Definition Language (IDL), developers define a service\\ncontract for each microservice. The\", \" contract, implemented as a text-based .proto file, describes the\\nmethods, inputs, and outputs for e\", \"ach service. The same contract file can be used for gRPC clients and\\nservices built on different dev\", \"elopment platforms.\\n\\nUsing the proto file, the Protobuf compiler, protoc, generates both client and \", \"service code for your\\ntarget platform. The code includes the following components:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nStrongl\", \"y typed objects, shared by the client and service, that represent the service operations\\nand data el\", \"ements for a message.\\n\\nA strongly typed base class with the required network plumbing that the remot\", \"e gRPC service\\ncan inherit and extend.\\n\\nA client stub that contains the required plumbing to invoke \", \"the remote gRPC service.\\n\\nAt run time, each message is serialized as a standard Protobuf representat\", \"ion and exchanged between\\nthe client and remote service. Unlike JSON or XML, Protobuf messages are s\", \"erialized as compiled\\nbinary bytes.\\n\\nThe book, gRPC for WCF Developers, available from the Microsoft\", \" Architecture site, provides in-depth\\ncoverage of gRPC and Protocol Buffers.\\n\\ngRPC support in .NET\\n\\n\", \"gRPC is integrated into .NET Core 3.0 SDK and later. The following tools support it:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nVisua\", \"l Studio 2022 with the ASP.NET and web development workload installed\\n\\nVisual Studio Code\\n\\nThe dotne\", \"t CLI\\n\\nThe SDK includes tooling for endpoint routing, built-in IoC, and logging. The open-source Kes\", \"trel web\\nserver supports HTTP/2 connections. Figure 4-20 shows a Visual Studio 2022 template that sc\", \"affolds a\\nskeleton project for a gRPC service. Note how .NET fully supports Windows, Linux, and macO\", \"S.\\n\\n81\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fFigure 4-20. gRPC support in Visual Studio\", \" 2022\\n\\nFigure 4-21 shows the skeleton gRPC service generated from the built-in scaffolding included \", \"in\\nVisual Studio 2022.\\n\\nFigure 4-21. gRPC project in Visual Studio 2022\\n\\nIn the previous figure, not\", \"e the proto description file and service code. As you\\u2019ll see shortly, Visual\\nStudio generates additi\", \"onal configuration in both the Startup class and underlying project file.\\n\\ngRPC usage\\n\\nFavor gRPC fo\", \"r the following scenarios:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n82\\n\\nSynchronous backend microservice-to-microservice communi\", \"cation where an immediate\\nresponse is required to continue processing.\\n\\nPolyglot environments that n\", \"eed to support mixed programming platforms.\\n\\nLow latency and high throughput communication where per\", \"formance is critical.\\n\\nPoint-to-point real-time communication - gRPC can push messages in real time \", \"without\\npolling and has excellent support for bi-directional streaming.\\n\\nCHAPTER 4 | Cloud-native co\", \"mmunication patterns\\n\\n\\f\\u2022\\n\\nNetwork constrained environments \\u2013 binary gRPC messages are always smaller\", \" than an\\nequivalent text-based JSON message.\\n\\nAt the time, of this writing, gRPC is primarily used w\", \"ith backend services. Modern browsers can\\u2019t\\nprovide the level of HTTP/2 control required to support \", \"a front-end gRPC client. That said, there\\u2019s\\nsupport for gRPC-Web with .NET that enables gRPC communi\", \"cation from browser-based apps built\\nwith JavaScript or Blazor WebAssembly technologies. gRPC-Web en\", \"ables an ASP.NET Core gRPC app\\nto support gRPC features in browser apps:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nStrongly typed, c\", \"ode-generated clients\\n\\nCompact Protobuf messages\\n\\nServer streaming\\n\\ngRPC implementation\\n\\nThe microse\", \"rvice reference architecture, eShop on Containers, from Microsoft, shows how to\\nimplement gRPC servi\", \"ces in .NET applications. Figure 4-22 presents the back-end architecture.\\n\\nFigure 4-22. Backend arch\", \"itecture for eShop on Containers\\n\\nIn the previous figure, note how eShop embraces the Backend for Fr\", \"ontends pattern (BFF) by\\nexposing multiple API gateways. We discussed the BFF pattern earlier in thi\", \"s chapter. Pay close\\n\\n83\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fattention to the Aggrega\", \"tor microservice (in gray) that sits between the Web-Shopping API Gateway\\nand backend Shopping micro\", \"services. The Aggregator receives a single request from a client,\\ndispatches it to various microserv\", \"ices, aggregates the results, and sends them back to the requesting\\nclient. Such operations typicall\", \"y require synchronous communication as to produce an immediate\\nresponse. In eShop, backend calls fro\", \"m the Aggregator are performed using gRPC as shown in Figure\\n4-23.\\n\\nFigure 4-23. gRPC in eShop on Co\", \"ntainers\\n\\ngRPC communication requires both client and server components. In the previous figure, not\", \"e how\\nthe Shopping Aggregator implements a gRPC client. The client makes synchronous gRPC calls (in \", \"red)\\nto backend microservices, each of which implement a gRPC server. Both the client and server tak\", \"e\\nadvantage of the built-in gRPC plumbing from the .NET SDK. Client-side stubs provide the plumbing\\n\", \"to invoke remote gRPC calls. Server-side components provide gRPC plumbing that custom service\\nclasse\", \"s can inherit and consume.\\n\\nMicroservices that expose both a RESTful API and gRPC communication requ\", \"ire multiple endpoints to\\nmanage traffic. You would open an endpoint that listens for HTTP traffic f\", \"or the RESTful calls and\\nanother for gRPC calls. The gRPC endpoint must be configured for the HTTP/2\", \" protocol that is\\nrequired for gRPC communication.\\n\\nWhile we strive to decouple microservices with a\", \"synchronous communication patterns, some\\noperations require direct calls. gRPC should be the primary\", \" choice for direct synchronous\\ncommunication between microservices. Its high-performance communicati\", \"on protocol, based on\\nHTTP/2 and protocol buffers, make it a perfect choice.\\n\\n84\\n\\nCHAPTER 4 | Cloud-\", \"native communication patterns\\n\\n\\fLooking ahead\\n\\nLooking ahead, gRPC will continue to gain traction fo\", \"r cloud-native systems. The performance\\nbenefits and ease of development are compelling. However, RE\", \"ST will likely be around for a long time.\\nIt excels for publicly exposed APIs and for backward compa\", \"tibility reasons.\\n\\nService Mesh communication infrastructure\\n\\nThroughout this chapter, we\\u2019ve explore\", \"d the challenges of microservice communication. We said that\\ndevelopment teams need to be sensitive \", \"to how back-end services communicate with each other.\\nIdeally, the less inter-service communication,\", \" the better. However, avoidance isn\\u2019t always possible as\\nback-end services often rely on one another\", \" to complete operations.\\n\\nWe explored different approaches for implementing synchronous HTTP communi\", \"cation and\\nasynchronous messaging. In each of the cases, the developer is burdened with implementing\", \"\\ncommunication code. Communication code is complex and time intensive. Incorrect decisions can\\nlead \", \"to significant performance issues.\\n\\nA more modern approach to microservice communication centers aro\", \"und a new and rapidly evolving\\ntechnology entitled Service Mesh. A service mesh is a configurable in\", \"frastructure layer with built-in\\ncapabilities to handle service-to-service communication, resiliency\", \", and many cross-cutting concerns.\\nIt moves the responsibility for these concerns out of the microse\", \"rvices and into service mesh layer.\\nCommunication is abstracted away from your microservices.\\n\\nA key\", \" component of a service mesh is a proxy. In a cloud-native application, an instance of a proxy is\\nty\", \"pically colocated with each microservice. While they execute in separate processes, the two are\\nclos\", \"ely linked and share the same lifecycle. This pattern, known as the Sidecar pattern, and is shown in\", \"\\nFigure 4-24.\\n\\nFigure 4-24. Service mesh with a side car\\n\\n85\\n\\nCHAPTER 4 | Cloud-native communication\", \" patterns\\n\\n\\fNote in the previous figure how messages are intercepted by a proxy that runs alongside \", \"each\\nmicroservice. Each proxy can be configured with traffic rules specific to the microservice. It\\n\", \"understands messages and can route them across your services and the outside world.\\n\\nAlong with mana\", \"ging service-to-service communication, the Service Mesh provides support for\\nservice discovery and l\", \"oad balancing.\\n\\nOnce configured, a service mesh is highly functional. The mesh retrieves a correspon\", \"ding pool of\\ninstances from a service discovery endpoint. It sends a request to a specific service i\", \"nstance, recording\\nthe latency and response type of the result. It chooses the instance most likely \", \"to return a fast\\nresponse based on different factors, including the observed latency for recent requ\", \"ests.\\n\\nA service mesh manages traffic, communication, and networking concerns at the application lev\", \"el. It\\nunderstands messages and requests. A service mesh typically integrates with a container orche\", \"strator.\\nKubernetes supports an extensible architecture in which a service mesh can be added.\\n\\nIn ch\", \"apter 6, we deep-dive into Service Mesh technologies including a discussion on its architecture\\nand \", \"available open-source implementations.\\n\\nSummary\\n\\nIn this chapter, we discussed cloud-native communic\", \"ation patterns. We started by examining how\\nfront-end clients communicate with back-end microservice\", \"s. Along the way, we talked about API\\nGateway platforms and real-time communication. We then looked \", \"at how microservices communicate\\nwith other back-end services. We looked at both synchronous HTTP co\", \"mmunication and\\nasynchronous messaging across services. We covered gRPC, an upcoming technology in t\", \"he cloud-\\nnative world. Finally, we introduced a new and rapidly evolving technology entitled Servic\", \"e Mesh that\\ncan streamline microservice communication.\\n\\nSpecial emphasis was on managed Azure servic\", \"es that can help implement communication in cloud-\\nnative systems:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure Appli\", \"cation Gateway\\n\\nAzure API Management\\n\\nAzure SignalR Service\\n\\nAzure Storage Queues\\n\\nAzure Service Bus\", \"\\n\\nAzure Event Grid\\n\\nAzure Event Hub\\n\\nWe next move to distributed data in cloud-native systems and th\", \"e benefits and challenges that it\\npresents.\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n86\\n\\n.NET Microservices: Architectu\", \"re for Containerized .NET applications\\n\\nDesigning Interservice Communication for Microservices\\n\\nAzur\", \"e SignalR Service, a fully managed service to add real-time functionality\\n\\nCHAPTER 4 | Cloud-native \", \"communication patterns\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure API Gateway Ingress Controller\\n\\ngRPC Documentation\\n\\ngR\", \"PC for WCF Developers\\n\\nComparing gRPC Services with HTTP APIs\\n\\nBuilding gRPC Services with .NET vide\", \"o\\n\\n87\\n\\nCHAPTER 4 | Cloud-native communication patterns\\n\\n\\fCHAPTER  5\\n\\nCloud-native data patterns\\n\\nAs \", \"we\\u2019ve seen throughout this book, a cloud-native approach changes the way you design, deploy,\\nand man\", \"age applications. It also changes the way you manage and store data.\\n\\nFigure 5-1 contrasts the diffe\", \"rences.\\n\\nFigure 5-1. Data management in cloud-native applications\\n\\nExperienced developers will easil\", \"y recognize the architecture on the left-side of figure 5-1. In this\\nmonolithic application, busines\", \"s service components collocate together in a shared services tier,\\nsharing data from a single relati\", \"onal database.\\n\\nIn many ways, a single database keeps data management simple. Querying data across m\", \"ultiple tables\\nis straightforward. Changes to data update together or they all rollback. ACID transa\", \"ctions guarantee\\nstrong and immediate consistency.\\n\\nDesigning for cloud-native, we take a different \", \"approach. On the right-side of Figure 5-1, note how\\nbusiness functionality segregates into small, in\", \"dependent microservices. Each microservice\\nencapsulates a specific business capability and its own d\", \"ata. The monolithic database decomposes\\n\\n88\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\finto a distrib\", \"uted data model with many smaller databases, each aligning with a microservice. When\\nthe smoke clear\", \"s, we emerge with a design that exposes a database per microservice.\\n\\nDatabase-per-microservice, why\", \"?\\n\\nThis database per microservice provides many benefits, especially for systems that must evolve ra\", \"pidly\\nand support massive scale. With this model\\u2026\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nDomain data is encapsulated within th\", \"e service\\n\\nData schema can evolve without directly impacting other services\\n\\nEach data store can ind\", \"ependently scale\\n\\nA data store failure in one service won\\u2019t directly impact other services\\n\\nSegregat\", \"ing data also enables each microservice to implement the data store type that is best\\noptimized for \", \"its workload, storage needs, and read/write patterns. Choices include relational,\\ndocument, key-valu\", \"e, and even graph-based data stores.\\n\\nFigure 5-2 presents the principle of polyglot persistence in a\", \" cloud-native system.\\n\\nFigure 5-2. Polyglot data persistence\\n\\nNote in the previous figure how each m\", \"icroservice supports a different type of data store.\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n89\\n\\nThe product catalog microservice \", \"consumes a relational database to accommodate the rich\\nrelational structure of its underlying data.\\n\", \"\\nThe shopping cart microservice consumes a distributed cache that supports its simple, key-\\nvalue da\", \"ta store.\\n\\nThe ordering microservice consumes both a NoSql document database for write operations\\nal\", \"ong with a highly denormalized key/value store to accommodate high-volumes of read\\noperations.\\n\\nCHAP\", \"TER 5 | Cloud-native data patterns\\n\\n\\fWhile relational databases remain relevant for microservices wi\", \"th complex data, NoSQL databases\\nhave gained considerable popularity. They provide massive scale and\", \" high availability. Their\\nschemaless nature allows developers to move away from an architecture of t\", \"yped data classes and\\nORMs that make change expensive and time-consuming. We cover NoSQL databases l\", \"ater in this\\nchapter.\\n\\nWhile encapsulating data into separate microservices can increase agility, pe\", \"rformance, and scalability,\\nit also presents many challenges. In the next section, we discuss these \", \"challenges along with patterns\\nand practices to help overcome them.\\n\\nCross-service queries\\n\\nWhile mi\", \"croservices are independent and focus on specific functional capabilities, like inventory,\\nshipping,\", \" or ordering, they frequently require integration with other microservices. Often the\\nintegration in\", \"volves one microservice querying another for data. Figure 5-3 shows the scenario.\\n\\nFigure 5-3. Query\", \"ing across microservices\\n\\nIn the preceding figure, we see a shopping basket microservice that adds a\", \"n item to a user\\u2019s shopping\\nbasket. While the data store for this microservice contains basket and l\", \"ine item data, it doesn\\u2019t\\nmaintain product or pricing data. Instead, those data items are owned by t\", \"he catalog and pricing\\nmicroservices. This aspect presents a problem. How can the shopping basket mi\", \"croservice add a\\nproduct to the user\\u2019s shopping basket when it doesn\\u2019t have product nor pricing data\", \" in its database?\\n\\nOne option discussed in Chapter 4 is a direct HTTP call from the shopping basket \", \"to the catalog and\\npricing microservices. However, in chapter 4, we said synchronous HTTP calls coup\", \"le microservices\\ntogether, reducing their autonomy and diminishing their architectural benefits.\\n\\nWe\", \" could also implement a request-reply pattern with separate inbound and outbound queues for\\neach ser\", \"vice. However, this pattern is complicated and requires plumbing to correlate request and\\nresponse m\", \"essages. While it does decouple the backend microservice calls, the calling service must\\nstill synch\", \"ronously wait for the call to complete. Network congestion, transient faults, or an\\noverloaded micro\", \"service and can result in long-running and even failed operations.\\n\\n90\\n\\nCHAPTER 5 | Cloud-native dat\", \"a patterns\\n\\n\\fInstead, a widely accepted pattern for removing cross-service dependencies is the Mater\", \"ialized View\\nPattern, shown in Figure 5-4.\\n\\nFigure 5-4. Materialized View Pattern\\n\\nWith this pattern\", \", you place a local data table (known as a read model) in the shopping basket service.\\nThis table co\", \"ntains a denormalized copy of the data needed from the product and pricing\\nmicroservices. Copying th\", \"e data directly into the shopping basket microservice eliminates the need for\\nexpensive cross-servic\", \"e calls. With the data local to the service, you improve the service\\u2019s response\\ntime and reliability\", \". Additionally, having its own copy of the data makes the shopping basket service\\nmore resilient. If\", \" the catalog service should become unavailable, it wouldn\\u2019t directly impact the\\nshopping basket serv\", \"ice. The shopping basket can continue operating with the data from its own\\nstore.\\n\\nThe catch with th\", \"is approach is that you now have duplicate data in your system. However,\\nstrategically duplicating d\", \"ata in cloud-native systems is an established practice and not considered an\\nanti-pattern, or bad pr\", \"actice. Keep in mind that one and only one service can own a data set and have\\nauthority over it. Yo\", \"u\\u2019ll need to synchronize the read models when the system of record is updated.\\nSynchronization is ty\", \"pically implemented via asynchronous messaging with a publish/subscribe\\npattern, as shown in Figure \", \"5.4.\\n\\nDistributed transactions\\n\\nWhile querying data across microservices is difficult, implementing \", \"a transaction across several\\nmicroservices is even more complex. The inherent challenge of maintaini\", \"ng data consistency across\\nindependent data sources in different microservices can\\u2019t be understated.\", \" The lack of distributed\\ntransactions in cloud-native applications means that you must manage distri\", \"buted transactions\\nprogrammatically. You move from a world of immediate consistency to that of event\", \"ual consistency.\\n\\nFigure 5-5 shows the problem.\\n\\n91\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fFigure\", \" 5-5. Implementing a transaction across microservices\\n\\nIn the preceding figure, five independent mic\", \"roservices participate in a distributed transaction that\\ncreates an order. Each microservice maintai\", \"ns its own data store and implements a local transaction\\nfor its store. To create the order, the loc\", \"al transaction for each individual microservice must succeed,\\nor all must abort and roll back the op\", \"eration. While built-in transactional support is available inside\\neach of the microservices, there\\u2019s\", \" no support for a distributed transaction that would span across all\\nfive services to keep data cons\", \"istent.\\n\\nInstead, you must construct this distributed transaction programmatically.\\n\\nA popular patte\", \"rn for adding distributed transactional support is the Saga pattern. It\\u2019s implemented\\nby grouping lo\", \"cal transactions together programmatically and sequentially invoking each one. If any\\nof the local t\", \"ransactions fail, the Saga aborts the operation and invokes a set of compensating\\ntransactions. The \", \"compensating transactions undo the changes made by the preceding local\\ntransactions and restore data\", \" consistency. Figure 5-6 shows a failed transaction with the Saga pattern.\\n\\nFigure 5-6. Rolling back\", \" a transaction\\n\\n92\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fIn the previous figure, the Update Inve\", \"ntory operation has failed in the Inventory microservice. The\\nSaga invokes a set of compensating tra\", \"nsactions (in red) to adjust the inventory counts, cancel the\\npayment and the order, and return the \", \"data for each microservice back to a consistent state.\\n\\nSaga patterns are typically choreographed as\", \" a series of related events, or orchestrated as a set of\\nrelated commands. In Chapter 4, we discusse\", \"d the service aggregator pattern that would be the\\nfoundation for an orchestrated saga implementatio\", \"n. We also discussed eventing along with Azure\\nService Bus and Azure Event Grid topics that would be\", \" a foundation for a choreographed saga\\nimplementation.\\n\\nHigh volume data\\n\\nLarge cloud-native applica\", \"tions often support high-volume data requirements. In these scenarios,\\ntraditional data storage tech\", \"niques can cause bottlenecks. For complex systems that deploy on a large\\nscale, both Command and Que\", \"ry Responsibility Segregation (CQRS) and Event Sourcing may improve\\napplication performance.\\n\\nCQRS\\n\\n\", \"CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The\", \"\\npattern separates operations that read data from those operations that write data.\\n\\nFor normal scen\", \"arios, the same entity model and data repository object are used for both read and\\nwrite operations.\", \"\\n\\nHowever, a high volume data scenario can benefit from separate models and data tables for reads\\nan\", \"d writes. To improve performance, the read operation could query against a highly denormalized\\nrepre\", \"sentation of the data to avoid expensive repetitive table joins and table locks. The write\\noperation\", \", known as a command, would update against a fully normalized representation of the data\\nthat would \", \"guarantee consistency. You then need to implement a mechanism to keep both\\nrepresentations in sync. \", \"Typically, whenever the write table is modified, it publishes an event that\\nreplicates the modificat\", \"ion to the read table.\\n\\nFigure 5-7 shows an implementation of the CQRS pattern.\\n\\nFigure 5-7. CQRS im\", \"plementation\\n\\n93\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fIn the previous figure, separate command \", \"and query models are implemented. Each data write\\noperation is saved to the write store and then pro\", \"pagated to the read store. Pay close attention to\\nhow the data propagation process operates on the p\", \"rinciple of eventual consistency. The read model\\neventually synchronizes with the write model, but t\", \"here may be some lag in the process. We discuss\\neventual consistency in the next section.\\n\\nThis sepa\", \"ration enables reads and writes to scale independently. Read operations use a schema\\noptimized for q\", \"ueries, while the writes use a schema optimized for updates. Read queries go against\\ndenormalized da\", \"ta, while complex business logic can be applied to the write model. As well, you\\nmight impose tighte\", \"r security on write operations than those exposing reads.\\n\\nImplementing CQRS can improve application\", \" performance for cloud-native services. However, it does\\nresult in a more complex design. Apply this\", \" principle carefully and strategically to those sections of\\nyour cloud-native application that will \", \"benefit from it. For more on CQRS, see the Microsoft book .NET\\nMicroservices: Architecture for Conta\", \"inerized .NET Applications.\\n\\nEvent sourcing\\n\\nAnother approach to optimizing high volume data scenari\", \"os involves Event Sourcing.\\n\\nA system typically stores the current state of a data entity. If a user\", \" changes their phone number, for\\nexample, the customer record is updated with the new number. We alw\", \"ays know the current state of a\\ndata entity, but each update overwrites the previous state.\\n\\nIn most\", \" cases, this model works fine. In high volume systems, however, overhead from transactional\\nlocking \", \"and frequent update operations can impact database performance, responsiveness, and limit\\nscalabilit\", \"y.\\n\\nEvent Sourcing takes a different approach to capturing data. Each operation that affects data is\", \"\\npersisted to an event store. Instead of updating the state of a data record, we append each change \", \"to\\na sequential list of past events - similar to an accountant\\u2019s ledger. The Event Store becomes the\", \"\\nsystem of record for the data. It\\u2019s used to propagate various materialized views within the bounded\", \"\\ncontext of a microservice. Figure 5.8 shows the pattern.\\n\\n94\\n\\nCHAPTER 5 | Cloud-native data pattern\", \"s\\n\\n\\fFigure 5-8. Event Sourcing\\n\\nIn the previous figure, note how each entry (in blue) for a user\\u2019s s\", \"hopping cart is appended to an\\nunderlying event store. In the adjoining materialized view, the syste\", \"m projects the current state by\\nreplaying all the events associated with each shopping cart. This vi\", \"ew, or read model, is then exposed\\nback to the UI. Events can also be integrated with external syste\", \"ms and applications or queried to\\ndetermine the current state of an entity. With this approach, you \", \"maintain history. You know not only\\nthe current state of an entity, but also how you reached this st\", \"ate.\\n\\nMechanically speaking, event sourcing simplifies the write model. There are no updates or dele\", \"tes.\\nAppending each data entry as an immutable event minimizes contention, locking, and concurrency\\n\", \"conflicts associated with relational databases. Building read models with the materialized view patt\", \"ern\\nenables you to decouple the view from the write model and choose the best data store to optimize\", \"\\nthe needs of your application UI.\\n\\nFor this pattern, consider a data store that directly supports e\", \"vent sourcing. Azure Cosmos DB,\\nMongoDB, Cassandra, CouchDB, and RavenDB are good candidates.\\n\\nAs wi\", \"th all patterns and technologies, implement strategically and when needed. While event sourcing\\ncan \", \"provide increased performance and scalability, it comes at the expense of complexity and a\\nlearning \", \"curve.\\n\\n95\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fRelational vs. NoSQL data\\n\\nRelational and NoSQL\", \" are two types of database systems commonly implemented in cloud-native\\napps. They\\u2019re built differen\", \"tly, store data differently, and accessed differently. In this section, we\\u2019ll look\\nat both. Later in\", \" this chapter, we\\u2019ll look at an emerging database technology called NewSQL.\\n\\nRelational databases ha\", \"ve been a prevalent technology for decades. They\\u2019re mature, proven, and\\nwidely implemented. Competin\", \"g database products, tooling, and expertise abound. Relational\\ndatabases provide a store of related \", \"data tables. These tables have a fixed schema, use SQL\\n(Structured Query Language) to manage data, a\", \"nd support ACID guarantees.\\n\\nNo-SQL databases refer to high-performance, non-relational data stores.\", \" They excel in their ease-of-\\nuse, scalability, resilience, and availability characteristics. Instea\", \"d of joining tables of normalized data,\\nNoSQL stores unstructured or semi-structured data, often in \", \"key-value pairs or JSON documents. No-\\nSQL databases typically don\\u2019t provide ACID guarantees beyond \", \"the scope of a single database\\npartition. High volume services that require sub second response time\", \" favor NoSQL datastores.\\n\\nThe impact of NoSQL technologies for distributed cloud-native systems can\\u2019\", \"t be overstated. The\\nproliferation of new data technologies in this space has disrupted solutions th\", \"at once exclusively\\nrelied on relational databases.\\n\\nNoSQL databases include several different model\", \"s for accessing and managing data, each suited to\\nspecific use cases. Figure 5-9 presents four commo\", \"n models.\\n\\nFigure 5-9: Data models for NoSQL databases\\n\\nModel\\n\\nCharacteristics\\n\\nDocument Store\\n\\nKey \", \"Value Store\\n\\nWide-Column Store\\n\\nGraph Store\\n\\nData and metadata are stored hierarchically in\\nJSON-bas\", \"ed documents inside the database.\\n\\nThe simplest of the NoSQL databases, data is\\nrepresented as a col\", \"lection of key-value pairs.\\n\\nRelated data is stored as a set of nested-\\nkey/value pairs within a sin\", \"gle column.\\n\\nData is stored in a graph structure as node,\\nedge, and data properties.\\n\\n96\\n\\nCHAPTER 5 \", \"| Cloud-native data patterns\\n\\n\\fThe CAP theorem\\n\\nAs a way to understand the differences between these\", \" types of databases, consider the CAP theorem,\\na set of principles applied to distributed systems th\", \"at store state. Figure 5-10 shows the three\\nproperties of the CAP theorem.\\n\\nFigure 5-10. The CAP the\", \"orem\\n\\nThe theorem states that distributed data systems will offer a trade-off between consistency,\\na\", \"vailability, and partition tolerance. And, that any database can only guarantee two of the three\\npro\", \"perties:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nConsistency. Every node in the cluster responds with the most recent data, even i\", \"f the system\\nmust block the request until all replicas update. If you query a \\u201cconsistent system\\u201d fo\", \"r an item\\nthat is currently updating, you\\u2019ll wait for that response until all replicas successfully \", \"update.\\nHowever, you\\u2019ll receive the most current data.\\n\\nAvailability. Every node returns an immediat\", \"e response, even if that response isn\\u2019t the most\\nrecent data. If you query an \\u201cavailable system\\u201d for\", \" an item that is updating, you\\u2019ll get the best\\npossible answer the service can provide at that momen\", \"t.\\n\\nPartition Tolerance. Guarantees the system continues to operate even if a replicated data\\nnode f\", \"ails or loses connectivity with other replicated data nodes.\\n\\nCAP theorem explains the tradeoffs ass\", \"ociated with managing consistency and availability during a\\nnetwork partition; however tradeoffs wit\", \"h respect to consistency and performance also exist with the\\nabsence of a network partition. CAP the\", \"orem is often further extended to PACELC to explain the\\ntradeoffs more comprehensively.\\n\\nRelational \", \"databases typically provide consistency and availability, but not partition tolerance. They\\u2019re\\ntypic\", \"ally provisioned to a single server and scale vertically by adding more resources to the machine.\\n\\n9\", \"7\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fMany relational database systems support built-in replic\", \"ation features where copies of the primary\\ndatabase can be made to other secondary server instances.\", \" Write operations are made to the primary\\ninstance and replicated to each of the secondaries. Upon a\", \" failure, the primary instance can fail over\\nto a secondary to provide high availability. Secondarie\", \"s can also be used to distribute read operations.\\nWhile writes operations always go against the prim\", \"ary replica, read operations can be routed to any of\\nthe secondaries to reduce system load.\\n\\nData ca\", \"n also be horizontally partitioned across multiple nodes, such as with sharding. But, sharding\\ndrama\", \"tically increases operational overhead by spitting data across many pieces that cannot easily\\ncommun\", \"icate. It can be costly and time consuming to manage. Relational features that include table\\njoins, \", \"transactions, and referential integrity require steep performance penalties in sharded\\ndeployments.\\n\", \"\\nReplication consistency and recovery point objectives can be tuned by configuring whether replicati\", \"on\\noccurs synchronously or asynchronously. If data replicas were to lose network connectivity in a \\u201c\", \"highly\\nconsistent\\u201d or synchronous relational database cluster, you wouldn\\u2019t be able to write to the \", \"database.\\nThe system would reject the write operation as it can\\u2019t replicate that change to the other\", \" data replica.\\nEvery data replica has to update before the transaction can complete.\\n\\nNoSQL database\", \"s typically support high availability and partition tolerance. They scale out\\nhorizontally, often ac\", \"ross commodity servers. This approach provides tremendous availability, both\\nwithin and across geogr\", \"aphical regions at a reduced cost. You partition and replicate data across\\nthese machines, or nodes,\", \" providing redundancy and fault tolerance. Consistency is typically tuned\\nthrough consensus protocol\", \"s or quorum mechanisms. They provide more control when navigating\\ntradeoffs between tuning synchrono\", \"us versus asynchronous replication in relational systems.\\n\\nIf data replicas were to lose connectivit\", \"y in a \\u201chighly available\\u201d NoSQL database cluster, you could still\\ncomplete a write operation to the \", \"database. The database cluster would allow the write operation and\\nupdate each data replica as it be\", \"comes available. NoSQL databases that support multiple writable\\nreplicas can further strengthen high\", \" availability by avoiding the need for failover when optimizing\\nrecovery time objective.\\n\\nModern NoS\", \"QL databases typically implement partitioning capabilities as a feature of their system\\ndesign. Part\", \"ition management is often built-in to the database, and routing is achieved through\\nplacement hints \", \"- often called partition keys. A flexible data models enables the NoSQL databases to\\nlower the burde\", \"n of schema management and improve availability when deploying application\\nupdates that require data\", \" model changes.\\n\\nHigh availability and massive scalability are often more critical to the business t\", \"han relational table\\njoins and referential integrity. Developers can implement techniques and patter\", \"ns such as Sagas,\\nCQRS, and asynchronous messaging to embrace eventual consistency.\\n\\nNowadays, care \", \"must be taken when considering the CAP theorem constraints. A new type of\\ndatabase, called NewSQL, h\", \"as emerged which extends the relational database engine to support both\\nhorizontal scalability and t\", \"he scalable performance of NoSQL systems.\\n\\n98\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fConsideratio\", \"ns for relational vs. NoSQL systems\\n\\nBased upon specific data requirements, a cloud-native-based mic\", \"roservice can implement a relational,\\nNoSQL datastore or both.\\n\\nConsider a NoSQL datastore when:\\n\\nCo\", \"nsider a relational database when:\\n\\nYou have high volume workloads that require\\npredictable latency \", \"at large scale (for example,\\nlatency measured in milliseconds while\\nperforming millions of transacti\", \"ons per second)\\n\\nYour data is dynamic and frequently changes\\n\\nYour workload volume generally fits wi\", \"thin\\nthousands of transactions per second\\n\\nYour data is highly structured and requires\\nreferential i\", \"ntegrity\\n\\nRelationships can be de-normalized data\\nmodels\\n\\nRelationships are expressed through table \", \"joins\\non normalized data models\\n\\nData retrieval is simple and expressed without\\ntable joins\\n\\nData is\", \" typically replicated across geographies\\nand requires finer control over consistency,\\navailability, \", \"and performance\\n\\nYou work with complex queries and reports\\n\\nData is typically centralized, or can be\", \" replicated\\nregions asynchronously\\n\\nYour application will be deployed to commodity\\nhardware, such as\", \" with public clouds\\n\\nYour application will be deployed to large, high-\\nend hardware\\n\\nIn the next sec\", \"tions, we\\u2019ll explore the options available in the Azure cloud for storing and managing\\nyour cloud-na\", \"tive data.\\n\\nDatabase as a Service\\n\\nTo start, you could provision an Azure virtual machine and instal\", \"l your database of choice for each\\nservice. While you\\u2019d have full control over the environment, you\\u2019\", \"d forgo many built-in features of the\\ncloud platform. You\\u2019d also be responsible for managing the vir\", \"tual machine and database for each\\nservice. This approach could quickly become time-consuming and ex\", \"pensive.\\n\\nInstead, cloud-native applications favor data services exposed as a Database as a Service \", \"(DBaaS).\\nFully managed by a cloud vendor, these services provide built-in security, scalability, and\", \" monitoring.\\nInstead of owning the service, you simply consume it as a backing service. The provider\", \" operates the\\nresource at scale and bears the responsibility for performance and maintenance.\\n\\nThey \", \"can be configured across cloud availability zones and regions to achieve high availability. They\\nall\", \" support just-in-time capacity and a pay-as-you-go model. Azure features different kinds of\\nmanaged \", \"data service options, each with specific benefits.\\n\\nWe\\u2019ll first look at relational DBaaS services av\", \"ailable in Azure. You\\u2019ll see that Microsoft\\u2019s flagship SQL\\nServer database is available along with s\", \"everal open-source options. Then, we\\u2019ll talk about the NoSQL\\ndata services in Azure.\\n\\n99\\n\\nCHAPTER 5 \", \"| Cloud-native data patterns\\n\\n\\fAzure relational databases\\n\\nFor cloud-native microservices that requi\", \"re relational data, Azure offers four managed relational\\ndatabases as a service (DBaaS) offerings, s\", \"hown in Figure 5-11.\\n\\nFigure 5-11. Managed relational databases available in Azure\\n\\nIn the previous \", \"figure, note how each sits upon a common DBaaS infrastructure which features key\\ncapabilities at no \", \"additional cost.\\n\\nThese features are especially important to organizations who provision large numbe\", \"rs of databases,\\nbut have limited resources to administer them. You can provision an Azure database \", \"in minutes by\\nselecting the amount of processing cores, memory, and underlying storage. You can scal\", \"e the\\ndatabase on-the-fly and dynamically adjust resources with little to no downtime.\\n\\nAzure SQL Da\", \"tabase\\n\\nDevelopment teams with expertise in Microsoft SQL Server should consider Azure SQL Database.\", \" It\\u2019s a\\nfully managed relational database-as-a-service (DBaaS) based on the Microsoft SQL Server Dat\", \"abase\\nEngine. The service shares many features found in the on-premises version of SQL Server and ru\", \"ns the\\nlatest stable version of the SQL Server Database Engine.\\n\\nFor use with a cloud-native microse\", \"rvice, Azure SQL Database is available with three deployment\\noptions:\\n\\n\\u2022\\n\\n\\u2022\\n\\n100\\n\\nA Single Database \", \"represents a fully managed SQL Database running on an Azure SQL\\nDatabase server in the Azure cloud. \", \"The database is considered contained as it has no\\nconfiguration dependencies on the underlying datab\", \"ase server.\\n\\nA Managed Instance is a fully managed instance of the Microsoft SQL Server Database Eng\", \"ine\\nthat provides near-100% compatibility with an on-premises SQL Server. This option supports\\nlarge\", \"r databases, up to 35 TB and is placed in an Azure Virtual Network for better isolation.\\n\\nCHAPTER 5 \", \"| Cloud-native data patterns\\n\\n\\f\\u2022\\n\\nAzure SQL Database serverless is a compute tier for a single datab\", \"ase that automatically\\nscales based on workload demand. It bills only for the amount of compute used\", \" per second.\\nThe service is well suited for workloads with intermittent, unpredictable usage pattern\", \"s,\\ninterspersed with periods of inactivity. The serverless compute tier also automatically pauses\\nda\", \"tabases during inactive periods so that only storage charges are billed. It automatically\\nresumes wh\", \"en activity returns.\\n\\nBeyond the traditional Microsoft SQL Server stack, Azure also features managed\", \" versions of three\\npopular open-source databases.\\n\\nOpen-source databases in Azure\\n\\nOpen-source relat\", \"ional databases have become a popular choice for cloud-native applications. Many\\nenterprises favor t\", \"hem over commercial database products, especially for cost savings. Many\\ndevelopment teams enjoy the\", \"ir flexibility, community-backed development, and ecosystem of tools\\nand extensions. Open-source dat\", \"abases can be deployed across multiple cloud providers, helping\\nminimize the concern of \\u201cvendor lock\", \"-in.\\u201d\\n\\nDevelopers can easily self-host any open-source database on an Azure VM. While providing full\", \"\\ncontrol, this approach puts you on the hook for the management, monitoring, and maintenance of\\nthe \", \"database and VM.\\n\\nHowever, Microsoft continues its commitment to keeping Azure an \\u201copen platform\\u201d by\", \" offering\\nseveral popular open-source databases as fully managed DBaaS services.\\n\\nAzure Database for\", \" MySQL\\n\\nMySQL is an open-source relational database and a pillar for applications built on the LAMP \", \"software\\nstack. Widely chosen for read heavy workloads, it\\u2019s used by many large organizations, inclu\", \"ding\\nFacebook, Twitter, and YouTube. The community edition is available for free, while the enterpri\", \"se\\nedition requires a license purchase. Originally created in 1995, the product was purchased by Sun\", \"\\nMicrosystems in 2008. Oracle acquired Sun and MySQL in 2010.\\n\\nAzure Database for MySQL is a managed\", \" relational database service based on the open-source\\nMySQL Server engine. It uses the MySQL Communi\", \"ty edition. The Azure MySQL server is the\\nadministrative point for the service. It\\u2019s the same MySQL \", \"server engine used for on-premises\\ndeployments. The engine can create a single database per server o\", \"r multiple databases per server that\\nshare resources. You can continue to manage data using the same\", \" open-source tools without having\\nto learn new skills or manage virtual machines.\\n\\nAzure Database fo\", \"r MariaDB\\n\\nMariaDB Server is another popular open-source database server. It was created as a fork o\", \"f MySQL\\nwhen Oracle purchased Sun Microsystems, who owned MySQL. The intent was to ensure that Maria\", \"DB\\nremained open-source. As MariaDB is a fork of MySQL, the data and table definitions are compatibl\", \"e,\\nand the client protocols, structures, and APIs, are close-knit.\\n\\n101\\n\\nCHAPTER 5 | Cloud-native da\", \"ta patterns\\n\\n\\fMariaDB has a strong community and is used by many large enterprises. While Oracle con\", \"tinues to\\nmaintain, enhance, and support MySQL, the MariaDB foundation manages MariaDB, allowing pub\", \"lic\\ncontributions to the product and documentation.\\n\\nAzure Database for MariaDB is a fully managed r\", \"elational database as a service in the Azure cloud. The\\nservice is based on the MariaDB community ed\", \"ition server engine. It can handle mission-critical\\nworkloads with predictable performance and dynam\", \"ic scalability.\\n\\nAzure Database for PostgreSQL\\n\\nPostgreSQL is an open-source relational database wit\", \"h over 30 years of active development.\\nPostgreSQL has a strong reputation for reliability and data i\", \"ntegrity. It\\u2019s feature rich, SQL compliant,\\nand considered more performant than MySQL - especially f\", \"or workloads with complex queries and\\nheavy writes. Many large enterprises including Apple, Red Hat,\", \" and Fujitsu have built products using\\nPostgreSQL.\\n\\nAzure Database for PostgreSQL is a fully managed\", \" relational database service, based on the open-\\nsource Postgres database engine. The service suppor\", \"ts many development platforms, including C++,\\nJava, Python, Node, C#, and PHP. You can migrate Postg\", \"reSQL databases to it using the command-\\nline interface tool or Azure Data Migration Service.\\n\\nAzure\", \" Database for PostgreSQL is available with two deployment options:\\n\\n\\u2022\\n\\n\\u2022\\n\\nThe Single Server deployme\", \"nt option is a central administrative point for multiple databases\\nto which you can deploy many data\", \"bases. The pricing is structured per-server based upon\\ncores and storage.\\n\\nThe Hyperscale (Citus) op\", \"tion is powered by Citus Data technology. It enables high\\nperformance by horizontally scaling a sing\", \"le database across hundreds of nodes to deliver fast\\nperformance and scale. This option allows the e\", \"ngine to fit more data in memory, parallelize\\nqueries across hundreds of nodes, and index data faste\", \"r.\\n\\nNoSQL data in Azure\\n\\nCosmos DB is a fully managed, globally distributed NoSQL database service i\", \"n the Azure cloud. It has\\nbeen adopted by many large companies across the world, including Coca-Cola\", \", Skype, ExxonMobil,\\nand Liberty Mutual.\\n\\nIf your services require fast response from anywhere in th\", \"e world, high availability, or elastic\\nscalability, Cosmos DB is a great choice. Figure 5-12 shows C\", \"osmos DB.\\n\\n102\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fFigure 5-12: Overview of Azure Cosmos DB\\n\\nT\", \"he previous figure presents many of the built-in cloud-native capabilities available in Cosmos DB. I\", \"n\\nthis section, we\\u2019ll take a closer look at them.\\n\\nGlobal support\\n\\nCloud-native applications often h\", \"ave a global audience and require global scale.\\n\\nYou can distribute Cosmos databases across regions \", \"or around the world, placing data close to your\\nusers, improving response time, and reducing latency\", \". You can add or remove a database from a\\nregion without pausing or redeploying your services. In th\", \"e background, Cosmos DB transparently\\nreplicates the data to each of the configured regions.\\n\\nCosmos\", \" DB supports active/active clustering at the global level, enabling you to configure any of your\\ndat\", \"abase regions to support both writes and reads.\\n\\nThe Multi-region write protocol is an important fea\", \"ture in Cosmos DB that enables the following\\nfunctionality:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nUnlimited elastic write and re\", \"ad scalability.\\n\\n99.999% read and write availability all around the world.\\n\\nGuaranteed reads and wri\", \"tes served in less than 10 milliseconds at the 99th percentile.\\n\\nWith the Cosmos DB Multi-Homing API\", \"s, your microservice is automatically aware of the nearest\\nAzure region and sends requests to it. Th\", \"e nearest region is identified by Cosmos DB without any\\nconfiguration changes. Should a region becom\", \"e unavailable, the Multi-Homing feature will\\nautomatically route requests to the next nearest availa\", \"ble region.\\n\\nMulti-model support\\n\\nWhen replatforming monolithic applications to a cloud-native archi\", \"tecture, development teams\\nsometimes have to migrate open-source, NoSQL data stores. Cosmos DB can h\", \"elp you preserve your\\n\\n103\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\finvestment in these NoSQL datas\", \"tores with its multi-model data platform. The following table shows\\nthe supported NoSQL compatibilit\", \"y APIs.\\n\\nProvider\\n\\nDescription\\n\\nNoSQL API\\n\\nMongo DB API\\n\\nGremlin API\\n\\nCassandra API\\n\\nTable API\\n\\nPost\", \"greSQL API\\n\\nAPI for NoSQL stores data in document format\\n\\nSupports Mongo DB APIs and JSON documents\\n\", \"\\nSupports Gremlin API with graph-based nodes\\nand edge data representations\\n\\nSupports Casandra API fo\", \"r wide-column data\\nrepresentations\\n\\nSupports Azure Table Storage with premium\\nenhancements\\n\\nManaged \", \"service for running PostgreSQL at any\\nscale\\n\\nDevelopment teams can migrate existing Mongo, Gremlin, \", \"or Cassandra databases into Cosmos DB\\nwith minimal changes to data or code. For new apps, developmen\", \"t teams can choose among open-\\nsource options or the built-in SQL API model.\\n\\nInternally, Cosmos sto\", \"res the data in a simple struct format made up of primitive data types. For each\\nrequest, the databa\", \"se engine translates the primitive data into the model representation you\\u2019ve\\nselected.\\n\\nIn the previ\", \"ous table, note the Table API option. This API is an evolution of Azure Table Storage. Both\\nshare th\", \"e same underlying table model, but the Cosmos DB Table API adds premium enhancements\\nnot available i\", \"n the Azure Storage API. The following table contrasts the features.\\n\\nFeature\\n\\nAzure Table Storage\\n\\n\", \"Azure Cosmos DB\\n\\nLatency\\n\\nFast\\n\\nSingle-digit millisecond latency for reads and\\nwrites anywhere in th\", \"e world\\n\\nThroughp\\nut\\n\\nGlobal\\nDistributio\\nn\\n\\nIndexing\\n\\nLimit of 20,000 operations per table\\n\\nUnlimite\", \"d operations per table\\n\\nSingle region with optional single\\nsecondary read region\\n\\nTurnkey distributi\", \"ons to all regions with\\nautomatic failover\\n\\nAvailable for partition and row key\\nproperties only\\n\\nAut\", \"omatic indexing of all properties\\n\\nPricing\\n\\nOptimized for cold workloads (low\\nthroughput : storage r\", \"atio)\\n\\nOptimized for hot workloads (high\\nthroughput : storage ratio)\\n\\nMicroservices that consume Azu\", \"re Table storage can easily migrate to the Cosmos DB Table API. No\\ncode changes are required.\\n\\n104\\n\\n\", \"CHAPTER 5 | Cloud-native data patterns\\n\\n\\fTunable consistency\\n\\nEarlier in the Relational vs. NoSQL se\", \"ction, we discussed the subject of data consistency. Data\\nconsistency refers to the integrity of you\", \"r data. Cloud-native services with distributed data rely on\\nreplication and must make a fundamental \", \"tradeoff between read consistency, availability, and latency.\\n\\nMost distributed databases allow deve\", \"lopers to choose between two consistency\\nmodels: strong consistency and eventual consistency. Strong\", \" consistency is the gold standard of data\\nprogrammability. It guarantees that a query will always re\", \"turn the most current data - even if the\\nsystem must incur latency waiting for an update to replicat\", \"e across all database copies. While a\\ndatabase configured for eventual consistency will return data \", \"immediately, even if that data isn\\u2019t the\\nmost current copy. The latter option enables higher availab\", \"ility, greater scale, and increased\\nperformance.\\n\\nAzure Cosmos DB offers five well-defined consisten\", \"cy models shown in Figure 5-13.\\n\\nFigure 5-13: Cosmos DB Consistency Levels\\n\\nThese options enable you\", \" to make precise choices and granular tradeoffs for consistency, availability,\\nand the performance f\", \"or your data. The levels are presented in the following table.\\n\\nConsistency Level\\n\\nDescription\\n\\nEven\", \"tual\\n\\nConstant Prefix\\n\\nSession\\n\\nBounded Staleness\\n\\nStrong\\n\\nNo ordering guarantee for reads. Replicas\", \" will\\neventually converge.\\n\\nReads are still eventual, but data is returned in\\nthe ordering in which \", \"it is written.\\n\\nGuarantees you can read any data written\\nduring the current session. It is the defau\", \"lt\\nconsistency level.\\n\\nReads trail writes by interval that you specify.\\n\\nReads are guaranteed to ret\", \"urn most recent\\ncommitted version of an item. A client never\\nsees an uncommitted or partial read.\\n\\nI\", \"n the article Getting Behind the 9-Ball: Cosmos DB Consistency Levels Explained, Microsoft Program\\nM\", \"anager Jeremy Likness provides an excellent explanation of the five models.\\n\\nPartitioning\\n\\nAzure Cos\", \"mos DB embraces automatic partitioning to scale a database to meet the performance\\nneeds of your clo\", \"ud-native services.\\n\\nYou manage data in Cosmos DB data by creating databases, containers, and items.\", \"\\n\\n105\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fContainers live in a Cosmos DB database and represen\", \"t a schema-agnostic grouping of items. Items\\nare the data that you add to the container. They\\u2019re rep\", \"resented as documents, rows, nodes, or edges.\\nAll items added to a container are automatically index\", \"ed.\\n\\nTo partition the container, items are divided into distinct subsets called logical partitions. \", \"Logical\\npartitions are populated based on the value of a partition key that is associated with each \", \"item in a\\ncontainer. Figure 5-14 shows two containers each with a logical partition based on a parti\", \"tion key\\nvalue.\\n\\nFigure 5-14: Cosmos DB partitioning mechanics\\n\\nNote in the previous figure how each\", \" item includes a partition key of either \\u2018city\\u2019 or \\u2018airport\\u2019. The key\\ndetermines the item\\u2019s logical \", \"partition. Items with a city code are assigned to the container on the left,\\nand items with an airpo\", \"rt code, to the container on the right. Combining the partition key value with\\nthe ID value creates \", \"an item\\u2019s index, which uniquely identifies the item.\\n\\nInternally, Cosmos DB automatically manages th\", \"e placement of logical partitions on physical\\npartitions to satisfy the scalability and performance \", \"needs of the container. As application throughput\\nand storage requirements increase, Azure Cosmos DB\", \" redistributes logical partitions across a greater\\nnumber of servers. Redistribution operations are \", \"managed by Cosmos DB and invoked without\\ninterruption or downtime.\\n\\nNewSQL databases\\n\\nNewSQL is an e\", \"merging database technology that combines the distributed scalability of NoSQL with\\nthe ACID guarant\", \"ees of a relational database. NewSQL databases are important for business systems\\nthat must process \", \"high-volumes of data, across distributed environments, with full transactional\\nsupport and ACID comp\", \"liance. While a NoSQL database can provide massive scalability, it does not\\nguarantee data consisten\", \"cy. Intermittent problems from inconsistent data can place a burden on the\\ndevelopment team. Develop\", \"ers must construct safeguards into their microservice code to manage\\nproblems caused by inconsistent\", \" data.\\n\\nThe Cloud Native Computing Foundation (CNCF) features several NewSQL database projects.\\n\\n106\", \"\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fCockroach DB\\n\\nTiDB\\n\\nYugabyteDB\\n\\nVitess\\n\\nProject\\n\\nCharacte\", \"ristics\\n\\nAn ACID-compliant, relational database that\\nscales globally. Add a new node to a cluster an\", \"d\\nCockroachDB takes care of balancing the data\\nacross instances and geographies. It creates,\\nmanages\", \", and distributes replicas to ensure\\nreliability. It\\u2019s open source and freely available.\\n\\nAn open-so\", \"urce database that supports Hybrid\\nTransactional and Analytical Processing (HTAP)\\nworkloads. It is M\", \"ySQL-compatible and features\\nhorizontal scalability, strong consistency, and\\nhigh availability. TiDB\", \" acts like a MySQL server.\\nYou can continue to use existing MySQL client\\nlibraries, without requirin\", \"g extensive code\\nchanges to your application.\\n\\nAn open source, high-performance, distributed\\nSQL dat\", \"abase. It supports low query latency,\\nresilience against failures, and global data\\ndistribution. Yug\", \"abyteDB is PostgreSQL-\\ncompatible and handles scale-out RDBMS and\\ninternet-scale OLTP workloads. The\", \" product also\\nsupports NoSQL and is compatible with\\nCassandra.\\n\\nVitess is a database solution for de\", \"ploying,\\nscaling, and managing large clusters of MySQL\\ninstances. It can run in a public or private \", \"cloud\\narchitecture. Vitess combines and extends many\\nimportant MySQL features and features both\\nvert\", \"ical and horizontal sharding support.\\nOriginated by YouTube, Vitess has been serving\\nall YouTube dat\", \"abase traffic since 2011.\\n\\nThe open-source projects in the previous figure are available from the Cl\", \"oud Native Computing\\nFoundation. Three of the offerings are full database products, which include .N\", \"ET support. The other,\\nVitess, is a database clustering system that horizontally scales large cluste\", \"rs of MySQL instances.\\n\\nA key design goal for NewSQL databases is to work natively in Kubernetes, ta\", \"king advantage of the\\nplatform\\u2019s resiliency and scalability.\\n\\nNewSQL databases are designed to thriv\", \"e in ephemeral cloud environments where underlying virtual\\nmachines can be restarted or rescheduled \", \"at a moment\\u2019s notice. The databases are designed to\\nsurvive node failures without data loss nor down\", \"time. CockroachDB, for example, is able to survive a\\nmachine loss by maintaining three consistent re\", \"plicas of any data across the nodes in a cluster.\\n\\nKubernetes uses a Services construct to allow a c\", \"lient to address a group of identical NewSQL\\ndatabases processes from a single DNS entry. By decoupl\", \"ing the database instances from the address\\n\\n107\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fof the se\", \"rvice with which it\\u2019s associated, we can scale without disrupting existing application instances.\\nSe\", \"nding a request to any service at a given time will always yield the same result.\\n\\nIn this scenario,\", \" all database instances are equal. There are no primary or secondary relationships.\\nTechniques like \", \"consensus replication found in CockroachDB allow any database node to handle any\\nrequest. If the nod\", \"e that receives a load-balanced request has the data it needs locally, it responds\\nimmediately. If n\", \"ot, the node becomes a gateway and forwards the request to the appropriate nodes\\nto get the correct \", \"answer. From the client\\u2019s perspective, every database node is the same: They appear\\nas a single logi\", \"cal database with the consistency guarantees of a single-machine system, despite\\nhaving dozens or ev\", \"en hundreds of nodes that are working behind the scenes.\\n\\nFor a detailed look at the mechanics behin\", \"d NewSQL databases, see the DASH: Four Properties of\\nKubernetes-Native Databases article.\\n\\nData migr\", \"ation to the cloud\\n\\nOne of the more time-consuming tasks is migrating data from one data platform to\", \" another. The\\nAzure Data Migration Service can help expedite such efforts. It can migrate data from \", \"several external\\ndatabase sources into Azure Data platforms with minimal downtime. Target platforms \", \"include the\\nfollowing services:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure SQL Database\\n\\nAzure Database for MySQL\\n\\nAzure \", \"Database for MariaDB\\n\\nAzure Database for PostgreSQL\\n\\nAzure Cosmos DB\\n\\nThe service provides recommend\", \"ations to guide you through the changes required to execute a\\nmigration, both small or large.\\n\\nCachi\", \"ng in a cloud-native app\\n\\nThe benefits of caching are well understood. The technique works by tempor\", \"arily copying frequently\\naccessed data from a backend data store to fast storage that\\u2019s located clos\", \"er to the application.\\nCaching is often implemented where\\u2026\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nData remains relatively static.\", \"\\n\\nData access is slow, especially compared to the speed of the cache.\\n\\nData is subject to high level\", \"s of contention.\\n\\nWhy?\\n\\nAs discussed in the Microsoft caching guidance, caching can increase perform\", \"ance, scalability, and\\navailability for individual microservices and the system as a whole. It reduc\", \"es the latency and\\ncontention of handling large volumes of concurrent requests to a data store. As d\", \"ata volume and the\\nnumber of users increase, the greater the benefits of caching become.\\n\\n108\\n\\nCHAPT\", \"ER 5 | Cloud-native data patterns\\n\\n\\fCaching is most effective when a client repeatedly reads data th\", \"at is immutable or that changes\\ninfrequently. Examples include reference information such as product\", \" and pricing information, or\\nshared static resources that are costly to construct.\\n\\nWhile microservi\", \"ces should be stateless, a distributed cache can support concurrent access to session\\nstate data whe\", \"n absolutely required.\\n\\nAlso consider caching to avoid repetitive computations. If an operation tran\", \"sforms data or performs a\\ncomplicated calculation, cache the result for subsequent requests.\\n\\nCachin\", \"g architecture\\n\\nCloud native applications typically implement a distributed caching architecture. Th\", \"e cache is hosted\\nas a cloud-based backing service, separate from the microservices. Figure 5-15 sho\", \"ws the architecture.\\n\\nFigure 5-15: Caching in a cloud native app\\n\\nIn the previous figure, note how t\", \"he cache is independent of and shared by the microservices. In this\\nscenario, the cache is invoked b\", \"y the API Gateway. As discussed in chapter 4, the gateway serves as a\\nfront end for all incoming req\", \"uests. The distributed cache increases system responsiveness by\\nreturning cached data whenever possi\", \"ble. Additionally, separating the cache from the services allows\\nthe cache to scale up or out indepe\", \"ndently to meet increased traffic demands.\\n\\nThe previous figure presents a common caching pattern kn\", \"own as the cache-aside pattern. For an\\nincoming request, you first query the cache (step #1) for a r\", \"esponse. If found, the data is returned\\nimmediately. If the data doesn\\u2019t exist in the cache (known a\", \"s a cache miss), it\\u2019s retrieved from a local\\ndatabase in a downstream service (step #2). It\\u2019s then w\", \"ritten to the cache for future requests (step #3),\\nand returned to the caller. Care must be taken to\", \" periodically evict cached data so that the system\\nremains timely and consistent.\\n\\nAs a shared cache\", \" grows, it might prove beneficial to partition its data across multiple nodes. Doing\\nso can help min\", \"imize contention and improve scalability. Many Caching services support the ability to\\n\\n109\\n\\nCHAPTER\", \" 5 | Cloud-native data patterns\\n\\n\\fdynamically add and remove nodes and rebalance data across partiti\", \"ons. This approach typically\\ninvolves clustering. Clustering exposes a collection of federated nodes\", \" as a seamless, single cache.\\nInternally, however, the data is dispersed across the nodes following \", \"a predefined distribution strategy\\nthat balances the load evenly.\\n\\nAzure Cache for Redis\\n\\nAzure Cach\", \"e for Redis is a secure data caching and messaging broker service, fully managed by\\nMicrosoft. Consu\", \"med as a Platform as a Service (PaaS) offering, it provides high throughput and low-\\nlatency access \", \"to data. The service is accessible to any application within or outside of Azure.\\n\\nThe Azure Cache f\", \"or Redis service manages access to open-source Redis servers hosted across Azure\\ndata centers. The s\", \"ervice acts as a facade providing management, access control, and security. The\\nservice natively sup\", \"ports a rich set of data structures, including strings, hashes, lists, and sets. If your\\napplication\", \" already uses Redis, it will work as-is with Azure Cache for Redis.\\n\\nAzure Cache for Redis is more t\", \"han a simple cache server. It can support a number of scenarios to\\nenhance a microservices architect\", \"ure:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAn in-memory data store\\n\\nA distributed non-relational database\\n\\nA message broker\\n\\n\", \"A configuration or discovery server\\n\\nFor advanced scenarios, a copy of the cached data can be persis\", \"ted to disk. If a catastrophic event\\ndisables both the primary and replica caches, the cache is reco\", \"nstructed from the most recent\\nsnapshot.\\n\\nAzure Redis Cache is available across a number of predefin\", \"ed configurations and pricing tiers. The\\nPremium tier features many enterprise-level features such a\", \"s clustering, data persistence, geo-\\nreplication, and virtual-network isolation.\\n\\nElasticsearch in a\", \" cloud-native app\\n\\nElasticsearch is a distributed search and analytics system that enables complex s\", \"earch capabilities\\nacross diverse types of data. It\\u2019s open source and widely popular. Consider how t\", \"he following\\ncompanies integrate Elasticsearch into their application:\\n\\n\\u2022  Wikipedia for full-text a\", \"nd incremental (search as you type) searching.\\n\\n\\u2022\\n\\n\\u2022\\n\\nGitHub to index and expose over 8 million code\", \" repositories.\\n\\nDocker for making its container library discoverable.\\n\\nElasticsearch is built on top\", \" of the Apache Lucene full-text search engine. Lucene provides high-\\nperformance document indexing a\", \"nd querying. It indexes data with an inverted indexing scheme \\u2013\\ninstead of mapping pages to keywords\", \", it maps keywords to pages just like a glossary at the end of a\\nbook. Lucene has powerful query syn\", \"tax capabilities and can query data by:\\n\\n110\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\nTerm (a\", \" full word)\\n\\nPrefix (starts-with word)\\n\\n\\u2022  Wildcard (using \\u201c*\\u201d or \\u201c?\\u201d filters)\\n\\n\\u2022\\n\\n\\u2022\\n\\nPhrase (a sequ\", \"ence of text in a document)\\n\\nBoolean value (complex searches combining queries)\\n\\nWhile Lucene provid\", \"es low-level plumbing for searching, Elasticsearch provides the server that sits on\\ntop of Lucene. E\", \"lasticsearch adds higher-level functionality to simplify working Lucene, including a\\nRESTful API to \", \"access Lucene\\u2019s indexing and searching functionality. It also provides a distributed\\ninfrastructure \", \"capable of massive scalability, fault tolerance, and high availability.\\n\\nFor larger cloud-native app\", \"lications with complex search requirements, Elasticsearch is available as\\nmanaged service in Azure. \", \"The Microsoft Azure Marketplace features preconfigured templates which\\ndevelopers can use to deploy \", \"an Elasticsearch cluster on Azure.\\n\\nFrom the Microsoft Azure Marketplace, developers can use preconf\", \"igured templates built to quickly\\ndeploy an Elasticsearch cluster on Azure. Using the Azure-managed \", \"offering, you can deploy up to 50\\ndata nodes, 20 coordinating nodes, and three dedicated master node\", \"s.\\n\\nSummary\\n\\nThis chapter presented a detailed look at data in cloud-native systems. We started by c\", \"ontrasting data\\nstorage in monolithic applications with data storage patterns in cloud-native system\", \"s. We looked at\\ndata patterns implemented in cloud-native systems, including cross-service queries, \", \"distributed\\ntransactions, and patterns to deal with high-volume systems. We contrasted SQL with NoSQ\", \"L data.\\nWe looked at data storage options available in Azure that include both Microsoft-centric and\", \" open-\\nsource options. Finally, we discussed caching and Elasticsearch in a cloud-native application\", \".\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\nCommand and Query Responsibility Segregation (CQRS) pattern\\n\\nEvent Sourcing pat\", \"tern\\n\\n\\u2022  Why isn\\u2019t RDBMS Partition Tolerant in CAP Theorem and why is it Available?\\n\\n\\u2022  Materialized\", \" View\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n111\\n\\nAll you really need to know about open source databases\\n\\nCompensati\", \"ng Transaction pattern\\n\\nSaga Pattern\\n\\nSaga Patterns | How to implement business transactions using m\", \"icroservices\\n\\nCompensating Transaction pattern\\n\\nGetting Behind the 9-Ball: Cosmos DB Consistency Lev\", \"els Explained\\n\\nOn RDBMS, NoSQL and NewSQL databases. Interview with John Ryan\\n\\nCHAPTER 5 | Cloud-nat\", \"ive data patterns\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nSQL vs NoSQL vs NewSQL: The Full Comparison\\n\\nDASH: Four \", \"Properties of Kubernetes-Native Databases\\n\\nCockroachDB\\n\\nTiDB\\n\\nYugabyteDB\\n\\nVitess\\n\\nElasticsearch: The\", \" Definitive Guide\\n\\nIntroduction to Apache Lucene\\n\\n112\\n\\nCHAPTER 5 | Cloud-native data patterns\\n\\n\\fCHAP\", \"TER  6\\n\\nCloud-native resiliency\\n\\nResiliency is the ability of your system to react to failure and st\", \"ill remain functional. It\\u2019s not about\\navoiding failure, but accepting failure and constructing your \", \"cloud-native services to respond to it.\\nYou want to return to a fully functioning state quickly as p\", \"ossible.\\n\\nUnlike traditional monolithic applications, where everything runs together in a single pro\", \"cess, cloud-\\nnative systems embrace a distributed architecture as shown in Figure 6-1:\\n\\nFigure 6-1. \", \"Distributed cloud-native environment\\n\\nIn the previous figure, each microservice and cloud-based back\", \"ing service execute in a separate\\nprocess, across server infrastructure, communicating via network-b\", \"ased calls.\\n\\nOperating in this environment, a service must be sensitive to many different challenges\", \":\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n113\\n\\nUnexpected network latency - the time for a service request to travel to t\", \"he receiver and back.\\n\\nTransient faults - short-lived network connectivity errors.\\n\\nBlockage by a lo\", \"ng-running synchronous operation.\\n\\nA host process that has crashed and is being restarted or moved.\\n\", \"\\nAn overloaded microservice that can\\u2019t respond for a short time.\\n\\nAn in-flight orchestrator operatio\", \"n such as a rolling upgrade or moving a service from one\\nnode to another.\\n\\nCHAPTER 6 | Cloud-native \", \"resiliency\\n\\n\\f\\u2022\\n\\nHardware failures.\\n\\nCloud platforms can detect and mitigate many of these infrastruc\", \"ture issues. It may restart, scale out,\\nand even redistribute your service to a different node. Howe\", \"ver, to take full advantage of this built-in\\nprotection, you must design your services to react to i\", \"t and thrive in this dynamic environment.\\n\\nIn the following sections, we\\u2019ll explore defensive techni\", \"ques that your service and managed cloud\\nresources can leverage to minimize downtime and disruption.\", \"\\n\\nApplication resiliency patterns\\n\\nThe first line of defense is application resiliency.\\n\\nWhile you c\", \"ould invest considerable time writing your own resiliency framework, such products\\nalready exist. Po\", \"lly is a comprehensive .NET resilience and transient-fault-handling library that allows\\ndevelopers t\", \"o express resiliency policies in a fluent and thread-safe manner. Polly targets applications\\nbuilt w\", \"ith either .NET Framework or .NET 7. The following table describes the resiliency features, called\\np\", \"olicies, available in the Polly Library. They can be applied individually or grouped together.\\n\\nPoli\", \"cy\\n\\nExperience\\n\\nRetry\\n\\nCircuit Breaker\\n\\nTimeout\\n\\nBulkhead\\n\\nCache\\n\\nFallback\\n\\nConfigures retry operati\", \"ons on designated\\noperations.\\n\\nBlocks requested operations for a predefined\\nperiod when faults excee\", \"d a configured\\nthreshold\\n\\nPlaces limit on the duration for which a caller\\ncan wait for a response.\\n\\n\", \"Constrains actions to fixed-size resource pool to\\nprevent failing calls from swamping a resource.\\n\\nS\", \"tores responses automatically.\\n\\nDefines structured behavior upon a failure.\\n\\nNote how in the previou\", \"s figure the resiliency policies apply to request messages, whether coming\\nfrom an external client o\", \"r back-end service. The goal is to compensate the request for a service that\\nmight be momentarily un\", \"available. These short-lived interruptions typically manifest themselves with\\nthe HTTP status codes \", \"shown in the following table.\\n\\nHTTP Status Code\\n\\nCause\\n\\n404\\n\\n408\\n\\n429\\n\\n502\\n\\n503\\n\\n114\\n\\nNot Found\\n\\nReq\", \"uest timeout\\n\\nToo many requests (you\\u2019ve most likely been throttled)\\n\\nBad gateway\\n\\nService unavailabl\", \"e\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\fHTTP Status Code\\n\\nCause\\n\\n504\\n\\nGateway timeout\\n\\nQuestion: Wo\", \"uld you retry an HTTP Status Code of 403 - Forbidden? No. Here, the system is\\nfunctioning properly, \", \"but informing the caller that they aren\\u2019t authorized to perform the requested\\noperation. Care must b\", \"e taken to retry only those operations caused by failures.\\n\\nAs recommended in Chapter 1, Microsoft d\", \"evelopers constructing cloud-native applications should\\ntarget the .NET platform. Version 2.1 introd\", \"uced the HTTPClientFactory library for creating HTTP Client\\ninstances for interacting with URL-based\", \" resources. Superseding the original HTTPClient class, the\\nfactory class supports many enhanced feat\", \"ures, one of which is tight integration with the Polly\\nresiliency library. With it, you can easily d\", \"efine resiliency policies in the application Startup class to\\nhandle partial failures and connectivi\", \"ty issues.\\n\\nNext, let\\u2019s expand on retry and circuit breaker patterns.\\n\\nRetry pattern\\n\\nIn a distribut\", \"ed cloud-native environment, calls to services and cloud resources can fail because of\\ntransient (sh\", \"ort-lived) failures, which typically correct themselves after a brief period of time.\\nImplementing a\", \" retry strategy helps a cloud-native service mitigate these scenarios.\\n\\nThe Retry pattern enables a \", \"service to retry a failed request operation a (configurable) number of\\ntimes with an exponentially i\", \"ncreasing wait time. Figure 6-2 shows a retry in action.\\n\\nFigure 6-2. Retry pattern in action\\n\\nIn th\", \"e previous figure, a retry pattern has been implemented for a request operation. It\\u2019s configured to\\n\", \"allow up to four retries before failing with a backoff interval (wait time) starting at two seconds,\", \" which\\nexponentially doubles for each subsequent attempt.\\n\\n\\u2022\\n\\nThe first invocation fails and returns\", \" an HTTP status code of 500. The application waits for two\\nseconds and retries the call.\\n\\n115\\n\\nCHAPT\", \"ER 6 | Cloud-native resiliency\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nThe second invocation also fails and returns an HTTP st\", \"atus code of 500. The application now\\ndoubles the backoff interval to four seconds and retries the c\", \"all.\\n\\nFinally, the third call succeeds.\\n\\nIn this scenario, the retry operation would have attempted \", \"up to four retries while doubling\\nthe backoff duration before failing the call.\\n\\nHad the 4th retry a\", \"ttempt failed, a fallback policy would be invoked to gracefully handle the\\nproblem.\\n\\nIt\\u2019s important \", \"to increase the backoff period before retrying the call to allow the service time to self-\\ncorrect. \", \"It\\u2019s a best practice to implement an exponentially increasing backoff (doubling the period on\\neach r\", \"etry) to allow adequate correction time.\\n\\nCircuit breaker pattern\\n\\nWhile the retry pattern can help \", \"salvage a request entangled in a partial failure, there are situations\\nwhere failures can be caused \", \"by unanticipated events that will require longer periods of time to\\nresolve. These faults can range \", \"in severity from a partial loss of connectivity to the complete failure of\\na service. In these situa\", \"tions, it\\u2019s pointless for an application to continually retry an operation that is\\nunlikely to succe\", \"ed.\\n\\nTo make things worse, executing continual retry operations on a non-responsive service can move\", \"\\nyou into a self-imposed denial of service scenario where you flood your service with continual call\", \"s\\nexhausting resources such as memory, threads and database connections, causing failure in unrelate\", \"d\\nparts of the system that use the same resources.\\n\\nIn these situations, it would be preferable for \", \"the operation to fail immediately and only attempt to\\ninvoke the service if it\\u2019s likely to succeed.\\n\", \"\\nThe Circuit Breaker pattern can prevent an application from repeatedly trying to execute an operati\", \"on\\nthat\\u2019s likely to fail. After a pre-defined number of failed calls, it blocks all traffic to the s\", \"ervice.\\nPeriodically, it will allow a trial call to determine whether the fault has resolved. Figure\", \" 6-3 shows the\\nCircuit Breaker pattern in action.\\n\\n116\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\fFigure\", \" 6-3. Circuit breaker pattern in action\\n\\nIn the previous figure, a Circuit Breaker pattern has been \", \"added to the original retry pattern. Note how\\nafter 100 failed requests, the circuit breakers opens \", \"and no longer allows calls to the service. The\\nCheckCircuit value, set at 30 seconds, specifies how \", \"often the library allows one request to proceed to\\nthe service. If that call succeeds, the circuit c\", \"loses and the service is once again available to traffic.\\n\\nKeep in mind that the intent of the Circu\", \"it Breaker pattern is different than that of the Retry pattern.\\nThe Retry pattern enables an applica\", \"tion to retry an operation in the expectation that it will succeed.\\nThe Circuit Breaker pattern prev\", \"ents an application from doing an operation that is likely to fail.\\nTypically, an application will c\", \"ombine these two patterns by using the Retry pattern to invoke an\\noperation through a circuit breake\", \"r.\\n\\nTesting for resiliency\\n\\nTesting for resiliency cannot always be done the same way that you test \", \"application functionality (by\\nrunning unit tests, integration tests, and so on). Instead, you must t\", \"est how the end-to-end workload\\nperforms under failure conditions, which only occur intermittently. \", \"For example: inject failures by\\ncrashing processes, expired certificates, make dependent services un\", \"available etc. Frameworks like\\nchaos-monkey can be used for such chaos testing.\\n\\nApplication resilie\", \"ncy is a must for handling problematic requested operations. But, it\\u2019s only half of the\\nstory. Next,\", \" we cover resiliency features available in the Azure cloud.\\n\\nAzure platform resiliency\\n\\nBuilding a r\", \"eliable application in the cloud is different from traditional on-premises application\\ndevelopment. \", \"While historically you purchased higher-end hardware to scale up, in a cloud\\nenvironment you scale o\", \"ut. Instead of trying to prevent failures, the goal is to minimize their effects\\nand keep the system\", \" stable.\\n\\n117\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\fThat said, reliable cloud applications display \", \"distinct characteristics:\\n\\n\\u2022\\n\\n\\u2022\\n\\nThey\\u2019re resilient, recover gracefully from problems, and continue t\", \"o function.\\n\\nThey\\u2019re highly available (HA) and run as designed in a healthy state with no significan\", \"t\\ndowntime.\\n\\nUnderstanding how these characteristics work together - and how they affect cost - is e\", \"ssential to\\nbuilding a reliable cloud-native application. We\\u2019ll next look at ways that you can build\", \" resiliency and\\navailability into your cloud-native applications leveraging features from the Azure \", \"cloud.\\n\\nDesign with resiliency\\n\\nWe\\u2019ve said resiliency enables your application to react to failure a\", \"nd still remain functional. The\\nwhitepaper, Resilience in Azure whitepaper, provides guidance for ac\", \"hieving resilience in the Azure\\nplatform. Here are some key recommendations:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nHardwar\", \"e failure. Build redundancy into the application by deploying components across\\ndifferent fault doma\", \"ins. For example, ensure that Azure VMs are placed in different racks by\\nusing Availability Sets.\\n\\nD\", \"atacenter failure. Build redundancy into the application with fault isolation zones across\\ndatacente\", \"rs. For example, ensure that Azure VMs are placed in different fault-isolated\\ndatacenters by using A\", \"zure Availability Zones.\\n\\nRegional failure. Replicate the data and components into another region so\", \" that applications\\ncan be quickly recovered. For example, use Azure Site Recovery to replicate Azure\", \" VMs to\\nanother Azure region.\\n\\nHeavy load. Load balance across instances to handle spikes in usage. \", \"For example, put two or\\nmore Azure VMs behind a load balancer to distribute traffic to all VMs.\\n\\nAcc\", \"idental data deletion or corruption. Back up data so it can be restored if there\\u2019s any\\ndeletion or c\", \"orruption. For example, use Azure Backup to periodically back up your Azure\\nVMs.\\n\\nDesign with redund\", \"ancy\\n\\nFailures vary in scope of impact. A hardware failure, such as a failed disk, can affect a sing\", \"le node in a\\ncluster. A failed network switch could affect an entire server rack. Less common failur\", \"es, such as loss of\\npower, could disrupt a whole datacenter. Rarely, an entire region becomes unavai\", \"lable.\\n\\nRedundancy is one way to provide application resilience. The exact level of redundancy neede\", \"d\\ndepends upon your business requirements and will affect both the cost and complexity of your\\nsyste\", \"m. For example, a multi-region deployment is more expensive and more complex to manage\\nthan a single\", \"-region deployment. You\\u2019ll need operational procedures to manage failover and failback.\\nThe addition\", \"al cost and complexity might be justified for some business scenarios, but not others.\\n\\nTo architect\", \" redundancy, you need to identify the critical paths in your application, and then\\ndetermine if ther\", \"e\\u2019s redundancy at each point in the path? If a subsystem should fail, will the\\napplication fail over\", \" to something else? Finally, you need a clear understanding of those features built\\n\\n118\\n\\nCHAPTER 6 \", \"| Cloud-native resiliency\\n\\n\\finto the Azure cloud platform that you can leverage to meet your redunda\", \"ncy requirements. Here are\\nrecommendations for architecting redundancy:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nDeploy multiple\", \" instances of services. If your application depends on a single instance of a\\nservice, it creates a \", \"single point of failure. Provisioning multiple instances improves both\\nresiliency and scalability. W\", \"hen hosting in Azure Kubernetes Service, you can declaratively\\nconfigure redundant instances (replic\", \"a sets) in the Kubernetes manifest file. The replica count\\nvalue can be managed programmatically, in\", \" the portal, or through autoscaling features.\\n\\nLeveraging a load balancer. Load-balancing distribute\", \"s your application\\u2019s requests to healthy\\nservice instances and automatically removes unhealthy insta\", \"nces from rotation. When\\ndeploying to Kubernetes, load balancing can be specified in the Kubernetes \", \"manifest file in\\nthe Services section.\\n\\nPlan for multiregion deployment. If you deploy your applicat\", \"ion to a single region, and that\\nregion becomes unavailable, your application will also become unava\", \"ilable. This may be\\nunacceptable under the terms of your application\\u2019s service level agreements. Ins\", \"tead, consider\\ndeploying your application and its services across multiple regions. For example, an \", \"Azure\\nKubernetes Service (AKS) cluster is deployed to a single region. To protect your system from a\", \"\\nregional failure, you might deploy your application to multiple AKS clusters across different\\nregio\", \"ns and use the Paired Regions feature to coordinate platform updates and prioritize\\nrecovery efforts\", \".\\n\\nEnable geo-replication. Geo-replication for services such as Azure SQL Database and Cosmos\\nDB wil\", \"l create secondary replicas of your data across multiple regions. While both services will\\nautomatic\", \"ally replicate data within the same region, geo-replication protects you against a\\nregional outage b\", \"y enabling you to fail over to a secondary region. Another best practice for\\ngeo-replication centers\", \" around storing container images. To deploy a service in AKS, you\\nneed to store and pull the image f\", \"rom a repository. Azure Container Registry integrates with\\nAKS and can securely store container imag\", \"es. To improve performance and availability,\\nconsider geo-replicating your images to a registry in e\", \"ach region where you have an AKS\\ncluster. Each AKS cluster then pulls container images from the loca\", \"l container registry in its\\nregion as shown in Figure 6-4:\\n\\nFigure 6-4. Replicated resources across \", \"regions\\n\\n119\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\f\\u2022\\n\\nImplement a DNS traffic load balancer. Azure \", \"Traffic Manager provides high-availability for\\ncritical applications by load-balancing at the DNS le\", \"vel. It can route traffic to different regions\\nbased on geography, cluster response time, and even a\", \"pplication endpoint health. For\\nexample, Azure Traffic Manager can direct customers to the closest A\", \"KS cluster and\\napplication instance. If you have multiple AKS clusters in different regions, use Tra\", \"ffic\\nManager to control how traffic flows to the applications that run in each cluster. Figure 6-5\\ns\", \"hows this scenario.\\n\\nFigure 6-5. AKS and Azure Traffic Manager\\n\\nDesign for scalability\\n\\nThe cloud th\", \"rives on scaling. The ability to increase/decrease system resources to address\\nincreasing/decreasing\", \" system load is a key tenet of the Azure cloud. But, to effectively scale an\\napplication, you need a\", \"n understanding of the scaling features of each Azure service that you include\\nin your application. \", \"Here are recommendations for effectively implementing scaling in your system.\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n120\\n\\nDesign \", \"for scaling. An application must be designed for scaling. To start, services should be\\nstateless so \", \"that requests can be routed to any instance. Having stateless services also means\\nthat adding or rem\", \"oving an instance doesn\\u2019t adversely impact current users.\\n\\nPartition workloads. Decomposing domains \", \"into independent, self-contained microservices\\nenable each service to scale independently of others.\", \" Typically, services will have different\\nscalability needs and requirements. Partitioning enables yo\", \"u to scale only what needs to be\\nscaled without the unnecessary cost of scaling an entire applicatio\", \"n.\\n\\nFavor scale-out. Cloud-based applications favor scaling out resources as opposed to scaling\\nup. \", \"Scaling out (also known as horizontal scaling) involves adding more service resources to\\nan existing\", \" system to meet and share a desired level of performance. Scaling up (also known\\n\\nCHAPTER 6 | Cloud-\", \"native resiliency\\n\\n\\fas vertical scaling) involves replacing existing resources with more powerful ha\", \"rdware (more\\ndisk, memory, and processing cores). Scaling out can be invoked automatically with the\\n\", \"autoscaling features available in some Azure cloud resources. Scaling out across multiple\\nresources \", \"also adds redundancy to the overall system. Finally scaling up a single resource is\\ntypically more e\", \"xpensive than scaling out across many smaller resources. Figure 6-6 shows the\\ntwo approaches:\\n\\nFigur\", \"e 6-6. Scale up versus scale out\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nScale proportionally. When scaling a service, think in\", \" terms of resource sets. If you were to\\ndramatically scale out a specific service, what impact would\", \" that have on back-end data\\nstores, caches and dependent services? Some resources such as Cosmos DB \", \"can scale out\\nproportionally, while many others can\\u2019t. You want to ensure that you don\\u2019t scale out a\", \"\\nresource to a point where it will exhaust other associated resources.\\n\\nAvoid affinity. A best pract\", \"ice is to ensure a node doesn\\u2019t require local affinity, often referred\\nto as a sticky session. A req\", \"uest should be able to route to any instance. If you need to persist\\nstate, it should be saved to a \", \"distributed cache, such as Azure Redis cache.\\n\\nTake advantage of platform autoscaling features. Use \", \"built-in autoscaling features whenever\\npossible, rather than custom or third-party mechanisms. Where\", \" possible, use scheduled\\nscaling rules to ensure that resources are available without a startup dela\", \"y, but add reactive\\nautoscaling to the rules as appropriate, to cope with unexpected changes in dema\", \"nd. For\\nmore information, see Autoscaling guidance.\\n\\nScale out aggressively. A final practice would \", \"be to scale out aggressively so that you can\\nquickly meet immediate spikes in traffic without losing\", \" business. And, then scale in (that is,\\nremove unneeded instances) conservatively to keep the system\", \" stable. A simple way to\\nimplement this is to set the cool down period, which is the time to wait be\", \"tween scaling\\noperations, to five minutes for adding resources and up to 15 minutes for removing ins\", \"tances.\\n\\nBuilt-in retry in services\\n\\nWe encouraged the best practice of implementing programmatic re\", \"try operations in an earlier section.\\nKeep in mind that many Azure services and their corresponding \", \"client SDKs also include retry\\n\\n121\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\fmechanisms. The following\", \" list summarizes retry features in the many of the Azure services that are\\ndiscussed in this book:\\n\\n\", \"\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure Cosmos DB. The DocumentClient class from the client API automatically retire\", \"s failed\\nattempts. The number of retries and maximum wait time are configurable. Exceptions thrown\\nb\", \"y the client API are either requests that exceed the retry policy or non-transient errors.\\n\\nAzure Re\", \"dis Cache. The Redis StackExchange client uses a connection manager class that\\nincludes retries on f\", \"ailed attempts. The number of retries, specific retry policy and wait time\\nare all configurable.\\n\\nAz\", \"ure Service Bus. The Service Bus client exposes a RetryPolicy class that can be configured\\nwith a ba\", \"ck-off interval, retry count, and TerminationTimeBuffer, which specifies the maximum\\ntime an operati\", \"on can take. The default policy is nine maximum retry attempts with a 30-\\nsecond backoff period betw\", \"een attempts.\\n\\nAzure SQL Database. Retry support is provided when using the Entity Framework Core li\", \"brary.\\n\\nAzure Storage. The storage client library support retry operations. The strategies vary acro\", \"ss\\nAzure storage tables, blobs, and queues. As well, alternate retries switch between primary and\\nse\", \"condary storage services locations when the geo-redundancy feature is enabled.\\n\\nAzure Event Hubs. Th\", \"e Event Hub client library features a RetryPolicy property, which includes\\na configurable exponentia\", \"l backoff feature.\\n\\nResilient communications\\n\\nThroughout this book, we\\u2019ve embraced a microservice-ba\", \"sed architectural approach. While such an\\narchitecture provides important benefits, it presents many\", \" challenges:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nOut-of-process network communication. Each microservice communicates\", \" over a network\\nprotocol that introduces network congestion, latency, and transient faults.\\n\\nService\", \" discovery. How do microservices discover and communicate with each other when\\nrunning across a clus\", \"ter of machines with their own IP addresses and ports?\\n\\nResiliency. How do you manage short-lived fa\", \"ilures and keep the system stable?\\n\\nLoad balancing. How does inbound traffic get distributed across \", \"multiple instances of a\\nmicroservice?\\n\\nSecurity. How are security concerns such as transport-level e\", \"ncryption and certificate\\nmanagement enforced?\\n\\nDistributed Monitoring. - How do you correlate and c\", \"apture traceability and monitoring for a\\nsingle request across multiple consuming microservices?\\n\\nYo\", \"u can address these concerns with different libraries and frameworks, but the implementation can\\nbe \", \"expensive, complex, and time-consuming. You also end up with infrastructure concerns coupled to\\nbusi\", \"ness logic.\\n\\n122\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\fService mesh\\n\\nA better approach is an evolvi\", \"ng technology entitled Service Mesh. A service mesh is a configurable\\ninfrastructure layer with buil\", \"t-in capabilities to handle service communication and the other\\nchallenges mentioned above. It decou\", \"ples these concerns by moving them into a service proxy. The\\nproxy is deployed into a separate proce\", \"ss (called a sidecar) to provide isolation from business code.\\nHowever, the sidecar is linked to the\", \" service - it\\u2019s created with it and shares its lifecycle. Figure 6-7\\nshows this scenario.\\n\\nFigure 6-\", \"7. Service mesh with a side car\\n\\nIn the previous figure, note how the proxy intercepts and manages c\", \"ommunication among the\\nmicroservices and the cluster.\\n\\nA service mesh is logically split into two di\", \"sparate components: A data plane and control plane. Figure\\n6-8 shows these components and their resp\", \"onsibilities.\\n\\n123\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\fFigure 6-8. Service mesh control and data \", \"plane\\n\\nOnce configured, a service mesh is highly functional. It can retrieve a corresponding pool of\", \" instances\\nfrom a service discovery endpoint. The mesh can then send a request to a specific instanc\", \"e, recording\\nthe latency and response type of the result. A mesh can choose the instance most likely\", \" to return a\\nfast response based on many factors, including its observed latency for recent requests\", \".\\n\\nIf an instance is unresponsive or fails, the mesh will retry the request on another instance. If \", \"it returns\\nerrors, a mesh will evict the instance from the load-balancing pool and restate it after \", \"it heals. If a\\nrequest times out, a mesh can fail and then retry the request. A mesh captures and em\", \"its metrics and\\ndistributed tracing to a centralized metrics system.\\n\\nIstio and Envoy\\n\\nWhile a few s\", \"ervice mesh options currently exist, Istio is the most popular at the time of this writing.\\nIstio is\", \" a joint venture from IBM, Google, and Lyft. It\\u2019s an open-source offering that can be integrated\\nint\", \"o a new or existing distributed application. The technology provides a consistent and complete\\nsolut\", \"ion to secure, connect, and monitor microservices. Its features include:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nSecure serv\", \"ice-to-service communication in a cluster with strong identity-based\\nauthentication and authorizatio\", \"n.\\n\\nAutomatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n\\nFine-grained control of tr\", \"affic behavior with rich routing rules, retries, failovers, and fault\\ninjection.\\n\\nA pluggable policy\", \" layer and configuration API supporting access controls, rate limits, and\\nquotas.\\n\\nAutomatic metrics\", \", logs, and traces for all traffic within a cluster, including cluster ingress and\\negress.\\n\\nA key co\", \"mponent for an Istio implementation is a proxy service entitled the Envoy proxy. It runs\\nalongside e\", \"ach service and provides a platform-agnostic foundation for the following features:\\n\\nDynamic service\", \" discovery.\\n\\nLoad balancing.\\n\\n\\u2022\\n\\n\\u2022\\n\\n124\\n\\nCHAPTER 6 | Cloud-native resiliency\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nTLS te\", \"rmination.\\n\\nHTTP and gRPC proxies.\\n\\nCircuit breaker resiliency.\\n\\nHealth checks.\\n\\nRolling updates wit\", \"h canary deployments.\\n\\nAs previously discussed, Envoy is deployed as a sidecar to each microservice \", \"in the cluster.\\n\\nIntegration with Azure Kubernetes Services\\n\\nThe Azure cloud embraces Istio and prov\", \"ides direct support for it within Azure Kubernetes Services.\\nThe following links can help you get st\", \"arted:\\n\\n\\u2022\\n\\n\\u2022\\n\\nInstalling Istio in AKS\\n\\nUsing AKS and Istio\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\", \"\\n\\n\\u2022\\n\\nPolly\\n\\nRetry pattern\\n\\nCircuit Breaker pattern\\n\\nResilience in Azure whitepaper\\n\\nnetwork latency\\n\", \"\\nRedundancy\\n\\ngeo-replication\\n\\nAzure Traffic Manager\\n\\nAutoscaling guidance\\n\\nIstio\\n\\nEnvoy proxy\\n\\n125\\n\\n\", \"CHAPTER 6 | Cloud-native resiliency\\n\\n\\fCHAPTER  7\\n\\nMonitoring and health\\n\\nMicroservices and cloud-nat\", \"ive applications go hand in hand with good DevOps practices. DevOps is\\nmany things to many people bu\", \"t perhaps one of the better definitions comes from cloud advocate\\nand DevOps evangelist Donovan Brow\", \"n:\\n\\n\\u201cDevOps is the union of people, process, and products to enable continuous delivery of value to \", \"our\\nend users.\\u201d\\n\\nUnfortunately, with terse definitions, there\\u2019s always room to say more things. One \", \"of the key\\ncomponents of DevOps is ensuring that the applications running in production are function\", \"ing\\nproperly and efficiently. To gauge the health of the application in production, it\\u2019s necessary t\", \"o monitor\\nthe various logs and metrics being produced from the servers, hosts, and the application p\", \"roper. The\\nnumber of different services running in support of a cloud-native application makes monit\", \"oring the\\nhealth of individual components and the application as a whole a critical challenge.\\n\\nObse\", \"rvability patterns\\n\\nJust as patterns have been developed to aid in the layout of code in application\", \"s, there are patterns\\nfor operating applications in a reliable way. Three useful patterns in maintai\", \"ning applications have\\nemerged: logging, monitoring, and alerts.\\n\\nWhen to use logging\\n\\nNo matter how\", \" careful we are, applications almost always behave in unexpected ways in production.\\nWhen users repo\", \"rt problems with an application, it\\u2019s useful to be able to see what was going on with\\nthe app when t\", \"he problem occurred. One of the most tried and true ways of capturing information\\nabout what an appl\", \"ication is doing while it\\u2019s running is to have the application write down what it\\u2019s\\ndoing. This proc\", \"ess is known as logging. Anytime failures or problems occur in production, the goal\\nshould be to rep\", \"roduce the conditions under which the failures occurred, in a non-production\\nenvironment. Having goo\", \"d logging in place provides a roadmap for developers to follow in order to\\nduplicate problems in an \", \"environment that can be tested and experimented with.\\n\\nChallenges when logging with cloud-native app\", \"lications\\n\\nIn traditional applications, log files are typically stored on the local machine. In fact\", \", on Unix-like\\noperating systems, there\\u2019s a folder structure defined to hold any logs, typically und\", \"er /var/log.\\n\\n126\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fFigure 7-1. Logging to a file in a monolithic\", \" app.\\n\\nThe usefulness of logging to a flat file on a single machine is vastly reduced in a cloud env\", \"ironment.\\nApplications producing logs may not have access to the local disk or the local disk may be\", \" highly\\ntransient as containers are shuffled around physical machines. Even simple scaling up of mon\", \"olithic\\napplications across multiple nodes can make it challenging to locate the appropriate file-ba\", \"sed log\\nfile.\\n\\nFigure 7-2. Logging to files in a scaled monolithic app.\\n\\nCloud-native applications d\", \"eveloped using a microservices architecture also pose some challenges for\\nfile-based loggers. User r\", \"equests may now span multiple services that are run on different machines\\n\\n127\\n\\nCHAPTER 7 | Monitori\", \"ng and health\\n\\n\\fand may include serverless functions with no access to a local file system at all. I\", \"t would be very\\nchallenging to correlate the logs from a user or a session across these many service\", \"s and machines.\\n\\nFigure 7-3. Logging to local files in a microservices app.\\n\\nFinally, the number of \", \"users in some cloud-native applications is high. Imagine that each user\\ngenerates a hundred lines of\", \" log messages when they log into an application. In isolation, that is\\nmanageable, but multiply that\", \" over 100,000 users and the volume of logs becomes large enough that\\nspecialized tools are needed to\", \" support effective use of the logs.\\n\\nLogging in cloud-native applications\\n\\nEvery programming languag\", \"e has tooling that permits writing logs, and typically the overhead for\\nwriting these logs is low. M\", \"any of the logging libraries provide logging different kinds of criticalities,\\nwhich can be tuned at\", \" run time. For instance, the Serilog library is a popular structured logging library\\nfor .NET that p\", \"rovides the following logging levels:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nVerbose\\n\\nDebug\\n\\nInformation\\n\\n\\u2022  Warning\\n\\n\\u2022\\n\\n\\u2022\\n\\nError\", \"\\n\\nFatal\\n\\nThese different log levels provide granularity in logging. When the application is function\", \"ing properly\\nin production, it may be configured to only log important messages. When the applicatio\", \"n is\\n\\n128\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fmisbehaving, then the log level can be increased so m\", \"ore verbose logs are gathered. This balances\\nperformance against ease of debugging.\\n\\nThe high perfor\", \"mance of logging tools and the tunability of verbosity should encourage developers to\\nlog frequently\", \". Many favor a pattern of logging the entry and exit of each method. This approach may\\nsound like ov\", \"erkill, but it\\u2019s infrequent that developers will wish for less logging. In fact, it\\u2019s not\\nuncommon t\", \"o perform deployments for the sole purpose of adding logging around a problematic\\nmethod. Err on the\", \" side of too much logging and not on too little. Some tools can be used to\\nautomatically provide thi\", \"s kind of logging.\\n\\nBecause of the challenges associated with using file-based logs in cloud-native \", \"apps, centralized logs\\nare preferred. Logs are collected by the applications and shipped to a centra\", \"l logging application\\nwhich indexes and stores the logs. This class of system can ingest tens of gig\", \"abytes of logs every day.\\n\\nIt\\u2019s also helpful to follow some standard practices when building logging\", \" that spans many services.\\nFor instance, generating a correlation ID at the start of a lengthy inter\", \"action, and then logging it in\\neach message that is related to that interaction, makes it easier to \", \"search for all related messages. One\\nneed only find a single message and extract the correlation ID \", \"to find all the related messages.\\nAnother example is ensuring that the log format is the same for ev\", \"ery service, whatever the language\\nor logging library it uses. This standardization makes reading lo\", \"gs much easier. Figure 7-4\\ndemonstrates how a microservices architecture can leverage centralized lo\", \"gging as part of its\\nworkflow.\\n\\nFigure 7-4. Logs from various sources are ingested into a centralize\", \"d log store.\\n\\n129\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fChallenges with detecting and responding to p\", \"otential app health\\nissues\\n\\nSome applications aren\\u2019t mission critical. Maybe they\\u2019re only used inter\", \"nally, and when a problem\\noccurs, the user can contact the team responsible and the application can \", \"be restarted. However,\\ncustomers often have higher expectations for the applications they consume. Y\", \"ou should know when\\nproblems occur with your application before users do, or before users notify you\", \". Otherwise, the first\\nyou know about a problem may be when you notice an angry deluge of social med\", \"ia posts deriding\\nyour application or even your organization.\\n\\nSome scenarios you may need to consid\", \"er include:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nOne service in your application keeps failing and restarting, resulting in int\", \"ermittent slow\\nresponses.\\n\\nAt some times of the day, your application\\u2019s response time is slow.\\n\\nAfte\", \"r a recent deployment, load on the database has tripled.\\n\\nImplemented properly, monitoring can let y\", \"ou know about conditions that will lead to problems,\\nletting you address underlying conditions befor\", \"e they result in any significant user impact.\\n\\nMonitoring cloud-native apps\\n\\nSome centralized loggin\", \"g systems take on an additional role of collecting telemetry outside of pure\\nlogs. They can collect \", \"metrics, such as time to run a database query, average response time from a\\nweb server, and even CPU\", \" load averages and memory pressure as reported by the operating system.\\nIn conjunction with the logs\", \", these systems can provide a holistic view of the health of nodes in the\\nsystem and the application\", \" as a whole.\\n\\nThe metric-gathering capabilities of the monitoring tools can also be fed manually fro\", \"m within the\\napplication. Business flows that are of particular interest such as new users signing u\", \"p or orders being\\nplaced, may be instrumented such that they increment a counter in the central moni\", \"toring system.\\nThis aspect unlocks the monitoring tools to not only monitor the health of the applic\", \"ation but the\\nhealth of the business.\\n\\nQueries can be constructed in the log aggregation tools to lo\", \"ok for certain statistics or patterns, which\\ncan then be displayed in graphical form, on custom dash\", \"boards. Frequently, teams will invest in large,\\nwall-mounted displays that rotate through the statis\", \"tics related to an application. This way, it\\u2019s simple\\nto see the problems as they occur.\\n\\nCloud-nati\", \"ve monitoring tools provide real-time telemetry and insight into apps regardless of whether\\nthey\\u2019re \", \"single-process monolithic applications or distributed microservice architectures. They include\\ntools\", \" that allow collection of data from the app as well as tools for querying and displaying\\ninformation\", \" about the app\\u2019s health.\\n\\nChallenges with reacting to critical problems in cloud-native apps\\n\\nIf you\", \" need to react to problems with your application, you need some way to alert the right\\npersonnel. Th\", \"is is the third cloud-native application observability pattern and depends on logging and\\nmonitoring\", \". Your application needs to have logging in place to allow problems to be diagnosed, and\\n\\n130\\n\\nCHAPT\", \"ER 7 | Monitoring and health\\n\\n\\fin some cases to feed into monitoring tools. It needs monitoring to a\", \"ggregate application metrics and\\nhealth data in one place. Once this has been established, rules can\", \" be created that will trigger alerts\\nwhen certain metrics fall outside of acceptable levels.\\n\\nGenera\", \"lly, alerts are layered on top of monitoring such that certain conditions trigger appropriate\\nalerts\", \" to notify team members of urgent problems. Some scenarios that may require alerts include:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\", \"\\n\\nOne of your application\\u2019s services is not responding after 1 minute of downtime.\\n\\nYour application\", \" is returning unsuccessful HTTP responses to more than 1% of requests.\\n\\nYour application\\u2019s average r\", \"esponse time for key endpoints exceeds 2000 ms.\\n\\nAlerts in cloud-native apps\\n\\nYou can craft queries \", \"against the monitoring tools to look for known failure conditions. For instance,\\nqueries could searc\", \"h through the incoming logs for indications of HTTP status code 500, which\\nindicates a problem on a \", \"web server. As soon as one of these is detected, then an e-mail or an SMS\\ncould be sent to the owner\", \" of the originating service who can begin to investigate.\\n\\nTypically, though, a single 500 error isn\", \"\\u2019t enough to determine that a problem has occurred. It could\\nmean that a user mistyped their passwor\", \"d or entered some malformed data. The alert queries can be\\ncrafted to only fire when a larger than a\", \"verage number of 500 errors are detected.\\n\\nOne of the most damaging patterns in alerting is to fire \", \"too many alerts for humans to investigate.\\nService owners will rapidly become desensitized to errors\", \" that they\\u2019ve previously investigated and\\nfound to be benign. Then, when true errors occur, they\\u2019ll \", \"be lost in the noise of hundreds of false\\npositives. The parable of the Boy Who Cried Wolf is freque\", \"ntly told to children to warn them of this\\nvery danger. It\\u2019s important to ensure that the alerts tha\", \"t do fire are indicative of a real problem.\\n\\nLogging with Elastic Stack\\n\\nThere are many good central\", \"ized logging tools and they vary in cost from being free, open-source\\ntools, to more expensive optio\", \"ns. In many cases, the free tools are as good as or better than the paid\\nofferings. One such tool is\", \" a combination of three open-source components: Elasticsearch, Logstash,\\nand Kibana.\\n\\nCollectively t\", \"hese tools are known as the Elastic Stack or ELK stack.\\n\\nElastic Stack\\n\\nThe Elastic Stack is a power\", \"ful option for gathering information from a Kubernetes cluster. Kubernetes\\nsupports sending logs to \", \"an Elasticsearch endpoint, and for the most part, all you need to get started\\nis to set the environm\", \"ent variables as shown in Figure 7-5:\\n\\nKUBE_LOGGING_DESTINATION=elasticsearch\\nKUBE_ENABLE_NODE_LOGGI\", \"NG=true\\n\\nFigure 7-5. Configuration variables for Kubernetes\\n\\nThis step will install Elasticsearch on\", \" the cluster and target sending all the cluster logs to it.\\n\\n131\\n\\nCHAPTER 7 | Monitoring and health\\n\", \"\\n\\fFigure 7-6. An example of a Kibana dashboard showing the results of a query against logs that are \", \"ingested from\\nKubernetes\\n\\nWhat are the advantages of Elastic Stack?\\n\\nElastic Stack provides centrali\", \"zed logging in a low-cost, scalable, cloud-friendly manner. Its user\\ninterface streamlines data anal\", \"ysis so you can spend your time gleaning insights from your data\\ninstead of fighting with a clunky i\", \"nterface. It supports a wide variety of inputs so as your distributed\\napplication spans more and dif\", \"ferent kinds of services, you can expect to continue to be able to feed\\nlog and metric data into the\", \" system. The Elastic Stack also supports fast searches even across large\\ndata sets, making it possib\", \"le even for large applications to log detailed data and still be able to have\\nvisibility into it in \", \"a performant fashion.\\n\\nLogstash\\n\\nThe first component is Logstash. This tool is used to gather log in\", \"formation from a large variety of\\ndifferent sources. For instance, Logstash can read logs from disk \", \"and also receive messages from\\nlogging libraries like Serilog. Logstash can do some basic filtering \", \"and expansion on the logs as they\\narrive. For instance, if your logs contain IP addresses then Logst\", \"ash may be configured to do a\\ngeographical lookup and obtain a country/region or even city of origin\", \" for that message.\\n\\nSerilog is a logging library for .NET languages, which allows for parameterized \", \"logging. Instead of\\ngenerating a textual log message that embeds fields, parameters are kept separat\", \"e. This library allows\\nfor more intelligent filtering and searching. A sample Serilog configuration \", \"for writing to Logstash\\nappears in Figure 7-7.\\n\\nvar log = new LoggerConfiguration()\\n         .WriteT\", \"o.Http(\\\"http://localhost:8080\\\")\\n         .CreateLogger();\\n\\nFigure 7-7. Serilog config for writing lo\", \"g information directly to logstash over HTTP\\n\\nLogstash would use a configuration like the one shown \", \"in Figure 7-8.\\n\\n132\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\finput {\\n    http {\\n        #default host 0.\", \"0.0.0:8080\\n        codec => json\\n    }\\n}\\n\\noutput {\\n    elasticsearch {\\n        hosts => \\\"elasticsear\", \"ch:9200\\\"\\n        index=>\\\"sales-%{+xxxx.ww}\\\"\\n    }\\n}\\n\\nFigure 7-8. A Logstash configuration for consum\", \"ing logs from Serilog\\n\\nFor scenarios where extensive log manipulation isn\\u2019t needed there\\u2019s an altern\", \"ative to Logstash known\\nas Beats. Beats is a family of tools that can gather a wide variety of data \", \"from logs to network data\\nand uptime information. Many applications will use both Logstash and Beats\", \".\\n\\nOnce the logs have been gathered by Logstash, it needs somewhere to put them. While Logstash\\nsupp\", \"orts many different outputs, one of the more exciting ones is Elasticsearch.\\n\\nElasticsearch\\n\\nElastic\", \"search is a powerful search engine that can index logs as they arrive. It makes running queries\\nagai\", \"nst the logs quick. Elasticsearch can handle huge quantities of logs and, in extreme cases, can be\\ns\", \"caled out across many nodes.\\n\\nLog messages that have been crafted to contain parameters or that have\", \" had parameters split from\\nthem through Logstash processing, can be queried directly as Elasticsearc\", \"h preserves this information.\\n\\nA query that searches for the top 10 pages visited by jill@example.co\", \"m, appears in Figure 7-9.\\n\\n\\\"query\\\": {\\n    \\\"match\\\": {\\n      \\\"user\\\": \\\"jill@example.com\\\"\\n    }\\n  },\\n  \\\"\", \"aggregations\\\": {\\n    \\\"top_10_pages\\\": {\\n      \\\"terms\\\": {\\n        \\\"field\\\": \\\"page\\\",\\n        \\\"size\\\": 10\\n\", \"      }\\n    }\\n  }\\n\\nFigure 7-9. An Elasticsearch query for finding top 10 pages visited by a user\\n\\nVi\", \"sualizing information with Kibana web dashboards\\n\\nThe final component of the stack is Kibana. This t\", \"ool is used to provide interactive visualizations in a\\nweb dashboard. Dashboards may be crafted even\", \" by users who are non-technical. Most data that is\\nresident in the Elasticsearch index, can be inclu\", \"ded in the Kibana dashboards. Individual users may\\n\\n133\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fhave di\", \"fferent dashboard desires and Kibana enables this customization through allowing user-\\nspecific dash\", \"boards.\\n\\nInstalling Elastic Stack on Azure\\n\\nThe Elastic stack can be installed on Azure in many ways\", \". As always, it\\u2019s possible to provision virtual\\nmachines and install Elastic Stack on them directly.\", \" This option is preferred by some experienced users\\nas it offers the highest degree of customizabili\", \"ty. Deploying on infrastructure as a service introduces\\nsignificant management overhead forcing thos\", \"e who take that path to take ownership of all the tasks\\nassociated with infrastructure as a service \", \"such as securing the machines and keeping up-to-date with\\npatches.\\n\\nAn option with less overhead is \", \"to make use of one of the many Docker containers on which the\\nElastic Stack has already been configu\", \"red. These containers can be dropped into an existing\\nKubernetes cluster and run alongside applicati\", \"on code. The sebp/elk container is a well-documented\\nand tested Elastic Stack container.\\n\\nAnother op\", \"tion is a recently announced ELK-as-a-service offering.\\n\\nReferences\\n\\n\\u2022\\n\\nInstall Elastic Stack on Azu\", \"re\\n\\nMonitoring in Azure Kubernetes Services\\n\\nThe built-in logging in Kubernetes is primitive. Howeve\", \"r, there are some great options for getting the\\nlogs out of Kubernetes and into a place where they c\", \"an be properly analyzed. If you need to monitor\\nyour AKS clusters, configuring Elastic Stack for Kub\", \"ernetes is a great solution.\\n\\nAzure Monitor for Containers\\n\\nAzure Monitor for Containers supports co\", \"nsuming logs from not just Kubernetes but also from other\\norchestration engines such as DC/OS, Docke\", \"r Swarm, and Red Hat OpenShift.\\n\\n134\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fFigure 7-10. Consuming log\", \"s from various containers\\n\\nPrometheus is a popular open source metric monitoring solution. It is par\", \"t of the Cloud Native\\nCompute Foundation. Typically, using Prometheus requires managing a Prometheus\", \" server with its\\nown store. However, Azure Monitor for Containers provides direct integration with P\", \"rometheus\\nmetrics endpoints, so a separate server is not required.\\n\\nLog and metric information is ga\", \"thered not just from the containers running in the cluster but also\\nfrom the cluster hosts themselve\", \"s. It allows correlating log information from the two making it much\\neasier to track down an error.\\n\", \"\\nInstalling the log collectors differs on Windows and Linux clusters. But in both cases the log coll\", \"ection\\nis implemented as a Kubernetes DaemonSet, meaning that the log collector is run as a containe\", \"r on\\neach of the nodes.\\n\\nNo matter which orchestrator or operating system is running the Azure Monit\", \"or daemon, the log\\ninformation is forwarded to the same Azure Monitor tools with which users are fam\", \"iliar. This approach\\nensures a parallel experience in environments that mix different log sources su\", \"ch as a hybrid\\nKubernetes/Azure Functions environment.\\n\\n135\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fFig\", \"ure 7-11. A sample dashboard showing logging and metric information from many running containers.\\n\\nL\", \"og.Finalize()\\n\\nLogging is one of the most overlooked and yet most important parts of deploying any a\", \"pplication at\\nscale. As the size and complexity of applications increase, then so does the difficult\", \"y of debugging\\nthem. Having top quality logs available makes debugging much easier and moves it from\", \" the realm of\\n\\u201cnearly impossible\\u201d to \\u201ca pleasant experience\\u201d.\\n\\nAzure Monitor\\n\\nNo other cloud provide\", \"r has as mature of a cloud application monitoring solution than that found in\\nAzure. Azure Monitor i\", \"s an umbrella name for a collection of tools designed to provide visibility into\\nthe state of your s\", \"ystem. It helps you understand how your cloud-native services are performing and\\nproactively identif\", \"ies issues affecting them. Figure 7-12 presents a high level of view of Azure Monitor.\\n\\n136\\n\\nCHAPTER\", \" 7 | Monitoring and health\\n\\n\\fFigure 7-12. High-level view of Azure Monitor.\\n\\nGathering logs and metr\", \"ics\\n\\nThe first step in any monitoring solution is to gather as much data as possible. The more data\\n\", \"gathered, the deeper the insights. Instrumenting systems has traditionally been difficult. Simple\\nNe\", \"twork Management Protocol (SNMP) was the gold standard protocol for collecting machine level\\ninforma\", \"tion, but it required a great deal of knowledge and configuration. Fortunately, much of this\\nhard wo\", \"rk has been eliminated as the most common metrics are gathered automatically by Azure\\nMonitor.\\n\\nAppl\", \"ication level metrics and events aren\\u2019t possible to instrument automatically because they\\u2019re\\nspecifi\", \"c to the application being deployed. In order to gather these metrics, there are SDKs and APIs\\navail\", \"able to directly report such information, such as when a customer signs up or completes an order.\\nEx\", \"ceptions can also be captured and reported back into Azure Monitor via Application Insights. The\\nSDK\", \"s support most every language found in Cloud Native Applications including Go, Python,\\nJavaScript, a\", \"nd the .NET languages.\\n\\nThe ultimate goal of gathering information about the state of your applicati\", \"on is to ensure that your\\nend users have a good experience. What better way to tell if users are exp\", \"eriencing issues than doing\\noutside-in web tests? These tests can be as simple as pinging your websi\", \"te from locations around the\\nworld or as involved as having agents log into the site and simulate us\", \"er actions.\\n\\nReporting data\\n\\nOnce the data is gathered, it can be manipulated, summarized, and plott\", \"ed into charts, which allow\\nusers to instantly see when there are problems. These charts can be gath\", \"ered into dashboards or into\\nWorkbooks, a multi-page report designed to tell a story about some aspe\", \"ct of the system.\\n\\n137\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fNo modern application would be complete \", \"without some artificial intelligence or machine learning. To\\nthis end, data can be passed to the var\", \"ious machine learning tools in Azure to allow you to extract\\ntrends and information that would other\", \"wise be hidden.\\n\\nApplication Insights provides a powerful (SQL-like) query language called Kusto tha\", \"t can query\\nrecords, summarize them, and even plot charts. For example, the following query will loc\", \"ate all records\\nfor the month of November 2007, group them by state, and plot the top 10 as a pie ch\", \"art.\\n\\nStormEvents\\n| where StartTime >= datetime(2007-11-01) and StartTime < datetime(2007-12-01)\\n| s\", \"ummarize count() by State\\n| top 10 by count_\\n| render piechart\\n\\nFigure 7-13 shows the results of thi\", \"s Application Insights Query.\\n\\nFigure 7-13. Application Insights query results.\\n\\nThere is a playgrou\", \"nd for experimenting with Kusto queries. Reading sample queries can also be\\ninstructive.\\n\\nDashboards\", \"\\n\\nThere are several different dashboard technologies that may be used to surface the information fro\", \"m\\nAzure Monitor. Perhaps the simplest is to just run queries in Application Insights and plot the da\", \"ta\\ninto a chart.\\n\\n138\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fFigure 7-14. An example of Application In\", \"sights charts embedded in the main Azure Dashboard.\\n\\nThese charts can then be embedded in the Azure \", \"portal proper through use of the dashboard feature.\\nFor users with more exacting requirements, such \", \"as being able to drill down into several tiers of data,\\nAzure Monitor data is available to Power BI.\", \" Power BI is an industry-leading, enterprise class, business\\nintelligence tool that can aggregate da\", \"ta from many different data sources.\\n\\n139\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fFigure 7-15. An examp\", \"le Power BI dashboard.\\n\\nAlerts\\n\\nSometimes, having data dashboards is insufficient. If nobody is awak\", \"e to watch the dashboards, then\\nit can still be many hours before a problem is addressed, or even de\", \"tected. To this end, Azure Monitor\\nalso provides a top notch alerting solution. Alerts can be trigge\", \"red by a wide range of conditions\\nincluding:\\n\\n\\u2022  Metric values\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nLog search queries\\n\\nActi\", \"vity Log events\\n\\nHealth of the underlying Azure platform\\n\\nTests for web site availability\\n\\nWhen trig\", \"gered, the alerts can perform a wide variety of tasks. On the simple side, the alerts may just\\nsend \", \"an e-mail notification to a mailing list or a text message to an individual. More involved alerts\\n\\n1\", \"40\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fmight trigger a workflow in a tool such as PagerDuty, which \", \"is aware of who is on call for a particular\\napplication. Alerts can trigger actions in Microsoft Flo\", \"w unlocking near limitless possibilities for\\nworkflows.\\n\\nAs common causes of alerts are identified, \", \"the alerts can be enhanced with details about the common\\ncauses of the alerts and the steps to take \", \"to resolve them. Highly mature cloud-native application\\ndeployments may opt to kick off self-healing\", \" tasks, which perform actions such as removing failing\\nnodes from a scale set or triggering an autos\", \"caling activity. Eventually it may no longer be necessary\\nto wake up on-call personnel at 2AM to res\", \"olve a live-site issue as the system will be able to adjust\\nitself to compensate or at least limp al\", \"ong until somebody arrives at work the next morning.\\n\\nAzure Monitor automatically leverages machine \", \"learning to understand the normal operating\\nparameters of deployed applications. This approach enabl\", \"es it to detect services that are operating\\noutside of their normal parameters. For instance, the ty\", \"pical weekday traffic on the site might be\\n10,000 requests per minute. And then, on a given week, su\", \"ddenly the number of requests hits a highly\\nunusual 20,000 requests per minute. Smart Detection will\", \" notice this deviation from the norm and\\ntrigger an alert. At the same time, the trend analysis is s\", \"mart enough to avoid firing false positives\\nwhen the traffic load is expected.\\n\\nReferences\\n\\n\\u2022\\n\\nAzure\", \" Monitor\\n\\n141\\n\\nCHAPTER 7 | Monitoring and health\\n\\n\\fCHAPTER  8\\n\\nCloud-native identity\\n\\nMost software \", \"applications need to have some knowledge of the user or process that is calling them.\\nThe user or pr\", \"ocess interacting with an application is known as a security principal, and the process of\\nauthentic\", \"ating and authorizing these principals is known as identity management, or simply identity.\\nSimple a\", \"pplications may include all of their identity management within the application, but this\\napproach d\", \"oesn\\u2019t scale well with many applications and many kinds of security principals. Windows\\nsupports the\", \" use of Active Directory to provide centralized authentication and authorization.\\n\\nWhile this soluti\", \"on is effective within corporate networks, it isn\\u2019t designed for use by users or\\napplications that a\", \"re outside of the AD domain. With the growth of Internet-based applications and\\nthe rise of cloud-na\", \"tive apps, security models have evolved.\\n\\nIn today\\u2019s cloud-native identity model, architecture is as\", \"sumed to be distributed. Apps can be\\ndeployed anywhere and may communicate with other apps anywhere.\", \" Clients may communicate with\\nthese apps from anywhere, and in fact, clients may consist of any comb\", \"ination of platforms and\\ndevices. Cloud-native identity solutions use open standards to achieve secu\", \"re application access from\\nclients. These clients range from human users on PCs or phones, to other \", \"apps hosted anywhere\\nonline, to set-top boxes and IOT devices running any software platform anywhere\", \" in the world.\\n\\nModern cloud-native identity solutions typically use access tokens that are issued b\", \"y a secure token\\nservice/server (STS) to a security principal once their identity is determined. The\", \" access token, typically\\na JSON Web Token (JWT), includes claims about the security principal. These\", \" claims will minimally\\ninclude the user\\u2019s identity but may also include other claims that can be use\", \"d by applications to\\ndetermine the level of access to grant the principal.\\n\\nTypically, the STS is on\", \"ly responsible for authenticating the principal. Determining their level of access\\nto resources is l\", \"eft to other parts of the application.\\n\\nReferences\\n\\n\\u2022  Microsoft identity platform\\n\\nAuthentication a\", \"nd authorization in cloud-native\\napps\\n\\nAuthentication is the process of determining the identity of \", \"a security principal. Authorization is the act\\nof granting an authenticated principal permission to \", \"perform an action or access a resource.\\nSometimes authentication is shortened to AuthN and authoriza\", \"tion is shortened to AuthZ. Cloud-\\n\\n142\\n\\nCHAPTER 8 | Cloud-native identity\\n\\n\\fnative applications nee\", \"d to rely on open HTTP-based protocols to authenticate security principals\\nsince both clients and ap\", \"plications could be running anywhere in the world on any platform or device.\\nThe only common factor \", \"is HTTP.\\n\\nMany organizations still rely on local authentication services like Active Directory Feder\", \"ation Services\\n(ADFS). While this approach has traditionally served organizations well for on premis\", \"es authentication\\nneeds, cloud-native applications benefit from systems designed specifically for th\", \"e cloud. A recent\\n2019 United Kingdom National Cyber Security Centre (NCSC) advisory states that \\u201cor\", \"ganizations using\\nAzure AD as their primary authentication source will actually lower their risk com\", \"pared to ADFS.\\u201d\\nSome reasons outlined in this analysis include:\\n\\n\\u2022\\n\\nAccess to full set of Microsoft \", \"credential protection technologies.\\n\\n\\u2022  Most organizations are already relying on Azure AD to some e\", \"xtent.\\n\\n\\u2022\\n\\nDouble hashing of NTLM hashes ensures compromise won\\u2019t allow credentials that work in\\nloc\", \"al Active Directory.\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAuthentication basics\\n\\nAccess tokens and claims\\n\\nIt may b\", \"e time to ditch your on premises authentication services\\n\\nAzure Active Directory\\n\\nMicrosoft Azure Ac\", \"tive Directory (Azure AD) offers identity and access management as a service.\\nCustomers use it to co\", \"nfigure and maintain who users are, what information to store about them, who\\ncan access that inform\", \"ation, who can manage it, and what apps can access it. AAD can authenticate\\nusers for applications c\", \"onfigured to use it, providing a single sign-on (SSO) experience. It can be used\\non its own or be in\", \"tegrated with Windows AD running on premises.\\n\\nAzure AD is built for the cloud. It\\u2019s truly a cloud-n\", \"ative identity solution that uses a REST-based Graph\\nAPI and OData syntax for queries, unlike Window\", \"s AD, which uses LDAP. On premises Active Directory\\ncan sync user attributes to the cloud using Iden\", \"tity Sync Services, allowing all authentication to take\\nplace in the cloud using Azure AD. Alternate\", \"ly, authentication can be configured via Connect to pass\\nback to local Active Directory via ADFS to \", \"be completed by Windows AD on premises.\\n\\nAzure AD supports company branded sign-in screens, multi-fa\", \"ctory authentication, and cloud-based\\napplication proxies that are used to provide SSO for applicati\", \"ons hosted on premises. It offers\\ndifferent kinds of security reporting and alert capabilities.\\n\\nRef\", \"erences\\n\\n\\u2022  Microsoft identity platform\\n\\n143\\n\\nCHAPTER 8 | Cloud-native identity\\n\\n\\fIdentityServer for\", \" cloud-native applications\\n\\nIdentityServer is an authentication server that implements OpenID Connec\", \"t (OIDC) and OAuth 2.0\\nstandards for ASP.NET Core. It\\u2019s designed to provide a common way to authenti\", \"cate requests to all of\\nyour applications, whether they\\u2019re web, native, mobile, or API endpoints. Id\", \"entityServer can be used to\\nimplement Single Sign-On (SSO) for multiple applications and application\", \" types. It can be used to\\nauthenticate actual users via sign-in forms and similar user interfaces as\", \" well as service-based\\nauthentication that typically involves token issuance, verification, and rene\", \"wal without any user\\ninterface. IdentityServer is designed to be a customizable solution. Each insta\", \"nce is typically\\ncustomized to suit an individual organization and/or set of applications\\u2019 needs.\\n\\nC\", \"ommon web app scenarios\\n\\nTypically, applications need to support some or all of the following scenar\", \"ios:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nHuman users accessing web applications with a browser.\\n\\nHuman users accessing b\", \"ack-end Web APIs from browser-based apps.\\n\\nHuman users on mobile/native clients accessing back-end W\", \"eb APIs.\\n\\nOther applications accessing back-end Web APIs (without an active user or user interface).\", \"\\n\\nAny application may need to interact with other Web APIs, using its own identity or\\ndelegating to \", \"the user\\u2019s identity.\\n\\nFigure 8-1. Application types and scenarios.\\n\\nIn each of these scenarios, the \", \"exposed functionality needs to be secured against unauthorized use. At\\na minimum, this typically req\", \"uires authenticating the user or principal making a request for a resource.\\nThis authentication may \", \"use one of several common protocols such as SAML2p, WS-Fed, or OpenID\\nConnect. Communicating with AP\", \"Is typically uses the OAuth2 protocol and its support for security\\ntokens. Separating these critical\", \" cross-cutting security concerns and their implementation details from\\nthe applications themselves e\", \"nsures consistency and improves security and maintainability.\\n\\n144\\n\\nCHAPTER 8 | Cloud-native identit\", \"y\\n\\n\\fOutsourcing these concerns to a dedicated product like IdentityServer helps the requirement for \", \"every\\napplication to solve these problems itself.\\n\\nIdentityServer provides middleware that runs with\", \"in an ASP.NET Core application and adds support\\nfor OpenID Connect and OAuth2 (see supported specifi\", \"cations). Organizations would create their own\\nASP.NET Core app using IdentityServer middleware to a\", \"ct as the STS for all of their token-based\\nsecurity protocols. The IdentityServer middleware exposes\", \" endpoints to support standard\\nfunctionality, including:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAuthorize (authent\", \"icate the end user)\\n\\nToken (request a token programmatically)\\n\\nDiscovery (metadata about the server)\", \"\\n\\nUser Info (get user information with a valid access token)\\n\\nDevice Authorization (used to start de\", \"vice flow authorization)\\n\\nIntrospection (token validation)\\n\\nRevocation (token revocation)\\n\\nEnd Sessi\", \"on (trigger single sign-out across all apps)\\n\\nGetting started\\n\\nIdentityServer4 is available under du\", \"al license:\\n\\n\\u2022\\n\\n\\u2022\\n\\nRPL - lets you use the IdentityServer4 free if used in open-source work\\n\\nPaid - l\", \"ets you use the IdentityServer4 in a commercial scenario\\n\\nFor more information about pricing, see th\", \"e official product\\u2019s pricing page.\\n\\nYou can add it to your applications using its NuGet packages. Th\", \"e main package is IdentityServer4,\\nwhich has been downloaded over four million times. The base packa\", \"ge doesn\\u2019t include any user\\ninterface code and only supports in-memory configuration. To use it with\", \" a database, you\\u2019ll also want\\na data provider like IdentityServer4.EntityFramework, which uses Entit\", \"y Framework Core to store\\nconfiguration and operational data for IdentityServer. For user interface,\", \" you can copy files from the\\nQuickstart UI repository into your ASP.NET Core MVC application to add \", \"support for sign in and sign\\nout using IdentityServer middleware.\\n\\nConfiguration\\n\\nIdentityServer sup\", \"ports different kinds of protocols and social authentication providers that can be\\nconfigured as par\", \"t of each custom installation. This is typically done in the ASP.NET Core application\\u2019s\\nProgram clas\", \"s (or in the Startup class in the ConfigureServices method). The configuration involves\\nspecifying t\", \"he supported protocols and the paths to the servers and endpoints that will be used.\\nFigure 8-2 show\", \"s an example configuration taken from the IdentityServer4 Quickstart UI project:\\n\\npublic class Start\", \"up\\n{\\n    public void ConfigureServices(IServiceCollection services)\\n    {\\n        services.AddMvc();\", \"\\n\\n145\\n\\nCHAPTER 8 | Cloud-native identity\\n\\n\\f        // some details omitted\\n        services.AddIdent\", \"ityServer();\\n\\n          services.AddAuthentication()\\n            .AddGoogle(\\\"Google\\\", options =>\\n   \", \"         {\\n                options.SignInScheme =\\nIdentityServerConstants.ExternalCookieAuthenticati\", \"onScheme;\\n\\n                options.ClientId = \\\"<insert here>\\\";\\n                options.ClientSecret \", \"= \\\"<insert here>\\\";\\n            })\\n            .AddOpenIdConnect(\\\"demoidsrv\\\", \\\"IdentityServer\\\", optio\", \"ns =>\\n            {\\n                options.SignInScheme =\\nIdentityServerConstants.ExternalCookieAut\", \"henticationScheme;\\n                options.SignOutScheme = IdentityServerConstants.SignoutScheme;\\n\\n \", \"               options.Authority = \\\"https://demo.identityserver.io/\\\";\\n                options.Client\", \"Id = \\\"implicit\\\";\\n                options.ResponseType = \\\"id_token\\\";\\n                options.SaveToke\", \"ns = true;\\n                options.CallbackPath = new PathString(\\\"/signin-idsrv\\\");\\n                o\", \"ptions.SignedOutCallbackPath = new PathString(\\\"/signout-callback-idsrv\\\");\\n                options.Re\", \"moteSignOutPath = new PathString(\\\"/signout-idsrv\\\");\\n\\n                options.TokenValidationParamete\", \"rs = new TokenValidationParameters\\n                {\\n                    NameClaimType = \\\"name\\\",\\n   \", \"                 RoleClaimType = \\\"role\\\"\\n                };\\n            });\\n    }\\n}\\n\\nFigure 8-2. Conf\", \"iguring IdentityServer.\\n\\nJavaScript clients\\n\\nMany cloud-native applications use server-side APIs and\", \" rich client single page applications (SPAs) on\\nthe front end. IdentityServer ships a JavaScript cli\", \"ent (oidc-client.js) via NPM that can be added to\\nSPAs to enable them to use IdentityServer for sign\", \" in, sign out, and token-based authentication of\\nweb APIs.\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nIdentityServer docu\", \"mentation\\n\\nApplication types\\n\\nJavaScript OIDC client\\n\\n146\\n\\nCHAPTER 8 | Cloud-native identity\\n\\n\\fCHAPT\", \"ER  9\\n\\nCloud-native security\\n\\nNot a day goes by where the news doesn\\u2019t contain some story about a co\", \"mpany being hacked or\\nsomehow losing their customers\\u2019 data. Even countries/regions aren\\u2019t immune to \", \"the problems created\\nby treating security as an afterthought. For years, companies have treated the \", \"security of customer\\ndata and, in fact, their entire networks as something of a \\u201cnice to have\\u201d. Wind\", \"ows servers were left\\nunpatched, ancient versions of PHP kept running, and MongoDB databases left wi\", \"de open to the\\nworld.\\n\\nHowever, there are starting to be real-world consequences for not maintaining\", \" a security mindset\\nwhen building and deploying applications. Many companies learned the hard way wh\", \"at can happen\\nwhen servers and desktops aren\\u2019t patched during the 2017 outbreak of NotPetya. The cos\", \"t of these\\nattacks has easily reached into the billions, with some estimates putting the losses from\", \" this single\\nattack at 10 billion US dollars.\\n\\nEven governments aren\\u2019t immune to hacking incidents. \", \"The city of Baltimore was held ransom by\\ncriminals making it impossible for citizens to pay their bi\", \"lls or use city services.\\n\\nThere has also been an increase in legislation that mandates certain data\", \" protections for personal\\ndata. In Europe, GDPR has been in effect for more than a year and, more re\", \"cently, California passed\\ntheir own version called CCDA, which comes into effect January 1, 2020. Th\", \"e fines under GDPR can be\\nso punishing as to put companies out of business. Google has already been \", \"fined 50 million Euros for\\nviolations, but that\\u2019s just a drop in the bucket compared with the potent\", \"ial fines.\\n\\nIn short, security is serious business.\\n\\nAzure security for cloud-native apps\\n\\nCloud-nat\", \"ive applications can be both easier and more difficult to secure than traditional applications.\\nOn t\", \"he downside, you need to secure more smaller applications and dedicate more energy to build\\nout the \", \"security infrastructure. The heterogeneous nature of programming languages and styles in\\nmost servic\", \"e deployments also means you need to pay more attention to security bulletins from many\\ndifferent pr\", \"oviders.\\n\\nOn the flip side, smaller services, each with their own data store, limit the scope of an \", \"attack. If an\\nattacker compromises one system, it\\u2019s probably more difficult for the attacker to make\", \" the jump to\\nanother system than it is in a monolithic application. Process boundaries are strong bo\", \"undaries. Also,\\nif a database backup gets exposed, then the damage is more limited, as that database\", \" contains only a\\nsubset of data and is unlikely to contain personal data.\\n\\n147\\n\\nCHAPTER 9 | Cloud-na\", \"tive security\\n\\n\\fThreat modeling\\n\\nNo matter if the advantages outweigh the disadvantages of cloud-nat\", \"ive applications, the same\\nholistic security mindset must be followed. Security and secure thinking \", \"must be part of every step of\\nthe development and operations story. When planning an application ask\", \" questions like:\\n\\n\\u2022  What would be the impact of this data being lost?\\n\\n\\u2022\\n\\nHow can we limit the dama\", \"ge from bad data being injected into this service?\\n\\n\\u2022  Who should have access to this data?\\n\\n\\u2022\\n\\nAre \", \"there auditing policies in place around the development and release process?\\n\\nAll these questions ar\", \"e part of a process called threat modeling. This process tries to answer the\\nquestion of what threat\", \"s there are to the system, how likely the threats are, and the potential damage\\nfrom them.\\n\\nOnce the\", \" list of threats has been established, you need to decide whether they\\u2019re worth mitigating.\\nSometime\", \"s a threat is so unlikely and expensive to plan for that it isn\\u2019t worth spending energy on it.\\nFor i\", \"nstance, some state level actor could inject changes into the design of a process that is used by\\nmi\", \"llions of devices. Now, instead of running a certain piece of code in Ring 3, that code is run in Ri\", \"ng\\n0. This process allows an exploit that can bypass the hypervisor and run the attack code on the b\", \"are\\nmetal machines, allowing attacks on all the virtual machines that are running on that hardware.\\n\", \"\\nThe altered processors are difficult to detect without a microscope and advanced knowledge of the o\", \"n\\nsilicon design of that processor. This scenario is unlikely to happen and expensive to mitigate, s\", \"o\\nprobably no threat model would recommend building exploit protection for it.\\n\\nMore likely threats,\", \" such as broken access controls permitting Id incrementing attacks (replacing Id=2\\nwith Id=3 in the \", \"URL) or SQL injection, are more attractive to build protections against. The\\nmitigations for these t\", \"hreats are quite reasonable to build and prevent embarrassing security holes\\nthat smear the company\\u2019\", \"s reputation.\\n\\nPrinciple of least privilege\\n\\nOne of the founding ideas in computer security is the P\", \"rinciple of Least Privilege (POLP). It\\u2019s actually a\\nfoundational idea in most any form of security b\", \"e it digital or physical. In short, the principle is that\\nany user or process should have the smalle\", \"st number of rights possible to execute its task.\\n\\nAs an example, think of the tellers at a bank: ac\", \"cessing the safe is an uncommon activity. So, the\\naverage teller can\\u2019t open the safe themselves. To \", \"gain access, they need to escalate their request\\nthrough a bank manager, who performs additional sec\", \"urity checks.\\n\\nIn a computer system, a fantastic example is the rights of a user connecting to a dat\", \"abase. In many\\ncases, there\\u2019s a single user account used to both build the database structure and ru\", \"n the application.\\nExcept in extreme cases, the account running the application doesn\\u2019t need the abi\", \"lity to update\\nschema information. There should be several accounts that provide different levels of\", \" privilege. The\\napplication should only use the permission level that grants read and writes access \", \"to the data in the\\ntables. This kind of protection would eliminate attacks that aimed to drop databa\", \"se tables or\\nintroduce malicious triggers.\\n\\n148\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\fAlmost every pa\", \"rt of building a cloud-native application can benefit from remembering the principle\\nof least privil\", \"ege. You can find it at play when setting up firewalls, network security groups, roles, and\\nscopes i\", \"n Role-based access control (RBAC).\\n\\nPenetration testing\\n\\nAs applications become more complicated th\", \"e number of attack vectors increases at an alarming rate.\\nThreat modeling is flawed in that it tends\", \" to be executed by the same people building the system. In\\nthe same way that many developers have tr\", \"ouble envisioning user interactions and then build\\nunusable user interfaces, most developers have di\", \"fficulty seeing every attack vector. It\\u2019s also possible\\nthat the developers building the system aren\", \"\\u2019t well versed in attack methodologies and miss\\nsomething crucial.\\n\\nPenetration testing or \\u201cpen test\", \"ing\\u201d involves bringing in external actors to attempt to attack the\\nsystem. These attackers may be an\", \" external consulting company or other developers with good\\nsecurity knowledge from another part of t\", \"he business. They\\u2019re given carte blanche to attempt to\\nsubvert the system. Frequently, they\\u2019ll find \", \"extensive security holes that need to be patched.\\nSometimes the attack vector will be something tota\", \"lly unexpected like exploiting a phishing attack\\nagainst the CEO.\\n\\nAzure itself is constantly underg\", \"oing attacks from a team of hackers inside Microsoft. Over the years,\\nthey\\u2019ve been the first to find\", \" dozens of potentially catastrophic attack vectors, closing them before\\nthey can be exploited extern\", \"ally. The more tempting a target, the more likely that eternal actors will\\nattempt to exploit it and\", \" there are a few targets in the world more tempting than Azure.\\n\\nMonitoring\\n\\nShould an attacker atte\", \"mpt to penetrate an application, there should be some warning of it.\\nFrequently, attacks can be spot\", \"ted by examining the logs from services. Attacks leave telltale signs\\nthat can be spotted before the\", \"y succeed. For instance, an attacker attempting to guess a password\\nwill make many requests to a log\", \"in system. Monitoring around the login system can detect weird\\npatterns that are out of line with th\", \"e typical access pattern. This monitoring can be turned into an\\nalert that can, in turn, alert an op\", \"erations person to activate some sort of countermeasure. A highly\\nmature monitoring system might eve\", \"n take action based on these deviations proactively adding rules\\nto block requests or throttle respo\", \"nses.\\n\\nSecuring the build\\n\\nOne place where security is often overlooked is around the build process.\", \" Not only should the build\\nrun security checks, such as scanning for insecure code or checked-in cre\", \"dentials, but the build itself\\nshould be secure. If the build server is compromised, then it provide\", \"s a fantastic vector for introducing\\narbitrary code into the product.\\n\\nImagine that an attacker is l\", \"ooking to steal the passwords of people signing into a web application.\\nThey could introduce a build\", \" step that modifies the checked-out code to mirror any login request to\\nanother server. The next tim\", \"e code goes through the build, it\\u2019s silently updated. The source code\\nvulnerability scanning won\\u2019t c\", \"atch this vulnerability as it runs before the build. Equally, nobody will\\n\\n149\\n\\nCHAPTER 9 | Cloud-na\", \"tive security\\n\\n\\fcatch it in a code review because the build steps live on the build server. The expl\", \"oited code will go to\\nproduction where it can harvest passwords. Probably there\\u2019s no audit log of th\", \"e build process\\nchanges, or at least nobody monitoring the audit.\\n\\nThis scenario is a perfect exampl\", \"e of a seemingly low-value target that can be used to break into the\\nsystem. Once an attacker breach\", \"es the perimeter of the system, they can start working on finding\\nways to elevate their permissions \", \"to the point that they can cause real harm anywhere they like.\\n\\nBuilding secure code\\n\\n.NET Framework\", \" is already a quite secure framework. It avoids some of the pitfalls of unmanaged\\ncode, such as walk\", \"ing off the ends of arrays. Work is actively done to fix security holes as they\\u2019re\\ndiscovered. There\", \"\\u2019s even a bug bounty program that pays researchers to find issues in the framework\\nand report them i\", \"nstead of exploiting them.\\n\\nThere are many ways to make .NET code more secure. Following guidelines \", \"such as the Secure coding\\nguidelines for .NET article is a reasonable step to take to ensure that th\", \"e code is secure from the\\nground up. The OWASP top 10 is another invaluable guide to build secure co\", \"de.\\n\\nThe build process is a good place to put scanning tools to detect problems in source code befor\", \"e they\\nmake it into production. Most every project has dependencies on some other packages. A tool t\", \"hat\\ncan scan for outdated packages will catch problems in a nightly build. Even when building Docker\", \"\\nimages, it\\u2019s useful to check and make sure that the base image doesn\\u2019t have known vulnerabilities.\\n\", \"Another thing to check is that nobody has accidentally checked in credentials.\\n\\nBuilt-in security\\n\\nA\", \"zure is designed to balance usability and security for most users. Different users are going to have\", \"\\ndifferent security requirements, so they need to fine-tune their approach to cloud security. Micros\", \"oft\\npublishes a great deal of security information in the Trust Center. This resource should be the \", \"first\\nstop for those professionals interested in understanding how the built-in attack mitigation\\nte\", \"chnologies work.\\n\\nWithin the Azure portal, the Azure Advisor is a system that is constantly scanning\", \" an environment and\\nmaking recommendations. Some of these recommendations are designed to save users\", \" money, but\\nothers are designed to identify potentially insecure configurations, such as having a st\", \"orage container\\nopen to the world and not protected by a Virtual Network.\\n\\nAzure network infrastruct\", \"ure\\n\\nIn an on-premises deployment environment, a great deal of energy is dedicated to setting up\\nnet\", \"working. Setting up routers, switches, and the such is complicated work. Networks allow certain\\nreso\", \"urces to talk to other resources and prevent access in some cases. A frequent network rule is to\\nres\", \"trict access to the production environment from the development environment on the off chance\\nthat a\", \" half-developed piece of code runs awry and deletes a swath of data.\\n\\nOut of the box, most PaaS Azur\", \"e resources have only the most basic and permissive networking setup.\\nFor instance, anybody on the I\", \"nternet can access an app service. New SQL Server instances typically\\n\\n150\\n\\nCHAPTER 9 | Cloud-native\", \" security\\n\\n\\fcome restricted, so that external parties can\\u2019t access them, but the IP address ranges u\", \"sed by Azure\\nitself are permitted through. So, while the SQL server is protected from external threa\", \"ts, an attacker\\nonly needs to set up an Azure bridgehead from where they can launch attacks against \", \"all SQL\\ninstances on Azure.\\n\\nFortunately, most Azure resources can be placed into an Azure Virtual N\", \"etwork that allows fine-\\ngrained access control. Similar to the way that on-premises networks establ\", \"ish private networks that\\nare protected from the wider world, virtual networks are islands of privat\", \"e IP addresses that are\\nlocated within the Azure network.\\n\\nFigure 9-1. A virtual network in Azure.\\n\\n\", \"In the same way that on-premises networks have a firewall governing access to the network, you can\\ne\", \"stablish a similar firewall at the boundary of the virtual network. By default, all the resources on\", \" a\\nvirtual network can still talk to the Internet. It\\u2019s only incoming connections that require some \", \"form of\\nexplicit firewall exception.\\n\\nWith the network established, internal resources like storage \", \"accounts can be set up to only allow for\\naccess by resources that are also on the Virtual Network. T\", \"his firewall provides an extra level of\\nsecurity, should the keys for that storage account be leaked\", \", attackers wouldn\\u2019t be able to connect to\\nit to exploit the leaked keys. This scenario is another e\", \"xample of the principle of least privilege.\\n\\nThe nodes in an Azure Kubernetes cluster can participat\", \"e in a virtual network just like other resources\\nthat are more native to Azure. This functionality i\", \"s called Azure Container Networking Interface. In\\neffect, it allocates a subnet within the virtual n\", \"etwork on which virtual machines and container images\\nare allocated.\\n\\nContinuing down the path of il\", \"lustrating the principle of least privilege, not every resource within a\\nVirtual Network needs to ta\", \"lk to every other resource. For instance, in an application that provides a\\n\\n151\\n\\nCHAPTER 9 | Cloud-\", \"native security\\n\\n\\fweb API over a storage account and a SQL database, it\\u2019s unlikely that the database\", \" and the storage\\naccount need to talk to one another. Any data sharing between them would go through\", \" the web\\napplication. So, a network security group (NSG) could be used to deny traffic between the t\", \"wo\\nservices.\\n\\nA policy of denying communication between resources can be annoying to implement, espe\", \"cially\\ncoming from a background of using Azure without traffic restrictions. On some other clouds, t\", \"he\\nconcept of network security groups is much more prevalent. For instance, the default policy on AW\", \"S is\\nthat resources can\\u2019t communicate among themselves until enabled by rules in an NSG. While slowe\", \"r\\nto develop this, a more restrictive environment provides a more secure default. Making use of prop\", \"er\\nDevOps practices, especially using Azure Resource Manager or Terraform to manage permissions can\\n\", \"make controlling the rules easier.\\n\\nVirtual Networks can also be useful when setting up communicatio\", \"n between on-premises and cloud\\nresources. A virtual private network can be used to seamlessly attac\", \"h the two networks together. This\\napproach allows running a virtual network without any sort of gate\", \"way for scenarios where all the\\nusers are on-site. There are a number of technologies that can be us\", \"ed to establish this network. The\\nsimplest is to use a site-to-site VPN that can be established betw\", \"een many routers and Azure. Traffic\\nis encrypted and tunneled over the Internet at the same cost per\", \" byte as any other traffic. For\\nscenarios where more bandwidth or more security is desirable, Azure \", \"offers a service called Express\\nRoute that uses a private circuit between an on-premises network and\", \" Azure. It\\u2019s more costly and\\ndifficult to establish but also more secure.\\n\\nRole-based access control\", \" for restricting access to Azure resources\\n\\nRBAC is a system that provides an identity to applicatio\", \"ns running in Azure. Applications can access\\nresources using this identity instead of or in addition\", \" to using keys or passwords.\\n\\nSecurity Principals\\n\\nThe first component in RBAC is a security princip\", \"al. A security principal can be a user, group, service\\nprincipal, or managed identity.\\n\\nFigure 9-2. \", \"Different types of security principals.\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n152\\n\\nUser - Any user who has an account in Azure A\", \"ctive Directory is a user.\\n\\nGroup - A collection of users from Azure Active Directory. As a member o\", \"f a group, a user\\ntakes on the roles of that group in addition to their own.\\n\\nService principal - A \", \"security identity under which services or applications run.\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\f\\u2022  \", \"Managed identity - An Azure Active Directory identity managed by Azure. Managed identities\\nare typic\", \"ally used when developing cloud applications that manage the credentials for\\nauthenticating to Azure\", \" services.\\n\\nThe security principal can be applied to most any resource. This aspect means that it\\u2019s \", \"possible to\\nassign a security principal to a container running within Azure Kubernetes, allowing it \", \"to access secrets\\nstored in Key Vault. An Azure Function could take on a permission allowing it to t\", \"alk to an Active\\nDirectory instance to validate a JWT for a calling user. Once services are enabled \", \"with a service\\nprincipal, their permissions can be managed granularly using roles and scopes.\\n\\nRoles\", \"\\n\\nA security principal can take on many roles or, using a more sartorial analogy, wear many hats. Ea\", \"ch\\nrole defines a series of permissions such as \\u201cRead messages from Azure Service Bus endpoint\\u201d. The\", \"\\neffective permission set of a security principal is the combination of all the permissions assigned\", \" to all\\nthe roles that a security principal has. Azure has a large number of built-in roles and user\", \"s can define\\ntheir own roles.\\n\\nFigure 9-3. RBAC role definitions.\\n\\nBuilt into Azure are also a numbe\", \"r of high-level roles such as Owner, Contributor, Reader, and User\\nAccount Administrator. With the O\", \"wner role, a security principal can access all resources and assign\\npermissions to others. A contrib\", \"utor has the same level of access to all resources but they can\\u2019t assign\\npermissions. A Reader can o\", \"nly view existing Azure resources and a User Account Administrator can\\nmanage access to Azure resour\", \"ces.\\n\\nMore granular built-in roles such as DNS Zone Contributor have rights limited to a single serv\", \"ice.\\nSecurity principals can take on any number of roles.\\n\\n153\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\f\", \"Scopes\\n\\nRoles can be applied to a restricted set of resources within Azure. For instance, applying s\", \"cope to the\\nprevious example of reading from a Service Bus queue, you can narrow the permission to a\", \" single\\nqueue: \\u201cRead messages from Azure Service Bus endpoint blah.servicebus.windows.net/queue1\\u201d\\n\\nT\", \"he scope can be as narrow as a single resource or it can be applied to an entire resource group,\\nsub\", \"scription, or even management group.\\n\\nWhen testing if a security principal has certain permission, t\", \"he combination of role and scope are\\ntaken into account. This combination provides a powerful author\", \"ization mechanism.\\n\\nDeny\\n\\nPreviously, only \\u201callow\\u201d rules were permitted for RBAC. This behavior made\", \" some scopes complicated\\nto build. For instance, allowing a security principal access to all storage\", \" accounts except one required\\ngranting explicit permission to a potentially endless list of storage \", \"accounts. Every time a new storage\\naccount was created, it would have to be added to this list of ac\", \"counts. This added management\\noverhead that certainly wasn\\u2019t desirable.\\n\\nDeny rules take precedence \", \"over allow rules. Now representing the same \\u201callow all but one\\u201d scope\\ncould be represented as two ru\", \"les \\u201callow all\\u201d and \\u201cdeny this one specific one\\u201d. Deny rules not only\\nease management but allow for \", \"resources that are extra secure by denying access to everybody.\\n\\nChecking access\\n\\nAs you can imagine\", \", having a large number of roles and scopes can make figuring out the effective\\npermission of a serv\", \"ice principal quite difficult. Piling deny rules on top of that, only serves to increase\\nthe complex\", \"ity. Fortunately, there\\u2019s a permissions calculator that can show the effective permissions\\nfor any s\", \"ervice principal. It\\u2019s typically found under the IAM tab in the portal, as shown in Figure 9-3.\\n\\nFig\", \"ure 9-4. Permission calculator for an app service.\\n\\n154\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\fSecurin\", \"g secrets\\n\\nPasswords and certificates are a common attack vector for attackers. Password-cracking ha\", \"rdware can\\ndo a brute-force attack and try to guess billions of passwords per second. So it\\u2019s import\", \"ant that the\\npasswords that are used to access resources are strong, with a large variety of charact\", \"ers. These\\npasswords are exactly the kind of passwords that are near impossible to remember. Fortuna\", \"tely, the\\npasswords in Azure don\\u2019t actually need to be known by any human.\\n\\nMany security experts su\", \"ggest that using a password manager to keep your own passwords is the\\nbest approach. While it centra\", \"lizes your passwords in one location, it also allows using highly complex\\npasswords and ensuring the\", \"y\\u2019re unique for each account. The same system exists within Azure: a\\ncentral store for secrets.\\n\\nAzu\", \"re Key Vault\\n\\nAzure Key Vault provides a centralized location to store passwords for things such as \", \"databases, API\\nkeys, and certificates. Once a secret is entered into the Vault, it\\u2019s never shown aga\", \"in and the\\ncommands to extract and view it are purposefully complicated. The information in the safe\", \" is\\nprotected using either software encryption or FIPS 140-2 Level 2 validated Hardware Security\\nMod\", \"ules.\\n\\nAccess to the key vault is provided through RBACs, meaning that not just any user can access \", \"the\\ninformation in the vault. Say a web application wishes to access the database connection string \", \"stored\\nin Azure Key Vault. To gain access, applications need to run using a service principal. Under\", \" this\\nassumed role, they can read the secrets from the safe. There are a number of different securit\", \"y\\nsettings that can further limit the access that an application has to the vault, so that it can\\u2019t \", \"update\\nsecrets but only read them.\\n\\nAccess to the key vault can be monitored to ensure that only the\", \" expected applications are accessing\\nthe vault. The logs can be integrated back into Azure Monitor, \", \"unlocking the ability to set up alerts\\nwhen unexpected conditions are encountered.\\n\\nKubernetes\\n\\nWith\", \"in Kubernetes, there\\u2019s a similar service for maintaining small pieces of secret information.\\nKuberne\", \"tes Secrets can be set via the typical kubectl executable.\\n\\nCreating a secret is as simple as findin\", \"g the base64 version of the values to be stored:\\n\\necho -n 'admin' | base64\\nYWRtaW4=\\necho -n '1f2d1e2\", \"e67df' | base64\\nMWYyZDFlMmU2N2Rm\\n\\nThen adding it to a secrets file named secret.yml for example that\", \" looks similar to the following\\nexample:\\n\\napiVersion: v1\\nkind: Secret\\nmetadata:\\n  name: mysecret\\n\\n15\", \"5\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\ftype: Opaque\\ndata:\\n  username: YWRtaW4=\\n  password: MWYyZDFlM\", \"mU2N2Rm\\n\\nFinally, this file can be loaded into Kubernetes by running the following command:\\n\\nkubectl\", \" apply -f ./secret.yaml\\n\\nThese secrets can then be mounted into volumes or exposed to container proc\", \"esses through\\nenvironment variables. The Twelve-factor app approach to building applications suggest\", \"s using the\\nlowest common denominator to transmit settings to an application. Environment variables \", \"are the\\nlowest common denominator, because they\\u2019re supported no matter the operating system or\\nappli\", \"cation.\\n\\nAn alternative to use the built-in Kubernetes secrets is to access the secrets in Azure Key\", \" Vault from\\nwithin Kubernetes. The simplest way to do this is to assign an RBAC role to the containe\", \"r looking to\\nload secrets. The application can then use the Azure Key Vault APIs to access the secre\", \"ts. However,\\nthis approach requires modifications to the code and doesn\\u2019t follow the pattern of usin\", \"g environment\\nvariables. Instead, it\\u2019s possible to inject values into a container. This approach is \", \"actually more secure\\nthan using the Kubernetes secrets directly, as they can be accessed by users on\", \" the cluster.\\n\\nEncryption in transit and at rest\\n\\nKeeping data safe is important whether it\\u2019s on dis\", \"k or transiting between various different services.\\nThe most effective way to keep data from leaking\", \" is to encrypt it into a format that can\\u2019t be easily read\\nby others. Azure supports a wide range of \", \"encryption options.\\n\\nIn transit\\n\\nThere are several ways to encrypt traffic on the network in Azure. \", \"The access to Azure services is\\ntypically done over connections that use Transport Layer Security (T\", \"LS). For instance, all the\\nconnections to the Azure APIs require TLS connections. Equally, connectio\", \"ns to endpoints in Azure\\nstorage can be restricted to work only over TLS encrypted connections.\\n\\nTLS\", \" is a complicated protocol and simply knowing that the connection is using TLS isn\\u2019t sufficient to\\ne\", \"nsure security. For instance, TLS 1.0 is chronically insecure, and TLS 1.1 isn\\u2019t much better. Even w\", \"ithin\\nthe versions of TLS, there are various settings that can make the connections easier to decryp\", \"t. The\\nbest course of action is to check and see if the server connection is using up-to-date and we\", \"ll\\nconfigured protocols.\\n\\nThis check can be done by an external service such as SSL labs\\u2019 SSL Server\", \" Test. A test run against a\\ntypical Azure endpoint, in this case a service bus endpoint, yields a ne\", \"ar perfect score of A.\\n\\nEven services like Azure SQL databases use TLS encryption to keep data hidde\", \"n. The interesting part\\nabout encrypting the data in transit using TLS is that it isn\\u2019t possible, ev\", \"en for Microsoft, to listen in on\\nthe connection between computers running TLS. This should provide \", \"comfort for companies\\nconcerned that their data may be at risk from Microsoft proper or even a state\", \" actor with more\\nresources than the standard attacker.\\n\\n156\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\fFig\", \"ure 9-5. SSL labs report showing a score of A for a Service Bus endpoint.\\n\\nWhile this level of encry\", \"ption isn\\u2019t going to be sufficient for all time, it should inspire confidence that\\nAzure TLS connect\", \"ions are quite secure. Azure will continue to evolve its security standards as\\nencryption improves. \", \"It\\u2019s nice to know that there\\u2019s somebody watching the security standards and\\nupdating Azure as they i\", \"mprove.\\n\\nAt rest\\n\\nIn any application, there are a number of places where data rests on the disk. The\", \" application code\\nitself is loaded from some storage mechanism. Most applications also use some kind\", \" of a database\\nsuch as SQL Server, Cosmos DB, or even the amazingly price-efficient Table Storage. T\", \"hese databases\\nall use heavily encrypted storage to ensure that nobody other than the applications w\", \"ith proper\\npermissions can read your data. Even the system operators can\\u2019t read data that has been e\", \"ncrypted.\\nSo customers can remain confident their secret information remains secret.\\n\\nStorage\\n\\nThe u\", \"nderpinning of much of Azure is the Azure Storage engine. Virtual machine disks are mounted\\non top o\", \"f Azure Storage. Azure Kubernetes Service runs on virtual machines that, themselves, are\\nhosted on A\", \"zure Storage. Even serverless technologies, such as Azure Functions Apps and Azure\\nContainer Instanc\", \"es, run out of disk that is part of Azure Storage.\\n\\nIf Azure Storage is well encrypted, then it prov\", \"ides for a foundation for most everything else to also\\nbe encrypted. Azure Storage is encrypted with\", \" FIPS 140-2 compliant 256-bit AES. This is a well-\\nregarded encryption technology having been the su\", \"bject of extensive academic scrutiny over the last\\n20 or so years. At present, there\\u2019s no known prac\", \"tical attack that would allow someone without\\nknowledge of the key to read data encrypted by AES.\\n\\nB\", \"y default, the keys used for encrypting Azure Storage are managed by Microsoft. There are extensive\\n\", \"protections in place to ensure to prevent malicious access to these keys. However, users with\\npartic\", \"ular encryption requirements can also provide their own storage keys that are managed in Azure\\n\\n157\\n\", \"\\nCHAPTER 9 | Cloud-native security\\n\\n\\fKey Vault. These keys can be revoked at any time, which would e\", \"ffectively render the contents of the\\nStorage account using them inaccessible.\\n\\nVirtual machines use\", \" encrypted storage, but it\\u2019s possible to provide another layer of encryption by\\nusing technologies l\", \"ike BitLocker on Windows or DM-Crypt on Linux. These technologies mean that\\neven if the disk image w\", \"as leaked off of storage, it would remain near impossible to read it.\\n\\nAzure SQL\\n\\nDatabases hosted o\", \"n Azure SQL use a technology called Transparent Data Encryption (TDE) to ensure\\ndata remains encrypt\", \"ed. It\\u2019s enabled by default on all newly created SQL databases, but must be\\nenabled manually for leg\", \"acy databases. TDE executes real-time encryption and decryption of not just\\nthe database, but also t\", \"he backups and transaction logs.\\n\\nThe encryption parameters are stored in the master database and, o\", \"n startup, are read into memory\\nfor the remaining operations. This means that the master database mu\", \"st remain unencrypted. The\\nactual key is managed by Microsoft. However, users with exacting security\", \" requirements may provide\\ntheir own key in Key Vault in much the same way as is done for Azure Stora\", \"ge. The Key Vault provides\\nfor such services as key rotation and revocation.\\n\\nThe \\u201cTransparent\\u201d part\", \" of TDS comes from the fact that there aren\\u2019t client changes needed to use an\\nencrypted database. Wh\", \"ile this approach provides for good security, leaking the database password is\\nenough for users to b\", \"e able to decrypt the data. There\\u2019s another approach that encrypts individual\\ncolumns or tables in a\", \" database. Always Encrypted ensures that at no point the encrypted data\\nappears in plain text inside\", \" the database.\\n\\nSetting up this tier of encryption requires running through a wizard in SQL Server M\", \"anagement Studio\\nto select the sort of encryption and where in Key Vault to store the associated key\", \"s.\\n\\n158\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\fFigure 9-6. Selecting columns in a table to be encrypte\", \"d using Always Encrypted.\\n\\nClient applications that read information from these encrypted columns ne\", \"ed to make special\\nallowances to read encrypted data. Connection strings need to be updated with Col\", \"umn Encryption\\nSetting=Enabled and client credentials must be retrieved from the Key Vault. The SQL \", \"Server client\\nmust then be primed with the column encryption keys. Once that is done, the remaining \", \"actions use\\nthe standard interfaces to SQL Client. That is, tools like Dapper and Entity Framework, \", \"which are built\\non top of SQL Client, will continue to work without changes. Always Encrypted may no\", \"t yet be\\navailable for every SQL Server driver on every language.\\n\\nThe combination of TDE and Always\", \" Encrypted, both of which can be used with client-specific keys,\\nensures that even the most exacting\", \" encryption requirements are supported.\\n\\nCosmos DB\\n\\nCosmos DB is the newest database provided by Mic\", \"rosoft in Azure. It has been built from the ground\\nup with security and cryptography in mind. AES-25\", \"6bit encryption is standard for all Cosmos DB\\n\\n159\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\fdatabases an\", \"d can\\u2019t be disabled. Coupled with the TLS 1.2 requirement for communication, the entire\\nstorage solu\", \"tion is encrypted.\\n\\nFigure 9-7. The flow of data encryption within Cosmos DB.\\n\\nWhile Cosmos DB doesn\", \"\\u2019t provide for supplying customer encryption keys, there has been significant\\nwork done by the team \", \"to ensure it remains PCI-DSS compliant without that. Cosmos DB also doesn\\u2019t\\nsupport any sort of sing\", \"le column encryption similar to Azure SQL\\u2019s Always Encrypted yet.\\n\\nKeeping secure\\n\\nAzure has all the\", \" tools necessary to release a highly secure product. However, a chain is only as strong\\nas its weake\", \"st link. If the applications deployed on top of Azure aren\\u2019t developed with a proper\\nsecurity mindse\", \"t and good security audits, then they become the weak link in the chain. There are\\nmany great static\", \" analysis tools, encryption libraries, and security practices that can be used to ensure\\nthat the so\", \"ftware installed on Azure is as secure as Azure itself. Examples include static analysis tools,\\nencr\", \"yption libraries, and security practices.\\n\\n160\\n\\nCHAPTER 9 | Cloud-native security\\n\\n\\fCHAPTER  10\\n\\nDev\", \"Ops\\n\\nThe favorite mantra of software consultants is to answer \\u201cIt depends\\u201d to any question posed. It\", \" isn\\u2019t\\nbecause software consultants are fond of not taking a position. It\\u2019s because there\\u2019s no one t\", \"rue\\nanswer to any questions in software. There\\u2019s no absolute right and wrong, but rather a balance\\nb\", \"etween opposites.\\n\\nTake, for instance, the two major schools of developing web applications: Single \", \"Page Applications\\n(SPAs) versus server-side applications. On the one hand, the user experience tends\", \" to be better with\\nSPAs and the amount of traffic to the web server can be minimized making it possi\", \"ble to host them\\non something as simple as static hosting. On the other hand, SPAs tend to be slower\", \" to develop and\\nmore difficult to test. Which one is the right choice? Well, it depends on your situ\", \"ation.\\n\\nCloud-native applications aren\\u2019t immune to that same dichotomy. They have clear advantages i\", \"n\\nterms of speed of development, stability, and scalability, but managing them can be quite a bit mo\", \"re\\ndifficult.\\n\\nYears ago, it wasn\\u2019t uncommon for the process of moving an application from developme\", \"nt to\\nproduction to take a month, or even more. Companies released software on a 6-month or even eve\", \"ry\\nyear cadence. One needs to look no further than Microsoft Windows to get an idea for the cadence \", \"of\\nreleases that were acceptable before the ever-green days of Windows 10. Five years passed between\", \"\\nWindows XP and Vista, a further three between Vista and Windows 7.\\n\\nIt\\u2019s now fairly well establishe\", \"d that being able to release software rapidly gives fast-moving companies\\na huge market advantage ov\", \"er their more sloth-like competitors. It\\u2019s for that reason that major\\nupdates to Windows 10 are now \", \"approximately every six months.\\n\\nThe patterns and practices that enable faster, more reliable releas\", \"es to deliver value to the business\\nare collectively known as DevOps. They consist of a wide range o\", \"f ideas spanning the entire software\\ndevelopment life cycle from specifying an application all the w\", \"ay up to delivering and operating that\\napplication.\\n\\nDevOps emerged before microservices and it\\u2019s li\", \"kely that the movement towards smaller, more fit to\\npurpose services wouldn\\u2019t have been possible wit\", \"hout DevOps to make releasing and operating not\\njust one but many applications in production easier.\", \"\\n\\n161\\n\\nCHAPTER 10 | DevOps\\n\\n\\fFigure 10-1 - DevOps and microservices.\\n\\nThrough good DevOps practices,\", \" it\\u2019s possible to realize the advantages of cloud-native applications\\nwithout suffocating under a mo\", \"untain of work actually operating the applications.\\n\\nThere\\u2019s no golden hammer when it comes to DevOp\", \"s. Nobody can sell a complete and all-\\nencompassing solution for releasing and operating high-qualit\", \"y applications. This is because each\\napplication is wildly different from all others. However, there\", \" are tools that can make DevOps a far less\\ndaunting proposition. One of these tools is known as Azur\", \"e DevOps.\\n\\nAzure DevOps\\n\\nAzure DevOps has a long pedigree. It can trace its roots back to when Team \", \"Foundation Server first\\nmoved online and through the various name changes: Visual Studio Online and \", \"Visual Studio Team\\nServices. Through the years, however, it has become far more than its predecessor\", \"s.\\n\\nAzure DevOps is divided into five major components:\\n\\nFigure 10-2 - Azure DevOps.\\n\\nAzure Repos - \", \"Source code management that supports the venerable Team Foundation Version\\nControl (TFVC) and the in\", \"dustry favorite Git. Pull requests provide a way to enable social coding by\\nfostering discussion of \", \"changes as they\\u2019re made.\\n\\n162\\n\\nCHAPTER 10 | DevOps\\n\\n\\fAzure Boards - Provides an issue and work item \", \"tracking tool that strives to allow users to pick the\\nworkflows that work best for them. It comes wi\", \"th a number of pre-configured templates including\\nones to support SCRUM and Kanban styles of develop\", \"ment.\\n\\nAzure Pipelines - A build and release management system that supports tight integration with \", \"Azure.\\nBuilds can be run on various platforms from Windows to Linux to macOS. Build agents may be\\npr\", \"ovisioned in the cloud or on-premises.\\n\\nAzure Test Plans - No QA person will be left behind with the\", \" test management and exploratory\\ntesting support offered by the Test Plans feature.\\n\\nAzure Artifacts\", \" - An artifact feed that allows companies to create their own, internal, versions of\\nNuGet, npm, and\", \" others. It serves a double purpose of acting as a cache of upstream packages if\\nthere\\u2019s a failure o\", \"f a centralized repository.\\n\\nThe top-level organizational unit in Azure DevOps is known as a Project\", \". Within each project the\\nvarious components, such as Azure Artifacts, can be turned on and off. Eac\", \"h of these components\\nprovides different advantages for cloud-native applications. The three most us\", \"eful are repositories,\\nboards, and pipelines. If users want to manage their source code in another r\", \"epository stack, such as\\nGitHub, but still take advantage of Azure Pipelines and other components, t\", \"hat\\u2019s perfectly possible.\\n\\nFortunately, development teams have many options when selecting a reposit\", \"ory. One of them is\\nGitHub.\\n\\nGitHub Actions\\n\\nFounded in 2009, GitHub is a widely popular web-based r\", \"epository for hosting projects,\\ndocumentation, and code. Many large tech companies, such as Apple, A\", \"mazon, Google, and\\nmainstream corporations use GitHub. GitHub uses the open-source, distributed vers\", \"ion control system\\nnamed Git as its foundation. On top, it then adds its own set of features, includ\", \"ing defect tracking,\\nfeature and pull requests, tasks management, and wikis for each code base.\\n\\nAs \", \"GitHub evolves, it too is adding DevOps features. For example, GitHub has its own continuous\\nintegra\", \"tion/continuous delivery (CI/CD) pipeline, called GitHub Actions. GitHub Actions is a\\ncommunity-powe\", \"red workflow automation tool. It lets DevOps teams integrate with their existing\\ntooling, mix and ma\", \"tch new products, and hook into their software lifecycle, including existing CI/CD\\npartners.\\u201d\\n\\nGitHu\", \"b has over 40 million users, making it the largest host of source code in the world. In October of\\n2\", \"018, Microsoft purchased GitHub. Microsoft has pledged that GitHub will remain an open platform\\nthat\", \" any developer can plug into and extend. It continues to operate as an independent company.\\nGitHub o\", \"ffers plans for enterprise, team, professional, and free accounts.\\n\\nSource control\\n\\nOrganizing the c\", \"ode for a cloud-native application can be challenging. Instead of a single giant\\napplication, the cl\", \"oud-native applications tend to be made up of a web of smaller applications that\\ntalk with one anoth\", \"er. As with all things in computing, the best arrangement of code remains an open\\n\\n163\\n\\nCHAPTER 10 |\", \" DevOps\\n\\n\\fquestion. There are examples of successful applications using different kinds of layouts, \", \"but two\\nvariants seem to have the most popularity.\\n\\nBefore getting down into the actual source contr\", \"ol itself, it\\u2019s probably worth deciding on how many\\nprojects are appropriate. Within a single projec\", \"t, there\\u2019s support for multiple repositories, and build\\npipelines. Boards are a little more complica\", \"ted, but there too, the tasks can easily be assigned to\\nmultiple teams within a single project. It\\u2019s\", \" possible to support hundreds, even thousands of\\ndevelopers, out of a single Azure DevOps project. D\", \"oing so is likely the best approach as it provides a\\nsingle place for all developer to work out of a\", \"nd reduces the confusion of finding that one application\\nwhen developers are unsure in which project\", \" in which it resides.\\n\\nSplitting up code for microservices within the Azure DevOps project can be sl\", \"ightly more challenging.\\n\\nFigure 10-3 - One vs. many repositories.\\n\\nRepository per microservice\\n\\nAt \", \"first glance, this approach seems like the most logical approach to splitting up the source code for\", \"\\nmicroservices. Each repository can contain the code needed to build the one microservice. The\\nadvan\", \"tages to this approach are readily visible:\\n\\n1.\\n\\n2.\\n\\n3.\\n\\n4.\\n\\n164\\n\\nInstructions for building and main\", \"taining the application can be added to a README file at\\nthe root of each repository. When flipping \", \"through the repositories, it\\u2019s easy to find these\\ninstructions, reducing spin-up time for developers\", \".\\n\\nEvery service is located in a logical place, easily found by knowing the name of the service.\\n\\nBu\", \"ilds can easily be set up such that they\\u2019re only triggered when a change is made to the\\nowning repos\", \"itory.\\n\\nThe number of changes coming into a repository is limited to the small number of developers\\n\", \"working on the project.\\n\\nCHAPTER 10 | DevOps\\n\\n\\f5.\\n\\n6.\\n\\nSecurity is easy to set up by restricting the\", \" repositories to which developers have read and\\nwrite permissions.\\n\\nRepository level settings can be\", \" changed by the owning team with a minimum of discussion\\nwith others.\\n\\nOne of the key ideas behind m\", \"icroservices is that services should be siloed and separated from each\\nother. When using Domain Driv\", \"en Design to decide on the boundaries for services the services act as\\ntransactional boundaries. Dat\", \"abase updates shouldn\\u2019t span multiple services. This collection of related\\ndata is referred to as a \", \"bounded context. This idea is reflected by the isolation of microservice data to\\na database separate\", \" and autonomous from the rest of the services. It makes a great deal of sense to\\ncarry this idea all\", \" the way through to the source code.\\n\\nHowever, this approach isn\\u2019t without its issues. One of the mo\", \"re gnarly development problems of our\\ntime is managing dependencies. Consider the number of files th\", \"at make up the average\\nnode_modules directory. A fresh install of something like create-react-app is\", \" likely to bring with it\\nthousands of packages. The question of how to manage these dependencies is \", \"a difficult one.\\n\\nIf a dependency is updated, then downstream packages must also update this depende\", \"ncy.\\nUnfortunately, that takes development work so, invariably, the node_modules directory ends up w\", \"ith\\nmultiple versions of a single package, each one a dependency of some other package that is\\nversi\", \"oned at a slightly different cadence. When deploying an application, which version of a\\ndependency s\", \"hould be used? The version that is currently in production? The version that is currently\\nin Beta bu\", \"t is likely to be in production by the time the consumer makes it to production? Difficult\\nproblems \", \"that aren\\u2019t resolved by just using microservices.\\n\\nThere are libraries that are depended upon by a w\", \"ide variety of projects. By dividing the microservices\\nup with one in each repository the internal d\", \"ependencies can best be resolved by using the internal\\nrepository, Azure Artifacts. Builds for libra\", \"ries will push their latest versions into Azure Artifacts for\\ninternal consumption. The downstream p\", \"roject must still be manually updated to take a dependency\\non the newly updated packages.\\n\\nAnother d\", \"isadvantage presents itself when moving code between services. Although it would be nice\\nto believe \", \"that the first division of an application into microservices is 100% correct, the reality is that\\nra\", \"rely we\\u2019re so prescient as to make no service division mistakes. Thus, functionality and the code th\", \"at\\ndrives it will need to move from service to service: repository to repository. When leaping from \", \"one\\nrepository to another, the code loses its history. There are many cases, especially in the event\", \" of an\\naudit, where having full history on a piece of code is invaluable.\\n\\nThe final and most import\", \"ant disadvantage is coordinating changes. In a true microservices\\napplication, there should be no de\", \"ployment dependencies between services. It should be possible to\\ndeploy services A, B, and C in any \", \"order as they have loose coupling. In reality, however, there are\\ntimes when it\\u2019s desirable to make \", \"a change that crosses multiple repositories at the same time. Some\\nexamples include updating a libra\", \"ry to close a security hole or changing a communication protocol\\nused by all services.\\n\\nTo do a cros\", \"s-repository change requires a commit to each repository be made in succession. Each\\nchange in each \", \"repository will need to be pull-requested and reviewed separately. This activity can be\\ndifficult to\", \" coordinate.\\n\\n165\\n\\nCHAPTER 10 | DevOps\\n\\n\\fAn alternative to using many repositories is to put all the\", \" source code together in a giant, all knowing,\\nsingle repository.\\n\\nSingle repository\\n\\nIn this approa\", \"ch, sometimes referred to as a monorepository, all the source code for every service is\\nput into the\", \" same repository. At first, this approach seems like a terrible idea likely to make dealing\\nwith sou\", \"rce code unwieldy. There are, however, some marked advantages to working this way.\\n\\nThe first advant\", \"age is that it\\u2019s easier to manage dependencies between projects. Instead of relying on\\nsome external\", \" artifact feed, projects can directly import one another. This means that updates are\\ninstant, and c\", \"onflicting versions are likely to be found at compile time on the developer\\u2019s workstation.\\nIn effect\", \", shifting some of the integration testing left.\\n\\nWhen moving code between projects, it\\u2019s now easier\", \" to preserve the history as the files will be\\ndetected as having been moved rather than being rewrit\", \"ten.\\n\\nAnother advantage is that wide ranging changes that cross service boundaries can be made in a\\n\", \"single commit. This activity reduces the overhead of having potentially dozens of changes to review\\n\", \"individually.\\n\\nThere are many tools that can perform static analysis of code to detect insecure prog\", \"ramming\\npractices or problematic use of APIs. In a multi-repository world, each repository will need\", \" to be\\niterated over to find the problems in them. The single repository allows running the analysis\", \" all in one\\nplace.\\n\\nThere are also many disadvantages to the single repository approach. One of the \", \"most worrying ones\\nis that having a single repository raises security concerns. If the contents of a\", \" repository are leaked in\\na repository per service model, the amount of code lost is minimal. With a\", \" single repository,\\neverything the company owns could be lost. There have been many examples in the \", \"past of this\\nhappening and derailing entire game development efforts. Having multiple repositories e\", \"xposes less\\nsurface area, which is a desirable trait in most security practices.\\n\\nThe size of the si\", \"ngle repository is likely to become unmanageable rapidly. This presents some\\ninteresting performance\", \" implications. It may become necessary to use specialized tools such as Virtual\\nFile System for Git,\", \" which was originally designed to improve the experience for developers on the\\nWindows team.\\n\\nFreque\", \"ntly the argument for using a single repository boils down to an argument that Facebook or\\nGoogle us\", \"e this method for source code arrangement. If the approach is good enough for these\\ncompanies, then,\", \" surely, it\\u2019s the correct approach for all companies. The truth of the matter is that few\\ncompanies \", \"operate on anything like the scale of Facebook or Google. The problems that occur at\\nthose scales ar\", \"e different from those most developers will face. What is good for the goose may not\\nbe good for the\", \" gander.\\n\\nIn the end, either solution can be used to host the source code for microservices. However\", \", in most\\ncases, the management, and engineering overhead of operating in a single repository isn\\u2019t \", \"worth the\\nmeager advantages. Splitting code up over multiple repositories encourages better separati\", \"on of\\nconcerns and encourages autonomy among development teams.\\n\\n166\\n\\nCHAPTER 10 | DevOps\\n\\n\\fStandard\", \" directory structure\\n\\nRegardless of the single versus multiple repositories debate each service will\", \" have its own directory.\\nOne of the best optimizations to allow developers to cross between projects\", \" quickly is to maintain a\\nstandard directory structure.\\n\\nFigure 10-4 - Standard directory structure.\", \"\\n\\nWhenever a new project is created, a template that puts in place the correct structure should be u\", \"sed.\\nThis template can also include such useful items as a skeleton README file and an azure-\\npipeli\", \"nes.yml. In any microservice architecture, a high degree of variance between projects makes bulk\\nope\", \"rations against the services more difficult.\\n\\nThere are many tools that can provide templating for a\", \"n entire directory, containing several source\\ncode directories. Yeoman is popular in the JavaScript \", \"world and GitHub have recently released\\nRepository Templates, which provide much of the same functio\", \"nality.\\n\\nTask management\\n\\nManaging tasks in any project can be difficult. Up front there are countle\", \"ss questions to be answered\\nabout the sort of workflows to set up to ensure optimal developer produc\", \"tivity.\\n\\nCloud-native applications tend to be smaller than traditional software products or at least\", \" they\\u2019re\\ndivided into smaller services. Tracking of issues or tasks related to these services remain\", \"s as important\\nas with any other software project. Nobody wants to lose track of some work item or e\", \"xplain to a\\ncustomer that their issue wasn\\u2019t properly logged. Boards are configured at the project l\", \"evel but within\\neach project, areas can be defined. These allow breaking down issues across several \", \"components. The\\nadvantage to keeping all the work for the entire application in one place is that it\", \"\\u2019s easy to move work\\nitems from one team to another as they\\u2019re understood better.\\n\\n167\\n\\nCHAPTER 10 |\", \" DevOps\\n\\n\\fAzure DevOps comes with a number of popular templates pre-configured. In the most basic\\nco\", \"nfiguration, all that is needed to know is what\\u2019s in the backlog, what people are working on, and\\nwh\", \"at\\u2019s done. It\\u2019s important to have this visibility into the process of building software, so that wor\", \"k\\ncan be prioritized and completed tasks reported to the customer. Of course, few software projects\\n\", \"stick to a process as simple as to do, doing, and done. It doesn\\u2019t take long for people to start add\", \"ing\\nsteps like QA or Detailed Specification to the process.\\n\\nOne of the more important parts of Agil\", \"e methodologies is self-introspection at regular intervals.\\nThese reviews are meant to provide insig\", \"ht into what problems the team is facing and how they can\\nbe improved. Frequently, this means changi\", \"ng the flow of issues and features through the\\ndevelopment process. So, it\\u2019s perfectly healthy to ex\", \"pand the layouts of the boards with additional\\nstages.\\n\\nThe stages in the boards aren\\u2019t the only org\", \"anizational tool. Depending on the configuration of the\\nboard, there\\u2019s a hierarchy of work items. Th\", \"e most granular item that can appear on a board is a task.\\nOut of the box a task contains fields for\", \" a title, description, a priority, an estimate of the amount of\\nwork remaining and the ability to li\", \"nk to other work items or development items (branches, commits,\\npull requests, builds, and so forth)\", \". Work items can be classified into different areas of the application\\nand different iterations (spr\", \"ints) to make finding them easier.\\n\\nFigure 10-5 - Task in Azure DevOps.\\n\\nThe description field suppo\", \"rts the normal styles you\\u2019d expect (bold, italic underscore and strike\\nthrough) and the ability to i\", \"nsert images. This makes it a powerful tool for use when specifying work\\nor bugs.\\n\\nTasks can be roll\", \"ed up into features, which define a larger unit of work. Features, in turn, can be rolled\\nup into ep\", \"ics. Classifying tasks in this hierarchy makes it much easier to understand how close a large\\nfeatur\", \"e is to rolling out.\\n\\n168\\n\\nCHAPTER 10 | DevOps\\n\\n\\fFigure 10-6 - Work item in Azure DevOps.\\n\\nThere are\", \" different kinds of views into the issues in Azure Boards. Items that aren\\u2019t yet scheduled\\nappear in\", \" the backlog. From there, they can be assigned to a sprint. A sprint is a time box during\\nwhich it\\u2019s\", \" expected some quantity of work will be completed. This work can include tasks but also the\\nresoluti\", \"on of tickets. Once there, the entire sprint can be managed from the Sprint board section. This\\nview\", \" shows how work is progressing and includes a burn down chart to give an ever-updating\\nestimate of i\", \"f the sprint will be successful.\\n\\nFigure 10-7 - Board in Azure DevOps.\\n\\nBy now, it should be apparen\", \"t that there\\u2019s a great deal of power in the Boards in Azure DevOps. For\\ndevelopers, there are easy v\", \"iews of what is being worked on. For project managers views into\\nupcoming work as well as an overvie\", \"w of existing work. For managers, there are plenty of reports\\nabout resourcing and capacity. Unfortu\", \"nately, there\\u2019s nothing magical about cloud-native applications\\nthat eliminate the need to track wor\", \"k. But if you must track work, there are a few places where the\\nexperience is better than in Azure D\", \"evOps.\\n\\nCI/CD pipelines\\n\\nAlmost no change in the software development life cycle has been so revolut\", \"ionary as the advent of\\ncontinuous integration (CI) and continuous delivery (CD). Building and runni\", \"ng automated tests\\nagainst the source code of a project as soon as a change is checked in catches mi\", \"stakes early. Prior to\\nthe advent of continuous integration builds, it wouldn\\u2019t be uncommon to pull \", \"code from the\\n\\n169\\n\\nCHAPTER 10 | DevOps\\n\\n\\frepository and find that it didn\\u2019t pass tests or couldn\\u2019t \", \"even be built. This resulted in tracking down\\nthe source of the breakage.\\n\\nTraditionally shipping so\", \"ftware to the production environment required extensive documentation and\\na list of steps. Each one \", \"of these steps needed to be manually completed in a very error prone\\nprocess.\\n\\nFigure 10-8 - Checkli\", \"st.\\n\\nThe sister of continuous integration is continuous delivery in which the freshly built packages\", \" are\\ndeployed to an environment. The manual process can\\u2019t scale to match the speed of development so\", \"\\nautomation becomes more important. Checklists are replaced by scripts that can execute the same\\ntas\", \"ks faster and more accurately than any human.\\n\\nThe environment to which continuous delivery delivers\", \" might be a test environment or, as is being\\ndone by many major technology companies, it could be th\", \"e production environment. The latter\\nrequires an investment in high-quality tests that can give conf\", \"idence that a change isn\\u2019t going to\\nbreak production for users. In the same way that continuous inte\", \"gration caught issues in the code\\nearly continuous delivery catches issues in the deployment process\", \" early.\\n\\nThe importance of automating the build and delivery process is accentuated by cloud-native\\n\", \"applications. Deployments happen more frequently and to more environments so manually deploying\\nbord\", \"ers on impossible.\\n\\nAzure Builds\\n\\nAzure DevOps provides a set of tools to make continuous integratio\", \"n and deployment easier than\\never. These tools are located under Azure Pipelines. The first of them \", \"is Azure Builds, which is a tool\\nfor running YAML-based build definitions at scale. Users can either\", \" bring their own build machines\\n(great for if the build requires a meticulously set up environment) \", \"or use a machine from a constantly\\nrefreshed pool of Azure hosted virtual machines. These hosted bui\", \"ld agents come pre-installed with a\\n\\n170\\n\\nCHAPTER 10 | DevOps\\n\\n\\fwide range of development tools for \", \"not just .NET development but for everything from Java to\\nPython to iPhone development.\\n\\nDevOps incl\", \"udes a wide range of out of the box build definitions that can be customized for any build.\\nThe buil\", \"d definitions are defined in a file called azure-pipelines.yml and checked into the repository so\\nth\", \"ey can be versioned along with the source code. This makes it much easier to make changes to the\\nbui\", \"ld pipeline in a branch as the changes can be checked into just that branch. An example azure-\\npipel\", \"ines.yml for building an ASP.NET web application on full framework is show in Figure 10-9.\\n\\nname: $(\", \"rev:r)\\n\\nvariables:\\n  version: 9.2.0.$(Build.BuildNumber)\\n  solution: Portals.sln\\n  artifactName: dro\", \"p\\n  buildPlatform: any cpu\\n  buildConfiguration: release\\n\\npool:\\n  name: Hosted VisualStudio\\n  demand\", \"s:\\n  - msbuild\\n  - visualstudio\\n  - vstest\\n\\nsteps:\\n- task: NuGetToolInstaller@0\\n  displayName: 'Use \", \"NuGet 4.4.1'\\n  inputs:\\n    versionSpec: 4.4.1\\n\\n- task: NuGetCommand@2\\n  displayName: 'NuGet restore'\", \"\\n  inputs:\\n    restoreSolution: '$(solution)'\\n\\n- task: VSBuild@1\\n  displayName: 'Build solution'\\n  i\", \"nputs:\\n    solution: '$(solution)'\\n    msbuildArgs: '-p:DeployOnBuild=true -p:WebPublishMethod=Packa\", \"ge -\\np:PackageAsSingleFile=true -p:SkipInvalidConfigurations=true -\\np:PackageLocation=\\\"$(build.artif\", \"actstagingdirectory)\\\\\\\\\\\"'\\n    platform: '$(buildPlatform)'\\n    configuration: '$(buildConfiguration)'\", \"\\n\\n- task: VSTest@2\\n  displayName: 'Test Assemblies'\\n  inputs:\\n    testAssemblyVer2: |\\n     **\\\\$(buil\", \"dConfiguration)\\\\**\\\\*test*.dll\\n     !**\\\\obj\\\\**\\n     !**\\\\*testadapter.dll\\n    platform: '$(buildPlatfo\", \"rm)'\\n    configuration: '$(buildConfiguration)'\\n\\n- task: CopyFiles@2\\n  displayName: 'Copy UI Test Fi\", \"les to: $(build.artifactstagingdirectory)'\\n  inputs:\\n\\n171\\n\\nCHAPTER 10 | DevOps\\n\\n\\f    SourceFolder: U\", \"ITests\\n    TargetFolder: '$(build.artifactstagingdirectory)/uitests'\\n\\n- task: PublishBuildArtifacts@\", \"1\\n  displayName: 'Publish Artifact'\\n  inputs:\\n    PathtoPublish: '$(build.artifactstagingdirectory)'\", \"\\n    ArtifactName: '$(artifactName)'\\n  condition: succeededOrFailed()\\n\\nFigure 10-9 - A sample azure-\", \"pipelines.yml\\n\\nThis build definition uses a number of built-in tasks that make creating builds as si\", \"mple as building a\\nLego set (simpler than the giant Millennium Falcon). For instance, the NuGet task\", \" restores NuGet\\npackages, while the VSBuild task calls the Visual Studio build tools to perform the \", \"actual compilation.\\nThere are hundreds of different tasks available in Azure DevOps, with thousands \", \"more that are\\nmaintained by the community. It\\u2019s likely that no matter what build tasks you\\u2019re lookin\", \"g to run,\\nsomebody has built one already.\\n\\nBuilds can be triggered manually, by a check-in, on a sch\", \"edule, or by the completion of another build.\\nIn most cases, building on every check-in is desirable\", \". Builds can be filtered so that different builds run\\nagainst different parts of the repository or a\", \"gainst different branches. This allows for scenarios like\\nrunning fast builds with reduced testing o\", \"n pull requests and running a full regression suite against\\nthe trunk on a nightly basis.\\n\\nThe end r\", \"esult of a build is a collection of files known as build artifacts. These artifacts can be passed\\nal\", \"ong to the next step in the build process or added to an Azure Artifacts feed, so they can be\\nconsum\", \"ed by other builds.\\n\\nAzure DevOps releases\\n\\nBuilds take care of compiling the software into a shippa\", \"ble package, but the artifacts still need to be\\npushed out to a testing environment to complete cont\", \"inuous delivery. For this, Azure DevOps uses a\\nseparate tool called Releases. The Releases tool make\", \"s use of the same tasks\\u2019 library that were\\navailable to the Build but introduce a concept of \\u201cstages\", \"\\u201d. A stage is an isolated environment into\\nwhich the package is installed. For instance, a product m\", \"ight make use of a development, a QA, and a\\nproduction environment. Code is continuously delivered i\", \"nto the development environment where\\nautomated tests can be run against it. Once those tests pass t\", \"he release moves onto the QA\\nenvironment for manual testing. Finally, the code is pushed to producti\", \"on where it\\u2019s visible to\\neverybody.\\n\\nFigure 10-10 - Release pipeline\\n\\nEach stage in the build can be\", \" automatically triggered by the completion of the previous phase. In\\nmany cases, however, this isn\\u2019t\", \" desirable. Moving code into production might require approval from\\nsomebody. The Releases tool supp\", \"orts this by allowing approvers at each step of the release pipeline.\\nRules can be set up such that \", \"a specific person or group of people must sign off on a release before it\\n\\n172\\n\\nCHAPTER 10 | DevOps\\n\", \"\\n\\fmakes into production. These gates allow for manual quality checks and also for compliance with an\", \"y\\nregulatory requirements related to control what goes into production.\\n\\nEverybody gets a build pipe\", \"line\\n\\nThere\\u2019s no cost to configuring many build pipelines, so it\\u2019s advantageous to have at least one\", \" build\\npipeline per microservice. Ideally, microservices are independently deployable to any environ\", \"ment so\\nhaving each one able to be released via its own pipeline without releasing a mass of unrelat\", \"ed code is\\nperfect. Each pipeline can have its own set of approvals allowing for variations in build\", \" process for\\neach service.\\n\\nVersioning releases\\n\\nOne drawback to using the Releases functionality is\", \" that it can\\u2019t be defined in a checked-in azure-\\npipelines.yml file. There are many reasons you migh\", \"t want to do that from having per-branch release\\ndefinitions to including a release skeleton in your\", \" project template. Fortunately, work is ongoing to\\nshift some of the stages support into the Build c\", \"omponent. This will be known as multi-stage build\\nand the first version is available now!\\n\\nFeature f\", \"lags\\n\\nIn chapter 1, we affirmed that cloud native is much about speed and agility. Users expect rapi\", \"d\\nresponsiveness, innovative features, and zero downtime. Feature flags are a modern deployment\\ntech\", \"nique that helps increase agility for cloud-native applications. They enable you to deploy new\\nfeatu\", \"res into a production environment, but restrict their availability. With the flick of a switch, you \", \"can\\nactivate a new feature for specific users without restarting the app or deploying new code. They\", \"\\nseparate the release of new features from their code deployment.\\n\\nFeature flags are built upon cond\", \"itional logic that control visibility of functionality for users at run\\ntime. In modern cloud-native\", \" systems, it\\u2019s common to deploy new features into production early, but\\ntest them with a limited aud\", \"ience. As confidence increases, the feature can be incrementally rolled out\\nto wider audiences.\\n\\nOth\", \"er use cases for feature flags include:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nRestrict premium functionality to specific cust\", \"omer groups willing to pay higher subscription\\nfees.\\n\\nStabilize a system by quickly deactivating a p\", \"roblem feature, avoiding the risks of a rollback or\\nimmediate hotfix.\\n\\nDisable an optional feature w\", \"ith high resource consumption during peak usage periods.\\n\\nConduct experimental feature releases to s\", \"mall user segments to validate feasibility and\\npopularity.\\n\\nFeature flags also promote trunk-based d\", \"evelopment. It\\u2019s a source-control branching model where\\ndevelopers collaborate on features in a sing\", \"le branch. The approach minimizes the risk and complexity\\nof merging large numbers of long-running f\", \"eature branches. Features are unavailable until activated.\\n\\n173\\n\\nCHAPTER 10 | DevOps\\n\\n\\fImplementing \", \"feature flags\\n\\nAt its core, a feature flag is a reference to a simple decision object. It returns a \", \"Boolean state of on or\\noff. The flag typically wraps a block of code that encapsulates a feature cap\", \"ability. The state of the flag\\ndetermines whether that code block executes for a given user. Figure \", \"10-11 shows the\\nimplementation.\\n\\nif (featureFlag) {\\n    // Run this code block if the featureFlag va\", \"lue is true\\n} else {\\n    // Run this code block if the featureFlag value is false\\n}\\n\\nFigure 10-11 - \", \"Simple feature flag implementation.\\n\\nNote how this approach separates the decision logic from the fe\", \"ature code.\\n\\nIn chapter 1, we discussed the Twelve-Factor App. The guidance recommended keeping conf\", \"iguration\\nsettings external from application executable code. When needed, settings can be read in f\", \"rom the\\nexternal source. Feature flag configuration values should also be independent from their cod\", \"ebase. By\\nexternalizing flag configuration in a separate repository, you can change flag state witho\", \"ut modifying\\nand redeploying the application.\\n\\nAzure App Configuration provides a centralized reposi\", \"tory for feature flags. With it, you define\\ndifferent kinds of feature flags and manipulate their st\", \"ates quickly and confidently. You add the App\\nConfiguration client libraries to your application to \", \"enable feature flag functionality. Various\\nprogramming language frameworks are supported.\\n\\nFeature f\", \"lags can be easily implemented in an ASP.NET Core service. Installing the .NET Feature\\nManagement li\", \"braries and App Configuration provider enable you to declaratively add feature flags to\\nyour code. T\", \"hey enable FeatureGate attributes so that you don\\u2019t have to manually write if statements\\nacross your\", \" codebase.\\n\\nOnce configured in your Startup class, you can add feature flag functionality at the con\", \"troller, action,\\nor middleware level. Figure 10-12 presents controller and action implementation:\\n\\n[\", \"FeatureGate(MyFeatureFlags.FeatureA)]\\npublic class ProductController : Controller\\n{\\n    ...\\n}\\n\\n[Feat\", \"ureGate(MyFeatureFlags.FeatureA)]\\npublic IActionResult UpdateProductStatus()\\n{\\n    return ObjectResu\", \"lt(ProductDto);\\n}\\n\\nFigure 10-12 - Feature flag implementation in a controller and action.\\n\\nIf a feat\", \"ure flag is disabled, the user will receive a 404 (Not Found) status code with no response body.\\n\\nFe\", \"ature flags can also be injected directly into C# classes. Figure 10-13 shows feature flag injection\", \":\\n\\n174\\n\\nCHAPTER 10 | DevOps\\n\\n\\fpublic class ProductController : Controller\\n{\\n    private readonly IFe\", \"atureManager _featureManager;\\n\\n    public ProductController(IFeatureManager featureManager)\\n    {\\n  \", \"      _featureManager = featureManager;\\n    }\\n}\\n\\nFigure 10-13 - Feature flag injection into a class.\", \"\\n\\nThe Feature Management libraries manage the feature flag lifecycle behind the scenes. For example,\", \"\\nto minimize high numbers of calls to the configuration store, the libraries cache flag states for a\", \"\\nspecified duration. They can guarantee the immutability of flag states during a request call. They \", \"also\\noffer a Point-in-time snapshot. You can reconstruct the history of any key-value and provide it\", \"s past\\nvalue at any moment within the previous seven days.\\n\\nInfrastructure as code\\n\\nCloud-native sys\", \"tems embrace microservices, containers, and modern system design to achieve speed\\nand agility. They \", \"provide automated build and release stages to ensure consistent and quality code.\\nBut, that\\u2019s only p\", \"art of the story. How do you provision the cloud environments upon which these\\nsystems run?\\n\\nModern \", \"cloud-native applications embrace the widely accepted practice of Infrastructure as Code, or\\nIaC. Wi\", \"th IaC, you automate platform provisioning. You essentially apply software engineering\\npractices suc\", \"h as testing and versioning to your DevOps practices. Your infrastructure and\\ndeployments are automa\", \"ted, consistent, and repeatable. Just as continuous delivery automated the\\ntraditional model of manu\", \"al deployments, Infrastructure as Code (IaC) is evolving how application\\nenvironments are managed.\\n\\n\", \"Tools like Azure Resource Manager (ARM), Terraform, and the Azure Command Line Interface (CLI)\\nenabl\", \"e you to declaratively script the cloud infrastructure you require.\\n\\nAzure Resource Manager template\", \"s\\n\\nARM stands for Azure Resource Manager. It\\u2019s an API provisioning engine that is built into Azure a\", \"nd\\nexposed as an API service. ARM enables you to deploy, update, delete, and manage the resources\\nco\", \"ntained in Azure resource group in a single, coordinated operation. You provide the engine with a\\nJS\", \"ON-based template that specifies the resources you require and their configuration. ARM\\nautomaticall\", \"y orchestrates the deployment in the correct order respecting dependencies. The engine\\nensures idemp\", \"otency. If a desired resource already exists with the same configuration, provisioning\\nwill be ignor\", \"ed.\\n\\nAzure Resource Manager templates are a JSON-based language for defining various resources in\\nAz\", \"ure. The basic schema looks something like Figure 10-14.\\n\\n{\\n  \\\"$schema\\\": \\\"https://schema.management.\", \"azure.com/schemas/2015-01-\\n\\n175\\n\\nCHAPTER 10 | DevOps\\n\\n\\f01/deploymentTemplate.json#\\\",\\n  \\\"contentVersi\", \"on\\\": \\\"\\\",\\n  \\\"apiProfile\\\": \\\"\\\",\\n  \\\"parameters\\\": {  },\\n  \\\"variables\\\": {  },\\n  \\\"functions\\\": [  ],\\n  \\\"reso\", \"urces\\\": [  ],\\n  \\\"outputs\\\": {  }\\n}\\n\\nFigure 10-14 - The schema for a Resource Manager template\\n\\nWithin\", \" this template, one might define a storage container inside the resources section like so:\\n\\n\\\"resourc\", \"es\\\": [\\n    {\\n      \\\"type\\\": \\\"Microsoft.Storage/storageAccounts\\\",\\n      \\\"name\\\": \\\"[variables('storageAc\", \"countName')]\\\",\\n      \\\"location\\\": \\\"[parameters('location')]\\\",\\n      \\\"apiVersion\\\": \\\"2018-07-01\\\",\\n     \", \" \\\"sku\\\": {\\n        \\\"name\\\": \\\"[parameters('storageAccountType')]\\\"\\n      },\\n      \\\"kind\\\": \\\"StorageV2\\\",\\n \", \"     \\\"properties\\\": {}\\n    }\\n  ],\\n\\nFigure 10-15 - An example of a storage account defined in a Resour\", \"ce Manager template\\n\\nAn ARM template can be parameterized with dynamic environment and configuration\", \" information.\\nDoing so enables it to be reused to define different environments, such as development\", \", QA, or\\nproduction. Normally, the template creates all resources within a single Azure resource gro\", \"up. It\\u2019s\\npossible to define multiple resource groups in a single Resource Manager template, if neede\", \"d. You\\ncan delete all resources in an environment by deleting the resource group itself. Cost analys\", \"is can also\\nbe run at the resource group level, allowing for quick accounting of how much each envir\", \"onment is\\ncosting.\\n\\nThere are many examples of ARM templates available in the Azure Quickstart Templ\", \"ates project on\\nGitHub. They can help accelerate creating a new template or modifying an existing on\", \"e.\\n\\nResource Manager templates can be run in many of ways. Perhaps the simplest way is to simply pas\", \"te\\nthem into the Azure portal. For experimental deployments, this method can be quick. They can also\", \" be\\nrun as part of a build or release process in Azure DevOps. There are tasks that will leverage\\nco\", \"nnections into Azure to run the templates. Changes to Resource Manager templates are applied\\nincreme\", \"ntally, meaning that to add a new resource requires just adding it to the template. The tooling\\nwill\", \" reconcile differences between the current resources and those defined in the template. Resources\\nwi\", \"ll then be created or altered so they match what is defined in the template.\\n\\nTerraform\\n\\nCloud-nativ\", \"e applications are often constructed to be cloud agnostic. Being so means the application\\nisn\\u2019t tigh\", \"tly coupled to a particular cloud vendor and can be deployed to any public cloud.\\n\\n176\\n\\nCHAPTER 10 |\", \" DevOps\\n\\n\\fTerraform is a commercial templating tool that can provision cloud-native applications acr\", \"oss all the\\nmajor cloud players: Azure, Google Cloud Platform, AWS, and AliCloud. Instead of using J\", \"SON as the\\ntemplate definition language, it uses the slightly more terse HCL (Hashicorp Configuratio\", \"n Language).\\n\\nAn example Terraform file that does the same as the previous Resource Manager template\", \" (Figure 10-\\n15) is shown in Figure 10-16:\\n\\nprovider \\\"azurerm\\\" {\\n  version = \\\"=1.28.0\\\"\\n}\\n\\nresource \\\"\", \"azurerm_resource_group\\\" \\\"testrg\\\" {\\n  name     = \\\"production\\\"\\n  location = \\\"West US\\\"\\n}\\n\\nresource \\\"azu\", \"rerm_storage_account\\\" \\\"testsa\\\" {\\n  name                     = \\\"${var.storageAccountName}\\\"\\n  resource\", \"_group_name      = \\\"${azurerm_resource_group.testrg.name}\\\"\\n  location                 = \\\"${var.regio\", \"n}\\\"\\n  account_tier             = \\\"${var.tier}\\\"\\n  account_replication_type = \\\"${var.replicationType}\\\"\", \"\\n\\n}\\n\\nFigure 10-16 - An example of a Resource Manager template\\n\\nTerraform also provides intuitive err\", \"or messages for problem templates. There\\u2019s also a handy validate\\ntask that can be used in the build \", \"phase to catch template errors early.\\n\\nAs with Resource Manager templates, command-line tools are av\", \"ailable to deploy Terraform\\ntemplates. There are also community-created tasks in Azure Pipelines tha\", \"t can validate and apply\\nTerraform templates.\\n\\nSometimes Terraform and ARM templates output meaningf\", \"ul values, such as a connection string to a\\nnewly created database. This information can be captured\", \" in the build pipeline and used in\\nsubsequent tasks.\\n\\nAzure CLI Scripts and Tasks\\n\\nFinally, you can \", \"leverage Azure CLI to declaratively script your cloud infrastructure. Azure CLI scripts\\ncan be creat\", \"ed, found, and shared to provision and configure almost any Azure resource. The CLI is\\nsimple to use\", \" with a gentle learning curve. Scripts are executed within either PowerShell or Bash.\\nThey\\u2019re also s\", \"traightforward to debug, especially when compared with ARM templates.\\n\\nAzure CLI scripts work well w\", \"hen you need to tear down and redeploy your infrastructure. Updating\\nan existing environment can be \", \"tricky. Many CLI commands aren\\u2019t idempotent. That means they\\u2019ll\\nrecreate the resource each time they\", \"\\u2019re run, even if the resource already exists. It\\u2019s always possible to\\nadd code that checks for the e\", \"xistence of each resource before creating it. But, doing so, your script\\ncan become bloated and diff\", \"icult to manage.\\n\\nThese scripts can also be embedded in Azure DevOps pipelines as Azure CLI tasks. E\", \"xecuting the\\npipeline invokes the script.\\n\\n177\\n\\nCHAPTER 10 | DevOps\\n\\n\\fFigure 10-17 shows a YAML snip\", \"pet that lists the version of Azure CLI and the details of the\\nsubscription. Note how Azure CLI comm\", \"ands are included in an inline script.\\n\\n- task: AzureCLI@2\\n  displayName: Azure CLI\\n  inputs:\\n    az\", \"ureSubscription: <Name of the Azure Resource Manager service connection>\\n    scriptType: ps\\n    scri\", \"ptLocation: inlineScript\\n    inlineScript: |\\n      az --version\\n      az account show\\n\\nFigure 10-17 \", \"- Azure CLI script\\n\\nIn the article, What is Infrastructure as Code, Author Sam Guckenheimer describe\", \"s how, \\u201cTeams who\\nimplement IaC can deliver stable environments rapidly and at scale. Teams avoid ma\", \"nual\\nconfiguration of environments and enforce consistency by representing the desired state of thei\", \"r\\nenvironments via code. Infrastructure deployments with IaC are repeatable and prevent runtime issu\", \"es\\ncaused by configuration drift or missing dependencies. DevOps teams can work together with a\\nunif\", \"ied set of practices and tools to deliver applications and their supporting infrastructure rapidly,\\n\", \"reliably, and at scale.\\u201d\\n\\nCloud Native Application Bundles\\n\\nA key property of cloud-native applicati\", \"ons is that they leverage the capabilities of the cloud to speed\\nup development. This design often m\", \"eans that a full application uses different kinds of technologies.\\nApplications may be shipped in Do\", \"cker containers, some services may use Azure Functions, while\\nother parts may run directly on virtua\", \"l machines allocated on large metal servers with hardware GPU\\nacceleration. No two cloud-native appl\", \"ications are the same, so it\\u2019s been difficult to provide a single\\nmechanism for shipping them.\\n\\nThe \", \"Docker containers may run on Kubernetes using a Helm Chart for deployment. The Azure\\nFunctions may b\", \"e allocated using Terraform templates. Finally, the virtual machines may be allocated\\nusing Terrafor\", \"m but built out using Ansible. This is a large variety of technologies and there has been\\nno way to \", \"package them all together into a reasonable package. Until now.\\n\\nCloud Native Application Bundles (C\", \"NABs) are a joint effort by many community-minded companies\\nsuch as Microsoft, Docker, and HashiCorp\", \" to develop a specification to package distributed\\napplications.\\n\\nThe effort was announced in Decemb\", \"er of 2018, so there\\u2019s still a fair bit of work to do to expose the\\neffort to the greater community.\", \" However, there\\u2019s already an open specification and a reference\\nimplementation known as Duffle. This\", \" tool, which was written in Go, is a joint effort between Docker\\nand Microsoft.\\n\\nThe CNABs can conta\", \"in different kinds of installation technologies. This aspect allows things like Helm\\nCharts, Terrafo\", \"rm templates, and Ansible Playbooks to coexist in the same package. Once built, the\\npackages are sel\", \"f-contained and portable; they can be installed from a USB stick. The packages are\\ncryptographically\", \" signed to ensure they originate from the party they claim.\\n\\n178\\n\\nCHAPTER 10 | DevOps\\n\\n\\fThe core of \", \"a CNAB is a file called bundle.json. This file defines the contents of the bundle, be they\\nTerraform\", \" or images or anything else. Figure 11-9 defines a CNAB that invokes some Terraform.\\nNotice, however\", \", that it actually defines an invocation image that is used to invoke the Terraform.\\nWhen packaged u\", \"p, the Docker file that is located in the cnab directory is built into a Docker image,\\nwhich will be\", \" included in the bundle. Having Terraform installed inside a Docker container in the\\nbundle means th\", \"at users don\\u2019t need to have Terraform installed on their machine to run the bundling.\\n\\n{\\n    \\\"name\\\":\", \" \\\"terraform\\\",\\n    \\\"version\\\": \\\"0.1.0\\\",\\n    \\\"schemaVersion\\\": \\\"v1.0.0-WD\\\",\\n    \\\"parameters\\\": {\\n        \", \"\\\"backend\\\": {\\n            \\\"type\\\": \\\"boolean\\\",\\n            \\\"defaultValue\\\": false,\\n            \\\"destinat\", \"ion\\\": {\\n                \\\"env\\\": \\\"TF_VAR_backend\\\"\\n            }\\n        }\\n    },\\n    \\\"invocationImages\", \"\\\": [\\n        {\\n        \\\"imageType\\\": \\\"docker\\\",\\n        \\\"image\\\": \\\"cnab/terraform:latest\\\"\\n        }\\n   \", \" ],\\n    \\\"credentials\\\": {\\n        \\\"tenant_id\\\": {\\n            \\\"env\\\": \\\"TF_VAR_tenant_id\\\"\\n        },\\n   \", \"     \\\"client_id\\\": {\\n            \\\"env\\\": \\\"TF_VAR_client_id\\\"\\n        },\\n        \\\"client_secret\\\": {\\n    \", \"        \\\"env\\\": \\\"TF_VAR_client_secret\\\"\\n        },\\n        \\\"subscription_id\\\": {\\n            \\\"env\\\": \\\"TF\", \"_VAR_subscription_id\\\"\\n        },\\n        \\\"ssh_authorized_key\\\": {\\n            \\\"env\\\": \\\"TF_VAR_ssh_auth\", \"orized_key\\\"\\n        }\\n    },\\n    \\\"actions\\\": {\\n        \\\"status\\\": {\\n            \\\"modifies\\\": true\\n     \", \"   }\\n    }\\n}\\n\\nFigure 10-18 - An example Terraform file\\n\\nThe bundle.json also defines a set of parame\", \"ters that are passed down into the Terraform.\\nParameterization of the bundle allows for installation\", \" in various different environments.\\n\\nThe CNAB format is also flexible, allowing it to be used agains\", \"t any cloud. It can even be used against\\non-premises solutions such as OpenStack.\\n\\n179\\n\\nCHAPTER 10 |\", \" DevOps\\n\\n\\fDevOps Decisions\\n\\nThere are so many great tools in the DevOps space these days and even mo\", \"re fantastic books and\\npapers on how to succeed. A favorite book to get started on the DevOps journe\", \"y is The Phoenix\\nProject, which follows the transformation of a fictional company from NoOps to DevO\", \"ps. One thing is\\nfor certain: DevOps is no longer a \\u201cnice to have\\u201d when deploying complex, Cloud Nat\", \"ive Applications.\\nIt\\u2019s a requirement and should be planned for and resourced at the start of any pro\", \"ject.\\n\\nReferences\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nAzure DevOps\\n\\nAzure Resource Manager\\n\\nTerraform\\n\\nAzure CLI\\n\\n180\\n\\nCHAP\", \"TER 10 | DevOps\\n\\n\\fCHAPTER  11\\n\\nSummary: Architecting\\ncloud-native apps\\n\\nIn summary, here are importa\", \"nt conclusions from this guide:\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nCloud-native is about designing modern applications that e\", \"mbrace rapid change, large scale,\\nand resilience, in modern, dynamic environments such as public, pr\", \"ivate, and hybrid clouds.\\n\\nThe Cloud Native Computing Foundation (CNCF) is an influential open-sourc\", \"e consortium\\nof over 300 major corporations. It\\u2019s responsible for driving the adoption of cloud-nati\", \"ve\\ncomputing across technology and cloud stacks.\\n\\nCNCF guidelines recommend that cloud-native applic\", \"ations embrace six important pillars as\\nshown in Figure 11-1:\\n\\nFigure 11-1. Cloud-native foundationa\", \"l pillars\\n\\n\\u2022\\n\\nThese cloud-native pillars include:\\n\\n\\u2013\\n\\nThe cloud and its underlying service model\\n\\n\\u2013 \", \" Modern design principles\\n\\n\\u2013  Microservices\\n\\n\\u2013\\n\\n\\u2013\\n\\n\\u2013\\n\\nContainerization and container orchestration\\n\\n\", \"Cloud-based backing services, such as databases and message brokers\\n\\nAutomation, including Infrastru\", \"cture as Code and code deployment\\n\\n181\\n\\nCHAPTER 11 | Summary: Architecting cloud-native apps\\n\\n\\f\\u2022\\n\\n\\u2022\\n\", \"\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\n\\u2022\\n\\nKubernetes is the hosting environment of choice for most cloud-native applicat\", \"ions. Smaller,\\nsimple services are sometimes hosted in serverless platforms, such as Azure Functions\", \". Among\\nmany key automation features, both environments provide automatic scaling to handle\\nfluctuat\", \"ing workload volumes.\\n\\nService communication becomes a significant design decision when constructing\", \" a cloud-\\nnative application. Applications typically expose an API gateway to manage front-end clien\", \"t\\ncommunication. Then backend microservices strive to communicate with each other\\nimplementing async\", \"hronous communication patterns, when possible.\\n\\ngRPC is a modern, high-performance framework that ev\", \"olves the age-old remote procedure\\ncall (RPC) protocol. Cloud-native applications often embrace gRPC\", \" to streamline messaging\\nbetween back-end services. gRPC uses HTTP/2 for its transport protocol. It \", \"can be up to 8x\\nfaster than JSON serialization with message sizes 60-80% smaller. gRPC is open sourc\", \"e and\\nmanaged by the Cloud Native Computing Foundation (CNCF).\\n\\nDistributed data is a model often im\", \"plemented by cloud-native applications. Applications\\nsegregate business functionality into small, in\", \"dependent microservices. Each microservice\\nencapsulates its own dependencies, data, and state. The c\", \"lassic shared database model\\nevolves into one of many smaller databases, each aligning with a micros\", \"ervice. When the\\nsmoke clears, we emerge with a design that exposes a database-per-microservice mode\", \"l.\\n\\nNo-SQL databases refer to high-performance, non-relational data stores. They excel in their\\nease\", \"-of-use, scalability, resilience, and availability characteristics. High volume services that\\nrequir\", \"e sub second response time favor NoSQL datastores. The proliferation of NoSQL\\ntechnologies for distr\", \"ibuted cloud-native systems can\\u2019t be overstated.\\n\\nNewSQL is an emerging database technology that com\", \"bines the distributed scalability of\\nNoSQL and the ACID guarantees of a relational database. NewSQL \", \"databases target business\\nsystems that must process high-volumes of data, across distributed environ\", \"ments, with full\\ntransactional/ACID compliance. The Cloud Native Computing Foundation (CNCF) feature\", \"s\\nseveral NewSQL database projects.\\n\\nResiliency is the ability of your system to react to failure an\", \"d still remain functional. Cloud-\\nnative systems embrace distributed architecture where failure is i\", \"nevitable. Applications must\\nbe constructed to respond elegantly to failure and quickly return to a \", \"fully functioning state.\\n\\nService meshes are a configurable infrastructure layer with built-in capab\", \"ilities to handle\\nservice communication and other cross-cutting challenges. They decouple cross-cutt\", \"ing\\nresponsibilities from your business code. These responsibilities move into a service proxy.\\nRefe\", \"rred to as the Sidecar pattern, the proxy is deployed into a separate process to provide\\nisolation f\", \"rom your business code.\\n\\nObservability is a key design consideration for cloud-native applications. \", \"As services are\\ndistributed across a cluster of nodes, centralized logging, monitoring, and alerts, \", \"become\\nmandatory. Azure Monitor is a collection of cloud-based tools designed to provide visibility\\n\", \"into the state of your system.\\n\\n182\\n\\nCHAPTER 11 | Summary: Architecting cloud-native apps\\n\\n\\f\\u2022\\n\\n\\u2022\\n\\nIn\", \"frastructure as Code is a widely accepted practice that automates platform provisioning.\\nYour infras\", \"tructure and deployments are automated, consistent, and repeatable. Tools like\\nAzure Resource Manage\", \"r, Terraform, and the Azure CLI, enable you to declaratively script the\\ncloud infrastructure you req\", \"uire.\\n\\nCode automation is a requirement for cloud-native applications. Modern CI/CD systems help\\nful\", \"fill this principle. They provide separate build and deployment steps that help ensure\\nconsistent an\", \"d quality code. The build stage transforms the code into a binary artifact. The\\nrelease stage picks \", \"up the binary artifact, applies external environment configuration, and\\ndeploys it to a specified en\", \"vironment. Azure DevOps and GitHub are full-featured DevOps\\nenvironments.\\n\\n183\\n\\nCHAPTER 11 | Summary\", \": Architecting cloud-native apps\\n\\n\"]"