"[\"EDITION v1.0.3\\nRefer changelog for the book updates and community contributions.\\nPUBLISHED BY\\nMicros\", \"oft Developer Division, .NET, and Visual Studio product teams\\nA division of Microsoft Corporation\\nOn\", \"e Microsoft Way\\nRedmond, Washington 98052-6399\\nCopyright \\u00a9 2023 by Microsoft Corporation\\nAll rights \", \"reserved. No part of the contents of this book may be reproduced or transmitted in any \\nform or by a\", \"ny means without the written permission of the publisher.\\nThis book is provided \\u201cas-is\\u201d and expresse\", \"s the author\\u2019s views and opinions. The views, opinions, and \\ninformation expressed in this book, inc\", \"luding URL and other Internet website references, may change \\nwithout notice.\\nSome examples depicted\", \" herein are provided for illustration only and are fictitious. No real association \\nor connection is\", \" intended or should be inferred.\\nMicrosoft and the trademarks listed at https://www.microsoft.com on\", \" the \\u201cTrademarks\\u201d webpage are \\ntrademarks of the Microsoft group of companies.\\nMac and macOS are tra\", \"demarks of Apple Inc.\\nThe Docker whale logo is a registered trademark of Docker, Inc. Used by permis\", \"sion.\\nAll other marks and logos are property of their respective owners.\\nAuthors:\\nRob Vettor, Princi\", \"pal MTC (Microsoft Technology Center) Architect for Cloud App Innovation -\\nthinkingincloudnative.com\", \", Microsoft\\nSteve \\u201cardalis\\u201d Smith, Software Architect and Trainer - Ardalis.com\\nParticipants and Rev\", \"iewers:\\nCesar De la Torre, Principal Program Manager, .NET team, Microsoft\\nNish Anil, Senior Program\", \" Manager, .NET team, Microsoft\\nJeremy Likness, Senior Program Manager, .NET team, Microsoft\\nCecil Ph\", \"illip, Senior Cloud Advocate, Microsoft\\nSumit Ghosh, Principal Consultant at Neudesic\\nEditors:\\nMaira\", \" Wenzel, Program Manager, .NET team, MicrosoftDavid Pine, Senior Content Developer, .NET docs, Micro\", \"soft\\nVersion\\nThis guide has been written to cover .NET 7 version along with many additional updates \", \"related to \\nthe same \\u201cwave\\u201d of technologies (that is, Azure and additional third-party technologies)\", \" coinciding in \\ntime with the .NET 7 release.\\nWho should use this guide\\nThe audience for this guide \", \"is mainly developers, development leads, and architects who are \\ninterested in learning how to build\", \" applications designed for the cloud.\\nA secondary audience is technical decision-makers who plan to \", \"choose whether to build their \\napplications using a cloud-native approach.\\nHow you can use this guid\", \"e\\nThis guide begins by defining cloud native and introducing a reference application built using clo\", \"ud\\u0002native principles and technologies. Beyond these first two chapters, the rest of the book is brok\", \"en up \\ninto specific chapters focused on topics common to most cloud-native applications. You can ju\", \"mp to \\nany of these chapters to learn about cloud-native approaches to:\\n\\u2022 Data and data access\\n\\u2022 Com\", \"munication patterns\\n\\u2022 Scaling and scalability\\n\\u2022 Application resiliency\\n\\u2022 Monitoring and health\\n\\u2022 Ide\", \"ntity and security\\n\\u2022 DevOps\\nThis guide is available both in PDF form and online. Feel free to forwar\", \"d this document or links to its \\nonline version to your team to help ensure common understanding of \", \"these topics. Most of these \\ntopics benefit from a consistent understanding of the underlying princi\", \"ples and patterns, as well as \\nthe trade-offs involved in decisions related to these topics. Our goa\", \"l with this document is to equip \\nteams and their leaders with the information they need to make wel\", \"l-informed decisions for their \\napplications\\u2019 architecture, development, and hosting.i Contents\\nCont\", \"ents\\nIntroduction to cloud-native applications......................................................\", \"...................... 1\\nCloud-native computing ....................................................\", \"..............................................................................................3\\nWhat\", \" is Cloud Native?...................................................................................\", \".........................................................................4\\nThe pillars of cloud nati\", \"ve..................................................................................................\", \"..............................................5\\nThe cloud...........................................\", \"....................................................................................................\", \".................................5\\nModern design....................................................\", \"....................................................................................................\", \".............6\\nMicroservices .......................................................................\", \".................................................................................................9\\nC\", \"ontainers ..........................................................................................\", \"................................................................................. 12\\nBacking service\", \"s...................................................................................................\", \"............................................................. 15\\nAutomation.........................\", \"....................................................................................................\", \"............................................ 17\\nCandidate apps for cloud native.....................\", \"....................................................................................................\", \"............ 19\\nModernizing legacy apps.............................................................\", \"................................................................................. 19\\nSummary........\", \"....................................................................................................\", \".................................................................. 21\\nIntroducing eShopOnContainers \", \"reference app ................................................................ 22\\nFeatures and requi\", \"rements ............................................................................................\", \"................................................... 23\\nOverview of the code ........................\", \"....................................................................................................\", \".............................. 25\\nUnderstanding microservices.......................................\", \"....................................................................................................\", \" 27\\nMapping eShopOnContainers to Azure Services.....................................................\", \"................................................ 27\\nContainer orchestration and clustering..........\", \"....................................................................................................\", \"..... 28\\nAPI Gateway ...............................................................................\", \"........................................................................................ 28\\nData ...\", \"....................................................................................................\", \"................................................................................ 29\\nEvent Bus ......\", \"....................................................................................................\", \"................................................................... 30\\nResiliency...................\", \"....................................................................................................\", \"...................................................... 30\\nDeploying eShopOnContainers to Azure......\", \"....................................................................................................\", \".......... 30\\nAzure Kubernetes Service .............................................................\", \"................................................................................ 30\\nDeploying to Azu\", \"re Kubernetes Service using Helm ...................................................................\", \"...................... 30\\nAzure Functions and Logic Apps (Serverless) ..............................\", \"......................................................................... 32\\nCentralized configurati\", \"on..................................................................................................\", \"................................................ 33ii Contents\\nAzure App Configuration .............\", \"....................................................................................................\", \"............................. 33\\nAzure Key Vault....................................................\", \"....................................................................................................\", \"......... 34\\nConfiguration in eShop.................................................................\", \"................................................................................. 34\\nReferences.....\", \"....................................................................................................\", \".................................................................. 34\\nScaling cloud-native applicati\", \"ons........................................................................................ 36\\nLever\", \"aging containers and orchestrators..................................................................\", \".................................................. 36\\nChallenges with monolithic deployments .......\", \"....................................................................................................\", \"... 36\\nWhat are the benefits of containers and orchestrators? ......................................\", \"............................................ 38\\nWhat are the scaling benefits?......................\", \"....................................................................................................\", \".......... 40\\nWhat scenarios are ideal for containers and orchestrators?............................\", \"............................................... 42\\nWhen should you avoid using containers and orches\", \"trators?....................................................................... 42\\nDevelopment resou\", \"rces................................................................................................\", \"................................................. 42\\nLeveraging serverless functions ...............\", \"....................................................................................................\", \"................... 46\\nWhat is serverless?..........................................................\", \"................................................................................................. 47\", \"\\nWhat challenges are solved by serverless? .........................................................\", \"................................................... 47\\nWhat is the difference between a microservice\", \" and a serverless function? ............................................. 47\\nWhat scenarios are appr\", \"opriate for serverless?.............................................................................\", \"...................... 47\\nWhen should you avoid serverless?.........................................\", \"................................................................................. 48\\nCombining conta\", \"iners and serverless approaches.....................................................................\", \"............................. 49\\nWhen does it make sense to use containers with serverless?.........\", \"............................................................... 49\\nWhen should you avoid using conta\", \"iners with Azure Functions? ................................................................ 49\\nHow \", \"to combine serverless and Docker containers ........................................................\", \"................................... 49\\nHow to combine serverless and Kubernetes with KEDA ..........\", \"........................................................................ 50\\nDeploying containers in \", \"Azure ..............................................................................................\", \".......................................... 50\\nAzure Container Registry .............................\", \"....................................................................................................\", \"............. 50\\nACR Tasks .........................................................................\", \"................................................................................................... \", \"52\\nAzure Kubernetes Service ........................................................................\", \"..................................................................... 52\\nAzure Bridge to Kubernetes.\", \"....................................................................................................\", \".................................... 53\\nScaling containers and serverless applications .............\", \"............................................................................................ 53\\nThe \", \"simple solution: scaling up ........................................................................\", \".......................................................... 53\\nScaling out cloud-native apps ........\", \"....................................................................................................\", \"........................ 54\\nOther container deployment options .....................................\", \"...................................................................................... 55iii Content\", \"s\\nWhen does it make sense to deploy to App Service for Containers? .................................\", \"........................ 55\\nHow to deploy to App Service for Containers.............................\", \"......................................................................... 55\\nWhen does it make sense\", \" to deploy to Azure Container Instances? .......................................................... \", \"55\\nHow to deploy an app to Azure Container Instances................................................\", \"........................................ 55\\nReferences..............................................\", \"....................................................................................................\", \"......................... 56\\nCloud-native communication patterns ...................................\", \"............................................ 58\\nCommunication considerations .......................\", \"....................................................................................................\", \"........... 58\\nFront-end client communication.......................................................\", \"............................................................................. 60\\nSimple Gateways ...\", \"....................................................................................................\", \"....................................................... 62\\nAzure Application Gateway................\", \"....................................................................................................\", \"...................... 63\\nAzure API Management......................................................\", \"........................................................................................... 63\\nReal-\", \"time communication .................................................................................\", \"........................................................... 66\\nService-to-service communication ....\", \"....................................................................................................\", \"........................ 67\\nQueries ................................................................\", \"....................................................................................................\", \"............. 68\\nCommands...........................................................................\", \"............................................................................................... 71\\nE\", \"vents...............................................................................................\", \"..................................................................................... 74\\ngRPC.......\", \"....................................................................................................\", \"................................................................................ 80\\nWhat is gRPC? ..\", \"....................................................................................................\", \"............................................................. 80\\ngRPC Benefits .....................\", \"....................................................................................................\", \"........................................... 80\\nProtocol Buffers ....................................\", \"....................................................................................................\", \"........................ 81\\ngRPC support in .NET ...................................................\", \".................................................................................................. 8\", \"1\\ngRPC usage........................................................................................\", \"................................................................................. 82\\ngRPC implementa\", \"tion ...............................................................................................\", \"..................................................... 83\\nLooking ahead..............................\", \"....................................................................................................\", \"................................. 85\\nService Mesh communication infrastructure .....................\", \"........................................................................................ 85\\nSummary.\", \"....................................................................................................\", \"......................................................................... 86\\nCloud-native data patte\", \"rns ................................................................................................\", \".. 88\\nDatabase-per-microservice, why? ..............................................................\", \".................................................................... 89\\nCross-service queries.......\", \"....................................................................................................\", \"................................................ 90\\nDistributed transactions .......................\", \"....................................................................................................\", \".......................... 91\\nHigh volume data .....................................................\", \"....................................................................................................\", \"........ 93\\nCQRS ...................................................................................\", \".................................................................................................. 9\", \"3iv Contents\\nEvent sourcing.........................................................................\", \".......................................................................................... 94\\nRelati\", \"onal vs. NoSQL data ................................................................................\", \"................................................................. 96\\nThe CAP theorem ...............\", \"....................................................................................................\", \".......................................... 97\\nConsiderations for relational vs. NoSQL systems.......\", \"......................................................................................... 99\\nDatabas\", \"e as a Service .....................................................................................\", \"................................................................ 99\\nAzure relational databases .....\", \"....................................................................................................\", \"................................100\\nAzure SQL Database..............................................\", \"....................................................................................................\", \"....100\\nOpen-source databases in Azure..............................................................\", \"...............................................................101\\nNoSQL data in Azure .............\", \"....................................................................................................\", \"...................................102\\nNewSQL databases.............................................\", \"....................................................................................................\", \".......106\\nData migration to the cloud .............................................................\", \".........................................................................108\\nCaching in a cloud-nati\", \"ve app..............................................................................................\", \".........................................108\\nWhy?...................................................\", \"....................................................................................................\", \".............................108\\nCaching architecture...............................................\", \"....................................................................................................\", \"..109\\nAzure Cache for Redis ........................................................................\", \"..........................................................................110\\nElasticsearch in a clo\", \"ud-native app ......................................................................................\", \".......................................110\\nSummary..................................................\", \"....................................................................................................\", \"......................111\\nCloud-native resiliency ..................................................\", \"..................................................... 113\\nApplication resiliency patterns ..........\", \"....................................................................................................\", \"........................114\\nCircuit breaker pattern.................................................\", \"................................................................................................116\\n\", \"Testing for resiliency..............................................................................\", \".......................................................................117\\nAzure platform resiliency\", \" ...................................................................................................\", \"..............................................117\\nDesign with resiliency............................\", \"....................................................................................................\", \"...................118\\nDesign with redundancy.......................................................\", \".......................................................................................118\\nDesign fo\", \"r scalability.......................................................................................\", \"..............................................................120\\nBuilt-in retry in services .......\", \"....................................................................................................\", \"....................................121\\nResilient communications....................................\", \"....................................................................................................\", \"........122\\nService mesh ...........................................................................\", \".........................................................................................123\\nIstio a\", \"nd Envoy............................................................................................\", \"....................................................................124\\nIntegration with Azure Kuber\", \"netes Services .....................................................................................\", \"..................125\\nMonitoring and health.........................................................\", \"............................................... 126\\nObservability patterns..........................\", \"....................................................................................................\", \".........................126v Contents\\nWhen to use logging..........................................\", \"....................................................................................................\", \"......126\\nChallenges with detecting and responding to potential app health issues ..................\", \".........................130\\nChallenges with reacting to critical problems in cloud-native apps.....\", \".....................................................130\\nLogging with Elastic Stack.................\", \"....................................................................................................\", \"..........................131\\nElastic Stack ........................................................\", \"....................................................................................................\", \"..........131\\nWhat are the advantages of Elastic Stack? ............................................\", \"..............................................................132\\nLogstash..........................\", \"....................................................................................................\", \"...............................................132\\nElasticsearch ...................................\", \"....................................................................................................\", \"..............................133\\nVisualizing information with Kibana web dashboards ...............\", \".....................................................................133\\nInstalling Elastic Stack on\", \" Azure..............................................................................................\", \".................................134\\nReferences.....................................................\", \"....................................................................................................\", \"................134\\nMonitoring in Azure Kubernetes Services.........................................\", \"........................................................................134\\nAzure Monitor for Contai\", \"ners ...............................................................................................\", \"....................................134\\nLog.Finalize() .............................................\", \"....................................................................................................\", \"...................136\\nAzure Monitor................................................................\", \"....................................................................................................\", \"..136\\nGathering logs and metrics....................................................................\", \"....................................................................137\\nReporting data .............\", \"....................................................................................................\", \"...............................................137\\nDashboards.......................................\", \"....................................................................................................\", \"............................138\\nAlerts .............................................................\", \"....................................................................................................\", \"..................140\\nReferences....................................................................\", \"....................................................................................................\", \".141\\nCloud-native identity .........................................................................\", \"................................. 142\\nReferences ...................................................\", \"....................................................................................................\", \"......................142\\nAuthentication and authorization in cloud-native apps ....................\", \".................................................................142\\nReferences.....................\", \"....................................................................................................\", \"................................................143\\nAzure Active Directory .........................\", \"....................................................................................................\", \".........................143\\nReferences.............................................................\", \"....................................................................................................\", \"........143\\nIdentityServer for cloud-native applications............................................\", \"................................................................144\\nCommon web app scenarios .......\", \"....................................................................................................\", \"..........................144\\nGetting started ......................................................\", \"....................................................................................................\", \"......145\\nConfiguration.............................................................................\", \"......................................................................................145\\nJavaScript\", \" clients ...........................................................................................\", \".................................................................146\\nReferences.....................\", \"....................................................................................................\", \"................................................146vi Contents\\nCloud-native security................\", \".......................................................................................... 147\\nAzure\", \" security for cloud-native apps ....................................................................\", \"......................................................147\\nThreat modeling ..........................\", \"....................................................................................................\", \"...............................148\\nPrinciple of least privilege.....................................\", \"....................................................................................................\", \"..148\\nPenetration testing...........................................................................\", \".............................................................................149\\nMonitoring.........\", \"....................................................................................................\", \"...........................................................149\\nSecuring the build...................\", \"....................................................................................................\", \"...................................149\\nBuilding secure code ........................................\", \"....................................................................................................\", \"........150\\nBuilt-in security ......................................................................\", \".........................................................................................150\\nAzure n\", \"etwork infrastructure...............................................................................\", \"......................................................150\\nRole-based access control for restricting \", \"access to Azure resources........................................................152\\nSecurity Princi\", \"pals ...............................................................................................\", \"...........................................................152\\nRoles................................\", \"....................................................................................................\", \"................................................153\\nScopes .........................................\", \"....................................................................................................\", \"...................................154\\nDeny ........................................................\", \"....................................................................................................\", \"........................154\\nChecking access.........................................................\", \"....................................................................................................\", \".154\\nSecuring secrets...............................................................................\", \"...............................................................................155\\nAzure Key Vault..\", \"....................................................................................................\", \".........................................................155\\nKubernetes.............................\", \"....................................................................................................\", \".......................................155\\nEncryption in transit and at rest........................\", \"....................................................................................................\", \"...156\\nKeeping secure...............................................................................\", \".................................................................................160\\nDevOps ........\", \"....................................................................................................\", \"..................... 161\\nAzure DevOps..............................................................\", \"....................................................................................................\", \".....162\\nGitHub Actions.............................................................................\", \"........................................................................................163\\nSource c\", \"ontrol..............................................................................................\", \"........................................................................163\\nRepository per microserv\", \"ice ................................................................................................\", \"......................................164\\nSingle repository ........................................\", \"....................................................................................................\", \"................166\\nStandard directory structure....................................................\", \"..................................................................................167\\nTask managemen\", \"t ..................................................................................................\", \"............................................................167\\nCI/CD pipelines ....................\", \"....................................................................................................\", \"............................................169\\nAzure Builds........................................\", \"....................................................................................................\", \"..........................170\\nAzure DevOps releases ................................................\", \"................................................................................................172v\", \"ii Contents\\nEverybody gets a build pipeline.........................................................\", \"......................................................................173\\nVersioning releases.......\", \"....................................................................................................\", \".............................................173\\nFeature flags .....................................\", \"....................................................................................................\", \"................................173\\nImplementing feature flags......................................\", \"..................................................................................................17\", \"4\\nInfrastructure as code ...........................................................................\", \"............................................................................175\\nAzure Resource Manag\", \"er templates .......................................................................................\", \"...............................175\\nTerraform........................................................\", \"....................................................................................................\", \"...............176\\nAzure CLI Scripts and Tasks......................................................\", \"..................................................................................177\\nCloud Native A\", \"pplication Bundles .................................................................................\", \"..............................................178\\nDevOps Decisions .................................\", \"....................................................................................................\", \".....................180\\nReferences.................................................................\", \"....................................................................................................\", \"....180\\nSummary: Architecting cloud-native apps ....................................................\", \"................... 1811 CHAPTER 1 | Introduction to cloud-native applications\\nCHAPTER 1\\nIntroductio\", \"n to cloud\\u0002native applications\\nAnother day, at the office, working on \\u201cthe next big thing.\\u201d\\nYour cel\", \"lphone rings. It\\u2019s your friendly recruiter - the one who calls daily with exciting new \\nopportunitie\", \"s.\\nBut this time it\\u2019s different: Start-up, equity, and plenty of funding.\\nThe mention of the cloud, \", \"microservices, and cutting-edge technology pushes you over the edge.\\nFast forward a few weeks and yo\", \"u\\u2019re now a new employee in a design session architecting a major \\neCommerce application. You\\u2019re goin\", \"g to compete with the leading eCommerce sites.\\nHow will you build it?\\nIf you follow the guidance fro\", \"m past 15 years, you\\u2019ll most likely build the system shown in Figure 1.1.\\nFigure 1-1. Traditional mo\", \"nolithic design\\nYou construct a large core application containing all of your domain logic. It inclu\", \"des modules such as \\nIdentity, Catalog, Ordering, and more. They directly communicate with each othe\", \"r within a single \\nserver process. The modules share a large relational database. The core exposes f\", \"unctionality via an \\nHTML interface and a mobile app.\\nCongratulations! You just created a monolithic\", \" application.\\nNot all is bad. Monoliths offer some distinct advantages. For example, they\\u2019re straigh\", \"tforward to\\u20262 CHAPTER 1 | Introduction to cloud-native applications\\n\\u2022 build\\n\\u2022 test\\n\\u2022 deploy\\n\\u2022 troubl\", \"eshoot\\n\\u2022 vertically scale\\nMany successful apps that exist today were created as monoliths. The app i\", \"s a hit and continues to \\nevolve, iteration after iteration, adding more functionality.\\nAt some poin\", \"t, however, you begin to feel uncomfortable. You find yourself losing control of the \\napplication. A\", \"s time goes on, the feeling becomes more intense, and you eventually enter a state \\nknown as the Fea\", \"r Cycle:\\n\\u2022 The app has become so overwhelmingly complicated that no single person understands it.\\n\\u2022 \", \"You fear making changes - each change has unintended and costly side effects.\\n\\u2022 New features/fixes b\", \"ecome tricky, time-consuming, and expensive to implement.\\n\\u2022 Each release becomes as small as possibl\", \"e and requires a full deployment of the entire \\napplication.\\n\\u2022 One unstable component can crash the \", \"entire system.\\n\\u2022 New technologies and frameworks aren\\u2019t an option.\\n\\u2022 It\\u2019s difficult to implement agi\", \"le delivery methodologies.\\n\\u2022 Architectural erosion sets in as the code base deteriorates with never-\", \"ending \\u201cquick fixes.\\u201d\\n\\u2022 Finally, the consultants come in and tell you to rewrite it.\\nSound familiar?\", \"\\nMany organizations have addressed this monolithic fear cycle by adopting a cloud-native approach to\", \" \\nbuilding systems. Figure 1-2 shows the same system built applying cloud-native techniques and \\npra\", \"ctices.3 CHAPTER 1 | Introduction to cloud-native applications\\nFigure 1-2. Cloud-native design\\nNote \", \"how the application is decomposed across a set of small isolated microservices. Each service is \\nsel\", \"f-contained and encapsulates its own code, data, and dependencies. Each is deployed in a software \\nc\", \"ontainer and managed by a container orchestrator. Instead of a large relational database, each \\nserv\", \"ice owns it own datastore, the type of which vary based upon the data needs. Note how some \\nservices\", \" depend on a relational database, but other on NoSQL databases. One service stores its state \\nin a d\", \"istributed cache. Note how all traffic routes through an API Gateway service that is responsible \\nfo\", \"r routing traffic to the core back-end services and enforcing many cross-cutting concerns. Most \\nimp\", \"ortantly, the application takes full advantage of the scalability, availability, and resiliency feat\", \"ures \\nfound in modern cloud platforms.\\nCloud-native computing\\nHmm\\u2026 We just used the term, Cloud Nati\", \"ve. Your first thought might be, \\u201cWhat exactly does that \\nmean?\\u201d Another industry buzzword concocted\", \" by software vendors to market more stuff?\\u201d\\nFortunately it\\u2019s far different, and hopefully this book \", \"will help convince you.\\nWithin a short time, cloud native has become a driving trend in the software\", \" industry. It\\u2019s a new way \\nto construct large, complex systems. The approach takes full advantage of\", \" modern software \\ndevelopment practices, technologies, and cloud infrastructure. Cloud native change\", \"s the way you \\ndesign, implement, deploy, and operationalize systems.\\nUnlike the continuous hype tha\", \"t drives our industry, cloud native is for-real. Consider the Cloud Native \\nComputing Foundation (CN\", \"CF), a consortium of over 400 major corporations. Its charter is to make 4 CHAPTER 1 | Introduction \", \"to cloud-native applications\\ncloud-native computing ubiquitous across technology and cloud stacks. A\", \"s one of the most influential \\nopen-source groups, it hosts many of the fastest-growing open source-\", \"projects in GitHub. These \\nprojects include Kubernetes, Prometheus, Helm, Envoy, and gRPC.\\nThe CNCF \", \"fosters an ecosystem of open-source and vendor-neutrality. Following that lead, this book \\npresents \", \"cloud-native principles, patterns, and best practices that are technology agnostic. At the \\nsame tim\", \"e, we discuss the services and infrastructure available in the Microsoft Azure cloud for \\nconstructi\", \"ng cloud-native systems.\\nSo, what exactly is Cloud Native? Sit back, relax, and let us help you expl\", \"ore this new world.\\nWhat is Cloud Native?\\nStop what you\\u2019re doing and ask your colleagues to define t\", \"he term \\u201cCloud Native\\u201d. There\\u2019s a good \\nchance you\\u2019ll get several different answers.\\nLet\\u2019s start wit\", \"h a simple definition:\\nCloud-native architecture and technologies are an approach to designing, cons\", \"tructing, and operating \\nworkloads that are built in the cloud and take full advantage of the cloud \", \"computing model.\\nThe Cloud Native Computing Foundation provides the official definition:\\nCloud-nativ\", \"e technologies empower organizations to build and run scalable applications in modern, \\ndynamic envi\", \"ronments such as public, private, and hybrid clouds. Containers, service meshes, \\nmicroservices, imm\", \"utable infrastructure, and declarative APIs exemplify this approach.\\nThese techniques enable loosely\", \" coupled systems that are resilient, manageable, and observable. \\nCombined with robust automation, t\", \"hey allow engineers to make high-impact changes frequently and \\npredictably with minimal toil.\\nCloud\", \" native is about speed and agility. Business systems are evolving from enabling business \\ncapabiliti\", \"es to weapons of strategic transformation that accelerate business velocity and growth. It\\u2019s \\nimpera\", \"tive to get new ideas to market immediately.\\nAt the same time, business systems have also become inc\", \"reasingly complex with users demanding \\nmore. They expect rapid responsiveness, innovative features,\", \" and zero downtime. Performance \\nproblems, recurring errors, and the inability to move fast are no l\", \"onger acceptable. Your users will visit \\nyour competitor. Cloud-native systems are designed to embra\", \"ce rapid change, large scale, and \\nresilience.\\nHere are some companies who have implemented cloud-na\", \"tive techniques. Think about the speed, \\nagility, and scalability they\\u2019ve achieved.\\nCompany Experien\", \"ce\\nNetflix Has 600+ services in production. Deploys 100 \\ntimes per day.\\nUber Has 1,000+ services in \", \"production. Deploys \\nseveral thousand times each week.5 CHAPTER 1 | Introduction to cloud-native app\", \"lications\\nCompany Experience\\nWeChat Has 3,000+ services in production. Deploys \\n1,000 times a day.\\nA\", \"s you can see, Netflix, Uber, and, WeChat expose cloud-native systems that consist of many \\nindepend\", \"ent services. This architectural style enables them to rapidly respond to market conditions. \\nThey i\", \"nstantaneously update small areas of a live, complex application, without a full redeployment. \\nThey\", \" individually scale services as needed.\\nThe pillars of cloud native\\nThe speed and agility of cloud n\", \"ative derive from many factors. Foremost is cloud infrastructure. But \\nthere\\u2019s more: Five other foun\", \"dational pillars shown in Figure 1-3 also provide the bedrock for cloud\\u0002native systems.\\nFigure 1-3. \", \"Cloud-native foundational pillars\\nLet\\u2019s take some time to better understand the significance of each\", \" pillar.\\nThe cloud\\nCloud-native systems take full advantage of the cloud service model.\\nDesigned to \", \"thrive in a dynamic, virtualized cloud environment, these systems make extensive use of \\nPlatform as\", \" a Service (PaaS) compute infrastructure and managed services. They treat the underlying \\ninfrastruc\", \"ture as disposable - provisioned in minutes and resized, scaled, or destroyed on demand \\u2013\\nvia automa\", \"tion.\\nConsider the widely accepted DevOps concept of Pets vs. Cattle. In a traditional data center, \", \"servers \\nare treated as Pets: a physical machine, given a meaningful name, and cared for. You scale \", \"by adding \\nmore resources to the same machine (scaling up). If the server becomes sick, you nurse it\", \" back to \\nhealth. Should the server become unavailable, everyone notices.\\nThe Cattle service model i\", \"s different. You provision each instance as a virtual machine or container. \\nThey\\u2019re identical and a\", \"ssigned a system identifier such as Service-01, Service-02, and so on. You scale \\nby creating more o\", \"f them (scaling out). When one becomes unavailable, nobody notices.6 CHAPTER 1 | Introduction to clo\", \"ud-native applications\\nThe cattle model embraces immutable infrastructure. Servers aren\\u2019t repaired o\", \"r modified. If one fails or \\nrequires updating, it\\u2019s destroyed and a new one is provisioned \\u2013 all do\", \"ne via automation.\\nCloud-native systems embrace the Cattle service model. They continue to run as th\", \"e infrastructure \\nscales in or out with no regard to the machines upon which they\\u2019re running.\\nThe Az\", \"ure cloud platform supports this type of highly elastic infrastructure with automatic scaling, \\nself\", \"-healing, and monitoring capabilities.\\nModern design\\nHow would you design a cloud-native app? What w\", \"ould your architecture look like? To what \\nprinciples, patterns, and best practices would you adhere\", \"? What infrastructure and operational \\nconcerns would be important?\\nThe Twelve-Factor Application\\nA \", \"widely accepted methodology for constructing cloud-based applications is the Twelve-Factor \\nApplicat\", \"ion. It describes a set of principles and practices that developers follow to construct \\napplication\", \"s optimized for modern cloud environments. Special attention is given to portability across \\nenviron\", \"ments and declarative automation.\\nWhile applicable to any web-based application, many practitioners \", \"consider Twelve-Factor a solid \\nfoundation for building cloud-native apps. Systems built upon these \", \"principles can deploy and scale \\nrapidly and add features to react quickly to market changes.\\nThe fo\", \"llowing table highlights the Twelve-Factor methodology:\\nFactor Explanation\\n1 - Code Base A single co\", \"de base for each microservice, stored \\nin its own repository. Tracked with version \\ncontrol, it can \", \"deploy to multiple environments \\n(QA, Staging, Production).\\n2 - Dependencies Each microservice isola\", \"tes and packages its own \\ndependencies, embracing changes without \\nimpacting the entire system.\\n3 - \", \"Configurations Configuration information is moved out of the \\nmicroservice and externalized through \", \"a \\nconfiguration management tool outside of the \\ncode. The same deployment can propagate \\nacross env\", \"ironments with the correct \\nconfiguration applied.\\n4 - Backing Services Ancillary resources (data st\", \"ores, caches, \\nmessage brokers) should be exposed via an \\naddressable URL. Doing so decouples the \\nr\", \"esource from the application, enabling it to be \\ninterchangeable.7 CHAPTER 1 | Introduction to cloud\", \"-native applications\\nFactor Explanation\\n5 - Build, Release, Run Each release must enforce a strict s\", \"eparation \\nacross the build, release, and run stages. Each \\nshould be tagged with a unique ID and su\", \"pport \\nthe ability to roll back. Modern CI/CD systems \\nhelp fulfill this principle.\\n6 - Processes Ea\", \"ch microservice should execute in its own \\nprocess, isolated from other running services. \\nExternali\", \"ze required state to a backing service \\nsuch as a distributed cache or data store.\\n7 - Port Binding \", \"Each microservice should be self-contained with \\nits interfaces and functionality exposed on its \\now\", \"n port. Doing so provides isolation from \\nother microservices.\\n8 - Concurrency When capacity needs t\", \"o increase, scale out \\nservices horizontally across multiple identical \\nprocesses (copies) as oppose\", \"d to scaling-up a \\nsingle large instance on the most powerful \\nmachine available. Develop the applic\", \"ation to be \\nconcurrent making scaling out in cloud \\nenvironments seamless.\\n9 - Disposability Servic\", \"e instances should be disposable. Favor \\nfast startup to increase scalability opportunities \\nand gra\", \"ceful shutdowns to leave the system in a \\ncorrect state. Docker containers along with an \\norchestrat\", \"or inherently satisfy this requirement.\\n10 - Dev/Prod Parity Keep environments across the applicatio\", \"n \\nlifecycle as similar as possible, avoiding costly \\nshortcuts. Here, the adoption of containers ca\", \"n \\ngreatly contribute by promoting the same \\nexecution environment.\\n11 - Logging Treat logs generate\", \"d by microservices as event \\nstreams. Process them with an event \\naggregator. Propagate log data to \", \"data\\u0002mining/log management tools like Azure \\nMonitor or Splunk and eventually to long-term \\narchival\", \".\\n12 - Admin Processes Run administrative/management tasks, such as \\ndata cleanup or computing analy\", \"tics, as one-off \\nprocesses. Use independent tools to invoke \\nthese tasks from the production enviro\", \"nment, \\nbut separately from the application.8 CHAPTER 1 | Introduction to cloud-native applications\\n\", \"In the book, Beyond the Twelve-Factor App, author Kevin Hoffman details each of the original 12 \\nfac\", \"tors (written in 2011). Additionally, he discusses three extra factors that reflect today\\u2019s modern \\n\", \"cloud application design.\\nNew Factor Explanation\\n13 - API First Make everything a service. Assume yo\", \"ur code \\nwill be consumed by a front-end client, gateway, \\nor another service.\\n14 - Telemetry On a w\", \"orkstation, you have deep visibility into \\nyour application and its behavior. In the cloud, \\nyou don\", \"\\u2019t. Make sure your design includes the \\ncollection of monitoring, domain-specific, and \\nhealth/syste\", \"m data.\\n15 - Authentication/ Authorization Implement identity from the start. Consider \\nRBAC (role-b\", \"ased access control) features \\navailable in public clouds.\\nWe\\u2019ll refer to many of the 12+ factors in\", \" this chapter and throughout the book.\\nAzure Well-Architected Framework\\nDesigning and deploying clou\", \"d-based workloads can be challenging, especially when implementing \\ncloud-native architecture. Micro\", \"soft provides industry standard best practices to help you and your \\nteam deliver robust cloud solut\", \"ions.\\nThe Microsoft Well-Architected Framework provides a set of guiding tenets that can be used to \", \"\\nimprove the quality of a cloud-native workload. The framework consists of five pillars of architect\", \"ure \\nexcellence:\\nTenets Description\\nCost management Focus on generating incremental value early. \\nAp\", \"ply Build-Measure-Learn principles to \\naccelerate time to market while avoiding \\ncapital-intensive s\", \"olutions. Using a pay-as-you\\u0002go strategy, invest as you scale out, rather than \\ndelivering a large i\", \"nvestment up front.\\nOperational excellence Automate the environment and operations to \\nincrease spee\", \"d and reduce human error. Roll \\nproblem updates back or forward quickly. \\nImplement monitoring and d\", \"iagnostics from the \\nstart.\\nPerformance efficiency Efficiently meet demands placed on your \\nworkload\", \"s. Favor horizontal scaling (scaling out) \\nand design it into your systems. Continually \\nconduct per\", \"formance and load testing to \\nidentify potential bottlenecks.9 CHAPTER 1 | Introduction to cloud-nat\", \"ive applications\\nTenets Description\\nReliability Build workloads that are both resilient and \\navailab\", \"le. Resiliency enables workloads to \\nrecover from failures and continue functioning. \\nAvailability e\", \"nsures users access to your \\nworkload at all times. Design applications to \\nexpect failures and reco\", \"ver from them.\\nSecurity Implement security across the entire lifecycle of \\nan application, from desi\", \"gn and implementation \\nto deployment and operations. Pay close \\nattention to identity management, in\", \"frastructure \\naccess, application security, and data \\nsovereignty and encryption.\\nTo get started, Mi\", \"crosoft provides a set of online assessments to help you assess your current cloud \\nworkloads agains\", \"t the five well-architected pillars.\\nMicroservices\\nCloud-native systems embrace microservices, a pop\", \"ular architectural style for constructing modern \\napplications.\\nBuilt as a distributed set of small,\", \" independent services that interact through a shared fabric, \\nmicroservices share the following char\", \"acteristics:\\n\\u2022 Each implements a specific business capability within a larger domain context.\\n\\u2022 Each\", \" is developed autonomously and can be deployed independently.\\n\\u2022 Each is self-contained encapsulating\", \" its own data storage technology, dependencies, and \\nprogramming platform.\\n\\u2022 Each runs in its own pr\", \"ocess and communicates with others using standard communication \\nprotocols such as HTTP/HTTPS, gRPC,\", \" WebSockets, or AMQP.\\n\\u2022 They compose together to form an application.\\nFigure 1-4 contrasts a monolit\", \"hic application approach with a microservices approach. Note how the \\nmonolith is composed of a laye\", \"red architecture, which executes in a single process. It typically \\nconsumes a relational database. \", \"The microservice approach, however, segregates functionality into \\nindependent services, each with i\", \"ts own logic, state, and data. Each microservice hosts its own \\ndatastore.10 CHAPTER 1 | Introductio\", \"n to cloud-native applications\\nFigure 1-4. Monolithic versus microservices architecture\\nNote how mic\", \"roservices promote the Processes principle from the Twelve-Factor Application, \\ndiscussed earlier in\", \" the chapter.\\nFactor #6 specifies \\u201cEach microservice should execute in its own process, isolated fro\", \"m other running \\nservices.\\u201d\\nWhy microservices?\\nMicroservices provide agility.\\nEarlier in the chapter\", \", we compared an eCommerce application built as a monolith to that with \\nmicroservices. In the examp\", \"le, we saw some clear benefits:\\n\\u2022 Each microservice has an autonomous lifecycle and can evolve indep\", \"endently and deploy \\nfrequently. You don\\u2019t have to wait for a quarterly release to deploy a new feat\", \"ure or update. \\nYou can update a small area of a live application with less risk of disrupting the e\", \"ntire system. \\nThe update can be made without a full redeployment of the application.\\n\\u2022 Each microse\", \"rvice can scale independently. Instead of scaling the entire application as a single \\nunit, you scal\", \"e out only those services that require more processing power to meet desired \\nperformance levels and\", \" service-level agreements. Fine-grained scaling provides for greater \\ncontrol of your system and hel\", \"ps reduce overall costs as you scale portions of your system, \\nnot everything.\\nAn excellent referenc\", \"e guide for understanding microservices is .NET Microservices: Architecture for \\nContainerized .NET \", \"Applications. The book deep dives into microservices design and architecture. It\\u2019s \\na companion for \", \"a full-stack microservice reference architecture available as a free download from \\nMicrosoft.\\nDevel\", \"oping microservices\\nMicroservices can be created upon any modern development platform.11 CHAPTER 1 |\", \" Introduction to cloud-native applications\\nThe Microsoft .NET platform is an excellent choice. Free \", \"and open source, it has many built-in features \\nthat simplify microservice development. .NET is cros\", \"s-platform. Applications can be built and run on \\nWindows, macOS, and most flavors of Linux.\\n.NET is\", \" highly performant and has scored well in comparison to Node.js and other competing \\nplatforms. Inte\", \"restingly, TechEmpower conducted an extensive set of performance benchmarks across \\nmany web applica\", \"tion platforms and frameworks. .NET scored in the top 10 - well above Node.js and \\nother competing p\", \"latforms.\\n.NET is maintained by Microsoft and the .NET community on GitHub.\\nMicroservice challenges\\n\", \"While distributed cloud-native microservices can provide immense agility and speed, they present \\nma\", \"ny challenges:\\nCommunication\\nHow will front-end client applications communicate with backed-end core\", \" microservices? Will you \\nallow direct communication? Or, might you abstract the back-end microservi\", \"ces with a gateway \\nfacade that provides flexibility, control, and security?\\nHow will back-end core \", \"microservices communicate with each other? Will you allow direct HTTP calls \\nthat can increase coupl\", \"ing and impact performance and agility? Or might you consider decoupled \\nmessaging with queue and to\", \"pic technologies?\\nCommunication is covered in the Cloud-native communication patterns chapter.\\nResil\", \"iency\\nA microservices architecture moves your system from in-process to out-of-process network \\ncomm\", \"unication. In a distributed architecture, what happens when Service B isn\\u2019t responding to a \\nnetwork\", \" call from Service A? Or, what happens when Service C becomes temporarily unavailable and \\nother ser\", \"vices calling it become blocked?\\nResiliency is covered in the Cloud-native resiliency chapter.\\nDistr\", \"ibuted Data\\nBy design, each microservice encapsulates its own data, exposing operations via its publ\", \"ic interface. If \\nso, how do you query data or implement a transaction across multiple services?\\nDis\", \"tributed data is covered in the Cloud-native data patterns chapter.\\nSecrets\\nHow will your microservi\", \"ces securely store and manage secrets and sensitive configuration data?\\nSecrets are covered in detai\", \"l Cloud-native security.12 CHAPTER 1 | Introduction to cloud-native applications\\nManage Complexity w\", \"ith Dapr\\nDapr is a distributed, open-source application runtime. Through an architecture of pluggabl\", \"e \\ncomponents, it dramatically simplifies the plumbing behind distributed applications. It provides \", \"a \\ndynamic glue that binds your application with pre-built infrastructure capabilities and component\", \"s \\nfrom the Dapr runtime. Figure 1-5 shows Dapr from 20,000 feet.\\nFigure 1-5. Dapr at 20,000 feet.\\nI\", \"n the top row of the figure, note how Dapr provides language-specific SDKs for popular development \\n\", \"platforms. Dapr v1 includes support for .NET, Go, Node.js, Python, PHP, Java, and JavaScript.\\nWhile \", \"language-specific SDKs enhance the developer experience, Dapr is platform agnostic. Under the \\nhood,\", \" Dapr\\u2019s programming model exposes capabilities through standard HTTP/gRPC communication \\nprotocols. \", \"Any programming platform can call Dapr via its native HTTP and gRPC APIs.\\nThe blue boxes across the \", \"center of the figure represent the Dapr building blocks. Each exposes pre\\u0002built plumbing code for a \", \"distributed application capability that your application can consume.\\nThe components row represents \", \"a large set of pre-defined infrastructure components that your \\napplication can consume. Think of co\", \"mponents as infrastructure code you don\\u2019t have to write.\\nThe bottom row highlights the portability o\", \"f Dapr and the diverse environments across which it can \\nrun.\\nMicrosoft features a free ebook Dapr f\", \"or .NET Developers for learning Dapr.\\nLooking ahead, Dapr has the potential to have a profound impac\", \"t on cloud-native application \\ndevelopment.\\nContainers\\nIt\\u2019s natural to hear the term container menti\", \"oned in any cloud native conversation. In the book, Cloud \\nNative Patterns, author Cornelia Davis ob\", \"serves that, \\u201cContainers are a great enabler of cloud-native 13 CHAPTER 1 | Introduction to cloud-na\", \"tive applications\\nsoftware.\\u201d The Cloud Native Computing Foundation places microservice containerizat\", \"ion as the first \\nstep in their Cloud-Native Trail Map - guidance for enterprises beginning their cl\", \"oud-native journey.\\nContainerizing a microservice is simple and straightforward. The code, its depen\", \"dencies, and runtime \\nare packaged into a binary called a container image. Images are stored in a co\", \"ntainer registry, which \\nacts as a repository or library for images. A registry can be located on yo\", \"ur development computer, in \\nyour data center, or in a public cloud. Docker itself maintains a publi\", \"c registry via Docker Hub. The \\nAzure cloud features a private container registry to store container\", \" images close to the cloud \\napplications that will run them.\\nWhen an application starts or scales, y\", \"ou transform the container image into a running container \\ninstance. The instance runs on any comput\", \"er that has a container runtime engine installed. You can \\nhave as many instances of the containeriz\", \"ed service as needed.\\nFigure 1-6 shows three different microservices, each in its own container, all\", \" running on a single host.\\nFigure 1-6. Multiple containers running on a container host\\nNote how each\", \" container maintains its own set of dependencies and runtime, which can be different \\nfrom one anoth\", \"er. Here, we see different versions of the Product microservice running on the same \\nhost. Each cont\", \"ainer shares a slice of the underlying host operating system, memory, and processor, \\nbut is isolate\", \"d from one another.\\nNote how well the container model embraces the Dependencies principle from the T\", \"welve-Factor \\nApplication.\\nFactor #2 specifies that \\u201cEach microservice isolates and packages its own\", \" dependencies, embracing \\nchanges without impacting the entire system.\\u201d\\nContainers support both Linu\", \"x and Windows workloads. The Azure cloud openly embraces both. \\nInterestingly, it\\u2019s Linux, not Windo\", \"ws Server, that has become the more popular operating system in \\nAzure.14 CHAPTER 1 | Introduction t\", \"o cloud-native applications\\nWhile several container vendors exist, Docker has captured the lion\\u2019s sh\", \"are of the market. The \\ncompany has been driving the software container movement. It has become the \", \"de facto standard for \\npackaging, deploying, and running cloud-native applications.\\nWhy containers?\\n\", \"Containers provide portability and guarantee consistency across environments. By encapsulating \\never\", \"ything into a single package, you isolate the microservice and its dependencies from the \\nunderlying\", \" infrastructure.\\nYou can deploy the container in any environment that hosts the Docker runtime engin\", \"e. Containerized \\nworkloads also eliminate the expense of pre-configuring each environment with fram\", \"eworks, software \\nlibraries, and runtime engines.\\nBy sharing the underlying operating system and hos\", \"t resources, a container has a much smaller \\nfootprint than a full virtual machine. The smaller size\", \" increases the density, or number of \\nmicroservices, that a given host can run at one time.\\nContaine\", \"r orchestration\\nWhile tools such as Docker create images and run containers, you also need tools to \", \"manage them. \\nContainer management is done with a special software program called a container orches\", \"trator. \\nWhen operating at scale with many independent running containers, orchestration is essentia\", \"l.\\nFigure 1-7 shows management tasks that container orchestrators automate.\\nFigure 1-7. What contain\", \"er orchestrators do\\nThe following table describes common orchestration tasks.\\nTasks Explanation\\nSche\", \"duling Automatically provision container instances.\\nAffinity/anti-affinity Provision containers near\", \"by or far apart from \\neach other, helping availability and \\nperformance.\\nHealth monitoring Automatic\", \"ally detect and correct failures.\\nFailover Automatically reprovision a failed instance to a \\nhealthy\", \" machine.15 CHAPTER 1 | Introduction to cloud-native applications\\nTasks Explanation\\nScaling Automati\", \"cally add or remove a container \\ninstance to meet demand.\\nNetworking Manage a networking overlay for\", \" container \\ncommunication.\\nService Discovery Enable containers to locate each other.\\nRolling Upgrade\", \"s Coordinate incremental upgrades with zero \\ndowntime deployment. Automatically roll back \\nproblemat\", \"ic changes.\\nNote how container orchestrators embrace the Disposability and Concurrency principles fr\", \"om the \\nTwelve-Factor Application.\\nFactor #9 specifies that \\u201cService instances should be disposable,\", \" favoring fast startups to increase \\nscalability opportunities and graceful shutdowns to leave the s\", \"ystem in a correct state.\\u201d Docker \\ncontainers along with an orchestrator inherently satisfy this req\", \"uirement.\\u201d\\nFactor #8 specifies that \\u201cServices scale out across a large number of small identical pro\", \"cesses (copies) as \\nopposed to scaling-up a single large instance on the most powerful machine avail\", \"able.\\u201d\\nWhile several container orchestrators exist, Kubernetes has become the de facto standard for \", \"the \\ncloud-native world. It\\u2019s a portable, extensible, open-source platform for managing containerize\", \"d \\nworkloads.\\nYou could host your own instance of Kubernetes, but then you\\u2019d be responsible for prov\", \"isioning and \\nmanaging its resources - which can be complex. The Azure cloud features Kubernetes as \", \"a managed \\nservice. Both Azure Kubernetes Service (AKS) and Azure Red Hat OpenShift (ARO) enable you\", \" to fully \\nleverage the features and power of Kubernetes as a managed service, without having to ins\", \"tall and \\nmaintain it.\\nContainer orchestration is covered in detail in Scaling Cloud-Native Applicat\", \"ions.\\nBacking services\\nCloud-native systems depend upon many different ancillary resources, such as \", \"data stores, message \\nbrokers, monitoring, and identity services. These services are known as backin\", \"g services.\\nFigure 1-8 shows many common backing services that cloud-native systems consume.16 CHAPT\", \"ER 1 | Introduction to cloud-native applications\\nFigure 1-8. Common backing services\\nYou could host \", \"your own backing services, but then you\\u2019d be responsible for licensing, provisioning, \\nand managing \", \"those resources.\\nCloud providers offer a rich assortment of managed backing services. Instead of own\", \"ing the service, \\nyou simply consume it. The cloud provider operates the resource at scale and bears\", \" the responsibility \\nfor performance, security, and maintenance. Monitoring, redundancy, and availab\", \"ility are built into the \\nservice. Providers guarantee service level performance and fully support t\", \"heir managed services -\\nopen a ticket and they fix your issue.\\nCloud-native systems favor managed ba\", \"cking services from cloud vendors. The savings in time and \\nlabor can be significant. The operationa\", \"l risk of hosting your own and experiencing trouble can get \\nexpensive fast.\\nA best practice is to t\", \"reat a backing service as an attached resource, dynamically bound to a \\nmicroservice with configurat\", \"ion information (a URL and credentials) stored in an external \\nconfiguration. This guidance is spell\", \"ed out in the Twelve-Factor Application, discussed earlier in the \\nchapter.\\nFactor #4 specifies that\", \" backing services \\u201cshould be exposed via an addressable URL. Doing so \\ndecouples the resource from t\", \"he application, enabling it to be interchangeable.\\u201d\\nFactor #3 specifies that \\u201cConfiguration informat\", \"ion is moved out of the microservice and externalized \\nthrough a configuration management tool outsi\", \"de of the code.\\u201d\\nWith this pattern, a backing service can be attached and detached without code chan\", \"ges. You might \\npromote a microservice from QA to a staging environment. You update the microservice\", \" \\nconfiguration to point to the backing services in staging and inject the settings into your contai\", \"ner \\nthrough an environment variable.17 CHAPTER 1 | Introduction to cloud-native applications\\nCloud \", \"vendors provide APIs for you to communicate with their proprietary backing services. These \\nlibrarie\", \"s encapsulate the proprietary plumbing and complexity. However, communicating directly with \\nthese A\", \"PIs will tightly couple your code to that specific backing service. It\\u2019s a widely accepted practice \", \"\\nto insulate the implementation details of the vendor API. Introduce an intermediation layer, or \\nin\", \"termediate API, exposing generic operations to your service code and wrap the vendor code inside \\nit\", \". This loose coupling enables you to swap out one backing service for another or move your code to \\n\", \"a different cloud environment without having to make changes to the mainline service code. Dapr, \\ndi\", \"scussed earlier, follows this model with its set of prebuilt building blocks.\\nOn a final thought, ba\", \"cking services also promote the Statelessness principle from the Twelve-Factor \\nApplication, discuss\", \"ed earlier in the chapter.\\nFactor #6 specifies that, \\u201cEach microservice should execute in its own pr\", \"ocess, isolated from other \\nrunning services. Externalize required state to a backing service such a\", \"s a distributed cache or data \\nstore.\\u201d\\nBacking services are discussed in Cloud-native data patterns \", \"and Cloud-native communication \\npatterns.\\nAutomation\\nAs you\\u2019ve seen, cloud-native systems embrace mi\", \"croservices, containers, and modern system design \\nto achieve speed and agility. But, that\\u2019s only pa\", \"rt of the story. How do you provision the cloud \\nenvironments upon which these systems run? How do y\", \"ou rapidly deploy app features and updates? \\nHow do you round out the full picture?\\nEnter the widely\", \" accepted practice of Infrastructure as Code, or IaC.\\nWith IaC, you automate platform provisioning a\", \"nd application deployment. You essentially apply \\nsoftware engineering practices such as testing and\", \" versioning to your DevOps practices. Your \\ninfrastructure and deployments are automated, consistent\", \", and repeatable.\\nAutomating infrastructure\\nTools like Azure Resource Manager, Azure Bicep, Terrafor\", \"m from HashiCorp, and the Azure CLI, enable \\nyou to declaratively script the cloud infrastructure yo\", \"u require. Resource names, locations, capacities, \\nand secrets are parameterized and dynamic. The sc\", \"ript is versioned and checked into source control \\nas an artifact of your project. You invoke the sc\", \"ript to provision a consistent and repeatable \\ninfrastructure across system environments, such as QA\", \", staging, and production.\\nUnder the hood, IaC is idempotent, meaning that you can run the same scri\", \"pt over and over without \\nside effects. If the team needs to make a change, they edit and rerun the \", \"script. Only the updated \\nresources are affected.\\nIn the article, What is Infrastructure as Code, Au\", \"thor Sam Guckenheimer describes how, \\u201cTeams who \\nimplement IaC can deliver stable environments rapid\", \"ly and at scale. They avoid manual configuration \\nof environments and enforce consistency by represe\", \"nting the desired state of their environments via \\ncode. Infrastructure deployments with IaC are rep\", \"eatable and prevent runtime issues caused by \\nconfiguration drift or missing dependencies. DevOps te\", \"ams can work together with a unified set of 18 CHAPTER 1 | Introduction to cloud-native applications\", \"\\npractices and tools to deliver applications and their supporting infrastructure rapidly, reliably, \", \"and at \\nscale.\\u201d\\nAutomating deployments\\nThe Twelve-Factor Application, discussed earlier, calls for s\", \"eparate steps when transforming \\ncompleted code into a running application.\\nFactor #5 specifies that\", \" \\u201cEach release must enforce a strict separation across the build, release and run \\nstages. Each shou\", \"ld be tagged with a unique ID and support the ability to roll back.\\u201d\\nModern CI/CD systems help fulfi\", \"ll this principle. They provide separate build and delivery steps that \\nhelp ensure consistent and q\", \"uality code that\\u2019s readily available to users.\\nFigure 1-9 shows the separation across the deployment\", \" process.\\nFigure 1-9. Deployment steps in a CI/CD Pipeline\\nIn the previous figure, pay special atten\", \"tion to separation of tasks:\\n1. The developer constructs a feature in their development environment,\", \" iterating through what \\nis called the \\u201cinner loop\\u201d of code, run, and debug.\\n2. When complete, that \", \"code is pushed into a code repository, such as GitHub, Azure DevOps, or \\nBitBucket.\\n3. The push trig\", \"gers a build stage that transforms the code into a binary artifact. The work is \\nimplemented with a \", \"Continuous Integration (CI) pipeline. It automatically builds, tests, and \\npackages the application.\", \"\\n4. The release stage picks up the binary artifact, applies external application and environment \\nco\", \"nfiguration information, and produces an immutable release. The release is deployed to a \\nspecified \", \"environment. The work is implemented with a Continuous Delivery (CD) pipeline. \\nEach release should \", \"be identifiable. You can say, \\u201cThis deployment is running Release 2.1.1 of \\nthe application.\\u201d19 CHAP\", \"TER 1 | Introduction to cloud-native applications\\n5. Finally, the released feature is run in the tar\", \"get execution environment. Releases are \\nimmutable meaning that any change must create a new release\", \".\\nApplying these practices, organizations have radically evolved how they ship software. Many have \\n\", \"moved from quarterly releases to on-demand updates. The goal is to catch problems early in the \\ndeve\", \"lopment cycle when they\\u2019re less expensive to fix. The longer the duration between integrations, \\nthe\", \" more expensive problems become to resolve. With consistency in the integration process, teams \\ncan \", \"commit code changes more frequently, leading to better collaboration and software quality.\\nInfrastru\", \"cture as code and deployment automation, along with GitHub and Azure DevOps are \\ndiscussed in detail\", \" in DevOps.\\nCandidate apps for cloud native\\nThink about the apps your organization needs to build. T\", \"hen, look at the existing apps in your \\nportfolio. How many of them warrant a cloud-native architect\", \"ure? All of them? Perhaps some?\\nApplying cost/benefit analysis, there\\u2019s a good chance some wouldn\\u2019t \", \"support the effort. The cost of \\nbecoming cloud native would far exceed the business value of the ap\", \"plication.\\nWhat type of application might be a candidate for cloud native?\\n\\u2022 Strategic enterprise sy\", \"stems that need to constantly evolve business capabilities/features\\n\\u2022 An application that requires a\", \" high release velocity - with high confidence\\n\\u2022 A system where individual features must release with\", \"out a full redeployment of the entire \\nsystem\\n\\u2022 An application developed by teams with expertise in \", \"different technology stacks\\n\\u2022 An application with components that must scale independently\\nSmaller, \", \"less impactful line-of-business applications might fare well with a simple monolithic \\narchitecture \", \"hosted in a Cloud PaaS environment.\\nThen there are legacy systems. While we\\u2019d all like to build new \", \"applications, we\\u2019re often responsible \\nfor modernizing legacy workloads that are critical to the bus\", \"iness.\\nModernizing legacy apps\\nThe free Microsoft e-book Modernize existing .NET applications with A\", \"zure cloud and Windows \\nContainers provides guidance about migrating on-premises workloads into clou\", \"d. Figure 1-10 shows \\nthat there isn\\u2019t a single, one-size-fits-all strategy for modernizing legacy a\", \"pplications.20 CHAPTER 1 | Introduction to cloud-native applications\\nFigure 1-10. Strategies for mig\", \"rating legacy workloads\\nMonolithic apps that are non-critical might benefit from a quick lift-and-sh\", \"ift (Cloud Infrastructure\\u0002Ready) migration. Here, the on-premises workload is rehosted to a cloud-ba\", \"sed VM, without changes. \\nThis approach uses the IaaS (Infrastructure as a Service) model. Azure inc\", \"ludes several tools such as \\nAzure Migrate, Azure Site Recovery, and Azure Database Migration Servic\", \"e to help streamline the \\nmove. While this strategy can yield some cost savings, such applications t\", \"ypically weren\\u2019t designed to \\nunlock and leverage the benefits of cloud computing.\\nLegacy apps that \", \"are critical to the business often benefit from an enhanced Cloud Optimized\\nmigration. This approach\", \" includes deployment optimizations that enable key cloud services - without \\nchanging the core archi\", \"tecture of the application. For example, you might containerize the application \\nand deploy it to a \", \"container orchestrator, like Azure Kubernetes Services, discussed later in this book. \\nOnce in the c\", \"loud, the application can consume cloud backing services such as databases, message \\nqueues, monitor\", \"ing, and distributed caching.\\nFinally, monolithic apps that provide strategic enterprise functions m\", \"ight best benefit from a Cloud\\u0002Native approach, the subject of this book. This approach provides agi\", \"lity and velocity. But, it comes at \\na cost of replatforming, rearchitecting, and rewriting code. Ov\", \"er time, a legacy application could be \\ndecomposed into microservices, containerized, and ultimately\", \" replatformed into a cloud-native \\narchitecture.\\nIf you and your team believe a cloud-native approac\", \"h is appropriate, it behooves you to rationalize \\nthe decision with your organization. What exactly \", \"is the business problem that a cloud-native \\napproach will solve? How would it align with business n\", \"eeds?\\n\\u2022 Rapid releases of features with increased confidence?\\n\\u2022 Fine-grained scalability - more effi\", \"cient usage of resources?21 CHAPTER 1 | Introduction to cloud-native applications\\n\\u2022 Improved system \", \"resiliency?\\n\\u2022 Improved system performance?\\n\\u2022 More visibility into operations?\\n\\u2022 Blend development pl\", \"atforms and data stores to arrive at the best tool for the job?\\n\\u2022 Future-proof application investmen\", \"t?\\nThe right migration strategy depends on organizational priorities and the systems you\\u2019re targetin\", \"g. \\nFor many, it may be more cost effective to cloud-optimize a monolithic application or add coarse\", \"\\u0002grained services to an N-Tier app. In these cases, you can still make full use of cloud PaaS capabi\", \"lities \\nlike the ones offered by Azure App Service.\\nSummary\\nIn this chapter, we introduced cloud-nat\", \"ive computing. We provided a definition along with the key \\ncapabilities that drive a cloud-native a\", \"pplication. We looked at the types of applications that might \\njustify this investment and effort.\\nW\", \"ith the introduction behind, we now dive into a much more detailed look at cloud native.\\nReferences\\n\", \"\\u2022 Cloud Native Computing Foundation\\n\\u2022 .NET Microservices: Architecture for Containerized .NET applic\", \"ations\\n\\u2022 Microsoft Azure Well-Architected Framework\\n\\u2022 Modernize existing .NET applications with Azur\", \"e cloud and Windows Containers\\n\\u2022 Cloud Native Patterns by Cornelia Davis\\n\\u2022 Cloud native applications\", \": Ship faster, reduce risk, and grow your business\\n\\u2022 Dapr for .NET Developers\\n\\u2022 Dapr documents\\n\\u2022 Bey\", \"ond the Twelve-Factor Application\\n\\u2022 What is Infrastructure as Code\\n\\u2022 Uber Engineering\\u2019s Micro Deploy\", \": Deploying Daily with Confidence\\n\\u2022 How Netflix Deploys Code\\n\\u2022 Overload Control for Scaling WeChat M\", \"icroservices22 CHAPTER 2 | Introducing eShopOnContainers reference app\\nCHAPTER 2\\nIntroducing \\neShopO\", \"nContainers \\nreference app\\nMicrosoft, in partnership with leading community experts, has produced a \", \"full-featured cloud-native \\nmicroservices reference application, eShopOnContainers. This application\", \" is built to showcase using \\n.NET and Docker, and optionally Azure, Kubernetes, and Visual Studio, t\", \"o build an online storefront.\\nFigure 2-1. eShopOnContainers Sample App Screenshot.23 CHAPTER 2 | Int\", \"roducing eShopOnContainers reference app\\nBefore starting this chapter, we recommend that you downloa\", \"d the eShopOnContainers reference \\napplication. If you do so, it should be easier for you to follow \", \"along with the information presented.\\nFeatures and requirements\\nLet\\u2019s start with a review of the app\", \"lication\\u2019s features and requirements. The eShopOnContainers \\napplication represents an online store \", \"that sells various physical products like t-shirts and coffee \\nmugs. If you\\u2019ve bought anything onlin\", \"e before, the experience of using the store should be relatively \\nfamiliar. Here are some of the bas\", \"ic features the store implements:\\n\\u2022 List catalog items\\n\\u2022 Filter items by type\\n\\u2022 Filter items by bran\", \"d\\n\\u2022 Add items to the shopping basket\\n\\u2022 Edit or remove items from the basket\\n\\u2022 Checkout\\n\\u2022 Register an\", \" account\\n\\u2022 Sign in\\n\\u2022 Sign out\\n\\u2022 Review orders\\nThe application also has the following non-functional \", \"requirements:\\n\\u2022 It needs to be highly available and it must scale automatically to meet increased tr\", \"affic (and \\nscale back down once traffic subsides).\\n\\u2022 It should provide easy-to-use monitoring of it\", \"s health and diagnostic logs to help \\ntroubleshoot any issues it encounters.\\n\\u2022 It should support an \", \"agile development process, including support for continuous integration \\nand deployment (CI/CD).\\n\\u2022 I\", \"n addition to the two web front ends (traditional and Single Page Application), the \\napplication mus\", \"t also support mobile client apps running different kinds of operating \\nsystems.\\n\\u2022 It should support\", \" cross-platform hosting and cross-platform development.24 CHAPTER 2 | Introducing eShopOnContainers \", \"reference app\\nFigure 2-2. eShopOnContainers reference application development architecture.\\nThe eSho\", \"pOnContainers application is accessible from web or mobile clients that access the \\napplication over\", \" HTTPS targeting either the ASP.NET Core MVC server application or an appropriate \\nAPI Gateway. API \", \"Gateways offer several advantages, such as decoupling back-end services from \\nindividual front-end c\", \"lients and providing better security. The application also makes use of a related \\npattern known as \", \"Backends-for-Frontends (BFF), which recommends creating separate API gateways \\nfor each front-end cl\", \"ient. The reference architecture demonstrates breaking up the API gateways \\nbased on whether the req\", \"uest is coming from a web or mobile client.\\nThe application\\u2019s functionality is broken up into many d\", \"istinct microservices. There are services \\nresponsible for authentication and identity, listing item\", \"s from the product catalog, managing users\\u2019 \\nshopping baskets, and placing orders. Each of these sep\", \"arate services has its own persistent storage. \\nThere\\u2019s no single primary data store with which all \", \"services interact. Instead, coordination and \\ncommunication between the services is done on an as-ne\", \"eded basis and by using a message bus.\\nEach of the different microservices is designed differently, \", \"based on their individual requirements. This \\naspect means their technology stack may differ, althou\", \"gh they\\u2019re all built using .NET and designed for \\nthe cloud. Simpler services provide basic Create-R\", \"ead-Update-Delete (CRUD) access to the underlying \\ndata stores, while more advanced services use Dom\", \"ain-Driven Design approaches and patterns to \\nmanage business complexity.25 CHAPTER 2 | Introducing \", \"eShopOnContainers reference app\\nFigure 2-3. Different kinds of microservices.\\nOverview of the code\\nB\", \"ecause it uses microservices, the eShopOnContainers app includes quite a few separate projects and \\n\", \"solutions in its GitHub repository. In addition to separate solutions and executable files, the vari\", \"ous \\nservices are designed to run inside their own containers, both during local development and at \", \"run \\ntime in production. Figure 2-4 shows the full Visual Studio solution, in which the various diff\", \"erent \\nprojects are organized.26 CHAPTER 2 | Introducing eShopOnContainers reference app27 CHAPTER 2\", \" | Introducing eShopOnContainers reference app\\nFigure 2-4. Projects in Visual Studio solution.\\nThe c\", \"ode is organized to support the different microservices, and within each microservice, the code is \\n\", \"broken up into domain logic, infrastructure concerns, and user interface or service endpoint. In man\", \"y \\ncases, each service\\u2019s dependencies can be fulfilled by Azure services in production, and alternat\", \"ive \\noptions for local development. Let\\u2019s examine how the application\\u2019s requirements map to Azure \\ns\", \"ervices.\\nUnderstanding microservices\\nThis book focuses on cloud-native applications built using Azur\", \"e technology. To learn more about \\nmicroservices best practices and how to architect microservice-ba\", \"sed applications, read the \\ncompanion book, .NET Microservices: Architecture for Containerized .NET \", \"Applications.\\nMapping eShopOnContainers to Azure Services\\nAlthough not required, Azure is well-suite\", \"d to supporting the eShopOnContainers because the project \\nwas built to be a cloud-native applicatio\", \"n. The application is built with .NET, so it can run on Linux or \\nWindows containers depending on th\", \"e Docker host. The application is made up of multiple \\nautonomous microservices, each with its own d\", \"ata. The different microservices showcase different \\napproaches, ranging from simple CRUD operations\", \" to more complex DDD and CQRS patterns. \\nMicroservices communicate with clients over HTTP and with o\", \"ne another via message-based \\ncommunication. The application supports multiple platforms for clients\", \" as well, since it adopts HTTP \\nas a standard communication protocol and includes ASP.NET Core and X\", \"amarin mobile apps that run \\non Android, iOS, and Windows platforms.\\nThe application\\u2019s architecture \", \"is shown in Figure 2-5. On the left are the client apps, broken up into \\nmobile, traditional Web, an\", \"d Web Single Page Application (SPA) flavors. On the right are the server\\u0002side components that make u\", \"p the system, each of which can be hosted in Docker containers and \\nKubernetes clusters. The traditi\", \"onal web app is powered by the ASP.NET Core MVC application shown \\nin yellow. This app and the mobil\", \"e and web SPA applications communicate with the individual \\nmicroservices through one or more API ga\", \"teways. The API gateways follow the \\u201cbackends for front \\nends\\u201d (BFF) pattern, meaning that each gate\", \"way is designed to support a given front-end client. The \\nindividual microservices are listed to the\", \" right of the API gateways and include both business logic \\nand some kind of persistence store. The \", \"different services make use of SQL Server databases, Redis \\ncache instances, and MongoDB/CosmosDB st\", \"ores. On the far right is the system\\u2019s Event Bus, which is \\nused for communication between the micro\", \"services.28 CHAPTER 2 | Introducing eShopOnContainers reference app\\nFigure 2-5. The eShopOnContainer\", \"s Architecture.\\nThe server-side components of this architecture all map easily to Azure services.\\nCo\", \"ntainer orchestration and clustering\\nThe application\\u2019s container-hosted services, from ASP.NET Core \", \"MVC apps to individual Catalog and \\nOrdering microservices, can be hosted and managed in Azure Kuber\", \"netes Service (AKS). The \\napplication can run locally on Docker and Kubernetes, and the same contain\", \"ers can then be deployed \\nto staging and production environments hosted in AKS. This process can be \", \"automated as we\\u2019ll see in \\nthe next section.\\nAKS provides management services for individual cluster\", \"s of containers. The application will deploy \\nseparate containers for each microservice in the AKS c\", \"luster, as shown in the architecture diagram \\nabove. This approach allows each individual service to\", \" scale independently according to its resource \\ndemands. Each microservice can also be deployed inde\", \"pendently, and ideally such deployments \\nshould incur zero system downtime.\\nAPI Gateway\\nThe eShopOnC\", \"ontainers application has multiple front-end clients and multiple different back-end \\nservices. Ther\", \"e\\u2019s no one-to-one correspondence between the client applications and the microservices \\nthat support\", \" them. In such a scenario, there may be a great deal of complexity when writing client \\nsoftware to \", \"interface with the various back-end services in a secure manner. Each client would need to \\naddress \", \"this complexity on its own, resulting in duplication and many places in which to make updates \\nas se\", \"rvices change or new policies are implemented.\\nAzure API Management (APIM) helps organizations publi\", \"sh APIs in a consistent, manageable fashion. \\nAPIM consists of three components: the API Gateway, an\", \"d administration portal (the Azure portal), \\nand a developer portal.29 CHAPTER 2 | Introducing eShop\", \"OnContainers reference app\\nThe API Gateway accepts API calls and routes them to the appropriate back\", \"-end API. It can also \\nprovide additional services like verification of API keys or JWT tokens and A\", \"PI transformation on the \\nfly without code modifications (for instance, to accommodate clients expec\", \"ting an older interface).\\nThe Azure portal is where you define the API schema and package different \", \"APIs into products. You \\nalso configure user access, view reports, and configure policies for quotas\", \" or transformations.\\nThe developer portal serves as the main resource for developers. It provides de\", \"velopers with API \\ndocumentation, an interactive test console, and reports on their own usage. Devel\", \"opers also use the \\nportal to create and manage their own accounts, including subscription and API k\", \"ey support.\\nUsing APIM, applications can expose several different groups of services, each providing\", \" a back end \\nfor a particular front-end client. APIM is recommended for complex scenarios. For simpl\", \"er needs, the \\nlightweight API Gateway Ocelot can be used. The eShopOnContainers app uses Ocelot bec\", \"ause of its \\nsimplicity and because it can be deployed into the same application environment as the \", \"application \\nitself. Learn more about eShopOnContainers, APIM, and Ocelot.\\nAnother option if your ap\", \"plication is using AKS is to deploy the Azure Gateway Ingress Controller as a \\npod within your AKS c\", \"luster. This approach allows your cluster to integrate with an Azure Application \\nGateway, allowing \", \"the gateway to load-balance traffic to the AKS pods. Learn more about the Azure \\nGateway Ingress Con\", \"troller for AKS.\\nData\\nThe various back-end services used by eShopOnContainers have different storage\", \" requirements. \\nSeveral microservices use SQL Server databases. The Basket microservice leverages a \", \"Redis cache for \\nits persistence. The Locations microservice expects a MongoDB API for its data. Azu\", \"re supports each \\nof these data formats.\\nFor SQL Server database support, Azure has products for eve\", \"rything from single databases up to \\nhighly scalable SQL Database elastic pools. Individual microser\", \"vices can be configured to \\ncommunicate with their own individual SQL Server databases quickly and e\", \"asily. These databases can \\nbe scaled as needed to support each separate microservice according to i\", \"ts needs.\\nThe eShopOnContainers application stores the user\\u2019s current shopping basket between reques\", \"ts. This \\naspect is managed by the Basket microservice that stores the data in a Redis cache. In dev\", \"elopment, \\nthis cache can be deployed in a container, while in production it can utilize Azure Cache\", \" for Redis. \\nAzure Cache for Redis is a fully managed service offering high performance and reliabil\", \"ity without the \\nneed to deploy and manage Redis instances or containers on your own.\\nThe Locations \", \"microservice uses a MongoDB NoSQL database for its persistence. During \\ndevelopment, the database ca\", \"n be deployed in its own container, while in production the service can \\nleverage Azure Cosmos DB\\u2019s \", \"API for MongoDB. One of the benefits of Azure Cosmos DB is its ability \\nto leverage multiple differe\", \"nt communication protocols, including a SQL API and common NoSQL \\nAPIs including MongoDB, Cassandra,\", \" Gremlin, and Azure Table Storage. Azure Cosmos DB offers a \\nfully managed and globally distributed \", \"database as a service that can scale to meet the needs of the \\nservices that use it.\\nDistributed dat\", \"a in cloud-native applications is covered in more detail in chapter 5.30 CHAPTER 2 | Introducing eSh\", \"opOnContainers reference app\\nEvent Bus\\nThe application uses events to communicate changes between di\", \"fferent services. This functionality can \\nbe implemented with various implementations, and locally t\", \"he eShopOnContainers application uses \\nRabbitMQ. When hosted in Azure, the application would leverag\", \"e Azure Service Bus for its messaging. \\nAzure Service Bus is a fully managed integration message bro\", \"ker that allows applications and services \\nto communicate with one another in a decoupled, reliable,\", \" asynchronous manner. Azure Service Bus \\nsupports individual queues as well as separate topics to su\", \"pport publisher-subscriber scenarios. The \\neShopOnContainers application would leverage topics with \", \"Azure Service Bus to support distributing \\nmessages from one microservice to any other microservice \", \"that needed to react to a given message.\\nResiliency\\nOnce deployed to production, the eShopOnContaine\", \"rs application would be able to take advantage \\nof several Azure services available to improve its r\", \"esiliency. The application publishes health checks, \\nwhich can be integrated with Application Insigh\", \"ts to provide reporting and alerts based on the app\\u2019s \\navailability. Azure resources also provide di\", \"agnostic logs that can be used to identify and correct bugs \\nand performance issues. Resource logs p\", \"rovide detailed information on when and how different Azure \\nresources are used by the application. \", \"You\\u2019ll learn more about cloud-native resiliency features in \\nchapter 6.\\nDeploying eShopOnContainers \", \"to Azure\\nThe eShopOnContainers application can be deployed to various Azure platforms. The recommend\", \"ed \\napproach is to deploy the application to Azure Kubernetes Services (AKS). Helm, a Kubernetes \\nde\", \"ployment tool, is available to reduce deployment complexity. Optionally, developers may \\nimplement A\", \"zure Dev Spaces for Kubernetes to streamline their development process.\\nAzure Kubernetes Service\\nTo \", \"host eShop in AKS, the first step is to create an AKS cluster. To do so, you might use the Azure \\npo\", \"rtal, which will walk you through the required steps. You could also create a cluster from the Azure\", \" \\nCLI, taking care to enable Role-Based Access Control (RBAC) and application routing. The \\neShopOnC\", \"ontainers\\u2019 documentation details the steps for creating your own AKS cluster. Once created, \\nyou can\", \" access and manage the cluster from the Kubernetes dashboard.\\nYou can now deploy the eShop applicati\", \"on to the cluster using Helm.\\nDeploying to Azure Kubernetes Service using Helm\\nHelm is an applicatio\", \"n package manager tool that works directly with Kubernetes. It helps you define, \\ninstall, and upgra\", \"de Kubernetes applications. While simple apps can be deployed to AKS with custom \\nCLI scripts or sim\", \"ple deployment files, complex apps can contain many Kubernetes objects and benefit \\nfrom Helm.\\nUsing\", \" Helm, applications include text-based configuration files, called Helm charts, which declaratively \", \"\\ndescribe the application and configuration in Helm packages. Charts use standard YAML-formatted 31 \", \"CHAPTER 2 | Introducing eShopOnContainers reference app\\nfiles to describe a related set of Kubernete\", \"s resources. They\\u2019re versioned alongside the application \\ncode they describe. Helm Charts range from\", \" simple to complex depending on the requirements of the \\ninstallation they describe.\\nHelm is compose\", \"d of a command-line client tool, which consumes helm charts and launches \\ncommands to a server compo\", \"nent named, Tiller. Tiller communicates with the Kubernetes API to \\nensure the correct provisioning \", \"of your containerized workloads. Helm is maintained by the Cloud\\u0002native Computing Foundation.\\nThe fo\", \"llowing yaml file presents a Helm template:\\napiVersion: v1\\nkind: Service\\nmetadata:\\n name: {{ .Values\", \".app.svc.marketing }}\\n labels:\\n app: {{ template \\\"marketing-api.name\\\" . }}\\n chart: {{ template \\\"mark\", \"eting-api.chart\\\" . }}\\n release: {{ .Release.Name }}\\n heritage: {{ .Release.Service }}\\nspec:\\n type: {\", \"{ .Values.service.type }}\\n ports:\\n - port: {{ .Values.service.port }}\\n targetPort: http\\n protocol: T\", \"CP\\n name: http\\n selector:\\n app: {{ template \\\"marketing-api.name\\\" . }}\\n release: {{ .Release.Name }}\\n\", \"Note how the template describes a dynamic set of key/value pairs. When the template is invoked, \\nval\", \"ues that enclosed in curly braces are pulled in from other yaml-based configuration files.\\nYou\\u2019ll fi\", \"nd the eShopOnContainers helm charts in the /k8s/helm folder. Figure 2-6 shows how the \\ndifferent co\", \"mponents of the application are organized into a folder structure used by helm to define \\nand manage\", \"d deployments.32 CHAPTER 2 | Introducing eShopOnContainers reference app\\nFigure 2-6. The eShopOnCont\", \"ainers helm folder.\\nEach individual component is installed using a helm install command. eShop inclu\", \"des a \\u201cdeploy all\\u201d \\nscript that loops through and installs the components using their respective hel\", \"m charts. The result is \\na repeatable process, versioned with the application in source control, tha\", \"t anyone on the team can \\ndeploy to an AKS cluster with a one-line script command.\\nNote that version\", \" 3 of Helm officially removes the need for the Tiller server component. More \\ninformation on this en\", \"hancement can be found here.\\nAzure Functions and Logic Apps (Serverless)\\nThe eShopOnContainers sampl\", \"e includes support for tracking online marketing campaigns. An Azure \\nFunction is used to track mark\", \"eting campaign details for a given campaign ID. Rather than creating a 33 CHAPTER 2 | Introducing eS\", \"hopOnContainers reference app\\nfull microservice, a single Azure Function is simpler and sufficient. \", \"Azure Functions have a simple build \\nand deployment model, especially when configured to run in Kube\", \"rnetes. Deploying the function is \\nscripted using Azure Resource Manager (ARM) templates and the Azu\", \"re CLI. This campaign service \\nisn\\u2019t customer-facing and invokes a single operation, making it a gre\", \"at candidate for Azure Functions. \\nThe function requires minimal configuration, including a database\", \" connection string data and image \\nbase URI settings. You configure Azure Functions in the Azure por\", \"tal.\\nCentralized configuration\\nUnlike a monolithic app in which everything runs within a single inst\", \"ance, a cloud-native application \\nconsists of independent services distributed across virtual machin\", \"es, containers, and geographic \\nregions. Managing configuration settings for dozens of interdependen\", \"t services can be challenging. \\nDuplicate copies of configuration settings across different location\", \"s are error prone and difficult to \\nmanage. Centralized configuration is a critical requirement for \", \"distributed cloud-native applications.\\nAs discussed in Chapter 1, the Twelve-Factor App recommendati\", \"ons require strict separation between \\ncode and configuration. Configuration must be stored external\", \"ly from the application and read-in as \\nneeded. Storing configuration values as constants or literal\", \" values in code is a violation. The same \\nconfiguration values are often be used by many services in\", \" the same application. Additionally, we \\nmust support the same values across multiple environments, \", \"such as dev, testing, and production. The \\nbest practice is store them in a centralized configuratio\", \"n store.\\nThe Azure cloud presents several great options.\\nAzure App Configuration\\nAzure App Configura\", \"tion is a fully managed Azure service that stores non-secret configuration \\nsettings in a secure, ce\", \"ntralized location. Stored values can be shared among multiple services and \\napplications.\\nThe servi\", \"ce is simple to use and provides several benefits:\\n\\u2022 Flexible key/value representations and mappings\", \"\\n\\u2022 Tagging with Azure labels\\n\\u2022 Dedicated UI for management\\n\\u2022 Encryption of sensitive information\\n\\u2022 Q\", \"uerying and batch retrieval\\nAzure App Configuration maintains changes made to key-value settings for\", \" seven days. The point-in\\u0002time snapshot feature enables you to reconstruct the history of a setting \", \"and even rollback for a failed \\ndeployment.\\nApp Configuration automatically caches each setting to a\", \"void excessive calls to the configuration \\nstore. The refresh operation waits until the cached value\", \" of a setting expires to update that setting, \\neven when its value changes in the configuration stor\", \"e. The default cache expiration time is 30 \\nseconds. You can override the expiration time.34 CHAPTER\", \" 2 | Introducing eShopOnContainers reference app\\nApp Configuration encrypts all configuration values\", \" in transit and at rest. Key names and labels are \\nused as indexes for retrieving configuration data\", \" and aren\\u2019t encrypted.\\nAlthough App Configuration provides hardened security, Azure Key Vault is sti\", \"ll the best place for \\nstoring application secrets. Key Vault provides hardware-level encryption, gr\", \"anular access policies, and \\nmanagement operations such as certificate rotation. You can create App \", \"Configuration values that \\nreference secrets stored in a Key Vault.\\nAzure Key Vault\\nKey Vault is a m\", \"anaged service for securely storing and accessing secrets. A secret is anything that you \\nwant to ti\", \"ghtly control access to, such as API keys, passwords, or certificates. A vault is a logical group \\no\", \"f secrets.\\nKey Vault greatly reduces the chances that secrets may be accidentally leaked. When using\", \" Key Vault, \\napplication developers no longer need to store security information in their applicatio\", \"n. This practice \\neliminates the need to store this information inside your code. For example, an ap\", \"plication may need \\nto connect to a database. Instead of storing the connection string in the app\\u2019s \", \"code, you can store it \\nsecurely in Key Vault.\\nYour applications can securely access the information\", \" they need by using URIs. These URIs allow the \\napplications to retrieve specific versions of a secr\", \"et. There\\u2019s no need to write custom code to protect \\nany of the secret information stored in Key Vau\", \"lt.\\nAccess to Key Vault requires proper caller authentication and authorization. Typically, each clo\", \"ud\\u0002native microservice uses a ClientId/ClientSecret combination. It\\u2019s important to keep these creden\", \"tials \\noutside source control. A best practice is to set them in the application\\u2019s environment. Dire\", \"ct access to \\nKey Vault from AKS can be achieved using Key Vault FlexVolume.\\nConfiguration in eShop\\n\", \"The eShopOnContainers application includes local application settings files with each microservice. \", \"\\nThese files are checked into source control, but don\\u2019t include production secrets such as connectio\", \"n \\nstrings or API keys. In production, individual settings may be overwritten with per-service envir\", \"onment \\nvariables. Injecting secrets in environment variables is a common practice for hosted applic\", \"ations, but \\ndoesn\\u2019t provide a central configuration store. To support centralized management of con\", \"figuration \\nsettings, each microservice includes a setting to toggle between its use of local settin\", \"gs or Azure Key \\nVault settings.\\nReferences\\n\\u2022 The eShopOnContainers Architecture\\n\\u2022 Orchestrating mic\", \"roservices and multi-container applications for high scalability and \\navailability\\n\\u2022 Azure API Manag\", \"ement\\n\\u2022 Azure SQL Database Overview\\n\\u2022 Azure Cache for Redis\\n\\u2022 Azure Cosmos DB\\u2019s API for MongoDB35 CH\", \"APTER 2 | Introducing eShopOnContainers reference app\\n\\u2022 Azure Service Bus\\n\\u2022 Azure Monitor overview\\n\\u2022\", \" eShopOnContainers: Create Kubernetes cluster in AKS\\n\\u2022 eShopOnContainers: Azure Dev Spaces\\n\\u2022 Azure D\", \"ev Spaces36 CHAPTER 3 | Scaling cloud-native applications\\nCHAPTER 3\\nScaling cloud-native \\napplicatio\", \"ns\\nOne of the most-often touted advantages of moving to a cloud hosting environment is scalability. \", \"\\nScalability, or the ability for an application to accept additional user load without compromising \", \"\\nperformance for each user. It\\u2019s most often achieved by breaking up an application into small pieces\", \" \\nthat can each be given whatever resources they require. Cloud vendors enable massive scalability \\n\", \"anytime and anywhere in the world.\\nIn this chapter, we discuss technologies that enable cloud-native\", \" applications to scale to meet user \\ndemand. These technologies include:\\n\\u2022 Containers\\n\\u2022 Orchestrator\", \"s\\n\\u2022 Serverless computing\\nLeveraging containers and orchestrators\\nContainers and orchestrators are de\", \"signed to solve problems common to monolithic deployment \\napproaches.\\nChallenges with monolithic dep\", \"loyments\\nTraditionally, most applications have been deployed as a single unit. Such applications are\", \" referred to \\nas a monolith. This general approach of deploying applications as single units even if\", \" they\\u2019re \\ncomposed of multiple modules or assemblies is known as monolithic architecture, as shown i\", \"n Figure \\n3-1.37 CHAPTER 3 | Scaling cloud-native applications\\nFigure 3-1. Monolithic architecture.\\n\", \"Although they have the benefit of simplicity, monolithic architectures face many challenges:\\nDeploym\", \"ent\\nAdditionally, they require a restart of the application, which may temporarily impact availabili\", \"ty if \\nzero-downtime techniques are not applied while deploying.\\nScaling\\nA monolithic application is\", \" hosted entirely on a single machine instance, often requiring high\\u0002capability hardware. If any part\", \" of the monolith requires scaling, another copy of the entire application \\nmust be deployed to anoth\", \"er machine. With a monolith, you can\\u2019t scale application components \\nindividually - it\\u2019s all or noth\", \"ing. Scaling components that don\\u2019t require scaling results in inefficient and \\ncostly resource usage\", \".\\nEnvironment\\nMonolithic applications are typically deployed to a hosting environment with a pre-ins\", \"talled operating \\nsystem, runtime, and library dependencies. This environment may not match that upo\", \"n which the \\napplication was developed or tested. Inconsistencies across application environments ar\", \"e a common \\nsource of problems for monolithic deployments.\\nCoupling\\nA monolithic application is like\", \"ly to experience high coupling across its functional components. \\nWithout hard boundaries, system ch\", \"anges often result in unintended and costly side effects. New \\nfeatures/fixes become tricky, time-co\", \"nsuming, and expensive to implement. Updates require extensive \\ntesting. Coupling also makes it diff\", \"icult to refactor components or swap in alternative \\nimplementations. Even when constructed with a s\", \"trict separation of concerns, architectural erosion \\nsets in as the monolithic code base deteriorate\", \"s with never-ending \\u201cspecial cases.\\u201d38 CHAPTER 3 | Scaling cloud-native applications\\nPlatform lock-i\", \"n\\nA monolithic application is constructed with a single technology stack. While offering uniformity,\", \" this \\ncommitment can become a barrier to innovation. New features and components will be built usin\", \"g \\nthe application\\u2019s current stack - even when more modern technologies may be a better choice. A \\nl\", \"onger-term risk is your technology stack becoming outdated and obsolete. Rearchitecting an entire \\na\", \"pplication to a new, more modern platform is at best expensive and risky.\\nWhat are the benefits of c\", \"ontainers and orchestrators?\\nWe introduced containers in Chapter 1. We highlighted how the Cloud Nat\", \"ive Computing Foundation \\n(CNCF) ranks containerization as the first step in their Cloud-Native Trai\", \"l Map - guidance for \\nenterprises beginning their cloud-native journey. In this section, we discuss \", \"the benefits of containers.\\nDocker is the most popular container management platform. It works with \", \"containers on both Linux or \\nWindows. Containers provide separate but reproducible application envir\", \"onments that run the same \\nway on any system. This aspect makes them perfect for developing and host\", \"ing cloud-native services. \\nContainers are isolated from one another. Two containers on the same hos\", \"t hardware can have \\ndifferent versions of software, without causing conflicts.\\nContainers are defin\", \"ed by simple text-based files that become project artifacts and are checked into \\nsource control. Wh\", \"ile full servers and virtual machines require manual effort to update, containers are \\neasily versio\", \"n-controlled. Apps built to run in containers can be developed, tested, and deployed \\nusing automate\", \"d tools as part of a build pipeline.\\nContainers are immutable. Once you define a container, you can \", \"recreate and run it exactly the same \\nway. This immutability lends itself to component-based design.\", \" If some parts of an application evolve \\ndifferently than others, why redeploy the entire app when y\", \"ou can just deploy the parts that change \\nmost frequently? Different features and cross-cutting conc\", \"erns of an app can be broken up into \\nseparate units. Figure 3-2 shows how a monolithic app can take\", \" advantage of containers and \\nmicroservices by delegating certain features or functionality. The rem\", \"aining functionality in the app \\nitself has also been containerized.39 CHAPTER 3 | Scaling cloud-nat\", \"ive applications\\nFigure 3-2. Decomposing a monolithic app to embrace microservices.\\nEach cloud-nativ\", \"e service is built and deployed in a separate container. Each can update as needed. \\nIndividual serv\", \"ices can be hosted on nodes with resources appropriate to each service. The \\nenvironment each servic\", \"e runs in is immutable, shared across dev, test, and production environments, \\nand easily versioned.\", \" Coupling between different areas of the application occurs explicitly as calls or \\nmessages between\", \" services, not compile-time dependencies within the monolith. You can also choose \\nthe technology th\", \"at best suites a given capability without requiring changes to the rest of the app.\\nContainerized se\", \"rvices require automated management. It wouldn\\u2019t be feasible to manually administer \\na large set of \", \"independently deployed containers. For example, consider the following tasks:\\n\\u2022 How will container i\", \"nstances be provisioned across a cluster of many machines?\\n\\u2022 Once deployed, how will containers disc\", \"over and communicate with each other?\\n\\u2022 How can containers scale in or out on-demand?\\n\\u2022 How do you m\", \"onitor the health of each container?\\n\\u2022 How do you protect a container against hardware and software \", \"failures?\\n\\u2022 How do upgrade containers for a live application with zero downtime?\\nContainer orchestra\", \"tors address and automate these and other concerns.\\nIn the cloud-native eco-system, Kubernetes has b\", \"ecome the de facto container orchestrator. It\\u2019s an \\nopen-source platform managed by the Cloud Native\", \" Computing Foundation (CNCF). Kubernetes \\nautomates the deployment, scaling, and operational concern\", \"s of containerized workloads across a \\nmachine cluster. However, installing and managing Kubernetes \", \"is notoriously complex.40 CHAPTER 3 | Scaling cloud-native applications\\nA much better approach is to\", \" leverage Kubernetes as a managed service from a cloud vendor. The \\nAzure cloud features a fully man\", \"aged Kubernetes platform entitled Azure Kubernetes Service (AKS). \\nAKS abstracts the complexity and \", \"operational overhead of managing Kubernetes. You consume \\nKubernetes as a cloud service; Microsoft t\", \"akes responsibility for managing and supporting it. AKS also \\ntightly integrates with other Azure se\", \"rvices and dev tools.\\nAKS is a cluster-based technology. A pool of federated virtual machines, or no\", \"des, is deployed to the \\nAzure cloud. Together they form a highly available environment, or cluster.\", \" The cluster appears as a \\nseamless, single entity to your cloud-native application. Under the hood,\", \" AKS deploys your \\ncontainerized services across these nodes following a predefined strategy that ev\", \"enly distributes the \\nload.\\nWhat are the scaling benefits?\\nServices built on containers can leverage\", \" scaling benefits provided by orchestration tools like \\nKubernetes. By design containers only know a\", \"bout themselves. Once you have multiple containers \\nthat need to work together, you should organize \", \"them at a higher level. Organizing large numbers of \\ncontainers and their shared dependencies, such \", \"as network configuration, is where orchestration tools \\ncome in to save the day! Kubernetes creates \", \"an abstraction layer over groups of containers and \\norganizes them into pods. Pods run on worker mac\", \"hines referred to as nodes. This organized structure \\nis referred to as a cluster. Figure 3-3 shows \", \"the different components of a Kubernetes cluster.\\nFigure 3-3. Kubernetes cluster components.\\nScaling\", \" containerized workloads is a key feature of container orchestrators. AKS supports automatic \\nscalin\", \"g across two dimensions: Container instances and compute nodes. Together they give AKS the \\nability \", \"to quickly and efficiently respond to spikes in demand and add additional resources. We \\ndiscuss sca\", \"ling in AKS later in this chapter.41 CHAPTER 3 | Scaling cloud-native applications\\nDeclarative versu\", \"s imperative\\nKubernetes supports both declarative and imperative configuration. The imperative appro\", \"ach involves \\nrunning various commands that tell Kubernetes what to do each step of the way. Run thi\", \"s image. \\nDelete this pod. Expose this port. With the declarative approach, you create a configurati\", \"on file, called \\na manifest, to describe what you want instead of what to do. Kubernetes reads the m\", \"anifest and \\ntransforms your desired end state into actual end state.\\nImperative commands are great \", \"for learning and interactive experimentation. However, you\\u2019ll want to \\ndeclaratively create Kubernet\", \"es manifest files to embrace an infrastructure as code approach, \\nproviding for reliable and repeata\", \"ble deployments. The manifest file becomes a project artifact and is \\nused in your CI/CD pipeline fo\", \"r automating Kubernetes deployments.\\nIf you\\u2019ve already configured your cluster using imperative comm\", \"ands, you can export a declarative \\nmanifest by using kubectl get svc SERVICENAME -o yaml > service.\", \"yaml. This command produces a \\nmanifest similar to one shown below:\\napiVersion: v1\\nkind: Service\\nmet\", \"adata:\\n creationTimestamp: \\\"2019-09-13T13:58:47Z\\\"\\n labels:\\n component: apiserver\\n provider: kubernet\", \"es\\n name: kubernetes\\n namespace: default\\n resourceVersion: \\\"153\\\"\\n selfLink: /api/v1/namespaces/defau\", \"lt/services/kubernetes\\n uid: 9b1fac62-d62e-11e9-8968-00155d38010d\\nspec:\\n clusterIP: 10.96.0.1\\n ports\", \":\\n - name: https\\n port: 443\\n protocol: TCP\\n targetPort: 6443\\n sessionAffinity: None\\n type: ClusterIP\", \"\\nstatus:\\n loadBalancer: {}\\nWhen using declarative configuration, you can preview the changes that wi\", \"ll be made before \\ncommitting them by using kubectl diff -f FOLDERNAME against the folder where your\", \" configuration \\nfiles are located. Once you\\u2019re sure you want to apply the changes, run kubectl apply\", \" -f FOLDERNAME. \\nAdd -R to recursively process a folder hierarchy.\\nYou can also use declarative conf\", \"iguration with other Kubernetes features, one of which being \\ndeployments. Declarative deployments h\", \"elp manage releases, updates, and scaling. They instruct the \\nKubernetes deployment controller on ho\", \"w to deploy new changes, scale out load, or roll back to a \\nprevious revision. If a cluster is unsta\", \"ble, a declarative deployment will automatically return the cluster \\nback to a desired state. For ex\", \"ample, if a node should crash, the deployment mechanism will redeploy \\na replacement to achieve your\", \" desired state42 CHAPTER 3 | Scaling cloud-native applications\\nUsing declarative configuration allow\", \"s infrastructure to be represented as code that can be checked in \\nand versioned alongside the appli\", \"cation code. It provides improved change control and better \\nsupport for continuous deployment using\", \" a build and deploy pipeline.\\nWhat scenarios are ideal for containers and orchestrators?\\nThe followi\", \"ng scenarios are ideal for using containers and orchestrators.\\nApplications requiring high uptime an\", \"d scalability\\nIndividual applications that have high uptime and scalability requirements are ideal c\", \"andidates for \\ncloud-native architectures using microservices, containers, and orchestrators. They c\", \"an be developed \\nin containers, tested across versioned environments, and deployed into production w\", \"ith zero \\ndowntime. The use of Kubernetes clusters ensures such apps can also scale on demand and re\", \"cover \\nautomatically from node failures.\\nLarge numbers of applications\\nOrganizations that deploy and\", \" maintain large numbers of applications benefit from containers and \\norchestrators. The up front eff\", \"ort of setting up containerized environments and Kubernetes clusters is \\nprimarily a fixed cost. Dep\", \"loying, maintaining, and updating individual applications has a cost that \\nvaries with the number of\", \" applications. Beyond a few applications, the complexity of maintaining \\ncustom applications manuall\", \"y exceeds the cost of implementing a solution using containers and \\norchestrators.\\nWhen should you a\", \"void using containers and orchestrators?\\nIf you\\u2019re unable to build your application following the Tw\", \"elve-Factor App principles, you should \\nconsider avoiding containers and orchestrators. In these cas\", \"es, consider a VM-based hosting platform, \\nor possibly some hybrid system. With it, you can always s\", \"pin off certain pieces of functionality into \\nseparate containers or even serverless functions.\\nDeve\", \"lopment resources\\nThis section shows a short list of development resources that may help you get sta\", \"rted using \\ncontainers and orchestrators for your next application. If you\\u2019re looking for guidance o\", \"n how to \\ndesign your cloud-native microservices architecture app, read this book\\u2019s companion, .NET \", \"\\nMicroservices: Architecture for Containerized .NET Applications.\\nLocal Kubernetes Development\\nKuber\", \"netes deployments provide great value in production environments, but can also run locally on \\nyour \", \"development machine. While you may work on individual microservices independently, there \\nmay be tim\", \"es when you\\u2019ll need to run the entire system locally - just as it will run when deployed to \\nproduct\", \"ion. There are several tools that can help: Minikube and Docker Desktop. Visual Studio also \\nprovide\", \"s tooling for Docker development.43 CHAPTER 3 | Scaling cloud-native applications\\nMinikube\\nWhat is M\", \"inikube? The Minikube project says \\u201cMinikube implements a local Kubernetes cluster on \\nmacOS, Linux,\", \" and Windows.\\u201d Its primary goals are \\u201cto be the best tool for local Kubernetes \\napplication developm\", \"ent and to support all Kubernetes features that fit.\\u201d Installing Minikube is \\nseparate from Docker, \", \"but Minikube supports different hypervisors than Docker Desktop supports. \\nThe following Kubernetes \", \"features are currently supported by Minikube:\\n\\u2022 DNS\\n\\u2022 NodePorts\\n\\u2022 ConfigMaps and secrets\\n\\u2022 Dashboard\", \"s\\n\\u2022 Container runtimes: Docker, rkt, CRI-O, and containerd\\n\\u2022 Enabling Container Network Interface (C\", \"NI)\\n\\u2022 Ingress\\nAfter installing Minikube, you can quickly start using it by running the minikube star\", \"t command, which \\ndownloads an image and start the local Kubernetes cluster. Once the cluster is sta\", \"rted, you interact \\nwith it using the standard Kubernetes kubectl commands.\\nDocker Desktop\\nYou can a\", \"lso work with Kubernetes directly from Docker Desktop on Windows. It is your only option if \\nyou\\u2019re \", \"using Windows Containers, and is a great choice for non-Windows containers as well. Figure 3-\\n4 show\", \"s how to enable local Kubernetes support when running Docker Desktop.\\nFigure 3-4. Configuring Kubern\", \"etes in Docker Desktop.44 CHAPTER 3 | Scaling cloud-native applications\\nDocker Desktop is the most p\", \"opular tool for configuring and running containerized apps locally. \\nWhen you work with Docker Deskt\", \"op, you can develop locally against the exact same set of Docker \\ncontainer images that you\\u2019ll deplo\", \"y to production. Docker Desktop is designed to \\u201cbuild, test, and \\nship\\u201d containerized apps locally. \", \"It supports both Linux and Windows containers. Once you push your \\nimages to an image registry, like\", \" Azure Container Registry or Docker Hub, AKS can pull and deploy \\nthem to production.\\nVisual Studio \", \"Docker Tooling\\nVisual Studio supports Docker development for web-based applications. When you create\", \" a new \\nASP.NET Core application, you have an option to configure it with Docker support, as shown i\", \"n Figure \\n3-5.\\nFigure 3-5. Visual Studio Enable Docker Support\\nWhen this option is selected, the pro\", \"ject is created with a Dockerfile in its root, which can be used to \\nbuild and host the app in a Doc\", \"ker container. An example Dockerfile is shown in Figure 3-6.\\nFROM mcr.microsoft.com/dotnet/aspnet:7.\", \"0 AS base\\nWORKDIR /app\\nEXPOSE 80\\nEXPOSE 443\\nFROM mcr.microsoft.com/dotnet/sdk:7.0 AS build\\nWORKDIR /\", \"src\\nCOPY [\\\"eShopWeb/eShopWeb.csproj\\\", \\\"eShopWeb/\\\"]\\nRUN dotnet restore \\\"eShopWeb/eShopWeb.csproj\\\"\\nCOP\", \"Y . .\\nWORKDIR \\\"/src/eShopWeb\\\"\\nRUN dotnet build \\\"eShopWeb.csproj\\\" -c Release -o /app/build\\nFROM build\", \" AS publish\\nRUN dotnet publish \\\"eShopWeb.csproj\\\" -c Release -o /app/publish45 CHAPTER 3 | Scaling cl\", \"oud-native applications\\nFROM base AS final\\nWORKDIR /app\\nCOPY --from=publish /app/publish .\\nENTRYPOIN\", \"T [\\\"dotnet\\\", \\\"eShopWeb.dll\\\"]\\nFigure 3-6. Visual Studio generated Dockerfile\\nOnce support is added, y\", \"ou can run your application in a Docker container in Visual Studio. Figure 3-7 \\nshows the different \", \"run options available from a new ASP.NET Core project created with Docker \\nsupport added.\\nFigure 3-7\", \". Visual Studio Docker Run Options\\nAlso, at any time you can add Docker support to an existing ASP.N\", \"ET Core application. From the \\nVisual Studio Solution Explorer, right-click on the project and selec\", \"t Add > Docker Support, as shown \\nin Figure 3-8.46 CHAPTER 3 | Scaling cloud-native applications\\nFig\", \"ure 3-8. Adding Docker support to Visual Studio\\nVisual Studio Code Docker Tooling\\nThere are many ext\", \"ensions available for Visual Studio Code that support Docker development.\\nMicrosoft provides the Doc\", \"ker for Visual Studio Code extension. This extension simplifies the process \\nof adding container sup\", \"port to applications. It scaffolds required files, builds Docker images, and \\nenables you to debug y\", \"our app inside a container. The extension features a visual explorer that makes \\nit easy to take act\", \"ions on containers and images such as start, stop, inspect, remove, and more. The \\nextension also su\", \"pports Docker Compose enabling you to manage multiple running containers as a \\nsingle unit.\\nLeveragi\", \"ng serverless functions\\nIn the spectrum from managing physical machines to leveraging cloud capabili\", \"ties, serverless lives at \\nthe extreme end. Your only responsibility is your code, and you only pay \", \"when your code runs. Azure \\nFunctions provides a way to build serverless capabilities into your clou\", \"d-native applications.47 CHAPTER 3 | Scaling cloud-native applications\\nWhat is serverless?\\nServerles\", \"s is a relatively new service model of cloud computing. It doesn\\u2019t mean that servers are \\noptional -\", \" your code still runs on a server somewhere. The distinction is that the application team no \\nlonger\", \" concerns itself with managing server infrastructure. Instead, the cloud vendor own this \\nresponsibi\", \"lity. The development team increases its productivity by delivering business solutions to \\ncustomers\", \", not plumbing.\\nServerless computing uses event-triggered stateless containers to host your services\", \". They can scale \\nout and in to meet demand as-needed. Serverless platforms like Azure Functions hav\", \"e tight \\nintegration with other Azure services like queues, events, and storage.\\nWhat challenges are\", \" solved by serverless?\\nServerless platforms address many time-consuming and expensive concerns:\\n\\u2022 Pu\", \"rchasing machines and software licenses\\n\\u2022 Housing, securing, configuring, and maintaining the machin\", \"es and their networking, power, \\nand A/C requirements\\n\\u2022 Patching and upgrading operating systems and\", \" software\\n\\u2022 Configuring web servers or machine services to host application software\\n\\u2022 Configuring a\", \"pplication software within its platform\\nMany companies allocate large budgets to support hardware in\", \"frastructure concerns. Moving to the \\ncloud can help reduce these costs; shifting applications to se\", \"rverless can help eliminate them.\\nWhat is the difference between a microservice and a serverless \\nfu\", \"nction?\\nTypically, a microservice encapsulates a business capability, such as a shopping cart for an\", \" online \\neCommerce site. It exposes multiple operations that enable a user to manage their shopping \", \"\\nexperience. A function, however, is a small, lightweight block of code that executes a single-purpo\", \"se \\noperation in response to an event. Microservices are typically constructed to respond to request\", \"s, \\noften from an interface. Requests can be HTTP Rest- or gRPC-based. Serverless services respond t\", \"o \\nevents. Its event-driven architecture is ideal for processing short-running, background tasks.\\nWh\", \"at scenarios are appropriate for serverless?\\nServerless exposes individual short-running functions t\", \"hat are invoked in response to a trigger. This \\nmakes them ideal for processing background tasks.\\nAn\", \" application might need to send an email as a step in a workflow. Instead of sending the \\nnotificati\", \"on as part of a microservice request, place the message details onto a queue. An Azure \\nFunction can\", \" dequeue the message and asynchronously send the email. Doing so could improve the \\nperformance and \", \"scalability of the microservice. Queue-based load leveling can be implemented to \\navoid bottlenecks \", \"related to sending the emails. Additionally, this stand-alone service could be reused \\nas a utility \", \"across many different applications.48 CHAPTER 3 | Scaling cloud-native applications\\nAsynchronous mes\", \"saging from queues and topics is a common pattern to trigger serverless functions. \\nHowever, Azure F\", \"unctions can be triggered by other events, such as changes to Azure Blob Storage. A \\nservice that su\", \"pports image uploads could have an Azure Function responsible for optimizing the \\nimage size. The fu\", \"nction could be triggered directly by inserts into Azure Blob Storage, keeping \\ncomplexity out of th\", \"e microservice operations.\\nMany services have long-running processes as part of their workflows. Oft\", \"en these tasks are done as \\npart of the user\\u2019s interaction with the application. These tasks can for\", \"ce the user to wait, negatively \\nimpacting their experience. Serverless computing provides a great w\", \"ay to move slower tasks outside \\nof the user interaction loop. These tasks can scale with demand wit\", \"hout requiring the entire \\napplication to scale.\\nWhen should you avoid serverless?\\nServerless soluti\", \"ons provision and scale on demand. When a new instance is invoked, cold starts are a \\ncommon issue. \", \"A cold start is the period of time it takes to provision this instance. Normally, this delay \\nmight \", \"be a few seconds, but can be longer depending on various factors. Once provisioned, a single \\ninstan\", \"ce is kept alive as long as it receives periodic requests. But, if a service is called less frequent\", \"ly, \\nAzure may remove it from memory and require a cold start when reinvoked. Cold starts are also \\n\", \"required when a function scales out to a new instance.\\nFigure 3-9 shows a cold-start pattern. Note t\", \"he extra steps required when the app is cold.\\nFigure 3-9. Cold start versus warm start.\\nTo avoid col\", \"d starts entirely, you might switch from a consumption plan to a dedicated plan. You can \\nalso confi\", \"gure one or more pre-warmed instances with the premium plan upgrade. In these cases, \\nwhen you need \", \"to add another instance, it\\u2019s already up and ready to go. These options can help \\nmitigate the cold \", \"start issue associated with serverless computing.49 CHAPTER 3 | Scaling cloud-native applications\\nCl\", \"oud providers bill for serverless based on compute execution time and consumed memory. Long \\nrunning\", \" operations or high memory consumption workloads aren\\u2019t always the best candidates for \\nserverless. \", \"Serverless functions favor small chunks of work that can complete quickly. Most serverless \\nplatform\", \"s require individual functions to complete within a few minutes. Azure Functions defaults to a \\n5-mi\", \"nute time-out duration, which can be configured up to 10 minutes. The Azure Functions premium \\nplan \", \"can mitigate this issue as well, defaulting time-outs to 30 minutes with an unbounded higher \\nlimit \", \"that can be configured. Compute time isn\\u2019t calendar time. More advanced functions using the \\nAzure D\", \"urable Functions framework may pause execution over a course of several days. The billing is \\nbased \", \"on actual execution time - when the function wakes up and resumes processing.\\nFinally, leveraging Az\", \"ure Functions for application tasks adds complexity. It\\u2019s wise to first architect \\nyour application \", \"with a modular, loosely coupled design. Then, identify if there are benefits serverless \\nwould offer\", \" that justify the additional complexity.\\nCombining containers and serverless approaches\\nCloud-native\", \" applications typically implement services leveraging containers and orchestration. There \\nare often\", \" opportunities to expose some of the application\\u2019s services as Azure Functions. However, \\nwith a clo\", \"ud-native app deployed to Kubernetes, it would be nice to leverage Azure Functions within \\nthis same\", \" toolset. Fortunately, you can wrap Azure Functions inside Docker containers and deploy \\nthem using \", \"the same processes and tools as the rest of your Kubernetes-based app.\\nWhen does it make sense to us\", \"e containers with serverless?\\nYour Azure Function has no knowledge of the platform on which it\\u2019s dep\", \"loyed. For some scenarios, \\nyou may have specific requirements and need to customize the environment\", \" on which your function \\ncode will run. You\\u2019ll need a custom image that supports dependencies or a c\", \"onfiguration not \\nsupported by the default image. In these cases, it makes sense to deploy your func\", \"tion in a custom \\nDocker container.\\nWhen should you avoid using containers with Azure Functions?\\nIf \", \"you want to use consumption billing, you can\\u2019t run your function in a container. What\\u2019s more, if you\", \" \\ndeploy your function to a Kubernetes cluster, you\\u2019ll no longer benefit from the built-in scaling \\n\", \"provided by Azure Functions. You\\u2019ll need to use Kubernetes\\u2019 scaling features, described earlier in t\", \"his \\nchapter.\\nHow to combine serverless and Docker containers\\nTo wrap an Azure Function in a Docker \", \"container, install the Azure Functions Core Tools and then run \\nthe following command:\\nfunc init Pro\", \"jectName --worker-runtime dotnet --docker\\nWhen the project is created, it will include a Dockerfile \", \"and the worker runtime configured to dotnet. \\nNow, you can create and test your function locally. Bu\", \"ild and run it using the docker build and docker 50 CHAPTER 3 | Scaling cloud-native applications\\nru\", \"n commands. For detailed steps to get started building Azure Functions with Docker support, see \\nthe\", \" Create a function on Linux using a custom image tutorial.\\nHow to combine serverless and Kubernetes \", \"with KEDA\\nIn this chapter, you\\u2019ve seen that the Azure Functions\\u2019 platform automatically scales out t\", \"o meet \\ndemand. When deploying containerized functions to AKS, however, you lose the built-in scalin\", \"g \\nfunctionality. To the rescue comes Kubernetes-based Event Driven (KEDA). It enables fine-grained \", \"\\nautoscaling for event-driven Kubernetes workloads, including containerized functions.\\nKEDA provides\", \" event-driven scaling functionality to the Functions\\u2019 runtime in a Docker container. \\nKEDA can scale\", \" from zero instances (when no events are occurring) out to n instances, based on load. \\nIt enables a\", \"utoscaling by exposing custom metrics to the Kubernetes autoscaler (Horizontal Pod \\nAutoscaler). Usi\", \"ng Functions containers with KEDA makes it possible to replicate serverless function \\ncapabilities i\", \"n any Kubernetes cluster.\\nIt\\u2019s worth noting that the KEDA project is now managed by the Cloud Native\", \" Computing Foundation \\n(CNCF).\\nDeploying containers in Azure\\nWe\\u2019ve discussed containers in this chap\", \"ter and in chapter 1. We\\u2019ve seen that containers provide many \\nbenefits to cloud-native applications\", \", including portability. In the Azure cloud, you can deploy the \\nsame containerized services across \", \"staging and production environments. Azure provides several \\noptions for hosting these containerized\", \" workloads:\\n\\u2022 Azure Kubernetes Services (AKS)\\n\\u2022 Azure Container Instance (ACI)\\n\\u2022 Azure Web Apps for \", \"Containers\\nAzure Container Registry\\nWhen containerizing a microservice, you first build a container \", \"\\u201cimage.\\u201d The image is a binary \\nrepresentation of the service code, dependencies, and runtime. While\", \" you can manually create an \\nimage using the Docker Build command from the Docker API, a better appr\", \"oach is to create it as part \\nof an automated build process.\\nOnce created, container images are stor\", \"ed in container registries. They enable you to build, store, and \\nmanage container images. There are\", \" many registries available, both public and private. Azure \\nContainer Registry (ACR) is a fully mana\", \"ged container registry service in the Azure cloud. It persists \\nyour images inside the Azure network\", \", reducing the time to deploy them to Azure container hosts. \\nYou can also secure them using the sam\", \"e security and identity procedures that you use for other \\nAzure resources.\\nYou create an Azure Cont\", \"ainer Registry using the Azure portal, Azure CLI, or PowerShell tools. \\nCreating a registry in Azure\", \" is simple. It requires an Azure subscription, resource group, and a unique 51 CHAPTER 3 | Scaling c\", \"loud-native applications\\nname. Figure 3-10 shows the basic options for creating a registry, which wi\", \"ll be hosted at \\nregistryname.azurecr.io.\\nFigure 3-10. Create container registry\\nOnce you\\u2019ve created\", \" the registry, you\\u2019ll need to authenticate with it before you can use it. Typically, \\nyou\\u2019ll log int\", \"o the registry using the Azure CLI command:\\naz acr login --name *registryname*\\nOnce authenticated, y\", \"ou can use docker commands to push container images to it. Before you can do \\nso, however, you must \", \"tag your image with the fully qualified name (URL) of your ACR login server. It \\nwill have the forma\", \"t registryname.azurecr.io.\\ndocker tag mycontainer myregistry.azurecr.io/mycontainer:v1\\nAfter you\\u2019ve \", \"tagged the image, you use the docker push command to push the image to your ACR \\ninstance.\\ndocker pu\", \"sh myregistry.azurecr.io/mycontainer:v152 CHAPTER 3 | Scaling cloud-native applications\\nAfter you pu\", \"sh an image to the registry, it\\u2019s a good idea to remove the image from your local Docker \\nenvironmen\", \"t, using this command:\\ndocker rmi myregistry.azurecr.io/mycontainer:v1\\nAs a best practice, you shoul\", \"dn\\u2019t manually push images to a container registry. Instead, use a build \\npipeline defined in a tool \", \"like GitHub or Azure DevOps. Learn more in the Cloud-Native DevOps \\nchapter.\\nACR Tasks\\nACR Tasks is \", \"a set of features available from the Azure Container Registry. It extends your inner-loop \\ndevelopme\", \"nt cycle by building and managing container images in the Azure cloud. Instead of \\ninvoking a docker\", \" build and docker push locally on your development machine, they\\u2019re automatically \\nhandled by ACR Ta\", \"sks in the cloud.\\nThe following AZ CLI command both builds a container image and pushes it to ACR:\\n#\", \" create a container registry\\naz acr create --resource-group myResourceGroup --name myContainerRegist\", \"ry008 --sku \\nBasic\\n# build container image in ACR and push it into your container registry\\naz acr bu\", \"ild --image sample/hello-world:v1 --registry myContainerRegistry008 --file \\nDockerfile .\\nAs you can \", \"see from the previous command block, there\\u2019s no need to install Docker Desktop on your \\ndevelopment \", \"machine. Additionally, you can configure ACR Task triggers to rebuild containers images \\non both sou\", \"rce code and base image updates.\\nAzure Kubernetes Service\\nWe discussed Azure Kubernetes Service (AKS\", \") at length in this chapter. We\\u2019ve seen that it\\u2019s the de \\nfacto container orchestrator managing cont\", \"ainerized cloud-native applications.\\nOnce you deploy an image to a registry, such as ACR, you can co\", \"nfigure AKS to automatically pull and \\ndeploy it. With a CI/CD pipeline in place, you might configur\", \"e a canary release strategy to minimize \\nthe risk involved when rapidly deploying updates. The new v\", \"ersion of the app is initially configured in \\nproduction with no traffic routed to it. Then, the sys\", \"tem will route a small percentage of users to the \\nnewly deployed version. As the team gains confide\", \"nce in the new version, it can roll out more \\ninstances and retire the old. AKS easily supports this\", \" style of deployment.\\nAs with most resources in Azure, you can create an Azure Kubernetes Service cl\", \"uster using the portal, \\ncommand-line, or automation tools like Helm or Terraform. To get started wi\", \"th a new cluster, you \\nneed to provide the following information:\\n\\u2022 Azure subscription\\n\\u2022 Resource gr\", \"oup\\n\\u2022 Kubernetes cluster name\\n\\u2022 Region53 CHAPTER 3 | Scaling cloud-native applications\\n\\u2022 Kubernetes \", \"version\\n\\u2022 DNS name prefix\\n\\u2022 Node size\\n\\u2022 Node count\\nThis information is sufficient to get started. As\", \" part of the creation process in the Azure portal, you can \\nalso configure options for the following\", \" features of your cluster:\\n\\u2022 Scale\\n\\u2022 Authentication\\n\\u2022 Networking\\n\\u2022 Monitoring\\n\\u2022 Tags\\nThis quickstart\", \" walks through deploying an AKS cluster using the Azure portal.\\nAzure Bridge to Kubernetes\\nCloud-nat\", \"ive applications can grow large and complex, requiring significant compute resources to \\nrun. In the\", \"se scenarios, the entire application can\\u2019t be hosted on a development machine (especially a \\nlaptop)\", \". Azure Bridge to Kubernetes addresses the shortcoming. It enables developers to work with a \\nlocal \", \"version of their service while hosting the entire application in an AKS development cluster.\\nWhen re\", \"ady, developers test their changes locally while running against the full application in the AKS \\ncl\", \"uster - without replicating dependencies. Under the hood, the bridge merges code from the local \\nmac\", \"hine with services in AKS. Developers can rapidly iterate and debug code directly in Kubernetes \\nusi\", \"ng Visual Studio or Visual Studio Code.\\nGabe Monroy, former VP of Product Management at Microsoft, d\", \"escribes it well:\\nImagine you\\u2019re a new employee trying to fix a bug in a complex microservices appli\", \"cation consisting \\nof dozens of components, each with their own configuration and backing services. \", \"To get started, you \\nmust configure your local development environment so that it can mimic producti\", \"on including setting \\nup your IDE, building tool chain, containerized service dependencies, a local \", \"Kubernetes environment, \\nmocks for backing services, and more. With all the time involved setting up\", \" your development \\nenvironment, fixing that first bug could take days! Or you could just use Bridge \", \"to Kubernetes and \\nAKS.\\nScaling containers and serverless applications\\nThere are two ways to scale a\", \"n application: up or out. The former refers to adding capacity to a single \\nresource, while the latt\", \"er refers to adding more resources to increase capacity.\\nThe simple solution: scaling up\\nUpgrading a\", \"n existing host server with increased CPU, memory, disk I/O speed, and network I/O \\nspeed is known a\", \"s scaling up. Scaling up a cloud-native application involves choosing more capable 54 CHAPTER 3 | Sc\", \"aling cloud-native applications\\nresources from the cloud vendor. For example, you can create a new n\", \"ode pool with larger VMs in \\nyour Kubernetes cluster. Then, migrate your containerized services to t\", \"he new pool.\\nServerless apps scale up by choosing the premium Functions plan or premium instance siz\", \"es from a \\ndedicated app service plan.\\nScaling out cloud-native apps\\nCloud-native applications often\", \" experience large fluctuations in demand and require scale on a \\nmoment\\u2019s notice. They favor scaling\", \" out. Scaling out is done horizontally by adding additional \\nmachines (called nodes) or application \", \"instances to an existing cluster. In Kubernetes, you can scale \\nmanually by adjusting configuration \", \"settings for the app (for example, scaling a node pool), or \\nthrough autoscaling.\\nAKS clusters can a\", \"utoscale in one of two ways:\\nFirst, the Horizontal Pod Autoscaler monitors resource demand and autom\", \"atically scales your POD \\nreplicas to meet it. When traffic increases, additional replicas are autom\", \"atically provisioned to scale \\nout your services. Likewise, when demand decreases, they\\u2019re removed t\", \"o scale-in your services. You \\ndefine the metric on which to scale, for example, CPU usage. You can \", \"also specify the minimum and \\nmaximum number of replicas to run. AKS monitors that metric and scales\", \" accordingly.\\nNext, the AKS Cluster Autoscaler feature enables you to automatically scale compute no\", \"des across a \\nKubernetes cluster to meet demand. With it, you can automatically add new VMs to the u\", \"nderlying \\nAzure Virtual Machine Scale Set whenever more compute capacity of is required. It also re\", \"moves \\nnodes when no longer required.\\nFigure 3-11 shows the relationship between these two scaling s\", \"ervices.\\nFigure 3-11. Scaling out an App Service plan.55 CHAPTER 3 | Scaling cloud-native applicatio\", \"ns\\nWorking together, both ensure an optimal number of container instances and compute nodes to \\nsupp\", \"ort fluctuating demand. The horizontal pod autoscaler optimizes the number of pods required. \\nThe cl\", \"uster autoscaler optimizes the number of nodes required.\\nScaling Azure Functions\\nAzure Functions aut\", \"omatically scale out upon demand. Server resources are dynamically allocated and \\nremoved based on t\", \"he number of triggered events. You\\u2019re only charged for compute resources \\nconsumed when your functio\", \"ns run. Billing is based upon the number of executions, execution time, \\nand memory used.\\nWhile the \", \"default consumption plan provides an economical and scalable solution for most apps, the \\npremium op\", \"tion allows developers flexibility for custom Azure Functions requirements. Upgrading to \\nthe premiu\", \"m plan provides control over instance sizes, pre-warmed instances (to avoid cold start \\ndelays), and\", \" dedicated VMs.\\nOther container deployment options\\nAside from Azure Kubernetes Service (AKS), you ca\", \"n also deploy containers to Azure App Service for \\nContainers and Azure Container Instances.\\nWhen do\", \"es it make sense to deploy to App Service for Containers?\\nSimple production applications that don\\u2019t \", \"require orchestration are well suited to Azure App Service \\nfor Containers.\\nHow to deploy to App Ser\", \"vice for Containers\\nTo deploy to Azure App Service for Containers, you\\u2019ll need an Azure Container Re\", \"gistry (ACR) instance \\nand credentials to access it. Push your container image to the ACR repository\", \" so that your Azure App \\nService can pull it when needed. Once complete, you can configure the app f\", \"or Continuous \\nDeployment. Doing so will automatically deploy updates whenever the image changes in \", \"ACR.\\nWhen does it make sense to deploy to Azure Container Instances?\\nAzure Container Instances (ACI)\", \" enables you to run Docker containers in a managed, serverless cloud \\nenvironment, without having to\", \" set up virtual machines or clusters. It\\u2019s a great solution for short\\u0002running workloads that can run\", \" in an isolated container. Consider ACI for simple services, testing \\nscenarios, task automation, an\", \"d build jobs. ACI spins-up a container instance, performs the task, and \\nthen spins it down.\\nHow to \", \"deploy an app to Azure Container Instances\\nTo deploy to Azure Container Instances (ACI), you need an\", \" Azure Container Registry (ACR) and \\ncredentials for accessing it. Once you push your container imag\", \"e to the repository, it\\u2019s available to pull \\ninto ACI. You can work with ACI using the Azure portal \", \"or command-line interface. ACR provides tight\\nintegration with ACI. Figure 3-12 shows how to push an\", \" individual container image to ACR.56 CHAPTER 3 | Scaling cloud-native applications\\nFigure 3-12. Azu\", \"re Container Registry Run Instance\\nCreating an instance in ACI can be done quickly. Specify the imag\", \"e registry, Azure resource group \\ninformation, the amount of memory to allocate, and the port on whi\", \"ch to listen. This quickstart shows \\nhow to deploy a container instance to ACI using the Azure porta\", \"l.\\nOnce the deployment completes, find the newly deployed container\\u2019s IP address and communicate \\nwi\", \"th it over the port you specified.\\nAzure Container Instances offers the fastest way to run simple co\", \"ntainer workloads in Azure. You don\\u2019t \\nneed to configure an app service, orchestrator, or virtual ma\", \"chine. For scenarios where you require full \\ncontainer orchestration, service discovery, automatic s\", \"caling, or coordinated upgrades, we \\nrecommend Azure Kubernetes Service (AKS).\\nReferences\\n\\u2022 What is \", \"Kubernetes?\\n\\u2022 Installing Kubernetes with Minikube\\n\\u2022 MiniKube vs Docker Desktop\\n\\u2022 Visual Studio Tools\", \" for Docker57 CHAPTER 3 | Scaling cloud-native applications\\n\\u2022 Understanding serverless cold start\\n\\u2022 \", \"Pre-warmed Azure Functions instances\\n\\u2022 Create a function on Linux using a custom image\\n\\u2022 Run Azure F\", \"unctions in a Docker Container\\n\\u2022 Create a function on Linux using a custom image\\n\\u2022 Azure Functions w\", \"ith Kubernetes Event Driven Autoscaling\\n\\u2022 Canary Release\\n\\u2022 Azure Dev Spaces with VS Code\\n\\u2022 Azure Dev\", \" Spaces with Visual Studio\\n\\u2022 AKS Multiple Node Pools\\n\\u2022 AKS Cluster Autoscaler\\n\\u2022 Tutorial: Scale appl\", \"ications in AKS\\n\\u2022 Azure Functions scale and hosting\\n\\u2022 Azure Container Instances Docs\\n\\u2022 Deploy Contai\", \"ner Instance from ACR58 CHAPTER 4 | Cloud-native communication patterns\\nCHAPTER 4\\nCloud-native \\ncomm\", \"unication patterns\\nWhen constructing a cloud-native system, communication becomes a significant desi\", \"gn decision. How \\ndoes a front-end client application communicate with a back-end microservice? How \", \"do back-end \\nmicroservices communicate with each other? What are the principles, patterns, and best \", \"practices to \\nconsider when implementing communication in cloud-native applications?\\nCommunication c\", \"onsiderations\\nIn a monolithic application, communication is straightforward. The code modules execut\", \"e together in \\nthe same executable space (process) on a server. This approach can have performance a\", \"dvantages as \\neverything runs together in shared memory, but results in tightly coupled code that be\", \"comes difficult \\nto maintain, evolve, and scale.\\nCloud-native systems implement a microservice-based\", \" architecture with many small, independent \\nmicroservices. Each microservice executes in a separate \", \"process and typically runs inside a container \\nthat is deployed to a cluster.\\nA cluster groups a poo\", \"l of virtual machines together to form a highly available environment. They\\u2019re \\nmanaged with an orch\", \"estration tool, which is responsible for deploying and managing the \\ncontainerized microservices. Fi\", \"gure 4-1 shows a Kubernetes cluster deployed into the Azure cloud \\nwith the fully managed Azure Kube\", \"rnetes Services.59 CHAPTER 4 | Cloud-native communication patterns\\nFigure 4-1. A Kubernetes cluster \", \"in Azure\\nAcross the cluster, microservices communicate with each other through APIs and messaging \\nt\", \"echnologies.\\nWhile they provide many benefits, microservices are no free lunch. Local in-process met\", \"hod calls \\nbetween components are now replaced with network calls. Each microservice must communicat\", \"e over \\na network protocol, which adds complexity to your system:\\n\\u2022 Network congestion, latency, and\", \" transient faults are a constant concern.\\n\\u2022 Resiliency (that is, retrying failed requests) is essent\", \"ial.\\n\\u2022 Some calls must be idempotent as to keep consistent state.\\n\\u2022 Each microservice must authentic\", \"ate and authorize calls.\\n\\u2022 Each message must be serialized and then deserialized - which can be expe\", \"nsive.\\n\\u2022 Message encryption/decryption becomes important.\\nThe book .NET Microservices: Architecture \", \"for Containerized .NET Applications, available for free from \\nMicrosoft, provides an in-depth covera\", \"ge of communication patterns for microservice applications. In \\nthis chapter, we provide a high-leve\", \"l overview of these patterns along with implementation options \\navailable in the Azure cloud.\\nIn thi\", \"s chapter, we\\u2019ll first address communication between front-end applications and back-end \\nmicroservi\", \"ces. We\\u2019ll then look at back-end microservices communicate with each other. We\\u2019ll explore 60 CHAPTER\", \" 4 | Cloud-native communication patterns\\nthe up and gRPC communication technology. Finally, we\\u2019ll lo\", \"ok new innovative communication \\npatterns using service mesh technology. We\\u2019ll also see how the Azur\", \"e cloud provides different kinds \\nof backing services to support cloud-native communication.\\nFront-e\", \"nd client communication\\nIn a cloud-native system, front-end clients (mobile, web, and desktop applic\", \"ations) require a \\ncommunication channel to interact with independent back-end microservices.\\nWhat a\", \"re the options?\\nTo keep things simple, a front-end client could directly communicate with the back-e\", \"nd microservices, \\nshown in Figure 4-2.\\nFigure 4-2. Direct client to service communication\\nWith this\", \" approach, each microservice has a public endpoint that is accessible by front-end clients. In \\na pr\", \"oduction environment, you\\u2019d place a load balancer in front of the microservices, routing traffic \\npr\", \"oportionately.\\nWhile simple to implement, direct client communication would be acceptable only for s\", \"imple \\nmicroservice applications. This pattern tightly couples front-end clients to core back-end se\", \"rvices, \\nopening the door for many problems, including:\\n\\u2022 Client susceptibility to back-end service \", \"refactoring.\\n\\u2022 A wider attack surface as core back-end services are directly exposed.\\n\\u2022 Duplication \", \"of cross-cutting concerns across each microservice.\\n\\u2022 Overly complex client code - clients must keep\", \" track of multiple endpoints and handle failures \\nin a resilient way.61 CHAPTER 4 | Cloud-native com\", \"munication patterns\\nInstead, a widely accepted cloud design pattern is to implement an API Gateway S\", \"ervice between the \\nfront-end applications and back-end services. The pattern is shown in Figure 4-3\", \".\\nFigure 4-3. API gateway pattern\\nIn the previous figure, note how the API Gateway service abstracts\", \" the back-end core microservices. \\nImplemented as a web API, it acts as a reverse proxy, routing inc\", \"oming traffic to the internal \\nmicroservices.\\nThe gateway insulates the client from internal service\", \" partitioning and refactoring. If you change a \\nback-end service, you accommodate for it in the gate\", \"way without breaking the client. It\\u2019s also your \\nfirst line of defense for cross-cutting concerns, s\", \"uch as identity, caching, resiliency, metering, and \\nthrottling. Many of these cross-cutting concern\", \"s can be off-loaded from the back-end core services to \\nthe gateway, simplifying the back-end servic\", \"es.\\nCare must be taken to keep the API Gateway simple and fast. Typically, business logic is kept ou\", \"t of \\nthe gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. L\", \"arger \\nsystems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or\", \" \\nback-end functionality. The Backend for Frontends pattern provides direction for implementing \\nmul\", \"tiple gateways. The pattern is shown in Figure 4-4.62 CHAPTER 4 | Cloud-native communication pattern\", \"s\\nFigure 4-4. Backend for frontend pattern\\nNote in the previous figure how incoming traffic is sent \", \"to a specific API gateway - based upon client \\ntype: web, mobile, or desktop app. This approach make\", \"s sense as the capabilities of each device differ \\nsignificantly across form factor, performance, an\", \"d display limitations. Typically mobile applications \\nexpose less functionality than a browser or de\", \"sktop applications. Each gateway can be optimized to \\nmatch the capabilities and functionality of th\", \"e corresponding device.\\nSimple Gateways\\nTo start, you could build your own API Gateway service. A qu\", \"ick search of GitHub will provide many \\nexamples.\\nFor simple .NET cloud-native applications, you mig\", \"ht consider the Ocelot Gateway. Open source and \\ncreated for .NET microservices, it\\u2019s lightweight, f\", \"ast, scalable. Like any API Gateway, its primary \\nfunctionality is to forward incoming HTTP requests\", \" to downstream services. Additionally, it supports a \\nwide variety of capabilities that are configur\", \"able in a .NET middleware pipeline.\\nYARP (Yet Another Reverse proxy) is another open source reverse \", \"proxy led by a group of Microsoft \\nproduct teams. Downloadable as a NuGet package, YARP plugs into t\", \"he ASP.NET framework as \\nmiddleware and is highly customizable. You\\u2019ll find YARP well-documented wit\", \"h various usage \\nexamples.\\nFor enterprise cloud-native applications, there are several managed Azure\", \" services that can help \\njump-start your efforts.63 CHAPTER 4 | Cloud-native communication patterns\\n\", \"Azure Application Gateway\\nFor simple gateway requirements, you may consider Azure Application Gatewa\", \"y. Available as an Azure \\nPaaS service, it includes basic gateway features such as URL routing, SSL \", \"termination, and a Web \\nApplication Firewall. The service supports Layer-7 load balancing capabiliti\", \"es. With Layer 7, you can \\nroute requests based on the actual content of an HTTP message, not just l\", \"ow-level TCP network \\npackets.\\nThroughout this book, we evangelize hosting cloud-native systems in K\", \"ubernetes. A container \\norchestrator, Kubernetes automates the deployment, scaling, and operational \", \"concerns of \\ncontainerized workloads. Azure Application Gateway can be configured as an API gateway \", \"for Azure \\nKubernetes Service cluster.\\nThe Application Gateway Ingress Controller enables Azure Appl\", \"ication Gateway to work directly with \\nAzure Kubernetes Service. Figure 4.5 shows the architecture.\\n\", \"Figure 4-5. Application Gateway Ingress Controller\\nKubernetes includes a built-in feature that suppo\", \"rts HTTP (Level 7) load balancing, called Ingress. \\nIngress defines a set of rules for how microserv\", \"ice instances inside AKS can be exposed to the outside \\nworld. In the previous image, the ingress co\", \"ntroller interprets the ingress rules configured for the \\ncluster and automatically configures the A\", \"zure Application Gateway. Based on those rules, the \\nApplication Gateway routes traffic to microserv\", \"ices running inside AKS. The ingress controller listens \\nfor changes to ingress rules and makes the \", \"appropriate changes to the Azure Application Gateway.\\nAzure API Management\\nFor moderate to large-sca\", \"le cloud-native systems, you may consider Azure API Management. It\\u2019s a \\ncloud-based service that not\", \" only solves your API Gateway needs, but provides a full-featured \\ndeveloper and administrative expe\", \"rience. API Management is shown in Figure 4-6.64 CHAPTER 4 | Cloud-native communication patterns\\nFig\", \"ure 4-6. Azure API Management\\nTo start, API Management exposes a gateway server that allows controll\", \"ed access to back-end \\nservices based upon configurable rules and policies. These services can be in\", \" the Azure cloud, your \\non-prem data center, or other public clouds. API keys and JWT tokens determi\", \"ne who can do what. All \\ntraffic is logged for analytical purposes.\\nFor developers, API Management o\", \"ffers a developer portal that provides access to services, \\ndocumentation, and sample code for invok\", \"ing them. Developers can use Swagger/Open API to \\ninspect service endpoints and analyze their usage.\", \" The service works across the major development \\nplatforms: .NET, Java, Golang, and more.\\nThe publis\", \"her portal exposes a management dashboard where administrators expose APIs and \\nmanage their behavio\", \"r. Service access can be granted, service health monitored, and service telemetry \\ngathered. Adminis\", \"trators apply policies to each endpoint to affect behavior. Policies are pre-built \\nstatements that \", \"execute sequentially for each service call. Policies are configured for an inbound call, \\noutbound c\", \"all, or invoked upon an error. Policies can be applied at different service scopes as to \\nenable det\", \"erministic ordering when combining policies. The product ships with a large number of \\nprebuilt poli\", \"cies.\\nHere are examples of how policies can affect the behavior of your cloud-native services:\\n\\u2022 Res\", \"trict service access.\\n\\u2022 Enforce authentication.\\n\\u2022 Throttle calls from a single source, if necessary.\", \"\\n\\u2022 Enable caching.\\n\\u2022 Block calls from specific IP addresses.65 CHAPTER 4 | Cloud-native communicatio\", \"n patterns\\n\\u2022 Control the flow of the service.\\n\\u2022 Convert requests from SOAP to REST or between differ\", \"ent data formats, such as from XML to \\nJSON.\\nAzure API Management can expose back-end services that \", \"are hosted anywhere \\u2013 in the cloud or your \\ndata center. For legacy services that you may expose in \", \"your cloud-native systems, it supports both \\nREST and SOAP APIs. Even other Azure services can be ex\", \"posed through API Management. You could \\nplace a managed API on top of an Azure backing service like\", \" Azure Service Bus or Azure Logic Apps. \\nAzure API Management doesn\\u2019t include built-in load-balancin\", \"g support and should be used in \\nconjunction with a load-balancing service.\\nAzure API Management is \", \"available across four different tiers:\\n\\u2022 Developer\\n\\u2022 Basic\\n\\u2022 Standard\\n\\u2022 Premium\\nThe Developer tier i\", \"s meant for non-production workloads and evaluation. The other tiers offer \\nprogressively more power\", \", features, and higher service level agreements (SLAs). The Premium tier \\nprovides Azure Virtual Net\", \"work and multi-region support. All tiers have a fixed price per hour.\\nThe Azure cloud also offers a \", \"serverless tier for Azure API Management. Referred to as the \\nconsumption pricing tier, the service \", \"is a variant of API Management designed around the serverless \\ncomputing model. Unlike the \\u201cpre-allo\", \"cated\\u201d pricing tiers previously shown, the consumption tier \\nprovides instant provisioning and pay-p\", \"er-action pricing.\\nIt enables API Gateway features for the following use cases:\\n\\u2022 Microservices impl\", \"emented using serverless technologies such as Azure Functions and Azure \\nLogic Apps.\\n\\u2022 Azure backing\", \" service resources such as Service Bus queues and topics, Azure storage, and \\nothers.\\n\\u2022 Microservice\", \"s where traffic has occasional large spikes but remains low most the time.\\nThe consumption tier uses\", \" the same underlying service API Management components, but employs \\nan entirely different architect\", \"ure based on dynamically allocated resources. It aligns perfectly with the \\nserverless computing mod\", \"el:\\n\\u2022 No infrastructure to manage.\\n\\u2022 No idle capacity.\\n\\u2022 High-availability.\\n\\u2022 Automatic scaling.\\n\\u2022 C\", \"ost is based on actual usage.\\nThe new consumption tier is a great choice for cloud-native systems th\", \"at expose serverless resources \\nas APIs.66 CHAPTER 4 | Cloud-native communication patterns\\nReal-time\", \" communication\\nReal-time, or push, communication is another option for front-end applications that c\", \"ommunicate \\nwith back-end cloud-native systems over HTTP. Applications, such as financial-tickers, o\", \"nline \\neducation, gaming, and job-progress updates, require instantaneous, real-time responses from \", \"the \\nback-end. With normal HTTP communication, there\\u2019s no way for the client to know when new data i\", \"s \\navailable. The client must continually poll or send requests to the server. With real-time\\ncommun\", \"ication, the server can push new data to the client at any time.\\nReal-time systems are often charact\", \"erized by high-frequency data flows and large numbers of \\nconcurrent client connections. Manually im\", \"plementing real-time connectivity can quickly become \\ncomplex, requiring non-trivial infrastructure \", \"to ensure scalability and reliable messaging to connected \\nclients. You could find yourself managing\", \" an instance of Azure Redis Cache and a set of load \\nbalancers configured with sticky sessions for c\", \"lient affinity.\\nAzure SignalR Service is a fully managed Azure service that simplifies real-time com\", \"munication for \\nyour cloud-native applications. Technical implementation details like capacity provi\", \"sioning, scaling, \\nand persistent connections are abstracted away. They\\u2019re handled for you with a 99\", \".9% service-level \\nagreement. You focus on application features, not infrastructure plumbing.\\nOnce e\", \"nabled, a cloud-based HTTP service can push content updates directly to connected clients, \\nincludin\", \"g browser, mobile and desktop applications. Clients are updated without the need to poll the \\nserver\", \". Azure SignalR abstracts the transport technologies that create real-time connectivity, including \\n\", \"WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or \\nsp\", \"ecific subsets of connected clients.\\nFigure 4-7 shows a set of HTTP Clients connecting to a Cloud-na\", \"tive application with Azure SignalR \\nenabled.67 CHAPTER 4 | Cloud-native communication patterns\\nFigu\", \"re 4-7. Azure SignalR\\nAnother advantage of Azure SignalR Service comes with implementing Serverless \", \"cloud-native \\nservices. Perhaps your code is executed on demand with Azure Functions triggers. This \", \"scenario can \\nbe tricky because your code doesn\\u2019t maintain long connections with clients. Azure Sign\", \"alR Service can \\nhandle this situation since the service already manages connections for you.\\nAzure \", \"SignalR Service closely integrates with other Azure services, such as Azure SQL Database, \\nService B\", \"us, or Redis Cache, opening up many possibilities for your cloud-native applications.\\nService-to-ser\", \"vice communication\\nMoving from the front-end client, we now address back-end microservices communica\", \"te with each \\nother.\\nWhen constructing a cloud-native application, you\\u2019ll want to be sensitive to ho\", \"w back-end services \\ncommunicate with each other. Ideally, the less inter-service communication, the\", \" better. However, \\navoidance isn\\u2019t always possible as back-end services often rely on one another to\", \" complete an \\noperation.\\nThere are several widely accepted approaches to implementing cross-service \", \"communication. The type \\nof communication interaction will often determine the best approach.\\nConsid\", \"er the following interaction types:\\n\\u2022 Query \\u2013 when a calling microservice requires a response from a\", \" called microservice, such as, \\n\\u201cHey, give me the buyer information for a given customer Id.\\u201d68 CHAP\", \"TER 4 | Cloud-native communication patterns\\n\\u2022 Command \\u2013 when the calling microservice needs another \", \"microservice to execute an action \\nbut doesn\\u2019t require a response, such as, \\u201cHey, just ship this ord\", \"er.\\u201d\\n\\u2022 Event \\u2013 when a microservice, called the publisher, raises an event that state has changed or \", \"an \\naction has occurred. Other microservices, called subscribers, who are interested, can react to \\n\", \"the event appropriately. The publisher and the subscribers aren\\u2019t aware of each other.\\nMicroservice \", \"systems typically use a combination of these interaction types when executing \\noperations that requi\", \"re cross-service interaction. Let\\u2019s take a close look at each and how you might \\nimplement them.\\nQue\", \"ries\\nMany times, one microservice might need to query another, requiring an immediate response to \\nc\", \"omplete an operation. A shopping basket microservice may need product information and a price to \\nad\", \"d an item to its basket. There are many approaches for implementing query operations.\\nRequest/Respon\", \"se Messaging\\nOne option for implementing this scenario is for the calling back-end microservice to m\", \"ake direct \\nHTTP requests to the microservices it needs to query, shown in Figure 4-8.\\nFigure 4-8. D\", \"irect HTTP communication\\nWhile direct HTTP calls between microservices are relatively simple to impl\", \"ement, care should be \\ntaken to minimize this practice. To start, these calls are always synchronous\", \" and will block the \\noperation until a result is returned or the request times outs. What were once \", \"self-contained, \\nindependent services, able to evolve independently and deploy frequently, now becom\", \"e coupled to \\neach other. As coupling among microservices increase, their architectural benefits dim\", \"inish.69 CHAPTER 4 | Cloud-native communication patterns\\nExecuting an infrequent request that makes \", \"a single direct HTTP call to another microservice might be \\nacceptable for some systems. However, hi\", \"gh-volume calls that invoke direct HTTP calls to multiple \\nmicroservices aren\\u2019t advisable. They can \", \"increase latency and negatively impact the performance, \\nscalability, and availability of your syste\", \"m. Even worse, a long series of direct HTTP communication can \\nlead to deep and complex chains of sy\", \"nchronous microservices calls, shown in Figure 4-9:\\nFigure 4-9. Chaining HTTP queries\\nYou can certai\", \"nly imagine the risk in the design shown in the previous image. What happens if Step \\n#3 fails? Or S\", \"tep #8 fails? How do you recover? What if Step #6 is slow because the underlying service \\nis busy? H\", \"ow do you continue? Even if all works correctly, think of the latency this call would incur, \\nwhich \", \"is the sum of the latency of each step.\\nThe large degree of coupling in the previous image suggests \", \"the services weren\\u2019t optimally modeled. \\nIt would behoove the team to revisit their design.\\nMaterial\", \"ized View pattern\\nA popular option for removing microservice coupling is the Materialized View patte\", \"rn. With this \\npattern, a microservice stores its own local, denormalized copy of data that\\u2019s owned \", \"by other services. \\nInstead of the Shopping Basket microservice querying the Product Catalog and Pri\", \"cing microservices, \\nit maintains its own local copy of that data. This pattern eliminates unnecessa\", \"ry coupling and \\nimproves reliability and response time. The entire operation executes inside a sing\", \"le process. We \\nexplore this pattern and other data concerns in Chapter 5.\\nService Aggregator Patter\", \"n\\nAnother option for eliminating microservice-to-microservice coupling is an Aggregator microservice\", \", \\nshown in purple in Figure 4-10.70 CHAPTER 4 | Cloud-native communication patterns\\nFigure 4-10. Ag\", \"gregator microservice\\nThe pattern isolates an operation that makes calls to multiple back-end micros\", \"ervices, centralizing its \\nlogic into a specialized microservice. The purple checkout aggregator mic\", \"roservice in the previous \\nfigure orchestrates the workflow for the Checkout operation. It includes \", \"calls to several back-end \\nmicroservices in a sequenced order. Data from the workflow is aggregated \", \"and returned to the caller. \\nWhile it still implements direct HTTP calls, the aggregator microservic\", \"e reduces direct dependencies \\namong back-end microservices.\\nRequest/Reply Pattern\\nAnother approach \", \"for decoupling synchronous HTTP messages is a Request-Reply Pattern, which uses \\nqueuing communicati\", \"on. Communication using a queue is always a one-way channel, with a producer \\nsending the message an\", \"d consumer receiving it. With this pattern, both a request queue and response \\nqueue are implemented\", \", shown in Figure 4-11.71 CHAPTER 4 | Cloud-native communication patterns\\nFigure 4-11. Request-reply\", \" pattern\\nHere, the message producer creates a query-based message that contains a unique correlation\", \" ID and \\nplaces it into a request queue. The consuming service dequeues the messages, processes it a\", \"nd places \\nthe response into the response queue with the same correlation ID. The producer service d\", \"equeues \\nthe message, matches it with the correlation ID and continues processing. We cover queues i\", \"n detail \\nin the next section.\\nCommands\\nAnother type of communication interaction is a command. A mi\", \"croservice may need another \\nmicroservice to perform an action. The Ordering microservice may need t\", \"he Shipping microservice to \\ncreate a shipment for an approved order. In Figure 4-12, one microservi\", \"ce, called a Producer, sends a \\nmessage to another microservice, the Consumer, commanding it to do s\", \"omething.72 CHAPTER 4 | Cloud-native communication patterns\\nFigure 4-12. Command interaction with a \", \"queue\\nMost often, the Producer doesn\\u2019t require a response and can fire-and-forget the message. If a \", \"reply is \\nneeded, the Consumer sends a separate message back to Producer on another channel. A comma\", \"nd \\nmessage is best sent asynchronously with a message queue. supported by a lightweight message \\nbr\", \"oker. In the previous diagram, note how a queue separates and decouples both services.\\nA message que\", \"ue is an intermediary construct through which a producer and consumer pass a \\nmessage. Queues implem\", \"ent an asynchronous, point-to-point messaging pattern. The Producer \\nknows where a command needs to \", \"be sent and routes appropriately. The queue guarantees that a \\nmessage is processed by exactly one o\", \"f the consumer instances that are reading from the channel. In \\nthis scenario, either the producer o\", \"r consumer service can scale out without affecting the other. As \\nwell, technologies can be disparat\", \"e on each side, meaning that we might have a Java microservice \\ncalling a Golang microservice.\\nIn ch\", \"apter 1, we talked about backing services. Backing services are ancillary resources upon which \\nclou\", \"d-native systems depend. Message queues are backing services. The Azure cloud supports two \\ntypes of\", \" message queues that your cloud-native systems can consume to implement command \\nmessaging: Azure St\", \"orage Queues and Azure Service Bus Queues.\\nAzure Storage Queues\\nAzure storage queues offer a simple \", \"queueing infrastructure that is fast, affordable, and backed by \\nAzure storage accounts.\\nAzure Stora\", \"ge Queues feature a REST-based queuing mechanism with reliable and persistent \\nmessaging. They provi\", \"de a minimal feature set, but are inexpensive and store millions of messages. \\nTheir capacity ranges\", \" up to 500 TB. A single message can be up to 64 KB in size.\\nYou can access messages from anywhere in\", \" the world via authenticated calls using HTTP or HTTPS. \\nStorage queues can scale out to large numbe\", \"rs of concurrent clients to handle traffic spikes.73 CHAPTER 4 | Cloud-native communication patterns\", \"\\nThat said, there are limitations with the service:\\n\\u2022 Message order isn\\u2019t guaranteed.\\n\\u2022 A message ca\", \"n only persist for seven days before it\\u2019s automatically removed.\\n\\u2022 Support for state management, dup\", \"licate detection, or transactions isn\\u2019t available.\\nFigure 4-13 shows the hierarchy of an Azure Stora\", \"ge Queue.\\nFigure 4-13. Storage queue hierarchy\\nIn the previous figure, note how storage queues store\", \" their messages in the underlying Azure Storage \\naccount.\\nFor developers, Microsoft provides several\", \" client and server-side libraries for Storage queue \\nprocessing. Most major platforms are supported \", \"including .NET, Java, JavaScript, Ruby, Python, and \\nGo. Developers should never communicate directl\", \"y with these libraries. Doing so will tightly couple \\nyour microservice code to the Azure Storage Qu\", \"eue service. It\\u2019s a better practice to insulate the \\nimplementation details of the API. Introduce an\", \" intermediation layer, or intermediate API, that exposes \\ngeneric operations and encapsulates the co\", \"ncrete library. This loose coupling enables you to swap out \\none queuing service for another without\", \" having to make changes to the mainline service code.\\nAzure Storage queues are an economical option \", \"to implement command messaging in your cloud\\u0002native applications. Especially when a queue size will \", \"exceed 80 GB, or a simple feature set is \\nacceptable. You only pay for the storage of the messages; \", \"there are no fixed hourly charges.\\nAzure Service Bus Queues\\nFor more complex messaging requirements,\", \" consider Azure Service Bus queues.\\nSitting atop a robust message infrastructure, Azure Service Bus \", \"supports a brokered messaging model. \\nMessages are reliably stored in a broker (the queue) until rec\", \"eived by the consumer. The queue \\nguarantees First-In/First-Out (FIFO) message delivery, respecting \", \"the order in which messages were \\nadded to the queue.\\nThe size of a message can be much larger, up t\", \"o 256 KB. Messages are persisted in the queue for an \\nunlimited period of time. Service Bus supports\", \" not only HTTP-based calls, but also provides full 74 CHAPTER 4 | Cloud-native communication pattern\", \"s\\nsupport for the AMQP protocol. AMQP is an open-standard across vendors that supports a binary \\npro\", \"tocol and higher degrees of reliability.\\nService Bus provides a rich set of features, including tran\", \"saction support and a duplicate detection \\nfeature. The queue guarantees \\u201cat most once delivery\\u201d per\", \" message. It automatically discards a \\nmessage that has already been sent. If a producer is in doubt\", \", it can resend the same message, and \\nService Bus guarantees that only one copy will be processed. \", \"Duplicate detection frees you from \\nhaving to build additional infrastructure plumbing.\\nTwo more ent\", \"erprise features are partitioning and sessions. A conventional Service Bus queue is \\nhandled by a si\", \"ngle message broker and stored in a single message store. But, Service Bus Partitioning\\nspreads the \", \"queue across multiple message brokers and message stores. The overall throughput is no \\nlonger limit\", \"ed by the performance of a single message broker or messaging store. A temporary \\noutage of a messag\", \"ing store doesn\\u2019t render a partitioned queue unavailable.\\nService Bus Sessions provide a way to grou\", \"p-related messages. Imagine a workflow scenario where \\nmessages must be processed together and the o\", \"peration completed at the end. To take advantage, \\nsessions must be explicitly enabled for the queue\", \" and each related messaged must contain the same \\nsession ID.\\nHowever, there are some important cave\", \"ats: Service Bus queues size is limited to 80 GB, which is much \\nsmaller than what\\u2019s available from \", \"store queues. Additionally, Service Bus queues incur a base cost \\nand charge per operation.\\nFigure 4\", \"-14 outlines the high-level architecture of a Service Bus queue.\\nFigure 4-14. Service Bus queue\\nIn t\", \"he previous figure, note the point-to-point relationship. Two instances of the same provider are \\nen\", \"queuing messages into a single Service Bus queue. Each message is consumed by only one of three \\ncon\", \"sumer instances on the right. Next, we discuss how to implement messaging where different \\nconsumers\", \" may all be interested the same message.\\nEvents\\nMessage queuing is an effective way to implement com\", \"munication where a producer can \\nasynchronously send a consumer a message. However, what happens whe\", \"n many different consumers75 CHAPTER 4 | Cloud-native communication patterns\\nare interested in the s\", \"ame message? A dedicated message queue for each consumer wouldn\\u2019t scale \\nwell and would become diffi\", \"cult to manage.\\nTo address this scenario, we move to the third type of message interaction, the even\", \"t. One \\nmicroservice announces that an action had occurred. Other microservices, if interested, reac\", \"t to the \\naction, or event. This is also known as the event-driven architectural style.\\nEventing is \", \"a two-step process. For a given state change, a microservice publishes an event to a \\nmessage broker\", \", making it available to any other interested microservice. The interested microservice \\nis notified\", \" by subscribing to the event in the message broker. You use the Publish/Subscribe pattern \\nto implem\", \"ent event-based communication.\\nFigure 4-15 shows a shopping basket microservice publishing an event \", \"with two other microservices \\nsubscribing to it.\\nFigure 4-15. Event-Driven messaging\\nNote the event \", \"bus component that sits in the middle of the communication channel. It\\u2019s a custom \\nclass that encaps\", \"ulates the message broker and decouples it from the underlying application. The \\nordering and invent\", \"ory microservices independently operate the event with no knowledge of each \\nother, nor the shopping\", \" basket microservice. When the registered event is published to the event bus, \\nthey act upon it.\\nWi\", \"th eventing, we move from queuing technology to topics. A topic is similar to a queue, but supports \", \"\\na one-to-many messaging pattern. One microservice publishes a message. Multiple subscribing \\nmicros\", \"ervices can choose to receive and act upon that message. Figure 4-16 shows a topic \\narchitecture.76 \", \"CHAPTER 4 | Cloud-native communication patterns\\nFigure 4-16. Topic architecture\\nIn the previous figu\", \"re, publishers send messages to the topic. At the end, subscribers receive \\nmessages from subscripti\", \"ons. In the middle, the topic forwards messages to subscriptions based on a \\nset of rules, shown in \", \"dark blue boxes. Rules act as a filter that forward specific messages to a \\nsubscription. Here, a \\u201cG\", \"etPrice\\u201d event would be sent to the price and logging subscriptions as the \\nlogging subscription has\", \" chosen to receive all messages. A \\u201cGetInformation\\u201d event would be sent to \\nthe information and logg\", \"ing subscriptions.\\nThe Azure cloud supports two different topic services: Azure Service Bus Topics a\", \"nd Azure EventGrid.\\nAzure Service Bus Topics\\nSitting on top of the same robust brokered message mode\", \"l of Azure Service Bus queues are Azure \\nService Bus Topics. A topic can receive messages from multi\", \"ple independent publishers and send \\nmessages to up to 2,000 subscribers. Subscriptions can be dynam\", \"ically added or removed at run time \\nwithout stopping the system or recreating the topic.\\nMany advan\", \"ced features from Azure Service Bus queues are also available for topics, including \\nDuplicate Detec\", \"tion and Transaction support. By default, Service Bus topics are handled by a single \\nmessage broker\", \" and stored in a single message store. But, Service Bus Partitioning scales a topic by \\nspreading it\", \" across many message brokers and message stores.\\nScheduled Message Delivery tags a message with a sp\", \"ecific time for processing. The message won\\u2019t \\nappear in the topic before that time. Message Deferra\", \"l enables you to defer a retrieval of a message \\nto a later time. Both are commonly used in workflow\", \" processing scenarios where operations are \\nprocessed in a particular order. You can postpone proces\", \"sing of received messages until prior work \\nhas been completed.\\nService Bus topics are a robust and \", \"proven technology for enabling publish/subscribe communication \\nin your cloud-native systems.\\nAzure \", \"Event Grid\\nWhile Azure Service Bus is a battle-tested messaging broker with a full set of enterprise\", \" features, \\nAzure Event Grid is the new kid on the block.77 CHAPTER 4 | Cloud-native communication p\", \"atterns\\nAt first glance, Event Grid may look like just another topic-based messaging system. However\", \", it\\u2019s \\ndifferent in many ways. Focused on event-driven workloads, it enables real-time event proces\", \"sing, \\ndeep Azure integration, and an open-platform - all on serverless infrastructure. It\\u2019s designe\", \"d for \\ncontemporary cloud-native and serverless applications\\nAs a centralized eventing backplane, or\", \" pipe, Event Grid reacts to events inside Azure resources and \\nfrom your own services.\\nEvent notific\", \"ations are published to an Event Grid Topic, which, in turn, routes each event to a \\nsubscription. S\", \"ubscribers map to subscriptions and consume the events. Like Service Bus, Event Grid \\nsupports a fil\", \"tered subscriber model where a subscription sets rule for the events it wishes to receive. \\nEvent Gr\", \"id provides fast throughput with a guarantee of 10 million events per second enabling near \\nreal-tim\", \"e delivery - far more than what Azure Service Bus can generate.\\nA sweet spot for Event Grid is its d\", \"eep integration into the fabric of Azure infrastructure. An Azure \\nresource, such as Cosmos DB, can \", \"publish built-in events directly to other interested Azure resources -\\nwithout the need for custom c\", \"ode. Event Grid can publish events from an Azure Subscription, \\nResource Group, or Service, giving d\", \"evelopers fine-grained control over the lifecycle of cloud \\nresources. However, Event Grid isn\\u2019t lim\", \"ited to Azure. It\\u2019s an open platform that can consume custom \\nHTTP events published from application\", \"s or third-party services and route events to external \\nsubscribers.\\nWhen publishing and subscribing\", \" to native events from Azure resources, no coding is required. With \\nsimple configuration, you can i\", \"ntegrate events from one Azure resource to another leveraging built-in \\nplumbing for Topics and Subs\", \"criptions. Figure 4-17 shows the anatomy of Event Grid.\\nFigure 4-17. Event Grid anatomy78 CHAPTER 4 \", \"| Cloud-native communication patterns\\nA major difference between EventGrid and Service Bus is the un\", \"derlying message exchange pattern.\\nService Bus implements an older style pull model in which the dow\", \"nstream subscriber actively polls \\nthe topic subscription for new messages. On the upside, this appr\", \"oach gives the subscriber full control \\nof the pace at which it processes messages. It controls when\", \" and how many messages to process at \\nany given time. Unread messages remain in the subscription unt\", \"il processed. A significant \\nshortcoming is the latency between the time the event is generated and \", \"the polling operation that \\npulls that message to the subscriber for processing. Also, the overhead \", \"of constant polling for the \\nnext event consumes resources and money.\\nEventGrid, however, is differe\", \"nt. It implements a push model in which events are sent to the \\nEventHandlers as received, giving ne\", \"ar real-time event delivery. It also reduces cost as the service is \\ntriggered only when it\\u2019s needed\", \" to consume an event \\u2013 not continually as with polling. That said, an \\nevent handler must handle the\", \" incoming load and provide throttling mechanisms to protect itself \\nfrom becoming overwhelmed. Many \", \"Azure services that consume these events, such as Azure \\nFunctions and Logic Apps provide automatic \", \"autoscaling capabilities to handle increased loads.\\nEvent Grid is a fully managed serverless cloud s\", \"ervice. It dynamically scales based on your traffic and \\ncharges you only for your actual usage, not\", \" pre-purchased capacity. The first 100,000 operations per \\nmonth are free \\u2013 operations being defined\", \" as event ingress (incoming event notifications), \\nsubscription delivery attempts, management calls,\", \" and filtering by subject. With 99.99% availability, \\nEventGrid guarantees the delivery of an event \", \"within a 24-hour period, with built-in retry functionality \\nfor unsuccessful delivery. Undelivered m\", \"essages can be moved to a \\u201cdead-letter\\u201d queue for resolution. \\nUnlike Azure Service Bus, Event Grid \", \"is tuned for fast performance and doesn\\u2019t support features like \\nordered messaging, transactions, an\", \"d sessions.\\nStreaming messages in the Azure cloud\\nAzure Service Bus and Event Grid provide great sup\", \"port for applications that expose single, discrete \\nevents like a new document has been inserted int\", \"o a Cosmos DB. But, what if your cloud-native \\nsystem needs to process a stream of related events? E\", \"vent streams are more complex. They\\u2019re typically \\ntime-ordered, interrelated, and must be processed \", \"as a group.\\nAzure Event Hub is a data streaming platform and event ingestion service that collects, \", \"transforms, \\nand stores events. It\\u2019s fine-tuned to capture streaming data, such as continuous event \", \"notifications \\nemitted from a telemetry context. The service is highly scalable and can store and pr\", \"ocess millions of \\nevents per second. Shown in Figure 4-18, it\\u2019s often a front door for an event pip\", \"eline, decoupling \\ningest stream from event consumption.79 CHAPTER 4 | Cloud-native communication pa\", \"tterns\\nFigure 4-18. Azure Event Hub\\nEvent Hub supports low latency and configurable time retention. \", \"Unlike queues and topics, Event \\nHubs keep event data after it\\u2019s been read by a consumer. This featu\", \"re enables other data analytic \\nservices, both internal and external, to replay the data for further\", \" analysis. Events stored in event hub \\nare only deleted upon expiration of the retention period, whi\", \"ch is one day by default, but \\nconfigurable.\\nEvent Hub supports common event publishing protocols in\", \"cluding HTTPS and AMQP. It also supports \\nKafka 1.0. Existing Kafka applications can communicate wit\", \"h Event Hub using the Kafka protocol \\nproviding an alternative to managing large Kafka clusters. Man\", \"y open-source cloud-native systems \\nembrace Kafka.\\nEvent Hubs implements message streaming through a\", \" partitioned consumer model in which each \\nconsumer only reads a specific subset, or partition, of t\", \"he message stream. This pattern enables \\ntremendous horizontal scale for event processing and provid\", \"es other stream-focused features that are \\nunavailable in queues and topics. A partition is an order\", \"ed sequence of events that is held in an event \\nhub. As newer events arrive, they\\u2019re added to the en\", \"d of this sequence. Figure 4-19 shows partitioning \\nin an Event Hub.\\nFigure 4-19. Event Hub partitio\", \"ning\\nInstead of reading from the same resource, each consumer group reads across a subset, or partit\", \"ion, \\nof the message stream.80 CHAPTER 4 | Cloud-native communication patterns\\nFor cloud-native appl\", \"ications that must stream large numbers of events, Azure Event Hub can be a \\nrobust and affordable s\", \"olution.\\ngRPC\\nSo far in this book, we\\u2019ve focused on REST-based communication. We\\u2019ve seen that REST i\", \"s a flexible \\narchitectural style that defines CRUD-based operations against entity resources. Clien\", \"ts interact with \\nresources across HTTP with a request/response communication model. While REST is w\", \"idely \\nimplemented, a newer communication technology, gRPC, has gained tremendous momentum across \\nt\", \"he cloud-native community.\\nWhat is gRPC?\\ngRPC is a modern, high-performance framework that evolves t\", \"he age-old remote procedure call (RPC)\\nprotocol. At the application level, gRPC streamlines messagin\", \"g between clients and back-end services. \\nOriginating from Google, gRPC is open source and part of t\", \"he Cloud Native Computing Foundation \\n(CNCF) ecosystem of cloud-native offerings. CNCF considers gRP\", \"C an incubating project. Incubating \\nmeans end users are using the technology in production applicat\", \"ions, and the project has a healthy \\nnumber of contributors.\\nA typical gRPC client app will expose a\", \" local, in-process function that implements a business \\noperation. Under the covers, that local func\", \"tion invokes another function on a remote machine. What \\nappears to be a local call essentially beco\", \"mes a transparent out-of-process call to a remote service. \\nThe RPC plumbing abstracts the point-to-\", \"point networking communication, serialization, and \\nexecution between computers.\\nIn cloud-native app\", \"lications, developers often work across programming languages, frameworks, and \\ntechnologies. This i\", \"nteroperability complicates message contracts and the plumbing required for \\ncross-platform communic\", \"ation. gRPC provides a \\u201cuniform horizontal layer\\u201d that abstracts these \\nconcerns. Developers code in\", \" their native platform focused on business functionality, while gRPC \\nhandles communication plumbing\", \".\\ngRPC offers comprehensive support across most popular development stacks, including Java, \\nJavaScr\", \"ipt, C#, Go, Swift, and NodeJS.\\ngRPC Benefits\\ngRPC uses HTTP/2 for its transport protocol. While com\", \"patible with HTTP 1.1, HTTP/2 features many \\nadvanced capabilities:\\n\\u2022 A binary framing protocol for \", \"data transport - unlike HTTP 1.1, which is text based.\\n\\u2022 Multiplexing support for sending multiple p\", \"arallel requests over the same connection - HTTP \\n1.1 limits processing to one request/response mess\", \"age at a time.\\n\\u2022 Bidirectional full-duplex communication for sending both client requests and server\", \" responses \\nsimultaneously.\\n\\u2022 Built-in streaming enabling requests and responses to asynchronously s\", \"tream large data sets.\\n\\u2022 Header compression that reduces network usage.81 CHAPTER 4 | Cloud-native c\", \"ommunication patterns\\ngRPC is lightweight and highly performant. It can be up to 8x faster than JSON\", \" serialization with \\nmessages 60-80% smaller. In Microsoft Windows Communication Foundation (WCF) pa\", \"rlance, gRPC \\nperformance exceeds the speed and efficiency of the highly optimized NetTCP bindings. \", \"Unlike \\nNetTCP, which favors the Microsoft stack, gRPC is cross-platform.\\nProtocol Buffers\\ngRPC embr\", \"aces an open-source technology called Protocol Buffers. They provide a highly efficient \\nand platfor\", \"m-neutral serialization format for serializing structured messages that services send to \\neach other\", \". Using a cross-platform Interface Definition Language (IDL), developers define a service \\ncontract \", \"for each microservice. The contract, implemented as a text-based .proto file, describes the \\nmethods\", \", inputs, and outputs for each service. The same contract file can be used for gRPC clients and \\nser\", \"vices built on different development platforms.\\nUsing the proto file, the Protobuf compiler, protoc,\", \" generates both client and service code for your \\ntarget platform. The code includes the following c\", \"omponents:\\n\\u2022 Strongly typed objects, shared by the client and service, that represent the service op\", \"erations \\nand data elements for a message.\\n\\u2022 A strongly typed base class with the required network p\", \"lumbing that the remote gRPC service \\ncan inherit and extend.\\n\\u2022 A client stub that contains the requ\", \"ired plumbing to invoke the remote gRPC service.\\nAt run time, each message is serialized as a standa\", \"rd Protobuf representation and exchanged between \\nthe client and remote service. Unlike JSON or XML,\", \" Protobuf messages are serialized as compiled \\nbinary bytes.\\nThe book, gRPC for WCF Developers, avai\", \"lable from the Microsoft Architecture site, provides in-depth \\ncoverage of gRPC and Protocol Buffers\", \".\\ngRPC support in .NET\\ngRPC is integrated into .NET Core 3.0 SDK and later. The following tools supp\", \"ort it:\\n\\u2022 Visual Studio 2022 with the ASP.NET and web development workload installed\\n\\u2022 Visual Studio\", \" Code\\n\\u2022 The dotnet CLI\\nThe SDK includes tooling for endpoint routing, built-in IoC, and logging. The\", \" open-source Kestrel web \\nserver supports HTTP/2 connections. Figure 4-20 shows a Visual Studio 2022\", \" template that scaffolds a \\nskeleton project for a gRPC service. Note how .NET fully supports Window\", \"s, Linux, and macOS.82 CHAPTER 4 | Cloud-native communication patterns\\nFigure 4-20. gRPC support in \", \"Visual Studio 2022\\nFigure 4-21 shows the skeleton gRPC service generated from the built-in scaffoldi\", \"ng included in \\nVisual Studio 2022.\\nFigure 4-21. gRPC project in Visual Studio 2022\\nIn the previous \", \"figure, note the proto description file and service code. As you\\u2019ll see shortly, Visual \\nStudio gene\", \"rates additional configuration in both the Startup class and underlying project file.\\ngRPC usage\\nFav\", \"or gRPC for the following scenarios:\\n\\u2022 Synchronous backend microservice-to-microservice communicatio\", \"n where an immediate \\nresponse is required to continue processing.\\n\\u2022 Polyglot environments that need\", \" to support mixed programming platforms.\\n\\u2022 Low latency and high throughput communication where perfo\", \"rmance is critical.\\n\\u2022 Point-to-point real-time communication - gRPC can push messages in real time w\", \"ithout \\npolling and has excellent support for bi-directional streaming.83 CHAPTER 4 | Cloud-native c\", \"ommunication patterns\\n\\u2022 Network constrained environments \\u2013 binary gRPC messages are always smaller t\", \"han an \\nequivalent text-based JSON message.\\nAt the time, of this writing, gRPC is primarily used wit\", \"h backend services. Modern browsers can\\u2019t \\nprovide the level of HTTP/2 control required to support a\", \" front-end gRPC client. That said, there\\u2019s \\nsupport for gRPC-Web with .NET that enables gRPC communi\", \"cation from browser-based apps built \\nwith JavaScript or Blazor WebAssembly technologies. gRPC-Web e\", \"nables an ASP.NET Core gRPC app \\nto support gRPC features in browser apps:\\n\\u2022 Strongly typed, code-ge\", \"nerated clients\\n\\u2022 Compact Protobuf messages\\n\\u2022 Server streaming\\ngRPC implementation\\nThe microservice \", \"reference architecture, eShop on Containers, from Microsoft, shows how to \\nimplement gRPC services i\", \"n .NET applications. Figure 4-22 presents the back-end architecture.\\nFigure 4-22. Backend architectu\", \"re for eShop on Containers\\nIn the previous figure, note how eShop embraces the Backend for Frontends\", \" pattern (BFF) by \\nexposing multiple API gateways. We discussed the BFF pattern earlier in this chap\", \"ter. Pay close 84 CHAPTER 4 | Cloud-native communication patterns\\nattention to the Aggregator micros\", \"ervice (in gray) that sits between the Web-Shopping API Gateway \\nand backend Shopping microservices.\", \" The Aggregator receives a single request from a client, \\ndispatches it to various microservices, ag\", \"gregates the results, and sends them back to the requesting \\nclient. Such operations typically requi\", \"re synchronous communication as to produce an immediate \\nresponse. In eShop, backend calls from the \", \"Aggregator are performed using gRPC as shown in Figure \\n4-23.\\nFigure 4-23. gRPC in eShop on Containe\", \"rs\\ngRPC communication requires both client and server components. In the previous figure, note how \\n\", \"the Shopping Aggregator implements a gRPC client. The client makes synchronous gRPC calls (in red) \\n\", \"to backend microservices, each of which implement a gRPC server. Both the client and server take \\nad\", \"vantage of the built-in gRPC plumbing from the .NET SDK. Client-side stubs provide the plumbing \\nto \", \"invoke remote gRPC calls. Server-side components provide gRPC plumbing that custom service \\nclasses \", \"can inherit and consume.\\nMicroservices that expose both a RESTful API and gRPC communication require\", \" multiple endpoints to \\nmanage traffic. You would open an endpoint that listens for HTTP traffic for\", \" the RESTful calls and \\nanother for gRPC calls. The gRPC endpoint must be configured for the HTTP/2 \", \"protocol that is \\nrequired for gRPC communication.\\nWhile we strive to decouple microservices with as\", \"ynchronous communication patterns, some \\noperations require direct calls. gRPC should be the primary\", \" choice for direct synchronous \\ncommunication between microservices. Its high-performance communicat\", \"ion protocol, based on \\nHTTP/2 and protocol buffers, make it a perfect choice.85 CHAPTER 4 | Cloud-n\", \"ative communication patterns\\nLooking ahead\\nLooking ahead, gRPC will continue to gain traction for cl\", \"oud-native systems. The performance \\nbenefits and ease of development are compelling. However, REST \", \"will likely be around for a long time. \\nIt excels for publicly exposed APIs and for backward compati\", \"bility reasons.\\nService Mesh communication infrastructure\\nThroughout this chapter, we\\u2019ve explored th\", \"e challenges of microservice communication. We said that \\ndevelopment teams need to be sensitive to \", \"how back-end services communicate with each other. \\nIdeally, the less inter-service communication, t\", \"he better. However, avoidance isn\\u2019t always possible as \\nback-end services often rely on one another \", \"to complete operations.\\nWe explored different approaches for implementing synchronous HTTP communica\", \"tion and \\nasynchronous messaging. In each of the cases, the developer is burdened with implementing \", \"\\ncommunication code. Communication code is complex and time intensive. Incorrect decisions can \\nlead\", \" to significant performance issues.\\nA more modern approach to microservice communication centers aro\", \"und a new and rapidly evolving \\ntechnology entitled Service Mesh. A service mesh is a configurable i\", \"nfrastructure layer with built-in \\ncapabilities to handle service-to-service communication, resilien\", \"cy, and many cross-cutting concerns. \\nIt moves the responsibility for these concerns out of the micr\", \"oservices and into service mesh layer.\\nCommunication is abstracted away from your microservices.\\nA k\", \"ey component of a service mesh is a proxy. In a cloud-native application, an instance of a proxy is \", \"\\ntypically colocated with each microservice. While they execute in separate processes, the two are \\n\", \"closely linked and share the same lifecycle. This pattern, known as the Sidecar pattern, and is show\", \"n in \\nFigure 4-24.\\nFigure 4-24. Service mesh with a side car86 CHAPTER 4 | Cloud-native communicatio\", \"n patterns\\nNote in the previous figure how messages are intercepted by a proxy that runs alongside e\", \"ach \\nmicroservice. Each proxy can be configured with traffic rules specific to the microservice. It \", \"\\nunderstands messages and can route them across your services and the outside world.\\nAlong with mana\", \"ging service-to-service communication, the Service Mesh provides support for \\nservice discovery and \", \"load balancing.\\nOnce configured, a service mesh is highly functional. The mesh retrieves a correspon\", \"ding pool of \\ninstances from a service discovery endpoint. It sends a request to a specific service \", \"instance, recording \\nthe latency and response type of the result. It chooses the instance most likel\", \"y to return a fast \\nresponse based on different factors, including the observed latency for recent r\", \"equests.\\nA service mesh manages traffic, communication, and networking concerns at the application l\", \"evel. It \\nunderstands messages and requests. A service mesh typically integrates with a container or\", \"chestrator. \\nKubernetes supports an extensible architecture in which a service mesh can be added.\\nIn\", \" chapter 6, we deep-dive into Service Mesh technologies including a discussion on its architecture \\n\", \"and available open-source implementations.\\nSummary\\nIn this chapter, we discussed cloud-native commun\", \"ication patterns. We started by examining how \\nfront-end clients communicate with back-end microserv\", \"ices. Along the way, we talked about API \\nGateway platforms and real-time communication. We then loo\", \"ked at how microservices communicate \\nwith other back-end services. We looked at both synchronous HT\", \"TP communication and \\nasynchronous messaging across services. We covered gRPC, an upcoming technolog\", \"y in the cloud\\u0002native world. Finally, we introduced a new and rapidly evolving technology entitled S\", \"ervice Mesh that \\ncan streamline microservice communication.\\nSpecial emphasis was on managed Azure s\", \"ervices that can help implement communication in cloud\\u0002native systems:\\n\\u2022 Azure Application Gateway\\n\\u2022\", \" Azure API Management\\n\\u2022 Azure SignalR Service\\n\\u2022 Azure Storage Queues\\n\\u2022 Azure Service Bus\\n\\u2022 Azure Eve\", \"nt Grid\\n\\u2022 Azure Event Hub\\nWe next move to distributed data in cloud-native systems and the benefits \", \"and challenges that it \\npresents.\\nReferences\\n\\u2022 .NET Microservices: Architecture for Containerized .N\", \"ET applications\\n\\u2022 Designing Interservice Communication for Microservices\\n\\u2022 Azure SignalR Service, a \", \"fully managed service to add real-time functionality87 CHAPTER 4 | Cloud-native communication patter\", \"ns\\n\\u2022 Azure API Gateway Ingress Controller\\n\\u2022 gRPC Documentation\\n\\u2022 gRPC for WCF Developers\\n\\u2022 Comparing\", \" gRPC Services with HTTP APIs\\n\\u2022 Building gRPC Services with .NET video88 CHAPTER 5 | Cloud-native da\", \"ta patterns\\nCHAPTER 5\\nCloud-native data patterns\\nAs we\\u2019ve seen throughout this book, a cloud-native \", \"approach changes the way you design, deploy, \\nand manage applications. It also changes the way you m\", \"anage and store data.\\nFigure 5-1 contrasts the differences.\\nFigure 5-1. Data management in cloud-nat\", \"ive applications\\nExperienced developers will easily recognize the architecture on the left-side of f\", \"igure 5-1. In this \\nmonolithic application, business service components collocate together in a shar\", \"ed services tier, \\nsharing data from a single relational database.\\nIn many ways, a single database k\", \"eeps data management simple. Querying data across multiple tables \\nis straightforward. Changes to da\", \"ta update together or they all rollback. ACID transactions guarantee \\nstrong and immediate consisten\", \"cy.\\nDesigning for cloud-native, we take a different approach. On the right-side of Figure 5-1, note \", \"how \\nbusiness functionality segregates into small, independent microservices. Each microservice \\nenc\", \"apsulates a specific business capability and its own data. The monolithic database decomposes 89 CHA\", \"PTER 5 | Cloud-native data patterns\\ninto a distributed data model with many smaller databases, each \", \"aligning with a microservice. When \\nthe smoke clears, we emerge with a design that exposes a databas\", \"e per microservice.\\nDatabase-per-microservice, why?\\nThis database per microservice provides many ben\", \"efits, especially for systems that must evolve rapidly \\nand support massive scale. With this model\\u2026\\n\", \"\\u2022 Domain data is encapsulated within the service\\n\\u2022 Data schema can evolve without directly impacting\", \" other services\\n\\u2022 Each data store can independently scale\\n\\u2022 A data store failure in one service won\\u2019\", \"t directly impact other services\\nSegregating data also enables each microservice to implement the da\", \"ta store type that is best \\noptimized for its workload, storage needs, and read/write patterns. Choi\", \"ces include relational, \\ndocument, key-value, and even graph-based data stores.\\nFigure 5-2 presents \", \"the principle of polyglot persistence in a cloud-native system.\\nFigure 5-2. Polyglot data persistenc\", \"e\\nNote in the previous figure how each microservice supports a different type of data store.\\n\\u2022 The p\", \"roduct catalog microservice consumes a relational database to accommodate the rich \\nrelational struc\", \"ture of its underlying data.\\n\\u2022 The shopping cart microservice consumes a distributed cache that supp\", \"orts its simple, key\\u0002value data store.\\n\\u2022 The ordering microservice consumes both a NoSql document da\", \"tabase for write operations \\nalong with a highly denormalized key/value store to accommodate high-vo\", \"lumes of read \\noperations.90 CHAPTER 5 | Cloud-native data patterns\\nWhile relational databases remai\", \"n relevant for microservices with complex data, NoSQL databases \\nhave gained considerable popularity\", \". They provide massive scale and high availability. Their \\nschemaless nature allows developers to mo\", \"ve away from an architecture of typed data classes and \\nORMs that make change expensive and time-con\", \"suming. We cover NoSQL databases later in this \\nchapter.\\nWhile encapsulating data into separate micr\", \"oservices can increase agility, performance, and scalability, \\nit also presents many challenges. In \", \"the next section, we discuss these challenges along with patterns \\nand practices to help overcome th\", \"em.\\nCross-service queries\\nWhile microservices are independent and focus on specific functional capab\", \"ilities, like inventory, \\nshipping, or ordering, they frequently require integration with other micr\", \"oservices. Often the \\nintegration involves one microservice querying another for data. Figure 5-3 sh\", \"ows the scenario.\\nFigure 5-3. Querying across microservices\\nIn the preceding figure, we see a shoppi\", \"ng basket microservice that adds an item to a user\\u2019s shopping \\nbasket. While the data store for this\", \" microservice contains basket and line item data, it doesn\\u2019t \\nmaintain product or pricing data. Inst\", \"ead, those data items are owned by the catalog and pricing \\nmicroservices. This aspect presents a pr\", \"oblem. How can the shopping basket microservice add a \\nproduct to the user\\u2019s shopping basket when it\", \" doesn\\u2019t have product nor pricing data in its database?\\nOne option discussed in Chapter 4 is a direc\", \"t HTTP call from the shopping basket to the catalog and \\npricing microservices. However, in chapter \", \"4, we said synchronous HTTP calls couple microservices \\ntogether, reducing their autonomy and dimini\", \"shing their architectural benefits.\\nWe could also implement a request-reply pattern with separate in\", \"bound and outbound queues for \\neach service. However, this pattern is complicated and requires plumb\", \"ing to correlate request and \\nresponse messages. While it does decouple the backend microservice cal\", \"ls, the calling service must \\nstill synchronously wait for the call to complete. Network congestion,\", \" transient faults, or an \\noverloaded microservice and can result in long-running and even failed ope\", \"rations.91 CHAPTER 5 | Cloud-native data patterns\\nInstead, a widely accepted pattern for removing cr\", \"oss-service dependencies is the Materialized View \\nPattern, shown in Figure 5-4.\\nFigure 5-4. Materia\", \"lized View Pattern\\nWith this pattern, you place a local data table (known as a read model) in the sh\", \"opping basket service. \\nThis table contains a denormalized copy of the data needed from the product \", \"and pricing \\nmicroservices. Copying the data directly into the shopping basket microservice eliminat\", \"es the need for \\nexpensive cross-service calls. With the data local to the service, you improve the \", \"service\\u2019s response \\ntime and reliability. Additionally, having its own copy of the data makes the sh\", \"opping basket service \\nmore resilient. If the catalog service should become unavailable, it wouldn\\u2019t\", \" directly impact the \\nshopping basket service. The shopping basket can continue operating with the d\", \"ata from its own \\nstore.\\nThe catch with this approach is that you now have duplicate data in your sy\", \"stem. However, \\nstrategically duplicating data in cloud-native systems is an established practice an\", \"d not considered an \\nanti-pattern, or bad practice. Keep in mind that one and only one service can o\", \"wn a data set and have \\nauthority over it. You\\u2019ll need to synchronize the read models when the syste\", \"m of record is updated. \\nSynchronization is typically implemented via asynchronous messaging with a \", \"publish/subscribe \\npattern, as shown in Figure 5.4.\\nDistributed transactions\\nWhile querying data acr\", \"oss microservices is difficult, implementing a transaction across several \\nmicroservices is even mor\", \"e complex. The inherent challenge of maintaining data consistency across \\nindependent data sources i\", \"n different microservices can\\u2019t be understated. The lack of distributed \\ntransactions in cloud-nativ\", \"e applications means that you must manage distributed transactions \\nprogrammatically. You move from \", \"a world of immediate consistency to that of eventual consistency.\\nFigure 5-5 shows the problem.92 CH\", \"APTER 5 | Cloud-native data patterns\\nFigure 5-5. Implementing a transaction across microservices\\nIn \", \"the preceding figure, five independent microservices participate in a distributed transaction that \\n\", \"creates an order. Each microservice maintains its own data store and implements a local transaction \", \"\\nfor its store. To create the order, the local transaction for each individual microservice must suc\", \"ceed, \\nor all must abort and roll back the operation. While built-in transactional support is availa\", \"ble inside \\neach of the microservices, there\\u2019s no support for a distributed transaction that would s\", \"pan across all \\nfive services to keep data consistent.\\nInstead, you must construct this distributed \", \"transaction programmatically.\\nA popular pattern for adding distributed transactional support is the \", \"Saga pattern. It\\u2019s implemented \\nby grouping local transactions together programmatically and sequent\", \"ially invoking each one. If any \\nof the local transactions fail, the Saga aborts the operation and i\", \"nvokes a set of compensating \\ntransactions. The compensating transactions undo the changes made by t\", \"he preceding local \\ntransactions and restore data consistency. Figure 5-6 shows a failed transaction\", \" with the Saga pattern.\\nFigure 5-6. Rolling back a transaction93 CHAPTER 5 | Cloud-native data patte\", \"rns\\nIn the previous figure, the Update Inventory operation has failed in the Inventory microservice.\", \" The \\nSaga invokes a set of compensating transactions (in red) to adjust the inventory counts, cance\", \"l the \\npayment and the order, and return the data for each microservice back to a consistent state.\\n\", \"Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of\", \" \\nrelated commands. In Chapter 4, we discussed the service aggregator pattern that would be the \\nfou\", \"ndation for an orchestrated saga implementation. We also discussed eventing along with Azure \\nServic\", \"e Bus and Azure Event Grid topics that would be a foundation for a choreographed saga \\nimplementatio\", \"n.\\nHigh volume data\\nLarge cloud-native applications often support high-volume data requirements. In \", \"these scenarios, \\ntraditional data storage techniques can cause bottlenecks. For complex systems tha\", \"t deploy on a large \\nscale, both Command and Query Responsibility Segregation (CQRS) and Event Sourc\", \"ing may improve \\napplication performance.\\nCQRS\\nCQRS, is an architectural pattern that can help maxim\", \"ize performance, scalability, and security. The \\npattern separates operations that read data from th\", \"ose operations that write data.\\nFor normal scenarios, the same entity model and data repository obje\", \"ct are used for both read and \\nwrite operations.\\nHowever, a high volume data scenario can benefit fr\", \"om separate models and data tables for reads \\nand writes. To improve performance, the read operation\", \" could query against a highly denormalized \\nrepresentation of the data to avoid expensive repetitive\", \" table joins and table locks. The write\\noperation, known as a command, would update against a fully \", \"normalized representation of the data \\nthat would guarantee consistency. You then need to implement \", \"a mechanism to keep both \\nrepresentations in sync. Typically, whenever the write table is modified, \", \"it publishes an event that \\nreplicates the modification to the read table.\\nFigure 5-7 shows an imple\", \"mentation of the CQRS pattern.\\nFigure 5-7. CQRS implementation94 CHAPTER 5 | Cloud-native data patte\", \"rns\\nIn the previous figure, separate command and query models are implemented. Each data write \\noper\", \"ation is saved to the write store and then propagated to the read store. Pay close attention to \\nhow\", \" the data propagation process operates on the principle of eventual consistency. The read model \\neve\", \"ntually synchronizes with the write model, but there may be some lag in the process. We discuss \\neve\", \"ntual consistency in the next section.\\nThis separation enables reads and writes to scale independent\", \"ly. Read operations use a schema \\noptimized for queries, while the writes use a schema optimized for\", \" updates. Read queries go against \\ndenormalized data, while complex business logic can be applied to\", \" the write model. As well, you \\nmight impose tighter security on write operations than those exposin\", \"g reads.\\nImplementing CQRS can improve application performance for cloud-native services. However, i\", \"t does \\nresult in a more complex design. Apply this principle carefully and strategically to those s\", \"ections of \\nyour cloud-native application that will benefit from it. For more on CQRS, see the Micro\", \"soft book .NET \\nMicroservices: Architecture for Containerized .NET Applications.\\nEvent sourcing\\nAnot\", \"her approach to optimizing high volume data scenarios involves Event Sourcing.\\nA system typically st\", \"ores the current state of a data entity. If a user changes their phone number, for \\nexample, the cus\", \"tomer record is updated with the new number. We always know the current state of a \\ndata entity, but\", \" each update overwrites the previous state.\\nIn most cases, this model works fine. In high volume sys\", \"tems, however, overhead from transactional \\nlocking and frequent update operations can impact databa\", \"se performance, responsiveness, and limit \\nscalability.\\nEvent Sourcing takes a different approach to\", \" capturing data. Each operation that affects data is \\npersisted to an event store. Instead of updati\", \"ng the state of a data record, we append each change to \\na sequential list of past events - similar \", \"to an accountant\\u2019s ledger. The Event Store becomes the \\nsystem of record for the data. It\\u2019s used to \", \"propagate various materialized views within the bounded \\ncontext of a microservice. Figure 5.8 shows\", \" the pattern.95 CHAPTER 5 | Cloud-native data patterns\\nFigure 5-8. Event Sourcing\\nIn the previous fi\", \"gure, note how each entry (in blue) for a user\\u2019s shopping cart is appended to an \\nunderlying event s\", \"tore. In the adjoining materialized view, the system projects the current state by \\nreplaying all th\", \"e events associated with each shopping cart. This view, or read model, is then exposed \\nback to the \", \"UI. Events can also be integrated with external systems and applications or queried to \\ndetermine th\", \"e current state of an entity. With this approach, you maintain history. You know not only \\nthe curre\", \"nt state of an entity, but also how you reached this state.\\nMechanically speaking, event sourcing si\", \"mplifies the write model. There are no updates or deletes. \\nAppending each data entry as an immutabl\", \"e event minimizes contention, locking, and concurrency \\nconflicts associated with relational databas\", \"es. Building read models with the materialized view pattern \\nenables you to decouple the view from t\", \"he write model and choose the best data store to optimize \\nthe needs of your application UI.\\nFor thi\", \"s pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, \\nMongoDB, C\", \"assandra, CouchDB, and RavenDB are good candidates.\\nAs with all patterns and technologies, implement\", \" strategically and when needed. While event sourcing \\ncan provide increased performance and scalabil\", \"ity, it comes at the expense of complexity and a \\nlearning curve.96 CHAPTER 5 | Cloud-native data pa\", \"tterns\\nRelational vs. NoSQL data\\nRelational and NoSQL are two types of database systems commonly imp\", \"lemented in cloud-native \\napps. They\\u2019re built differently, store data differently, and accessed diff\", \"erently. In this section, we\\u2019ll look \\nat both. Later in this chapter, we\\u2019ll look at an emerging data\", \"base technology called NewSQL.\\nRelational databases have been a prevalent technology for decades. Th\", \"ey\\u2019re mature, proven, and \\nwidely implemented. Competing database products, tooling, and expertise a\", \"bound. Relational \\ndatabases provide a store of related data tables. These tables have a fixed schem\", \"a, use SQL \\n(Structured Query Language) to manage data, and support ACID guarantees.\\nNo-SQL database\", \"s refer to high-performance, non-relational data stores. They excel in their ease-of\\u0002use, scalabilit\", \"y, resilience, and availability characteristics. Instead of joining tables of normalized data, \\nNoSQ\", \"L stores unstructured or semi-structured data, often in key-value pairs or JSON documents. No\\u0002SQL da\", \"tabases typically don\\u2019t provide ACID guarantees beyond the scope of a single database \\npartition. Hi\", \"gh volume services that require sub second response time favor NoSQL datastores.\\nThe impact of NoSQL\", \" technologies for distributed cloud-native systems can\\u2019t be overstated. The \\nproliferation of new da\", \"ta technologies in this space has disrupted solutions that once exclusively \\nrelied on relational da\", \"tabases.\\nNoSQL databases include several different models for accessing and managing data, each suit\", \"ed to \\nspecific use cases. Figure 5-9 presents four common models.\\nFigure 5-9: Data models for NoSQL\", \" databases\\nModel Characteristics\\nDocument Store Data and metadata are stored hierarchically in \\nJSON\", \"-based documents inside the database.\\nKey Value Store The simplest of the NoSQL databases, data is \\n\", \"represented as a collection of key-value pairs.\\nWide-Column Store Related data is stored as a set of\", \" nested\\u0002key/value pairs within a single column.\\nGraph Store Data is stored in a graph structure as n\", \"ode, \\nedge, and data properties.97 CHAPTER 5 | Cloud-native data patterns\\nThe CAP theorem\\nAs a way t\", \"o understand the differences between these types of databases, consider the CAP theorem, \\na set of p\", \"rinciples applied to distributed systems that store state. Figure 5-10 shows the three \\nproperties o\", \"f the CAP theorem.\\nFigure 5-10. The CAP theorem\\nThe theorem states that distributed data systems wil\", \"l offer a trade-off between consistency, \\navailability, and partition tolerance. And, that any datab\", \"ase can only guarantee two of the three \\nproperties:\\n\\u2022 Consistency. Every node in the cluster respon\", \"ds with the most recent data, even if the system \\nmust block the request until all replicas update. \", \"If you query a \\u201cconsistent system\\u201d for an item \\nthat is currently updating, you\\u2019ll wait for that res\", \"ponse until all replicas successfully update. \\nHowever, you\\u2019ll receive the most current data.\\n\\u2022 Avai\", \"lability. Every node returns an immediate response, even if that response isn\\u2019t the most \\nrecent dat\", \"a. If you query an \\u201cavailable system\\u201d for an item that is updating, you\\u2019ll get the best \\npossible an\", \"swer the service can provide at that moment.\\n\\u2022 Partition Tolerance. Guarantees the system continues \", \"to operate even if a replicated data \\nnode fails or loses connectivity with other replicated data no\", \"des.\\nCAP theorem explains the tradeoffs associated with managing consistency and availability during\", \" a \\nnetwork partition; however tradeoffs with respect to consistency and performance also exist with\", \" the \\nabsence of a network partition. CAP theorem is often further extended to PACELC to explain the\", \" \\ntradeoffs more comprehensively.\\nRelational databases typically provide consistency and availabilit\", \"y, but not partition tolerance. They\\u2019re \\ntypically provisioned to a single server and scale vertical\", \"ly by adding more resources to the machine.98 CHAPTER 5 | Cloud-native data patterns\\nMany relational\", \" database systems support built-in replication features where copies of the primary \\ndatabase can be\", \" made to other secondary server instances. Write operations are made to the primary \\ninstance and re\", \"plicated to each of the secondaries. Upon a failure, the primary instance can fail over \\nto a second\", \"ary to provide high availability. Secondaries can also be used to distribute read operations. \\nWhile\", \" writes operations always go against the primary replica, read operations can be routed to any of \\nt\", \"he secondaries to reduce system load.\\nData can also be horizontally partitioned across multiple node\", \"s, such as with sharding. But, sharding \\ndramatically increases operational overhead by spitting dat\", \"a across many pieces that cannot easily \\ncommunicate. It can be costly and time consuming to manage.\", \" Relational features that include table \\njoins, transactions, and referential integrity require stee\", \"p performance penalties in sharded \\ndeployments.\\nReplication consistency and recovery point objectiv\", \"es can be tuned by configuring whether replication \\noccurs synchronously or asynchronously. If data \", \"replicas were to lose network connectivity in a \\u201chighly \\nconsistent\\u201d or synchronous relational datab\", \"ase cluster, you wouldn\\u2019t be able to write to the database. \\nThe system would reject the write opera\", \"tion as it can\\u2019t replicate that change to the other data replica. \\nEvery data replica has to update \", \"before the transaction can complete.\\nNoSQL databases typically support high availability and partiti\", \"on tolerance. They scale out \\nhorizontally, often across commodity servers. This approach provides t\", \"remendous availability, both \\nwithin and across geographical regions at a reduced cost. You partitio\", \"n and replicate data across \\nthese machines, or nodes, providing redundancy and fault tolerance. Con\", \"sistency is typically tuned \\nthrough consensus protocols or quorum mechanisms. They provide more con\", \"trol when navigating \\ntradeoffs between tuning synchronous versus asynchronous replication in relati\", \"onal systems.\\nIf data replicas were to lose connectivity in a \\u201chighly available\\u201d NoSQL database clus\", \"ter, you could still \\ncomplete a write operation to the database. The database cluster would allow t\", \"he write operation and \\nupdate each data replica as it becomes available. NoSQL databases that suppo\", \"rt multiple writable \\nreplicas can further strengthen high availability by avoiding the need for fai\", \"lover when optimizing \\nrecovery time objective.\\nModern NoSQL databases typically implement partition\", \"ing capabilities as a feature of their system \\ndesign. Partition management is often built-in to the\", \" database, and routing is achieved through \\nplacement hints - often called partition keys. A flexibl\", \"e data models enables the NoSQL databases to \\nlower the burden of schema management and improve avai\", \"lability when deploying application \\nupdates that require data model changes.\\nHigh availability and \", \"massive scalability are often more critical to the business than relational table \\njoins and referen\", \"tial integrity. Developers can implement techniques and patterns such as Sagas, \\nCQRS, and asynchron\", \"ous messaging to embrace eventual consistency.\\nNowadays, care must be taken when considering the CAP\", \" theorem constraints. A new type of \\ndatabase, called NewSQL, has emerged which extends the relation\", \"al database engine to support both \\nhorizontal scalability and the scalable performance of NoSQL sys\", \"tems.99 CHAPTER 5 | Cloud-native data patterns\\nConsiderations for relational vs. NoSQL systems\\nBased\", \" upon specific data requirements, a cloud-native-based microservice can implement a relational, \\nNoS\", \"QL datastore or both.\\nConsider a NoSQL datastore when: Consider a relational database when:\\nYou have\", \" high volume workloads that require \\npredictable latency at large scale (for example, \\nlatency measu\", \"red in milliseconds while \\nperforming millions of transactions per second)\\nYour workload volume gene\", \"rally fits within \\nthousands of transactions per second\\nYour data is dynamic and frequently changes \", \"Your data is highly structured and requires \\nreferential integrity\\nRelationships can be de-normalize\", \"d data \\nmodels\\nRelationships are expressed through table joins \\non normalized data models\\nData retri\", \"eval is simple and expressed without \\ntable joins\\nYou work with complex queries and reports\\nData is \", \"typically replicated across geographies \\nand requires finer control over consistency, \\navailability,\", \" and performance\\nData is typically centralized, or can be replicated \\nregions asynchronously\\nYour ap\", \"plication will be deployed to commodity \\nhardware, such as with public clouds\\nYour application will \", \"be deployed to large, high\\u0002end hardware\\nIn the next sections, we\\u2019ll explore the options available in\", \" the Azure cloud for storing and managing \\nyour cloud-native data.\\nDatabase as a Service\\nTo start, y\", \"ou could provision an Azure virtual machine and install your database of choice for each \\nservice. W\", \"hile you\\u2019d have full control over the environment, you\\u2019d forgo many built-in features of the \\ncloud \", \"platform. You\\u2019d also be responsible for managing the virtual machine and database for each \\nservice.\", \" This approach could quickly become time-consuming and expensive.\\nInstead, cloud-native applications\", \" favor data services exposed as a Database as a Service (DBaaS). \\nFully managed by a cloud vendor, t\", \"hese services provide built-in security, scalability, and monitoring. \\nInstead of owning the service\", \", you simply consume it as a backing service. The provider operates the \\nresource at scale and bears\", \" the responsibility for performance and maintenance.\\nThey can be configured across cloud availabilit\", \"y zones and regions to achieve high availability. They \\nall support just-in-time capacity and a pay-\", \"as-you-go model. Azure features different kinds of \\nmanaged data service options, each with specific\", \" benefits.\\nWe\\u2019ll first look at relational DBaaS services available in Azure. You\\u2019ll see that Microso\", \"ft\\u2019s flagship SQL \\nServer database is available along with several open-source options. Then, we\\u2019ll \", \"talk about the NoSQL \\ndata services in Azure.100 CHAPTER 5 | Cloud-native data patterns\\nAzure relati\", \"onal databases\\nFor cloud-native microservices that require relational data, Azure offers four manage\", \"d relational \\ndatabases as a service (DBaaS) offerings, shown in Figure 5-11.\\nFigure 5-11. Managed r\", \"elational databases available in Azure\\nIn the previous figure, note how each sits upon a common DBaa\", \"S infrastructure which features key \\ncapabilities at no additional cost.\\nThese features are especial\", \"ly important to organizations who provision large numbers of databases, \\nbut have limited resources \", \"to administer them. You can provision an Azure database in minutes by \\nselecting the amount of proce\", \"ssing cores, memory, and underlying storage. You can scale the \\ndatabase on-the-fly and dynamically \", \"adjust resources with little to no downtime.\\nAzure SQL Database\\nDevelopment teams with expertise in \", \"Microsoft SQL Server should consider Azure SQL Database. It\\u2019s a \\nfully managed relational database-a\", \"s-a-service (DBaaS) based on the Microsoft SQL Server Database \\nEngine. The service shares many feat\", \"ures found in the on-premises version of SQL Server and runs the \\nlatest stable version of the SQL S\", \"erver Database Engine.\\nFor use with a cloud-native microservice, Azure SQL Database is available wit\", \"h three deployment \\noptions:\\n\\u2022 A Single Database represents a fully managed SQL Database running on \", \"an Azure SQL \\nDatabase server in the Azure cloud. The database is considered contained as it has no \", \"\\nconfiguration dependencies on the underlying database server.\\n\\u2022 A Managed Instance is a fully manag\", \"ed instance of the Microsoft SQL Server Database Engine \\nthat provides near-100% compatibility with \", \"an on-premises SQL Server. This option supports \\nlarger databases, up to 35 TB and is placed in an A\", \"zure Virtual Network for better isolation.101 CHAPTER 5 | Cloud-native data patterns\\n\\u2022 Azure SQL Dat\", \"abase serverless is a compute tier for a single database that automatically \\nscales based on workloa\", \"d demand. It bills only for the amount of compute used per second. \\nThe service is well suited for w\", \"orkloads with intermittent, unpredictable usage patterns, \\ninterspersed with periods of inactivity. \", \"The serverless compute tier also automatically pauses \\ndatabases during inactive periods so that onl\", \"y storage charges are billed. It automatically \\nresumes when activity returns.\\nBeyond the traditiona\", \"l Microsoft SQL Server stack, Azure also features managed versions of three \\npopular open-source dat\", \"abases.\\nOpen-source databases in Azure\\nOpen-source relational databases have become a popular choice\", \" for cloud-native applications. Many \\nenterprises favor them over commercial database products, espe\", \"cially for cost savings. Many \\ndevelopment teams enjoy their flexibility, community-backed developme\", \"nt, and ecosystem of tools \\nand extensions. Open-source databases can be deployed across multiple cl\", \"oud providers, helping \\nminimize the concern of \\u201cvendor lock-in.\\u201d\\nDevelopers can easily self-host an\", \"y open-source database on an Azure VM. While providing full \\ncontrol, this approach puts you on the \", \"hook for the management, monitoring, and maintenance of \\nthe database and VM.\\nHowever, Microsoft con\", \"tinues its commitment to keeping Azure an \\u201copen platform\\u201d by offering \\nseveral popular open-source d\", \"atabases as fully managed DBaaS services.\\nAzure Database for MySQL\\nMySQL is an open-source relationa\", \"l database and a pillar for applications built on the LAMP software \\nstack. Widely chosen for read h\", \"eavy workloads, it\\u2019s used by many large organizations, including \\nFacebook, Twitter, and YouTube. Th\", \"e community edition is available for free, while the enterprise \\nedition requires a license purchase\", \". Originally created in 1995, the product was purchased by Sun\\nMicrosystems in 2008. Oracle acquired\", \" Sun and MySQL in 2010.\\nAzure Database for MySQL is a managed relational database service based on t\", \"he open-source \\nMySQL Server engine. It uses the MySQL Community edition. The Azure MySQL server is \", \"the \\nadministrative point for the service. It\\u2019s the same MySQL server engine used for on-premises \\nd\", \"eployments. The engine can create a single database per server or multiple databases per server that\", \" \\nshare resources. You can continue to manage data using the same open-source tools without having \\n\", \"to learn new skills or manage virtual machines.\\nAzure Database for MariaDB\\nMariaDB Server is another\", \" popular open-source database server. It was created as a fork of MySQL \\nwhen Oracle purchased Sun M\", \"icrosystems, who owned MySQL. The intent was to ensure that MariaDB \\nremained open-source. As MariaD\", \"B is a fork of MySQL, the data and table definitions are compatible, \\nand the client protocols, stru\", \"ctures, and APIs, are close-knit.102 CHAPTER 5 | Cloud-native data patterns\\nMariaDB has a strong com\", \"munity and is used by many large enterprises. While Oracle continues to \\nmaintain, enhance, and supp\", \"ort MySQL, the MariaDB foundation manages MariaDB, allowing public \\ncontributions to the product and\", \" documentation.\\nAzure Database for MariaDB is a fully managed relational database as a service in th\", \"e Azure cloud. The \\nservice is based on the MariaDB community edition server engine. It can handle m\", \"ission-critical \\nworkloads with predictable performance and dynamic scalability.\\nAzure Database for \", \"PostgreSQL\\nPostgreSQL is an open-source relational database with over 30 years of active development\", \". \\nPostgreSQL has a strong reputation for reliability and data integrity. It\\u2019s feature rich, SQL com\", \"pliant, \\nand considered more performant than MySQL - especially for workloads with complex queries a\", \"nd \\nheavy writes. Many large enterprises including Apple, Red Hat, and Fujitsu have built products u\", \"sing \\nPostgreSQL.\\nAzure Database for PostgreSQL is a fully managed relational database service, base\", \"d on the open\\u0002source Postgres database engine. The service supports many development platforms, incl\", \"uding C++, \\nJava, Python, Node, C#, and PHP. You can migrate PostgreSQL databases to it using the co\", \"mmand\\u0002line interface tool or Azure Data Migration Service.\\nAzure Database for PostgreSQL is availabl\", \"e with two deployment options:\\n\\u2022 The Single Server deployment option is a central administrative poi\", \"nt for multiple databases \\nto which you can deploy many databases. The pricing is structured per-ser\", \"ver based upon \\ncores and storage.\\n\\u2022 The Hyperscale (Citus) option is powered by Citus Data technolo\", \"gy. It enables high \\nperformance by horizontally scaling a single database across hundreds of nodes \", \"to deliver fast \\nperformance and scale. This option allows the engine to fit more data in memory, pa\", \"rallelize \\nqueries across hundreds of nodes, and index data faster.\\nNoSQL data in Azure\\nCosmos DB is\", \" a fully managed, globally distributed NoSQL database service in the Azure cloud. It has \\nbeen adopt\", \"ed by many large companies across the world, including Coca-Cola, Skype, ExxonMobil, \\nand Liberty Mu\", \"tual.\\nIf your services require fast response from anywhere in the world, high availability, or elast\", \"ic \\nscalability, Cosmos DB is a great choice. Figure 5-12 shows Cosmos DB.103 CHAPTER 5 | Cloud-nati\", \"ve data patterns\\nFigure 5-12: Overview of Azure Cosmos DB\\nThe previous figure presents many of the b\", \"uilt-in cloud-native capabilities available in Cosmos DB. In \\nthis section, we\\u2019ll take a closer look\", \" at them.\\nGlobal support\\nCloud-native applications often have a global audience and require global s\", \"cale.\\nYou can distribute Cosmos databases across regions or around the world, placing data close to \", \"your \\nusers, improving response time, and reducing latency. You can add or remove a database from a \", \"\\nregion without pausing or redeploying your services. In the background, Cosmos DB transparently \\nre\", \"plicates the data to each of the configured regions.\\nCosmos DB supports active/active clustering at \", \"the global level, enabling you to configure any of your \\ndatabase regions to support both writes and\", \" reads.\\nThe Multi-region write protocol is an important feature in Cosmos DB that enables the follow\", \"ing \\nfunctionality:\\n\\u2022 Unlimited elastic write and read scalability.\\n\\u2022 99.999% read and write availab\", \"ility all around the world.\\n\\u2022 Guaranteed reads and writes served in less than 10 milliseconds at the\", \" 99th percentile.\\nWith the Cosmos DB Multi-Homing APIs, your microservice is automatically aware of \", \"the nearest \\nAzure region and sends requests to it. The nearest region is identified by Cosmos DB wi\", \"thout any \\nconfiguration changes. Should a region become unavailable, the Multi-Homing feature will \", \"\\nautomatically route requests to the next nearest available region.\\nMulti-model support\\nWhen replatf\", \"orming monolithic applications to a cloud-native architecture, development teams \\nsometimes have to \", \"migrate open-source, NoSQL data stores. Cosmos DB can help you preserve your 104 CHAPTER 5 | Cloud-n\", \"ative data patterns\\ninvestment in these NoSQL datastores with its multi-model data platform. The fol\", \"lowing table shows \\nthe supported NoSQL compatibility APIs.\\nProvider Description\\nNoSQL API API for N\", \"oSQL stores data in document format\\nMongo DB API Supports Mongo DB APIs and JSON documents\\nGremlin A\", \"PI Supports Gremlin API with graph-based nodes \\nand edge data representations\\nCassandra API Supports\", \" Casandra API for wide-column data \\nrepresentations\\nTable API Supports Azure Table Storage with prem\", \"ium \\nenhancements\\nPostgreSQL API Managed service for running PostgreSQL at any \\nscale\\nDevelopment te\", \"ams can migrate existing Mongo, Gremlin, or Cassandra databases into Cosmos DB \\nwith minimal changes\", \" to data or code. For new apps, development teams can choose among open\\u0002source options or the built-\", \"in SQL API model.\\nInternally, Cosmos stores the data in a simple struct format made up of primitive \", \"data types. For each \\nrequest, the database engine translates the primitive data into the model repr\", \"esentation you\\u2019ve \\nselected.\\nIn the previous table, note the Table API option. This API is an evolut\", \"ion of Azure Table Storage. Both \\nshare the same underlying table model, but the Cosmos DB Table API\", \" adds premium enhancements \\nnot available in the Azure Storage API. The following table contrasts th\", \"e features.\\nFeature Azure Table Storage Azure Cosmos DB\\nLatency Fast Single-digit millisecond latenc\", \"y for reads and \\nwrites anywhere in the world\\nThroughp\\nut\\nLimit of 20,000 operations per table Unlim\", \"ited operations per table\\nGlobal \\nDistributio\\nn\\nSingle region with optional single \\nsecondary read r\", \"egion\\nTurnkey distributions to all regions with \\nautomatic failover\\nIndexing Available for partition\", \" and row key \\nproperties only\\nAutomatic indexing of all properties\\nPricing Optimized for cold worklo\", \"ads (low \\nthroughput : storage ratio)\\nOptimized for hot workloads (high \\nthroughput : storage ratio)\", \"\\nMicroservices that consume Azure Table storage can easily migrate to the Cosmos DB Table API. No \\nc\", \"ode changes are required.105 CHAPTER 5 | Cloud-native data patterns\\nTunable consistency\\nEarlier in t\", \"he Relational vs. NoSQL section, we discussed the subject of data consistency. Data \\nconsistency ref\", \"ers to the integrity of your data. Cloud-native services with distributed data rely on \\nreplication \", \"and must make a fundamental tradeoff between read consistency, availability, and latency.\\nMost distr\", \"ibuted databases allow developers to choose between two consistency \\nmodels: strong consistency and \", \"eventual consistency. Strong consistency is the gold standard of data \\nprogrammability. It guarantee\", \"s that a query will always return the most current data - even if the \\nsystem must incur latency wai\", \"ting for an update to replicate across all database copies. While a \\ndatabase configured for eventua\", \"l consistency will return data immediately, even if that data isn\\u2019t the \\nmost current copy. The latt\", \"er option enables higher availability, greater scale, and increased \\nperformance.\\nAzure Cosmos DB of\", \"fers five well-defined consistency models shown in Figure 5-13.\\nFigure 5-13: Cosmos DB Consistency L\", \"evels\\nThese options enable you to make precise choices and granular tradeoffs for consistency, avail\", \"ability, \\nand the performance for your data. The levels are presented in the following table.\\nConsis\", \"tency Level Description\\nEventual No ordering guarantee for reads. Replicas will \\neventually converge\", \".\\nConstant Prefix Reads are still eventual, but data is returned in \\nthe ordering in which it is wri\", \"tten.\\nSession Guarantees you can read any data written \\nduring the current session. It is the defaul\", \"t \\nconsistency level.\\nBounded Staleness Reads trail writes by interval that you specify.\\nStrong Read\", \"s are guaranteed to return most recent \\ncommitted version of an item. A client never \\nsees an uncomm\", \"itted or partial read.\\nIn the article Getting Behind the 9-Ball: Cosmos DB Consistency Levels Explai\", \"ned, Microsoft Program \\nManager Jeremy Likness provides an excellent explanation of the five models.\", \"\\nPartitioning\\nAzure Cosmos DB embraces automatic partitioning to scale a database to meet the perfor\", \"mance \\nneeds of your cloud-native services.\\nYou manage data in Cosmos DB data by creating databases,\", \" containers, and items.106 CHAPTER 5 | Cloud-native data patterns\\nContainers live in a Cosmos DB dat\", \"abase and represent a schema-agnostic grouping of items. Items \\nare the data that you add to the con\", \"tainer. They\\u2019re represented as documents, rows, nodes, or edges. \\nAll items added to a container are\", \" automatically indexed.\\nTo partition the container, items are divided into distinct subsets called l\", \"ogical partitions. Logical \\npartitions are populated based on the value of a partition key that is a\", \"ssociated with each item in a \\ncontainer. Figure 5-14 shows two containers each with a logical parti\", \"tion based on a partition key \\nvalue.\\nFigure 5-14: Cosmos DB partitioning mechanics\\nNote in the prev\", \"ious figure how each item includes a partition key of either \\u2018city\\u2019 or \\u2018airport\\u2019. The key \\ndetermine\", \"s the item\\u2019s logical partition. Items with a city code are assigned to the container on the left, \\na\", \"nd items with an airport code, to the container on the right. Combining the partition key value with\", \" \\nthe ID value creates an item\\u2019s index, which uniquely identifies the item.\\nInternally, Cosmos DB au\", \"tomatically manages the placement of logical partitions on physical \\npartitions to satisfy the scala\", \"bility and performance needs of the container. As application throughput \\nand storage requirements i\", \"ncrease, Azure Cosmos DB redistributes logical partitions across a greater \\nnumber of servers. Redis\", \"tribution operations are managed by Cosmos DB and invoked without \\ninterruption or downtime.\\nNewSQL \", \"databases\\nNewSQL is an emerging database technology that combines the distributed scalability of NoS\", \"QL with \\nthe ACID guarantees of a relational database. NewSQL databases are important for business s\", \"ystems \\nthat must process high-volumes of data, across distributed environments, with full transacti\", \"onal \\nsupport and ACID compliance. While a NoSQL database can provide massive scalability, it does n\", \"ot \\nguarantee data consistency. Intermittent problems from inconsistent data can place a burden on t\", \"he \\ndevelopment team. Developers must construct safeguards into their microservice code to manage \\np\", \"roblems caused by inconsistent data.\\nThe Cloud Native Computing Foundation (CNCF) features several N\", \"ewSQL database projects.107 CHAPTER 5 | Cloud-native data patterns\\nProject Characteristics\\nCockroach\", \" DB An ACID-compliant, relational database that \\nscales globally. Add a new node to a cluster and \\nC\", \"ockroachDB takes care of balancing the data \\nacross instances and geographies. It creates, \\nmanages,\", \" and distributes replicas to ensure \\nreliability. It\\u2019s open source and freely available.\\nTiDB An ope\", \"n-source database that supports Hybrid \\nTransactional and Analytical Processing (HTAP) \\nworkloads. I\", \"t is MySQL-compatible and features \\nhorizontal scalability, strong consistency, and \\nhigh availabili\", \"ty. TiDB acts like a MySQL server. \\nYou can continue to use existing MySQL client \\nlibraries, withou\", \"t requiring extensive code \\nchanges to your application.\\nYugabyteDB An open source, high-performance\", \", distributed \\nSQL database. It supports low query latency, \\nresilience against failures, and global\", \" data \\ndistribution. YugabyteDB is PostgreSQL\\u0002compatible and handles scale-out RDBMS and \\ninternet-s\", \"cale OLTP workloads. The product also \\nsupports NoSQL and is compatible with \\nCassandra.\\nVitess Vite\", \"ss is a database solution for deploying, \\nscaling, and managing large clusters of MySQL \\ninstances. \", \"It can run in a public or private cloud \\narchitecture. Vitess combines and extends many \\nimportant M\", \"ySQL features and features both \\nvertical and horizontal sharding support. \\nOriginated by YouTube, V\", \"itess has been serving \\nall YouTube database traffic since 2011.\\nThe open-source projects in the pre\", \"vious figure are available from the Cloud Native Computing \\nFoundation. Three of the offerings are f\", \"ull database products, which include .NET support. The other, \\nVitess, is a database clustering syst\", \"em that horizontally scales large clusters of MySQL instances.\\nA key design goal for NewSQL database\", \"s is to work natively in Kubernetes, taking advantage of the \\nplatform\\u2019s resiliency and scalability.\", \"\\nNewSQL databases are designed to thrive in ephemeral cloud environments where underlying virtual \\nm\", \"achines can be restarted or rescheduled at a moment\\u2019s notice. The databases are designed to \\nsurvive\", \" node failures without data loss nor downtime. CockroachDB, for example, is able to survive a \\nmachi\", \"ne loss by maintaining three consistent replicas of any data across the nodes in a cluster.\\nKubernet\", \"es uses a Services construct to allow a client to address a group of identical NewSQL \\ndatabases pro\", \"cesses from a single DNS entry. By decoupling the database instances from the address 108 CHAPTER 5 \", \"| Cloud-native data patterns\\nof the service with which it\\u2019s associated, we can scale without disrupt\", \"ing existing application instances. \\nSending a request to any service at a given time will always yi\", \"eld the same result.\\nIn this scenario, all database instances are equal. There are no primary or sec\", \"ondary relationships. \\nTechniques like consensus replication found in CockroachDB allow any database\", \" node to handle any \\nrequest. If the node that receives a load-balanced request has the data it need\", \"s locally, it responds \\nimmediately. If not, the node becomes a gateway and forwards the request to \", \"the appropriate nodes \\nto get the correct answer. From the client\\u2019s perspective, every database node\", \" is the same: They appear \\nas a single logical database with the consistency guarantees of a single-\", \"machine system, despite \\nhaving dozens or even hundreds of nodes that are working behind the scenes.\", \"\\nFor a detailed look at the mechanics behind NewSQL databases, see the DASH: Four Properties of \\nKub\", \"ernetes-Native Databases article.\\nData migration to the cloud\\nOne of the more time-consuming tasks i\", \"s migrating data from one data platform to another. The \\nAzure Data Migration Service can help exped\", \"ite such efforts. It can migrate data from several external \\ndatabase sources into Azure Data platfo\", \"rms with minimal downtime. Target platforms include the \\nfollowing services:\\n\\u2022 Azure SQL Database\\n\\u2022 \", \"Azure Database for MySQL\\n\\u2022 Azure Database for MariaDB\\n\\u2022 Azure Database for PostgreSQL\\n\\u2022 Azure Cosmos\", \" DB\\nThe service provides recommendations to guide you through the changes required to execute a \\nmig\", \"ration, both small or large.\\nCaching in a cloud-native app\\nThe benefits of caching are well understo\", \"od. The technique works by temporarily copying frequently \\naccessed data from a backend data store t\", \"o fast storage that\\u2019s located closer to the application. \\nCaching is often implemented where\\u2026\\n\\u2022 Data\", \" remains relatively static.\\n\\u2022 Data access is slow, especially compared to the speed of the cache.\\n\\u2022 \", \"Data is subject to high levels of contention.\\nWhy?\\nAs discussed in the Microsoft caching guidance, c\", \"aching can increase performance, scalability, and \\navailability for individual microservices and the\", \" system as a whole. It reduces the latency and \\ncontention of handling large volumes of concurrent r\", \"equests to a data store. As data volume and the \\nnumber of users increase, the greater the benefits \", \"of caching become.109 CHAPTER 5 | Cloud-native data patterns\\nCaching is most effective when a client\", \" repeatedly reads data that is immutable or that changes \\ninfrequently. Examples include reference i\", \"nformation such as product and pricing information, or \\nshared static resources that are costly to c\", \"onstruct.\\nWhile microservices should be stateless, a distributed cache can support concurrent access\", \" to session \\nstate data when absolutely required.\\nAlso consider caching to avoid repetitive computat\", \"ions. If an operation transforms data or performs a \\ncomplicated calculation, cache the result for s\", \"ubsequent requests.\\nCaching architecture\\nCloud native applications typically implement a distributed\", \" caching architecture. The cache is hosted \\nas a cloud-based backing service, separate from the micr\", \"oservices. Figure 5-15 shows the architecture.\\nFigure 5-15: Caching in a cloud native app\\nIn the pre\", \"vious figure, note how the cache is independent of and shared by the microservices. In this \\nscenari\", \"o, the cache is invoked by the API Gateway. As discussed in chapter 4, the gateway serves as a \\nfron\", \"t end for all incoming requests. The distributed cache increases system responsiveness by \\nreturning\", \" cached data whenever possible. Additionally, separating the cache from the services allows \\nthe cac\", \"he to scale up or out independently to meet increased traffic demands.\\nThe previous figure presents \", \"a common caching pattern known as the cache-aside pattern. For an \\nincoming request, you first query\", \" the cache (step #1) for a response. If found, the data is returned \\nimmediately. If the data doesn\\u2019\", \"t exist in the cache (known as a cache miss), it\\u2019s retrieved from a local \\ndatabase in a downstream \", \"service (step #2). It\\u2019s then written to the cache for future requests (step #3), \\nand returned to th\", \"e caller. Care must be taken to periodically evict cached data so that the system \\nremains timely an\", \"d consistent.\\nAs a shared cache grows, it might prove beneficial to partition its data across multip\", \"le nodes. Doing \\nso can help minimize contention and improve scalability. Many Caching services supp\", \"ort the ability to 110 CHAPTER 5 | Cloud-native data patterns\\ndynamically add and remove nodes and r\", \"ebalance data across partitions. This approach typically \\ninvolves clustering. Clustering exposes a \", \"collection of federated nodes as a seamless, single cache. \\nInternally, however, the data is dispers\", \"ed across the nodes following a predefined distribution strategy \\nthat balances the load evenly.\\nAzu\", \"re Cache for Redis\\nAzure Cache for Redis is a secure data caching and messaging broker service, full\", \"y managed by \\nMicrosoft. Consumed as a Platform as a Service (PaaS) offering, it provides high throu\", \"ghput and low\\u0002latency access to data. The service is accessible to any application within or outside\", \" of Azure.\\nThe Azure Cache for Redis service manages access to open-source Redis servers hosted acro\", \"ss Azure \\ndata centers. The service acts as a facade providing management, access control, and secur\", \"ity. The \\nservice natively supports a rich set of data structures, including strings, hashes, lists,\", \" and sets. If your \\napplication already uses Redis, it will work as-is with Azure Cache for Redis.\\nA\", \"zure Cache for Redis is more than a simple cache server. It can support a number of scenarios to \\nen\", \"hance a microservices architecture:\\n\\u2022 An in-memory data store\\n\\u2022 A distributed non-relational databas\", \"e\\n\\u2022 A message broker\\n\\u2022 A configuration or discovery server\\nFor advanced scenarios, a copy of the cac\", \"hed data can be persisted to disk. If a catastrophic event \\ndisables both the primary and replica ca\", \"ches, the cache is reconstructed from the most recent \\nsnapshot.\\nAzure Redis Cache is available acro\", \"ss a number of predefined configurations and pricing tiers. The \\nPremium tier features many enterpri\", \"se-level features such as clustering, data persistence, geo\\u0002replication, and virtual-network isolati\", \"on.\\nElasticsearch in a cloud-native app\\nElasticsearch is a distributed search and analytics system t\", \"hat enables complex search capabilities \\nacross diverse types of data. It\\u2019s open source and widely p\", \"opular. Consider how the following \\ncompanies integrate Elasticsearch into their application:\\n\\u2022 Wiki\", \"pedia for full-text and incremental (search as you type) searching.\\n\\u2022 GitHub to index and expose ove\", \"r 8 million code repositories.\\n\\u2022 Docker for making its container library discoverable.\\nElasticsearch\", \" is built on top of the Apache Lucene full-text search engine. Lucene provides high\\u0002performance docu\", \"ment indexing and querying. It indexes data with an inverted indexing scheme \\u2013\\ninstead of mapping pa\", \"ges to keywords, it maps keywords to pages just like a glossary at the end of a \\nbook. Lucene has po\", \"werful query syntax capabilities and can query data by:111 CHAPTER 5 | Cloud-native data patterns\\n\\u2022 \", \"Term (a full word)\\n\\u2022 Prefix (starts-with word)\\n\\u2022 Wildcard (using \\u201c*\\u201d or \\u201c?\\u201d filters)\\n\\u2022 Phrase (a seq\", \"uence of text in a document)\\n\\u2022 Boolean value (complex searches combining queries)\\nWhile Lucene provi\", \"des low-level plumbing for searching, Elasticsearch provides the server that sits on \\ntop of Lucene.\", \" Elasticsearch adds higher-level functionality to simplify working Lucene, including a \\nRESTful API \", \"to access Lucene\\u2019s indexing and searching functionality. It also provides a distributed \\ninfrastruct\", \"ure capable of massive scalability, fault tolerance, and high availability.\\nFor larger cloud-native \", \"applications with complex search requirements, Elasticsearch is available as \\nmanaged service in Azu\", \"re. The Microsoft Azure Marketplace features preconfigured templates which \\ndevelopers can use to de\", \"ploy an Elasticsearch cluster on Azure.\\nFrom the Microsoft Azure Marketplace, developers can use pre\", \"configured templates built to quickly \\ndeploy an Elasticsearch cluster on Azure. Using the Azure-man\", \"aged offering, you can deploy up to 50 \\ndata nodes, 20 coordinating nodes, and three dedicated maste\", \"r nodes.\\nSummary\\nThis chapter presented a detailed look at data in cloud-native systems. We started \", \"by contrasting data \\nstorage in monolithic applications with data storage patterns in cloud-native s\", \"ystems. We looked at \\ndata patterns implemented in cloud-native systems, including cross-service que\", \"ries, distributed \\ntransactions, and patterns to deal with high-volume systems. We contrasted SQL wi\", \"th NoSQL data. \\nWe looked at data storage options available in Azure that include both Microsoft-cen\", \"tric and open\\u0002source options. Finally, we discussed caching and Elasticsearch in a cloud-native appl\", \"ication.\\nReferences\\n\\u2022 Command and Query Responsibility Segregation (CQRS) pattern\\n\\u2022 Event Sourcing p\", \"attern\\n\\u2022 Why isn\\u2019t RDBMS Partition Tolerant in CAP Theorem and why is it Available?\\n\\u2022 Materialized V\", \"iew\\n\\u2022 All you really need to know about open source databases\\n\\u2022 Compensating Transaction pattern\\n\\u2022 S\", \"aga Pattern\\n\\u2022 Saga Patterns | How to implement business transactions using microservices\\n\\u2022 Compensat\", \"ing Transaction pattern\\n\\u2022 Getting Behind the 9-Ball: Cosmos DB Consistency Levels Explained\\n\\u2022 On RDB\", \"MS, NoSQL and NewSQL databases. Interview with John Ryan112 CHAPTER 5 | Cloud-native data patterns\\n\\u2022\", \" SQL vs NoSQL vs NewSQL: The Full Comparison\\n\\u2022 DASH: Four Properties of Kubernetes-Native Databases\\n\", \"\\u2022 CockroachDB\\n\\u2022 TiDB\\n\\u2022 YugabyteDB\\n\\u2022 Vitess\\n\\u2022 Elasticsearch: The Definitive Guide\\n\\u2022 Introduction to A\", \"pache Lucene113 CHAPTER 6 | Cloud-native resiliency\\nCHAPTER 6\\nCloud-native resiliency\\nResiliency is \", \"the ability of your system to react to failure and still remain functional. It\\u2019s not about \\navoiding\", \" failure, but accepting failure and constructing your cloud-native services to respond to it. \\nYou w\", \"ant to return to a fully functioning state quickly as possible.\\nUnlike traditional monolithic applic\", \"ations, where everything runs together in a single process, cloud\\u0002native systems embrace a distribut\", \"ed architecture as shown in Figure 6-1:\\nFigure 6-1. Distributed cloud-native environment\\nIn the prev\", \"ious figure, each microservice and cloud-based backing service execute in a separate \\nprocess, acros\", \"s server infrastructure, communicating via network-based calls.\\nOperating in this environment, a ser\", \"vice must be sensitive to many different challenges:\\n\\u2022 Unexpected network latency - the time for a s\", \"ervice request to travel to the receiver and back.\\n\\u2022 Transient faults - short-lived network connecti\", \"vity errors.\\n\\u2022 Blockage by a long-running synchronous operation.\\n\\u2022 A host process that has crashed a\", \"nd is being restarted or moved.\\n\\u2022 An overloaded microservice that can\\u2019t respond for a short time.\\n\\u2022 \", \"An in-flight orchestrator operation such as a rolling upgrade or moving a service from one \\nnode to \", \"another.114 CHAPTER 6 | Cloud-native resiliency\\n\\u2022 Hardware failures.\\nCloud platforms can detect and \", \"mitigate many of these infrastructure issues. It may restart, scale out, \\nand even redistribute your\", \" service to a different node. However, to take full advantage of this built-in \\nprotection, you must\", \" design your services to react to it and thrive in this dynamic environment.\\nIn the following sectio\", \"ns, we\\u2019ll explore defensive techniques that your service and managed cloud \\nresources can leverage t\", \"o minimize downtime and disruption.\\nApplication resiliency patterns\\nThe first line of defense is app\", \"lication resiliency.\\nWhile you could invest considerable time writing your own resiliency framework,\", \" such products \\nalready exist. Polly is a comprehensive .NET resilience and transient-fault-handling\", \" library that allows \\ndevelopers to express resiliency policies in a fluent and thread-safe manner. \", \"Polly targets applications \\nbuilt with either .NET Framework or .NET 7. The following table describe\", \"s the resiliency features, called \\npolicies, available in the Polly Library. They can be applied ind\", \"ividually or grouped together.\\nPolicy Experience\\nRetry Configures retry operations on designated \\nop\", \"erations.\\nCircuit Breaker Blocks requested operations for a predefined \\nperiod when faults exceed a \", \"configured \\nthreshold\\nTimeout Places limit on the duration for which a caller \\ncan wait for a respon\", \"se.\\nBulkhead Constrains actions to fixed-size resource pool to \\nprevent failing calls from swamping \", \"a resource.\\nCache Stores responses automatically.\\nFallback Defines structured behavior upon a failur\", \"e.\\nNote how in the previous figure the resiliency policies apply to request messages, whether coming\", \" \\nfrom an external client or back-end service. The goal is to compensate the request for a service t\", \"hat \\nmight be momentarily unavailable. These short-lived interruptions typically manifest themselves\", \" with \\nthe HTTP status codes shown in the following table.\\nHTTP Status Code Cause\\n404 Not Found\\n408 \", \"Request timeout\\n429 Too many requests (you\\u2019ve most likely been throttled)\\n502 Bad gateway\\n503 Servic\", \"e unavailable115 CHAPTER 6 | Cloud-native resiliency\\nHTTP Status Code Cause\\n504 Gateway timeout\\nQues\", \"tion: Would you retry an HTTP Status Code of 403 - Forbidden? No. Here, the system is \\nfunctioning p\", \"roperly, but informing the caller that they aren\\u2019t authorized to perform the requested \\noperation. C\", \"are must be taken to retry only those operations caused by failures.\\nAs recommended in Chapter 1, Mi\", \"crosoft developers constructing cloud-native applications should \\ntarget the .NET platform. Version \", \"2.1 introduced the HTTPClientFactory library for creating HTTP Client \\ninstances for interacting wit\", \"h URL-based resources. Superseding the original HTTPClient class, the \\nfactory class supports many e\", \"nhanced features, one of which is tight integration with the Polly \\nresiliency library. With it, you\", \" can easily define resiliency policies in the application Startup class to \\nhandle partial failures \", \"and connectivity issues.\\nNext, let\\u2019s expand on retry and circuit breaker patterns.\\nRetry pattern\\nIn \", \"a distributed cloud-native environment, calls to services and cloud resources can fail because of \\nt\", \"ransient (short-lived) failures, which typically correct themselves after a brief period of time. \\nI\", \"mplementing a retry strategy helps a cloud-native service mitigate these scenarios.\\nThe Retry patter\", \"n enables a service to retry a failed request operation a (configurable) number of \\ntimes with an ex\", \"ponentially increasing wait time. Figure 6-2 shows a retry in action.\\nFigure 6-2. Retry pattern in a\", \"ction\\nIn the previous figure, a retry pattern has been implemented for a request operation. It\\u2019s con\", \"figured to \\nallow up to four retries before failing with a backoff interval (wait time) starting at \", \"two seconds, which \\nexponentially doubles for each subsequent attempt.\\n\\u2022 The first invocation fails \", \"and returns an HTTP status code of 500. The application waits for two \\nseconds and retries the call.\", \"116 CHAPTER 6 | Cloud-native resiliency\\n\\u2022 The second invocation also fails and returns an HTTP statu\", \"s code of 500. The application now \\ndoubles the backoff interval to four seconds and retries the cal\", \"l.\\n\\u2022 Finally, the third call succeeds.\\n\\u2022 In this scenario, the retry operation would have attempted \", \"up to four retries while doubling \\nthe backoff duration before failing the call.\\n\\u2022 Had the 4th retry\", \" attempt failed, a fallback policy would be invoked to gracefully handle the \\nproblem.\\nIt\\u2019s importan\", \"t to increase the backoff period before retrying the call to allow the service time to self\\u0002correct.\", \" It\\u2019s a best practice to implement an exponentially increasing backoff (doubling the period on \\neach\", \" retry) to allow adequate correction time.\\nCircuit breaker pattern\\nWhile the retry pattern can help \", \"salvage a request entangled in a partial failure, there are situations \\nwhere failures can be caused\", \" by unanticipated events that will require longer periods of time to \\nresolve. These faults can rang\", \"e in severity from a partial loss of connectivity to the complete failure of \\na service. In these si\", \"tuations, it\\u2019s pointless for an application to continually retry an operation that is \\nunlikely to s\", \"ucceed.\\nTo make things worse, executing continual retry operations on a non-responsive service can m\", \"ove \\nyou into a self-imposed denial of service scenario where you flood your service with continual \", \"calls \\nexhausting resources such as memory, threads and database connections, causing failure in unr\", \"elated \\nparts of the system that use the same resources.\\nIn these situations, it would be preferable\", \" for the operation to fail immediately and only attempt to \\ninvoke the service if it\\u2019s likely to suc\", \"ceed.\\nThe Circuit Breaker pattern can prevent an application from repeatedly trying to execute an op\", \"eration \\nthat\\u2019s likely to fail. After a pre-defined number of failed calls, it blocks all traffic to\", \" the service. \\nPeriodically, it will allow a trial call to determine whether the fault has resolved.\", \" Figure 6-3 shows the \\nCircuit Breaker pattern in action.117 CHAPTER 6 | Cloud-native resiliency\\nFig\", \"ure 6-3. Circuit breaker pattern in action\\nIn the previous figure, a Circuit Breaker pattern has bee\", \"n added to the original retry pattern. Note how \\nafter 100 failed requests, the circuit breakers ope\", \"ns and no longer allows calls to the service. The \\nCheckCircuit value, set at 30 seconds, specifies \", \"how often the library allows one request to proceed to \\nthe service. If that call succeeds, the circ\", \"uit closes and the service is once again available to traffic.\\nKeep in mind that the intent of the C\", \"ircuit Breaker pattern is different than that of the Retry pattern. \\nThe Retry pattern enables an ap\", \"plication to retry an operation in the expectation that it will succeed. \\nThe Circuit Breaker patter\", \"n prevents an application from doing an operation that is likely to fail. \\nTypically, an application\", \" will combine these two patterns by using the Retry pattern to invoke an \\noperation through a circui\", \"t breaker.\\nTesting for resiliency\\nTesting for resiliency cannot always be done the same way that you\", \" test application functionality (by \\nrunning unit tests, integration tests, and so on). Instead, you\", \" must test how the end-to-end workload \\nperforms under failure conditions, which only occur intermit\", \"tently. For example: inject failures by \\ncrashing processes, expired certificates, make dependent se\", \"rvices unavailable etc. Frameworks like \\nchaos-monkey can be used for such chaos testing.\\nApplicatio\", \"n resiliency is a must for handling problematic requested operations. But, it\\u2019s only half of the \\nst\", \"ory. Next, we cover resiliency features available in the Azure cloud.\\nAzure platform resiliency\\nBuil\", \"ding a reliable application in the cloud is different from traditional on-premises application \\ndeve\", \"lopment. While historically you purchased higher-end hardware to scale up, in a cloud \\nenvironment y\", \"ou scale out. Instead of trying to prevent failures, the goal is to minimize their effects \\nand keep\", \" the system stable.118 CHAPTER 6 | Cloud-native resiliency\\nThat said, reliable cloud applications di\", \"splay distinct characteristics:\\n\\u2022 They\\u2019re resilient, recover gracefully from problems, and continue \", \"to function.\\n\\u2022 They\\u2019re highly available (HA) and run as designed in a healthy state with no signific\", \"ant \\ndowntime.\\nUnderstanding how these characteristics work together - and how they affect cost - is\", \" essential to \\nbuilding a reliable cloud-native application. We\\u2019ll next look at ways that you can bu\", \"ild resiliency and \\navailability into your cloud-native applications leveraging features from the Az\", \"ure cloud.\\nDesign with resiliency\\nWe\\u2019ve said resiliency enables your application to react to failure\", \" and still remain functional. The \\nwhitepaper, Resilience in Azure whitepaper, provides guidance for\", \" achieving resilience in the Azure \\nplatform. Here are some key recommendations:\\n\\u2022 Hardware failure.\", \" Build redundancy into the application by deploying components across \\ndifferent fault domains. For \", \"example, ensure that Azure VMs are placed in different racks by \\nusing Availability Sets.\\n\\u2022 Datacent\", \"er failure. Build redundancy into the application with fault isolation zones across \\ndatacenters. Fo\", \"r example, ensure that Azure VMs are placed in different fault-isolated \\ndatacenters by using Azure \", \"Availability Zones.\\n\\u2022 Regional failure. Replicate the data and components into another region so tha\", \"t applications \\ncan be quickly recovered. For example, use Azure Site Recovery to replicate Azure VM\", \"s to \\nanother Azure region.\\n\\u2022 Heavy load. Load balance across instances to handle spikes in usage. F\", \"or example, put two or \\nmore Azure VMs behind a load balancer to distribute traffic to all VMs.\\n\\u2022 Ac\", \"cidental data deletion or corruption. Back up data so it can be restored if there\\u2019s any \\ndeletion or\", \" corruption. For example, use Azure Backup to periodically back up your Azure \\nVMs.\\nDesign with redu\", \"ndancy\\nFailures vary in scope of impact. A hardware failure, such as a failed disk, can affect a sin\", \"gle node in a \\ncluster. A failed network switch could affect an entire server rack. Less common fail\", \"ures, such as loss of \\npower, could disrupt a whole datacenter. Rarely, an entire region becomes una\", \"vailable.\\nRedundancy is one way to provide application resilience. The exact level of redundancy nee\", \"ded \\ndepends upon your business requirements and will affect both the cost and complexity of your \\ns\", \"ystem. For example, a multi-region deployment is more expensive and more complex to manage \\nthan a s\", \"ingle-region deployment. You\\u2019ll need operational procedures to manage failover and failback. \\nThe ad\", \"ditional cost and complexity might be justified for some business scenarios, but not others.\\nTo arch\", \"itect redundancy, you need to identify the critical paths in your application, and then \\ndetermine i\", \"f there\\u2019s redundancy at each point in the path? If a subsystem should fail, will the \\napplication fa\", \"il over to something else? Finally, you need a clear understanding of those features built 119 CHAPT\", \"ER 6 | Cloud-native resiliency\\ninto the Azure cloud platform that you can leverage to meet your redu\", \"ndancy requirements. Here are \\nrecommendations for architecting redundancy:\\n\\u2022 Deploy multiple instan\", \"ces of services. If your application depends on a single instance of a \\nservice, it creates a single\", \" point of failure. Provisioning multiple instances improves both \\nresiliency and scalability. When h\", \"osting in Azure Kubernetes Service, you can declaratively \\nconfigure redundant instances (replica se\", \"ts) in the Kubernetes manifest file. The replica count \\nvalue can be managed programmatically, in th\", \"e portal, or through autoscaling features.\\n\\u2022 Leveraging a load balancer. Load-balancing distributes \", \"your application\\u2019s requests to healthy \\nservice instances and automatically removes unhealthy instan\", \"ces from rotation. When \\ndeploying to Kubernetes, load balancing can be specified in the Kubernetes \", \"manifest file in \\nthe Services section.\\n\\u2022 Plan for multiregion deployment. If you deploy your applic\", \"ation to a single region, and that \\nregion becomes unavailable, your application will also become un\", \"available. This may be \\nunacceptable under the terms of your application\\u2019s service level agreements.\", \" Instead, consider \\ndeploying your application and its services across multiple regions. For example\", \", an Azure \\nKubernetes Service (AKS) cluster is deployed to a single region. To protect your system \", \"from a \\nregional failure, you might deploy your application to multiple AKS clusters across differen\", \"t \\nregions and use the Paired Regions feature to coordinate platform updates and prioritize \\nrecover\", \"y efforts.\\n\\u2022 Enable geo-replication. Geo-replication for services such as Azure SQL Database and Cos\", \"mos \\nDB will create secondary replicas of your data across multiple regions. While both services wil\", \"l \\nautomatically replicate data within the same region, geo-replication protects you against a\\nregio\", \"nal outage by enabling you to fail over to a secondary region. Another best practice for \\ngeo-replic\", \"ation centers around storing container images. To deploy a service in AKS, you \\nneed to store and pu\", \"ll the image from a repository. Azure Container Registry integrates with \\nAKS and can securely store\", \" container images. To improve performance and availability, \\nconsider geo-replicating your images to\", \" a registry in each region where you have an AKS \\ncluster. Each AKS cluster then pulls container ima\", \"ges from the local container registry in its \\nregion as shown in Figure 6-4:\\nFigure 6-4. Replicated \", \"resources across regions120 CHAPTER 6 | Cloud-native resiliency\\n\\u2022 Implement a DNS traffic load balan\", \"cer. Azure Traffic Manager provides high-availability for \\ncritical applications by load-balancing a\", \"t the DNS level. It can route traffic to different regions \\nbased on geography, cluster response tim\", \"e, and even application endpoint health. For \\nexample, Azure Traffic Manager can direct customers to\", \" the closest AKS cluster and \\napplication instance. If you have multiple AKS clusters in different r\", \"egions, use Traffic \\nManager to control how traffic flows to the applications that run in each clust\", \"er. Figure 6-5 \\nshows this scenario.\\nFigure 6-5. AKS and Azure Traffic Manager\\nDesign for scalabilit\", \"y\\nThe cloud thrives on scaling. The ability to increase/decrease system resources to address \\nincrea\", \"sing/decreasing system load is a key tenet of the Azure cloud. But, to effectively scale an \\napplica\", \"tion, you need an understanding of the scaling features of each Azure service that you include \\nin y\", \"our application. Here are recommendations for effectively implementing scaling in your system.\\n\\u2022 Des\", \"ign for scaling. An application must be designed for scaling. To start, services should be \\nstateles\", \"s so that requests can be routed to any instance. Having stateless services also means \\nthat adding \", \"or removing an instance doesn\\u2019t adversely impact current users.\\n\\u2022 Partition workloads. Decomposing d\", \"omains into independent, self-contained microservices \\nenable each service to scale independently of\", \" others. Typically, services will have different \\nscalability needs and requirements. Partitioning e\", \"nables you to scale only what needs to be \\nscaled without the unnecessary cost of scaling an entire \", \"application.\\n\\u2022 Favor scale-out. Cloud-based applications favor scaling out resources as opposed to s\", \"caling \\nup. Scaling out (also known as horizontal scaling) involves adding more service resources to\", \" \\nan existing system to meet and share a desired level of performance. Scaling up (also known 121 CH\", \"APTER 6 | Cloud-native resiliency\\nas vertical scaling) involves replacing existing resources with mo\", \"re powerful hardware (more \\ndisk, memory, and processing cores). Scaling out can be invoked automati\", \"cally with the \\nautoscaling features available in some Azure cloud resources. Scaling out across mul\", \"tiple \\nresources also adds redundancy to the overall system. Finally scaling up a single resource is\", \" \\ntypically more expensive than scaling out across many smaller resources. Figure 6-6 shows the \\ntwo\", \" approaches:\\nFigure 6-6. Scale up versus scale out\\n\\u2022 Scale proportionally. When scaling a service, t\", \"hink in terms of resource sets. If you were to \\ndramatically scale out a specific service, what impa\", \"ct would that have on back-end data \\nstores, caches and dependent services? Some resources such as C\", \"osmos DB can scale out \\nproportionally, while many others can\\u2019t. You want to ensure that you don\\u2019t s\", \"cale out a \\nresource to a point where it will exhaust other associated resources.\\n\\u2022 Avoid affinity. \", \"A best practice is to ensure a node doesn\\u2019t require local affinity, often referred \\nto as a sticky s\", \"ession. A request should be able to route to any instance. If you need to persist \\nstate, it should \", \"be saved to a distributed cache, such as Azure Redis cache.\\n\\u2022 Take advantage of platform autoscaling\", \" features. Use built-in autoscaling features whenever \\npossible, rather than custom or third-party m\", \"echanisms. Where possible, use scheduled \\nscaling rules to ensure that resources are available witho\", \"ut a startup delay, but add reactive \\nautoscaling to the rules as appropriate, to cope with unexpect\", \"ed changes in demand. For \\nmore information, see Autoscaling guidance.\\n\\u2022 Scale out aggressively. A f\", \"inal practice would be to scale out aggressively so that you can \\nquickly meet immediate spikes in t\", \"raffic without losing business. And, then scale in (that is, \\nremove unneeded instances) conservativ\", \"ely to keep the system stable. A simple way to \\nimplement this is to set the cool down period, which\", \" is the time to wait between scaling \\noperations, to five minutes for adding resources and up to 15 \", \"minutes for removing instances.\\nBuilt-in retry in services\\nWe encouraged the best practice of implem\", \"enting programmatic retry operations in an earlier section. \\nKeep in mind that many Azure services a\", \"nd their corresponding client SDKs also include retry 122 CHAPTER 6 | Cloud-native resiliency\\nmechan\", \"isms. The following list summarizes retry features in the many of the Azure services that are \\ndiscu\", \"ssed in this book:\\n\\u2022 Azure Cosmos DB. The DocumentClient class from the client API automatically ret\", \"ires failed \\nattempts. The number of retries and maximum wait time are configurable. Exceptions thro\", \"wn \\nby the client API are either requests that exceed the retry policy or non-transient errors.\\n\\u2022 Az\", \"ure Redis Cache. The Redis StackExchange client uses a connection manager class that \\nincludes retri\", \"es on failed attempts. The number of retries, specific retry policy and wait time \\nare all configura\", \"ble.\\n\\u2022 Azure Service Bus. The Service Bus client exposes a RetryPolicy class that can be configured \", \"\\nwith a back-off interval, retry count, and TerminationTimeBuffer, which specifies the maximum \\ntime\", \" an operation can take. The default policy is nine maximum retry attempts with a 30-\\nsecond backoff \", \"period between attempts.\\n\\u2022 Azure SQL Database. Retry support is provided when using the Entity Frame\", \"work Core library.\\n\\u2022 Azure Storage. The storage client library support retry operations. The strateg\", \"ies vary across \\nAzure storage tables, blobs, and queues. As well, alternate retries switch between \", \"primary and \\nsecondary storage services locations when the geo-redundancy feature is enabled.\\n\\u2022 Azur\", \"e Event Hubs. The Event Hub client library features a RetryPolicy property, which includes \\na config\", \"urable exponential backoff feature.\\nResilient communications\\nThroughout this book, we\\u2019ve embraced a \", \"microservice-based architectural approach. While such an \\narchitecture provides important benefits, \", \"it presents many challenges:\\n\\u2022 Out-of-process network communication. Each microservice communicates \", \"over a network \\nprotocol that introduces network congestion, latency, and transient faults.\\n\\u2022 Servic\", \"e discovery. How do microservices discover and communicate with each other when \\nrunning across a cl\", \"uster of machines with their own IP addresses and ports?\\n\\u2022 Resiliency. How do you manage short-lived\", \" failures and keep the system stable?\\n\\u2022 Load balancing. How does inbound traffic get distributed acr\", \"oss multiple instances of a \\nmicroservice?\\n\\u2022 Security. How are security concerns such as transport-l\", \"evel encryption and certificate \\nmanagement enforced?\\n\\u2022 Distributed Monitoring. - How do you correla\", \"te and capture traceability and monitoring for a \\nsingle request across multiple consuming microserv\", \"ices?\\nYou can address these concerns with different libraries and frameworks, but the implementation\", \" can \\nbe expensive, complex, and time-consuming. You also end up with infrastructure concerns couple\", \"d to \\nbusiness logic.123 CHAPTER 6 | Cloud-native resiliency\\nService mesh\\nA better approach is an ev\", \"olving technology entitled Service Mesh. A service mesh is a configurable \\ninfrastructure layer with\", \" built-in capabilities to handle service communication and the other \\nchallenges mentioned above. It\", \" decouples these concerns by moving them into a service proxy. The \\nproxy is deployed into a separat\", \"e process (called a sidecar) to provide isolation from business code. \\nHowever, the sidecar is linke\", \"d to the service - it\\u2019s created with it and shares its lifecycle. Figure 6-7 \\nshows this scenario.\\nF\", \"igure 6-7. Service mesh with a side car\\nIn the previous figure, note how the proxy intercepts and ma\", \"nages communication among the \\nmicroservices and the cluster.\\nA service mesh is logically split into\", \" two disparate components: A data plane and control plane. Figure \\n6-8 shows these components and th\", \"eir responsibilities.124 CHAPTER 6 | Cloud-native resiliency\\nFigure 6-8. Service mesh control and da\", \"ta plane\\nOnce configured, a service mesh is highly functional. It can retrieve a corresponding pool \", \"of instances \\nfrom a service discovery endpoint. The mesh can then send a request to a specific inst\", \"ance, recording \\nthe latency and response type of the result. A mesh can choose the instance most li\", \"kely to return a \\nfast response based on many factors, including its observed latency for recent req\", \"uests.\\nIf an instance is unresponsive or fails, the mesh will retry the request on another instance.\", \" If it returns \\nerrors, a mesh will evict the instance from the load-balancing pool and restate it a\", \"fter it heals. If a \\nrequest times out, a mesh can fail and then retry the request. A mesh captures \", \"and emits metrics and \\ndistributed tracing to a centralized metrics system.\\nIstio and Envoy\\nWhile a \", \"few service mesh options currently exist, Istio is the most popular at the time of this writing. \\nIs\", \"tio is a joint venture from IBM, Google, and Lyft. It\\u2019s an open-source offering that can be integrat\", \"ed \\ninto a new or existing distributed application. The technology provides a consistent and complet\", \"e \\nsolution to secure, connect, and monitor microservices. Its features include:\\n\\u2022 Secure service-to\", \"-service communication in a cluster with strong identity-based \\nauthentication and authorization.\\n\\u2022 \", \"Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.\\n\\u2022 Fine-grained control of traff\", \"ic behavior with rich routing rules, retries, failovers, and fault \\ninjection.\\n\\u2022 A pluggable policy \", \"layer and configuration API supporting access controls, rate limits, and \\nquotas.\\n\\u2022 Automatic metric\", \"s, logs, and traces for all traffic within a cluster, including cluster ingress and \\negress.\\nA key c\", \"omponent for an Istio implementation is a proxy service entitled the Envoy proxy. It runs \\nalongside\", \" each service and provides a platform-agnostic foundation for the following features:\\n\\u2022 Dynamic serv\", \"ice discovery.\\n\\u2022 Load balancing.125 CHAPTER 6 | Cloud-native resiliency\\n\\u2022 TLS termination.\\n\\u2022 HTTP an\", \"d gRPC proxies.\\n\\u2022 Circuit breaker resiliency.\\n\\u2022 Health checks.\\n\\u2022 Rolling updates with canary deploym\", \"ents.\\nAs previously discussed, Envoy is deployed as a sidecar to each microservice in the cluster.\\nI\", \"ntegration with Azure Kubernetes Services\\nThe Azure cloud embraces Istio and provides direct support\", \" for it within Azure Kubernetes Services. \\nThe following links can help you get started:\\n\\u2022 Installin\", \"g Istio in AKS\\n\\u2022 Using AKS and Istio\\nReferences\\n\\u2022 Polly\\n\\u2022 Retry pattern\\n\\u2022 Circuit Breaker pattern\\n\\u2022 \", \"Resilience in Azure whitepaper\\n\\u2022 network latency\\n\\u2022 Redundancy\\n\\u2022 geo-replication\\n\\u2022 Azure Traffic Mana\", \"ger\\n\\u2022 Autoscaling guidance\\n\\u2022 Istio\\n\\u2022 Envoy proxy126 CHAPTER 7 | Monitoring and health\\nCHAPTER 7\\nMoni\", \"toring and health\\nMicroservices and cloud-native applications go hand in hand with good DevOps pract\", \"ices. DevOps is \\nmany things to many people but perhaps one of the better definitions comes from clo\", \"ud advocate \\nand DevOps evangelist Donovan Brown:\\n\\u201cDevOps is the union of people, process, and produ\", \"cts to enable continuous delivery of value to our \\nend users.\\u201d\\nUnfortunately, with terse definitions\", \", there\\u2019s always room to say more things. One of the key \\ncomponents of DevOps is ensuring that the \", \"applications running in production are functioning \\nproperly and efficiently. To gauge the health of\", \" the application in production, it\\u2019s necessary to monitor \\nthe various logs and metrics being produc\", \"ed from the servers, hosts, and the application proper. The \\nnumber of different services running in\", \" support of a cloud-native application makes monitoring the \\nhealth of individual components and the\", \" application as a whole a critical challenge.\\nObservability patterns\\nJust as patterns have been deve\", \"loped to aid in the layout of code in applications, there are patterns \\nfor operating applications i\", \"n a reliable way. Three useful patterns in maintaining applications have \\nemerged: logging, monitori\", \"ng, and alerts.\\nWhen to use logging\\nNo matter how careful we are, applications almost always behave \", \"in unexpected ways in production. \\nWhen users report problems with an application, it\\u2019s useful to be\", \" able to see what was going on with \\nthe app when the problem occurred. One of the most tried and tr\", \"ue ways of capturing information \\nabout what an application is doing while it\\u2019s running is to have t\", \"he application write down what it\\u2019s \\ndoing. This process is known as logging. Anytime failures or pr\", \"oblems occur in production, the goal \\nshould be to reproduce the conditions under which the failures\", \" occurred, in a non-production \\nenvironment. Having good logging in place provides a roadmap for dev\", \"elopers to follow in order to \\nduplicate problems in an environment that can be tested and experimen\", \"ted with.\\nChallenges when logging with cloud-native applications\\nIn traditional applications, log fi\", \"les are typically stored on the local machine. In fact, on Unix-like \\noperating systems, there\\u2019s a f\", \"older structure defined to hold any logs, typically under /var/log.127 CHAPTER 7 | Monitoring and he\", \"alth\\nFigure 7-1. Logging to a file in a monolithic app.\\nThe usefulness of logging to a flat file on \", \"a single machine is vastly reduced in a cloud environment. \\nApplications producing logs may not have\", \" access to the local disk or the local disk may be highly \\ntransient as containers are shuffled arou\", \"nd physical machines. Even simple scaling up of monolithic \\napplications across multiple nodes can m\", \"ake it challenging to locate the appropriate file-based log \\nfile.\\nFigure 7-2. Logging to files in a\", \" scaled monolithic app.\\nCloud-native applications developed using a microservices architecture also \", \"pose some challenges for \\nfile-based loggers. User requests may now span multiple services that are \", \"run on different machines 128 CHAPTER 7 | Monitoring and health\\nand may include serverless functions\", \" with no access to a local file system at all. It would be very \\nchallenging to correlate the logs f\", \"rom a user or a session across these many services and machines.\\nFigure 7-3. Logging to local files \", \"in a microservices app.\\nFinally, the number of users in some cloud-native applications is high. Imag\", \"ine that each user \\ngenerates a hundred lines of log messages when they log into an application. In \", \"isolation, that is \\nmanageable, but multiply that over 100,000 users and the volume of logs becomes \", \"large enough that \\nspecialized tools are needed to support effective use of the logs.\\nLogging in clo\", \"ud-native applications\\nEvery programming language has tooling that permits writing logs, and typical\", \"ly the overhead for \\nwriting these logs is low. Many of the logging libraries provide logging differ\", \"ent kinds of criticalities, \\nwhich can be tuned at run time. For instance, the Serilog library is a \", \"popular structured logging library \\nfor .NET that provides the following logging levels:\\n\\u2022 Verbose\\n\\u2022\", \" Debug\\n\\u2022 Information\\n\\u2022 Warning\\n\\u2022 Error\\n\\u2022 Fatal\\nThese different log levels provide granularity in log\", \"ging. When the application is functioning properly \\nin production, it may be configured to only log \", \"important messages. When the application is 129 CHAPTER 7 | Monitoring and health\\nmisbehaving, then \", \"the log level can be increased so more verbose logs are gathered. This balances \\nperformance against\", \" ease of debugging.\\nThe high performance of logging tools and the tunability of verbosity should enc\", \"ourage developers to \\nlog frequently. Many favor a pattern of logging the entry and exit of each met\", \"hod. This approach may \\nsound like overkill, but it\\u2019s infrequent that developers will wish for less \", \"logging. In fact, it\\u2019s not \\nuncommon to perform deployments for the sole purpose of adding logging a\", \"round a problematic \\nmethod. Err on the side of too much logging and not on too little. Some tools c\", \"an be used to \\nautomatically provide this kind of logging.\\nBecause of the challenges associated with\", \" using file-based logs in cloud-native apps, centralized logs \\nare preferred. Logs are collected by \", \"the applications and shipped to a central logging application \\nwhich indexes and stores the logs. Th\", \"is class of system can ingest tens of gigabytes of logs every day.\\nIt\\u2019s also helpful to follow some \", \"standard practices when building logging that spans many services. \\nFor instance, generating a corre\", \"lation ID at the start of a lengthy interaction, and then logging it in \\neach message that is relate\", \"d to that interaction, makes it easier to search for all related messages. One \\nneed only find a sin\", \"gle message and extract the correlation ID to find all the related messages. \\nAnother example is ens\", \"uring that the log format is the same for every service, whatever the language \\nor logging library i\", \"t uses. This standardization makes reading logs much easier. Figure 7-4 \\ndemonstrates how a microser\", \"vices architecture can leverage centralized logging as part of its \\nworkflow.\\nFigure 7-4. Logs from \", \"various sources are ingested into a centralized log store.130 CHAPTER 7 | Monitoring and health\\nChal\", \"lenges with detecting and responding to potential app health \\nissues\\nSome applications aren\\u2019t missio\", \"n critical. Maybe they\\u2019re only used internally, and when a problem \\noccurs, the user can contact the\", \" team responsible and the application can be restarted. However, \\ncustomers often have higher expect\", \"ations for the applications they consume. You should know when \\nproblems occur with your application\", \" before users do, or before users notify you. Otherwise, the first \\nyou know about a problem may be \", \"when you notice an angry deluge of social media posts deriding \\nyour application or even your organi\", \"zation.\\nSome scenarios you may need to consider include:\\n\\u2022 One service in your application keeps fai\", \"ling and restarting, resulting in intermittent slow \\nresponses.\\n\\u2022 At some times of the day, your app\", \"lication\\u2019s response time is slow.\\n\\u2022 After a recent deployment, load on the database has tripled.\\nImp\", \"lemented properly, monitoring can let you know about conditions that will lead to problems, \\nletting\", \" you address underlying conditions before they result in any significant user impact.\\nMonitoring clo\", \"ud-native apps\\nSome centralized logging systems take on an additional role of collecting telemetry o\", \"utside of pure \\nlogs. They can collect metrics, such as time to run a database query, average respon\", \"se time from a \\nweb server, and even CPU load averages and memory pressure as reported by the operat\", \"ing system. \\nIn conjunction with the logs, these systems can provide a holistic view of the health o\", \"f nodes in the \\nsystem and the application as a whole.\\nThe metric-gathering capabilities of the moni\", \"toring tools can also be fed manually from within the \\napplication. Business flows that are of parti\", \"cular interest such as new users signing up or orders being \\nplaced, may be instrumented such that t\", \"hey increment a counter in the central monitoring system. \\nThis aspect unlocks the monitoring tools \", \"to not only monitor the health of the application but the \\nhealth of the business.\\nQueries can be co\", \"nstructed in the log aggregation tools to look for certain statistics or patterns, which \\ncan then b\", \"e displayed in graphical form, on custom dashboards. Frequently, teams will invest in large, \\nwall-m\", \"ounted displays that rotate through the statistics related to an application. This way, it\\u2019s simple \", \"\\nto see the problems as they occur.\\nCloud-native monitoring tools provide real-time telemetry and in\", \"sight into apps regardless of whether \\nthey\\u2019re single-process monolithic applications or distributed\", \" microservice architectures. They include \\ntools that allow collection of data from the app as well \", \"as tools for querying and displaying \\ninformation about the app\\u2019s health.\\nChallenges with reacting t\", \"o critical problems in cloud-native apps\\nIf you need to react to problems with your application, you\", \" need some way to alert the right \\npersonnel. This is the third cloud-native application observabili\", \"ty pattern and depends on logging and \\nmonitoring. Your application needs to have logging in place t\", \"o allow problems to be diagnosed, and 131 CHAPTER 7 | Monitoring and health\\nin some cases to feed in\", \"to monitoring tools. It needs monitoring to aggregate application metrics and \\nhealth data in one pl\", \"ace. Once this has been established, rules can be created that will trigger alerts \\nwhen certain met\", \"rics fall outside of acceptable levels.\\nGenerally, alerts are layered on top of monitoring such that\", \" certain conditions trigger appropriate \\nalerts to notify team members of urgent problems. Some scen\", \"arios that may require alerts include:\\n\\u2022 One of your application\\u2019s services is not responding after \", \"1 minute of downtime.\\n\\u2022 Your application is returning unsuccessful HTTP responses to more than 1% of\", \" requests.\\n\\u2022 Your application\\u2019s average response time for key endpoints exceeds 2000 ms.\\nAlerts in c\", \"loud-native apps\\nYou can craft queries against the monitoring tools to look for known failure condit\", \"ions. For instance, \\nqueries could search through the incoming logs for indications of HTTP status c\", \"ode 500, which \\nindicates a problem on a web server. As soon as one of these is detected, then an e-\", \"mail or an SMS \\ncould be sent to the owner of the originating service who can begin to investigate.\\n\", \"Typically, though, a single 500 error isn\\u2019t enough to determine that a problem has occurred. It coul\", \"d \\nmean that a user mistyped their password or entered some malformed data. The alert queries can be\", \" \\ncrafted to only fire when a larger than average number of 500 errors are detected.\\nOne of the most\", \" damaging patterns in alerting is to fire too many alerts for humans to investigate. \\nService owners\", \" will rapidly become desensitized to errors that they\\u2019ve previously investigated and \\nfound to be be\", \"nign. Then, when true errors occur, they\\u2019ll be lost in the noise of hundreds of false \\npositives. Th\", \"e parable of the Boy Who Cried Wolf is frequently told to children to warn them of this \\nvery danger\", \". It\\u2019s important to ensure that the alerts that do fire are indicative of a real problem.\\nLogging wi\", \"th Elastic Stack\\nThere are many good centralized logging tools and they vary in cost from being free\", \", open-source \\ntools, to more expensive options. In many cases, the free tools are as good as or bet\", \"ter than the paid \\nofferings. One such tool is a combination of three open-source components: Elasti\", \"csearch, Logstash, \\nand Kibana.\\nCollectively these tools are known as the Elastic Stack or ELK stack\", \".\\nElastic Stack\\nThe Elastic Stack is a powerful option for gathering information from a Kubernetes c\", \"luster. Kubernetes \\nsupports sending logs to an Elasticsearch endpoint, and for the most part, all y\", \"ou need to get started \\nis to set the environment variables as shown in Figure 7-5:\\nKUBE_LOGGING_DES\", \"TINATION=elasticsearch\\nKUBE_ENABLE_NODE_LOGGING=true\\nFigure 7-5. Configuration variables for Kuberne\", \"tes\\nThis step will install Elasticsearch on the cluster and target sending all the cluster logs to i\", \"t.132 CHAPTER 7 | Monitoring and health\\nFigure 7-6. An example of a Kibana dashboard showing the res\", \"ults of a query against logs that are ingested from \\nKubernetes\\nWhat are the advantages of Elastic S\", \"tack?\\nElastic Stack provides centralized logging in a low-cost, scalable, cloud-friendly manner. Its\", \" user \\ninterface streamlines data analysis so you can spend your time gleaning insights from your da\", \"ta \\ninstead of fighting with a clunky interface. It supports a wide variety of inputs so as your dis\", \"tributed \\napplication spans more and different kinds of services, you can expect to continue to be a\", \"ble to feed \\nlog and metric data into the system. The Elastic Stack also supports fast searches even\", \" across large \\ndata sets, making it possible even for large applications to log detailed data and st\", \"ill be able to have \\nvisibility into it in a performant fashion.\\nLogstash\\nThe first component is Log\", \"stash. This tool is used to gather log information from a large variety of \\ndifferent sources. For i\", \"nstance, Logstash can read logs from disk and also receive messages from \\nlogging libraries like Ser\", \"ilog. Logstash can do some basic filtering and expansion on the logs as they \\narrive. For instance, \", \"if your logs contain IP addresses then Logstash may be configured to do a \\ngeographical lookup and o\", \"btain a country/region or even city of origin for that message.\\nSerilog is a logging library for .NE\", \"T languages, which allows for parameterized logging. Instead of \\ngenerating a textual log message th\", \"at embeds fields, parameters are kept separate. This library allows \\nfor more intelligent filtering \", \"and searching. A sample Serilog configuration for writing to Logstash \\nappears in Figure 7-7.\\nvar lo\", \"g = new LoggerConfiguration()\\n .WriteTo.Http(\\\"http://localhost:8080\\\")\\n .CreateLogger();\\nFigure 7-7. \", \"Serilog config for writing log information directly to logstash over HTTP\\nLogstash would use a confi\", \"guration like the one shown in Figure 7-8.133 CHAPTER 7 | Monitoring and health\\ninput {\\n http {\\n #de\", \"fault host 0.0.0.0:8080\\n codec => json\\n }\\n}\\noutput {\\n elasticsearch {\\n hosts => \\\"elasticsearch:9200\\\"\", \"\\n index=>\\\"sales-%{+xxxx.ww}\\\"\\n }\\n}\\nFigure 7-8. A Logstash configuration for consuming logs from Seril\", \"og\\nFor scenarios where extensive log manipulation isn\\u2019t needed there\\u2019s an alternative to Logstash kn\", \"own \\nas Beats. Beats is a family of tools that can gather a wide variety of data from logs to networ\", \"k data \\nand uptime information. Many applications will use both Logstash and Beats.\\nOnce the logs ha\", \"ve been gathered by Logstash, it needs somewhere to put them. While Logstash \\nsupports many differen\", \"t outputs, one of the more exciting ones is Elasticsearch.\\nElasticsearch\\nElasticsearch is a powerful\", \" search engine that can index logs as they arrive. It makes running queries \\nagainst the logs quick.\", \" Elasticsearch can handle huge quantities of logs and, in extreme cases, can be \\nscaled out across m\", \"any nodes.\\nLog messages that have been crafted to contain parameters or that have had parameters spl\", \"it from \\nthem through Logstash processing, can be queried directly as Elasticsearch preserves this i\", \"nformation.\\nA query that searches for the top 10 pages visited by jill@example.com, appears in Figur\", \"e 7-9.\\n\\\"query\\\": {\\n \\\"match\\\": {\\n \\\"user\\\": \\\"jill@example.com\\\"\\n }\\n },\\n \\\"aggregations\\\": {\\n \\\"top_10_pages\\\":\", \" {\\n \\\"terms\\\": {\\n \\\"field\\\": \\\"page\\\",\\n \\\"size\\\": 10\\n }\\n }\\n }\\nFigure 7-9. An Elasticsearch query for finding\", \" top 10 pages visited by a user\\nVisualizing information with Kibana web dashboards\\nThe final compone\", \"nt of the stack is Kibana. This tool is used to provide interactive visualizations in a \\nweb dashboa\", \"rd. Dashboards may be crafted even by users who are non-technical. Most data that is \\nresident in th\", \"e Elasticsearch index, can be included in the Kibana dashboards. Individual users may 134 CHAPTER 7 \", \"| Monitoring and health\\nhave different dashboard desires and Kibana enables this customization throu\", \"gh allowing user\\u0002specific dashboards.\\nInstalling Elastic Stack on Azure\\nThe Elastic stack can be ins\", \"talled on Azure in many ways. As always, it\\u2019s possible to provision virtual \\nmachines and install El\", \"astic Stack on them directly. This option is preferred by some experienced users \\nas it offers the h\", \"ighest degree of customizability. Deploying on infrastructure as a service introduces \\nsignificant m\", \"anagement overhead forcing those who take that path to take ownership of all the tasks\\nassociated wi\", \"th infrastructure as a service such as securing the machines and keeping up-to-date with \\npatches.\\nA\", \"n option with less overhead is to make use of one of the many Docker containers on which the \\nElasti\", \"c Stack has already been configured. These containers can be dropped into an existing \\nKubernetes cl\", \"uster and run alongside application code. The sebp/elk container is a well-documented \\nand tested El\", \"astic Stack container.\\nAnother option is a recently announced ELK-as-a-service offering.\\nReferences\\n\", \"\\u2022 Install Elastic Stack on Azure\\nMonitoring in Azure Kubernetes Services\\nThe built-in logging in Kub\", \"ernetes is primitive. However, there are some great options for getting the \\nlogs out of Kubernetes \", \"and into a place where they can be properly analyzed. If you need to monitor \\nyour AKS clusters, con\", \"figuring Elastic Stack for Kubernetes is a great solution.\\nAzure Monitor for Containers\\nAzure Monito\", \"r for Containers supports consuming logs from not just Kubernetes but also from other \\norchestration\", \" engines such as DC/OS, Docker Swarm, and Red Hat OpenShift.135 CHAPTER 7 | Monitoring and health\\nFi\", \"gure 7-10. Consuming logs from various containers\\nPrometheus is a popular open source metric monitor\", \"ing solution. It is part of the Cloud Native \\nCompute Foundation. Typically, using Prometheus requir\", \"es managing a Prometheus server with its \\nown store. However, Azure Monitor for Containers provides \", \"direct integration with Prometheus \\nmetrics endpoints, so a separate server is not required.\\nLog and\", \" metric information is gathered not just from the containers running in the cluster but also \\nfrom t\", \"he cluster hosts themselves. It allows correlating log information from the two making it much \\neasi\", \"er to track down an error.\\nInstalling the log collectors differs on Windows and Linux clusters. But \", \"in both cases the log collection \\nis implemented as a Kubernetes DaemonSet, meaning that the log col\", \"lector is run as a container on \\neach of the nodes.\\nNo matter which orchestrator or operating system\", \" is running the Azure Monitor daemon, the log \\ninformation is forwarded to the same Azure Monitor to\", \"ols with which users are familiar. This approach \\nensures a parallel experience in environments that\", \" mix different log sources such as a hybrid \\nKubernetes/Azure Functions environment.136 CHAPTER 7 | \", \"Monitoring and health\\nFigure 7-11. A sample dashboard showing logging and metric information from ma\", \"ny running containers.\\nLog.Finalize()\\nLogging is one of the most overlooked and yet most important p\", \"arts of deploying any application at \\nscale. As the size and complexity of applications increase, th\", \"en so does the difficulty of debugging \\nthem. Having top quality logs available makes debugging much\", \" easier and moves it from the realm of \\n\\u201cnearly impossible\\u201d to \\u201ca pleasant experience\\u201d.\\nAzure Monito\", \"r\\nNo other cloud provider has as mature of a cloud application monitoring solution than that found i\", \"n \\nAzure. Azure Monitor is an umbrella name for a collection of tools designed to provide visibility\", \" into \\nthe state of your system. It helps you understand how your cloud-native services are performi\", \"ng and \\nproactively identifies issues affecting them. Figure 7-12 presents a high level of view of A\", \"zure Monitor.137 CHAPTER 7 | Monitoring and health\\nFigure 7-12. High-level view of Azure Monitor.\\nGa\", \"thering logs and metrics\\nThe first step in any monitoring solution is to gather as much data as poss\", \"ible. The more data \\ngathered, the deeper the insights. Instrumenting systems has traditionally been\", \" difficult. Simple \\nNetwork Management Protocol (SNMP) was the gold standard protocol for collecting\", \" machine level \\ninformation, but it required a great deal of knowledge and configuration. Fortunatel\", \"y, much of this \\nhard work has been eliminated as the most common metrics are gathered automatically\", \" by Azure \\nMonitor.\\nApplication level metrics and events aren\\u2019t possible to instrument automatically\", \" because they\\u2019re \\nspecific to the application being deployed. In order to gather these metrics, ther\", \"e are SDKs and APIs \\navailable to directly report such information, such as when a customer signs up\", \" or completes an order. \\nExceptions can also be captured and reported back into Azure Monitor via Ap\", \"plication Insights. The \\nSDKs support most every language found in Cloud Native Applications includi\", \"ng Go, Python, \\nJavaScript, and the .NET languages.\\nThe ultimate goal of gathering information about\", \" the state of your application is to ensure that your \\nend users have a good experience. What better\", \" way to tell if users are experiencing issues than doing \\noutside-in web tests? These tests can be a\", \"s simple as pinging your website from locations around the \\nworld or as involved as having agents lo\", \"g into the site and simulate user actions.\\nReporting data\\nOnce the data is gathered, it can be manip\", \"ulated, summarized, and plotted into charts, which allow \\nusers to instantly see when there are prob\", \"lems. These charts can be gathered into dashboards or into \\nWorkbooks, a multi-page report designed \", \"to tell a story about some aspect of the system.138 CHAPTER 7 | Monitoring and health\\nNo modern appl\", \"ication would be complete without some artificial intelligence or machine learning. To \\nthis end, da\", \"ta can be passed to the various machine learning tools in Azure to allow you to extract \\ntrends and \", \"information that would otherwise be hidden.\\nApplication Insights provides a powerful (SQL-like) quer\", \"y language called Kusto that can query \\nrecords, summarize them, and even plot charts. For example, \", \"the following query will locate all records \\nfor the month of November 2007, group them by state, an\", \"d plot the top 10 as a pie chart.\\nStormEvents\\n| where StartTime >= datetime(2007-11-01) and StartTim\", \"e < datetime(2007-12-01)\\n| summarize count() by State\\n| top 10 by count_\\n| render piechart\\nFigure 7-\", \"13 shows the results of this Application Insights Query.\\nFigure 7-13. Application Insights query res\", \"ults.\\nThere is a playground for experimenting with Kusto queries. Reading sample queries can also be\", \" \\ninstructive.\\nDashboards\\nThere are several different dashboard technologies that may be used to sur\", \"face the information from \\nAzure Monitor. Perhaps the simplest is to just run queries in Application\", \" Insights and plot the data \\ninto a chart.139 CHAPTER 7 | Monitoring and health\\nFigure 7-14. An exam\", \"ple of Application Insights charts embedded in the main Azure Dashboard.\\nThese charts can then be em\", \"bedded in the Azure portal proper through use of the dashboard feature. \\nFor users with more exactin\", \"g requirements, such as being able to drill down into several tiers of data, \\nAzure Monitor data is \", \"available to Power BI. Power BI is an industry-leading, enterprise class, business \\nintelligence too\", \"l that can aggregate data from many different data sources.140 CHAPTER 7 | Monitoring and health\\nFig\", \"ure 7-15. An example Power BI dashboard.\\nAlerts\\nSometimes, having data dashboards is insufficient. I\", \"f nobody is awake to watch the dashboards, then \\nit can still be many hours before a problem is addr\", \"essed, or even detected. To this end, Azure Monitor \\nalso provides a top notch alerting solution. Al\", \"erts can be triggered by a wide range of conditions \\nincluding:\\n\\u2022 Metric values\\n\\u2022 Log search queries\", \"\\n\\u2022 Activity Log events\\n\\u2022 Health of the underlying Azure platform\\n\\u2022 Tests for web site availability\\nW\", \"hen triggered, the alerts can perform a wide variety of tasks. On the simple side, the alerts may ju\", \"st \\nsend an e-mail notification to a mailing list or a text message to an individual. More involved \", \"alerts 141 CHAPTER 7 | Monitoring and health\\nmight trigger a workflow in a tool such as PagerDuty, w\", \"hich is aware of who is on call for a particular \\napplication. Alerts can trigger actions in Microso\", \"ft Flow unlocking near limitless possibilities for \\nworkflows.\\nAs common causes of alerts are identi\", \"fied, the alerts can be enhanced with details about the common \\ncauses of the alerts and the steps t\", \"o take to resolve them. Highly mature cloud-native application \\ndeployments may opt to kick off self\", \"-healing tasks, which perform actions such as removing failing \\nnodes from a scale set or triggering\", \" an autoscaling activity. Eventually it may no longer be necessary \\nto wake up on-call personnel at \", \"2AM to resolve a live-site issue as the system will be able to adjust \\nitself to compensate or at le\", \"ast limp along until somebody arrives at work the next morning.\\nAzure Monitor automatically leverage\", \"s machine learning to understand the normal operating \\nparameters of deployed applications. This app\", \"roach enables it to detect services that are operating \\noutside of their normal parameters. For inst\", \"ance, the typical weekday traffic on the site might be \\n10,000 requests per minute. And then, on a g\", \"iven week, suddenly the number of requests hits a highly \\nunusual 20,000 requests per minute. Smart \", \"Detection will notice this deviation from the norm and \\ntrigger an alert. At the same time, the tren\", \"d analysis is smart enough to avoid firing false positives \\nwhen the traffic load is expected.\\nRefer\", \"ences\\n\\u2022 Azure Monitor142 CHAPTER 8 | Cloud-native identity\\nCHAPTER 8\\nCloud-native identity\\nMost soft\", \"ware applications need to have some knowledge of the user or process that is calling them. \\nThe user\", \" or process interacting with an application is known as a security principal, and the process of \\nau\", \"thenticating and authorizing these principals is known as identity management, or simply identity. \\n\", \"Simple applications may include all of their identity management within the application, but this \\na\", \"pproach doesn\\u2019t scale well with many applications and many kinds of security principals. Windows \\nsu\", \"pports the use of Active Directory to provide centralized authentication and authorization.\\nWhile th\", \"is solution is effective within corporate networks, it isn\\u2019t designed for use by users or \\napplicati\", \"ons that are outside of the AD domain. With the growth of Internet-based applications and \\nthe rise \", \"of cloud-native apps, security models have evolved.\\nIn today\\u2019s cloud-native identity model, architec\", \"ture is assumed to be distributed. Apps can be \\ndeployed anywhere and may communicate with other app\", \"s anywhere. Clients may communicate with \\nthese apps from anywhere, and in fact, clients may consist\", \" of any combination of platforms and \\ndevices. Cloud-native identity solutions use open standards to\", \" achieve secure application access from \\nclients. These clients range from human users on PCs or pho\", \"nes, to other apps hosted anywhere \\nonline, to set-top boxes and IOT devices running any software pl\", \"atform anywhere in the world.\\nModern cloud-native identity solutions typically use access tokens tha\", \"t are issued by a secure token \\nservice/server (STS) to a security principal once their identity is \", \"determined. The access token, typically \\na JSON Web Token (JWT), includes claims about the security \", \"principal. These claims will minimally \\ninclude the user\\u2019s identity but may also include other claim\", \"s that can be used by applications to \\ndetermine the level of access to grant the principal.\\nTypical\", \"ly, the STS is only responsible for authenticating the principal. Determining their level of access \", \"\\nto resources is left to other parts of the application.\\nReferences\\n\\u2022 Microsoft identity platform\\nAu\", \"thentication and authorization in cloud-native \\napps\\nAuthentication is the process of determining th\", \"e identity of a security principal. Authorization is the act \\nof granting an authenticated principal\", \" permission to perform an action or access a resource. \\nSometimes authentication is shortened to Aut\", \"hN and authorization is shortened to AuthZ. Cloud-143 CHAPTER 8 | Cloud-native identity\\nnative appli\", \"cations need to rely on open HTTP-based protocols to authenticate security principals \\nsince both cl\", \"ients and applications could be running anywhere in the world on any platform or device. \\nThe only c\", \"ommon factor is HTTP.\\nMany organizations still rely on local authentication services like Active Dir\", \"ectory Federation Services \\n(ADFS). While this approach has traditionally served organizations well \", \"for on premises authentication \\nneeds, cloud-native applications benefit from systems designed speci\", \"fically for the cloud. A recent \\n2019 United Kingdom National Cyber Security Centre (NCSC) advisory \", \"states that \\u201corganizations using \\nAzure AD as their primary authentication source will actually lowe\", \"r their risk compared to ADFS.\\u201d \\nSome reasons outlined in this analysis include:\\n\\u2022 Access to full se\", \"t of Microsoft credential protection technologies.\\n\\u2022 Most organizations are already relying on Azure\", \" AD to some extent.\\n\\u2022 Double hashing of NTLM hashes ensures compromise won\\u2019t allow credentials that \", \"work in \\nlocal Active Directory.\\nReferences\\n\\u2022 Authentication basics\\n\\u2022 Access tokens and claims\\n\\u2022 It \", \"may be time to ditch your on premises authentication services\\nAzure Active Directory\\nMicrosoft Azure\", \" Active Directory (Azure AD) offers identity and access management as a service. \\nCustomers use it t\", \"o configure and maintain who users are, what information to store about them, who \\ncan access that i\", \"nformation, who can manage it, and what apps can access it. AAD can authenticate \\nusers for applicat\", \"ions configured to use it, providing a single sign-on (SSO) experience. It can be used \\non its own o\", \"r be integrated with Windows AD running on premises.\\nAzure AD is built for the cloud. It\\u2019s truly a c\", \"loud-native identity solution that uses a REST-based Graph \\nAPI and OData syntax for queries, unlike\", \" Windows AD, which uses LDAP. On premises Active Directory \\ncan sync user attributes to the cloud us\", \"ing Identity Sync Services, allowing all authentication to take \\nplace in the cloud using Azure AD. \", \"Alternately, authentication can be configured via Connect to pass \\nback to local Active Directory vi\", \"a ADFS to be completed by Windows AD on premises.\\nAzure AD supports company branded sign-in screens,\", \" multi-factory authentication, and cloud-based \\napplication proxies that are used to provide SSO for\", \" applications hosted on premises. It offers \\ndifferent kinds of security reporting and alert capabil\", \"ities.\\nReferences\\n\\u2022 Microsoft identity platform144 CHAPTER 8 | Cloud-native identity\\nIdentityServer \", \"for cloud-native applications\\nIdentityServer is an authentication server that implements OpenID Conn\", \"ect (OIDC) and OAuth 2.0 \\nstandards for ASP.NET Core. It\\u2019s designed to provide a common way to authe\", \"nticate requests to all of \\nyour applications, whether they\\u2019re web, native, mobile, or API endpoints\", \". IdentityServer can be used to \\nimplement Single Sign-On (SSO) for multiple applications and applic\", \"ation types. It can be used to \\nauthenticate actual users via sign-in forms and similar user interfa\", \"ces as well as service-based \\nauthentication that typically involves token issuance, verification, a\", \"nd renewal without any user \\ninterface. IdentityServer is designed to be a customizable solution. Ea\", \"ch instance is typically \\ncustomized to suit an individual organization and/or set of applications\\u2019 \", \"needs.\\nCommon web app scenarios\\nTypically, applications need to support some or all of the following\", \" scenarios:\\n\\u2022 Human users accessing web applications with a browser.\\n\\u2022 Human users accessing back-en\", \"d Web APIs from browser-based apps.\\n\\u2022 Human users on mobile/native clients accessing back-end Web AP\", \"Is.\\n\\u2022 Other applications accessing back-end Web APIs (without an active user or user interface).\\n\\u2022 A\", \"ny application may need to interact with other Web APIs, using its own identity or \\ndelegating to th\", \"e user\\u2019s identity.\\nFigure 8-1. Application types and scenarios.\\nIn each of these scenarios, the expo\", \"sed functionality needs to be secured against unauthorized use. At \\na minimum, this typically requir\", \"es authenticating the user or principal making a request for a resource. \\nThis authentication may us\", \"e one of several common protocols such as SAML2p, WS-Fed, or OpenID \\nConnect. Communicating with API\", \"s typically uses the OAuth2 protocol and its support for security \\ntokens. Separating these critical\", \" cross-cutting security concerns and their implementation details from \\nthe applications themselves \", \"ensures consistency and improves security and maintainability. 145 CHAPTER 8 | Cloud-native identity\", \"\\nOutsourcing these concerns to a dedicated product like IdentityServer helps the requirement for eve\", \"ry \\napplication to solve these problems itself.\\nIdentityServer provides middleware that runs within \", \"an ASP.NET Core application and adds support \\nfor OpenID Connect and OAuth2 (see supported specifica\", \"tions). Organizations would create their own \\nASP.NET Core app using IdentityServer middleware to ac\", \"t as the STS for all of their token-based \\nsecurity protocols. The IdentityServer middleware exposes\", \" endpoints to support standard \\nfunctionality, including:\\n\\u2022 Authorize (authenticate the end user)\\n\\u2022 \", \"Token (request a token programmatically)\\n\\u2022 Discovery (metadata about the server)\\n\\u2022 User Info (get us\", \"er information with a valid access token)\\n\\u2022 Device Authorization (used to start device flow authoriz\", \"ation)\\n\\u2022 Introspection (token validation)\\n\\u2022 Revocation (token revocation)\\n\\u2022 End Session (trigger sin\", \"gle sign-out across all apps)\\nGetting started\\nIdentityServer4 is available under dual license:\\n\\u2022 RPL\", \" - lets you use the IdentityServer4 free if used in open-source work\\n\\u2022 Paid - lets you use the Ident\", \"ityServer4 in a commercial scenario\\nFor more information about pricing, see the official product\\u2019s p\", \"ricing page.\\nYou can add it to your applications using its NuGet packages. The main package is Ident\", \"ityServer4, \\nwhich has been downloaded over four million times. The base package doesn\\u2019t include any\", \" user \\ninterface code and only supports in-memory configuration. To use it with a database, you\\u2019ll a\", \"lso want \\na data provider like IdentityServer4.EntityFramework, which uses Entity Framework Core to \", \"store \\nconfiguration and operational data for IdentityServer. For user interface, you can copy files\", \" from the \\nQuickstart UI repository into your ASP.NET Core MVC application to add support for sign i\", \"n and sign \\nout using IdentityServer middleware.\\nConfiguration\\nIdentityServer supports different kin\", \"ds of protocols and social authentication providers that can be \\nconfigured as part of each custom i\", \"nstallation. This is typically done in the ASP.NET Core application\\u2019s \\nProgram class (or in the Star\", \"tup class in the ConfigureServices method). The configuration involves \\nspecifying the supported pro\", \"tocols and the paths to the servers and endpoints that will be used. \\nFigure 8-2 shows an example co\", \"nfiguration taken from the IdentityServer4 Quickstart UI project:\\npublic class Startup\\n{\\n public voi\", \"d ConfigureServices(IServiceCollection services)\\n {\\n services.AddMvc();146 CHAPTER 8 | Cloud-native \", \"identity\\n // some details omitted\\n services.AddIdentityServer();\\n services.AddAuthentication()\\n .Add\", \"Google(\\\"Google\\\", options =>\\n {\\n options.SignInScheme =\\nIdentityServerConstants.ExternalCookieAuthent\", \"icationScheme;\\n options.ClientId = \\\"<insert here>\\\";\\n options.ClientSecret = \\\"<insert here>\\\";\\n })\\n .A\", \"ddOpenIdConnect(\\\"demoidsrv\\\", \\\"IdentityServer\\\", options =>\\n {\\n options.SignInScheme =\\nIdentityServerC\", \"onstants.ExternalCookieAuthenticationScheme;\\n options.SignOutScheme = IdentityServerConstants.Signou\", \"tScheme;\\n options.Authority = \\\"https://demo.identityserver.io/\\\";\\n options.ClientId = \\\"implicit\\\";\\n op\", \"tions.ResponseType = \\\"id_token\\\";\\n options.SaveTokens = true;\\n options.CallbackPath = new PathString(\", \"\\\"/signin-idsrv\\\");\\n options.SignedOutCallbackPath = new PathString(\\\"/signout-callback-idsrv\\\");\\n optio\", \"ns.RemoteSignOutPath = new PathString(\\\"/signout-idsrv\\\");\\n options.TokenValidationParameters = new To\", \"kenValidationParameters\\n {\\n NameClaimType = \\\"name\\\",\\n RoleClaimType = \\\"role\\\"\\n };\\n });\\n }\\n}\\nFigure 8-2\", \". Configuring IdentityServer.\\nJavaScript clients\\nMany cloud-native applications use server-side APIs\", \" and rich client single page applications (SPAs) on \\nthe front end. IdentityServer ships a JavaScrip\", \"t client (oidc-client.js) via NPM that can be added to \\nSPAs to enable them to use IdentityServer fo\", \"r sign in, sign out, and token-based authentication of \\nweb APIs.\\nReferences\\n\\u2022 IdentityServer docume\", \"ntation\\n\\u2022 Application types\\n\\u2022 JavaScript OIDC client147 CHAPTER 9 | Cloud-native security\\nCHAPTER 9\\n\", \"Cloud-native security\\nNot a day goes by where the news doesn\\u2019t contain some story about a company be\", \"ing hacked or \\nsomehow losing their customers\\u2019 data. Even countries/regions aren\\u2019t immune to the pro\", \"blems created \\nby treating security as an afterthought. For years, companies have treated the securi\", \"ty of customer \\ndata and, in fact, their entire networks as something of a \\u201cnice to have\\u201d. Windows s\", \"ervers were left \\nunpatched, ancient versions of PHP kept running, and MongoDB databases left wide o\", \"pen to the \\nworld.\\nHowever, there are starting to be real-world consequences for not maintaining a s\", \"ecurity mindset \\nwhen building and deploying applications. Many companies learned the hard way what \", \"can happen \\nwhen servers and desktops aren\\u2019t patched during the 2017 outbreak of NotPetya. The cost \", \"of these \\nattacks has easily reached into the billions, with some estimates putting the losses from \", \"this single \\nattack at 10 billion US dollars.\\nEven governments aren\\u2019t immune to hacking incidents. T\", \"he city of Baltimore was held ransom by \\ncriminals making it impossible for citizens to pay their bi\", \"lls or use city services.\\nThere has also been an increase in legislation that mandates certain data \", \"protections for personal \\ndata. In Europe, GDPR has been in effect for more than a year and, more re\", \"cently, California passed \\ntheir own version called CCDA, which comes into effect January 1, 2020. T\", \"he fines under GDPR can be \\nso punishing as to put companies out of business. Google has already bee\", \"n fined 50 million Euros for \\nviolations, but that\\u2019s just a drop in the bucket compared with the pot\", \"ential fines.\\nIn short, security is serious business.\\nAzure security for cloud-native apps\\nCloud-nat\", \"ive applications can be both easier and more difficult to secure than traditional applications. \\nOn \", \"the downside, you need to secure more smaller applications and dedicate more energy to build \\nout th\", \"e security infrastructure. The heterogeneous nature of programming languages and styles in \\nmost ser\", \"vice deployments also means you need to pay more attention to security bulletins from many \\ndifferen\", \"t providers.\\nOn the flip side, smaller services, each with their own data store, limit the scope of \", \"an attack. If an \\nattacker compromises one system, it\\u2019s probably more difficult for the attacker to \", \"make the jump to \\nanother system than it is in a monolithic application. Process boundaries are stro\", \"ng boundaries. Also, \\nif a database backup gets exposed, then the damage is more limited, as that da\", \"tabase contains only a \\nsubset of data and is unlikely to contain personal data.148 CHAPTER 9 | Clou\", \"d-native security\\nThreat modeling\\nNo matter if the advantages outweigh the disadvantages of cloud-na\", \"tive applications, the same \\nholistic security mindset must be followed. Security and secure thinkin\", \"g must be part of every step of \\nthe development and operations story. When planning an application \", \"ask questions like:\\n\\u2022 What would be the impact of this data being lost?\\n\\u2022 How can we limit the damag\", \"e from bad data being injected into this service?\\n\\u2022 Who should have access to this data?\\n\\u2022 Are there\", \" auditing policies in place around the development and release process?\\nAll these questions are part\", \" of a process called threat modeling. This process tries to answer the \\nquestion of what threats the\", \"re are to the system, how likely the threats are, and the potential damage \\nfrom them.\\nOnce the list\", \" of threats has been established, you need to decide whether they\\u2019re worth mitigating. \\nSometimes a \", \"threat is so unlikely and expensive to plan for that it isn\\u2019t worth spending energy on it. \\nFor inst\", \"ance, some state level actor could inject changes into the design of a process that is used by \\nmill\", \"ions of devices. Now, instead of running a certain piece of code in Ring 3, that code is run in Ring\", \" \\n0. This process allows an exploit that can bypass the hypervisor and run the attack code on the ba\", \"re \\nmetal machines, allowing attacks on all the virtual machines that are running on that hardware.\\n\", \"The altered processors are difficult to detect without a microscope and advanced knowledge of the on\", \" \\nsilicon design of that processor. This scenario is unlikely to happen and expensive to mitigate, s\", \"o \\nprobably no threat model would recommend building exploit protection for it.\\nMore likely threats,\", \" such as broken access controls permitting Id incrementing attacks (replacing Id=2 \\nwith Id=3 in the\", \" URL) or SQL injection, are more attractive to build protections against. The \\nmitigations for these\", \" threats are quite reasonable to build and prevent embarrassing security holes \\nthat smear the compa\", \"ny\\u2019s reputation.\\nPrinciple of least privilege\\nOne of the founding ideas in computer security is the \", \"Principle of Least Privilege (POLP). It\\u2019s actually a \\nfoundational idea in most any form of security\", \" be it digital or physical. In short, the principle is that \\nany user or process should have the sma\", \"llest number of rights possible to execute its task.\\nAs an example, think of the tellers at a bank: \", \"accessing the safe is an uncommon activity. So, the \\naverage teller can\\u2019t open the safe themselves. \", \"To gain access, they need to escalate their request \\nthrough a bank manager, who performs additional\", \" security checks.\\nIn a computer system, a fantastic example is the rights of a user connecting to a \", \"database. In many \\ncases, there\\u2019s a single user account used to both build the database structure an\", \"d run the application. \\nExcept in extreme cases, the account running the application doesn\\u2019t need th\", \"e ability to update \\nschema information. There should be several accounts that provide different lev\", \"els of privilege. The \\napplication should only use the permission level that grants read and writes \", \"access to the data in the \\ntables. This kind of protection would eliminate attacks that aimed to dro\", \"p database tables or \\nintroduce malicious triggers.149 CHAPTER 9 | Cloud-native security\\nAlmost ever\", \"y part of building a cloud-native application can benefit from remembering the principle \\nof least p\", \"rivilege. You can find it at play when setting up firewalls, network security groups, roles, and \\nsc\", \"opes in Role-based access control (RBAC).\\nPenetration testing\\nAs applications become more complicate\", \"d the number of attack vectors increases at an alarming rate. \\nThreat modeling is flawed in that it \", \"tends to be executed by the same people building the system. In \\nthe same way that many developers h\", \"ave trouble envisioning user interactions and then build \\nunusable user interfaces, most developers \", \"have difficulty seeing every attack vector. It\\u2019s also possible \\nthat the developers building the sys\", \"tem aren\\u2019t well versed in attack methodologies and miss \\nsomething crucial.\\nPenetration testing or \\u201c\", \"pen testing\\u201d involves bringing in external actors to attempt to attack the \\nsystem. These attackers \", \"may be an external consulting company or other developers with good \\nsecurity knowledge from another\", \" part of the business. They\\u2019re given carte blanche to attempt to \\nsubvert the system. Frequently, th\", \"ey\\u2019ll find extensive security holes that need to be patched. \\nSometimes the attack vector will be so\", \"mething totally unexpected like exploiting a phishing attack \\nagainst the CEO.\\nAzure itself is const\", \"antly undergoing attacks from a team of hackers inside Microsoft. Over the years, \\nthey\\u2019ve been the \", \"first to find dozens of potentially catastrophic attack vectors, closing them before \\nthey can be ex\", \"ploited externally. The more tempting a target, the more likely that eternal actors will \\nattempt to\", \" exploit it and there are a few targets in the world more tempting than Azure.\\nMonitoring\\nShould an \", \"attacker attempt to penetrate an application, there should be some warning of it. \\nFrequently, attac\", \"ks can be spotted by examining the logs from services. Attacks leave telltale signs \\nthat can be spo\", \"tted before they succeed. For instance, an attacker attempting to guess a password \\nwill make many r\", \"equests to a login system. Monitoring around the login system can detect weird \\npatterns that are ou\", \"t of line with the typical access pattern. This monitoring can be turned into an \\nalert that can, in\", \" turn, alert an operations person to activate some sort of countermeasure. A highly \\nmature monitori\", \"ng system might even take action based on these deviations proactively adding rules \\nto block reques\", \"ts or throttle responses.\\nSecuring the build\\nOne place where security is often overlooked is around \", \"the build process. Not only should the build \\nrun security checks, such as scanning for insecure cod\", \"e or checked-in credentials, but the build itself \\nshould be secure. If the build server is compromi\", \"sed, then it provides a fantastic vector for introducing \\narbitrary code into the product.\\nImagine t\", \"hat an attacker is looking to steal the passwords of people signing into a web application. \\nThey co\", \"uld introduce a build step that modifies the checked-out code to mirror any login request to \\nanothe\", \"r server. The next time code goes through the build, it\\u2019s silently updated. The source code \\nvulnera\", \"bility scanning won\\u2019t catch this vulnerability as it runs before the build. Equally, nobody will 150\", \" CHAPTER 9 | Cloud-native security\\ncatch it in a code review because the build steps live on the bui\", \"ld server. The exploited code will go to \\nproduction where it can harvest passwords. Probably there\\u2019\", \"s no audit log of the build process \\nchanges, or at least nobody monitoring the audit.\\nThis scenario\", \" is a perfect example of a seemingly low-value target that can be used to break into the \\nsystem. On\", \"ce an attacker breaches the perimeter of the system, they can start working on finding \\nways to elev\", \"ate their permissions to the point that they can cause real harm anywhere they like.\\nBuilding secure\", \" code\\n.NET Framework is already a quite secure framework. It avoids some of the pitfalls of unmanage\", \"d \\ncode, such as walking off the ends of arrays. Work is actively done to fix security holes as they\", \"\\u2019re \\ndiscovered. There\\u2019s even a bug bounty program that pays researchers to find issues in the frame\", \"work \\nand report them instead of exploiting them.\\nThere are many ways to make .NET code more secure.\", \" Following guidelines such as the Secure coding \\nguidelines for .NET article is a reasonable step to\", \" take to ensure that the code is secure from the \\nground up. The OWASP top 10 is another invaluable \", \"guide to build secure code.\\nThe build process is a good place to put scanning tools to detect proble\", \"ms in source code before they \\nmake it into production. Most every project has dependencies on some \", \"other packages. A tool that \\ncan scan for outdated packages will catch problems in a nightly build. \", \"Even when building Docker \\nimages, it\\u2019s useful to check and make sure that the base image doesn\\u2019t ha\", \"ve known vulnerabilities. \\nAnother thing to check is that nobody has accidentally checked in credent\", \"ials.\\nBuilt-in security\\nAzure is designed to balance usability and security for most users. Differen\", \"t users are going to have \\ndifferent security requirements, so they need to fine-tune their approach\", \" to cloud security. Microsoft \\npublishes a great deal of security information in the Trust Center. T\", \"his resource should be the first \\nstop for those professionals interested in understanding how the b\", \"uilt-in attack mitigation \\ntechnologies work.\\nWithin the Azure portal, the Azure Advisor is a system\", \" that is constantly scanning an environment and \\nmaking recommendations. Some of these recommendatio\", \"ns are designed to save users money, but \\nothers are designed to identify potentially insecure confi\", \"gurations, such as having a storage container \\nopen to the world and not protected by a Virtual Netw\", \"ork.\\nAzure network infrastructure\\nIn an on-premises deployment environment, a great deal of energy i\", \"s dedicated to setting up \\nnetworking. Setting up routers, switches, and the such is complicated wor\", \"k. Networks allow certain \\nresources to talk to other resources and prevent access in some cases. A \", \"frequent network rule is to \\nrestrict access to the production environment from the development envi\", \"ronment on the off chance \\nthat a half-developed piece of code runs awry and deletes a swath of data\", \".\\nOut of the box, most PaaS Azure resources have only the most basic and permissive networking setup\", \". \\nFor instance, anybody on the Internet can access an app service. New SQL Server instances typical\", \"ly 151 CHAPTER 9 | Cloud-native security\\ncome restricted, so that external parties can\\u2019t access them\", \", but the IP address ranges used by Azure \\nitself are permitted through. So, while the SQL server is\", \" protected from external threats, an attacker \\nonly needs to set up an Azure bridgehead from where t\", \"hey can launch attacks against all SQL \\ninstances on Azure.\\nFortunately, most Azure resources can be\", \" placed into an Azure Virtual Network that allows fine\\u0002grained access control. Similar to the way th\", \"at on-premises networks establish private networks that \\nare protected from the wider world, virtual\", \" networks are islands of private IP addresses that are \\nlocated within the Azure network.\\nFigure 9-1\", \". A virtual network in Azure.\\nIn the same way that on-premises networks have a firewall governing ac\", \"cess to the network, you can \\nestablish a similar firewall at the boundary of the virtual network. B\", \"y default, all the resources on a \\nvirtual network can still talk to the Internet. It\\u2019s only incomin\", \"g connections that require some form of \\nexplicit firewall exception.\\nWith the network established, \", \"internal resources like storage accounts can be set up to only allow for \\naccess by resources that a\", \"re also on the Virtual Network. This firewall provides an extra level of \\nsecurity, should the keys \", \"for that storage account be leaked, attackers wouldn\\u2019t be able to connect to \\nit to exploit the leak\", \"ed keys. This scenario is another example of the principle of least privilege.\\nThe nodes in an Azure\", \" Kubernetes cluster can participate in a virtual network just like other resources \\nthat are more na\", \"tive to Azure. This functionality is called Azure Container Networking Interface. In \\neffect, it all\", \"ocates a subnet within the virtual network on which virtual machines and container images \\nare alloc\", \"ated.\\nContinuing down the path of illustrating the principle of least privilege, not every resource \", \"within a \\nVirtual Network needs to talk to every other resource. For instance, in an application tha\", \"t provides a 152 CHAPTER 9 | Cloud-native security\\nweb API over a storage account and a SQL database\", \", it\\u2019s unlikely that the database and the storage \\naccount need to talk to one another. Any data sha\", \"ring between them would go through the web \\napplication. So, a network security group (NSG) could be\", \" used to deny traffic between the two \\nservices.\\nA policy of denying communication between resources\", \" can be annoying to implement, especially \\ncoming from a background of using Azure without traffic r\", \"estrictions. On some other clouds, the \\nconcept of network security groups is much more prevalent. F\", \"or instance, the default policy on AWS is \\nthat resources can\\u2019t communicate among themselves until e\", \"nabled by rules in an NSG. While slower \\nto develop this, a more restrictive environment provides a \", \"more secure default. Making use of proper \\nDevOps practices, especially using Azure Resource Manager\", \" or Terraform to manage permissions can \\nmake controlling the rules easier.\\nVirtual Networks can als\", \"o be useful when setting up communication between on-premises and cloud \\nresources. A virtual privat\", \"e network can be used to seamlessly attach the two networks together. This \\napproach allows running \", \"a virtual network without any sort of gateway for scenarios where all the \\nusers are on-site. There \", \"are a number of technologies that can be used to establish this network. The \\nsimplest is to use a s\", \"ite-to-site VPN that can be established between many routers and Azure. Traffic \\nis encrypted and tu\", \"nneled over the Internet at the same cost per byte as any other traffic. For \\nscenarios where more b\", \"andwidth or more security is desirable, Azure offers a service called Express \\nRoute that uses a pri\", \"vate circuit between an on-premises network and Azure. It\\u2019s more costly and \\ndifficult to establish \", \"but also more secure.\\nRole-based access control for restricting access to Azure resources\\nRBAC is a \", \"system that provides an identity to applications running in Azure. Applications can access \\nresource\", \"s using this identity instead of or in addition to using keys or passwords.\\nSecurity Principals\\nThe \", \"first component in RBAC is a security principal. A security principal can be a user, group, service \", \"\\nprincipal, or managed identity.\\nFigure 9-2. Different types of security principals.\\n\\u2022 User - Any us\", \"er who has an account in Azure Active Directory is a user.\\n\\u2022 Group - A collection of users from Azur\", \"e Active Directory. As a member of a group, a user \\ntakes on the roles of that group in addition to \", \"their own.\\n\\u2022 Service principal - A security identity under which services or applications run.153 CH\", \"APTER 9 | Cloud-native security\\n\\u2022 Managed identity - An Azure Active Directory identity managed by A\", \"zure. Managed identities \\nare typically used when developing cloud applications that manage the cred\", \"entials for \\nauthenticating to Azure services.\\nThe security principal can be applied to most any res\", \"ource. This aspect means that it\\u2019s possible to \\nassign a security principal to a container running w\", \"ithin Azure Kubernetes, allowing it to access secrets \\nstored in Key Vault. An Azure Function could \", \"take on a permission allowing it to talk to an Active \\nDirectory instance to validate a JWT for a ca\", \"lling user. Once services are enabled with a service \\nprincipal, their permissions can be managed gr\", \"anularly using roles and scopes.\\nRoles\\nA security principal can take on many roles or, using a more \", \"sartorial analogy, wear many hats. Each \\nrole defines a series of permissions such as \\u201cRead messages\", \" from Azure Service Bus endpoint\\u201d. The \\neffective permission set of a security principal is the comb\", \"ination of all the permissions assigned to all \\nthe roles that a security principal has. Azure has a\", \" large number of built-in roles and users can define \\ntheir own roles.\\nFigure 9-3. RBAC role definit\", \"ions.\\nBuilt into Azure are also a number of high-level roles such as Owner, Contributor, Reader, and\", \" User \\nAccount Administrator. With the Owner role, a security principal can access all resources and\", \" assign \\npermissions to others. A contributor has the same level of access to all resources but they\", \" can\\u2019t assign \\npermissions. A Reader can only view existing Azure resources and a User Account Admin\", \"istrator can \\nmanage access to Azure resources.\\nMore granular built-in roles such as DNS Zone Contri\", \"butor have rights limited to a single service. \\nSecurity principals can take on any number of roles.\", \"154 CHAPTER 9 | Cloud-native security\\nScopes\\nRoles can be applied to a restricted set of resources w\", \"ithin Azure. For instance, applying scope to the \\nprevious example of reading from a Service Bus que\", \"ue, you can narrow the permission to a single \\nqueue: \\u201cRead messages from Azure Service Bus endpoint\", \" blah.servicebus.windows.net/queue1\\u201d\\nThe scope can be as narrow as a single resource or it can be ap\", \"plied to an entire resource group, \\nsubscription, or even management group.\\nWhen testing if a securi\", \"ty principal has certain permission, the combination of role and scope are \\ntaken into account. This\", \" combination provides a powerful authorization mechanism.\\nDeny\\nPreviously, only \\u201callow\\u201d rules were p\", \"ermitted for RBAC. This behavior made some scopes complicated \\nto build. For instance, allowing a se\", \"curity principal access to all storage accounts except one required \\ngranting explicit permission to\", \" a potentially endless list of storage accounts. Every time a new storage \\naccount was created, it w\", \"ould have to be added to this list of accounts. This added management \\noverhead that certainly wasn\\u2019\", \"t desirable.\\nDeny rules take precedence over allow rules. Now representing the same \\u201callow all but o\", \"ne\\u201d scope \\ncould be represented as two rules \\u201callow all\\u201d and \\u201cdeny this one specific one\\u201d. Deny rule\", \"s not only \\nease management but allow for resources that are extra secure by denying access to every\", \"body.\\nChecking access\\nAs you can imagine, having a large number of roles and scopes can make figurin\", \"g out the effective \\npermission of a service principal quite difficult. Piling deny rules on top of \", \"that, only serves to increase \\nthe complexity. Fortunately, there\\u2019s a permissions calculator that ca\", \"n show the effective permissions \\nfor any service principal. It\\u2019s typically found under the IAM tab \", \"in the portal, as shown in Figure 9-3.\\nFigure 9-4. Permission calculator for an app service.155 CHAP\", \"TER 9 | Cloud-native security\\nSecuring secrets\\nPasswords and certificates are a common attack vector\", \" for attackers. Password-cracking hardware can \\ndo a brute-force attack and try to guess billions of\", \" passwords per second. So it\\u2019s important that the \\npasswords that are used to access resources are s\", \"trong, with a large variety of characters. These \\npasswords are exactly the kind of passwords that a\", \"re near impossible to remember. Fortunately, the \\npasswords in Azure don\\u2019t actually need to be known\", \" by any human.\\nMany security experts suggest that using a password manager to keep your own password\", \"s is the \\nbest approach. While it centralizes your passwords in one location, it also allows using h\", \"ighly complex \\npasswords and ensuring they\\u2019re unique for each account. The same system exists within\", \" Azure: a \\ncentral store for secrets.\\nAzure Key Vault\\nAzure Key Vault provides a centralized locatio\", \"n to store passwords for things such as databases, API \\nkeys, and certificates. Once a secret is ent\", \"ered into the Vault, it\\u2019s never shown again and the \\ncommands to extract and view it are purposefull\", \"y complicated. The information in the safe is \\nprotected using either software encryption or FIPS 14\", \"0-2 Level 2 validated Hardware Security \\nModules.\\nAccess to the key vault is provided through RBACs,\", \" meaning that not just any user can access the \\ninformation in the vault. Say a web application wish\", \"es to access the database connection string stored \\nin Azure Key Vault. To gain access, applications\", \" need to run using a service principal. Under this \\nassumed role, they can read the secrets from the\", \" safe. There are a number of different security \\nsettings that can further limit the access that an \", \"application has to the vault, so that it can\\u2019t update \\nsecrets but only read them.\\nAccess to the key\", \" vault can be monitored to ensure that only the expected applications are accessing \\nthe vault. The \", \"logs can be integrated back into Azure Monitor, unlocking the ability to set up alerts \\nwhen unexpec\", \"ted conditions are encountered.\\nKubernetes\\nWithin Kubernetes, there\\u2019s a similar service for maintain\", \"ing small pieces of secret information. \\nKubernetes Secrets can be set via the typical kubectl execu\", \"table.\\nCreating a secret is as simple as finding the base64 version of the values to be stored:\\necho\", \" -n 'admin' | base64\\nYWRtaW4=\\necho -n '1f2d1e2e67df' | base64\\nMWYyZDFlMmU2N2Rm\\nThen adding it to a s\", \"ecrets file named secret.yml for example that looks similar to the following \\nexample:\\napiVersion: v\", \"1\\nkind: Secret\\nmetadata:\\n name: mysecret156 CHAPTER 9 | Cloud-native security\\ntype: Opaque\\ndata:\\n us\", \"ername: YWRtaW4=\\n password: MWYyZDFlMmU2N2Rm\\nFinally, this file can be loaded into Kubernetes by run\", \"ning the following command:\\nkubectl apply -f ./secret.yaml\\nThese secrets can then be mounted into vo\", \"lumes or exposed to container processes through \\nenvironment variables. The Twelve-factor app approa\", \"ch to building applications suggests using the \\nlowest common denominator to transmit settings to an\", \" application. Environment variables are the \\nlowest common denominator, because they\\u2019re supported no\", \" matter the operating system or \\napplication.\\nAn alternative to use the built-in Kubernetes secrets \", \"is to access the secrets in Azure Key Vault from \\nwithin Kubernetes. The simplest way to do this is \", \"to assign an RBAC role to the container looking to \\nload secrets. The application can then use the A\", \"zure Key Vault APIs to access the secrets. However, \\nthis approach requires modifications to the cod\", \"e and doesn\\u2019t follow the pattern of using environment \\nvariables. Instead, it\\u2019s possible to inject v\", \"alues into a container. This approach is actually more secure \\nthan using the Kubernetes secrets dir\", \"ectly, as they can be accessed by users on the cluster.\\nEncryption in transit and at rest\\nKeeping da\", \"ta safe is important whether it\\u2019s on disk or transiting between various different services. \\nThe mos\", \"t effective way to keep data from leaking is to encrypt it into a format that can\\u2019t be easily read \\n\", \"by others. Azure supports a wide range of encryption options.\\nIn transit\\nThere are several ways to e\", \"ncrypt traffic on the network in Azure. The access to Azure services is \\ntypically done over connect\", \"ions that use Transport Layer Security (TLS). For instance, all the \\nconnections to the Azure APIs r\", \"equire TLS connections. Equally, connections to endpoints in Azure \\nstorage can be restricted to wor\", \"k only over TLS encrypted connections.\\nTLS is a complicated protocol and simply knowing that the con\", \"nection is using TLS isn\\u2019t sufficient to \\nensure security. For instance, TLS 1.0 is chronically inse\", \"cure, and TLS 1.1 isn\\u2019t much better. Even within \\nthe versions of TLS, there are various settings th\", \"at can make the connections easier to decrypt. The \\nbest course of action is to check and see if the\", \" server connection is using up-to-date and well \\nconfigured protocols.\\nThis check can be done by an \", \"external service such as SSL labs\\u2019 SSL Server Test. A test run against a \\ntypical Azure endpoint, in\", \" this case a service bus endpoint, yields a near perfect score of A.\\nEven services like Azure SQL da\", \"tabases use TLS encryption to keep data hidden. The interesting part \\nabout encrypting the data in t\", \"ransit using TLS is that it isn\\u2019t possible, even for Microsoft, to listen in on \\nthe connection betw\", \"een computers running TLS. This should provide comfort for companies \\nconcerned that their data may \", \"be at risk from Microsoft proper or even a state actor with more \\nresources than the standard attack\", \"er.157 CHAPTER 9 | Cloud-native security\\nFigure 9-5. SSL labs report showing a score of A for a Serv\", \"ice Bus endpoint.\\nWhile this level of encryption isn\\u2019t going to be sufficient for all time, it shoul\", \"d inspire confidence that \\nAzure TLS connections are quite secure. Azure will continue to evolve its\", \" security standards as \\nencryption improves. It\\u2019s nice to know that there\\u2019s somebody watching the se\", \"curity standards and \\nupdating Azure as they improve.\\nAt rest\\nIn any application, there are a number\", \" of places where data rests on the disk. The application code \\nitself is loaded from some storage me\", \"chanism. Most applications also use some kind of a database \\nsuch as SQL Server, Cosmos DB, or even \", \"the amazingly price-efficient Table Storage. These databases \\nall use heavily encrypted storage to e\", \"nsure that nobody other than the applications with proper \\npermissions can read your data. Even the \", \"system operators can\\u2019t read data that has been encrypted. \\nSo customers can remain confident their s\", \"ecret information remains secret.\\nStorage\\nThe underpinning of much of Azure is the Azure Storage eng\", \"ine. Virtual machine disks are mounted \\non top of Azure Storage. Azure Kubernetes Service runs on vi\", \"rtual machines that, themselves, are \\nhosted on Azure Storage. Even serverless technologies, such as\", \" Azure Functions Apps and Azure \\nContainer Instances, run out of disk that is part of Azure Storage.\", \"\\nIf Azure Storage is well encrypted, then it provides for a foundation for most everything else to a\", \"lso \\nbe encrypted. Azure Storage is encrypted with FIPS 140-2 compliant 256-bit AES. This is a well\\u0002\", \"regarded encryption technology having been the subject of extensive academic scrutiny over the last \", \"\\n20 or so years. At present, there\\u2019s no known practical attack that would allow someone without \\nkno\", \"wledge of the key to read data encrypted by AES.\\nBy default, the keys used for encrypting Azure Stor\", \"age are managed by Microsoft. There are extensive \\nprotections in place to ensure to prevent malicio\", \"us access to these keys. However, users with \\nparticular encryption requirements can also provide th\", \"eir own storage keys that are managed in Azure 158 CHAPTER 9 | Cloud-native security\\nKey Vault. Thes\", \"e keys can be revoked at any time, which would effectively render the contents of the \\nStorage accou\", \"nt using them inaccessible.\\nVirtual machines use encrypted storage, but it\\u2019s possible to provide ano\", \"ther layer of encryption by \\nusing technologies like BitLocker on Windows or DM-Crypt on Linux. Thes\", \"e technologies mean that \\neven if the disk image was leaked off of storage, it would remain near imp\", \"ossible to read it.\\nAzure SQL\\nDatabases hosted on Azure SQL use a technology called Transparent Data\", \" Encryption (TDE) to ensure \\ndata remains encrypted. It\\u2019s enabled by default on all newly created SQ\", \"L databases, but must be \\nenabled manually for legacy databases. TDE executes real-time encryption a\", \"nd decryption of not just \\nthe database, but also the backups and transaction logs.\\nThe encryption p\", \"arameters are stored in the master database and, on startup, are read into memory \\nfor the remaining\", \" operations. This means that the master database must remain unencrypted. The \\nactual key is managed\", \" by Microsoft. However, users with exacting security requirements may provide \\ntheir own key in Key \", \"Vault in much the same way as is done for Azure Storage. The Key Vault provides \\nfor such services a\", \"s key rotation and revocation.\\nThe \\u201cTransparent\\u201d part of TDS comes from the fact that there aren\\u2019t c\", \"lient changes needed to use an \\nencrypted database. While this approach provides for good security, \", \"leaking the database password is \\nenough for users to be able to decrypt the data. There\\u2019s another a\", \"pproach that encrypts individual \\ncolumns or tables in a database. Always Encrypted ensures that at \", \"no point the encrypted data \\nappears in plain text inside the database.\\nSetting up this tier of encr\", \"yption requires running through a wizard in SQL Server Management Studio \\nto select the sort of encr\", \"yption and where in Key Vault to store the associated keys.159 CHAPTER 9 | Cloud-native security\\nFig\", \"ure 9-6. Selecting columns in a table to be encrypted using Always Encrypted.\\nClient applications th\", \"at read information from these encrypted columns need to make special \\nallowances to read encrypted \", \"data. Connection strings need to be updated with Column Encryption \\nSetting=Enabled and client crede\", \"ntials must be retrieved from the Key Vault. The SQL Server client \\nmust then be primed with the col\", \"umn encryption keys. Once that is done, the remaining actions use \\nthe standard interfaces to SQL Cl\", \"ient. That is, tools like Dapper and Entity Framework, which are built \\non top of SQL Client, will c\", \"ontinue to work without changes. Always Encrypted may not yet be \\navailable for every SQL Server dri\", \"ver on every language.\\nThe combination of TDE and Always Encrypted, both of which can be used with c\", \"lient-specific keys, \\nensures that even the most exacting encryption requirements are supported.\\nCos\", \"mos DB\\nCosmos DB is the newest database provided by Microsoft in Azure. It has been built from the g\", \"round \\nup with security and cryptography in mind. AES-256bit encryption is standard for all Cosmos D\", \"B 160 CHAPTER 9 | Cloud-native security\\ndatabases and can\\u2019t be disabled. Coupled with the TLS 1.2 re\", \"quirement for communication, the entire \\nstorage solution is encrypted.\\nFigure 9-7. The flow of data\", \" encryption within Cosmos DB.\\nWhile Cosmos DB doesn\\u2019t provide for supplying customer encryption keys\", \", there has been significant \\nwork done by the team to ensure it remains PCI-DSS compliant without t\", \"hat. Cosmos DB also doesn\\u2019t \\nsupport any sort of single column encryption similar to Azure SQL\\u2019s Alw\", \"ays Encrypted yet.\\nKeeping secure\\nAzure has all the tools necessary to release a highly secure produ\", \"ct. However, a chain is only as strong \\nas its weakest link. If the applications deployed on top of \", \"Azure aren\\u2019t developed with a proper \\nsecurity mindset and good security audits, then they become th\", \"e weak link in the chain. There are \\nmany great static analysis tools, encryption libraries, and sec\", \"urity practices that can be used to ensure \\nthat the software installed on Azure is as secure as Azu\", \"re itself. Examples include static analysis tools, \\nencryption libraries, and security practices.161\", \" CHAPTER 10 | DevOps\\nCHAPTER 10\\nDevOps\\nThe favorite mantra of software consultants is to answer \\u201cIt \", \"depends\\u201d to any question posed. It isn\\u2019t \\nbecause software consultants are fond of not taking a posi\", \"tion. It\\u2019s because there\\u2019s no one true \\nanswer to any questions in software. There\\u2019s no absolute rig\", \"ht and wrong, but rather a balance \\nbetween opposites.\\nTake, for instance, the two major schools of \", \"developing web applications: Single Page Applications \\n(SPAs) versus server-side applications. On th\", \"e one hand, the user experience tends to be better with \\nSPAs and the amount of traffic to the web s\", \"erver can be minimized making it possible to host them \\non something as simple as static hosting. On\", \" the other hand, SPAs tend to be slower to develop and \\nmore difficult to test. Which one is the rig\", \"ht choice? Well, it depends on your situation.\\nCloud-native applications aren\\u2019t immune to that same \", \"dichotomy. They have clear advantages in \\nterms of speed of development, stability, and scalability,\", \" but managing them can be quite a bit more \\ndifficult.\\nYears ago, it wasn\\u2019t uncommon for the process\", \" of moving an application from development to \\nproduction to take a month, or even more. Companies r\", \"eleased software on a 6-month or even every \\nyear cadence. One needs to look no further than Microso\", \"ft Windows to get an idea for the cadence of \\nreleases that were acceptable before the ever-green da\", \"ys of Windows 10. Five years passed between \\nWindows XP and Vista, a further three between Vista and\", \" Windows 7.\\nIt\\u2019s now fairly well established that being able to release software rapidly gives fast-\", \"moving companies \\na huge market advantage over their more sloth-like competitors. It\\u2019s for that reas\", \"on that major \\nupdates to Windows 10 are now approximately every six months.\\nThe patterns and practi\", \"ces that enable faster, more reliable releases to deliver value to the business \\nare collectively kn\", \"own as DevOps. They consist of a wide range of ideas spanning the entire software \\ndevelopment life \", \"cycle from specifying an application all the way up to delivering and operating that \\napplication.\\nD\", \"evOps emerged before microservices and it\\u2019s likely that the movement towards smaller, more fit to \\np\", \"urpose services wouldn\\u2019t have been possible without DevOps to make releasing and operating not \\njust\", \" one but many applications in production easier.162 CHAPTER 10 | DevOps\\nFigure 10-1 - DevOps and mic\", \"roservices.\\nThrough good DevOps practices, it\\u2019s possible to realize the advantages of cloud-native a\", \"pplications \\nwithout suffocating under a mountain of work actually operating the applications.\\nThere\", \"\\u2019s no golden hammer when it comes to DevOps. Nobody can sell a complete and all\\u0002encompassing solutio\", \"n for releasing and operating high-quality applications. This is because each \\napplication is wildly\", \" different from all others. However, there are tools that can make DevOps a far less \\ndaunting propo\", \"sition. One of these tools is known as Azure DevOps.\\nAzure DevOps\\nAzure DevOps has a long pedigree. \", \"It can trace its roots back to when Team Foundation Server first \\nmoved online and through the vario\", \"us name changes: Visual Studio Online and Visual Studio Team \\nServices. Through the years, however, \", \"it has become far more than its predecessors.\\nAzure DevOps is divided into five major components:\\nFi\", \"gure 10-2 - Azure DevOps.\\nAzure Repos - Source code management that supports the venerable Team Foun\", \"dation Version \\nControl (TFVC) and the industry favorite Git. Pull requests provide a way to enable \", \"social coding by \\nfostering discussion of changes as they\\u2019re made.163 CHAPTER 10 | DevOps\\nAzure Boar\", \"ds - Provides an issue and work item tracking tool that strives to allow users to pick the \\nworkflow\", \"s that work best for them. It comes with a number of pre-configured templates including \\nones to sup\", \"port SCRUM and Kanban styles of development.\\nAzure Pipelines - A build and release management system\", \" that supports tight integration with Azure. \\nBuilds can be run on various platforms from Windows to\", \" Linux to macOS. Build agents may be \\nprovisioned in the cloud or on-premises.\\nAzure Test Plans - No\", \" QA person will be left behind with the test management and exploratory \\ntesting support offered by \", \"the Test Plans feature.\\nAzure Artifacts - An artifact feed that allows companies to create their own\", \", internal, versions of \\nNuGet, npm, and others. It serves a double purpose of acting as a cache of \", \"upstream packages if \\nthere\\u2019s a failure of a centralized repository.\\nThe top-level organizational un\", \"it in Azure DevOps is known as a Project. Within each project the \\nvarious components, such as Azure\", \" Artifacts, can be turned on and off. Each of these components \\nprovides different advantages for cl\", \"oud-native applications. The three most useful are repositories, \\nboards, and pipelines. If users wa\", \"nt to manage their source code in another repository stack, such as \\nGitHub, but still take advantag\", \"e of Azure Pipelines and other components, that\\u2019s perfectly possible.\\nFortunately, development teams\", \" have many options when selecting a repository. One of them is \\nGitHub.\\nGitHub Actions\\nFounded in 20\", \"09, GitHub is a widely popular web-based repository for hosting projects, \\ndocumentation, and code. \", \"Many large tech companies, such as Apple, Amazon, Google, and \\nmainstream corporations use GitHub. G\", \"itHub uses the open-source, distributed version control system \\nnamed Git as its foundation. On top,\", \" it then adds its own set of features, including defect tracking, \\nfeature and pull requests, tasks \", \"management, and wikis for each code base.\\nAs GitHub evolves, it too is adding DevOps features. For e\", \"xample, GitHub has its own continuous \\nintegration/continuous delivery (CI/CD) pipeline, called GitH\", \"ub Actions. GitHub Actions is a \\ncommunity-powered workflow automation tool. It lets DevOps teams in\", \"tegrate with their existing \\ntooling, mix and match new products, and hook into their software lifec\", \"ycle, including existing CI/CD \\npartners.\\u201d\\nGitHub has over 40 million users, making it the largest h\", \"ost of source code in the world. In October of \\n2018, Microsoft purchased GitHub. Microsoft has pled\", \"ged that GitHub will remain an open platform\\nthat any developer can plug into and extend. It continu\", \"es to operate as an independent company. \\nGitHub offers plans for enterprise, team, professional, an\", \"d free accounts.\\nSource control\\nOrganizing the code for a cloud-native application can be challengin\", \"g. Instead of a single giant \\napplication, the cloud-native applications tend to be made up of a web\", \" of smaller applications that \\ntalk with one another. As with all things in computing, the best arra\", \"ngement of code remains an open 164 CHAPTER 10 | DevOps\\nquestion. There are examples of successful a\", \"pplications using different kinds of layouts, but two \\nvariants seem to have the most popularity.\\nBe\", \"fore getting down into the actual source control itself, it\\u2019s probably worth deciding on how many \\np\", \"rojects are appropriate. Within a single project, there\\u2019s support for multiple repositories, and bui\", \"ld \\npipelines. Boards are a little more complicated, but there too, the tasks can easily be assigned\", \" to \\nmultiple teams within a single project. It\\u2019s possible to support hundreds, even thousands of \\nd\", \"evelopers, out of a single Azure DevOps project. Doing so is likely the best approach as it provides\", \" a \\nsingle place for all developer to work out of and reduces the confusion of finding that one appl\", \"ication \\nwhen developers are unsure in which project in which it resides.\\nSplitting up code for micr\", \"oservices within the Azure DevOps project can be slightly more challenging.\\nFigure 10-3 - One vs. ma\", \"ny repositories.\\nRepository per microservice\\nAt first glance, this approach seems like the most logi\", \"cal approach to splitting up the source code for \\nmicroservices. Each repository can contain the cod\", \"e needed to build the one microservice. The \\nadvantages to this approach are readily visible:\\n1. Ins\", \"tructions for building and maintaining the application can be added to a README file at \\nthe root of\", \" each repository. When flipping through the repositories, it\\u2019s easy to find these \\ninstructions, red\", \"ucing spin-up time for developers.\\n2. Every service is located in a logical place, easily found by k\", \"nowing the name of the service.\\n3. Builds can easily be set up such that they\\u2019re only triggered when\", \" a change is made to the \\nowning repository.\\n4. The number of changes coming into a repository is li\", \"mited to the small number of developers \\nworking on the project.165 CHAPTER 10 | DevOps\\n5. Security \", \"is easy to set up by restricting the repositories to which developers have read and \\nwrite permissio\", \"ns.\\n6. Repository level settings can be changed by the owning team with a minimum of discussion \\nwit\", \"h others.\\nOne of the key ideas behind microservices is that services should be siloed and separated \", \"from each \\nother. When using Domain Driven Design to decide on the boundaries for services the servi\", \"ces act as \\ntransactional boundaries. Database updates shouldn\\u2019t span multiple services. This collec\", \"tion of related \\ndata is referred to as a bounded context. This idea is reflected by the isolation o\", \"f microservice data to \\na database separate and autonomous from the rest of the services. It makes a\", \" great deal of sense to \\ncarry this idea all the way through to the source code.\\nHowever, this appro\", \"ach isn\\u2019t without its issues. One of the more gnarly development problems of our \\ntime is managing d\", \"ependencies. Consider the number of files that make up the average \\nnode_modules directory. A fresh \", \"install of something like create-react-app is likely to bring with it \\nthousands of packages. The qu\", \"estion of how to manage these dependencies is a difficult one.\\nIf a dependency is updated, then down\", \"stream packages must also update this dependency. \\nUnfortunately, that takes development work so, in\", \"variably, the node_modules directory ends up with \\nmultiple versions of a single package, each one a\", \" dependency of some other package that is \\nversioned at a slightly different cadence. When deploying\", \" an application, which version of a \\ndependency should be used? The version that is currently in pro\", \"duction? The version that is currently \\nin Beta but is likely to be in production by the time the co\", \"nsumer makes it to production? Difficult \\nproblems that aren\\u2019t resolved by just using microservices.\", \"\\nThere are libraries that are depended upon by a wide variety of projects. By dividing the microserv\", \"ices \\nup with one in each repository the internal dependencies can best be resolved by using the int\", \"ernal \\nrepository, Azure Artifacts. Builds for libraries will push their latest versions into Azure \", \"Artifacts for \\ninternal consumption. The downstream project must still be manually updated to take a\", \" dependency \\non the newly updated packages.\\nAnother disadvantage presents itself when moving code be\", \"tween services. Although it would be nice \\nto believe that the first division of an application into\", \" microservices is 100% correct, the reality is that \\nrarely we\\u2019re so prescient as to make no service\", \" division mistakes. Thus, functionality and the code that \\ndrives it will need to move from service \", \"to service: repository to repository. When leaping from one \\nrepository to another, the code loses i\", \"ts history. There are many cases, especially in the event of an \\naudit, where having full history on\", \" a piece of code is invaluable.\\nThe final and most important disadvantage is coordinating changes. I\", \"n a true microservices \\napplication, there should be no deployment dependencies between services. It\", \" should be possible to \\ndeploy services A, B, and C in any order as they have loose coupling. In rea\", \"lity, however, there are \\ntimes when it\\u2019s desirable to make a change that crosses multiple repositor\", \"ies at the same time. Some \\nexamples include updating a library to close a security hole or changing\", \" a communication protocol \\nused by all services.\\nTo do a cross-repository change requires a commit t\", \"o each repository be made in succession. Each \\nchange in each repository will need to be pull-reques\", \"ted and reviewed separately. This activity can be \\ndifficult to coordinate.166 CHAPTER 10 | DevOps\\nA\", \"n alternative to using many repositories is to put all the source code together in a giant, all know\", \"ing, \\nsingle repository.\\nSingle repository\\nIn this approach, sometimes referred to as a monoreposito\", \"ry, all the source code for every service is \\nput into the same repository. At first, this approach \", \"seems like a terrible idea likely to make dealing \\nwith source code unwieldy. There are, however, so\", \"me marked advantages to working this way.\\nThe first advantage is that it\\u2019s easier to manage dependen\", \"cies between projects. Instead of relying on \\nsome external artifact feed, projects can directly imp\", \"ort one another. This means that updates are \\ninstant, and conflicting versions are likely to be fou\", \"nd at compile time on the developer\\u2019s workstation. \\nIn effect, shifting some of the integration test\", \"ing left.\\nWhen moving code between projects, it\\u2019s now easier to preserve the history as the files wi\", \"ll be \\ndetected as having been moved rather than being rewritten.\\nAnother advantage is that wide ran\", \"ging changes that cross service boundaries can be made in a \\nsingle commit. This activity reduces th\", \"e overhead of having potentially dozens of changes to review \\nindividually.\\nThere are many tools tha\", \"t can perform static analysis of code to detect insecure programming \\npractices or problematic use o\", \"f APIs. In a multi-repository world, each repository will need to be \\niterated over to find the prob\", \"lems in them. The single repository allows running the analysis all in one \\nplace.\\nThere are also ma\", \"ny disadvantages to the single repository approach. One of the most worrying ones \\nis that having a \", \"single repository raises security concerns. If the contents of a repository are leaked in \\na reposit\", \"ory per service model, the amount of code lost is minimal. With a single repository, \\neverything the\", \" company owns could be lost. There have been many examples in the past of this \\nhappening and derail\", \"ing entire game development efforts. Having multiple repositories exposes less \\nsurface area, which \", \"is a desirable trait in most security practices.\\nThe size of the single repository is likely to beco\", \"me unmanageable rapidly. This presents some \\ninteresting performance implications. It may become nec\", \"essary to use specialized tools such as Virtual \\nFile System for Git, which was originally designed \", \"to improve the experience for developers on the \\nWindows team.\\nFrequently the argument for using a s\", \"ingle repository boils down to an argument that Facebook or \\nGoogle use this method for source code \", \"arrangement. If the approach is good enough for these \\ncompanies, then, surely, it\\u2019s the correct app\", \"roach for all companies. The truth of the matter is that few \\ncompanies operate on anything like the\", \" scale of Facebook or Google. The problems that occur at \\nthose scales are different from those most\", \" developers will face. What is good for the goose may not \\nbe good for the gander.\\nIn the end, eithe\", \"r solution can be used to host the source code for microservices. However, in most \\ncases, the manag\", \"ement, and engineering overhead of operating in a single repository isn\\u2019t worth the \\nmeager advantag\", \"es. Splitting code up over multiple repositories encourages better separation of \\nconcerns and encou\", \"rages autonomy among development teams.167 CHAPTER 10 | DevOps\\nStandard directory structure\\nRegardle\", \"ss of the single versus multiple repositories debate each service will have its own directory. \\nOne \", \"of the best optimizations to allow developers to cross between projects quickly is to maintain a \\nst\", \"andard directory structure.\\nFigure 10-4 - Standard directory structure.\\nWhenever a new project is cr\", \"eated, a template that puts in place the correct structure should be used. \\nThis template can also i\", \"nclude such useful items as a skeleton README file and an azure\\u0002pipelines.yml. In any microservice a\", \"rchitecture, a high degree of variance between projects makes bulk \\noperations against the services \", \"more difficult.\\nThere are many tools that can provide templating for an entire directory, containing\", \" several source \\ncode directories. Yeoman is popular in the JavaScript world and GitHub have recentl\", \"y released \\nRepository Templates, which provide much of the same functionality.\\nTask management\\nMana\", \"ging tasks in any project can be difficult. Up front there are countless questions to be answered \\na\", \"bout the sort of workflows to set up to ensure optimal developer productivity.\\nCloud-native applicat\", \"ions tend to be smaller than traditional software products or at least they\\u2019re \\ndivided into smaller\", \" services. Tracking of issues or tasks related to these services remains as important \\nas with any o\", \"ther software project. Nobody wants to lose track of some work item or explain to a \\ncustomer that t\", \"heir issue wasn\\u2019t properly logged. Boards are configured at the project level but within \\neach proje\", \"ct, areas can be defined. These allow breaking down issues across several components. The \\nadvantage\", \" to keeping all the work for the entire application in one place is that it\\u2019s easy to move work \\nite\", \"ms from one team to another as they\\u2019re understood better.168 CHAPTER 10 | DevOps\\nAzure DevOps comes \", \"with a number of popular templates pre-configured. In the most basic \\nconfiguration, all that is nee\", \"ded to know is what\\u2019s in the backlog, what people are working on, and \\nwhat\\u2019s done. It\\u2019s important t\", \"o have this visibility into the process of building software, so that work \\ncan be prioritized and c\", \"ompleted tasks reported to the customer. Of course, few software projects \\nstick to a process as sim\", \"ple as to do, doing, and done. It doesn\\u2019t take long for people to start adding \\nsteps like QA or Det\", \"ailed Specification to the process.\\nOne of the more important parts of Agile methodologies is self-i\", \"ntrospection at regular intervals. \\nThese reviews are meant to provide insight into what problems th\", \"e team is facing and how they can \\nbe improved. Frequently, this means changing the flow of issues a\", \"nd features through the \\ndevelopment process. So, it\\u2019s perfectly healthy to expand the layouts of th\", \"e boards with additional \\nstages.\\nThe stages in the boards aren\\u2019t the only organizational tool. Depe\", \"nding on the configuration of the \\nboard, there\\u2019s a hierarchy of work items. The most granular item \", \"that can appear on a board is a task. \\nOut of the box a task contains fields for a title, descriptio\", \"n, a priority, an estimate of the amount of \\nwork remaining and the ability to link to other work it\", \"ems or development items (branches, commits, \\npull requests, builds, and so forth). Work items can b\", \"e classified into different areas of the application \\nand different iterations (sprints) to make fin\", \"ding them easier.\\nFigure 10-5 - Task in Azure DevOps.\\nThe description field supports the normal styl\", \"es you\\u2019d expect (bold, italic underscore and strike \\nthrough) and the ability to insert images. This\", \" makes it a powerful tool for use when specifying work \\nor bugs.\\nTasks can be rolled up into feature\", \"s, which define a larger unit of work. Features, in turn, can be rolled \\nup into epics. Classifying \", \"tasks in this hierarchy makes it much easier to understand how close a large \\nfeature is to rolling \", \"out.169 CHAPTER 10 | DevOps\\nFigure 10-6 - Work item in Azure DevOps.\\nThere are different kinds of vi\", \"ews into the issues in Azure Boards. Items that aren\\u2019t yet scheduled \\nappear in the backlog. From th\", \"ere, they can be assigned to a sprint. A sprint is a time box during \\nwhich it\\u2019s expected some quant\", \"ity of work will be completed. This work can include tasks but also the \\nresolution of tickets. Once\", \" there, the entire sprint can be managed from the Sprint board section. This \\nview shows how work is\", \" progressing and includes a burn down chart to give an ever-updating \\nestimate of if the sprint will\", \" be successful.\\nFigure 10-7 - Board in Azure DevOps.\\nBy now, it should be apparent that there\\u2019s a gr\", \"eat deal of power in the Boards in Azure DevOps. For \\ndevelopers, there are easy views of what is be\", \"ing worked on. For project managers views into \\nupcoming work as well as an overview of existing wor\", \"k. For managers, there are plenty of reports \\nabout resourcing and capacity. Unfortunately, there\\u2019s \", \"nothing magical about cloud-native applications \\nthat eliminate the need to track work. But if you m\", \"ust track work, there are a few places where the \\nexperience is better than in Azure DevOps.\\nCI/CD p\", \"ipelines\\nAlmost no change in the software development life cycle has been so revolutionary as the ad\", \"vent of \\ncontinuous integration (CI) and continuous delivery (CD). Building and running automated te\", \"sts \\nagainst the source code of a project as soon as a change is checked in catches mistakes early. \", \"Prior to \\nthe advent of continuous integration builds, it wouldn\\u2019t be uncommon to pull code from the\", \" 170 CHAPTER 10 | DevOps\\nrepository and find that it didn\\u2019t pass tests or couldn\\u2019t even be built. Th\", \"is resulted in tracking down \\nthe source of the breakage.\\nTraditionally shipping software to the pro\", \"duction environment required extensive documentation and \\na list of steps. Each one of these steps n\", \"eeded to be manually completed in a very error prone \\nprocess.\\nFigure 10-8 - Checklist.\\nThe sister o\", \"f continuous integration is continuous delivery in which the freshly built packages are \\ndeployed to\", \" an environment. The manual process can\\u2019t scale to match the speed of development so \\nautomation bec\", \"omes more important. Checklists are replaced by scripts that can execute the same \\ntasks faster and \", \"more accurately than any human.\\nThe environment to which continuous delivery delivers might be a tes\", \"t environment or, as is being \\ndone by many major technology companies, it could be the production e\", \"nvironment. The latter \\nrequires an investment in high-quality tests that can give confidence that a\", \" change isn\\u2019t going to \\nbreak production for users. In the same way that continuous integration caug\", \"ht issues in the code \\nearly continuous delivery catches issues in the deployment process early.\\nThe\", \" importance of automating the build and delivery process is accentuated by cloud-native \\napplication\", \"s. Deployments happen more frequently and to more environments so manually deploying \\nborders on imp\", \"ossible.\\nAzure Builds\\nAzure DevOps provides a set of tools to make continuous integration and deploy\", \"ment easier than \\never. These tools are located under Azure Pipelines. The first of them is Azure Bu\", \"ilds, which is a tool \\nfor running YAML-based build definitions at scale. Users can either bring the\", \"ir own build machines \\n(great for if the build requires a meticulously set up environment) or use a \", \"machine from a constantly \\nrefreshed pool of Azure hosted virtual machines. These hosted build agent\", \"s come pre-installed with a 171 CHAPTER 10 | DevOps\\nwide range of development tools for not just .NE\", \"T development but for everything from Java to \\nPython to iPhone development.\\nDevOps includes a wide \", \"range of out of the box build definitions that can be customized for any build. \\nThe build definitio\", \"ns are defined in a file called azure-pipelines.yml and checked into the repository so \\nthey can be \", \"versioned along with the source code. This makes it much easier to make changes to the \\nbuild pipeli\", \"ne in a branch as the changes can be checked into just that branch. An example azure\\u0002pipelines.yml f\", \"or building an ASP.NET web application on full framework is show in Figure 10-9.\\nname: $(rev:r)\\nvari\", \"ables:\\n version: 9.2.0.$(Build.BuildNumber)\\n solution: Portals.sln\\n artifactName: drop\\n buildPlatfor\", \"m: any cpu\\n buildConfiguration: release\\npool:\\n name: Hosted VisualStudio\\n demands:\\n - msbuild\\n - vis\", \"ualstudio\\n - vstest\\nsteps:\\n- task: NuGetToolInstaller@0\\n displayName: 'Use NuGet 4.4.1'\\n inputs:\\n ve\", \"rsionSpec: 4.4.1\\n- task: NuGetCommand@2\\n displayName: 'NuGet restore'\\n inputs:\\n restoreSolution: '$(\", \"solution)'\\n- task: VSBuild@1\\n displayName: 'Build solution'\\n inputs:\\n solution: '$(solution)'\\n msbui\", \"ldArgs: '-p:DeployOnBuild=true -p:WebPublishMethod=Package -\\np:PackageAsSingleFile=true -p:SkipInval\", \"idConfigurations=true -\\np:PackageLocation=\\\"$(build.artifactstagingdirectory)\\\\\\\\\\\"'\\n platform: '$(build\", \"Platform)'\\n configuration: '$(buildConfiguration)'\\n- task: VSTest@2\\n displayName: 'Test Assemblies'\\n\", \" inputs:\\n testAssemblyVer2: |\\n **\\\\$(buildConfiguration)\\\\**\\\\*test*.dll\\n !**\\\\obj\\\\**\\n !**\\\\*testadapter.\", \"dll\\n platform: '$(buildPlatform)'\\n configuration: '$(buildConfiguration)'\\n- task: CopyFiles@2\\n displ\", \"ayName: 'Copy UI Test Files to: $(build.artifactstagingdirectory)'\\n inputs:172 CHAPTER 10 | DevOps\\n \", \"SourceFolder: UITests\\n TargetFolder: '$(build.artifactstagingdirectory)/uitests'\\n- task: PublishBuil\", \"dArtifacts@1\\n displayName: 'Publish Artifact'\\n inputs:\\n PathtoPublish: '$(build.artifactstagingdirec\", \"tory)'\\n ArtifactName: '$(artifactName)'\\n condition: succeededOrFailed()\\nFigure 10-9 - A sample azure\", \"-pipelines.yml\\nThis build definition uses a number of built-in tasks that make creating builds as si\", \"mple as building a \\nLego set (simpler than the giant Millennium Falcon). For instance, the NuGet tas\", \"k restores NuGet \\npackages, while the VSBuild task calls the Visual Studio build tools to perform th\", \"e actual compilation. \\nThere are hundreds of different tasks available in Azure DevOps, with thousan\", \"ds more that are \\nmaintained by the community. It\\u2019s likely that no matter what build tasks you\\u2019re lo\", \"oking to run, \\nsomebody has built one already.\\nBuilds can be triggered manually, by a check-in, on a\", \" schedule, or by the completion of another build. \\nIn most cases, building on every check-in is desi\", \"rable. Builds can be filtered so that different builds run \\nagainst different parts of the repositor\", \"y or against different branches. This allows for scenarios like \\nrunning fast builds with reduced te\", \"sting on pull requests and running a full regression suite against \\nthe trunk on a nightly basis.\\nTh\", \"e end result of a build is a collection of files known as build artifacts. These artifacts can be pa\", \"ssed \\nalong to the next step in the build process or added to an Azure Artifacts feed, so they can b\", \"e \\nconsumed by other builds.\\nAzure DevOps releases\\nBuilds take care of compiling the software into a\", \" shippable package, but the artifacts still need to be \\npushed out to a testing environment to compl\", \"ete continuous delivery. For this, Azure DevOps uses a \\nseparate tool called Releases. The Releases \", \"tool makes use of the same tasks\\u2019 library that were \\navailable to the Build but introduce a concept \", \"of \\u201cstages\\u201d. A stage is an isolated environment into \\nwhich the package is installed. For instance, \", \"a product might make use of a development, a QA, and a \\nproduction environment. Code is continuously\", \" delivered into the development environment where \\nautomated tests can be run against it. Once those\", \" tests pass the release moves onto the QA \\nenvironment for manual testing. Finally, the code is push\", \"ed to production where it\\u2019s visible to \\neverybody.\\nFigure 10-10 - Release pipeline\\nEach stage in the\", \" build can be automatically triggered by the completion of the previous phase. In \\nmany cases, howev\", \"er, this isn\\u2019t desirable. Moving code into production might require approval from \\nsomebody. The Rel\", \"eases tool supports this by allowing approvers at each step of the release pipeline. \\nRules can be s\", \"et up such that a specific person or group of people must sign off on a release before it 173 CHAPTE\", \"R 10 | DevOps\\nmakes into production. These gates allow for manual quality checks and also for compli\", \"ance with any \\nregulatory requirements related to control what goes into production.\\nEverybody gets \", \"a build pipeline\\nThere\\u2019s no cost to configuring many build pipelines, so it\\u2019s advantageous to have a\", \"t least one build \\npipeline per microservice. Ideally, microservices are independently deployable to\", \" any environment so \\nhaving each one able to be released via its own pipeline without releasing a ma\", \"ss of unrelated code is \\nperfect. Each pipeline can have its own set of approvals allowing for varia\", \"tions in build process for \\neach service.\\nVersioning releases\\nOne drawback to using the Releases fun\", \"ctionality is that it can\\u2019t be defined in a checked-in azure\\u0002pipelines.yml file. There are many reas\", \"ons you might want to do that from having per-branch release \\ndefinitions to including a release ske\", \"leton in your project template. Fortunately, work is ongoing to \\nshift some of the stages support in\", \"to the Build component. This will be known as multi-stage build \\nand the first version is available \", \"now!\\nFeature flags\\nIn chapter 1, we affirmed that cloud native is much about speed and agility. User\", \"s expect rapid \\nresponsiveness, innovative features, and zero downtime. Feature flags are a modern d\", \"eployment \\ntechnique that helps increase agility for cloud-native applications. They enable you to d\", \"eploy new \\nfeatures into a production environment, but restrict their availability. With the flick o\", \"f a switch, you can \\nactivate a new feature for specific users without restarting the app or deployi\", \"ng new code. They \\nseparate the release of new features from their code deployment.\\nFeature flags ar\", \"e built upon conditional logic that control visibility of functionality for users at run \\ntime. In m\", \"odern cloud-native systems, it\\u2019s common to deploy new features into production early, but \\ntest them\", \" with a limited audience. As confidence increases, the feature can be incrementally rolled out \\nto w\", \"ider audiences.\\nOther use cases for feature flags include:\\n\\u2022 Restrict premium functionality to speci\", \"fic customer groups willing to pay higher subscription \\nfees.\\n\\u2022 Stabilize a system by quickly deacti\", \"vating a problem feature, avoiding the risks of a rollback or \\nimmediate hotfix.\\n\\u2022 Disable an option\", \"al feature with high resource consumption during peak usage periods.\\n\\u2022 Conduct experimental feature \", \"releases to small user segments to validate feasibility and \\npopularity.\\nFeature flags also promote \", \"trunk-based development. It\\u2019s a source-control branching model where \\ndevelopers collaborate on feat\", \"ures in a single branch. The approach minimizes the risk and complexity \\nof merging large numbers of\", \" long-running feature branches. Features are unavailable until activated.174 CHAPTER 10 | DevOps\\nImp\", \"lementing feature flags\\nAt its core, a feature flag is a reference to a simple decision object. It r\", \"eturns a Boolean state of on or \\noff. The flag typically wraps a block of code that encapsulates a f\", \"eature capability. The state of the flag \\ndetermines whether that code block executes for a given us\", \"er. Figure 10-11 shows the \\nimplementation.\\nif (featureFlag) {\\n // Run this code block if the featur\", \"eFlag value is true\\n} else {\\n // Run this code block if the featureFlag value is false\\n}\\nFigure 10-1\", \"1 - Simple feature flag implementation.\\nNote how this approach separates the decision logic from the\", \" feature code.\\nIn chapter 1, we discussed the Twelve-Factor App. The guidance recommended keeping co\", \"nfiguration \\nsettings external from application executable code. When needed, settings can be read i\", \"n from the \\nexternal source. Feature flag configuration values should also be independent from their\", \" codebase. By \\nexternalizing flag configuration in a separate repository, you can change flag state \", \"without modifying \\nand redeploying the application.\\nAzure App Configuration provides a centralized r\", \"epository for feature flags. With it, you define \\ndifferent kinds of feature flags and manipulate th\", \"eir states quickly and confidently. You add the App \\nConfiguration client libraries to your applicat\", \"ion to enable feature flag functionality. Various \\nprogramming language frameworks are supported.\\nFe\", \"ature flags can be easily implemented in an ASP.NET Core service. Installing the .NET Feature \\nManag\", \"ement libraries and App Configuration provider enable you to declaratively add feature flags to \\nyou\", \"r code. They enable FeatureGate attributes so that you don\\u2019t have to manually write if statements \\na\", \"cross your codebase.\\nOnce configured in your Startup class, you can add feature flag functionality a\", \"t the controller, action, \\nor middleware level. Figure 10-12 presents controller and action implemen\", \"tation:\\n[FeatureGate(MyFeatureFlags.FeatureA)]\\npublic class ProductController : Controller\\n{\\n ...\\n}\\n\", \"[FeatureGate(MyFeatureFlags.FeatureA)]\\npublic IActionResult UpdateProductStatus()\\n{\\n return ObjectRe\", \"sult(ProductDto);\\n}\\nFigure 10-12 - Feature flag implementation in a controller and action.\\nIf a feat\", \"ure flag is disabled, the user will receive a 404 (Not Found) status code with no response body.\\nFea\", \"ture flags can also be injected directly into C# classes. Figure 10-13 shows feature flag injection:\", \"175 CHAPTER 10 | DevOps\\npublic class ProductController : Controller\\n{\\n private readonly IFeatureMana\", \"ger _featureManager;\\n public ProductController(IFeatureManager featureManager)\\n {\\n _featureManager =\", \" featureManager;\\n }\\n}\\nFigure 10-13 - Feature flag injection into a class.\\nThe Feature Management lib\", \"raries manage the feature flag lifecycle behind the scenes. For example, \\nto minimize high numbers o\", \"f calls to the configuration store, the libraries cache flag states for a \\nspecified duration. They \", \"can guarantee the immutability of flag states during a request call. They also \\noffer a Point-in-tim\", \"e snapshot. You can reconstruct the history of any key-value and provide its past \\nvalue at any mome\", \"nt within the previous seven days.\\nInfrastructure as code\\nCloud-native systems embrace microservices\", \", containers, and modern system design to achieve speed \\nand agility. They provide automated build a\", \"nd release stages to ensure consistent and quality code. \\nBut, that\\u2019s only part of the story. How do\", \" you provision the cloud environments upon which these \\nsystems run?\\nModern cloud-native application\", \"s embrace the widely accepted practice of Infrastructure as Code, or \\nIaC. With IaC, you automate pl\", \"atform provisioning. You essentially apply software engineering \\npractices such as testing and versi\", \"oning to your DevOps practices. Your infrastructure and \\ndeployments are automated, consistent, and \", \"repeatable. Just as continuous delivery automated the \\ntraditional model of manual deployments, Infr\", \"astructure as Code (IaC) is evolving how application \\nenvironments are managed.\\nTools like Azure Res\", \"ource Manager (ARM), Terraform, and the Azure Command Line Interface (CLI) \\nenable you to declarativ\", \"ely script the cloud infrastructure you require.\\nAzure Resource Manager templates\\nARM stands for Azu\", \"re Resource Manager. It\\u2019s an API provisioning engine that is built into Azure and \\nexposed as an API\", \" service. ARM enables you to deploy, update, delete, and manage the resources \\ncontained in Azure re\", \"source group in a single, coordinated operation. You provide the engine with a \\nJSON-based template \", \"that specifies the resources you require and their configuration. ARM \\nautomatically orchestrates th\", \"e deployment in the correct order respecting dependencies. The engine \\nensures idempotency. If a des\", \"ired resource already exists with the same configuration, provisioning \\nwill be ignored.\\nAzure Resou\", \"rce Manager templates are a JSON-based language for defining various resources in \\nAzure. The basic \", \"schema looks something like Figure 10-14.\\n{\\n \\\"$schema\\\": \\\"https://schema.management.azure.com/schemas\", \"/2015-01-176 CHAPTER 10 | DevOps\\n01/deploymentTemplate.json#\\\",\\n \\\"contentVersion\\\": \\\"\\\",\\n \\\"apiProfile\\\":\", \" \\\"\\\",\\n \\\"parameters\\\": { },\\n \\\"variables\\\": { },\\n \\\"functions\\\": [ ],\\n \\\"resources\\\": [ ],\\n \\\"outputs\\\": { }\\n}\\n\", \"Figure 10-14 - The schema for a Resource Manager template\\nWithin this template, one might define a s\", \"torage container inside the resources section like so:\\n\\\"resources\\\": [\\n {\\n \\\"type\\\": \\\"Microsoft.Storage\", \"/storageAccounts\\\",\\n \\\"name\\\": \\\"[variables('storageAccountName')]\\\",\\n \\\"location\\\": \\\"[parameters('location\", \"')]\\\",\\n \\\"apiVersion\\\": \\\"2018-07-01\\\",\\n \\\"sku\\\": {\\n \\\"name\\\": \\\"[parameters('storageAccountType')]\\\"\\n },\\n \\\"kin\", \"d\\\": \\\"StorageV2\\\",\\n \\\"properties\\\": {}\\n }\\n ],\\nFigure 10-15 - An example of a storage account defined in \", \"a Resource Manager template\\nAn ARM template can be parameterized with dynamic environment and config\", \"uration information. \\nDoing so enables it to be reused to define different environments, such as dev\", \"elopment, QA, or \\nproduction. Normally, the template creates all resources within a single Azure res\", \"ource group. It\\u2019s \\npossible to define multiple resource groups in a single Resource Manager template\", \", if needed. You \\ncan delete all resources in an environment by deleting the resource group itself. \", \"Cost analysis can also \\nbe run at the resource group level, allowing for quick accounting of how muc\", \"h each environment is \\ncosting.\\nThere are many examples of ARM templates available in the Azure Quic\", \"kstart Templates project on \\nGitHub. They can help accelerate creating a new template or modifying a\", \"n existing one.\\nResource Manager templates can be run in many of ways. Perhaps the simplest way is t\", \"o simply paste \\nthem into the Azure portal. For experimental deployments, this method can be quick. \", \"They can also be \\nrun as part of a build or release process in Azure DevOps. There are tasks that wi\", \"ll leverage \\nconnections into Azure to run the templates. Changes to Resource Manager templates are \", \"applied \\nincrementally, meaning that to add a new resource requires just adding it to the template. \", \"The tooling \\nwill reconcile differences between the current resources and those defined in the templ\", \"ate. Resources \\nwill then be created or altered so they match what is defined in the template.\\nTerra\", \"form\\nCloud-native applications are often constructed to be cloud agnostic. Being so means the applic\", \"ation \\nisn\\u2019t tightly coupled to a particular cloud vendor and can be deployed to any public cloud.17\", \"7 CHAPTER 10 | DevOps\\nTerraform is a commercial templating tool that can provision cloud-native appl\", \"ications across all the \\nmajor cloud players: Azure, Google Cloud Platform, AWS, and AliCloud. Inste\", \"ad of using JSON as the \\ntemplate definition language, it uses the slightly more terse HCL (Hashicor\", \"p Configuration Language).\\nAn example Terraform file that does the same as the previous Resource Man\", \"ager template (Figure 10-\\n15) is shown in Figure 10-16:\\nprovider \\\"azurerm\\\" {\\n version = \\\"=1.28.0\\\"\\n}\\n\", \"resource \\\"azurerm_resource_group\\\" \\\"testrg\\\" {\\n name = \\\"production\\\"\\n location = \\\"West US\\\"\\n}\\nresource \\\"\", \"azurerm_storage_account\\\" \\\"testsa\\\" {\\n name = \\\"${var.storageAccountName}\\\"\\n resource_group_name = \\\"${az\", \"urerm_resource_group.testrg.name}\\\"\\n location = \\\"${var.region}\\\"\\n account_tier = \\\"${var.tier}\\\"\\n accoun\", \"t_replication_type = \\\"${var.replicationType}\\\"\\n}\\nFigure 10-16 - An example of a Resource Manager temp\", \"late\\nTerraform also provides intuitive error messages for problem templates. There\\u2019s also a handy va\", \"lidate \\ntask that can be used in the build phase to catch template errors early.\\nAs with Resource Ma\", \"nager templates, command-line tools are available to deploy Terraform \\ntemplates. There are also com\", \"munity-created tasks in Azure Pipelines that can validate and apply \\nTerraform templates.\\nSometimes \", \"Terraform and ARM templates output meaningful values, such as a connection string to a \\nnewly create\", \"d database. This information can be captured in the build pipeline and used in \\nsubsequent tasks.\\nAz\", \"ure CLI Scripts and Tasks\\nFinally, you can leverage Azure CLI to declaratively script your cloud inf\", \"rastructure. Azure CLI scripts \\ncan be created, found, and shared to provision and configure almost \", \"any Azure resource. The CLI is \\nsimple to use with a gentle learning curve. Scripts are executed wit\", \"hin either PowerShell or Bash. \\nThey\\u2019re also straightforward to debug, especially when compared with\", \" ARM templates.\\nAzure CLI scripts work well when you need to tear down and redeploy your infrastruct\", \"ure. Updating \\nan existing environment can be tricky. Many CLI commands aren\\u2019t idempotent. That mean\", \"s they\\u2019ll \\nrecreate the resource each time they\\u2019re run, even if the resource already exists. It\\u2019s al\", \"ways possible to \\nadd code that checks for the existence of each resource before creating it. But, d\", \"oing so, your script \\ncan become bloated and difficult to manage.\\nThese scripts can also be embedded\", \" in Azure DevOps pipelines as Azure CLI tasks. Executing the \\npipeline invokes the script.178 CHAPTE\", \"R 10 | DevOps\\nFigure 10-17 shows a YAML snippet that lists the version of Azure CLI and the details \", \"of the \\nsubscription. Note how Azure CLI commands are included in an inline script.\\n- task: AzureCLI\", \"@2\\n displayName: Azure CLI\\n inputs:\\n azureSubscription: <Name of the Azure Resource Manager service \", \"connection>\\n scriptType: ps\\n scriptLocation: inlineScript\\n inlineScript: |\\n az --version\\n az account\", \" show\\nFigure 10-17 - Azure CLI script\\nIn the article, What is Infrastructure as Code, Author Sam Guc\", \"kenheimer describes how, \\u201cTeams who \\nimplement IaC can deliver stable environments rapidly and at sc\", \"ale. Teams avoid manual \\nconfiguration of environments and enforce consistency by representing the d\", \"esired state of their \\nenvironments via code. Infrastructure deployments with IaC are repeatable and\", \" prevent runtime issues \\ncaused by configuration drift or missing dependencies. DevOps teams can wor\", \"k together with a \\nunified set of practices and tools to deliver applications and their supporting i\", \"nfrastructure rapidly, \\nreliably, and at scale.\\u201d\\nCloud Native Application Bundles\\nA key property of \", \"cloud-native applications is that they leverage the capabilities of the cloud to speed \\nup developme\", \"nt. This design often means that a full application uses different kinds of technologies. \\nApplicati\", \"ons may be shipped in Docker containers, some services may use Azure Functions, while \\nother parts m\", \"ay run directly on virtual machines allocated on large metal servers with hardware GPU \\nacceleration\", \". No two cloud-native applications are the same, so it\\u2019s been difficult to provide a single \\nmechani\", \"sm for shipping them.\\nThe Docker containers may run on Kubernetes using a Helm Chart for deployment.\", \" The Azure \\nFunctions may be allocated using Terraform templates. Finally, the virtual machines may \", \"be allocated \\nusing Terraform but built out using Ansible. This is a large variety of technologies a\", \"nd there has been \\nno way to package them all together into a reasonable package. Until now.\\nCloud N\", \"ative Application Bundles (CNABs) are a joint effort by many community-minded companies \\nsuch as Mic\", \"rosoft, Docker, and HashiCorp to develop a specification to package distributed \\napplications.\\nThe e\", \"ffort was announced in December of 2018, so there\\u2019s still a fair bit of work to do to expose the \\nef\", \"fort to the greater community. However, there\\u2019s already an open specification and a reference \\nimple\", \"mentation known as Duffle. This tool, which was written in Go, is a joint effort between Docker \\nand\", \" Microsoft.\\nThe CNABs can contain different kinds of installation technologies. This aspect allows t\", \"hings like Helm \\nCharts, Terraform templates, and Ansible Playbooks to coexist in the same package. \", \"Once built, the \\npackages are self-contained and portable; they can be installed from a USB stick. T\", \"he packages are \\ncryptographically signed to ensure they originate from the party they claim.179 CHA\", \"PTER 10 | DevOps\\nThe core of a CNAB is a file called bundle.json. This file defines the contents of \", \"the bundle, be they \\nTerraform or images or anything else. Figure 11-9 defines a CNAB that invokes s\", \"ome Terraform. \\nNotice, however, that it actually defines an invocation image that is used to invoke\", \" the Terraform. \\nWhen packaged up, the Docker file that is located in the cnab directory is built in\", \"to a Docker image, \\nwhich will be included in the bundle. Having Terraform installed inside a Docker\", \" container in the \\nbundle means that users don\\u2019t need to have Terraform installed on their machine t\", \"o run the bundling.\\n{\\n \\\"name\\\": \\\"terraform\\\",\\n \\\"version\\\": \\\"0.1.0\\\",\\n \\\"schemaVersion\\\": \\\"v1.0.0-WD\\\",\\n \\\"pa\", \"rameters\\\": {\\n \\\"backend\\\": {\\n \\\"type\\\": \\\"boolean\\\",\\n \\\"defaultValue\\\": false,\\n \\\"destination\\\": {\\n \\\"env\\\": \\\"TF\", \"_VAR_backend\\\"\\n }\\n }\\n },\\n \\\"invocationImages\\\": [\\n {\\n \\\"imageType\\\": \\\"docker\\\",\\n \\\"image\\\": \\\"cnab/terraform:\", \"latest\\\"\\n }\\n ],\\n \\\"credentials\\\": {\\n \\\"tenant_id\\\": {\\n \\\"env\\\": \\\"TF_VAR_tenant_id\\\"\\n },\\n \\\"client_id\\\": {\\n \\\"en\", \"v\\\": \\\"TF_VAR_client_id\\\"\\n },\\n \\\"client_secret\\\": {\\n \\\"env\\\": \\\"TF_VAR_client_secret\\\"\\n },\\n \\\"subscription_id\\\"\", \": {\\n \\\"env\\\": \\\"TF_VAR_subscription_id\\\"\\n },\\n \\\"ssh_authorized_key\\\": {\\n \\\"env\\\": \\\"TF_VAR_ssh_authorized_key\", \"\\\"\\n }\\n },\\n \\\"actions\\\": {\\n \\\"status\\\": {\\n \\\"modifies\\\": true\\n }\\n }\\n}\\nFigure 10-18 - An example Terraform fi\", \"le\\nThe bundle.json also defines a set of parameters that are passed down into the Terraform. \\nParame\", \"terization of the bundle allows for installation in various different environments.\\nThe CNAB format \", \"is also flexible, allowing it to be used against any cloud. It can even be used against \\non-premises\", \" solutions such as OpenStack.180 CHAPTER 10 | DevOps\\nDevOps Decisions\\nThere are so many great tools \", \"in the DevOps space these days and even more fantastic books and \\npapers on how to succeed. A favori\", \"te book to get started on the DevOps journey is The Phoenix \\nProject, which follows the transformati\", \"on of a fictional company from NoOps to DevOps. One thing is \\nfor certain: DevOps is no longer a \\u201cni\", \"ce to have\\u201d when deploying complex, Cloud Native Applications. \\nIt\\u2019s a requirement and should be pla\", \"nned for and resourced at the start of any project.\\nReferences\\n\\u2022 Azure DevOps\\n\\u2022 Azure Resource Manag\", \"er\\n\\u2022 Terraform\\n\\u2022 Azure CLI181 CHAPTER 11 | Summary: Architecting cloud-native apps\\nCHAPTER 11\\nSummar\", \"y: Architecting \\ncloud-native apps\\nIn summary, here are important conclusions from this guide:\\n\\u2022 Clo\", \"ud-native is about designing modern applications that embrace rapid change, large scale, \\nand resili\", \"ence, in modern, dynamic environments such as public, private, and hybrid clouds.\\n\\u2022 The Cloud Native\", \" Computing Foundation (CNCF) is an influential open-source consortium \\nof over 300 major corporation\", \"s. It\\u2019s responsible for driving the adoption of cloud-native \\ncomputing across technology and cloud \", \"stacks.\\n\\u2022 CNCF guidelines recommend that cloud-native applications embrace six important pillars as \", \"\\nshown in Figure 11-1:\\nFigure 11-1. Cloud-native foundational pillars\\n\\u2022 These cloud-native pillars i\", \"nclude:\\n\\u2013 The cloud and its underlying service model\\n\\u2013 Modern design principles\\n\\u2013 Microservices\\n\\u2013 Co\", \"ntainerization and container orchestration\\n\\u2013 Cloud-based backing services, such as databases and mes\", \"sage brokers\\n\\u2013 Automation, including Infrastructure as Code and code deployment182 CHAPTER 11 | Summ\", \"ary: Architecting cloud-native apps\\n\\u2022 Kubernetes is the hosting environment of choice for most cloud\", \"-native applications. Smaller, \\nsimple services are sometimes hosted in serverless platforms, such a\", \"s Azure Functions. Among \\nmany key automation features, both environments provide automatic scaling \", \"to handle \\nfluctuating workload volumes.\\n\\u2022 Service communication becomes a significant design decisi\", \"on when constructing a cloud\\u0002native application. Applications typically expose an API gateway to man\", \"age front-end client \\ncommunication. Then backend microservices strive to communicate with each othe\", \"r \\nimplementing asynchronous communication patterns, when possible.\\n\\u2022 gRPC is a modern, high-perform\", \"ance framework that evolves the age-old remote procedure \\ncall (RPC) protocol. Cloud-native applicat\", \"ions often embrace gRPC to streamline messaging \\nbetween back-end services. gRPC uses HTTP/2 for its\", \" transport protocol. It can be up to 8x \\nfaster than JSON serialization with message sizes 60-80% sm\", \"aller. gRPC is open source and \\nmanaged by the Cloud Native Computing Foundation (CNCF).\\n\\u2022 Distribut\", \"ed data is a model often implemented by cloud-native applications. Applications \\nsegregate business \", \"functionality into small, independent microservices. Each microservice \\nencapsulates its own depende\", \"ncies, data, and state. The classic shared database model \\nevolves into one of many smaller database\", \"s, each aligning with a microservice. When the \\nsmoke clears, we emerge with a design that exposes a\", \" database-per-microservice model.\\n\\u2022 No-SQL databases refer to high-performance, non-relational data \", \"stores. They excel in their \\nease-of-use, scalability, resilience, and availability characteristics.\", \" High volume services that \\nrequire sub second response time favor NoSQL datastores. The proliferati\", \"on of NoSQL \\ntechnologies for distributed cloud-native systems can\\u2019t be overstated.\\n\\u2022 NewSQL is an e\", \"merging database technology that combines the distributed scalability of \\nNoSQL and the ACID guarant\", \"ees of a relational database. NewSQL databases target business \\nsystems that must process high-volum\", \"es of data, across distributed environments, with full \\ntransactional/ACID compliance. The Cloud Nat\", \"ive Computing Foundation (CNCF) features \\nseveral NewSQL database projects.\\n\\u2022 Resiliency is the abil\", \"ity of your system to react to failure and still remain functional. Cloud\\u0002native systems embrace dis\", \"tributed architecture where failure is inevitable. Applications must \\nbe constructed to respond eleg\", \"antly to failure and quickly return to a fully functioning state.\\n\\u2022 Service meshes are a configurabl\", \"e infrastructure layer with built-in capabilities to handle \\nservice communication and other cross-c\", \"utting challenges. They decouple cross-cutting \\nresponsibilities from your business code. These resp\", \"onsibilities move into a service proxy. \\nReferred to as the Sidecar pattern, the proxy is deployed i\", \"nto a separate process to provide \\nisolation from your business code.\\n\\u2022 Observability is a key desig\", \"n consideration for cloud-native applications. As services are \\ndistributed across a cluster of node\", \"s, centralized logging, monitoring, and alerts, become \\nmandatory. Azure Monitor is a collection of \", \"cloud-based tools designed to provide visibility \\ninto the state of your system.183 CHAPTER 11 | Sum\", \"mary: Architecting cloud-native apps\\n\\u2022 Infrastructure as Code is a widely accepted practice that aut\", \"omates platform provisioning. \\nYour infrastructure and deployments are automated, consistent, and re\", \"peatable. Tools like \\nAzure Resource Manager, Terraform, and the Azure CLI, enable you to declarativ\", \"ely script the \\ncloud infrastructure you require.\\n\\u2022 Code automation is a requirement for cloud-nativ\", \"e applications. Modern CI/CD systems help \\nfulfill this principle. They provide separate build and d\", \"eployment steps that help ensure \\nconsistent and quality code. The build stage transforms the code i\", \"nto a binary artifact. The \\nrelease stage picks up the binary artifact, applies external environment\", \" configuration, and \\ndeploys it to a specified environment. Azure DevOps and GitHub are full-feature\", \"d DevOps \\nenvironments.\"]"