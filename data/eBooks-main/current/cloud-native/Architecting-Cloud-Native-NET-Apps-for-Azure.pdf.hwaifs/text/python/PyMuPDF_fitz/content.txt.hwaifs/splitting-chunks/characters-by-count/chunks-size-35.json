"[\" \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nEDITION v1.0.3 \\nRefer changelog for the \", \"book updates and community contributions. \\nPUBLISHED BY \\nMicrosoft Developer Division, .NET, and Vis\", \"ual Studio product teams \\nA division of Microsoft Corporation \\nOne Microsoft Way \\nRedmond, Washingto\", \"n 98052-6399 \\nCopyright \\u00a9 2023 by Microsoft Corporation \\nAll rights reserved. No part of the content\", \"s of this book may be reproduced or transmitted in any \\nform or by any means without the written per\", \"mission of the publisher. \\nThis book is provided \\u201cas-is\\u201d and expresses the author\\u2019s views and opinio\", \"ns. The views, opinions, and \\ninformation expressed in this book, including URL and other Internet w\", \"ebsite references, may change \\nwithout notice. \\nSome examples depicted herein are provided for illus\", \"tration only and are fictitious. No real association \\nor connection is intended or should be inferre\", \"d. \\nMicrosoft and the trademarks listed at https://www.microsoft.com on the \\u201cTrademarks\\u201d webpage are\", \" \\ntrademarks of the Microsoft group of companies. \\nMac and macOS are trademarks of Apple Inc. \\nThe D\", \"ocker whale logo is a registered trademark of Docker, Inc. Used by permission. \\nAll other marks and \", \"logos are property of their respective owners. \\nAuthors: \\nRob Vettor, Principal MTC (Microsoft Techn\", \"ology Center) Architect for Cloud App Innovation - \\nthinkingincloudnative.com, Microsoft \\nSteve \\u201card\", \"alis\\u201d Smith, Software Architect and Trainer - Ardalis.com \\nParticipants and Reviewers: \\nCesar De la \", \"Torre, Principal Program Manager, .NET team, Microsoft \\nNish Anil, Senior Program Manager, .NET team\", \", Microsoft \\nJeremy Likness, Senior Program Manager, .NET team, Microsoft \\nCecil Phillip, Senior Clo\", \"ud Advocate, Microsoft \\nSumit Ghosh, Principal Consultant at Neudesic \\nEditors: \\nMaira Wenzel, Progr\", \"am Manager, .NET team, Microsoft \\n \\nDavid Pine, Senior Content Developer, .NET docs, Microsoft \\nVers\", \"ion \\nThis guide has been written to cover .NET 7 version along with many additional updates related \", \"to \\nthe same \\u201cwave\\u201d of technologies (that is, Azure and additional third-party technologies) coincid\", \"ing in \\ntime with the .NET 7 release. \\nWho should use this guide \\nThe audience for this guide is mai\", \"nly developers, development leads, and architects who are \\ninterested in learning how to build appli\", \"cations designed for the cloud. \\nA secondary audience is technical decision-makers who plan to choos\", \"e whether to build their \\napplications using a cloud-native approach. \\nHow you can use this guide \\nT\", \"his guide begins by defining cloud native and introducing a reference application built using cloud-\", \"\\nnative principles and technologies. Beyond these first two chapters, the rest of the book is broken\", \" up \\ninto specific chapters focused on topics common to most cloud-native applications. You can jump\", \" to \\nany of these chapters to learn about cloud-native approaches to: \\n\\u2022 \\nData and data access \\n\\u2022 \\nC\", \"ommunication patterns \\n\\u2022 \\nScaling and scalability \\n\\u2022 \\nApplication resiliency \\n\\u2022 \\nMonitoring and heal\", \"th \\n\\u2022 \\nIdentity and security \\n\\u2022 \\nDevOps \\nThis guide is available both in PDF form and online. Feel f\", \"ree to forward this document or links to its \\nonline version to your team to help ensure common unde\", \"rstanding of these topics. Most of these \\ntopics benefit from a consistent understanding of the unde\", \"rlying principles and patterns, as well as \\nthe trade-offs involved in decisions related to these to\", \"pics. Our goal with this document is to equip \\nteams and their leaders with the information they nee\", \"d to make well-informed decisions for their \\napplications\\u2019 architecture, development, and hosting. \\n\", \" \\ni \\nContents \\n \\nContents \\nIntroduction to cloud-native applications ...............................\", \"............................................. 1 \\nCloud-native computing ............................\", \"....................................................................................................\", \".................. 3 \\nWhat is Cloud Native? ........................................................\", \"....................................................................................................\", \" 4 \\nThe pillars of cloud native ....................................................................\", \"............................................................................ 5 \\nThe cloud ..........\", \"....................................................................................................\", \".................................................................. 5 \\nModern design ................\", \"....................................................................................................\", \"................................................. 6 \\nMicroservices .................................\", \"....................................................................................................\", \"................................... 9 \\nContainers ..................................................\", \"....................................................................................................\", \"..................... 12 \\nBacking services .........................................................\", \"....................................................................................................\", \"... 15 \\nAutomation .................................................................................\", \"........................................................................................ 17 \\nCandida\", \"te apps for cloud native ...........................................................................\", \".......................................................... 19 \\nModernizing legacy apps .............\", \"....................................................................................................\", \"............................. 19 \\nSummary ..........................................................\", \"....................................................................................................\", \"................ 21 \\nIntroducing eShopOnContainers reference app ...................................\", \"............................. 22 \\nFeatures and requirements ........................................\", \"....................................................................................................\", \"... 23 \\nOverview of the code .......................................................................\", \"................................................................................... 25 \\nUnderstandin\", \"g microservices ....................................................................................\", \"....................................................... 27 \\nMapping eShopOnContainers to Azure Servi\", \"ces ................................................................................................\", \"..... 27 \\nContainer orchestration and clustering ...................................................\", \"................................................................ 28 \\nAPI Gateway ...................\", \"....................................................................................................\", \"................................................ 28 \\nData ..........................................\", \"....................................................................................................\", \"......................................... 29 \\nEvent Bus ............................................\", \"....................................................................................................\", \"............................. 30 \\nResiliency .......................................................\", \"....................................................................................................\", \".................. 30 \\nDeploying eShopOnContainers to Azure ........................................\", \"............................................................................ 30 \\nAzure Kubernetes Se\", \"rvice ..............................................................................................\", \"............................................... 30 \\nDeploying to Azure Kubernetes Service using Helm\", \" ......................................................................................... 30 \\nAzure\", \" Functions and Logic Apps (Serverless) .............................................................\", \".......................................... 32 \\nCentralized configuration ...........................\", \"....................................................................................................\", \"................... 33 \\n \\nii \\nContents \\nAzure App Configuration ....................................\", \"....................................................................................................\", \"...... 33 \\nAzure Key Vault .........................................................................\", \"........................................................................................ 34 \\nConfigu\", \"ration in eShop ....................................................................................\", \".............................................................. 34 \\nReferences ......................\", \"....................................................................................................\", \"................................................. 34 \\nScaling cloud-native applications ............\", \"............................................................................ 36 \\nLeveraging containe\", \"rs and orchestrators ...............................................................................\", \"..................................... 36 \\nChallenges with monolithic deployments ...................\", \"........................................................................................... 36 \\nWhat\", \" are the benefits of containers and orchestrators? .................................................\", \"................................. 38 \\nWhat are the scaling benefits? ...............................\", \"....................................................................................................\", \". 40 \\nWhat scenarios are ideal for containers and orchestrators?....................................\", \"....................................... 42 \\nWhen should you avoid using containers and orchestrators\", \"? ....................................................................... 42 \\nDevelopment resources \", \"....................................................................................................\", \"............................................. 42 \\nLeveraging serverless functions ..................\", \"....................................................................................................\", \"................ 46 \\nWhat is serverless? ...........................................................\", \"................................................................................................ 47 \", \"\\nWhat challenges are solved by serverless? .........................................................\", \"................................................... 47 \\nWhat is the difference between a microservic\", \"e and a serverless function? ............................................. 47 \\nWhat scenarios are ap\", \"propriate for serverless? ..........................................................................\", \"......................... 47 \\nWhen should you avoid serverless? ....................................\", \"...................................................................................... 48 \\nCombining\", \" containers and serverless approaches ..............................................................\", \".................................... 49 \\nWhen does it make sense to use containers with serverless? \", \"........................................................................ 49 \\nWhen should you avoid u\", \"sing containers with Azure Functions? ..............................................................\", \".. 49 \\nHow to combine serverless and Docker containers .............................................\", \".............................................. 49 \\nHow to combine serverless and Kubernetes with KED\", \"A .................................................................................. 50 \\nDeploying c\", \"ontainers in Azure .................................................................................\", \"....................................................... 50 \\nAzure Container Registry ...............\", \"....................................................................................................\", \"........................... 50 \\nACR Tasks ..........................................................\", \"....................................................................................................\", \".............. 52 \\nAzure Kubernetes Service ........................................................\", \"..................................................................................... 52 \\nAzure Brid\", \"ge to Kubernetes ...................................................................................\", \"...................................................... 53 \\nScaling containers and serverless applica\", \"tions ..............................................................................................\", \"........... 53 \\nThe simple solution: scaling up ....................................................\", \".............................................................................. 53 \\nScaling out cloud\", \"-native apps .......................................................................................\", \"............................................. 54 \\nOther container deployment options ...............\", \"....................................................................................................\", \"........ 55 \\n \\niii \\nContents \\nWhen does it make sense to deploy to App Service for Containers? .....\", \".................................................... 55 \\nHow to deploy to App Service for Containers\", \" ...................................................................................................\", \"... 55 \\nWhen does it make sense to deploy to Azure Container Instances? ............................\", \".............................. 55 \\nHow to deploy an app to Azure Container Instances ...............\", \"......................................................................... 55 \\nReferences ...........\", \"....................................................................................................\", \"............................................................ 56 \\nCloud-native communication patterns\", \" ............................................................................... 58 \\nCommunication c\", \"onsiderations ......................................................................................\", \"................................................ 58 \\nFront-end client communication ................\", \"....................................................................................................\", \"................ 60 \\nSimple Gateways ...............................................................\", \"............................................................................................... 62 \\n\", \"Azure Application Gateway ..........................................................................\", \"................................................................ 63 \\nAzure API Management ..........\", \"....................................................................................................\", \"................................... 63 \\nReal-time communication ....................................\", \"....................................................................................................\", \".... 66 \\nService-to-service communication ..........................................................\", \"...................................................................... 67 \\nQueries .................\", \"....................................................................................................\", \"............................................................ 68 \\nCommands ..........................\", \"....................................................................................................\", \"............................................ 71 \\nEvents ............................................\", \"....................................................................................................\", \".................................... 74 \\ngRPC ......................................................\", \"....................................................................................................\", \"................................. 80 \\nWhat is gRPC? ................................................\", \"....................................................................................................\", \"............... 80 \\ngRPC Benefits ..................................................................\", \".................................................................................................. 8\", \"0 \\nProtocol Buffers ................................................................................\", \"................................................................................ 81 \\ngRPC support in\", \" .NET ..............................................................................................\", \"....................................................... 81 \\ngRPC usage .............................\", \"....................................................................................................\", \"........................................ 82 \\ngRPC implementation ...................................\", \"....................................................................................................\", \"............. 83 \\nLooking ahead ....................................................................\", \"............................................................................................... 85 \\n\", \"Service Mesh communication infrastructure ..........................................................\", \"................................................... 85 \\nSummary ....................................\", \"....................................................................................................\", \"...................................... 86 \\nCloud-native data patterns ..............................\", \".................................................................... 88 \\nDatabase-per-microservice, \", \"why? ...............................................................................................\", \"................................... 89 \\nCross-service queries ......................................\", \"....................................................................................................\", \"................. 90 \\nDistributed transactions .....................................................\", \"................................................................................................ 91 \", \"\\nHigh volume data ..................................................................................\", \"............................................................................... 93 \\nCQRS ...........\", \"....................................................................................................\", \"...................................................................... 93 \\n \\niv \\nContents \\nEvent sou\", \"rcing ..............................................................................................\", \"..................................................................... 94 \\nRelational vs. NoSQL data \", \"....................................................................................................\", \"............................................. 96 \\nThe CAP theorem ..................................\", \"....................................................................................................\", \"....................... 97 \\nConsiderations for relational vs. NoSQL systems ........................\", \"........................................................................ 99 \\nDatabase as a Service .\", \"....................................................................................................\", \"................................................ 99 \\nAzure relational databases ....................\", \"....................................................................................................\", \"................. 100 \\nAzure SQL Database...........................................................\", \"........................................................................................... 100 \\nOpe\", \"n-source databases in Azure ........................................................................\", \"..................................................... 101 \\nNoSQL data in Azure .....................\", \"....................................................................................................\", \"........................... 102 \\nNewSQL databases ..................................................\", \"....................................................................................................\", \".. 106 \\nData migration to the cloud ................................................................\", \"...................................................................... 108 \\nCaching in a cloud-nativ\", \"e app ..............................................................................................\", \"......................................... 108 \\nWhy? ................................................\", \"....................................................................................................\", \"................................ 108 \\nCaching architecture .........................................\", \"....................................................................................................\", \"........ 109 \\nAzure Cache for Redis ................................................................\", \".................................................................................. 110 \\nElasticsearc\", \"h in a cloud-native app ............................................................................\", \"................................................. 110 \\nSummary .....................................\", \"....................................................................................................\", \"................................... 111 \\nCloud-native resiliency ...................................\", \".................................................................... 113 \\nApplication resiliency pat\", \"terns ..............................................................................................\", \"........................................ 114 \\nCircuit breaker pattern ..............................\", \"....................................................................................................\", \"............... 116 \\nTesting for resiliency ........................................................\", \"............................................................................................. 117 \\nA\", \"zure platform resiliency ...........................................................................\", \"...................................................................... 117 \\nDesign with resiliency .\", \"....................................................................................................\", \".............................................. 118 \\nDesign with redundancy..........................\", \"....................................................................................................\", \"................ 118 \\nDesign for scalability .......................................................\", \".............................................................................................. 120 \\n\", \"Built-in retry in services .........................................................................\", \"...................................................................... 121 \\nResilient communications\", \" ...................................................................................................\", \"............................................. 122 \\nService mesh ....................................\", \"....................................................................................................\", \"............................ 123 \\nIstio and Envoy ..................................................\", \"....................................................................................................\", \".......... 124 \\nIntegration with Azure Kubernetes Services .........................................\", \".............................................................. 125 \\nMonitoring and health ..........\", \".............................................................................................. 126 \\n\", \"Observability patterns .............................................................................\", \".......................................................................... 126 \\n \\nv \\nContents \\nWhen \", \"to use logging .....................................................................................\", \"............................................................... 126 \\nChallenges with detecting and r\", \"esponding to potential app health issues ........................................... 130 \\nChallenges\", \" with reacting to critical problems in cloud-native apps ...........................................\", \"............... 130 \\nLogging with Elastic Stack ....................................................\", \"........................................................................................... 131 \\nEla\", \"stic Stack .........................................................................................\", \"............................................................................. 131 \\nWhat are the adva\", \"ntages of Elastic Stack? ...........................................................................\", \"............................... 132 \\nLogstash ......................................................\", \"....................................................................................................\", \"................... 132 \\nElasticsearch .............................................................\", \"....................................................................................................\", \".... 133 \\nVisualizing information with Kibana web dashboards .......................................\", \"............................................. 133 \\nInstalling Elastic Stack on Azure ...............\", \"....................................................................................................\", \"............ 134 \\nReferences .......................................................................\", \".................................................................................................. 1\", \"34 \\nMonitoring in Azure Kubernetes Services ........................................................\", \"......................................................... 134 \\nAzure Monitor for Containers ........\", \"....................................................................................................\", \"....................... 134 \\nLog.Finalize() ........................................................\", \"....................................................................................................\", \"........ 136 \\nAzure Monitor ........................................................................\", \".............................................................................................. 136 \\n\", \"Gathering logs and metrics .........................................................................\", \"............................................................... 137 \\nReporting data ................\", \"....................................................................................................\", \"............................................ 137 \\nDashboards .......................................\", \"....................................................................................................\", \"............................ 138 \\nAlerts ...........................................................\", \"....................................................................................................\", \".................... 140 \\nReferences ...............................................................\", \"....................................................................................................\", \"...... 141 \\nCloud-native identity ..................................................................\", \"........................................ 142 \\nReferences ...........................................\", \"....................................................................................................\", \".............................. 142 \\nAuthentication and authorization in cloud-native apps ..........\", \"........................................................................... 142 \\nReferences ........\", \"....................................................................................................\", \"............................................................. 143 \\nAzure Active Directory ..........\", \"....................................................................................................\", \"........................................ 143 \\nReferences ...........................................\", \"....................................................................................................\", \".......................... 143 \\nIdentityServer for cloud-native applications .......................\", \"..................................................................................... 144 \\nCommon we\", \"b app scenarios ....................................................................................\", \"................................................. 144 \\nGetting started .............................\", \"....................................................................................................\", \"............................... 145 \\nConfiguration .................................................\", \"....................................................................................................\", \".............. 145 \\nJavaScript clients .............................................................\", \"............................................................................................... 146 \", \"\\nReferences ........................................................................................\", \"................................................................................. 146 \\n \\nvi \\nContent\", \"s \\nCloud-native security ...........................................................................\", \"............................... 147 \\nAzure security for cloud-native apps ..........................\", \"................................................................................................ 147\", \" \\nThreat modeling ..................................................................................\", \"........................................................................... 148 \\nPrinciple of least \", \"privilege ..........................................................................................\", \"................................................. 148 \\nPenetration testing .........................\", \"....................................................................................................\", \"........................... 149 \\nMonitoring ........................................................\", \"....................................................................................................\", \"............ 149 \\nSecuring the build ...............................................................\", \"........................................................................................... 149 \\nBui\", \"lding secure code ..................................................................................\", \".................................................................. 150 \\nBuilt-in security ..........\", \"....................................................................................................\", \"................................................. 150 \\nAzure network infrastructure ................\", \"....................................................................................................\", \"................. 150 \\nRole-based access control for restricting access to Azure resources..........\", \".............................................. 152 \\nSecurity Principals ............................\", \"....................................................................................................\", \".......................... 152 \\nRoles ..............................................................\", \"....................................................................................................\", \".................. 153 \\nScopes .....................................................................\", \"....................................................................................................\", \"....... 154 \\nDeny ..................................................................................\", \".................................................................................................. 1\", \"54 \\nChecking access ................................................................................\", \".............................................................................. 154 \\nSecuring secrets\", \" ...................................................................................................\", \"........................................................... 155 \\nAzure Key Vault ...................\", \"....................................................................................................\", \"........................................ 155 \\nKubernetes ...........................................\", \"....................................................................................................\", \"......................... 155 \\nEncryption in transit and at rest ...................................\", \"............................................................................................ 156 \\nKe\", \"eping secure .......................................................................................\", \"......................................................................... 160 \\nDevOps ..............\", \"....................................................................................................\", \"............... 161 \\nAzure DevOps ..................................................................\", \"....................................................................................................\", \". 162 \\nGitHub Actions ..............................................................................\", \"....................................................................................... 163 \\nSource \", \"control ............................................................................................\", \".......................................................................... 163 \\nRepository per micro\", \"service ............................................................................................\", \".......................................... 164 \\nSingle repository ..................................\", \"....................................................................................................\", \"...................... 166 \\nStandard directory structure ...........................................\", \"........................................................................................... 167 \\nTas\", \"k management .......................................................................................\", \"....................................................................... 167 \\nCI/CD pipelines .......\", \"....................................................................................................\", \"......................................................... 169 \\nAzure Builds ........................\", \"....................................................................................................\", \".......................................... 170 \\nAzure DevOps releases ..............................\", \"....................................................................................................\", \".............. 172 \\n \\nvii \\nContents \\nEverybody gets a build pipeline ...............................\", \"................................................................................................ 173\", \" \\nVersioning releases ..............................................................................\", \".......................................................................... 173 \\nFeature flags ......\", \"....................................................................................................\", \"............................................................... 173 \\nImplementing feature flags ....\", \"....................................................................................................\", \"................................ 174 \\nInfrastructure as code .......................................\", \"....................................................................................................\", \"............ 175 \\nAzure Resource Manager templates .................................................\", \"..................................................................... 175 \\nTerraform ...............\", \"....................................................................................................\", \"........................................................ 176 \\nAzure CLI Scripts and Tasks...........\", \"....................................................................................................\", \"......................... 177 \\nCloud Native Application Bundles ....................................\", \"........................................................................................... 178 \\nDev\", \"Ops Decisions ......................................................................................\", \".................................................................... 180 \\nReferences ...............\", \"....................................................................................................\", \"...................................................... 180 \\nSummary: Architecting cloud-native apps \", \"....................................................................... 181 \\n \\n1 \\nCHAPTER 1 | Introd\", \"uction to cloud-native applications \\n \\nCHAPTER 1 \\nIntroduction to cloud-\\nnative applications \\nAnothe\", \"r day, at the office, working on \\u201cthe next big thing.\\u201d \\nYour cellphone rings. It\\u2019s your friendly rec\", \"ruiter - the one who calls daily with exciting new \\nopportunities. \\nBut this time it\\u2019s different: St\", \"art-up, equity, and plenty of funding. \\nThe mention of the cloud, microservices, and cutting-edge te\", \"chnology pushes you over the edge. \\nFast forward a few weeks and you\\u2019re now a new employee in a desi\", \"gn session architecting a major \\neCommerce application. You\\u2019re going to compete with the leading eCo\", \"mmerce sites. \\nHow will you build it? \\nIf you follow the guidance from past 15 years, you\\u2019ll most li\", \"kely build the system shown in Figure 1.1. \\n \\nFigure 1-1. Traditional monolithic design \\nYou constru\", \"ct a large core application containing all of your domain logic. It includes modules such as \\nIdenti\", \"ty, Catalog, Ordering, and more. They directly communicate with each other within a single \\nserver p\", \"rocess. The modules share a large relational database. The core exposes functionality via an \\nHTML i\", \"nterface and a mobile app. \\nCongratulations! You just created a monolithic application. \\nNot all is \", \"bad. Monoliths offer some distinct advantages. For example, they\\u2019re straightforward to\\u2026 \\n \\n2 \\nCHAPTE\", \"R 1 | Introduction to cloud-native applications \\n \\n\\u2022 \\nbuild \\n\\u2022 \\ntest \\n\\u2022 \\ndeploy \\n\\u2022 \\ntroubleshoot \\n\\u2022 \", \"\\nvertically scale \\nMany successful apps that exist today were created as monoliths. The app is a hit\", \" and continues to \\nevolve, iteration after iteration, adding more functionality. \\nAt some point, how\", \"ever, you begin to feel uncomfortable. You find yourself losing control of the \\napplication. As time\", \" goes on, the feeling becomes more intense, and you eventually enter a state \\nknown as the Fear Cycl\", \"e: \\n\\u2022 \\nThe app has become so overwhelmingly complicated that no single person understands it. \\n\\u2022 \\nYo\", \"u fear making changes - each change has unintended and costly side effects. \\n\\u2022 \\nNew features/fixes b\", \"ecome tricky, time-consuming, and expensive to implement. \\n\\u2022 \\nEach release becomes as small as possi\", \"ble and requires a full deployment of the entire \\napplication. \\n\\u2022 \\nOne unstable component can crash \", \"the entire system. \\n\\u2022 \\nNew technologies and frameworks aren\\u2019t an option. \\n\\u2022 \\nIt\\u2019s difficult to imple\", \"ment agile delivery methodologies. \\n\\u2022 \\nArchitectural erosion sets in as the code base deteriorates w\", \"ith never-ending \\u201cquick fixes.\\u201d \\n\\u2022 \\nFinally, the consultants come in and tell you to rewrite it. \\nSo\", \"und familiar? \\nMany organizations have addressed this monolithic fear cycle by adopting a cloud-nati\", \"ve approach to \\nbuilding systems. Figure 1-2 shows the same system built applying cloud-native techn\", \"iques and \\npractices. \\n \\n3 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\n \\nFigure 1-2. C\", \"loud-native design \\nNote how the application is decomposed across a set of small isolated microservi\", \"ces. Each service is \\nself-contained and encapsulates its own code, data, and dependencies. Each is \", \"deployed in a software \\ncontainer and managed by a container orchestrator. Instead of a large relati\", \"onal database, each \\nservice owns it own datastore, the type of which vary based upon the data needs\", \". Note how some \\nservices depend on a relational database, but other on NoSQL databases. One service\", \" stores its state \\nin a distributed cache. Note how all traffic routes through an API Gateway servic\", \"e that is responsible \\nfor routing traffic to the core back-end services and enforcing many cross-cu\", \"tting concerns. Most \\nimportantly, the application takes full advantage of the scalability, availabi\", \"lity, and resiliency features \\nfound in modern cloud platforms. \\nCloud-native computing \\nHmm\\u2026 We jus\", \"t used the term, Cloud Native. Your first thought might be, \\u201cWhat exactly does that \\nmean?\\u201d Another \", \"industry buzzword concocted by software vendors to market more stuff?\\u201d \\nFortunately it\\u2019s far differe\", \"nt, and hopefully this book will help convince you. \\nWithin a short time, cloud native has become a \", \"driving trend in the software industry. It\\u2019s a new way \\nto construct large, complex systems. The app\", \"roach takes full advantage of modern software \\ndevelopment practices, technologies, and cloud infras\", \"tructure. Cloud native changes the way you \\ndesign, implement, deploy, and operationalize systems. \\n\", \"Unlike the continuous hype that drives our industry, cloud native is for-real. Consider the Cloud Na\", \"tive \\nComputing Foundation (CNCF), a consortium of over 400 major corporations. Its charter is to ma\", \"ke \\n \\n4 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\ncloud-native computing ubiquitous \", \"across technology and cloud stacks. As one of the most influential \\nopen-source groups, it hosts man\", \"y of the fastest-growing open source-projects in GitHub. These \\nprojects include Kubernetes, Prometh\", \"eus, Helm, Envoy, and gRPC. \\nThe CNCF fosters an ecosystem of open-source and vendor-neutrality. Fol\", \"lowing that lead, this book \\npresents cloud-native principles, patterns, and best practices that are\", \" technology agnostic. At the \\nsame time, we discuss the services and infrastructure available in the\", \" Microsoft Azure cloud for \\nconstructing cloud-native systems. \\nSo, what exactly is Cloud Native? Si\", \"t back, relax, and let us help you explore this new world. \\nWhat is Cloud Native? \\nStop what you\\u2019re \", \"doing and ask your colleagues to define the term \\u201cCloud Native\\u201d. There\\u2019s a good \\nchance you\\u2019ll get s\", \"everal different answers. \\nLet\\u2019s start with a simple definition: \\nCloud-native architecture and tech\", \"nologies are an approach to designing, constructing, and operating \\nworkloads that are built in the \", \"cloud and take full advantage of the cloud computing model. \\nThe Cloud Native Computing Foundation p\", \"rovides the official definition: \\nCloud-native technologies empower organizations to build and run s\", \"calable applications in modern, \\ndynamic environments such as public, private, and hybrid clouds. Co\", \"ntainers, service meshes, \\nmicroservices, immutable infrastructure, and declarative APIs exemplify t\", \"his approach. \\nThese techniques enable loosely coupled systems that are resilient, manageable, and o\", \"bservable. \\nCombined with robust automation, they allow engineers to make high-impact changes freque\", \"ntly and \\npredictably with minimal toil. \\nCloud native is about speed and agility. Business systems \", \"are evolving from enabling business \\ncapabilities to weapons of strategic transformation that accele\", \"rate business velocity and growth. It\\u2019s \\nimperative to get new ideas to market immediately. \\nAt the \", \"same time, business systems have also become increasingly complex with users demanding \\nmore. They e\", \"xpect rapid responsiveness, innovative features, and zero downtime. Performance \\nproblems, recurring\", \" errors, and the inability to move fast are no longer acceptable. Your users will visit \\nyour compet\", \"itor. Cloud-native systems are designed to embrace rapid change, large scale, and \\nresilience. \\nHere\", \" are some companies who have implemented cloud-native techniques. Think about the speed, \\nagility, a\", \"nd scalability they\\u2019ve achieved. \\nCompany \\nExperience \\nNetflix \\nHas 600+ services in production. Dep\", \"loys 100 \\ntimes per day. \\nUber \\nHas 1,000+ services in production. Deploys \\nseveral thousand times e\", \"ach week. \\n \\n5 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\nCompany \\nExperience \\nWeChat\", \" \\nHas 3,000+ services in production. Deploys \\n1,000 times a day. \\nAs you can see, Netflix, Uber, and\", \", WeChat expose cloud-native systems that consist of many \\nindependent services. This architectural \", \"style enables them to rapidly respond to market conditions. \\nThey instantaneously update small areas\", \" of a live, complex application, without a full redeployment. \\nThey individually scale services as n\", \"eeded. \\nThe pillars of cloud native \\nThe speed and agility of cloud native derive from many factors.\", \" Foremost is cloud infrastructure. But \\nthere\\u2019s more: Five other foundational pillars shown in Figur\", \"e 1-3 also provide the bedrock for cloud-\\nnative systems. \\n \\nFigure 1-3. Cloud-native foundational p\", \"illars \\nLet\\u2019s take some time to better understand the significance of each pillar. \\nThe cloud \\nCloud\", \"-native systems take full advantage of the cloud service model. \\nDesigned to thrive in a dynamic, vi\", \"rtualized cloud environment, these systems make extensive use of \\nPlatform as a Service (PaaS) compu\", \"te infrastructure and managed services. They treat the underlying \\ninfrastructure as disposable - pr\", \"ovisioned in minutes and resized, scaled, or destroyed on demand \\u2013 \\nvia automation. \\nConsider the wi\", \"dely accepted DevOps concept of Pets vs. Cattle. In a traditional data center, servers \\nare treated \", \"as Pets: a physical machine, given a meaningful name, and cared for. You scale by adding \\nmore resou\", \"rces to the same machine (scaling up). If the server becomes sick, you nurse it back to \\nhealth. Sho\", \"uld the server become unavailable, everyone notices. \\nThe Cattle service model is different. You pro\", \"vision each instance as a virtual machine or container. \\nThey\\u2019re identical and assigned a system ide\", \"ntifier such as Service-01, Service-02, and so on. You scale \\nby creating more of them (scaling out)\", \". When one becomes unavailable, nobody notices. \\n \\n6 \\nCHAPTER 1 | Introduction to cloud-native appli\", \"cations \\n \\nThe cattle model embraces immutable infrastructure. Servers aren\\u2019t repaired or modified. \", \"If one fails or \\nrequires updating, it\\u2019s destroyed and a new one is provisioned \\u2013 all done via autom\", \"ation. \\nCloud-native systems embrace the Cattle service model. They continue to run as the infrastru\", \"cture \\nscales in or out with no regard to the machines upon which they\\u2019re running. \\nThe Azure cloud \", \"platform supports this type of highly elastic infrastructure with automatic scaling, \\nself-healing, \", \"and monitoring capabilities. \\nModern design \\nHow would you design a cloud-native app? What would you\", \"r architecture look like? To what \\nprinciples, patterns, and best practices would you adhere? What i\", \"nfrastructure and operational \\nconcerns would be important? \\nThe Twelve-Factor Application \\nA widely\", \" accepted methodology for constructing cloud-based applications is the Twelve-Factor \\nApplication. I\", \"t describes a set of principles and practices that developers follow to construct \\napplications opti\", \"mized for modern cloud environments. Special attention is given to portability across \\nenvironments \", \"and declarative automation. \\nWhile applicable to any web-based application, many practitioners consi\", \"der Twelve-Factor a solid \\nfoundation for building cloud-native apps. Systems built upon these princ\", \"iples can deploy and scale \\nrapidly and add features to react quickly to market changes. \\nThe follow\", \"ing table highlights the Twelve-Factor methodology: \\nFactor \\nExplanation \\n1 - Code Base \\nA single co\", \"de base for each microservice, stored \\nin its own repository. Tracked with version \\ncontrol, it can \", \"deploy to multiple environments \\n(QA, Staging, Production). \\n2 - Dependencies \\nEach microservice iso\", \"lates and packages its own \\ndependencies, embracing changes without \\nimpacting the entire system. \\n3\", \" - Configurations \\nConfiguration information is moved out of the \\nmicroservice and externalized thro\", \"ugh a \\nconfiguration management tool outside of the \\ncode. The same deployment can propagate \\nacross\", \" environments with the correct \\nconfiguration applied. \\n4 - Backing Services \\nAncillary resources (d\", \"ata stores, caches, \\nmessage brokers) should be exposed via an \\naddressable URL. Doing so decouples \", \"the \\nresource from the application, enabling it to be \\ninterchangeable. \\n \\n7 \\nCHAPTER 1 | Introducti\", \"on to cloud-native applications \\n \\nFactor \\nExplanation \\n5 - Build, Release, Run \\nEach release must e\", \"nforce a strict separation \\nacross the build, release, and run stages. Each \\nshould be tagged with a\", \" unique ID and support \\nthe ability to roll back. Modern CI/CD systems \\nhelp fulfill this principle.\", \" \\n6 - Processes \\nEach microservice should execute in its own \\nprocess, isolated from other running s\", \"ervices. \\nExternalize required state to a backing service \\nsuch as a distributed cache or data store\", \". \\n7 - Port Binding \\nEach microservice should be self-contained with \\nits interfaces and functionali\", \"ty exposed on its \\nown port. Doing so provides isolation from \\nother microservices. \\n8 - Concurrency\", \" \\nWhen capacity needs to increase, scale out \\nservices horizontally across multiple identical \\nproce\", \"sses (copies) as opposed to scaling-up a \\nsingle large instance on the most powerful \\nmachine availa\", \"ble. Develop the application to be \\nconcurrent making scaling out in cloud \\nenvironments seamless. \\n\", \"9 - Disposability \\nService instances should be disposable. Favor \\nfast startup to increase scalabili\", \"ty opportunities \\nand graceful shutdowns to leave the system in a \\ncorrect state. Docker containers \", \"along with an \\norchestrator inherently satisfy this requirement. \\n10 - Dev/Prod Parity \\nKeep environ\", \"ments across the application \\nlifecycle as similar as possible, avoiding costly \\nshortcuts. Here, th\", \"e adoption of containers can \\ngreatly contribute by promoting the same \\nexecution environment. \\n11 -\", \" Logging \\nTreat logs generated by microservices as event \\nstreams. Process them with an event \\naggre\", \"gator. Propagate log data to data-\\nmining/log management tools like Azure \\nMonitor or Splunk and eve\", \"ntually to long-term \\narchival. \\n12 - Admin Processes \\nRun administrative/management tasks, such as \", \"\\ndata cleanup or computing analytics, as one-off \\nprocesses. Use independent tools to invoke \\nthese \", \"tasks from the production environment, \\nbut separately from the application. \\n \\n8 \\nCHAPTER 1 | Intro\", \"duction to cloud-native applications \\n \\nIn the book, Beyond the Twelve-Factor App, author Kevin Hoff\", \"man details each of the original 12 \\nfactors (written in 2011). Additionally, he discusses three ext\", \"ra factors that reflect today\\u2019s modern \\ncloud application design. \\nNew Factor \\nExplanation \\n13 - API\", \" First \\nMake everything a service. Assume your code \\nwill be consumed by a front-end client, gateway\", \", \\nor another service. \\n14 - Telemetry \\nOn a workstation, you have deep visibility into \\nyour applic\", \"ation and its behavior. In the cloud, \\nyou don\\u2019t. Make sure your design includes the \\ncollection of \", \"monitoring, domain-specific, and \\nhealth/system data. \\n15 - Authentication/ Authorization \\nImplement\", \" identity from the start. Consider \\nRBAC (role-based access control) features \\navailable in public c\", \"louds. \\nWe\\u2019ll refer to many of the 12+ factors in this chapter and throughout the book. \\nAzure Well-\", \"Architected Framework \\nDesigning and deploying cloud-based workloads can be challenging, especially \", \"when implementing \\ncloud-native architecture. Microsoft provides industry standard best practices to\", \" help you and your \\nteam deliver robust cloud solutions. \\nThe Microsoft Well-Architected Framework p\", \"rovides a set of guiding tenets that can be used to \\nimprove the quality of a cloud-native workload.\", \" The framework consists of five pillars of architecture \\nexcellence: \\nTenets \\nDescription \\nCost mana\", \"gement \\nFocus on generating incremental value early. \\nApply Build-Measure-Learn principles to \\naccel\", \"erate time to market while avoiding \\ncapital-intensive solutions. Using a pay-as-you-\\ngo strategy, i\", \"nvest as you scale out, rather than \\ndelivering a large investment up front. \\nOperational excellence\", \" \\nAutomate the environment and operations to \\nincrease speed and reduce human error. Roll \\nproblem u\", \"pdates back or forward quickly. \\nImplement monitoring and diagnostics from the \\nstart. \\nPerformance \", \"efficiency \\nEfficiently meet demands placed on your \\nworkloads. Favor horizontal scaling (scaling ou\", \"t) \\nand design it into your systems. Continually \\nconduct performance and load testing to \\nidentify \", \"potential bottlenecks. \\n \\n9 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\nTenets \\nDescri\", \"ption \\nReliability \\nBuild workloads that are both resilient and \\navailable. Resiliency enables workl\", \"oads to \\nrecover from failures and continue functioning. \\nAvailability ensures users access to your \", \"\\nworkload at all times. Design applications to \\nexpect failures and recover from them. \\nSecurity \\nIm\", \"plement security across the entire lifecycle of \\nan application, from design and implementation \\nto \", \"deployment and operations. Pay close \\nattention to identity management, infrastructure \\naccess, appl\", \"ication security, and data \\nsovereignty and encryption. \\nTo get started, Microsoft provides a set of\", \" online assessments to help you assess your current cloud \\nworkloads against the five well-architect\", \"ed pillars. \\nMicroservices \\nCloud-native systems embrace microservices, a popular architectural styl\", \"e for constructing modern \\napplications. \\nBuilt as a distributed set of small, independent services \", \"that interact through a shared fabric, \\nmicroservices share the following characteristics: \\n\\u2022 \\nEach \", \"implements a specific business capability within a larger domain context. \\n\\u2022 \\nEach is developed auto\", \"nomously and can be deployed independently. \\n\\u2022 \\nEach is self-contained encapsulating its own data st\", \"orage technology, dependencies, and \\nprogramming platform. \\n\\u2022 \\nEach runs in its own process and comm\", \"unicates with others using standard communication \\nprotocols such as HTTP/HTTPS, gRPC, WebSockets, o\", \"r AMQP. \\n\\u2022 \\nThey compose together to form an application. \\nFigure 1-4 contrasts a monolithic applica\", \"tion approach with a microservices approach. Note how the \\nmonolith is composed of a layered archite\", \"cture, which executes in a single process. It typically \\nconsumes a relational database. The microse\", \"rvice approach, however, segregates functionality into \\nindependent services, each with its own logi\", \"c, state, and data. Each microservice hosts its own \\ndatastore. \\n \\n10 \\nCHAPTER 1 | Introduction to c\", \"loud-native applications \\n \\n \\nFigure 1-4. Monolithic versus microservices architecture \\nNote how mic\", \"roservices promote the Processes principle from the Twelve-Factor Application, \\ndiscussed earlier in\", \" the chapter. \\nFactor #6 specifies \\u201cEach microservice should execute in its own process, isolated fr\", \"om other running \\nservices.\\u201d \\nWhy microservices? \\nMicroservices provide agility. \\nEarlier in the cha\", \"pter, we compared an eCommerce application built as a monolith to that with \\nmicroservices. In the e\", \"xample, we saw some clear benefits: \\n\\u2022 \\nEach microservice has an autonomous lifecycle and can evolve\", \" independently and deploy \\nfrequently. You don\\u2019t have to wait for a quarterly release to deploy a ne\", \"w feature or update. \\nYou can update a small area of a live application with less risk of disrupting\", \" the entire system. \\nThe update can be made without a full redeployment of the application. \\n\\u2022 \\nEach\", \" microservice can scale independently. Instead of scaling the entire application as a single \\nunit, \", \"you scale out only those services that require more processing power to meet desired \\nperformance le\", \"vels and service-level agreements. Fine-grained scaling provides for greater \\ncontrol of your system\", \" and helps reduce overall costs as you scale portions of your system, \\nnot everything. \\nAn excellent\", \" reference guide for understanding microservices is .NET Microservices: Architecture for \\nContaineri\", \"zed .NET Applications. The book deep dives into microservices design and architecture. It\\u2019s \\na compa\", \"nion for a full-stack microservice reference architecture available as a free download from \\nMicroso\", \"ft. \\nDeveloping microservices \\nMicroservices can be created upon any modern development platform. \\n \", \"\\n11 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\nThe Microsoft .NET platform is an exce\", \"llent choice. Free and open source, it has many built-in features \\nthat simplify microservice develo\", \"pment. .NET is cross-platform. Applications can be built and run on \\nWindows, macOS, and most flavor\", \"s of Linux. \\n.NET is highly performant and has scored well in comparison to Node.js and other compet\", \"ing \\nplatforms. Interestingly, TechEmpower conducted an extensive set of performance benchmarks acro\", \"ss \\nmany web application platforms and frameworks. .NET scored in the top 10 - well above Node.js an\", \"d \\nother competing platforms. \\n.NET is maintained by Microsoft and the .NET community on GitHub. \\nMi\", \"croservice challenges \\nWhile distributed cloud-native microservices can provide immense agility and \", \"speed, they present \\nmany challenges: \\nCommunication \\nHow will front-end client applications communi\", \"cate with backed-end core microservices? Will you \\nallow direct communication? Or, might you abstrac\", \"t the back-end microservices with a gateway \\nfacade that provides flexibility, control, and security\", \"? \\nHow will back-end core microservices communicate with each other? Will you allow direct HTTP call\", \"s \\nthat can increase coupling and impact performance and agility? Or might you consider decoupled \\nm\", \"essaging with queue and topic technologies? \\nCommunication is covered in the Cloud-native communicat\", \"ion patterns chapter. \\nResiliency \\nA microservices architecture moves your system from in-process to\", \" out-of-process network \\ncommunication. In a distributed architecture, what happens when Service B i\", \"sn\\u2019t responding to a \\nnetwork call from Service A? Or, what happens when Service C becomes temporari\", \"ly unavailable and \\nother services calling it become blocked? \\nResiliency is covered in the Cloud-na\", \"tive resiliency chapter. \\nDistributed Data \\nBy design, each microservice encapsulates its own data, \", \"exposing operations via its public interface. If \\nso, how do you query data or implement a transacti\", \"on across multiple services? \\nDistributed data is covered in the Cloud-native data patterns chapter.\", \" \\nSecrets \\nHow will your microservices securely store and manage secrets and sensitive configuration\", \" data? \\nSecrets are covered in detail Cloud-native security. \\n \\n12 \\nCHAPTER 1 | Introduction to clou\", \"d-native applications \\n \\nManage Complexity with Dapr \\nDapr is a distributed, open-source application\", \" runtime. Through an architecture of pluggable \\ncomponents, it dramatically simplifies the plumbing \", \"behind distributed applications. It provides a \\ndynamic glue that binds your application with pre-bu\", \"ilt infrastructure capabilities and components \\nfrom the Dapr runtime. Figure 1-5 shows Dapr from 20\", \",000 feet. \\n \\nFigure 1-5. Dapr at 20,000 feet. \\nIn the top row of the figure, note how Dapr provides\", \" language-specific SDKs for popular development \\nplatforms. Dapr v1 includes support for .NET, Go, N\", \"ode.js, Python, PHP, Java, and JavaScript. \\nWhile language-specific SDKs enhance the developer exper\", \"ience, Dapr is platform agnostic. Under the \\nhood, Dapr\\u2019s programming model exposes capabilities thr\", \"ough standard HTTP/gRPC communication \\nprotocols. Any programming platform can call Dapr via its nat\", \"ive HTTP and gRPC APIs. \\nThe blue boxes across the center of the figure represent the Dapr building \", \"blocks. Each exposes pre-\\nbuilt plumbing code for a distributed application capability that your app\", \"lication can consume. \\nThe components row represents a large set of pre-defined infrastructure compo\", \"nents that your \\napplication can consume. Think of components as infrastructure code you don\\u2019t have \", \"to write. \\nThe bottom row highlights the portability of Dapr and the diverse environments across whi\", \"ch it can \\nrun. \\nMicrosoft features a free ebook Dapr for .NET Developers for learning Dapr. \\nLookin\", \"g ahead, Dapr has the potential to have a profound impact on cloud-native application \\ndevelopment. \", \"\\nContainers \\nIt\\u2019s natural to hear the term container mentioned in any cloud native conversation. In \", \"the book, Cloud \\nNative Patterns, author Cornelia Davis observes that, \\u201cContainers are a great enabl\", \"er of cloud-native \\n \\n13 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\nsoftware.\\u201d The Cl\", \"oud Native Computing Foundation places microservice containerization as the first \\nstep in their Clo\", \"ud-Native Trail Map - guidance for enterprises beginning their cloud-native journey. \\nContainerizing\", \" a microservice is simple and straightforward. The code, its dependencies, and runtime \\nare packaged\", \" into a binary called a container image. Images are stored in a container registry, which \\nacts as a\", \" repository or library for images. A registry can be located on your development computer, in \\nyour \", \"data center, or in a public cloud. Docker itself maintains a public registry via Docker Hub. The \\nAz\", \"ure cloud features a private container registry to store container images close to the cloud \\napplic\", \"ations that will run them. \\nWhen an application starts or scales, you transform the container image \", \"into a running container \\ninstance. The instance runs on any computer that has a container runtime e\", \"ngine installed. You can \\nhave as many instances of the containerized service as needed. \\nFigure 1-6\", \" shows three different microservices, each in its own container, all running on a single host. \\n \\nFi\", \"gure 1-6. Multiple containers running on a container host \\nNote how each container maintains its own\", \" set of dependencies and runtime, which can be different \\nfrom one another. Here, we see different v\", \"ersions of the Product microservice running on the same \\nhost. Each container shares a slice of the \", \"underlying host operating system, memory, and processor, \\nbut is isolated from one another. \\nNote ho\", \"w well the container model embraces the Dependencies principle from the Twelve-Factor \\nApplication. \", \"\\nFactor #2 specifies that \\u201cEach microservice isolates and packages its own dependencies, embracing \\n\", \"changes without impacting the entire system.\\u201d \\nContainers support both Linux and Windows workloads. \", \"The Azure cloud openly embraces both. \\nInterestingly, it\\u2019s Linux, not Windows Server, that has becom\", \"e the more popular operating system in \\nAzure. \\n \\n14 \\nCHAPTER 1 | Introduction to cloud-native appli\", \"cations \\n \\nWhile several container vendors exist, Docker has captured the lion\\u2019s share of the market\", \". The \\ncompany has been driving the software container movement. It has become the de facto standard\", \" for \\npackaging, deploying, and running cloud-native applications. \\nWhy containers? \\nContainers prov\", \"ide portability and guarantee consistency across environments. By encapsulating \\neverything into a s\", \"ingle package, you isolate the microservice and its dependencies from the \\nunderlying infrastructure\", \". \\nYou can deploy the container in any environment that hosts the Docker runtime engine. Containeriz\", \"ed \\nworkloads also eliminate the expense of pre-configuring each environment with frameworks, softwa\", \"re \\nlibraries, and runtime engines. \\nBy sharing the underlying operating system and host resources, \", \"a container has a much smaller \\nfootprint than a full virtual machine. The smaller size increases th\", \"e density, or number of \\nmicroservices, that a given host can run at one time. \\nContainer orchestrat\", \"ion \\nWhile tools such as Docker create images and run containers, you also need tools to manage them\", \". \\nContainer management is done with a special software program called a container orchestrator. \\nWh\", \"en operating at scale with many independent running containers, orchestration is essential. \\nFigure \", \"1-7 shows management tasks that container orchestrators automate. \\n \\nFigure 1-7. What container orch\", \"estrators do \\nThe following table describes common orchestration tasks. \\nTasks \\nExplanation \\nSchedul\", \"ing \\nAutomatically provision container instances. \\nAffinity/anti-affinity \\nProvision containers near\", \"by or far apart from \\neach other, helping availability and \\nperformance. \\nHealth monitoring \\nAutomat\", \"ically detect and correct failures. \\nFailover \\nAutomatically reprovision a failed instance to a \\nhea\", \"lthy machine. \\n \\n15 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\nTasks \\nExplanation \\nSc\", \"aling \\nAutomatically add or remove a container \\ninstance to meet demand. \\nNetworking \\nManage a netwo\", \"rking overlay for container \\ncommunication. \\nService Discovery \\nEnable containers to locate each oth\", \"er. \\nRolling Upgrades \\nCoordinate incremental upgrades with zero \\ndowntime deployment. Automatically\", \" roll back \\nproblematic changes. \\nNote how container orchestrators embrace the Disposability and Con\", \"currency principles from the \\nTwelve-Factor Application. \\nFactor #9 specifies that \\u201cService instance\", \"s should be disposable, favoring fast startups to increase \\nscalability opportunities and graceful s\", \"hutdowns to leave the system in a correct state.\\u201d Docker \\ncontainers along with an orchestrator inhe\", \"rently satisfy this requirement.\\u201d \\nFactor #8 specifies that \\u201cServices scale out across a large numbe\", \"r of small identical processes (copies) as \\nopposed to scaling-up a single large instance on the mos\", \"t powerful machine available.\\u201d \\nWhile several container orchestrators exist, Kubernetes has become t\", \"he de facto standard for the \\ncloud-native world. It\\u2019s a portable, extensible, open-source platform \", \"for managing containerized \\nworkloads. \\nYou could host your own instance of Kubernetes, but then you\", \"\\u2019d be responsible for provisioning and \\nmanaging its resources - which can be complex. The Azure clo\", \"ud features Kubernetes as a managed \\nservice. Both Azure Kubernetes Service (AKS) and Azure Red Hat \", \"OpenShift (ARO) enable you to fully \\nleverage the features and power of Kubernetes as a managed serv\", \"ice, without having to install and \\nmaintain it. \\nContainer orchestration is covered in detail in Sc\", \"aling Cloud-Native Applications. \\nBacking services \\nCloud-native systems depend upon many different \", \"ancillary resources, such as data stores, message \\nbrokers, monitoring, and identity services. These\", \" services are known as backing services. \\nFigure 1-8 shows many common backing services that cloud-n\", \"ative systems consume. \\n \\n16 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\n \\nFigure 1-8.\", \" Common backing services \\nYou could host your own backing services, but then you\\u2019d be responsible fo\", \"r licensing, provisioning, \\nand managing those resources. \\nCloud providers offer a rich assortment o\", \"f managed backing services. Instead of owning the service, \\nyou simply consume it. The cloud provide\", \"r operates the resource at scale and bears the responsibility \\nfor performance, security, and mainte\", \"nance. Monitoring, redundancy, and availability are built into the \\nservice. Providers guarantee ser\", \"vice level performance and fully support their managed services - \\nopen a ticket and they fix your i\", \"ssue. \\nCloud-native systems favor managed backing services from cloud vendors. The savings in time a\", \"nd \\nlabor can be significant. The operational risk of hosting your own and experiencing trouble can \", \"get \\nexpensive fast. \\nA best practice is to treat a backing service as an attached resource, dynamic\", \"ally bound to a \\nmicroservice with configuration information (a URL and credentials) stored in an ex\", \"ternal \\nconfiguration. This guidance is spelled out in the Twelve-Factor Application, discussed earl\", \"ier in the \\nchapter. \\nFactor #4 specifies that backing services \\u201cshould be exposed via an addressabl\", \"e URL. Doing so \\ndecouples the resource from the application, enabling it to be interchangeable.\\u201d \\nF\", \"actor #3 specifies that \\u201cConfiguration information is moved out of the microservice and externalized\", \" \\nthrough a configuration management tool outside of the code.\\u201d \\nWith this pattern, a backing servic\", \"e can be attached and detached without code changes. You might \\npromote a microservice from QA to a \", \"staging environment. You update the microservice \\nconfiguration to point to the backing services in \", \"staging and inject the settings into your container \\nthrough an environment variable. \\n \\n17 \\nCHAPTER\", \" 1 | Introduction to cloud-native applications \\n \\nCloud vendors provide APIs for you to communicate \", \"with their proprietary backing services. These \\nlibraries encapsulate the proprietary plumbing and c\", \"omplexity. However, communicating directly with \\nthese APIs will tightly couple your code to that sp\", \"ecific backing service. It\\u2019s a widely accepted practice \\nto insulate the implementation details of t\", \"he vendor API. Introduce an intermediation layer, or \\nintermediate API, exposing generic operations \", \"to your service code and wrap the vendor code inside \\nit. This loose coupling enables you to swap ou\", \"t one backing service for another or move your code to \\na different cloud environment without having\", \" to make changes to the mainline service code. Dapr, \\ndiscussed earlier, follows this model with its\", \" set of prebuilt building blocks. \\nOn a final thought, backing services also promote the Statelessne\", \"ss principle from the Twelve-Factor \\nApplication, discussed earlier in the chapter. \\nFactor #6 speci\", \"fies that, \\u201cEach microservice should execute in its own process, isolated from other \\nrunning servic\", \"es. Externalize required state to a backing service such as a distributed cache or data \\nstore.\\u201d \\nBa\", \"cking services are discussed in Cloud-native data patterns and Cloud-native communication \\npatterns.\", \" \\nAutomation \\nAs you\\u2019ve seen, cloud-native systems embrace microservices, containers, and modern sys\", \"tem design \\nto achieve speed and agility. But, that\\u2019s only part of the story. How do you provision t\", \"he cloud \\nenvironments upon which these systems run? How do you rapidly deploy app features and upda\", \"tes? \\nHow do you round out the full picture? \\nEnter the widely accepted practice of Infrastructure a\", \"s Code, or IaC. \\nWith IaC, you automate platform provisioning and application deployment. You essent\", \"ially apply \\nsoftware engineering practices such as testing and versioning to your DevOps practices.\", \" Your \\ninfrastructure and deployments are automated, consistent, and repeatable. \\nAutomating infrast\", \"ructure \\nTools like Azure Resource Manager, Azure Bicep, Terraform from HashiCorp, and the Azure CLI\", \", enable \\nyou to declaratively script the cloud infrastructure you require. Resource names, location\", \"s, capacities, \\nand secrets are parameterized and dynamic. The script is versioned and checked into \", \"source control \\nas an artifact of your project. You invoke the script to provision a consistent and \", \"repeatable \\ninfrastructure across system environments, such as QA, staging, and production. \\nUnder t\", \"he hood, IaC is idempotent, meaning that you can run the same script over and over without \\nside eff\", \"ects. If the team needs to make a change, they edit and rerun the script. Only the updated \\nresource\", \"s are affected. \\nIn the article, What is Infrastructure as Code, Author Sam Guckenheimer describes h\", \"ow, \\u201cTeams who \\nimplement IaC can deliver stable environments rapidly and at scale. They avoid manua\", \"l configuration \\nof environments and enforce consistency by representing the desired state of their \", \"environments via \\ncode. Infrastructure deployments with IaC are repeatable and prevent runtime issue\", \"s caused by \\nconfiguration drift or missing dependencies. DevOps teams can work together with a unif\", \"ied set of \\n \\n18 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\npractices and tools to de\", \"liver applications and their supporting infrastructure rapidly, reliably, and at \\nscale.\\u201d \\nAutomatin\", \"g deployments \\nThe Twelve-Factor Application, discussed earlier, calls for separate steps when trans\", \"forming \\ncompleted code into a running application. \\nFactor #5 specifies that \\u201cEach release must enf\", \"orce a strict separation across the build, release and run \\nstages. Each should be tagged with a uni\", \"que ID and support the ability to roll back.\\u201d \\nModern CI/CD systems help fulfill this principle. The\", \"y provide separate build and delivery steps that \\nhelp ensure consistent and quality code that\\u2019s rea\", \"dily available to users. \\nFigure 1-9 shows the separation across the deployment process. \\n \\nFigure 1\", \"-9. Deployment steps in a CI/CD Pipeline \\nIn the previous figure, pay special attention to separatio\", \"n of tasks: \\n1. \\nThe developer constructs a feature in their development environment, iterating thro\", \"ugh what \\nis called the \\u201cinner loop\\u201d of code, run, and debug. \\n2. \\nWhen complete, that code is pushe\", \"d into a code repository, such as GitHub, Azure DevOps, or \\nBitBucket. \\n3. \\nThe push triggers a buil\", \"d stage that transforms the code into a binary artifact. The work is \\nimplemented with a Continuous \", \"Integration (CI) pipeline. It automatically builds, tests, and \\npackages the application. \\n4. \\nThe r\", \"elease stage picks up the binary artifact, applies external application and environment \\nconfigurati\", \"on information, and produces an immutable release. The release is deployed to a \\nspecified environme\", \"nt. The work is implemented with a Continuous Delivery (CD) pipeline. \\nEach release should be identi\", \"fiable. You can say, \\u201cThis deployment is running Release 2.1.1 of \\nthe application.\\u201d \\n \\n19 \\nCHAPTER \", \"1 | Introduction to cloud-native applications \\n \\n5. \\nFinally, the released feature is run in the tar\", \"get execution environment. Releases are \\nimmutable meaning that any change must create a new release\", \". \\nApplying these practices, organizations have radically evolved how they ship software. Many have \", \"\\nmoved from quarterly releases to on-demand updates. The goal is to catch problems early in the \\ndev\", \"elopment cycle when they\\u2019re less expensive to fix. The longer the duration between integrations, \\nth\", \"e more expensive problems become to resolve. With consistency in the integration process, teams \\ncan\", \" commit code changes more frequently, leading to better collaboration and software quality. \\nInfrast\", \"ructure as code and deployment automation, along with GitHub and Azure DevOps are \\ndiscussed in deta\", \"il in DevOps. \\nCandidate apps for cloud native \\nThink about the apps your organization needs to buil\", \"d. Then, look at the existing apps in your \\nportfolio. How many of them warrant a cloud-native archi\", \"tecture? All of them? Perhaps some? \\nApplying cost/benefit analysis, there\\u2019s a good chance some woul\", \"dn\\u2019t support the effort. The cost of \\nbecoming cloud native would far exceed the business value of t\", \"he application. \\nWhat type of application might be a candidate for cloud native? \\n\\u2022 \\nStrategic enter\", \"prise systems that need to constantly evolve business capabilities/features \\n\\u2022 \\nAn application that \", \"requires a high release velocity - with high confidence \\n\\u2022 \\nA system where individual features must \", \"release without a full redeployment of the entire \\nsystem \\n\\u2022 \\nAn application developed by teams with\", \" expertise in different technology stacks \\n\\u2022 \\nAn application with components that must scale indepen\", \"dently \\nSmaller, less impactful line-of-business applications might fare well with a simple monolith\", \"ic \\narchitecture hosted in a Cloud PaaS environment. \\nThen there are legacy systems. While we\\u2019d all \", \"like to build new applications, we\\u2019re often responsible \\nfor modernizing legacy workloads that are c\", \"ritical to the business. \\nModernizing legacy apps \\nThe free Microsoft e-book Modernize existing .NET\", \" applications with Azure cloud and Windows \\nContainers provides guidance about migrating on-premises\", \" workloads into cloud. Figure 1-10 shows \\nthat there isn\\u2019t a single, one-size-fits-all strategy for \", \"modernizing legacy applications. \\n \\n20 \\nCHAPTER 1 | Introduction to cloud-native applications \\n \\n \\nF\", \"igure 1-10. Strategies for migrating legacy workloads \\nMonolithic apps that are non-critical might b\", \"enefit from a quick lift-and-shift (Cloud Infrastructure-\\nReady) migration. Here, the on-premises wo\", \"rkload is rehosted to a cloud-based VM, without changes. \\nThis approach uses the IaaS (Infrastructur\", \"e as a Service) model. Azure includes several tools such as \\nAzure Migrate, Azure Site Recovery, and\", \" Azure Database Migration Service to help streamline the \\nmove. While this strategy can yield some c\", \"ost savings, such applications typically weren\\u2019t designed to \\nunlock and leverage the benefits of cl\", \"oud computing. \\nLegacy apps that are critical to the business often benefit from an enhanced Cloud O\", \"ptimized \\nmigration. This approach includes deployment optimizations that enable key cloud services \", \"- without \\nchanging the core architecture of the application. For example, you might containerize th\", \"e application \\nand deploy it to a container orchestrator, like Azure Kubernetes Services, discussed \", \"later in this book. \\nOnce in the cloud, the application can consume cloud backing services such as d\", \"atabases, message \\nqueues, monitoring, and distributed caching. \\nFinally, monolithic apps that provi\", \"de strategic enterprise functions might best benefit from a Cloud-\\nNative approach, the subject of t\", \"his book. This approach provides agility and velocity. But, it comes at \\na cost of replatforming, re\", \"architecting, and rewriting code. Over time, a legacy application could be \\ndecomposed into microser\", \"vices, containerized, and ultimately replatformed into a cloud-native \\narchitecture. \\nIf you and you\", \"r team believe a cloud-native approach is appropriate, it behooves you to rationalize \\nthe decision \", \"with your organization. What exactly is the business problem that a cloud-native \\napproach will solv\", \"e? How would it align with business needs? \\n\\u2022 \\nRapid releases of features with increased confidence?\", \" \\n\\u2022 \\nFine-grained scalability - more efficient usage of resources? \\n \\n21 \\nCHAPTER 1 | Introduction t\", \"o cloud-native applications \\n \\n\\u2022 \\nImproved system resiliency? \\n\\u2022 \\nImproved system performance? \\n\\u2022 \\nM\", \"ore visibility into operations? \\n\\u2022 \\nBlend development platforms and data stores to arrive at the bes\", \"t tool for the job? \\n\\u2022 \\nFuture-proof application investment? \\nThe right migration strategy depends o\", \"n organizational priorities and the systems you\\u2019re targeting. \\nFor many, it may be more cost effecti\", \"ve to cloud-optimize a monolithic application or add coarse-\\ngrained services to an N-Tier app. In t\", \"hese cases, you can still make full use of cloud PaaS capabilities \\nlike the ones offered by Azure A\", \"pp Service. \\nSummary \\nIn this chapter, we introduced cloud-native computing. We provided a definitio\", \"n along with the key \\ncapabilities that drive a cloud-native application. We looked at the types of \", \"applications that might \\njustify this investment and effort. \\nWith the introduction behind, we now d\", \"ive into a much more detailed look at cloud native. \\nReferences \\n\\u2022 \\nCloud Native Computing Foundatio\", \"n \\n\\u2022 \\n.NET Microservices: Architecture for Containerized .NET applications \\n\\u2022 \\nMicrosoft Azure Well-\", \"Architected Framework \\n\\u2022 \\nModernize existing .NET applications with Azure cloud and Windows Containe\", \"rs \\n\\u2022 \\nCloud Native Patterns by Cornelia Davis \\n\\u2022 \\nCloud native applications: Ship faster, reduce ri\", \"sk, and grow your business \\n\\u2022 \\nDapr for .NET Developers \\n\\u2022 \\nDapr documents \\n\\u2022 \\nBeyond the Twelve-Fac\", \"tor Application \\n\\u2022 \\nWhat is Infrastructure as Code \\n\\u2022 \\nUber Engineering\\u2019s Micro Deploy: Deploying Da\", \"ily with Confidence \\n\\u2022 \\nHow Netflix Deploys Code \\n\\u2022 \\nOverload Control for Scaling WeChat Microservic\", \"es \\n \\n22 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\nCHAPTER 2 \\nIntroducing \\neShopOn\", \"Containers \\nreference app \\nMicrosoft, in partnership with leading community experts, has produced a \", \"full-featured cloud-native \\nmicroservices reference application, eShopOnContainers. This application\", \" is built to showcase using \\n.NET and Docker, and optionally Azure, Kubernetes, and Visual Studio, t\", \"o build an online storefront. \\n \\nFigure 2-1. eShopOnContainers Sample App Screenshot. \\n \\n23 \\nCHAPTER\", \" 2 | Introducing eShopOnContainers reference app \\n \\nBefore starting this chapter, we recommend that \", \"you download the eShopOnContainers reference \\napplication. If you do so, it should be easier for you\", \" to follow along with the information presented. \\nFeatures and requirements \\nLet\\u2019s start with a revi\", \"ew of the application\\u2019s features and requirements. The eShopOnContainers \\napplication represents an \", \"online store that sells various physical products like t-shirts and coffee \\nmugs. If you\\u2019ve bought a\", \"nything online before, the experience of using the store should be relatively \\nfamiliar. Here are so\", \"me of the basic features the store implements: \\n\\u2022 \\nList catalog items \\n\\u2022 \\nFilter items by type \\n\\u2022 \\nF\", \"ilter items by brand \\n\\u2022 \\nAdd items to the shopping basket \\n\\u2022 \\nEdit or remove items from the basket \\n\", \"\\u2022 \\nCheckout \\n\\u2022 \\nRegister an account \\n\\u2022 \\nSign in \\n\\u2022 \\nSign out \\n\\u2022 \\nReview orders \\nThe application also\", \" has the following non-functional requirements: \\n\\u2022 \\nIt needs to be highly available and it must scal\", \"e automatically to meet increased traffic (and \\nscale back down once traffic subsides). \\n\\u2022 \\nIt shoul\", \"d provide easy-to-use monitoring of its health and diagnostic logs to help \\ntroubleshoot any issues \", \"it encounters. \\n\\u2022 \\nIt should support an agile development process, including support for continuous \", \"integration \\nand deployment (CI/CD). \\n\\u2022 \\nIn addition to the two web front ends (traditional and Sing\", \"le Page Application), the \\napplication must also support mobile client apps running different kinds \", \"of operating \\nsystems. \\n\\u2022 \\nIt should support cross-platform hosting and cross-platform development. \", \"\\n \\n24 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\n \\nFigure 2-2. eShopOnContainers re\", \"ference application development architecture. \\nThe eShopOnContainers application is accessible from \", \"web or mobile clients that access the \\napplication over HTTPS targeting either the ASP.NET Core MVC \", \"server application or an appropriate \\nAPI Gateway. API Gateways offer several advantages, such as de\", \"coupling back-end services from \\nindividual front-end clients and providing better security. The app\", \"lication also makes use of a related \\npattern known as Backends-for-Frontends (BFF), which recommend\", \"s creating separate API gateways \\nfor each front-end client. The reference architecture demonstrates\", \" breaking up the API gateways \\nbased on whether the request is coming from a web or mobile client. \\n\", \"The application\\u2019s functionality is broken up into many distinct microservices. There are services \\nr\", \"esponsible for authentication and identity, listing items from the product catalog, managing users\\u2019 \", \"\\nshopping baskets, and placing orders. Each of these separate services has its own persistent storag\", \"e. \\nThere\\u2019s no single primary data store with which all services interact. Instead, coordination and\", \" \\ncommunication between the services is done on an as-needed basis and by using a message bus. \\nEach\", \" of the different microservices is designed differently, based on their individual requirements. Thi\", \"s \\naspect means their technology stack may differ, although they\\u2019re all built using .NET and designe\", \"d for \\nthe cloud. Simpler services provide basic Create-Read-Update-Delete (CRUD) access to the unde\", \"rlying \\ndata stores, while more advanced services use Domain-Driven Design approaches and patterns t\", \"o \\nmanage business complexity. \\n \\n25 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\n \\nF\", \"igure 2-3. Different kinds of microservices. \\nOverview of the code \\nBecause it uses microservices, t\", \"he eShopOnContainers app includes quite a few separate projects and \\nsolutions in its GitHub reposit\", \"ory. In addition to separate solutions and executable files, the various \\nservices are designed to r\", \"un inside their own containers, both during local development and at run \\ntime in production. Figure\", \" 2-4 shows the full Visual Studio solution, in which the various different \\nprojects are organized. \", \"\\n \\n26 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\n \\n \\n27 \\nCHAPTER 2 | Introducing eS\", \"hopOnContainers reference app \\n \\nFigure 2-4. Projects in Visual Studio solution. \\nThe code is organi\", \"zed to support the different microservices, and within each microservice, the code is \\nbroken up int\", \"o domain logic, infrastructure concerns, and user interface or service endpoint. In many \\ncases, eac\", \"h service\\u2019s dependencies can be fulfilled by Azure services in production, and alternative \\noptions \", \"for local development. Let\\u2019s examine how the application\\u2019s requirements map to Azure \\nservices. \\nUnd\", \"erstanding microservices \\nThis book focuses on cloud-native applications built using Azure technolog\", \"y. To learn more about \\nmicroservices best practices and how to architect microservice-based applica\", \"tions, read the \\ncompanion book, .NET Microservices: Architecture for Containerized .NET Application\", \"s. \\nMapping eShopOnContainers to Azure Services \\nAlthough not required, Azure is well-suited to supp\", \"orting the eShopOnContainers because the project \\nwas built to be a cloud-native application. The ap\", \"plication is built with .NET, so it can run on Linux or \\nWindows containers depending on the Docker \", \"host. The application is made up of multiple \\nautonomous microservices, each with its own data. The \", \"different microservices showcase different \\napproaches, ranging from simple CRUD operations to more \", \"complex DDD and CQRS patterns. \\nMicroservices communicate with clients over HTTP and with one anothe\", \"r via message-based \\ncommunication. The application supports multiple platforms for clients as well,\", \" since it adopts HTTP \\nas a standard communication protocol and includes ASP.NET Core and Xamarin mo\", \"bile apps that run \\non Android, iOS, and Windows platforms. \\nThe application\\u2019s architecture is shown\", \" in Figure 2-5. On the left are the client apps, broken up into \\nmobile, traditional Web, and Web Si\", \"ngle Page Application (SPA) flavors. On the right are the server-\\nside components that make up the s\", \"ystem, each of which can be hosted in Docker containers and \\nKubernetes clusters. The traditional we\", \"b app is powered by the ASP.NET Core MVC application shown \\nin yellow. This app and the mobile and w\", \"eb SPA applications communicate with the individual \\nmicroservices through one or more API gateways.\", \" The API gateways follow the \\u201cbackends for front \\nends\\u201d (BFF) pattern, meaning that each gateway is \", \"designed to support a given front-end client. The \\nindividual microservices are listed to the right \", \"of the API gateways and include both business logic \\nand some kind of persistence store. The differe\", \"nt services make use of SQL Server databases, Redis \\ncache instances, and MongoDB/CosmosDB stores. O\", \"n the far right is the system\\u2019s Event Bus, which is \\nused for communication between the microservice\", \"s. \\n \\n28 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\n \\nFigure 2-5. The eShopOnContai\", \"ners Architecture. \\nThe server-side components of this architecture all map easily to Azure services\", \". \\nContainer orchestration and clustering \\nThe application\\u2019s container-hosted services, from ASP.NET\", \" Core MVC apps to individual Catalog and \\nOrdering microservices, can be hosted and managed in Azure\", \" Kubernetes Service (AKS). The \\napplication can run locally on Docker and Kubernetes, and the same c\", \"ontainers can then be deployed \\nto staging and production environments hosted in AKS. This process c\", \"an be automated as we\\u2019ll see in \\nthe next section. \\nAKS provides management services for individual \", \"clusters of containers. The application will deploy \\nseparate containers for each microservice in th\", \"e AKS cluster, as shown in the architecture diagram \\nabove. This approach allows each individual ser\", \"vice to scale independently according to its resource \\ndemands. Each microservice can also be deploy\", \"ed independently, and ideally such deployments \\nshould incur zero system downtime. \\nAPI Gateway \\nThe\", \" eShopOnContainers application has multiple front-end clients and multiple different back-end \\nservi\", \"ces. There\\u2019s no one-to-one correspondence between the client applications and the microservices \\ntha\", \"t support them. In such a scenario, there may be a great deal of complexity when writing client \\nsof\", \"tware to interface with the various back-end services in a secure manner. Each client would need to \", \"\\naddress this complexity on its own, resulting in duplication and many places in which to make updat\", \"es \\nas services change or new policies are implemented. \\nAzure API Management (APIM) helps organizat\", \"ions publish APIs in a consistent, manageable fashion. \\nAPIM consists of three components: the API G\", \"ateway, and administration portal (the Azure portal), \\nand a developer portal. \\n \\n29 \\nCHAPTER 2 | In\", \"troducing eShopOnContainers reference app \\n \\nThe API Gateway accepts API calls and routes them to th\", \"e appropriate back-end API. It can also \\nprovide additional services like verification of API keys o\", \"r JWT tokens and API transformation on the \\nfly without code modifications (for instance, to accommo\", \"date clients expecting an older interface). \\nThe Azure portal is where you define the API schema and\", \" package different APIs into products. You \\nalso configure user access, view reports, and configure \", \"policies for quotas or transformations. \\nThe developer portal serves as the main resource for develo\", \"pers. It provides developers with API \\ndocumentation, an interactive test console, and reports on th\", \"eir own usage. Developers also use the \\nportal to create and manage their own accounts, including su\", \"bscription and API key support. \\nUsing APIM, applications can expose several different groups of ser\", \"vices, each providing a back end \\nfor a particular front-end client. APIM is recommended for complex\", \" scenarios. For simpler needs, the \\nlightweight API Gateway Ocelot can be used. The eShopOnContainer\", \"s app uses Ocelot because of its \\nsimplicity and because it can be deployed into the same applicatio\", \"n environment as the application \\nitself. Learn more about eShopOnContainers, APIM, and Ocelot. \\nAno\", \"ther option if your application is using AKS is to deploy the Azure Gateway Ingress Controller as a \", \"\\npod within your AKS cluster. This approach allows your cluster to integrate with an Azure Applicati\", \"on \\nGateway, allowing the gateway to load-balance traffic to the AKS pods. Learn more about the Azur\", \"e \\nGateway Ingress Controller for AKS. \\nData \\nThe various back-end services used by eShopOnContainer\", \"s have different storage requirements. \\nSeveral microservices use SQL Server databases. The Basket m\", \"icroservice leverages a Redis cache for \\nits persistence. The Locations microservice expects a Mongo\", \"DB API for its data. Azure supports each \\nof these data formats. \\nFor SQL Server database support, A\", \"zure has products for everything from single databases up to \\nhighly scalable SQL Database elastic p\", \"ools. Individual microservices can be configured to \\ncommunicate with their own individual SQL Serve\", \"r databases quickly and easily. These databases can \\nbe scaled as needed to support each separate mi\", \"croservice according to its needs. \\nThe eShopOnContainers application stores the user\\u2019s current shop\", \"ping basket between requests. This \\naspect is managed by the Basket microservice that stores the dat\", \"a in a Redis cache. In development, \\nthis cache can be deployed in a container, while in production \", \"it can utilize Azure Cache for Redis. \\nAzure Cache for Redis is a fully managed service offering hig\", \"h performance and reliability without the \\nneed to deploy and manage Redis instances or containers o\", \"n your own. \\nThe Locations microservice uses a MongoDB NoSQL database for its persistence. During \\nd\", \"evelopment, the database can be deployed in its own container, while in production the service can \\n\", \"leverage Azure Cosmos DB\\u2019s API for MongoDB. One of the benefits of Azure Cosmos DB is its ability \\nt\", \"o leverage multiple different communication protocols, including a SQL API and common NoSQL \\nAPIs in\", \"cluding MongoDB, Cassandra, Gremlin, and Azure Table Storage. Azure Cosmos DB offers a \\nfully manage\", \"d and globally distributed database as a service that can scale to meet the needs of the \\nservices t\", \"hat use it. \\nDistributed data in cloud-native applications is covered in more detail in chapter 5. \\n\", \" \\n30 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\nEvent Bus \\nThe application uses eve\", \"nts to communicate changes between different services. This functionality can \\nbe implemented with v\", \"arious implementations, and locally the eShopOnContainers application uses \\nRabbitMQ. When hosted in\", \" Azure, the application would leverage Azure Service Bus for its messaging. \\nAzure Service Bus is a \", \"fully managed integration message broker that allows applications and services \\nto communicate with \", \"one another in a decoupled, reliable, asynchronous manner. Azure Service Bus \\nsupports individual qu\", \"eues as well as separate topics to support publisher-subscriber scenarios. The \\neShopOnContainers ap\", \"plication would leverage topics with Azure Service Bus to support distributing \\nmessages from one mi\", \"croservice to any other microservice that needed to react to a given message. \\nResiliency \\nOnce depl\", \"oyed to production, the eShopOnContainers application would be able to take advantage \\nof several Az\", \"ure services available to improve its resiliency. The application publishes health checks, \\nwhich ca\", \"n be integrated with Application Insights to provide reporting and alerts based on the app\\u2019s \\navaila\", \"bility. Azure resources also provide diagnostic logs that can be used to identify and correct bugs \\n\", \"and performance issues. Resource logs provide detailed information on when and how different Azure \\n\", \"resources are used by the application. You\\u2019ll learn more about cloud-native resiliency features in \\n\", \"chapter 6. \\nDeploying eShopOnContainers to Azure \\nThe eShopOnContainers application can be deployed \", \"to various Azure platforms. The recommended \\napproach is to deploy the application to Azure Kubernet\", \"es Services (AKS). Helm, a Kubernetes \\ndeployment tool, is available to reduce deployment complexity\", \". Optionally, developers may \\nimplement Azure Dev Spaces for Kubernetes to streamline their developm\", \"ent process. \\nAzure Kubernetes Service \\nTo host eShop in AKS, the first step is to create an AKS clu\", \"ster. To do so, you might use the Azure \\nportal, which will walk you through the required steps. You\", \" could also create a cluster from the Azure \\nCLI, taking care to enable Role-Based Access Control (R\", \"BAC) and application routing. The \\neShopOnContainers\\u2019 documentation details the steps for creating y\", \"our own AKS cluster. Once created, \\nyou can access and manage the cluster from the Kubernetes dashbo\", \"ard. \\nYou can now deploy the eShop application to the cluster using Helm. \\nDeploying to Azure Kubern\", \"etes Service using Helm \\nHelm is an application package manager tool that works directly with Kubern\", \"etes. It helps you define, \\ninstall, and upgrade Kubernetes applications. While simple apps can be d\", \"eployed to AKS with custom \\nCLI scripts or simple deployment files, complex apps can contain many Ku\", \"bernetes objects and benefit \\nfrom Helm. \\nUsing Helm, applications include text-based configuration \", \"files, called Helm charts, which declaratively \\ndescribe the application and configuration in Helm p\", \"ackages. Charts use standard YAML-formatted \\n \\n31 \\nCHAPTER 2 | Introducing eShopOnContainers referen\", \"ce app \\n \\nfiles to describe a related set of Kubernetes resources. They\\u2019re versioned alongside the a\", \"pplication \\ncode they describe. Helm Charts range from simple to complex depending on the requiremen\", \"ts of the \\ninstallation they describe. \\nHelm is composed of a command-line client tool, which consum\", \"es helm charts and launches \\ncommands to a server component named, Tiller. Tiller communicates with \", \"the Kubernetes API to \\nensure the correct provisioning of your containerized workloads. Helm is main\", \"tained by the Cloud-\\nnative Computing Foundation. \\nThe following yaml file presents a Helm template:\", \" \\napiVersion: v1 \\nkind: Service \\nmetadata: \\n  name: {{ .Values.app.svc.marketing }} \\n  labels: \\n    \", \"app: {{ template \\\"marketing-api.name\\\" . }} \\n    chart: {{ template \\\"marketing-api.chart\\\" . }} \\n    r\", \"elease: {{ .Release.Name }} \\n    heritage: {{ .Release.Service }} \\nspec: \\n  type: {{ .Values.service\", \".type }} \\n  ports: \\n    - port: {{ .Values.service.port }} \\n      targetPort: http \\n      protocol: \", \"TCP \\n      name: http \\n  selector: \\n    app: {{ template \\\"marketing-api.name\\\" . }} \\n    release: {{ \", \".Release.Name }} \\nNote how the template describes a dynamic set of key/value pairs. When the templat\", \"e is invoked, \\nvalues that enclosed in curly braces are pulled in from other yaml-based configuratio\", \"n files. \\nYou\\u2019ll find the eShopOnContainers helm charts in the /k8s/helm folder. Figure 2-6 shows ho\", \"w the \\ndifferent components of the application are organized into a folder structure used by helm to\", \" define \\nand managed deployments. \\n \\n32 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\n\", \" \\nFigure 2-6. The eShopOnContainers helm folder. \\nEach individual component is installed using a hel\", \"m install command. eShop includes a \\u201cdeploy all\\u201d \\nscript that loops through and installs the compone\", \"nts using their respective helm charts. The result is \\na repeatable process, versioned with the appl\", \"ication in source control, that anyone on the team can \\ndeploy to an AKS cluster with a one-line scr\", \"ipt command. \\nNote that version 3 of Helm officially removes the need for the Tiller server componen\", \"t. More \\ninformation on this enhancement can be found here. \\nAzure Functions and Logic Apps (Serverl\", \"ess) \\nThe eShopOnContainers sample includes support for tracking online marketing campaigns. An Azur\", \"e \\nFunction is used to track marketing campaign details for a given campaign ID. Rather than creatin\", \"g a \\n \\n33 \\nCHAPTER 2 | Introducing eShopOnContainers reference app \\n \\nfull microservice, a single Az\", \"ure Function is simpler and sufficient. Azure Functions have a simple build \\nand deployment model, e\", \"specially when configured to run in Kubernetes. Deploying the function is \\nscripted using Azure Reso\", \"urce Manager (ARM) templates and the Azure CLI. This campaign service \\nisn\\u2019t customer-facing and inv\", \"okes a single operation, making it a great candidate for Azure Functions. \\nThe function requires min\", \"imal configuration, including a database connection string data and image \\nbase URI settings. You co\", \"nfigure Azure Functions in the Azure portal. \\nCentralized configuration \\nUnlike a monolithic app in \", \"which everything runs within a single instance, a cloud-native application \\nconsists of independent \", \"services distributed across virtual machines, containers, and geographic \\nregions. Managing configur\", \"ation settings for dozens of interdependent services can be challenging. \\nDuplicate copies of config\", \"uration settings across different locations are error prone and difficult to \\nmanage. Centralized co\", \"nfiguration is a critical requirement for distributed cloud-native applications. \\nAs discussed in Ch\", \"apter 1, the Twelve-Factor App recommendations require strict separation between \\ncode and configura\", \"tion. Configuration must be stored externally from the application and read-in as \\nneeded. Storing c\", \"onfiguration values as constants or literal values in code is a violation. The same \\nconfiguration v\", \"alues are often be used by many services in the same application. Additionally, we \\nmust support the\", \" same values across multiple environments, such as dev, testing, and production. The \\nbest practice \", \"is store them in a centralized configuration store. \\nThe Azure cloud presents several great options.\", \" \\nAzure App Configuration \\nAzure App Configuration is a fully managed Azure service that stores non-\", \"secret configuration \\nsettings in a secure, centralized location. Stored values can be shared among \", \"multiple services and \\napplications. \\nThe service is simple to use and provides several benefits: \\n\\u2022\", \" \\nFlexible key/value representations and mappings \\n\\u2022 \\nTagging with Azure labels \\n\\u2022 \\nDedicated UI for\", \" management \\n\\u2022 \\nEncryption of sensitive information \\n\\u2022 \\nQuerying and batch retrieval \\nAzure App Conf\", \"iguration maintains changes made to key-value settings for seven days. The point-in-\\ntime snapshot f\", \"eature enables you to reconstruct the history of a setting and even rollback for a failed \\ndeploymen\", \"t. \\nApp Configuration automatically caches each setting to avoid excessive calls to the configuratio\", \"n \\nstore. The refresh operation waits until the cached value of a setting expires to update that set\", \"ting, \\neven when its value changes in the configuration store. The default cache expiration time is \", \"30 \\nseconds. You can override the expiration time. \\n \\n34 \\nCHAPTER 2 | Introducing eShopOnContainers \", \"reference app \\n \\nApp Configuration encrypts all configuration values in transit and at rest. Key nam\", \"es and labels are \\nused as indexes for retrieving configuration data and aren\\u2019t encrypted. \\nAlthough\", \" App Configuration provides hardened security, Azure Key Vault is still the best place for \\nstoring \", \"application secrets. Key Vault provides hardware-level encryption, granular access policies, and \\nma\", \"nagement operations such as certificate rotation. You can create App Configuration values that \\nrefe\", \"rence secrets stored in a Key Vault. \\nAzure Key Vault \\nKey Vault is a managed service for securely s\", \"toring and accessing secrets. A secret is anything that you \\nwant to tightly control access to, such\", \" as API keys, passwords, or certificates. A vault is a logical group \\nof secrets. \\nKey Vault greatly\", \" reduces the chances that secrets may be accidentally leaked. When using Key Vault, \\napplication dev\", \"elopers no longer need to store security information in their application. This practice \\neliminates\", \" the need to store this information inside your code. For example, an application may need \\nto conne\", \"ct to a database. Instead of storing the connection string in the app\\u2019s code, you can store it \\nsecu\", \"rely in Key Vault. \\nYour applications can securely access the information they need by using URIs. T\", \"hese URIs allow the \\napplications to retrieve specific versions of a secret. There\\u2019s no need to writ\", \"e custom code to protect \\nany of the secret information stored in Key Vault. \\nAccess to Key Vault re\", \"quires proper caller authentication and authorization. Typically, each cloud-\\nnative microservice us\", \"es a ClientId/ClientSecret combination. It\\u2019s important to keep these credentials \\noutside source con\", \"trol. A best practice is to set them in the application\\u2019s environment. Direct access to \\nKey Vault f\", \"rom AKS can be achieved using Key Vault FlexVolume. \\nConfiguration in eShop \\nThe eShopOnContainers a\", \"pplication includes local application settings files with each microservice. \\nThese files are checke\", \"d into source control, but don\\u2019t include production secrets such as connection \\nstrings or API keys.\", \" In production, individual settings may be overwritten with per-service environment \\nvariables. Inje\", \"cting secrets in environment variables is a common practice for hosted applications, but \\ndoesn\\u2019t pr\", \"ovide a central configuration store. To support centralized management of configuration \\nsettings, e\", \"ach microservice includes a setting to toggle between its use of local settings or Azure Key \\nVault \", \"settings. \\nReferences \\n\\u2022 \\nThe eShopOnContainers Architecture \\n\\u2022 \\nOrchestrating microservices and mul\", \"ti-container applications for high scalability and \\navailability \\n\\u2022 \\nAzure API Management \\n\\u2022 \\nAzure \", \"SQL Database Overview \\n\\u2022 \\nAzure Cache for Redis \\n\\u2022 \\nAzure Cosmos DB\\u2019s API for MongoDB \\n \\n35 \\nCHAPTER\", \" 2 | Introducing eShopOnContainers reference app \\n \\n\\u2022 \\nAzure Service Bus \\n\\u2022 \\nAzure Monitor overview \", \"\\n\\u2022 \\neShopOnContainers: Create Kubernetes cluster in AKS \\n\\u2022 \\neShopOnContainers: Azure Dev Spaces \\n\\u2022 \\n\", \"Azure Dev Spaces \\n \\n36 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nCHAPTER 3 \\nScaling cloud-na\", \"tive \\napplications \\nOne of the most-often touted advantages of moving to a cloud hosting environment\", \" is scalability. \\nScalability, or the ability for an application to accept additional user load with\", \"out compromising \\nperformance for each user. It\\u2019s most often achieved by breaking up an application \", \"into small pieces \\nthat can each be given whatever resources they require. Cloud vendors enable mass\", \"ive scalability \\nanytime and anywhere in the world. \\nIn this chapter, we discuss technologies that e\", \"nable cloud-native applications to scale to meet user \\ndemand. These technologies include: \\n\\u2022 \\nConta\", \"iners \\n\\u2022 \\nOrchestrators \\n\\u2022 \\nServerless computing \\nLeveraging containers and orchestrators \\nContainer\", \"s and orchestrators are designed to solve problems common to monolithic deployment \\napproaches. \\nCha\", \"llenges with monolithic deployments \\nTraditionally, most applications have been deployed as a single\", \" unit. Such applications are referred to \\nas a monolith. This general approach of deploying applicat\", \"ions as single units even if they\\u2019re \\ncomposed of multiple modules or assemblies is known as monolit\", \"hic architecture, as shown in Figure \\n3-1. \\n \\n37 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\n \", \"\\nFigure 3-1. Monolithic architecture. \\nAlthough they have the benefit of simplicity, monolithic arch\", \"itectures face many challenges: \\nDeployment \\nAdditionally, they require a restart of the application\", \", which may temporarily impact availability if \\nzero-downtime techniques are not applied while deplo\", \"ying. \\nScaling \\nA monolithic application is hosted entirely on a single machine instance, often requ\", \"iring high-\\ncapability hardware. If any part of the monolith requires scaling, another copy of the e\", \"ntire application \\nmust be deployed to another machine. With a monolith, you can\\u2019t scale application\", \" components \\nindividually - it\\u2019s all or nothing. Scaling components that don\\u2019t require scaling resul\", \"ts in inefficient and \\ncostly resource usage. \\nEnvironment \\nMonolithic applications are typically de\", \"ployed to a hosting environment with a pre-installed operating \\nsystem, runtime, and library depende\", \"ncies. This environment may not match that upon which the \\napplication was developed or tested. Inco\", \"nsistencies across application environments are a common \\nsource of problems for monolithic deployme\", \"nts. \\nCoupling \\nA monolithic application is likely to experience high coupling across its functional\", \" components. \\nWithout hard boundaries, system changes often result in unintended and costly side eff\", \"ects. New \\nfeatures/fixes become tricky, time-consuming, and expensive to implement. Updates require\", \" extensive \\ntesting. Coupling also makes it difficult to refactor components or swap in alternative \", \"\\nimplementations. Even when constructed with a strict separation of concerns, architectural erosion \", \"\\nsets in as the monolithic code base deteriorates with never-ending \\u201cspecial cases.\\u201d \\n \\n38 \\nCHAPTER \", \"3 | Scaling cloud-native applications \\n \\nPlatform lock-in \\nA monolithic application is constructed w\", \"ith a single technology stack. While offering uniformity, this \\ncommitment can become a barrier to i\", \"nnovation. New features and components will be built using \\nthe application\\u2019s current stack - even w\", \"hen more modern technologies may be a better choice. A \\nlonger-term risk is your technology stack be\", \"coming outdated and obsolete. Rearchitecting an entire \\napplication to a new, more modern platform i\", \"s at best expensive and risky. \\nWhat are the benefits of containers and orchestrators? \\nWe introduce\", \"d containers in Chapter 1. We highlighted how the Cloud Native Computing Foundation \\n(CNCF) ranks co\", \"ntainerization as the first step in their Cloud-Native Trail Map - guidance for \\nenterprises beginni\", \"ng their cloud-native journey. In this section, we discuss the benefits of containers. \\nDocker is th\", \"e most popular container management platform. It works with containers on both Linux or \\nWindows. Co\", \"ntainers provide separate but reproducible application environments that run the same \\nway on any sy\", \"stem. This aspect makes them perfect for developing and hosting cloud-native services. \\nContainers a\", \"re isolated from one another. Two containers on the same host hardware can have \\ndifferent versions \", \"of software, without causing conflicts. \\nContainers are defined by simple text-based files that beco\", \"me project artifacts and are checked into \\nsource control. While full servers and virtual machines r\", \"equire manual effort to update, containers are \\neasily version-controlled. Apps built to run in cont\", \"ainers can be developed, tested, and deployed \\nusing automated tools as part of a build pipeline. \\nC\", \"ontainers are immutable. Once you define a container, you can recreate and run it exactly the same \\n\", \"way. This immutability lends itself to component-based design. If some parts of an application evolv\", \"e \\ndifferently than others, why redeploy the entire app when you can just deploy the parts that chan\", \"ge \\nmost frequently? Different features and cross-cutting concerns of an app can be broken up into \\n\", \"separate units. Figure 3-2 shows how a monolithic app can take advantage of containers and \\nmicroser\", \"vices by delegating certain features or functionality. The remaining functionality in the app \\nitsel\", \"f has also been containerized. \\n \\n39 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\n \\nFigure 3-2.\", \" Decomposing a monolithic app to embrace microservices. \\nEach cloud-native service is built and depl\", \"oyed in a separate container. Each can update as needed. \\nIndividual services can be hosted on nodes\", \" with resources appropriate to each service. The \\nenvironment each service runs in is immutable, sha\", \"red across dev, test, and production environments, \\nand easily versioned. Coupling between different\", \" areas of the application occurs explicitly as calls or \\nmessages between services, not compile-time\", \" dependencies within the monolith. You can also choose \\nthe technology that best suites a given capa\", \"bility without requiring changes to the rest of the app. \\nContainerized services require automated m\", \"anagement. It wouldn\\u2019t be feasible to manually administer \\na large set of independently deployed con\", \"tainers. For example, consider the following tasks: \\n\\u2022 \\nHow will container instances be provisioned \", \"across a cluster of many machines? \\n\\u2022 \\nOnce deployed, how will containers discover and communicate w\", \"ith each other? \\n\\u2022 \\nHow can containers scale in or out on-demand? \\n\\u2022 \\nHow do you monitor the health \", \"of each container? \\n\\u2022 \\nHow do you protect a container against hardware and software failures? \\n\\u2022 \\nHo\", \"w do upgrade containers for a live application with zero downtime? \\nContainer orchestrators address \", \"and automate these and other concerns. \\nIn the cloud-native eco-system, Kubernetes has become the de\", \" facto container orchestrator. It\\u2019s an \\nopen-source platform managed by the Cloud Native Computing F\", \"oundation (CNCF). Kubernetes \\nautomates the deployment, scaling, and operational concerns of contain\", \"erized workloads across a \\nmachine cluster. However, installing and managing Kubernetes is notorious\", \"ly complex. \\n \\n40 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nA much better approach is to lev\", \"erage Kubernetes as a managed service from a cloud vendor. The \\nAzure cloud features a fully managed\", \" Kubernetes platform entitled Azure Kubernetes Service (AKS). \\nAKS abstracts the complexity and oper\", \"ational overhead of managing Kubernetes. You consume \\nKubernetes as a cloud service; Microsoft takes\", \" responsibility for managing and supporting it. AKS also \\ntightly integrates with other Azure servic\", \"es and dev tools. \\nAKS is a cluster-based technology. A pool of federated virtual machines, or nodes\", \", is deployed to the \\nAzure cloud. Together they form a highly available environment, or cluster. Th\", \"e cluster appears as a \\nseamless, single entity to your cloud-native application. Under the hood, AK\", \"S deploys your \\ncontainerized services across these nodes following a predefined strategy that evenl\", \"y distributes the \\nload. \\nWhat are the scaling benefits? \\nServices built on containers can leverage \", \"scaling benefits provided by orchestration tools like \\nKubernetes. By design containers only know ab\", \"out themselves. Once you have multiple containers \\nthat need to work together, you should organize t\", \"hem at a higher level. Organizing large numbers of \\ncontainers and their shared dependencies, such a\", \"s network configuration, is where orchestration tools \\ncome in to save the day! Kubernetes creates a\", \"n abstraction layer over groups of containers and \\norganizes them into pods. Pods run on worker mach\", \"ines referred to as nodes. This organized structure \\nis referred to as a cluster. Figure 3-3 shows t\", \"he different components of a Kubernetes cluster. \\n \\nFigure 3-3. Kubernetes cluster components. \\nScal\", \"ing containerized workloads is a key feature of container orchestrators. AKS supports automatic \\nsca\", \"ling across two dimensions: Container instances and compute nodes. Together they give AKS the \\nabili\", \"ty to quickly and efficiently respond to spikes in demand and add additional resources. We \\ndiscuss \", \"scaling in AKS later in this chapter. \\n \\n41 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nDeclar\", \"ative versus imperative \\nKubernetes supports both declarative and imperative configuration. The impe\", \"rative approach involves \\nrunning various commands that tell Kubernetes what to do each step of the \", \"way. Run this image. \\nDelete this pod. Expose this port. With the declarative approach, you create a\", \" configuration file, called \\na manifest, to describe what you want instead of what to do. Kubernetes\", \" reads the manifest and \\ntransforms your desired end state into actual end state. \\nImperative comman\", \"ds are great for learning and interactive experimentation. However, you\\u2019ll want to \\ndeclaratively cr\", \"eate Kubernetes manifest files to embrace an infrastructure as code approach, \\nproviding for reliabl\", \"e and repeatable deployments. The manifest file becomes a project artifact and is \\nused in your CI/C\", \"D pipeline for automating Kubernetes deployments. \\nIf you\\u2019ve already configured your cluster using i\", \"mperative commands, you can export a declarative \\nmanifest by using kubectl get svc SERVICENAME -o y\", \"aml > service.yaml. This command produces a \\nmanifest similar to one shown below: \\napiVersion: v1 \\nk\", \"ind: Service \\nmetadata: \\n  creationTimestamp: \\\"2019-09-13T13:58:47Z\\\" \\n  labels: \\n    component: apis\", \"erver \\n    provider: kubernetes \\n  name: kubernetes \\n  namespace: default \\n  resourceVersion: \\\"153\\\" \", \"\\n  selfLink: /api/v1/namespaces/default/services/kubernetes \\n  uid: 9b1fac62-d62e-11e9-8968-00155d38\", \"010d \\nspec: \\n  clusterIP: 10.96.0.1 \\n  ports: \\n  - name: https \\n    port: 443 \\n    protocol: TCP \\n  \", \"  targetPort: 6443 \\n  sessionAffinity: None \\n  type: ClusterIP \\nstatus: \\n  loadBalancer: {} \\nWhen us\", \"ing declarative configuration, you can preview the changes that will be made before \\ncommitting them\", \" by using kubectl diff -f FOLDERNAME against the folder where your configuration \\nfiles are located.\", \" Once you\\u2019re sure you want to apply the changes, run kubectl apply -f FOLDERNAME. \\nAdd -R to recursi\", \"vely process a folder hierarchy. \\nYou can also use declarative configuration with other Kubernetes f\", \"eatures, one of which being \\ndeployments. Declarative deployments help manage releases, updates, and\", \" scaling. They instruct the \\nKubernetes deployment controller on how to deploy new changes, scale ou\", \"t load, or roll back to a \\nprevious revision. If a cluster is unstable, a declarative deployment wil\", \"l automatically return the cluster \\nback to a desired state. For example, if a node should crash, th\", \"e deployment mechanism will redeploy \\na replacement to achieve your desired state \\n \\n42 \\nCHAPTER 3 |\", \" Scaling cloud-native applications \\n \\nUsing declarative configuration allows infrastructure to be re\", \"presented as code that can be checked in \\nand versioned alongside the application code. It provides \", \"improved change control and better \\nsupport for continuous deployment using a build and deploy pipel\", \"ine. \\nWhat scenarios are ideal for containers and orchestrators? \\nThe following scenarios are ideal \", \"for using containers and orchestrators. \\nApplications requiring high uptime and scalability \\nIndivid\", \"ual applications that have high uptime and scalability requirements are ideal candidates for \\ncloud-\", \"native architectures using microservices, containers, and orchestrators. They can be developed \\nin c\", \"ontainers, tested across versioned environments, and deployed into production with zero \\ndowntime. T\", \"he use of Kubernetes clusters ensures such apps can also scale on demand and recover \\nautomatically \", \"from node failures. \\nLarge numbers of applications \\nOrganizations that deploy and maintain large num\", \"bers of applications benefit from containers and \\norchestrators. The up front effort of setting up c\", \"ontainerized environments and Kubernetes clusters is \\nprimarily a fixed cost. Deploying, maintaining\", \", and updating individual applications has a cost that \\nvaries with the number of applications. Beyo\", \"nd a few applications, the complexity of maintaining \\ncustom applications manually exceeds the cost \", \"of implementing a solution using containers and \\norchestrators. \\nWhen should you avoid using contain\", \"ers and orchestrators? \\nIf you\\u2019re unable to build your application following the Twelve-Factor App p\", \"rinciples, you should \\nconsider avoiding containers and orchestrators. In these cases, consider a VM\", \"-based hosting platform, \\nor possibly some hybrid system. With it, you can always spin off certain p\", \"ieces of functionality into \\nseparate containers or even serverless functions. \\nDevelopment resource\", \"s \\nThis section shows a short list of development resources that may help you get started using \\ncon\", \"tainers and orchestrators for your next application. If you\\u2019re looking for guidance on how to \\ndesig\", \"n your cloud-native microservices architecture app, read this book\\u2019s companion, .NET \\nMicroservices:\", \" Architecture for Containerized .NET Applications. \\nLocal Kubernetes Development \\nKubernetes deploym\", \"ents provide great value in production environments, but can also run locally on \\nyour development m\", \"achine. While you may work on individual microservices independently, there \\nmay be times when you\\u2019l\", \"l need to run the entire system locally - just as it will run when deployed to \\nproduction. There ar\", \"e several tools that can help: Minikube and Docker Desktop. Visual Studio also \\nprovides tooling for\", \" Docker development. \\n \\n43 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nMinikube \\nWhat is Minik\", \"ube? The Minikube project says \\u201cMinikube implements a local Kubernetes cluster on \\nmacOS, Linux, and\", \" Windows.\\u201d Its primary goals are \\u201cto be the best tool for local Kubernetes \\napplication development \", \"and to support all Kubernetes features that fit.\\u201d Installing Minikube is \\nseparate from Docker, but \", \"Minikube supports different hypervisors than Docker Desktop supports. \\nThe following Kubernetes feat\", \"ures are currently supported by Minikube: \\n\\u2022 \\nDNS \\n\\u2022 \\nNodePorts \\n\\u2022 \\nConfigMaps and secrets \\n\\u2022 \\nDashb\", \"oards \\n\\u2022 \\nContainer runtimes: Docker, rkt, CRI-O, and containerd \\n\\u2022 \\nEnabling Container Network Inte\", \"rface (CNI) \\n\\u2022 \\nIngress \\nAfter installing Minikube, you can quickly start using it by running the mi\", \"nikube start command, which \\ndownloads an image and start the local Kubernetes cluster. Once the clu\", \"ster is started, you interact \\nwith it using the standard Kubernetes kubectl commands. \\nDocker Deskt\", \"op \\nYou can also work with Kubernetes directly from Docker Desktop on Windows. It is your only optio\", \"n if \\nyou\\u2019re using Windows Containers, and is a great choice for non-Windows containers as well. Fig\", \"ure 3-\\n4 shows how to enable local Kubernetes support when running Docker Desktop. \\n \\nFigure 3-4. Co\", \"nfiguring Kubernetes in Docker Desktop. \\n \\n44 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nDock\", \"er Desktop is the most popular tool for configuring and running containerized apps locally. \\nWhen yo\", \"u work with Docker Desktop, you can develop locally against the exact same set of Docker \\ncontainer \", \"images that you\\u2019ll deploy to production. Docker Desktop is designed to \\u201cbuild, test, and \\nship\\u201d cont\", \"ainerized apps locally. It supports both Linux and Windows containers. Once you push your \\nimages to\", \" an image registry, like Azure Container Registry or Docker Hub, AKS can pull and deploy \\nthem to pr\", \"oduction. \\nVisual Studio Docker Tooling \\nVisual Studio supports Docker development for web-based app\", \"lications. When you create a new \\nASP.NET Core application, you have an option to configure it with \", \"Docker support, as shown in Figure \\n3-5. \\n \\nFigure 3-5. Visual Studio Enable Docker Support \\nWhen th\", \"is option is selected, the project is created with a Dockerfile in its root, which can be used to \\nb\", \"uild and host the app in a Docker container. An example Dockerfile is shown in Figure 3-6. \\nFROM mcr\", \".microsoft.com/dotnet/aspnet:7.0 AS base \\nWORKDIR /app \\nEXPOSE 80 \\nEXPOSE 443 \\n \\nFROM mcr.microsoft.\", \"com/dotnet/sdk:7.0 AS build \\nWORKDIR /src \\nCOPY [\\\"eShopWeb/eShopWeb.csproj\\\", \\\"eShopWeb/\\\"] \\nRUN dotne\", \"t restore \\\"eShopWeb/eShopWeb.csproj\\\" \\nCOPY . . \\nWORKDIR \\\"/src/eShopWeb\\\" \\nRUN dotnet build \\\"eShopWeb.\", \"csproj\\\" -c Release -o /app/build \\n \\nFROM build AS publish \\nRUN dotnet publish \\\"eShopWeb.csproj\\\" -c R\", \"elease -o /app/publish \\n \\n \\n45 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nFROM base AS final \", \"\\nWORKDIR /app \\nCOPY --from=publish /app/publish . \\nENTRYPOINT [\\\"dotnet\\\", \\\"eShopWeb.dll\\\"] \\nFigure 3-6\", \". Visual Studio generated Dockerfile \\nOnce support is added, you can run your application in a Docke\", \"r container in Visual Studio. Figure 3-7 \\nshows the different run options available from a new ASP.N\", \"ET Core project created with Docker \\nsupport added. \\n \\nFigure 3-7. Visual Studio Docker Run Options \", \"\\nAlso, at any time you can add Docker support to an existing ASP.NET Core application. From the \\nVis\", \"ual Studio Solution Explorer, right-click on the project and select Add > Docker Support, as shown \\n\", \"in Figure 3-8. \\n \\n46 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\n \\nFigure 3-8. Adding Docker s\", \"upport to Visual Studio \\nVisual Studio Code Docker Tooling \\nThere are many extensions available for \", \"Visual Studio Code that support Docker development. \\nMicrosoft provides the Docker for Visual Studio\", \" Code extension. This extension simplifies the process \\nof adding container support to applications.\", \" It scaffolds required files, builds Docker images, and \\nenables you to debug your app inside a cont\", \"ainer. The extension features a visual explorer that makes \\nit easy to take actions on containers an\", \"d images such as start, stop, inspect, remove, and more. The \\nextension also supports Docker Compose\", \" enabling you to manage multiple running containers as a \\nsingle unit. \\nLeveraging serverless functi\", \"ons \\nIn the spectrum from managing physical machines to leveraging cloud capabilities, serverless li\", \"ves at \\nthe extreme end. Your only responsibility is your code, and you only pay when your code runs\", \". Azure \\nFunctions provides a way to build serverless capabilities into your cloud-native applicatio\", \"ns. \\n \\n47 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nWhat is serverless? \\nServerless is a rel\", \"atively new service model of cloud computing. It doesn\\u2019t mean that servers are \\noptional - your code\", \" still runs on a server somewhere. The distinction is that the application team no \\nlonger concerns \", \"itself with managing server infrastructure. Instead, the cloud vendor own this \\nresponsibility. The \", \"development team increases its productivity by delivering business solutions to \\ncustomers, not plum\", \"bing. \\nServerless computing uses event-triggered stateless containers to host your services. They ca\", \"n scale \\nout and in to meet demand as-needed. Serverless platforms like Azure Functions have tight \\n\", \"integration with other Azure services like queues, events, and storage. \\nWhat challenges are solved \", \"by serverless? \\nServerless platforms address many time-consuming and expensive concerns: \\n\\u2022 \\nPurchas\", \"ing machines and software licenses \\n\\u2022 \\nHousing, securing, configuring, and maintaining the machines \", \"and their networking, power, \\nand A/C requirements \\n\\u2022 \\nPatching and upgrading operating systems and \", \"software \\n\\u2022 \\nConfiguring web servers or machine services to host application software \\n\\u2022 \\nConfigurin\", \"g application software within its platform \\nMany companies allocate large budgets to support hardwar\", \"e infrastructure concerns. Moving to the \\ncloud can help reduce these costs; shifting applications t\", \"o serverless can help eliminate them. \\nWhat is the difference between a microservice and a serverles\", \"s \\nfunction? \\nTypically, a microservice encapsulates a business capability, such as a shopping cart \", \"for an online \\neCommerce site. It exposes multiple operations that enable a user to manage their sho\", \"pping \\nexperience. A function, however, is a small, lightweight block of code that executes a single\", \"-purpose \\noperation in response to an event. Microservices are typically constructed to respond to r\", \"equests, \\noften from an interface. Requests can be HTTP Rest- or gRPC-based. Serverless services res\", \"pond to \\nevents. Its event-driven architecture is ideal for processing short-running, background tas\", \"ks. \\nWhat scenarios are appropriate for serverless? \\nServerless exposes individual short-running fun\", \"ctions that are invoked in response to a trigger. This \\nmakes them ideal for processing background t\", \"asks. \\nAn application might need to send an email as a step in a workflow. Instead of sending the \\nn\", \"otification as part of a microservice request, place the message details onto a queue. An Azure \\nFun\", \"ction can dequeue the message and asynchronously send the email. Doing so could improve the \\nperform\", \"ance and scalability of the microservice. Queue-based load leveling can be implemented to \\navoid bot\", \"tlenecks related to sending the emails. Additionally, this stand-alone service could be reused \\nas a\", \" utility across many different applications. \\n \\n48 \\nCHAPTER 3 | Scaling cloud-native applications \\n \", \"\\nAsynchronous messaging from queues and topics is a common pattern to trigger serverless functions. \", \"\\nHowever, Azure Functions can be triggered by other events, such as changes to Azure Blob Storage. A\", \" \\nservice that supports image uploads could have an Azure Function responsible for optimizing the \\ni\", \"mage size. The function could be triggered directly by inserts into Azure Blob Storage, keeping \\ncom\", \"plexity out of the microservice operations. \\nMany services have long-running processes as part of th\", \"eir workflows. Often these tasks are done as \\npart of the user\\u2019s interaction with the application. T\", \"hese tasks can force the user to wait, negatively \\nimpacting their experience. Serverless computing \", \"provides a great way to move slower tasks outside \\nof the user interaction loop. These tasks can sca\", \"le with demand without requiring the entire \\napplication to scale. \\nWhen should you avoid serverless\", \"? \\nServerless solutions provision and scale on demand. When a new instance is invoked, cold starts a\", \"re a \\ncommon issue. A cold start is the period of time it takes to provision this instance. Normally\", \", this delay \\nmight be a few seconds, but can be longer depending on various factors. Once provision\", \"ed, a single \\ninstance is kept alive as long as it receives periodic requests. But, if a service is \", \"called less frequently, \\nAzure may remove it from memory and require a cold start when reinvoked. Co\", \"ld starts are also \\nrequired when a function scales out to a new instance. \\nFigure 3-9 shows a cold-\", \"start pattern. Note the extra steps required when the app is cold. \\n \\nFigure 3-9. Cold start versus \", \"warm start. \\nTo avoid cold starts entirely, you might switch from a consumption plan to a dedicated \", \"plan. You can \\nalso configure one or more pre-warmed instances with the premium plan upgrade. In the\", \"se cases, \\nwhen you need to add another instance, it\\u2019s already up and ready to go. These options can\", \" help \\nmitigate the cold start issue associated with serverless computing. \\n \\n49 \\nCHAPTER 3 | Scalin\", \"g cloud-native applications \\n \\nCloud providers bill for serverless based on compute execution time a\", \"nd consumed memory. Long \\nrunning operations or high memory consumption workloads aren\\u2019t always the \", \"best candidates for \\nserverless. Serverless functions favor small chunks of work that can complete q\", \"uickly. Most serverless \\nplatforms require individual functions to complete within a few minutes. Az\", \"ure Functions defaults to a \\n5-minute time-out duration, which can be configured up to 10 minutes. T\", \"he Azure Functions premium \\nplan can mitigate this issue as well, defaulting time-outs to 30 minutes\", \" with an unbounded higher \\nlimit that can be configured. Compute time isn\\u2019t calendar time. More adva\", \"nced functions using the \\nAzure Durable Functions framework may pause execution over a course of sev\", \"eral days. The billing is \\nbased on actual execution time - when the function wakes up and resumes p\", \"rocessing. \\nFinally, leveraging Azure Functions for application tasks adds complexity. It\\u2019s wise to \", \"first architect \\nyour application with a modular, loosely coupled design. Then, identify if there ar\", \"e benefits serverless \\nwould offer that justify the additional complexity. \\nCombining containers and\", \" serverless approaches \\nCloud-native applications typically implement services leveraging containers\", \" and orchestration. There \\nare often opportunities to expose some of the application\\u2019s services as A\", \"zure Functions. However, \\nwith a cloud-native app deployed to Kubernetes, it would be nice to levera\", \"ge Azure Functions within \\nthis same toolset. Fortunately, you can wrap Azure Functions inside Docke\", \"r containers and deploy \\nthem using the same processes and tools as the rest of your Kubernetes-base\", \"d app. \\nWhen does it make sense to use containers with serverless? \\nYour Azure Function has no knowl\", \"edge of the platform on which it\\u2019s deployed. For some scenarios, \\nyou may have specific requirements\", \" and need to customize the environment on which your function \\ncode will run. You\\u2019ll need a custom i\", \"mage that supports dependencies or a configuration not \\nsupported by the default image. In these cas\", \"es, it makes sense to deploy your function in a custom \\nDocker container. \\nWhen should you avoid usi\", \"ng containers with Azure Functions? \\nIf you want to use consumption billing, you can\\u2019t run your func\", \"tion in a container. What\\u2019s more, if you \\ndeploy your function to a Kubernetes cluster, you\\u2019ll no lo\", \"nger benefit from the built-in scaling \\nprovided by Azure Functions. You\\u2019ll need to use Kubernetes\\u2019 \", \"scaling features, described earlier in this \\nchapter. \\nHow to combine serverless and Docker containe\", \"rs \\nTo wrap an Azure Function in a Docker container, install the Azure Functions Core Tools and then\", \" run \\nthe following command: \\nfunc init ProjectName --worker-runtime dotnet --docker \\nWhen the proje\", \"ct is created, it will include a Dockerfile and the worker runtime configured to dotnet. \\nNow, you c\", \"an create and test your function locally. Build and run it using the docker build and docker \\n \\n50 \\n\", \"CHAPTER 3 | Scaling cloud-native applications \\n \\nrun commands. For detailed steps to get started bui\", \"lding Azure Functions with Docker support, see \\nthe Create a function on Linux using a custom image \", \"tutorial. \\nHow to combine serverless and Kubernetes with KEDA \\nIn this chapter, you\\u2019ve seen that the\", \" Azure Functions\\u2019 platform automatically scales out to meet \\ndemand. When deploying containerized fu\", \"nctions to AKS, however, you lose the built-in scaling \\nfunctionality. To the rescue comes Kubernete\", \"s-based Event Driven (KEDA). It enables fine-grained \\nautoscaling for event-driven Kubernetes worklo\", \"ads, including containerized functions. \\nKEDA provides event-driven scaling functionality to the Fun\", \"ctions\\u2019 runtime in a Docker container. \\nKEDA can scale from zero instances (when no events are occur\", \"ring) out to n instances, based on load. \\nIt enables autoscaling by exposing custom metrics to the K\", \"ubernetes autoscaler (Horizontal Pod \\nAutoscaler). Using Functions containers with KEDA makes it pos\", \"sible to replicate serverless function \\ncapabilities in any Kubernetes cluster. \\nIt\\u2019s worth noting t\", \"hat the KEDA project is now managed by the Cloud Native Computing Foundation \\n(CNCF). \\nDeploying con\", \"tainers in Azure \\nWe\\u2019ve discussed containers in this chapter and in chapter 1. We\\u2019ve seen that conta\", \"iners provide many \\nbenefits to cloud-native applications, including portability. In the Azure cloud\", \", you can deploy the \\nsame containerized services across staging and production environments. Azure \", \"provides several \\noptions for hosting these containerized workloads: \\n\\u2022 \\nAzure Kubernetes Services (\", \"AKS) \\n\\u2022 \\nAzure Container Instance (ACI) \\n\\u2022 \\nAzure Web Apps for Containers \\nAzure Container Registry \", \"\\nWhen containerizing a microservice, you first build a container \\u201cimage.\\u201d The image is a binary \\nrep\", \"resentation of the service code, dependencies, and runtime. While you can manually create an \\nimage \", \"using the Docker Build command from the Docker API, a better approach is to create it as part \\nof an\", \" automated build process. \\nOnce created, container images are stored in container registries. They e\", \"nable you to build, store, and \\nmanage container images. There are many registries available, both p\", \"ublic and private. Azure \\nContainer Registry (ACR) is a fully managed container registry service in \", \"the Azure cloud. It persists \\nyour images inside the Azure network, reducing the time to deploy them\", \" to Azure container hosts. \\nYou can also secure them using the same security and identity procedures\", \" that you use for other \\nAzure resources. \\nYou create an Azure Container Registry using the Azure po\", \"rtal, Azure CLI, or PowerShell tools. \\nCreating a registry in Azure is simple. It requires an Azure \", \"subscription, resource group, and a unique \\n \\n51 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nn\", \"ame. Figure 3-10 shows the basic options for creating a registry, which will be hosted at \\nregistryn\", \"ame.azurecr.io. \\n \\nFigure 3-10. Create container registry \\nOnce you\\u2019ve created the registry, you\\u2019ll \", \"need to authenticate with it before you can use it. Typically, \\nyou\\u2019ll log into the registry using t\", \"he Azure CLI command: \\naz acr login --name *registryname* \\nOnce authenticated, you can use docker co\", \"mmands to push container images to it. Before you can do \\nso, however, you must tag your image with \", \"the fully qualified name (URL) of your ACR login server. It \\nwill have the format registryname.azure\", \"cr.io. \\ndocker tag mycontainer myregistry.azurecr.io/mycontainer:v1 \\nAfter you\\u2019ve tagged the image, \", \"you use the docker push command to push the image to your ACR \\ninstance. \\ndocker push myregistry.azu\", \"recr.io/mycontainer:v1 \\n \\n52 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nAfter you push an ima\", \"ge to the registry, it\\u2019s a good idea to remove the image from your local Docker \\nenvironment, using \", \"this command: \\ndocker rmi myregistry.azurecr.io/mycontainer:v1 \\nAs a best practice, you shouldn\\u2019t ma\", \"nually push images to a container registry. Instead, use a build \\npipeline defined in a tool like Gi\", \"tHub or Azure DevOps. Learn more in the Cloud-Native DevOps \\nchapter. \\nACR Tasks \\nACR Tasks is a set\", \" of features available from the Azure Container Registry. It extends your inner-loop \\ndevelopment cy\", \"cle by building and managing container images in the Azure cloud. Instead of \\ninvoking a docker buil\", \"d and docker push locally on your development machine, they\\u2019re automatically \\nhandled by ACR Tasks i\", \"n the cloud. \\nThe following AZ CLI command both builds a container image and pushes it to ACR: \\n# cr\", \"eate a container registry \\naz acr create --resource-group myResourceGroup --name myContainerRegistry\", \"008 --sku \\nBasic \\n \\n# build container image in ACR and push it into your container registry \\naz acr \", \"build --image sample/hello-world:v1  --registry myContainerRegistry008 --file \\nDockerfile . \\nAs you \", \"can see from the previous command block, there\\u2019s no need to install Docker Desktop on your \\ndevelopm\", \"ent machine. Additionally, you can configure ACR Task triggers to rebuild containers images \\non both\", \" source code and base image updates. \\nAzure Kubernetes Service \\nWe discussed Azure Kubernetes Servic\", \"e (AKS) at length in this chapter. We\\u2019ve seen that it\\u2019s the de \\nfacto container orchestrator managin\", \"g containerized cloud-native applications. \\nOnce you deploy an image to a registry, such as ACR, you\", \" can configure AKS to automatically pull and \\ndeploy it. With a CI/CD pipeline in place, you might c\", \"onfigure a canary release strategy to minimize \\nthe risk involved when rapidly deploying updates. Th\", \"e new version of the app is initially configured in \\nproduction with no traffic routed to it. Then, \", \"the system will route a small percentage of users to the \\nnewly deployed version. As the team gains \", \"confidence in the new version, it can roll out more \\ninstances and retire the old. AKS easily suppor\", \"ts this style of deployment. \\nAs with most resources in Azure, you can create an Azure Kubernetes Se\", \"rvice cluster using the portal, \\ncommand-line, or automation tools like Helm or Terraform. To get st\", \"arted with a new cluster, you \\nneed to provide the following information: \\n\\u2022 \\nAzure subscription \\n\\u2022 \", \"\\nResource group \\n\\u2022 \\nKubernetes cluster name \\n\\u2022 \\nRegion \\n \\n53 \\nCHAPTER 3 | Scaling cloud-native appli\", \"cations \\n \\n\\u2022 \\nKubernetes version \\n\\u2022 \\nDNS name prefix \\n\\u2022 \\nNode size \\n\\u2022 \\nNode count \\nThis information \", \"is sufficient to get started. As part of the creation process in the Azure portal, you can \\nalso con\", \"figure options for the following features of your cluster: \\n\\u2022 \\nScale \\n\\u2022 \\nAuthentication \\n\\u2022 \\nNetworki\", \"ng \\n\\u2022 \\nMonitoring \\n\\u2022 \\nTags \\nThis quickstart walks through deploying an AKS cluster using the Azure p\", \"ortal. \\nAzure Bridge to Kubernetes \\nCloud-native applications can grow large and complex, requiring \", \"significant compute resources to \\nrun. In these scenarios, the entire application can\\u2019t be hosted on\", \" a development machine (especially a \\nlaptop). Azure Bridge to Kubernetes addresses the shortcoming.\", \" It enables developers to work with a \\nlocal version of their service while hosting the entire appli\", \"cation in an AKS development cluster. \\nWhen ready, developers test their changes locally while runni\", \"ng against the full application in the AKS \\ncluster - without replicating dependencies. Under the ho\", \"od, the bridge merges code from the local \\nmachine with services in AKS. Developers can rapidly iter\", \"ate and debug code directly in Kubernetes \\nusing Visual Studio or Visual Studio Code. \\nGabe Monroy, \", \"former VP of Product Management at Microsoft, describes it well: \\nImagine you\\u2019re a new employee tryi\", \"ng to fix a bug in a complex microservices application consisting \\nof dozens of components, each wit\", \"h their own configuration and backing services. To get started, you \\nmust configure your local devel\", \"opment environment so that it can mimic production including setting \\nup your IDE, building tool cha\", \"in, containerized service dependencies, a local Kubernetes environment, \\nmocks for backing services,\", \" and more. With all the time involved setting up your development \\nenvironment, fixing that first bu\", \"g could take days! Or you could just use Bridge to Kubernetes and \\nAKS. \\nScaling containers and serv\", \"erless applications \\nThere are two ways to scale an application: up or out. The former refers to add\", \"ing capacity to a single \\nresource, while the latter refers to adding more resources to increase cap\", \"acity. \\nThe simple solution: scaling up \\nUpgrading an existing host server with increased CPU, memor\", \"y, disk I/O speed, and network I/O \\nspeed is known as scaling up. Scaling up a cloud-native applicat\", \"ion involves choosing more capable \\n \\n54 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nresources\", \" from the cloud vendor. For example, you can create a new node pool with larger VMs in \\nyour Kuberne\", \"tes cluster. Then, migrate your containerized services to the new pool. \\nServerless apps scale up by\", \" choosing the premium Functions plan or premium instance sizes from a \\ndedicated app service plan. \\n\", \"Scaling out cloud-native apps \\nCloud-native applications often experience large fluctuations in dema\", \"nd and require scale on a \\nmoment\\u2019s notice. They favor scaling out. Scaling out is done horizontally\", \" by adding additional \\nmachines (called nodes) or application instances to an existing cluster. In K\", \"ubernetes, you can scale \\nmanually by adjusting configuration settings for the app (for example, sca\", \"ling a node pool), or \\nthrough autoscaling. \\nAKS clusters can autoscale in one of two ways: \\nFirst, \", \"the Horizontal Pod Autoscaler monitors resource demand and automatically scales your POD \\nreplicas t\", \"o meet it. When traffic increases, additional replicas are automatically provisioned to scale \\nout y\", \"our services. Likewise, when demand decreases, they\\u2019re removed to scale-in your services. You \\ndefin\", \"e the metric on which to scale, for example, CPU usage. You can also specify the minimum and \\nmaximu\", \"m number of replicas to run. AKS monitors that metric and scales accordingly. \\nNext, the AKS Cluster\", \" Autoscaler feature enables you to automatically scale compute nodes across a \\nKubernetes cluster to\", \" meet demand. With it, you can automatically add new VMs to the underlying \\nAzure Virtual Machine Sc\", \"ale Set whenever more compute capacity of is required. It also removes \\nnodes when no longer require\", \"d. \\nFigure 3-11 shows the relationship between these two scaling services. \\n \\nFigure 3-11. Scaling o\", \"ut an App Service plan. \\n \\n55 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\nWorking together, bo\", \"th ensure an optimal number of container instances and compute nodes to \\nsupport fluctuating demand.\", \" The horizontal pod autoscaler optimizes the number of pods required. \\nThe cluster autoscaler optimi\", \"zes the number of nodes required. \\nScaling Azure Functions \\nAzure Functions automatically scale out \", \"upon demand. Server resources are dynamically allocated and \\nremoved based on the number of triggere\", \"d events. You\\u2019re only charged for compute resources \\nconsumed when your functions run. Billing is ba\", \"sed upon the number of executions, execution time, \\nand memory used. \\nWhile the default consumption \", \"plan provides an economical and scalable solution for most apps, the \\npremium option allows develope\", \"rs flexibility for custom Azure Functions requirements. Upgrading to \\nthe premium plan provides cont\", \"rol over instance sizes, pre-warmed instances (to avoid cold start \\ndelays), and dedicated VMs. \\nOth\", \"er container deployment options \\nAside from Azure Kubernetes Service (AKS), you can also deploy cont\", \"ainers to Azure App Service for \\nContainers and Azure Container Instances. \\nWhen does it make sense \", \"to deploy to App Service for Containers? \\nSimple production applications that don\\u2019t require orchestr\", \"ation are well suited to Azure App Service \\nfor Containers. \\nHow to deploy to App Service for Contai\", \"ners \\nTo deploy to Azure App Service for Containers, you\\u2019ll need an Azure Container Registry (ACR) i\", \"nstance \\nand credentials to access it. Push your container image to the ACR repository so that your \", \"Azure App \\nService can pull it when needed. Once complete, you can configure the app for Continuous \", \"\\nDeployment. Doing so will automatically deploy updates whenever the image changes in ACR. \\nWhen doe\", \"s it make sense to deploy to Azure Container Instances? \\nAzure Container Instances (ACI) enables you\", \" to run Docker containers in a managed, serverless cloud \\nenvironment, without having to set up virt\", \"ual machines or clusters. It\\u2019s a great solution for short-\\nrunning workloads that can run in an isol\", \"ated container. Consider ACI for simple services, testing \\nscenarios, task automation, and build job\", \"s. ACI spins-up a container instance, performs the task, and \\nthen spins it down. \\nHow to deploy an \", \"app to Azure Container Instances \\nTo deploy to Azure Container Instances (ACI), you need an Azure Co\", \"ntainer Registry (ACR) and \\ncredentials for accessing it. Once you push your container image to the \", \"repository, it\\u2019s available to pull \\ninto ACI. You can work with ACI using the Azure portal or comman\", \"d-line interface. ACR provides tight \\nintegration with ACI. Figure 3-12 shows how to push an individ\", \"ual container image to ACR. \\n \\n56 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\n \\nFigure 3-12. A\", \"zure Container Registry Run Instance \\nCreating an instance in ACI can be done quickly. Specify the i\", \"mage registry, Azure resource group \\ninformation, the amount of memory to allocate, and the port on \", \"which to listen. This quickstart shows \\nhow to deploy a container instance to ACI using the Azure po\", \"rtal. \\nOnce the deployment completes, find the newly deployed container\\u2019s IP address and communicate\", \" \\nwith it over the port you specified. \\nAzure Container Instances offers the fastest way to run simp\", \"le container workloads in Azure. You don\\u2019t \\nneed to configure an app service, orchestrator, or virtu\", \"al machine. For scenarios where you require full \\ncontainer orchestration, service discovery, automa\", \"tic scaling, or coordinated upgrades, we \\nrecommend Azure Kubernetes Service (AKS). \\nReferences \\n\\u2022 \\n\", \"What is Kubernetes? \\n\\u2022 \\nInstalling Kubernetes with Minikube \\n\\u2022 \\nMiniKube vs Docker Desktop \\n\\u2022 \\nVisua\", \"l Studio Tools for Docker \\n \\n57 \\nCHAPTER 3 | Scaling cloud-native applications \\n \\n\\u2022 \\nUnderstanding s\", \"erverless cold start \\n\\u2022 \\nPre-warmed Azure Functions instances \\n\\u2022 \\nCreate a function on Linux using a\", \" custom image \\n\\u2022 \\nRun Azure Functions in a Docker Container \\n\\u2022 \\nCreate a function on Linux using a c\", \"ustom image \\n\\u2022 \\nAzure Functions with Kubernetes Event Driven Autoscaling \\n\\u2022 \\nCanary Release \\n\\u2022 \\nAzur\", \"e Dev Spaces with VS Code \\n\\u2022 \\nAzure Dev Spaces with Visual Studio \\n\\u2022 \\nAKS Multiple Node Pools \\n\\u2022 \\nAK\", \"S Cluster Autoscaler \\n\\u2022 \\nTutorial: Scale applications in AKS \\n\\u2022 \\nAzure Functions scale and hosting \\n\", \"\\u2022 \\nAzure Container Instances Docs \\n\\u2022 \\nDeploy Container Instance from ACR \\n \\n58 \\nCHAPTER 4 | Cloud-na\", \"tive communication patterns \\n \\nCHAPTER 4 \\nCloud-native \\ncommunication patterns \\nWhen constructing a \", \"cloud-native system, communication becomes a significant design decision. How \\ndoes a front-end clie\", \"nt application communicate with a back-end microservice? How do back-end \\nmicroservices communicate \", \"with each other? What are the principles, patterns, and best practices to \\nconsider when implementin\", \"g communication in cloud-native applications? \\nCommunication considerations \\nIn a monolithic applica\", \"tion, communication is straightforward. The code modules execute together in \\nthe same executable sp\", \"ace (process) on a server. This approach can have performance advantages as \\neverything runs togethe\", \"r in shared memory, but results in tightly coupled code that becomes difficult \\nto maintain, evolve,\", \" and scale. \\nCloud-native systems implement a microservice-based architecture with many small, indep\", \"endent \\nmicroservices. Each microservice executes in a separate process and typically runs inside a \", \"container \\nthat is deployed to a cluster. \\nA cluster groups a pool of virtual machines together to f\", \"orm a highly available environment. They\\u2019re \\nmanaged with an orchestration tool, which is responsibl\", \"e for deploying and managing the \\ncontainerized microservices. Figure 4-1 shows a Kubernetes cluster\", \" deployed into the Azure cloud \\nwith the fully managed Azure Kubernetes Services. \\n \\n59 \\nCHAPTER 4 |\", \" Cloud-native communication patterns \\n \\n \\nFigure 4-1. A Kubernetes cluster in Azure \\nAcross the clus\", \"ter, microservices communicate with each other through APIs and messaging \\ntechnologies. \\nWhile they\", \" provide many benefits, microservices are no free lunch. Local in-process method calls \\nbetween comp\", \"onents are now replaced with network calls. Each microservice must communicate over \\na network proto\", \"col, which adds complexity to your system: \\n\\u2022 \\nNetwork congestion, latency, and transient faults are\", \" a constant concern. \\n\\u2022 \\nResiliency (that is, retrying failed requests) is essential. \\n\\u2022 \\nSome calls\", \" must be idempotent as to keep consistent state. \\n\\u2022 \\nEach microservice must authenticate and authori\", \"ze calls. \\n\\u2022 \\nEach message must be serialized and then deserialized - which can be expensive. \\n\\u2022 \\nMe\", \"ssage encryption/decryption becomes important. \\nThe book .NET Microservices: Architecture for Contai\", \"nerized .NET Applications, available for free from \\nMicrosoft, provides an in-depth coverage of comm\", \"unication patterns for microservice applications. In \\nthis chapter, we provide a high-level overview\", \" of these patterns along with implementation options \\navailable in the Azure cloud. \\nIn this chapter\", \", we\\u2019ll first address communication between front-end applications and back-end \\nmicroservices. We\\u2019l\", \"l then look at back-end microservices communicate with each other. We\\u2019ll explore \\n \\n60 \\nCHAPTER 4 | \", \"Cloud-native communication patterns \\n \\nthe up and gRPC communication technology. Finally, we\\u2019ll look\", \" new innovative communication \\npatterns using service mesh technology. We\\u2019ll also see how the Azure \", \"cloud provides different kinds \\nof backing services to support cloud-native communication. \\nFront-en\", \"d client communication \\nIn a cloud-native system, front-end clients (mobile, web, and desktop applic\", \"ations) require a \\ncommunication channel to interact with independent back-end microservices. \\nWhat \", \"are the options? \\nTo keep things simple, a front-end client could directly communicate with the back\", \"-end microservices, \\nshown in Figure 4-2. \\n \\nFigure 4-2. Direct client to service communication \\nWit\", \"h this approach, each microservice has a public endpoint that is accessible by front-end clients. In\", \" \\na production environment, you\\u2019d place a load balancer in front of the microservices, routing traff\", \"ic \\nproportionately. \\nWhile simple to implement, direct client communication would be acceptable onl\", \"y for simple \\nmicroservice applications. This pattern tightly couples front-end clients to core back\", \"-end services, \\nopening the door for many problems, including: \\n\\u2022 \\nClient susceptibility to back-end\", \" service refactoring. \\n\\u2022 \\nA wider attack surface as core back-end services are directly exposed. \\n\\u2022 \", \"\\nDuplication of cross-cutting concerns across each microservice. \\n\\u2022 \\nOverly complex client code - cl\", \"ients must keep track of multiple endpoints and handle failures \\nin a resilient way. \\n \\n61 \\nCHAPTER \", \"4 | Cloud-native communication patterns \\n \\nInstead, a widely accepted cloud design pattern is to imp\", \"lement an API Gateway Service between the \\nfront-end applications and back-end services. The pattern\", \" is shown in Figure 4-3. \\n \\nFigure 4-3. API gateway pattern \\nIn the previous figure, note how the AP\", \"I Gateway service abstracts the back-end core microservices. \\nImplemented as a web API, it acts as a\", \" reverse proxy, routing incoming traffic to the internal \\nmicroservices. \\nThe gateway insulates the \", \"client from internal service partitioning and refactoring. If you change a \\nback-end service, you ac\", \"commodate for it in the gateway without breaking the client. It\\u2019s also your \\nfirst line of defense f\", \"or cross-cutting concerns, such as identity, caching, resiliency, metering, and \\nthrottling. Many of\", \" these cross-cutting concerns can be off-loaded from the back-end core services to \\nthe gateway, sim\", \"plifying the back-end services. \\nCare must be taken to keep the API Gateway simple and fast. Typical\", \"ly, business logic is kept out of \\nthe gateway. A complex gateway risks becoming a bottleneck and ev\", \"entually a monolith itself. Larger \\nsystems often expose multiple API Gateways segmented by client t\", \"ype (mobile, web, desktop) or \\nback-end functionality. The Backend for Frontends pattern provides di\", \"rection for implementing \\nmultiple gateways. The pattern is shown in Figure 4-4. \\n \\n62 \\nCHAPTER 4 | \", \"Cloud-native communication patterns \\n \\n \\nFigure 4-4. Backend for frontend pattern \\nNote in the previ\", \"ous figure how incoming traffic is sent to a specific API gateway - based upon client \\ntype: web, mo\", \"bile, or desktop app. This approach makes sense as the capabilities of each device differ \\nsignifica\", \"ntly across form factor, performance, and display limitations. Typically mobile applications \\nexpose\", \" less functionality than a browser or desktop applications. Each gateway can be optimized to \\nmatch \", \"the capabilities and functionality of the corresponding device. \\nSimple Gateways \\nTo start, you coul\", \"d build your own API Gateway service. A quick search of GitHub will provide many \\nexamples. \\nFor sim\", \"ple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and \\ncreated \", \"for .NET microservices, it\\u2019s lightweight, fast, scalable. Like any API Gateway, its primary \\nfunctio\", \"nality is to forward incoming HTTP requests to downstream services. Additionally, it supports a \\nwid\", \"e variety of capabilities that are configurable in a .NET middleware pipeline. \\nYARP (Yet Another Re\", \"verse proxy) is another open source reverse proxy led by a group of Microsoft \\nproduct teams. Downlo\", \"adable as a NuGet package, YARP plugs into the ASP.NET framework as \\nmiddleware and is highly custom\", \"izable. You\\u2019ll find YARP well-documented with various usage \\nexamples. \\nFor enterprise cloud-native \", \"applications, there are several managed Azure services that can help \\njump-start your efforts. \\n \\n63\", \" \\nCHAPTER 4 | Cloud-native communication patterns \\n \\nAzure Application Gateway \\nFor simple gateway r\", \"equirements, you may consider Azure Application Gateway. Available as an Azure \\nPaaS service, it inc\", \"ludes basic gateway features such as URL routing, SSL termination, and a Web \\nApplication Firewall. \", \"The service supports Layer-7 load balancing capabilities. With Layer 7, you can \\nroute requests base\", \"d on the actual content of an HTTP message, not just low-level TCP network \\npackets. \\nThroughout thi\", \"s book, we evangelize hosting cloud-native systems in Kubernetes. A container \\norchestrator, Kuberne\", \"tes automates the deployment, scaling, and operational concerns of \\ncontainerized workloads. Azure A\", \"pplication Gateway can be configured as an API gateway for Azure \\nKubernetes Service cluster. \\nThe A\", \"pplication Gateway Ingress Controller enables Azure Application Gateway to work directly with \\nAzure\", \" Kubernetes Service. Figure 4.5 shows the architecture. \\n \\nFigure 4-5. Application Gateway Ingress C\", \"ontroller \\nKubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, calle\", \"d Ingress. \\nIngress defines a set of rules for how microservice instances inside AKS can be exposed \", \"to the outside \\nworld. In the previous image, the ingress controller interprets the ingress rules co\", \"nfigured for the \\ncluster and automatically configures the Azure Application Gateway. Based on those\", \" rules, the \\nApplication Gateway routes traffic to microservices running inside AKS. The ingress con\", \"troller listens \\nfor changes to ingress rules and makes the appropriate changes to the Azure Applica\", \"tion Gateway. \\nAzure API Management \\nFor moderate to large-scale cloud-native systems, you may consi\", \"der Azure API Management. It\\u2019s a \\ncloud-based service that not only solves your API Gateway needs, b\", \"ut provides a full-featured \\ndeveloper and administrative experience. API Management is shown in Fig\", \"ure 4-6. \\n \\n64 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n \\nFigure 4-6. Azure API Managemen\", \"t \\nTo start, API Management exposes a gateway server that allows controlled access to back-end \\nserv\", \"ices based upon configurable rules and policies. These services can be in the Azure cloud, your \\non-\", \"prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All \\ntr\", \"affic is logged for analytical purposes. \\nFor developers, API Management offers a developer portal t\", \"hat provides access to services, \\ndocumentation, and sample code for invoking them. Developers can u\", \"se Swagger/Open API to \\ninspect service endpoints and analyze their usage. The service works across \", \"the major development \\nplatforms: .NET, Java, Golang, and more. \\nThe publisher portal exposes a mana\", \"gement dashboard where administrators expose APIs and \\nmanage their behavior. Service access can be \", \"granted, service health monitored, and service telemetry \\ngathered. Administrators apply policies to\", \" each endpoint to affect behavior. Policies are pre-built \\nstatements that execute sequentially for \", \"each service call. Policies are configured for an inbound call, \\noutbound call, or invoked upon an e\", \"rror. Policies can be applied at different service scopes as to \\nenable deterministic ordering when \", \"combining policies. The product ships with a large number of \\nprebuilt policies. \\nHere are examples \", \"of how policies can affect the behavior of your cloud-native services: \\n\\u2022 \\nRestrict service access. \", \"\\n\\u2022 \\nEnforce authentication. \\n \\n\\u2022 \\nThrottle calls from a single source, if necessary. \\n\\u2022 \\nEnable cach\", \"ing. \\n\\u2022 \\nBlock calls from specific IP addresses. \\n \\n65 \\nCHAPTER 4 | Cloud-native communication patte\", \"rns \\n \\n\\u2022 \\nControl the flow of the service. \\n\\u2022 \\nConvert requests from SOAP to REST or between differe\", \"nt data formats, such as from XML to \\nJSON. \\nAzure API Management can expose back-end services that \", \"are hosted anywhere \\u2013 in the cloud or your \\ndata center. For legacy services that you may expose in \", \"your cloud-native systems, it supports both \\nREST and SOAP APIs. Even other Azure services can be ex\", \"posed through API Management. You could \\nplace a managed API on top of an Azure backing service like\", \" Azure Service Bus or Azure Logic Apps. \\nAzure API Management doesn\\u2019t include built-in load-balancin\", \"g support and should be used in \\nconjunction with a load-balancing service. \\nAzure API Management is\", \" available across four different tiers: \\n\\u2022 \\nDeveloper \\n\\u2022 \\nBasic \\n\\u2022 \\nStandard \\n\\u2022 \\nPremium \\nThe Develo\", \"per tier is meant for non-production workloads and evaluation. The other tiers offer \\nprogressively \", \"more power, features, and higher service level agreements (SLAs). The Premium tier \\nprovides Azure V\", \"irtual Network and multi-region support. All tiers have a fixed price per hour. \\nThe Azure cloud als\", \"o offers a serverless tier for Azure API Management. Referred to as the \\nconsumption pricing tier, t\", \"he service is a variant of API Management designed around the serverless \\ncomputing model. Unlike th\", \"e \\u201cpre-allocated\\u201d pricing tiers previously shown, the consumption tier \\nprovides instant provisionin\", \"g and pay-per-action pricing. \\nIt enables API Gateway features for the following use cases: \\n\\u2022 \\nMicr\", \"oservices implemented using serverless technologies such as Azure Functions and Azure \\nLogic Apps. \\n\", \"\\u2022 \\nAzure backing service resources such as Service Bus queues and topics, Azure storage, and \\nothers\", \". \\n\\u2022 \\nMicroservices where traffic has occasional large spikes but remains low most the time. \\nThe co\", \"nsumption tier uses the same underlying service API Management components, but employs \\nan entirely \", \"different architecture based on dynamically allocated resources. It aligns perfectly with the \\nserve\", \"rless computing model: \\n\\u2022 \\nNo infrastructure to manage. \\n\\u2022 \\nNo idle capacity. \\n\\u2022 \\nHigh-availability.\", \" \\n\\u2022 \\nAutomatic scaling. \\n\\u2022 \\nCost is based on actual usage. \\nThe new consumption tier is a great choi\", \"ce for cloud-native systems that expose serverless resources \\nas APIs. \\n \\n66 \\nCHAPTER 4 | Cloud-nati\", \"ve communication patterns \\n \\nReal-time communication \\nReal-time, or push, communication is another o\", \"ption for front-end applications that communicate \\nwith back-end cloud-native systems over HTTP. App\", \"lications, such as financial-tickers, online \\neducation, gaming, and job-progress updates, require i\", \"nstantaneous, real-time responses from the \\nback-end. With normal HTTP communication, there\\u2019s no way\", \" for the client to know when new data is \\navailable. The client must continually poll or send reques\", \"ts to the server. With real-time \\ncommunication, the server can push new data to the client at any t\", \"ime. \\nReal-time systems are often characterized by high-frequency data flows and large numbers of \\nc\", \"oncurrent client connections. Manually implementing real-time connectivity can quickly become \\ncompl\", \"ex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected \\n\", \"clients. You could find yourself managing an instance of Azure Redis Cache and a set of load \\nbalanc\", \"ers configured with sticky sessions for client affinity. \\nAzure SignalR Service is a fully managed A\", \"zure service that simplifies real-time communication for \\nyour cloud-native applications. Technical \", \"implementation details like capacity provisioning, scaling, \\nand persistent connections are abstract\", \"ed away. They\\u2019re handled for you with a 99.9% service-level \\nagreement. You focus on application fea\", \"tures, not infrastructure plumbing. \\nOnce enabled, a cloud-based HTTP service can push content updat\", \"es directly to connected clients, \\nincluding browser, mobile and desktop applications. Clients are u\", \"pdated without the need to poll the \\nserver. Azure SignalR abstracts the transport technologies that\", \" create real-time connectivity, including \\nWebSockets, Server-Side Events, and Long Polling. Develop\", \"ers focus on sending messages to all or \\nspecific subsets of connected clients. \\nFigure 4-7 shows a \", \"set of HTTP Clients connecting to a Cloud-native application with Azure SignalR \\nenabled. \\n \\n67 \\nCHA\", \"PTER 4 | Cloud-native communication patterns \\n \\n \\nFigure 4-7. Azure SignalR \\nAnother advantage of Az\", \"ure SignalR Service comes with implementing Serverless cloud-native \\nservices. Perhaps your code is \", \"executed on demand with Azure Functions triggers. This scenario can \\nbe tricky because your code doe\", \"sn\\u2019t maintain long connections with clients. Azure SignalR Service can \\nhandle this situation since \", \"the service already manages connections for you. \\nAzure SignalR Service closely integrates with othe\", \"r Azure services, such as Azure SQL Database, \\nService Bus, or Redis Cache, opening up many possibil\", \"ities for your cloud-native applications. \\nService-to-service communication \\nMoving from the front-e\", \"nd client, we now address back-end microservices communicate with each \\nother. \\nWhen constructing a \", \"cloud-native application, you\\u2019ll want to be sensitive to how back-end services \\ncommunicate with eac\", \"h other. Ideally, the less inter-service communication, the better. However, \\navoidance isn\\u2019t always\", \" possible as back-end services often rely on one another to complete an \\noperation. \\nThere are sever\", \"al widely accepted approaches to implementing cross-service communication. The type \\nof communicatio\", \"n interaction will often determine the best approach. \\nConsider the following interaction types: \\n\\u2022 \", \"\\nQuery \\u2013 when a calling microservice requires a response from a called microservice, such as, \\n\\u201cHey,\", \" give me the buyer information for a given customer Id.\\u201d \\n \\n68 \\nCHAPTER 4 | Cloud-native communicati\", \"on patterns \\n \\n\\u2022 \\nCommand \\u2013 when the calling microservice needs another microservice to execute an a\", \"ction \\nbut doesn\\u2019t require a response, such as, \\u201cHey, just ship this order.\\u201d \\n\\u2022 \\nEvent \\u2013 when a micr\", \"oservice, called the publisher, raises an event that state has changed or an \\naction has occurred. O\", \"ther microservices, called subscribers, who are interested, can react to \\nthe event appropriately. T\", \"he publisher and the subscribers aren\\u2019t aware of each other. \\nMicroservice systems typically use a c\", \"ombination of these interaction types when executing \\noperations that require cross-service interact\", \"ion. Let\\u2019s take a close look at each and how you might \\nimplement them. \\nQueries \\nMany times, one mi\", \"croservice might need to query another, requiring an immediate response to \\ncomplete an operation. A\", \" shopping basket microservice may need product information and a price to \\nadd an item to its basket\", \". There are many approaches for implementing query operations. \\nRequest/Response Messaging \\nOne opti\", \"on for implementing this scenario is for the calling back-end microservice to make direct \\nHTTP requ\", \"ests to the microservices it needs to query, shown in Figure 4-8. \\n \\nFigure 4-8. Direct HTTP communi\", \"cation \\nWhile direct HTTP calls between microservices are relatively simple to implement, care shoul\", \"d be \\ntaken to minimize this practice. To start, these calls are always synchronous and will block t\", \"he \\noperation until a result is returned or the request times outs. What were once self-contained, \\n\", \"independent services, able to evolve independently and deploy frequently, now become coupled to \\neac\", \"h other. As coupling among microservices increase, their architectural benefits diminish. \\n \\n69 \\nCHA\", \"PTER 4 | Cloud-native communication patterns \\n \\nExecuting an infrequent request that makes a single \", \"direct HTTP call to another microservice might be \\nacceptable for some systems. However, high-volume\", \" calls that invoke direct HTTP calls to multiple \\nmicroservices aren\\u2019t advisable. They can increase \", \"latency and negatively impact the performance, \\nscalability, and availability of your system. Even w\", \"orse, a long series of direct HTTP communication can \\nlead to deep and complex chains of synchronous\", \" microservices calls, shown in Figure 4-9: \\n \\nFigure 4-9. Chaining HTTP queries \\nYou can certainly i\", \"magine the risk in the design shown in the previous image. What happens if Step \\n#3 fails? Or Step #\", \"8 fails? How do you recover? What if Step #6 is slow because the underlying service \\nis busy? How do\", \" you continue? Even if all works correctly, think of the latency this call would incur, \\nwhich is th\", \"e sum of the latency of each step. \\nThe large degree of coupling in the previous image suggests the \", \"services weren\\u2019t optimally modeled. \\nIt would behoove the team to revisit their design. \\nMaterialize\", \"d View pattern \\nA popular option for removing microservice coupling is the Materialized View pattern\", \". With this \\npattern, a microservice stores its own local, denormalized copy of data that\\u2019s owned by\", \" other services. \\nInstead of the Shopping Basket microservice querying the Product Catalog and Prici\", \"ng microservices, \\nit maintains its own local copy of that data. This pattern eliminates unnecessary\", \" coupling and \\nimproves reliability and response time. The entire operation executes inside a single\", \" process. We \\nexplore this pattern and other data concerns in Chapter 5. \\nService Aggregator Pattern\", \" \\nAnother option for eliminating microservice-to-microservice coupling is an Aggregator microservice\", \", \\nshown in purple in Figure 4-10. \\n \\n70 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n \\nFigur\", \"e 4-10. Aggregator microservice \\nThe pattern isolates an operation that makes calls to multiple back\", \"-end microservices, centralizing its \\nlogic into a specialized microservice. The purple checkout agg\", \"regator microservice in the previous \\nfigure orchestrates the workflow for the Checkout operation. I\", \"t includes calls to several back-end \\nmicroservices in a sequenced order. Data from the workflow is \", \"aggregated and returned to the caller. \\nWhile it still implements direct HTTP calls, the aggregator \", \"microservice reduces direct dependencies \\namong back-end microservices. \\nRequest/Reply Pattern \\nAnot\", \"her approach for decoupling synchronous HTTP messages is a Request-Reply Pattern, which uses \\nqueuin\", \"g communication. Communication using a queue is always a one-way channel, with a producer \\nsending t\", \"he message and consumer receiving it. With this pattern, both a request queue and response \\nqueue ar\", \"e implemented, shown in Figure 4-11. \\n \\n71 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n \\nFig\", \"ure 4-11. Request-reply pattern \\nHere, the message producer creates a query-based message that conta\", \"ins a unique correlation ID and \\nplaces it into a request queue. The consuming service dequeues the \", \"messages, processes it and places \\nthe response into the response queue with the same correlation ID\", \". The producer service dequeues \\nthe message, matches it with the correlation ID and continues proce\", \"ssing. We cover queues in detail \\nin the next section. \\nCommands \\nAnother type of communication inte\", \"raction is a command. A microservice may need another \\nmicroservice to perform an action. The Orderi\", \"ng microservice may need the Shipping microservice to \\ncreate a shipment for an approved order. In F\", \"igure 4-12, one microservice, called a Producer, sends a \\nmessage to another microservice, the Consu\", \"mer, commanding it to do something. \\n \\n72 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n \\nFigu\", \"re 4-12. Command interaction with a queue \\nMost often, the Producer doesn\\u2019t require a response and c\", \"an fire-and-forget the message. If a reply is \\nneeded, the Consumer sends a separate message back to\", \" Producer on another channel. A command \\nmessage is best sent asynchronously with a message queue. s\", \"upported by a lightweight message \\nbroker. In the previous diagram, note how a queue separates and d\", \"ecouples both services. \\nA message queue is an intermediary construct through which a producer and c\", \"onsumer pass a \\nmessage. Queues implement an asynchronous, point-to-point messaging pattern. The Pro\", \"ducer \\nknows where a command needs to be sent and routes appropriately. The queue guarantees that a \", \"\\nmessage is processed by exactly one of the consumer instances that are reading from the channel. In\", \" \\nthis scenario, either the producer or consumer service can scale out without affecting the other. \", \"As \\nwell, technologies can be disparate on each side, meaning that we might have a Java microservice\", \" \\ncalling a Golang microservice. \\nIn chapter 1, we talked about backing services. Backing services a\", \"re ancillary resources upon which \\ncloud-native systems depend. Message queues are backing services.\", \" The Azure cloud supports two \\ntypes of message queues that your cloud-native systems can consume to\", \" implement command \\nmessaging: Azure Storage Queues and Azure Service Bus Queues. \\nAzure Storage Que\", \"ues \\nAzure storage queues offer a simple queueing infrastructure that is fast, affordable, and backe\", \"d by \\nAzure storage accounts. \\nAzure Storage Queues feature a REST-based queuing mechanism with reli\", \"able and persistent \\nmessaging. They provide a minimal feature set, but are inexpensive and store mi\", \"llions of messages. \\nTheir capacity ranges up to 500 TB. A single message can be up to 64 KB in size\", \". \\nYou can access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. \\n\", \"Storage queues can scale out to large numbers of concurrent clients to handle traffic spikes. \\n \\n73 \", \"\\nCHAPTER 4 | Cloud-native communication patterns \\n \\nThat said, there are limitations with the servic\", \"e: \\n\\u2022 \\nMessage order isn\\u2019t guaranteed. \\n\\u2022 \\nA message can only persist for seven days before it\\u2019s aut\", \"omatically removed. \\n\\u2022 \\nSupport for state management, duplicate detection, or transactions isn\\u2019t ava\", \"ilable. \\nFigure 4-13 shows the hierarchy of an Azure Storage Queue. \\n \\nFigure 4-13. Storage queue hi\", \"erarchy \\nIn the previous figure, note how storage queues store their messages in the underlying Azur\", \"e Storage \\naccount. \\nFor developers, Microsoft provides several client and server-side libraries for\", \" Storage queue \\nprocessing. Most major platforms are supported including .NET, Java, JavaScript, Rub\", \"y, Python, and \\nGo. Developers should never communicate directly with these libraries. Doing so will\", \" tightly couple \\nyour microservice code to the Azure Storage Queue service. It\\u2019s a better practice t\", \"o insulate the \\nimplementation details of the API. Introduce an intermediation layer, or intermediat\", \"e API, that exposes \\ngeneric operations and encapsulates the concrete library. This loose coupling e\", \"nables you to swap out \\none queuing service for another without having to make changes to the mainli\", \"ne service code. \\nAzure Storage queues are an economical option to implement command messaging in yo\", \"ur cloud-\\nnative applications. Especially when a queue size will exceed 80 GB, or a simple feature s\", \"et is \\nacceptable. You only pay for the storage of the messages; there are no fixed hourly charges. \", \"\\nAzure Service Bus Queues \\nFor more complex messaging requirements, consider Azure Service Bus queue\", \"s. \\nSitting atop a robust message infrastructure, Azure Service Bus supports a brokered messaging mo\", \"del. \\nMessages are reliably stored in a broker (the queue) until received by the consumer. The queue\", \" \\nguarantees First-In/First-Out (FIFO) message delivery, respecting the order in which messages were\", \" \\nadded to the queue. \\nThe size of a message can be much larger, up to 256 KB. Messages are persiste\", \"d in the queue for an \\nunlimited period of time. Service Bus supports not only HTTP-based calls, but\", \" also provides full \\n \\n74 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\nsupport for the AMQP p\", \"rotocol. AMQP is an open-standard across vendors that supports a binary \\nprotocol and higher degrees\", \" of reliability. \\nService Bus provides a rich set of features, including transaction support and a d\", \"uplicate detection \\nfeature. The queue guarantees \\u201cat most once delivery\\u201d per message. It automatica\", \"lly discards a \\nmessage that has already been sent. If a producer is in doubt, it can resend the sam\", \"e message, and \\nService Bus guarantees that only one copy will be processed. Duplicate detection fre\", \"es you from \\nhaving to build additional infrastructure plumbing. \\nTwo more enterprise features are p\", \"artitioning and sessions. A conventional Service Bus queue is \\nhandled by a single message broker an\", \"d stored in a single message store. But, Service Bus Partitioning \\nspreads the queue across multiple\", \" message brokers and message stores. The overall throughput is no \\nlonger limited by the performance\", \" of a single message broker or messaging store. A temporary \\noutage of a messaging store doesn\\u2019t ren\", \"der a partitioned queue unavailable. \\nService Bus Sessions provide a way to group-related messages. \", \"Imagine a workflow scenario where \\nmessages must be processed together and the operation completed a\", \"t the end. To take advantage, \\nsessions must be explicitly enabled for the queue and each related me\", \"ssaged must contain the same \\nsession ID. \\nHowever, there are some important caveats: Service Bus qu\", \"eues size is limited to 80 GB, which is much \\nsmaller than what\\u2019s available from store queues. Addit\", \"ionally, Service Bus queues incur a base cost \\nand charge per operation. \\nFigure 4-14 outlines the h\", \"igh-level architecture of a Service Bus queue. \\n \\nFigure 4-14. Service Bus queue \\nIn the previous fi\", \"gure, note the point-to-point relationship. Two instances of the same provider are \\nenqueuing messag\", \"es into a single Service Bus queue. Each message is consumed by only one of three \\nconsumer instance\", \"s on the right. Next, we discuss how to implement messaging where different \\nconsumers may all be in\", \"terested the same message. \\nEvents \\nMessage queuing is an effective way to implement communication w\", \"here a producer can \\nasynchronously send a consumer a message. However, what happens when many diffe\", \"rent consumers \\n \\n75 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\nare interested in the same \", \"message? A dedicated message queue for each consumer wouldn\\u2019t scale \\nwell and would become difficult\", \" to manage. \\nTo address this scenario, we move to the third type of message interaction, the event. \", \"One \\nmicroservice announces that an action had occurred. Other microservices, if interested, react t\", \"o the \\naction, or event. This is also known as the event-driven architectural style. \\nEventing is a \", \"two-step process. For a given state change, a microservice publishes an event to a \\nmessage broker, \", \"making it available to any other interested microservice. The interested microservice \\nis notified b\", \"y subscribing to the event in the message broker. You use the Publish/Subscribe pattern \\nto implemen\", \"t event-based communication. \\nFigure 4-15 shows a shopping basket microservice publishing an event w\", \"ith two other microservices \\nsubscribing to it. \\n \\nFigure 4-15. Event-Driven messaging \\nNote the eve\", \"nt bus component that sits in the middle of the communication channel. It\\u2019s a custom \\nclass that enc\", \"apsulates the message broker and decouples it from the underlying application. The \\nordering and inv\", \"entory microservices independently operate the event with no knowledge of each \\nother, nor the shopp\", \"ing basket microservice. When the registered event is published to the event bus, \\nthey act upon it.\", \" \\nWith eventing, we move from queuing technology to topics. A topic is similar to a queue, but suppo\", \"rts \\na one-to-many messaging pattern. One microservice publishes a message. Multiple subscribing \\nmi\", \"croservices can choose to receive and act upon that message. Figure 4-16 shows a topic \\narchitecture\", \". \\n \\n76 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n \\nFigure 4-16. Topic architecture \\nIn th\", \"e previous figure, publishers send messages to the topic. At the end, subscribers receive \\nmessages \", \"from subscriptions. In the middle, the topic forwards messages to subscriptions based on a \\nset of r\", \"ules, shown in dark blue boxes. Rules act as a filter that forward specific messages to a \\nsubscript\", \"ion. Here, a \\u201cGetPrice\\u201d event would be sent to the price and logging subscriptions as the \\nlogging s\", \"ubscription has chosen to receive all messages. A \\u201cGetInformation\\u201d event would be sent to \\nthe infor\", \"mation and logging subscriptions. \\nThe Azure cloud supports two different topic services: Azure Serv\", \"ice Bus Topics and Azure EventGrid. \\nAzure Service Bus Topics \\nSitting on top of the same robust bro\", \"kered message model of Azure Service Bus queues are Azure \\nService Bus Topics. A topic can receive m\", \"essages from multiple independent publishers and send \\nmessages to up to 2,000 subscribers. Subscrip\", \"tions can be dynamically added or removed at run time \\nwithout stopping the system or recreating the\", \" topic. \\nMany advanced features from Azure Service Bus queues are also available for topics, includi\", \"ng \\nDuplicate Detection and Transaction support. By default, Service Bus topics are handled by a sin\", \"gle \\nmessage broker and stored in a single message store. But, Service Bus Partitioning scales a top\", \"ic by \\nspreading it across many message brokers and message stores. \\nScheduled Message Delivery tags\", \" a message with a specific time for processing. The message won\\u2019t \\nappear in the topic before that t\", \"ime. Message Deferral enables you to defer a retrieval of a message \\nto a later time. Both are commo\", \"nly used in workflow processing scenarios where operations are \\nprocessed in a particular order. You\", \" can postpone processing of received messages until prior work \\nhas been completed. \\nService Bus top\", \"ics are a robust and proven technology for enabling publish/subscribe communication \\nin your cloud-n\", \"ative systems. \\nAzure Event Grid \\nWhile Azure Service Bus is a battle-tested messaging broker with a\", \" full set of enterprise features, \\nAzure Event Grid is the new kid on the block. \\n \\n77 \\nCHAPTER 4 | \", \"Cloud-native communication patterns \\n \\nAt first glance, Event Grid may look like just another topic-\", \"based messaging system. However, it\\u2019s \\ndifferent in many ways. Focused on event-driven workloads, it\", \" enables real-time event processing, \\ndeep Azure integration, and an open-platform - all on serverle\", \"ss infrastructure. It\\u2019s designed for \\ncontemporary cloud-native and serverless applications \\nAs a ce\", \"ntralized eventing backplane, or pipe, Event Grid reacts to events inside Azure resources and \\nfrom \", \"your own services. \\nEvent notifications are published to an Event Grid Topic, which, in turn, routes\", \" each event to a \\nsubscription. Subscribers map to subscriptions and consume the events. Like Servic\", \"e Bus, Event Grid \\nsupports a filtered subscriber model where a subscription sets rule for the event\", \"s it wishes to receive. \\nEvent Grid provides fast throughput with a guarantee of 10 million events p\", \"er second enabling near \\nreal-time delivery - far more than what Azure Service Bus can generate. \\nA \", \"sweet spot for Event Grid is its deep integration into the fabric of Azure infrastructure. An Azure \", \"\\nresource, such as Cosmos DB, can publish built-in events directly to other interested Azure resourc\", \"es - \\nwithout the need for custom code. Event Grid can publish events from an Azure Subscription, \\nR\", \"esource Group, or Service, giving developers fine-grained control over the lifecycle of cloud \\nresou\", \"rces. However, Event Grid isn\\u2019t limited to Azure. It\\u2019s an open platform that can consume custom \\nHTT\", \"P events published from applications or third-party services and route events to external \\nsubscribe\", \"rs. \\nWhen publishing and subscribing to native events from Azure resources, no coding is required. W\", \"ith \\nsimple configuration, you can integrate events from one Azure resource to another leveraging bu\", \"ilt-in \\nplumbing for Topics and Subscriptions. Figure 4-17 shows the anatomy of Event Grid. \\n \\nFigur\", \"e 4-17. Event Grid anatomy \\n \\n78 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\nA major differe\", \"nce between EventGrid and Service Bus is the underlying message exchange pattern. \\nService Bus imple\", \"ments an older style pull model in which the downstream subscriber actively polls \\nthe topic subscri\", \"ption for new messages. On the upside, this approach gives the subscriber full control \\nof the pace \", \"at which it processes messages. It controls when and how many messages to process at \\nany given time\", \". Unread messages remain in the subscription until processed. A significant \\nshortcoming is the late\", \"ncy between the time the event is generated and the polling operation that \\npulls that message to th\", \"e subscriber for processing. Also, the overhead of constant polling for the \\nnext event consumes res\", \"ources and money. \\nEventGrid, however, is different. It implements a push model in which events are \", \"sent to the \\nEventHandlers as received, giving near real-time event delivery. It also reduces cost a\", \"s the service is \\ntriggered only when it\\u2019s needed to consume an event \\u2013 not continually as with poll\", \"ing. That said, an \\nevent handler must handle the incoming load and provide throttling mechanisms to\", \" protect itself \\nfrom becoming overwhelmed. Many Azure services that consume these events, such as A\", \"zure \\nFunctions and Logic Apps provide automatic autoscaling capabilities to handle increased loads.\", \" \\nEvent Grid is a fully managed serverless cloud service. It dynamically scales based on your traffi\", \"c and \\ncharges you only for your actual usage, not pre-purchased capacity. The first 100,000 operati\", \"ons per \\nmonth are free \\u2013 operations being defined as event ingress (incoming event notifications), \", \"\\nsubscription delivery attempts, management calls, and filtering by subject. With 99.99% availabilit\", \"y, \\nEventGrid guarantees the delivery of an event within a 24-hour period, with built-in retry funct\", \"ionality \\nfor unsuccessful delivery. Undelivered messages can be moved to a \\u201cdead-letter\\u201d queue for \", \"resolution. \\nUnlike Azure Service Bus, Event Grid is tuned for fast performance and doesn\\u2019t support \", \"features like \\nordered messaging, transactions, and sessions. \\nStreaming messages in the Azure cloud\", \" \\nAzure Service Bus and Event Grid provide great support for applications that expose single, discre\", \"te \\nevents like a new document has been inserted into a Cosmos DB. But, what if your cloud-native \\ns\", \"ystem needs to process a stream of related events? Event streams are more complex. They\\u2019re typically\", \" \\ntime-ordered, interrelated, and must be processed as a group. \\nAzure Event Hub is a data streaming\", \" platform and event ingestion service that collects, transforms, \\nand stores events. It\\u2019s fine-tuned\", \" to capture streaming data, such as continuous event notifications \\nemitted from a telemetry context\", \". The service is highly scalable and can store and process millions of \\nevents per second. Shown in \", \"Figure 4-18, it\\u2019s often a front door for an event pipeline, decoupling \\ningest stream from event con\", \"sumption. \\n \\n79 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n \\nFigure 4-18. Azure Event Hub \\n\", \"Event Hub supports low latency and configurable time retention. Unlike queues and topics, Event \\nHub\", \"s keep event data after it\\u2019s been read by a consumer. This feature enables other data analytic \\nserv\", \"ices, both internal and external, to replay the data for further analysis. Events stored in event hu\", \"b \\nare only deleted upon expiration of the retention period, which is one day by default, but \\nconfi\", \"gurable. \\nEvent Hub supports common event publishing protocols including HTTPS and AMQP. It also sup\", \"ports \\nKafka 1.0. Existing Kafka applications can communicate with Event Hub using the Kafka protoco\", \"l \\nproviding an alternative to managing large Kafka clusters. Many open-source cloud-native systems \", \"\\nembrace Kafka. \\nEvent Hubs implements message streaming through a partitioned consumer model in whi\", \"ch each \\nconsumer only reads a specific subset, or partition, of the message stream. This pattern en\", \"ables \\ntremendous horizontal scale for event processing and provides other stream-focused features t\", \"hat are \\nunavailable in queues and topics. A partition is an ordered sequence of events that is held\", \" in an event \\nhub. As newer events arrive, they\\u2019re added to the end of this sequence. Figure 4-19 sh\", \"ows partitioning \\nin an Event Hub. \\n \\nFigure 4-19. Event Hub partitioning \\nInstead of reading from t\", \"he same resource, each consumer group reads across a subset, or partition, \\nof the message stream. \\n\", \" \\n80 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\nFor cloud-native applications that must str\", \"eam large numbers of events, Azure Event Hub can be a \\nrobust and affordable solution. \\ngRPC \\nSo far\", \" in this book, we\\u2019ve focused on REST-based communication. We\\u2019ve seen that REST is a flexible \\narchit\", \"ectural style that defines CRUD-based operations against entity resources. Clients interact with \\nre\", \"sources across HTTP with a request/response communication model. While REST is widely \\nimplemented, \", \"a newer communication technology, gRPC, has gained tremendous momentum across \\nthe cloud-native comm\", \"unity. \\nWhat is gRPC? \\ngRPC is a modern, high-performance framework that evolves the age-old remote \", \"procedure call (RPC) \\nprotocol. At the application level, gRPC streamlines messaging between clients\", \" and back-end services. \\nOriginating from Google, gRPC is open source and part of the Cloud Native C\", \"omputing Foundation \\n(CNCF) ecosystem of cloud-native offerings. CNCF considers gRPC an incubating p\", \"roject. Incubating \\nmeans end users are using the technology in production applications, and the pro\", \"ject has a healthy \\nnumber of contributors. \\nA typical gRPC client app will expose a local, in-proce\", \"ss function that implements a business \\noperation. Under the covers, that local function invokes ano\", \"ther function on a remote machine. What \\nappears to be a local call essentially becomes a transparen\", \"t out-of-process call to a remote service. \\nThe RPC plumbing abstracts the point-to-point networking\", \" communication, serialization, and \\nexecution between computers. \\nIn cloud-native applications, deve\", \"lopers often work across programming languages, frameworks, and \\ntechnologies. This interoperability\", \" complicates message contracts and the plumbing required for \\ncross-platform communication. gRPC pro\", \"vides a \\u201cuniform horizontal layer\\u201d that abstracts these \\nconcerns. Developers code in their native p\", \"latform focused on business functionality, while gRPC \\nhandles communication plumbing. \\ngRPC offers \", \"comprehensive support across most popular development stacks, including Java, \\nJavaScript, C#, Go, S\", \"wift, and NodeJS. \\ngRPC Benefits \\ngRPC uses HTTP/2 for its transport protocol. While compatible with\", \" HTTP 1.1, HTTP/2 features many \\nadvanced capabilities: \\n\\u2022 \\nA binary framing protocol for data trans\", \"port - unlike HTTP 1.1, which is text based. \\n\\u2022 \\nMultiplexing support for sending multiple parallel \", \"requests over the same connection - HTTP \\n1.1 limits processing to one request/response message at a\", \" time. \\n\\u2022 \\nBidirectional full-duplex communication for sending both client requests and server respo\", \"nses \\nsimultaneously. \\n\\u2022 \\nBuilt-in streaming enabling requests and responses to asynchronously strea\", \"m large data sets. \\n\\u2022 \\nHeader compression that reduces network usage. \\n \\n81 \\nCHAPTER 4 | Cloud-nativ\", \"e communication patterns \\n \\ngRPC is lightweight and highly performant. It can be up to 8x faster tha\", \"n JSON serialization with \\nmessages 60-80% smaller. In Microsoft Windows Communication Foundation (W\", \"CF) parlance, gRPC \\nperformance exceeds the speed and efficiency of the highly optimized NetTCP bind\", \"ings. Unlike \\nNetTCP, which favors the Microsoft stack, gRPC is cross-platform. \\nProtocol Buffers \\ng\", \"RPC embraces an open-source technology called Protocol Buffers. They provide a highly efficient \\nand\", \" platform-neutral serialization format for serializing structured messages that services send to \\nea\", \"ch other. Using a cross-platform Interface Definition Language (IDL), developers define a service \\nc\", \"ontract for each microservice. The contract, implemented as a text-based .proto file, describes the \", \"\\nmethods, inputs, and outputs for each service. The same contract file can be used for gRPC clients \", \"and \\nservices built on different development platforms. \\nUsing the proto file, the Protobuf compiler\", \", protoc, generates both client and service code for your \\ntarget platform. The code includes the fo\", \"llowing components: \\n\\u2022 \\nStrongly typed objects, shared by the client and service, that represent the\", \" service operations \\nand data elements for a message. \\n\\u2022 \\nA strongly typed base class with the requi\", \"red network plumbing that the remote gRPC service \\ncan inherit and extend. \\n\\u2022 \\nA client stub that co\", \"ntains the required plumbing to invoke the remote gRPC service. \\nAt run time, each message is serial\", \"ized as a standard Protobuf representation and exchanged between \\nthe client and remote service. Unl\", \"ike JSON or XML, Protobuf messages are serialized as compiled \\nbinary bytes. \\nThe book, gRPC for WCF\", \" Developers, available from the Microsoft Architecture site, provides in-depth \\ncoverage of gRPC and\", \" Protocol Buffers. \\ngRPC support in .NET \\ngRPC is integrated into .NET Core 3.0 SDK and later. The f\", \"ollowing tools support it: \\n\\u2022 \\nVisual Studio 2022 with the ASP.NET and web development workload inst\", \"alled \\n\\u2022 \\nVisual Studio Code \\n\\u2022 \\nThe dotnet CLI \\nThe SDK includes tooling for endpoint routing, buil\", \"t-in IoC, and logging. The open-source Kestrel web \\nserver supports HTTP/2 connections. Figure 4-20 \", \"shows a Visual Studio 2022 template that scaffolds a \\nskeleton project for a gRPC service. Note how \", \".NET fully supports Windows, Linux, and macOS. \\n \\n82 \\nCHAPTER 4 | Cloud-native communication pattern\", \"s \\n \\n \\nFigure 4-20. gRPC support in Visual Studio 2022 \\nFigure 4-21 shows the skeleton gRPC service \", \"generated from the built-in scaffolding included in \\nVisual Studio 2022. \\n \\nFigure 4-21. gRPC projec\", \"t in Visual Studio 2022 \\nIn the previous figure, note the proto description file and service code. A\", \"s you\\u2019ll see shortly, Visual \\nStudio generates additional configuration in both the Startup class an\", \"d underlying project file. \\ngRPC usage \\nFavor gRPC for the following scenarios: \\n\\u2022 \\nSynchronous back\", \"end microservice-to-microservice communication where an immediate \\nresponse is required to continue \", \"processing. \\n\\u2022 \\nPolyglot environments that need to support mixed programming platforms. \\n\\u2022 \\nLow late\", \"ncy and high throughput communication where performance is critical. \\n\\u2022 \\nPoint-to-point real-time co\", \"mmunication - gRPC can push messages in real time without \\npolling and has excellent support for bi-\", \"directional streaming. \\n \\n83 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n\\u2022 \\nNetwork constrai\", \"ned environments \\u2013 binary gRPC messages are always smaller than an \\nequivalent text-based JSON messa\", \"ge. \\nAt the time, of this writing, gRPC is primarily used with backend services. Modern browsers can\", \"\\u2019t \\nprovide the level of HTTP/2 control required to support a front-end gRPC client. That said, ther\", \"e\\u2019s \\nsupport for gRPC-Web with .NET that enables gRPC communication from browser-based apps built \\nw\", \"ith JavaScript or Blazor WebAssembly technologies. gRPC-Web enables an ASP.NET Core gRPC app \\nto sup\", \"port gRPC features in browser apps: \\n\\u2022 \\nStrongly typed, code-generated clients \\n\\u2022 \\nCompact Protobuf \", \"messages \\n\\u2022 \\nServer streaming \\ngRPC implementation \\nThe microservice reference architecture, eShop o\", \"n Containers, from Microsoft, shows how to \\nimplement gRPC services in .NET applications. Figure 4-2\", \"2 presents the back-end architecture. \\n \\nFigure 4-22. Backend architecture for eShop on Containers \\n\", \"In the previous figure, note how eShop embraces the Backend for Frontends pattern (BFF) by \\nexposing\", \" multiple API gateways. We discussed the BFF pattern earlier in this chapter. Pay close \\n \\n84 \\nCHAPT\", \"ER 4 | Cloud-native communication patterns \\n \\nattention to the Aggregator microservice (in gray) tha\", \"t sits between the Web-Shopping API Gateway \\nand backend Shopping microservices. The Aggregator rece\", \"ives a single request from a client, \\ndispatches it to various microservices, aggregates the results\", \", and sends them back to the requesting \\nclient. Such operations typically require synchronous commu\", \"nication as to produce an immediate \\nresponse. In eShop, backend calls from the Aggregator are perfo\", \"rmed using gRPC as shown in Figure \\n4-23. \\n \\nFigure 4-23. gRPC in eShop on Containers \\ngRPC communic\", \"ation requires both client and server components. In the previous figure, note how \\nthe Shopping Agg\", \"regator implements a gRPC client. The client makes synchronous gRPC calls (in red) \\nto backend micro\", \"services, each of which implement a gRPC server. Both the client and server take \\nadvantage of the b\", \"uilt-in gRPC plumbing from the .NET SDK. Client-side stubs provide the plumbing \\nto invoke remote gR\", \"PC calls. Server-side components provide gRPC plumbing that custom service \\nclasses can inherit and \", \"consume. \\nMicroservices that expose both a RESTful API and gRPC communication require multiple endpo\", \"ints to \\nmanage traffic. You would open an endpoint that listens for HTTP traffic for the RESTful ca\", \"lls and \\nanother for gRPC calls. The gRPC endpoint must be configured for the HTTP/2 protocol that i\", \"s \\nrequired for gRPC communication. \\nWhile we strive to decouple microservices with asynchronous com\", \"munication patterns, some \\noperations require direct calls. gRPC should be the primary choice for di\", \"rect synchronous \\ncommunication between microservices. Its high-performance communication protocol, \", \"based on \\nHTTP/2 and protocol buffers, make it a perfect choice. \\n \\n85 \\nCHAPTER 4 | Cloud-native com\", \"munication patterns \\n \\nLooking ahead \\nLooking ahead, gRPC will continue to gain traction for cloud-n\", \"ative systems. The performance \\nbenefits and ease of development are compelling. However, REST will \", \"likely be around for a long time. \\nIt excels for publicly exposed APIs and for backward compatibilit\", \"y reasons. \\nService Mesh communication infrastructure \\nThroughout this chapter, we\\u2019ve explored the c\", \"hallenges of microservice communication. We said that \\ndevelopment teams need to be sensitive to how\", \" back-end services communicate with each other. \\nIdeally, the less inter-service communication, the \", \"better. However, avoidance isn\\u2019t always possible as \\nback-end services often rely on one another to \", \"complete operations. \\nWe explored different approaches for implementing synchronous HTTP communicati\", \"on and \\nasynchronous messaging. In each of the cases, the developer is burdened with implementing \\nc\", \"ommunication code. Communication code is complex and time intensive. Incorrect decisions can \\nlead t\", \"o significant performance issues. \\nA more modern approach to microservice communication centers arou\", \"nd a new and rapidly evolving \\ntechnology entitled Service Mesh. A service mesh is a configurable in\", \"frastructure layer with built-in \\ncapabilities to handle service-to-service communication, resilienc\", \"y, and many cross-cutting concerns. \\nIt moves the responsibility for these concerns out of the micro\", \"services and into service mesh layer. \\nCommunication is abstracted away from your microservices. \\nA \", \"key component of a service mesh is a proxy. In a cloud-native application, an instance of a proxy is\", \" \\ntypically colocated with each microservice. While they execute in separate processes, the two are \", \"\\nclosely linked and share the same lifecycle. This pattern, known as the Sidecar pattern, and is sho\", \"wn in \\nFigure 4-24. \\n \\nFigure 4-24. Service mesh with a side car \\n \\n86 \\nCHAPTER 4 | Cloud-native com\", \"munication patterns \\n \\nNote in the previous figure how messages are intercepted by a proxy that runs\", \" alongside each \\nmicroservice. Each proxy can be configured with traffic rules specific to the micro\", \"service. It \\nunderstands messages and can route them across your services and the outside world. \\nAl\", \"ong with managing service-to-service communication, the Service Mesh provides support for \\nservice d\", \"iscovery and load balancing. \\nOnce configured, a service mesh is highly functional. The mesh retriev\", \"es a corresponding pool of \\ninstances from a service discovery endpoint. It sends a request to a spe\", \"cific service instance, recording \\nthe latency and response type of the result. It chooses the insta\", \"nce most likely to return a fast \\nresponse based on different factors, including the observed latenc\", \"y for recent requests. \\nA service mesh manages traffic, communication, and networking concerns at th\", \"e application level. It \\nunderstands messages and requests. A service mesh typically integrates with\", \" a container orchestrator. \\nKubernetes supports an extensible architecture in which a service mesh c\", \"an be added. \\nIn chapter 6, we deep-dive into Service Mesh technologies including a discussion on it\", \"s architecture \\nand available open-source implementations. \\nSummary \\nIn this chapter, we discussed c\", \"loud-native communication patterns. We started by examining how \\nfront-end clients communicate with \", \"back-end microservices. Along the way, we talked about API \\nGateway platforms and real-time communic\", \"ation. We then looked at how microservices communicate \\nwith other back-end services. We looked at b\", \"oth synchronous HTTP communication and \\nasynchronous messaging across services. We covered gRPC, an \", \"upcoming technology in the cloud-\\nnative world. Finally, we introduced a new and rapidly evolving te\", \"chnology entitled Service Mesh that \\ncan streamline microservice communication. \\nSpecial emphasis wa\", \"s on managed Azure services that can help implement communication in cloud-\\nnative systems: \\n\\u2022 \\nAzur\", \"e Application Gateway \\n\\u2022 \\nAzure API Management \\n\\u2022 \\nAzure SignalR Service \\n\\u2022 \\nAzure Storage Queues \\n\\u2022\", \" \\nAzure Service Bus \\n\\u2022 \\nAzure Event Grid \\n\\u2022 \\nAzure Event Hub \\nWe next move to distributed data in cl\", \"oud-native systems and the benefits and challenges that it \\npresents. \\nReferences \\n\\u2022 \\n.NET Microserv\", \"ices: Architecture for Containerized .NET applications \\n\\u2022 \\nDesigning Interservice Communication for \", \"Microservices \\n\\u2022 \\nAzure SignalR Service, a fully managed service to add real-time functionality \\n \\n8\", \"7 \\nCHAPTER 4 | Cloud-native communication patterns \\n \\n\\u2022 \\nAzure API Gateway Ingress Controller \\n\\u2022 \\ngR\", \"PC Documentation \\n\\u2022 \\ngRPC for WCF Developers \\n\\u2022 \\nComparing gRPC Services with HTTP APIs \\n\\u2022 \\nBuilding\", \" gRPC Services with .NET video \\n \\n88 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nCHAPTER 5 \\nCloud-nat\", \"ive data patterns \\nAs we\\u2019ve seen throughout this book, a cloud-native approach changes the way you d\", \"esign, deploy, \\nand manage applications. It also changes the way you manage and store data. \\nFigure \", \"5-1 contrasts the differences. \\n \\nFigure 5-1. Data management in cloud-native applications \\nExperien\", \"ced developers will easily recognize the architecture on the left-side of figure 5-1. In this \\nmonol\", \"ithic application, business service components collocate together in a shared services tier, \\nsharin\", \"g data from a single relational database. \\nIn many ways, a single database keeps data management sim\", \"ple. Querying data across multiple tables \\nis straightforward. Changes to data update together or th\", \"ey all rollback. ACID transactions guarantee \\nstrong and immediate consistency. \\nDesigning for cloud\", \"-native, we take a different approach. On the right-side of Figure 5-1, note how \\nbusiness functiona\", \"lity segregates into small, independent microservices. Each microservice \\nencapsulates a specific bu\", \"siness capability and its own data. The monolithic database decomposes \\n \\n89 \\nCHAPTER 5 | Cloud-nati\", \"ve data patterns \\n \\ninto a distributed data model with many smaller databases, each aligning with a \", \"microservice. When \\nthe smoke clears, we emerge with a design that exposes a database per microservi\", \"ce. \\nDatabase-per-microservice, why? \\nThis database per microservice provides many benefits, especia\", \"lly for systems that must evolve rapidly \\nand support massive scale. With this model\\u2026 \\n\\u2022 \\nDomain dat\", \"a is encapsulated within the service \\n\\u2022 \\nData schema can evolve without directly impacting other ser\", \"vices \\n\\u2022 \\nEach data store can independently scale \\n\\u2022 \\nA data store failure in one service won\\u2019t dire\", \"ctly impact other services \\nSegregating data also enables each microservice to implement the data st\", \"ore type that is best \\noptimized for its workload, storage needs, and read/write patterns. Choices i\", \"nclude relational, \\ndocument, key-value, and even graph-based data stores. \\nFigure 5-2 presents the \", \"principle of polyglot persistence in a cloud-native system. \\n \\nFigure 5-2. Polyglot data persistence\", \" \\nNote in the previous figure how each microservice supports a different type of data store. \\n\\u2022 \\nThe\", \" product catalog microservice consumes a relational database to accommodate the rich \\nrelational str\", \"ucture of its underlying data. \\n\\u2022 \\nThe shopping cart microservice consumes a distributed cache that \", \"supports its simple, key-\\nvalue data store. \\n\\u2022 \\nThe ordering microservice consumes both a NoSql docu\", \"ment database for write operations \\nalong with a highly denormalized key/value store to accommodate \", \"high-volumes of read \\noperations. \\n \\n90 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nWhile relational \", \"databases remain relevant for microservices with complex data, NoSQL databases \\nhave gained consider\", \"able popularity. They provide massive scale and high availability. Their \\nschemaless nature allows d\", \"evelopers to move away from an architecture of typed data classes and \\nORMs that make change expensi\", \"ve and time-consuming. We cover NoSQL databases later in this \\nchapter. \\nWhile encapsulating data in\", \"to separate microservices can increase agility, performance, and scalability, \\nit also presents many\", \" challenges. In the next section, we discuss these challenges along with patterns \\nand practices to \", \"help overcome them. \\nCross-service queries \\nWhile microservices are independent and focus on specifi\", \"c functional capabilities, like inventory, \\nshipping, or ordering, they frequently require integrati\", \"on with other microservices. Often the \\nintegration involves one microservice querying another for d\", \"ata. Figure 5-3 shows the scenario. \\n \\nFigure 5-3. Querying across microservices \\nIn the preceding f\", \"igure, we see a shopping basket microservice that adds an item to a user\\u2019s shopping \\nbasket. While t\", \"he data store for this microservice contains basket and line item data, it doesn\\u2019t \\nmaintain product\", \" or pricing data. Instead, those data items are owned by the catalog and pricing \\nmicroservices. Thi\", \"s aspect presents a problem. How can the shopping basket microservice add a \\nproduct to the user\\u2019s s\", \"hopping basket when it doesn\\u2019t have product nor pricing data in its database? \\nOne option discussed \", \"in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and \\npricing microservice\", \"s. However, in chapter 4, we said synchronous HTTP calls couple microservices \\ntogether, reducing th\", \"eir autonomy and diminishing their architectural benefits. \\nWe could also implement a request-reply \", \"pattern with separate inbound and outbound queues for \\neach service. However, this pattern is compli\", \"cated and requires plumbing to correlate request and \\nresponse messages. While it does decouple the \", \"backend microservice calls, the calling service must \\nstill synchronously wait for the call to compl\", \"ete. Network congestion, transient faults, or an \\noverloaded microservice and can result in long-run\", \"ning and even failed operations. \\n \\n91 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nInstead, a widely \", \"accepted pattern for removing cross-service dependencies is the Materialized View \\nPattern, shown in\", \" Figure 5-4. \\n \\nFigure 5-4. Materialized View Pattern \\nWith this pattern, you place a local data tab\", \"le (known as a read model) in the shopping basket service. \\nThis table contains a denormalized copy \", \"of the data needed from the product and pricing \\nmicroservices. Copying the data directly into the s\", \"hopping basket microservice eliminates the need for \\nexpensive cross-service calls. With the data lo\", \"cal to the service, you improve the service\\u2019s response \\ntime and reliability. Additionally, having i\", \"ts own copy of the data makes the shopping basket service \\nmore resilient. If the catalog service sh\", \"ould become unavailable, it wouldn\\u2019t directly impact the \\nshopping basket service. The shopping bask\", \"et can continue operating with the data from its own \\nstore. \\nThe catch with this approach is that y\", \"ou now have duplicate data in your system. However, \\nstrategically duplicating data in cloud-native \", \"systems is an established practice and not considered an \\nanti-pattern, or bad practice. Keep in min\", \"d that one and only one service can own a data set and have \\nauthority over it. You\\u2019ll need to synch\", \"ronize the read models when the system of record is updated. \\nSynchronization is typically implement\", \"ed via asynchronous messaging with a publish/subscribe \\npattern, as shown in Figure 5.4. \\nDistribute\", \"d transactions \\nWhile querying data across microservices is difficult, implementing a transaction ac\", \"ross several \\nmicroservices is even more complex. The inherent challenge of maintaining data consist\", \"ency across \\nindependent data sources in different microservices can\\u2019t be understated. The lack of d\", \"istributed \\ntransactions in cloud-native applications means that you must manage distributed transac\", \"tions \\nprogrammatically. You move from a world of immediate consistency to that of eventual consiste\", \"ncy. \\nFigure 5-5 shows the problem. \\n \\n92 \\nCHAPTER 5 | Cloud-native data patterns \\n \\n \\nFigure 5-5. I\", \"mplementing a transaction across microservices \\nIn the preceding figure, five independent microservi\", \"ces participate in a distributed transaction that \\ncreates an order. Each microservice maintains its\", \" own data store and implements a local transaction \\nfor its store. To create the order, the local tr\", \"ansaction for each individual microservice must succeed, \\nor all must abort and roll back the operat\", \"ion. While built-in transactional support is available inside \\neach of the microservices, there\\u2019s no\", \" support for a distributed transaction that would span across all \\nfive services to keep data consis\", \"tent. \\nInstead, you must construct this distributed transaction programmatically. \\nA popular pattern\", \" for adding distributed transactional support is the Saga pattern. It\\u2019s implemented \\nby grouping loc\", \"al transactions together programmatically and sequentially invoking each one. If any \\nof the local t\", \"ransactions fail, the Saga aborts the operation and invokes a set of compensating \\ntransactions. The\", \" compensating transactions undo the changes made by the preceding local \\ntransactions and restore da\", \"ta consistency. Figure 5-6 shows a failed transaction with the Saga pattern. \\n \\nFigure 5-6. Rolling \", \"back a transaction \\n \\n93 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nIn the previous figure, the Upda\", \"te Inventory operation has failed in the Inventory microservice. The \\nSaga invokes a set of compensa\", \"ting transactions (in red) to adjust the inventory counts, cancel the \\npayment and the order, and re\", \"turn the data for each microservice back to a consistent state. \\nSaga patterns are typically choreog\", \"raphed as a series of related events, or orchestrated as a set of \\nrelated commands. In Chapter 4, w\", \"e discussed the service aggregator pattern that would be the \\nfoundation for an orchestrated saga im\", \"plementation. We also discussed eventing along with Azure \\nService Bus and Azure Event Grid topics t\", \"hat would be a foundation for a choreographed saga \\nimplementation. \\nHigh volume data \\nLarge cloud-n\", \"ative applications often support high-volume data requirements. In these scenarios, \\ntraditional dat\", \"a storage techniques can cause bottlenecks. For complex systems that deploy on a large \\nscale, both \", \"Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve \\napplication perf\", \"ormance. \\nCQRS \\nCQRS, is an architectural pattern that can help maximize performance, scalability, a\", \"nd security. The \\npattern separates operations that read data from those operations that write data.\", \" \\nFor normal scenarios, the same entity model and data repository object are used for both read and \", \"\\nwrite operations. \\nHowever, a high volume data scenario can benefit from separate models and data t\", \"ables for reads \\nand writes. To improve performance, the read operation could query against a highly\", \" denormalized \\nrepresentation of the data to avoid expensive repetitive table joins and table locks.\", \" The write \\noperation, known as a command, would update against a fully normalized representation of\", \" the data \\nthat would guarantee consistency. You then need to implement a mechanism to keep both \\nre\", \"presentations in sync. Typically, whenever the write table is modified, it publishes an event that \\n\", \"replicates the modification to the read table. \\nFigure 5-7 shows an implementation of the CQRS patte\", \"rn. \\n \\nFigure 5-7. CQRS implementation \\n \\n94 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nIn the previ\", \"ous figure, separate command and query models are implemented. Each data write \\noperation is saved t\", \"o the write store and then propagated to the read store. Pay close attention to \\nhow the data propag\", \"ation process operates on the principle of eventual consistency. The read model \\neventually synchron\", \"izes with the write model, but there may be some lag in the process. We discuss \\neventual consistenc\", \"y in the next section. \\nThis separation enables reads and writes to scale independently. Read operat\", \"ions use a schema \\noptimized for queries, while the writes use a schema optimized for updates. Read \", \"queries go against \\ndenormalized data, while complex business logic can be applied to the write mode\", \"l. As well, you \\nmight impose tighter security on write operations than those exposing reads. \\nImple\", \"menting CQRS can improve application performance for cloud-native services. However, it does \\nresult\", \" in a more complex design. Apply this principle carefully and strategically to those sections of \\nyo\", \"ur cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET\", \" \\nMicroservices: Architecture for Containerized .NET Applications. \\nEvent sourcing \\nAnother approach\", \" to optimizing high volume data scenarios involves Event Sourcing. \\nA system typically stores the cu\", \"rrent state of a data entity. If a user changes their phone number, for \\nexample, the customer recor\", \"d is updated with the new number. We always know the current state of a \\ndata entity, but each updat\", \"e overwrites the previous state. \\nIn most cases, this model works fine. In high volume systems, howe\", \"ver, overhead from transactional \\nlocking and frequent update operations can impact database perform\", \"ance, responsiveness, and limit \\nscalability. \\nEvent Sourcing takes a different approach to capturin\", \"g data. Each operation that affects data is \\npersisted to an event store. Instead of updating the st\", \"ate of a data record, we append each change to \\na sequential list of past events - similar to an acc\", \"ountant\\u2019s ledger. The Event Store becomes the \\nsystem of record for the data. It\\u2019s used to propagate\", \" various materialized views within the bounded \\ncontext of a microservice. Figure 5.8 shows the patt\", \"ern. \\n \\n95 \\nCHAPTER 5 | Cloud-native data patterns \\n \\n \\nFigure 5-8. Event Sourcing \\nIn the previous \", \"figure, note how each entry (in blue) for a user\\u2019s shopping cart is appended to an \\nunderlying event\", \" store. In the adjoining materialized view, the system projects the current state by \\nreplaying all \", \"the events associated with each shopping cart. This view, or read model, is then exposed \\nback to th\", \"e UI. Events can also be integrated with external systems and applications or queried to \\ndetermine \", \"the current state of an entity. With this approach, you maintain history. You know not only \\nthe cur\", \"rent state of an entity, but also how you reached this state. \\nMechanically speaking, event sourcing\", \" simplifies the write model. There are no updates or deletes. \\nAppending each data entry as an immut\", \"able event minimizes contention, locking, and concurrency \\nconflicts associated with relational data\", \"bases. Building read models with the materialized view pattern \\nenables you to decouple the view fro\", \"m the write model and choose the best data store to optimize \\nthe needs of your application UI. \\nFor\", \" this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, \\nMongoD\", \"B, Cassandra, CouchDB, and RavenDB are good candidates. \\nAs with all patterns and technologies, impl\", \"ement strategically and when needed. While event sourcing \\ncan provide increased performance and sca\", \"lability, it comes at the expense of complexity and a \\nlearning curve. \\n \\n96 \\nCHAPTER 5 | Cloud-nati\", \"ve data patterns \\n \\nRelational vs. NoSQL data \\nRelational and NoSQL are two types of database system\", \"s commonly implemented in cloud-native \\napps. They\\u2019re built differently, store data differently, and\", \" accessed differently. In this section, we\\u2019ll look \\nat both. Later in this chapter, we\\u2019ll look at an\", \" emerging database technology called NewSQL. \\nRelational databases have been a prevalent technology \", \"for decades. They\\u2019re mature, proven, and \\nwidely implemented. Competing database products, tooling, \", \"and expertise abound. Relational \\ndatabases provide a store of related data tables. These tables hav\", \"e a fixed schema, use SQL \\n(Structured Query Language) to manage data, and support ACID guarantees. \", \"\\nNo-SQL databases refer to high-performance, non-relational data stores. They excel in their ease-of\", \"-\\nuse, scalability, resilience, and availability characteristics. Instead of joining tables of norma\", \"lized data, \\nNoSQL stores unstructured or semi-structured data, often in key-value pairs or JSON doc\", \"uments. No-\\nSQL databases typically don\\u2019t provide ACID guarantees beyond the scope of a single datab\", \"ase \\npartition. High volume services that require sub second response time favor NoSQL datastores. \\n\", \"The impact of NoSQL technologies for distributed cloud-native systems can\\u2019t be overstated. The \\nprol\", \"iferation of new data technologies in this space has disrupted solutions that once exclusively \\nreli\", \"ed on relational databases. \\nNoSQL databases include several different models for accessing and mana\", \"ging data, each suited to \\nspecific use cases. Figure 5-9 presents four common models. \\n \\nFigure 5-9\", \": Data models for NoSQL databases \\nModel \\nCharacteristics \\nDocument Store \\nData and metadata are sto\", \"red hierarchically in \\nJSON-based documents inside the database. \\nKey Value Store \\nThe simplest of t\", \"he NoSQL databases, data is \\nrepresented as a collection of key-value pairs. \\nWide-Column Store \\nRel\", \"ated data is stored as a set of nested-\\nkey/value pairs within a single column. \\nGraph Store \\nData i\", \"s stored in a graph structure as node, \\nedge, and data properties. \\n \\n97 \\nCHAPTER 5 | Cloud-native d\", \"ata patterns \\n \\nThe CAP theorem \\nAs a way to understand the differences between these types of datab\", \"ases, consider the CAP theorem, \\na set of principles applied to distributed systems that store state\", \". Figure 5-10 shows the three \\nproperties of the CAP theorem. \\n \\nFigure 5-10. The CAP theorem \\nThe t\", \"heorem states that distributed data systems will offer a trade-off between consistency, \\navailabilit\", \"y, and partition tolerance. And, that any database can only guarantee two of the three \\nproperties: \", \"\\n\\u2022 \\nConsistency. Every node in the cluster responds with the most recent data, even if the system \\nm\", \"ust block the request until all replicas update. If you query a \\u201cconsistent system\\u201d for an item \\ntha\", \"t is currently updating, you\\u2019ll wait for that response until all replicas successfully update. \\nHowe\", \"ver, you\\u2019ll receive the most current data. \\n\\u2022 \\nAvailability. Every node returns an immediate respons\", \"e, even if that response isn\\u2019t the most \\nrecent data. If you query an \\u201cavailable system\\u201d for an item\", \" that is updating, you\\u2019ll get the best \\npossible answer the service can provide at that moment. \\n\\u2022 \\n\", \"Partition Tolerance. Guarantees the system continues to operate even if a replicated data \\nnode fail\", \"s or loses connectivity with other replicated data nodes. \\nCAP theorem explains the tradeoffs associ\", \"ated with managing consistency and availability during a \\nnetwork partition; however tradeoffs with \", \"respect to consistency and performance also exist with the \\nabsence of a network partition. CAP theo\", \"rem is often further extended to PACELC to explain the \\ntradeoffs more comprehensively. \\nRelational \", \"databases typically provide consistency and availability, but not partition tolerance. They\\u2019re \\ntypi\", \"cally provisioned to a single server and scale vertically by adding more resources to the machine. \\n\", \" \\n98 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nMany relational database systems support built-in re\", \"plication features where copies of the primary \\ndatabase can be made to other secondary server insta\", \"nces. Write operations are made to the primary \\ninstance and replicated to each of the secondaries. \", \"Upon a failure, the primary instance can fail over \\nto a secondary to provide high availability. Sec\", \"ondaries can also be used to distribute read operations. \\nWhile writes operations always go against \", \"the primary replica, read operations can be routed to any of \\nthe secondaries to reduce system load.\", \" \\nData can also be horizontally partitioned across multiple nodes, such as with sharding. But, shard\", \"ing \\ndramatically increases operational overhead by spitting data across many pieces that cannot eas\", \"ily \\ncommunicate. It can be costly and time consuming to manage. Relational features that include ta\", \"ble \\njoins, transactions, and referential integrity require steep performance penalties in sharded \\n\", \"deployments. \\nReplication consistency and recovery point objectives can be tuned by configuring whet\", \"her replication \\noccurs synchronously or asynchronously. If data replicas were to lose network conne\", \"ctivity in a \\u201chighly \\nconsistent\\u201d or synchronous relational database cluster, you wouldn\\u2019t be able t\", \"o write to the database. \\nThe system would reject the write operation as it can\\u2019t replicate that cha\", \"nge to the other data replica. \\nEvery data replica has to update before the transaction can complete\", \". \\nNoSQL databases typically support high availability and partition tolerance. They scale out \\nhori\", \"zontally, often across commodity servers. This approach provides tremendous availability, both \\nwith\", \"in and across geographical regions at a reduced cost. You partition and replicate data across \\nthese\", \" machines, or nodes, providing redundancy and fault tolerance. Consistency is typically tuned \\nthrou\", \"gh consensus protocols or quorum mechanisms. They provide more control when navigating \\ntradeoffs be\", \"tween tuning synchronous versus asynchronous replication in relational systems. \\nIf data replicas we\", \"re to lose connectivity in a \\u201chighly available\\u201d NoSQL database cluster, you could still \\ncomplete a \", \"write operation to the database. The database cluster would allow the write operation and \\nupdate ea\", \"ch data replica as it becomes available. NoSQL databases that support multiple writable \\nreplicas ca\", \"n further strengthen high availability by avoiding the need for failover when optimizing \\nrecovery t\", \"ime objective. \\nModern NoSQL databases typically implement partitioning capabilities as a feature of\", \" their system \\ndesign. Partition management is often built-in to the database, and routing is achiev\", \"ed through \\nplacement hints - often called partition keys. A flexible data models enables the NoSQL \", \"databases to \\nlower the burden of schema management and improve availability when deploying applicat\", \"ion \\nupdates that require data model changes. \\nHigh availability and massive scalability are often m\", \"ore critical to the business than relational table \\njoins and referential integrity. Developers can \", \"implement techniques and patterns such as Sagas, \\nCQRS, and asynchronous messaging to embrace eventu\", \"al consistency. \\nNowadays, care must be taken when considering the CAP theorem constraints. A new ty\", \"pe of \\ndatabase, called NewSQL, has emerged which extends the relational database engine to support \", \"both \\nhorizontal scalability and the scalable performance of NoSQL systems. \\n \\n99 \\nCHAPTER 5 | Cloud\", \"-native data patterns \\n \\nConsiderations for relational vs. NoSQL systems \\nBased upon specific data r\", \"equirements, a cloud-native-based microservice can implement a relational, \\nNoSQL datastore or both.\", \" \\nConsider a NoSQL datastore when: \\nConsider a relational database when: \\nYou have high volume workl\", \"oads that require \\npredictable latency at large scale (for example, \\nlatency measured in millisecond\", \"s while \\nperforming millions of transactions per second) \\nYour workload volume generally fits within\", \" \\nthousands of transactions per second \\nYour data is dynamic and frequently changes \\nYour data is hi\", \"ghly structured and requires \\nreferential integrity \\nRelationships can be de-normalized data \\nmodels\", \" \\nRelationships are expressed through table joins \\non normalized data models \\nData retrieval is simp\", \"le and expressed without \\ntable joins \\nYou work with complex queries and reports \\nData is typically \", \"replicated across geographies \\nand requires finer control over consistency, \\navailability, and perfo\", \"rmance \\nData is typically centralized, or can be replicated \\nregions asynchronously \\nYour applicatio\", \"n will be deployed to commodity \\nhardware, such as with public clouds \\nYour application will be depl\", \"oyed to large, high-\\nend hardware \\nIn the next sections, we\\u2019ll explore the options available in the \", \"Azure cloud for storing and managing \\nyour cloud-native data. \\nDatabase as a Service \\nTo start, you \", \"could provision an Azure virtual machine and install your database of choice for each \\nservice. Whil\", \"e you\\u2019d have full control over the environment, you\\u2019d forgo many built-in features of the \\ncloud pla\", \"tform. You\\u2019d also be responsible for managing the virtual machine and database for each \\nservice. Th\", \"is approach could quickly become time-consuming and expensive. \\nInstead, cloud-native applications f\", \"avor data services exposed as a Database as a Service (DBaaS). \\nFully managed by a cloud vendor, the\", \"se services provide built-in security, scalability, and monitoring. \\nInstead of owning the service, \", \"you simply consume it as a backing service. The provider operates the \\nresource at scale and bears t\", \"he responsibility for performance and maintenance. \\nThey can be configured across cloud availability\", \" zones and regions to achieve high availability. They \\nall support just-in-time capacity and a pay-a\", \"s-you-go model. Azure features different kinds of \\nmanaged data service options, each with specific \", \"benefits. \\nWe\\u2019ll first look at relational DBaaS services available in Azure. You\\u2019ll see that Microso\", \"ft\\u2019s flagship SQL \\nServer database is available along with several open-source options. Then, we\\u2019ll \", \"talk about the NoSQL \\ndata services in Azure. \\n \\n100 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nAzur\", \"e relational databases \\nFor cloud-native microservices that require relational data, Azure offers fo\", \"ur managed relational \\ndatabases as a service (DBaaS) offerings, shown in Figure 5-11. \\n \\nFigure 5-1\", \"1. Managed relational databases available in Azure \\nIn the previous figure, note how each sits upon \", \"a common DBaaS infrastructure which features key \\ncapabilities at no additional cost. \\nThese feature\", \"s are especially important to organizations who provision large numbers of databases, \\nbut have limi\", \"ted resources to administer them. You can provision an Azure database in minutes by \\nselecting the a\", \"mount of processing cores, memory, and underlying storage. You can scale the \\ndatabase on-the-fly an\", \"d dynamically adjust resources with little to no downtime. \\nAzure SQL Database \\nDevelopment teams wi\", \"th expertise in Microsoft SQL Server should consider Azure SQL Database. It\\u2019s a \\nfully managed relat\", \"ional database-as-a-service (DBaaS) based on the Microsoft SQL Server Database \\nEngine. The service \", \"shares many features found in the on-premises version of SQL Server and runs the \\nlatest stable vers\", \"ion of the SQL Server Database Engine. \\nFor use with a cloud-native microservice, Azure SQL Database\", \" is available with three deployment \\noptions: \\n\\u2022 \\nA Single Database represents a fully managed SQL D\", \"atabase running on an Azure SQL \\nDatabase server in the Azure cloud. The database is considered cont\", \"ained as it has no \\nconfiguration dependencies on the underlying database server. \\n\\u2022 \\nA Managed Inst\", \"ance is a fully managed instance of the Microsoft SQL Server Database Engine \\nthat provides near-100\", \"% compatibility with an on-premises SQL Server. This option supports \\nlarger databases, up to 35 TB \", \"and is placed in an Azure Virtual Network for better isolation. \\n \\n101 \\nCHAPTER 5 | Cloud-native dat\", \"a patterns \\n \\n\\u2022 \\nAzure SQL Database serverless is a compute tier for a single database that automati\", \"cally \\nscales based on workload demand. It bills only for the amount of compute used per second. \\nTh\", \"e service is well suited for workloads with intermittent, unpredictable usage patterns, \\nintersperse\", \"d with periods of inactivity. The serverless compute tier also automatically pauses \\ndatabases durin\", \"g inactive periods so that only storage charges are billed. It automatically \\nresumes when activity \", \"returns. \\nBeyond the traditional Microsoft SQL Server stack, Azure also features managed versions of\", \" three \\npopular open-source databases. \\nOpen-source databases in Azure \\nOpen-source relational datab\", \"ases have become a popular choice for cloud-native applications. Many \\nenterprises favor them over c\", \"ommercial database products, especially for cost savings. Many \\ndevelopment teams enjoy their flexib\", \"ility, community-backed development, and ecosystem of tools \\nand extensions. Open-source databases c\", \"an be deployed across multiple cloud providers, helping \\nminimize the concern of \\u201cvendor lock-in.\\u201d \\n\", \"Developers can easily self-host any open-source database on an Azure VM. While providing full \\ncontr\", \"ol, this approach puts you on the hook for the management, monitoring, and maintenance of \\nthe datab\", \"ase and VM. \\nHowever, Microsoft continues its commitment to keeping Azure an \\u201copen platform\\u201d by offe\", \"ring \\nseveral popular open-source databases as fully managed DBaaS services. \\nAzure Database for MyS\", \"QL \\nMySQL is an open-source relational database and a pillar for applications built on the LAMP soft\", \"ware \\nstack. Widely chosen for read heavy workloads, it\\u2019s used by many large organizations, includin\", \"g \\nFacebook, Twitter, and YouTube. The community edition is available for free, while the enterprise\", \" \\nedition requires a license purchase. Originally created in 1995, the product was purchased by Sun \", \"\\nMicrosystems in 2008. Oracle acquired Sun and MySQL in 2010. \\nAzure Database for MySQL is a managed\", \" relational database service based on the open-source \\nMySQL Server engine. It uses the MySQL Commun\", \"ity edition. The Azure MySQL server is the \\nadministrative point for the service. It\\u2019s the same MySQ\", \"L server engine used for on-premises \\ndeployments. The engine can create a single database per serve\", \"r or multiple databases per server that \\nshare resources. You can continue to manage data using the \", \"same open-source tools without having \\nto learn new skills or manage virtual machines. \\nAzure Databa\", \"se for MariaDB \\nMariaDB Server is another popular open-source database server. It was created as a f\", \"ork of MySQL \\nwhen Oracle purchased Sun Microsystems, who owned MySQL. The intent was to ensure that\", \" MariaDB \\nremained open-source. As MariaDB is a fork of MySQL, the data and table definitions are co\", \"mpatible, \\nand the client protocols, structures, and APIs, are close-knit. \\n \\n102 \\nCHAPTER 5 | Cloud\", \"-native data patterns \\n \\nMariaDB has a strong community and is used by many large enterprises. While\", \" Oracle continues to \\nmaintain, enhance, and support MySQL, the MariaDB foundation manages MariaDB, \", \"allowing public \\ncontributions to the product and documentation. \\nAzure Database for MariaDB is a fu\", \"lly managed relational database as a service in the Azure cloud. The \\nservice is based on the MariaD\", \"B community edition server engine. It can handle mission-critical \\nworkloads with predictable perfor\", \"mance and dynamic scalability. \\nAzure Database for PostgreSQL \\nPostgreSQL is an open-source relation\", \"al database with over 30 years of active development. \\nPostgreSQL has a strong reputation for reliab\", \"ility and data integrity. It\\u2019s feature rich, SQL compliant, \\nand considered more performant than MyS\", \"QL - especially for workloads with complex queries and \\nheavy writes. Many large enterprises includi\", \"ng Apple, Red Hat, and Fujitsu have built products using \\nPostgreSQL. \\nAzure Database for PostgreSQL\", \" is a fully managed relational database service, based on the open-\\nsource Postgres database engine.\", \" The service supports many development platforms, including C++, \\nJava, Python, Node, C#, and PHP. Y\", \"ou can migrate PostgreSQL databases to it using the command-\\nline interface tool or Azure Data Migra\", \"tion Service. \\nAzure Database for PostgreSQL is available with two deployment options: \\n\\u2022 \\nThe Singl\", \"e Server deployment option is a central administrative point for multiple databases \\nto which you ca\", \"n deploy many databases. The pricing is structured per-server based upon \\ncores and storage. \\n\\u2022 \\nThe\", \" Hyperscale (Citus) option is powered by Citus Data technology. It enables high \\nperformance by hori\", \"zontally scaling a single database across hundreds of nodes to deliver fast \\nperformance and scale. \", \"This option allows the engine to fit more data in memory, parallelize \\nqueries across hundreds of no\", \"des, and index data faster. \\nNoSQL data in Azure \\nCosmos DB is a fully managed, globally distributed\", \" NoSQL database service in the Azure cloud. It has \\nbeen adopted by many large companies across the \", \"world, including Coca-Cola, Skype, ExxonMobil, \\nand Liberty Mutual. \\nIf your services require fast r\", \"esponse from anywhere in the world, high availability, or elastic \\nscalability, Cosmos DB is a great\", \" choice. Figure 5-12 shows Cosmos DB. \\n \\n103 \\nCHAPTER 5 | Cloud-native data patterns \\n \\n \\nFigure 5-1\", \"2: Overview of Azure Cosmos DB \\nThe previous figure presents many of the built-in cloud-native capab\", \"ilities available in Cosmos DB. In \\nthis section, we\\u2019ll take a closer look at them. \\nGlobal support \", \"\\nCloud-native applications often have a global audience and require global scale. \\nYou can distribut\", \"e Cosmos databases across regions or around the world, placing data close to your \\nusers, improving \", \"response time, and reducing latency. You can add or remove a database from a \\nregion without pausing\", \" or redeploying your services. In the background, Cosmos DB transparently \\nreplicates the data to ea\", \"ch of the configured regions. \\nCosmos DB supports active/active clustering at the global level, enab\", \"ling you to configure any of your \\ndatabase regions to support both writes and reads. \\nThe Multi-reg\", \"ion write protocol is an important feature in Cosmos DB that enables the following \\nfunctionality: \\n\", \"\\u2022 \\nUnlimited elastic write and read scalability. \\n\\u2022 \\n99.999% read and write availability all around \", \"the world. \\n\\u2022 \\nGuaranteed reads and writes served in less than 10 milliseconds at the 99th percentil\", \"e. \\nWith the Cosmos DB Multi-Homing APIs, your microservice is automatically aware of the nearest \\nA\", \"zure region and sends requests to it. The nearest region is identified by Cosmos DB without any \\ncon\", \"figuration changes. Should a region become unavailable, the Multi-Homing feature will \\nautomatically\", \" route requests to the next nearest available region. \\nMulti-model support \\nWhen replatforming monol\", \"ithic applications to a cloud-native architecture, development teams \\nsometimes have to migrate open\", \"-source, NoSQL data stores. Cosmos DB can help you preserve your \\n \\n104 \\nCHAPTER 5 | Cloud-native da\", \"ta patterns \\n \\ninvestment in these NoSQL datastores with its multi-model data platform. The followin\", \"g table shows \\nthe supported NoSQL compatibility APIs. \\nProvider \\nDescription \\nNoSQL API \\nAPI for No\", \"SQL stores data in document format \\nMongo DB API \\nSupports Mongo DB APIs and JSON documents \\nGremlin\", \" API \\nSupports Gremlin API with graph-based nodes \\nand edge data representations \\nCassandra API \\nSup\", \"ports Casandra API for wide-column data \\nrepresentations \\nTable API \\nSupports Azure Table Storage wi\", \"th premium \\nenhancements \\nPostgreSQL API \\nManaged service for running PostgreSQL at any \\nscale \\nDeve\", \"lopment teams can migrate existing Mongo, Gremlin, or Cassandra databases into Cosmos DB \\nwith minim\", \"al changes to data or code. For new apps, development teams can choose among open-\\nsource options or\", \" the built-in SQL API model. \\nInternally, Cosmos stores the data in a simple struct format made up o\", \"f primitive data types. For each \\nrequest, the database engine translates the primitive data into th\", \"e model representation you\\u2019ve \\nselected. \\nIn the previous table, note the Table API option. This API\", \" is an evolution of Azure Table Storage. Both \\nshare the same underlying table model, but the Cosmos\", \" DB Table API adds premium enhancements \\nnot available in the Azure Storage API. The following table\", \" contrasts the features. \\nFeature \\nAzure Table Storage \\nAzure Cosmos DB \\nLatency \\nFast \\nSingle-digit\", \" millisecond latency for reads and \\nwrites anywhere in the world \\nThroughp\\nut \\nLimit of 20,000 opera\", \"tions per table \\nUnlimited operations per table \\nGlobal \\nDistributio\\nn \\nSingle region with optional \", \"single \\nsecondary read region \\nTurnkey distributions to all regions with \\nautomatic failover \\nIndexi\", \"ng \\nAvailable for partition and row key \\nproperties only \\nAutomatic indexing of all properties \\nPric\", \"ing \\nOptimized for cold workloads (low \\nthroughput : storage ratio) \\nOptimized for hot workloads (hi\", \"gh \\nthroughput : storage ratio) \\nMicroservices that consume Azure Table storage can easily migrate t\", \"o the Cosmos DB Table API. No \\ncode changes are required. \\n \\n105 \\nCHAPTER 5 | Cloud-native data patt\", \"erns \\n \\nTunable consistency \\nEarlier in the Relational vs. NoSQL section, we discussed the subject o\", \"f data consistency. Data \\nconsistency refers to the integrity of your data. Cloud-native services wi\", \"th distributed data rely on \\nreplication and must make a fundamental tradeoff between read consisten\", \"cy, availability, and latency. \\nMost distributed databases allow developers to choose between two co\", \"nsistency \\nmodels: strong consistency and eventual consistency. Strong consistency is the gold stand\", \"ard of data \\nprogrammability. It guarantees that a query will always return the most current data - \", \"even if the \\nsystem must incur latency waiting for an update to replicate across all database copies\", \". While a \\ndatabase configured for eventual consistency will return data immediately, even if that d\", \"ata isn\\u2019t the \\nmost current copy. The latter option enables higher availability, greater scale, and \", \"increased \\nperformance. \\nAzure Cosmos DB offers five well-defined consistency models shown in Figure\", \" 5-13. \\n \\nFigure 5-13: Cosmos DB Consistency Levels \\nThese options enable you to make precise choice\", \"s and granular tradeoffs for consistency, availability, \\nand the performance for your data. The leve\", \"ls are presented in the following table. \\nConsistency Level \\nDescription \\nEventual \\nNo ordering guar\", \"antee for reads. Replicas will \\neventually converge. \\nConstant Prefix \\nReads are still eventual, but\", \" data is returned in \\nthe ordering in which it is written. \\nSession \\nGuarantees you can read any dat\", \"a written \\nduring the current session. It is the default \\nconsistency level. \\nBounded Staleness \\nRea\", \"ds trail writes by interval that you specify. \\nStrong \\nReads are guaranteed to return most recent \\nc\", \"ommitted version of an item. A client never \\nsees an uncommitted or partial read. \\nIn the article Ge\", \"tting Behind the 9-Ball: Cosmos DB Consistency Levels Explained, Microsoft Program \\nManager Jeremy L\", \"ikness provides an excellent explanation of the five models. \\nPartitioning \\nAzure Cosmos DB embraces\", \" automatic partitioning to scale a database to meet the performance \\nneeds of your cloud-native serv\", \"ices. \\nYou manage data in Cosmos DB data by creating databases, containers, and items. \\n \\n106 \\nCHAPT\", \"ER 5 | Cloud-native data patterns \\n \\nContainers live in a Cosmos DB database and represent a schema-\", \"agnostic grouping of items. Items \\nare the data that you add to the container. They\\u2019re represented a\", \"s documents, rows, nodes, or edges. \\nAll items added to a container are automatically indexed. \\nTo p\", \"artition the container, items are divided into distinct subsets called logical partitions. Logical \\n\", \"partitions are populated based on the value of a partition key that is associated with each item in \", \"a \\ncontainer. Figure 5-14 shows two containers each with a logical partition based on a partition ke\", \"y \\nvalue. \\n \\nFigure 5-14: Cosmos DB partitioning mechanics \\nNote in the previous figure how each ite\", \"m includes a partition key of either \\u2018city\\u2019 or \\u2018airport\\u2019. The key \\ndetermines the item\\u2019s logical par\", \"tition. Items with a city code are assigned to the container on the left, \\nand items with an airport\", \" code, to the container on the right. Combining the partition key value with \\nthe ID value creates a\", \"n item\\u2019s index, which uniquely identifies the item. \\nInternally, Cosmos DB automatically manages the\", \" placement of logical partitions on physical \\npartitions to satisfy the scalability and performance \", \"needs of the container. As application throughput \\nand storage requirements increase, Azure Cosmos D\", \"B redistributes logical partitions across a greater \\nnumber of servers. Redistribution operations ar\", \"e managed by Cosmos DB and invoked without \\ninterruption or downtime. \\nNewSQL databases \\nNewSQL is a\", \"n emerging database technology that combines the distributed scalability of NoSQL with \\nthe ACID gua\", \"rantees of a relational database. NewSQL databases are important for business systems \\nthat must pro\", \"cess high-volumes of data, across distributed environments, with full transactional \\nsupport and ACI\", \"D compliance. While a NoSQL database can provide massive scalability, it does not \\nguarantee data co\", \"nsistency. Intermittent problems from inconsistent data can place a burden on the \\ndevelopment team.\", \" Developers must construct safeguards into their microservice code to manage \\nproblems caused by inc\", \"onsistent data. \\nThe Cloud Native Computing Foundation (CNCF) features several NewSQL database proje\", \"cts. \\n \\n107 \\nCHAPTER 5 | Cloud-native data patterns \\n \\nProject \\nCharacteristics \\nCockroach DB \\nAn AC\", \"ID-compliant, relational database that \\nscales globally. Add a new node to a cluster and \\nCockroachD\", \"B takes care of balancing the data \\nacross instances and geographies. It creates, \\nmanages, and dist\", \"ributes replicas to ensure \\nreliability. It\\u2019s open source and freely available. \\nTiDB \\nAn open-sourc\", \"e database that supports Hybrid \\nTransactional and Analytical Processing (HTAP) \\nworkloads. It is My\", \"SQL-compatible and features \\nhorizontal scalability, strong consistency, and \\nhigh availability. TiD\", \"B acts like a MySQL server. \\nYou can continue to use existing MySQL client \\nlibraries, without requi\", \"ring extensive code \\nchanges to your application. \\nYugabyteDB \\nAn open source, high-performance, dis\", \"tributed \\nSQL database. It supports low query latency, \\nresilience against failures, and global data\", \" \\ndistribution. YugabyteDB is PostgreSQL-\\ncompatible and handles scale-out RDBMS and \\ninternet-scale\", \" OLTP workloads. The product also \\nsupports NoSQL and is compatible with \\nCassandra. \\nVitess \\nVitess\", \" is a database solution for deploying, \\nscaling, and managing large clusters of MySQL \\ninstances. It\", \" can run in a public or private cloud \\narchitecture. Vitess combines and extends many \\nimportant MyS\", \"QL features and features both \\nvertical and horizontal sharding support. \\nOriginated by YouTube, Vit\", \"ess has been serving \\nall YouTube database traffic since 2011. \\nThe open-source projects in the prev\", \"ious figure are available from the Cloud Native Computing \\nFoundation. Three of the offerings are fu\", \"ll database products, which include .NET support. The other, \\nVitess, is a database clustering syste\", \"m that horizontally scales large clusters of MySQL instances. \\nA key design goal for NewSQL database\", \"s is to work natively in Kubernetes, taking advantage of the \\nplatform\\u2019s resiliency and scalability.\", \" \\nNewSQL databases are designed to thrive in ephemeral cloud environments where underlying virtual \\n\", \"machines can be restarted or rescheduled at a moment\\u2019s notice. The databases are designed to \\nsurviv\", \"e node failures without data loss nor downtime. CockroachDB, for example, is able to survive a \\nmach\", \"ine loss by maintaining three consistent replicas of any data across the nodes in a cluster. \\nKubern\", \"etes uses a Services construct to allow a client to address a group of identical NewSQL \\ndatabases p\", \"rocesses from a single DNS entry. By decoupling the database instances from the address \\n \\n108 \\nCHAP\", \"TER 5 | Cloud-native data patterns \\n \\nof the service with which it\\u2019s associated, we can scale withou\", \"t disrupting existing application instances. \\nSending a request to any service at a given time will \", \"always yield the same result. \\nIn this scenario, all database instances are equal. There are no prim\", \"ary or secondary relationships. \\nTechniques like consensus replication found in CockroachDB allow an\", \"y database node to handle any \\nrequest. If the node that receives a load-balanced request has the da\", \"ta it needs locally, it responds \\nimmediately. If not, the node becomes a gateway and forwards the r\", \"equest to the appropriate nodes \\nto get the correct answer. From the client\\u2019s perspective, every dat\", \"abase node is the same: They appear \\nas a single logical database with the consistency guarantees of\", \" a single-machine system, despite \\nhaving dozens or even hundreds of nodes that are working behind t\", \"he scenes. \\nFor a detailed look at the mechanics behind NewSQL databases, see the DASH: Four Propert\", \"ies of \\nKubernetes-Native Databases article. \\nData migration to the cloud \\nOne of the more time-cons\", \"uming tasks is migrating data from one data platform to another. The \\nAzure Data Migration Service c\", \"an help expedite such efforts. It can migrate data from several external \\ndatabase sources into Azur\", \"e Data platforms with minimal downtime. Target platforms include the \\nfollowing services: \\n\\u2022 \\nAzure \", \"SQL Database \\n\\u2022 \\nAzure Database for MySQL \\n\\u2022 \\nAzure Database for MariaDB \\n\\u2022 \\nAzure Database for Post\", \"greSQL \\n\\u2022 \\nAzure Cosmos DB \\nThe service provides recommendations to guide you through the changes re\", \"quired to execute a \\nmigration, both small or large. \\nCaching in a cloud-native app \\nThe benefits of\", \" caching are well understood. The technique works by temporarily copying frequently \\naccessed data f\", \"rom a backend data store to fast storage that\\u2019s located closer to the application. \\nCaching is often\", \" implemented where\\u2026 \\n\\u2022 \\nData remains relatively static. \\n\\u2022 \\nData access is slow, especially compared\", \" to the speed of the cache. \\n\\u2022 \\nData is subject to high levels of contention. \\nWhy? \\nAs discussed in\", \" the Microsoft caching guidance, caching can increase performance, scalability, and \\navailability fo\", \"r individual microservices and the system as a whole. It reduces the latency and \\ncontention of hand\", \"ling large volumes of concurrent requests to a data store. As data volume and the \\nnumber of users i\", \"ncrease, the greater the benefits of caching become. \\n \\n109 \\nCHAPTER 5 | Cloud-native data patterns \", \"\\n \\nCaching is most effective when a client repeatedly reads data that is immutable or that changes \\n\", \"infrequently. Examples include reference information such as product and pricing information, or \\nsh\", \"ared static resources that are costly to construct. \\nWhile microservices should be stateless, a dist\", \"ributed cache can support concurrent access to session \\nstate data when absolutely required. \\nAlso c\", \"onsider caching to avoid repetitive computations. If an operation transforms data or performs a \\ncom\", \"plicated calculation, cache the result for subsequent requests. \\nCaching architecture \\nCloud native \", \"applications typically implement a distributed caching architecture. The cache is hosted \\nas a cloud\", \"-based backing service, separate from the microservices. Figure 5-15 shows the architecture. \\n \\nFigu\", \"re 5-15: Caching in a cloud native app \\nIn the previous figure, note how the cache is independent of\", \" and shared by the microservices. In this \\nscenario, the cache is invoked by the API Gateway. As dis\", \"cussed in chapter 4, the gateway serves as a \\nfront end for all incoming requests. The distributed c\", \"ache increases system responsiveness by \\nreturning cached data whenever possible. Additionally, sepa\", \"rating the cache from the services allows \\nthe cache to scale up or out independently to meet increa\", \"sed traffic demands. \\nThe previous figure presents a common caching pattern known as the cache-aside\", \" pattern. For an \\nincoming request, you first query the cache (step #1) for a response. If found, th\", \"e data is returned \\nimmediately. If the data doesn\\u2019t exist in the cache (known as a cache miss), it\\u2019\", \"s retrieved from a local \\ndatabase in a downstream service (step #2). It\\u2019s then written to the cache\", \" for future requests (step #3), \\nand returned to the caller. Care must be taken to periodically evic\", \"t cached data so that the system \\nremains timely and consistent. \\nAs a shared cache grows, it might \", \"prove beneficial to partition its data across multiple nodes. Doing \\nso can help minimize contention\", \" and improve scalability. Many Caching services support the ability to \\n \\n110 \\nCHAPTER 5 | Cloud-nat\", \"ive data patterns \\n \\ndynamically add and remove nodes and rebalance data across partitions. This app\", \"roach typically \\ninvolves clustering. Clustering exposes a collection of federated nodes as a seamle\", \"ss, single cache. \\nInternally, however, the data is dispersed across the nodes following a predefine\", \"d distribution strategy \\nthat balances the load evenly. \\nAzure Cache for Redis \\nAzure Cache for Redi\", \"s is a secure data caching and messaging broker service, fully managed by \\nMicrosoft. Consumed as a \", \"Platform as a Service (PaaS) offering, it provides high throughput and low-\\nlatency access to data. \", \"The service is accessible to any application within or outside of Azure. \\nThe Azure Cache for Redis \", \"service manages access to open-source Redis servers hosted across Azure \\ndata centers. The service a\", \"cts as a facade providing management, access control, and security. The \\nservice natively supports a\", \" rich set of data structures, including strings, hashes, lists, and sets. If your \\napplication alrea\", \"dy uses Redis, it will work as-is with Azure Cache for Redis. \\nAzure Cache for Redis is more than a \", \"simple cache server. It can support a number of scenarios to \\nenhance a microservices architecture: \", \"\\n\\u2022 \\nAn in-memory data store \\n\\u2022 \\nA distributed non-relational database \\n\\u2022 \\nA message broker \\n\\u2022 \\nA con\", \"figuration or discovery server \\nFor advanced scenarios, a copy of the cached data can be persisted t\", \"o disk. If a catastrophic event \\ndisables both the primary and replica caches, the cache is reconstr\", \"ucted from the most recent \\nsnapshot. \\nAzure Redis Cache is available across a number of predefined \", \"configurations and pricing tiers. The \\nPremium tier features many enterprise-level features such as \", \"clustering, data persistence, geo-\\nreplication, and virtual-network isolation. \\nElasticsearch in a c\", \"loud-native app \\nElasticsearch is a distributed search and analytics system that enables complex sea\", \"rch capabilities \\nacross diverse types of data. It\\u2019s open source and widely popular. Consider how th\", \"e following \\ncompanies integrate Elasticsearch into their application: \\n\\u2022 \\nWikipedia for full-text a\", \"nd incremental (search as you type) searching. \\n\\u2022 \\nGitHub to index and expose over 8 million code re\", \"positories. \\n \\n\\u2022 \\nDocker for making its container library discoverable. \\nElasticsearch is built on t\", \"op of the Apache Lucene full-text search engine. Lucene provides high-\\nperformance document indexing\", \" and querying. It indexes data with an inverted indexing scheme \\u2013 \\ninstead of mapping pages to keywo\", \"rds, it maps keywords to pages just like a glossary at the end of a \\nbook. Lucene has powerful query\", \" syntax capabilities and can query data by: \\n \\n111 \\nCHAPTER 5 | Cloud-native data patterns \\n \\n\\u2022 \\nTer\", \"m (a full word) \\n\\u2022 \\nPrefix (starts-with word) \\n\\u2022 \\nWildcard (using \\u201c*\\u201d or \\u201c?\\u201d filters) \\n\\u2022 \\nPhrase (a \", \"sequence of text in a document) \\n\\u2022 \\nBoolean value (complex searches combining queries) \\nWhile Lucene\", \" provides low-level plumbing for searching, Elasticsearch provides the server that sits on \\ntop of L\", \"ucene. Elasticsearch adds higher-level functionality to simplify working Lucene, including a \\nRESTfu\", \"l API to access Lucene\\u2019s indexing and searching functionality. It also provides a distributed \\ninfra\", \"structure capable of massive scalability, fault tolerance, and high availability. \\nFor larger cloud-\", \"native applications with complex search requirements, Elasticsearch is available as \\nmanaged service\", \" in Azure. The Microsoft Azure Marketplace features preconfigured templates which \\ndevelopers can us\", \"e to deploy an Elasticsearch cluster on Azure. \\nFrom the Microsoft Azure Marketplace, developers can\", \" use preconfigured templates built to quickly \\ndeploy an Elasticsearch cluster on Azure. Using the A\", \"zure-managed offering, you can deploy up to 50 \\ndata nodes, 20 coordinating nodes, and three dedicat\", \"ed master nodes. \\nSummary \\nThis chapter presented a detailed look at data in cloud-native systems. W\", \"e started by contrasting data \\nstorage in monolithic applications with data storage patterns in clou\", \"d-native systems. We looked at \\ndata patterns implemented in cloud-native systems, including cross-s\", \"ervice queries, distributed \\ntransactions, and patterns to deal with high-volume systems. We contras\", \"ted SQL with NoSQL data. \\nWe looked at data storage options available in Azure that include both Mic\", \"rosoft-centric and open-\\nsource options. Finally, we discussed caching and Elasticsearch in a cloud-\", \"native application. \\nReferences \\n\\u2022 \\nCommand and Query Responsibility Segregation (CQRS) pattern \\n\\u2022 \\n\", \"Event Sourcing pattern \\n\\u2022 \\nWhy isn\\u2019t RDBMS Partition Tolerant in CAP Theorem and why is it Available\", \"? \\n\\u2022 \\nMaterialized View \\n\\u2022 \\nAll you really need to know about open source databases \\n\\u2022 \\nCompensating\", \" Transaction pattern \\n\\u2022 \\nSaga Pattern \\n\\u2022 \\nSaga Patterns | How to implement business transactions usi\", \"ng microservices \\n\\u2022 \\nCompensating Transaction pattern \\n\\u2022 \\nGetting Behind the 9-Ball: Cosmos DB Consi\", \"stency Levels Explained \\n\\u2022 \\nOn RDBMS, NoSQL and NewSQL databases. Interview with John Ryan \\n \\n112 \\nC\", \"HAPTER 5 | Cloud-native data patterns \\n \\n\\u2022 \\nSQL vs NoSQL vs NewSQL: The Full Comparison \\n\\u2022 \\nDASH: Fo\", \"ur Properties of Kubernetes-Native Databases \\n\\u2022 \\nCockroachDB \\n\\u2022 \\nTiDB \\n\\u2022 \\nYugabyteDB \\n\\u2022 \\nVitess \\n\\u2022 \\n\", \"Elasticsearch: The Definitive Guide \\n\\u2022 \\nIntroduction to Apache Lucene \\n \\n113 \\nCHAPTER 6 | Cloud-nati\", \"ve resiliency \\n \\nCHAPTER 6 \\nCloud-native resiliency \\nResiliency is the ability of your system to rea\", \"ct to failure and still remain functional. It\\u2019s not about \\navoiding failure, but accepting failure a\", \"nd constructing your cloud-native services to respond to it. \\nYou want to return to a fully function\", \"ing state quickly as possible. \\nUnlike traditional monolithic applications, where everything runs to\", \"gether in a single process, cloud-\\nnative systems embrace a distributed architecture as shown in Fig\", \"ure 6-1: \\n \\nFigure 6-1. Distributed cloud-native environment \\nIn the previous figure, each microserv\", \"ice and cloud-based backing service execute in a separate \\nprocess, across server infrastructure, co\", \"mmunicating via network-based calls. \\nOperating in this environment, a service must be sensitive to \", \"many different challenges: \\n\\u2022 \\nUnexpected network latency - the time for a service request to travel\", \" to the receiver and back. \\n\\u2022 \\nTransient faults - short-lived network connectivity errors. \\n\\u2022 \\nBlock\", \"age by a long-running synchronous operation. \\n\\u2022 \\nA host process that has crashed and is being restar\", \"ted or moved. \\n\\u2022 \\nAn overloaded microservice that can\\u2019t respond for a short time. \\n\\u2022 \\nAn in-flight o\", \"rchestrator operation such as a rolling upgrade or moving a service from one \\nnode to another. \\n \\n11\", \"4 \\nCHAPTER 6 | Cloud-native resiliency \\n \\n\\u2022 \\nHardware failures. \\nCloud platforms can detect and miti\", \"gate many of these infrastructure issues. It may restart, scale out, \\nand even redistribute your ser\", \"vice to a different node. However, to take full advantage of this built-in \\nprotection, you must des\", \"ign your services to react to it and thrive in this dynamic environment. \\nIn the following sections,\", \" we\\u2019ll explore defensive techniques that your service and managed cloud \\nresources can leverage to m\", \"inimize downtime and disruption. \\nApplication resiliency patterns \\nThe first line of defense is appl\", \"ication resiliency. \\nWhile you could invest considerable time writing your own resiliency framework,\", \" such products \\nalready exist. Polly is a comprehensive .NET resilience and transient-fault-handling\", \" library that allows \\ndevelopers to express resiliency policies in a fluent and thread-safe manner. \", \"Polly targets applications \\nbuilt with either .NET Framework or .NET 7. The following table describe\", \"s the resiliency features, called \\npolicies, available in the Polly Library. They can be applied ind\", \"ividually or grouped together. \\nPolicy \\nExperience \\nRetry \\nConfigures retry operations on designated\", \" \\noperations. \\nCircuit Breaker \\nBlocks requested operations for a predefined \\nperiod when faults exc\", \"eed a configured \\nthreshold \\nTimeout \\nPlaces limit on the duration for which a caller \\ncan wait for \", \"a response. \\nBulkhead \\nConstrains actions to fixed-size resource pool to \\nprevent failing calls from\", \" swamping a resource. \\nCache \\nStores responses automatically. \\nFallback \\nDefines structured behavior\", \" upon a failure. \\nNote how in the previous figure the resiliency policies apply to request messages,\", \" whether coming \\nfrom an external client or back-end service. The goal is to compensate the request \", \"for a service that \\nmight be momentarily unavailable. These short-lived interruptions typically mani\", \"fest themselves with \\nthe HTTP status codes shown in the following table. \\nHTTP Status Code \\nCause \\n\", \"404 \\nNot Found \\n408 \\nRequest timeout \\n429 \\nToo many requests (you\\u2019ve most likely been throttled) \\n50\", \"2 \\nBad gateway \\n503 \\nService unavailable \\n \\n115 \\nCHAPTER 6 | Cloud-native resiliency \\n \\nHTTP Status \", \"Code \\nCause \\n504 \\nGateway timeout \\nQuestion: Would you retry an HTTP Status Code of 403 - Forbidden?\", \" No. Here, the system is \\nfunctioning properly, but informing the caller that they aren\\u2019t authorized\", \" to perform the requested \\noperation. Care must be taken to retry only those operations caused by fa\", \"ilures. \\nAs recommended in Chapter 1, Microsoft developers constructing cloud-native applications sh\", \"ould \\ntarget the .NET platform. Version 2.1 introduced the HTTPClientFactory library for creating HT\", \"TP Client \\ninstances for interacting with URL-based resources. Superseding the original HTTPClient c\", \"lass, the \\nfactory class supports many enhanced features, one of which is tight integration with the\", \" Polly \\nresiliency library. With it, you can easily define resiliency policies in the application St\", \"artup class to \\nhandle partial failures and connectivity issues. \\nNext, let\\u2019s expand on retry and ci\", \"rcuit breaker patterns. \\nRetry pattern \\nIn a distributed cloud-native environment, calls to services\", \" and cloud resources can fail because of \\ntransient (short-lived) failures, which typically correct \", \"themselves after a brief period of time. \\nImplementing a retry strategy helps a cloud-native service\", \" mitigate these scenarios. \\nThe Retry pattern enables a service to retry a failed request operation \", \"a (configurable) number of \\ntimes with an exponentially increasing wait time. Figure 6-2 shows a ret\", \"ry in action. \\n \\nFigure 6-2. Retry pattern in action \\nIn the previous figure, a retry pattern has be\", \"en implemented for a request operation. It\\u2019s configured to \\nallow up to four retries before failing \", \"with a backoff interval (wait time) starting at two seconds, which \\nexponentially doubles for each s\", \"ubsequent attempt. \\n\\u2022 \\nThe first invocation fails and returns an HTTP status code of 500. The applic\", \"ation waits for two \\nseconds and retries the call. \\n \\n116 \\nCHAPTER 6 | Cloud-native resiliency \\n \\n\\u2022 \", \"\\nThe second invocation also fails and returns an HTTP status code of 500. The application now \\ndoubl\", \"es the backoff interval to four seconds and retries the call. \\n\\u2022 \\nFinally, the third call succeeds. \", \"\\n\\u2022 \\nIn this scenario, the retry operation would have attempted up to four retries while doubling \\nth\", \"e backoff duration before failing the call. \\n\\u2022 \\nHad the 4th retry attempt failed, a fallback policy \", \"would be invoked to gracefully handle the \\nproblem. \\nIt\\u2019s important to increase the backoff period b\", \"efore retrying the call to allow the service time to self-\\ncorrect. It\\u2019s a best practice to implemen\", \"t an exponentially increasing backoff (doubling the period on \\neach retry) to allow adequate correct\", \"ion time. \\nCircuit breaker pattern \\nWhile the retry pattern can help salvage a request entangled in \", \"a partial failure, there are situations \\nwhere failures can be caused by unanticipated events that w\", \"ill require longer periods of time to \\nresolve. These faults can range in severity from a partial lo\", \"ss of connectivity to the complete failure of \\na service. In these situations, it\\u2019s pointless for an\", \" application to continually retry an operation that is \\nunlikely to succeed. \\nTo make things worse, \", \"executing continual retry operations on a non-responsive service can move \\nyou into a self-imposed d\", \"enial of service scenario where you flood your service with continual calls \\nexhausting resources su\", \"ch as memory, threads and database connections, causing failure in unrelated \\nparts of the system th\", \"at use the same resources. \\nIn these situations, it would be preferable for the operation to fail im\", \"mediately and only attempt to \\ninvoke the service if it\\u2019s likely to succeed. \\nThe Circuit Breaker pa\", \"ttern can prevent an application from repeatedly trying to execute an operation \\nthat\\u2019s likely to fa\", \"il. After a pre-defined number of failed calls, it blocks all traffic to the service. \\nPeriodically,\", \" it will allow a trial call to determine whether the fault has resolved. Figure 6-3 shows the \\nCircu\", \"it Breaker pattern in action. \\n \\n117 \\nCHAPTER 6 | Cloud-native resiliency \\n \\n \\nFigure 6-3. Circuit b\", \"reaker pattern in action \\nIn the previous figure, a Circuit Breaker pattern has been added to the or\", \"iginal retry pattern. Note how \\nafter 100 failed requests, the circuit breakers opens and no longer \", \"allows calls to the service. The \\nCheckCircuit value, set at 30 seconds, specifies how often the lib\", \"rary allows one request to proceed to \\nthe service. If that call succeeds, the circuit closes and th\", \"e service is once again available to traffic. \\nKeep in mind that the intent of the Circuit Breaker p\", \"attern is different than that of the Retry pattern. \\nThe Retry pattern enables an application to ret\", \"ry an operation in the expectation that it will succeed. \\nThe Circuit Breaker pattern prevents an ap\", \"plication from doing an operation that is likely to fail. \\nTypically, an application will combine th\", \"ese two patterns by using the Retry pattern to invoke an \\noperation through a circuit breaker. \\nTest\", \"ing for resiliency \\nTesting for resiliency cannot always be done the same way that you test applicat\", \"ion functionality (by \\nrunning unit tests, integration tests, and so on). Instead, you must test how\", \" the end-to-end workload \\nperforms under failure conditions, which only occur intermittently. For ex\", \"ample: inject failures by \\ncrashing processes, expired certificates, make dependent services unavail\", \"able etc. Frameworks like \\nchaos-monkey can be used for such chaos testing. \\nApplication resiliency \", \"is a must for handling problematic requested operations. But, it\\u2019s only half of the \\nstory. Next, we\", \" cover resiliency features available in the Azure cloud. \\nAzure platform resiliency \\nBuilding a reli\", \"able application in the cloud is different from traditional on-premises application \\ndevelopment. Wh\", \"ile historically you purchased higher-end hardware to scale up, in a cloud \\nenvironment you scale ou\", \"t. Instead of trying to prevent failures, the goal is to minimize their effects \\nand keep the system\", \" stable. \\n \\n118 \\nCHAPTER 6 | Cloud-native resiliency \\n \\nThat said, reliable cloud applications displ\", \"ay distinct characteristics: \\n\\u2022 \\nThey\\u2019re resilient, recover gracefully from problems, and continue t\", \"o function. \\n\\u2022 \\nThey\\u2019re highly available (HA) and run as designed in a healthy state with no signifi\", \"cant \\ndowntime. \\nUnderstanding how these characteristics work together - and how they affect cost - \", \"is essential to \\nbuilding a reliable cloud-native application. We\\u2019ll next look at ways that you can \", \"build resiliency and \\navailability into your cloud-native applications leveraging features from the \", \"Azure cloud. \\nDesign with resiliency \\nWe\\u2019ve said resiliency enables your application to react to fai\", \"lure and still remain functional. The \\nwhitepaper, Resilience in Azure whitepaper, provides guidance\", \" for achieving resilience in the Azure \\nplatform. Here are some key recommendations: \\n\\u2022 \\nHardware fa\", \"ilure. Build redundancy into the application by deploying components across \\ndifferent fault domains\", \". For example, ensure that Azure VMs are placed in different racks by \\nusing Availability Sets. \\n\\u2022 \\n\", \"Datacenter failure. Build redundancy into the application with fault isolation zones across \\ndatacen\", \"ters. For example, ensure that Azure VMs are placed in different fault-isolated \\ndatacenters by usin\", \"g Azure Availability Zones. \\n\\u2022 \\nRegional failure. Replicate the data and components into another reg\", \"ion so that applications \\ncan be quickly recovered. For example, use Azure Site Recovery to replicat\", \"e Azure VMs to \\nanother Azure region. \\n\\u2022 \\nHeavy load. Load balance across instances to handle spikes\", \" in usage. For example, put two or \\nmore Azure VMs behind a load balancer to distribute traffic to a\", \"ll VMs. \\n\\u2022 \\nAccidental data deletion or corruption. Back up data so it can be restored if there\\u2019s an\", \"y \\ndeletion or corruption. For example, use Azure Backup to periodically back up your Azure \\nVMs. \\nD\", \"esign with redundancy \\nFailures vary in scope of impact. A hardware failure, such as a failed disk, \", \"can affect a single node in a \\ncluster. A failed network switch could affect an entire server rack. \", \"Less common failures, such as loss of \\npower, could disrupt a whole datacenter. Rarely, an entire re\", \"gion becomes unavailable. \\nRedundancy is one way to provide application resilience. The exact level \", \"of redundancy needed \\ndepends upon your business requirements and will affect both the cost and comp\", \"lexity of your \\nsystem. For example, a multi-region deployment is more expensive and more complex to\", \" manage \\nthan a single-region deployment. You\\u2019ll need operational procedures to manage failover and \", \"failback. \\nThe additional cost and complexity might be justified for some business scenarios, but no\", \"t others. \\nTo architect redundancy, you need to identify the critical paths in your application, and\", \" then \\ndetermine if there\\u2019s redundancy at each point in the path? If a subsystem should fail, will t\", \"he \\napplication fail over to something else? Finally, you need a clear understanding of those featur\", \"es built \\n \\n119 \\nCHAPTER 6 | Cloud-native resiliency \\n \\ninto the Azure cloud platform that you can l\", \"everage to meet your redundancy requirements. Here are \\nrecommendations for architecting redundancy:\", \" \\n\\u2022 \\nDeploy multiple instances of services. If your application depends on a single instance of a \\ns\", \"ervice, it creates a single point of failure. Provisioning multiple instances improves both \\nresilie\", \"ncy and scalability. When hosting in Azure Kubernetes Service, you can declaratively \\nconfigure redu\", \"ndant instances (replica sets) in the Kubernetes manifest file. The replica count \\nvalue can be mana\", \"ged programmatically, in the portal, or through autoscaling features. \\n\\u2022 \\nLeveraging a load balancer\", \". Load-balancing distributes your application\\u2019s requests to healthy \\nservice instances and automatic\", \"ally removes unhealthy instances from rotation. When \\ndeploying to Kubernetes, load balancing can be\", \" specified in the Kubernetes manifest file in \\nthe Services section. \\n\\u2022 \\nPlan for multiregion deploy\", \"ment. If you deploy your application to a single region, and that \\nregion becomes unavailable, your \", \"application will also become unavailable. This may be \\nunacceptable under the terms of your applicat\", \"ion\\u2019s service level agreements. Instead, consider \\ndeploying your application and its services acros\", \"s multiple regions. For example, an Azure \\nKubernetes Service (AKS) cluster is deployed to a single \", \"region. To protect your system from a \\nregional failure, you might deploy your application to multip\", \"le AKS clusters across different \\nregions and use the Paired Regions feature to coordinate platform \", \"updates and prioritize \\nrecovery efforts. \\n\\u2022 \\nEnable geo-replication. Geo-replication for services s\", \"uch as Azure SQL Database and Cosmos \\nDB will create secondary replicas of your data across multiple\", \" regions. While both services will \\nautomatically replicate data within the same region, geo-replica\", \"tion protects you against a \\nregional outage by enabling you to fail over to a secondary region. Ano\", \"ther best practice for \\ngeo-replication centers around storing container images. To deploy a service\", \" in AKS, you \\nneed to store and pull the image from a repository. Azure Container Registry integrate\", \"s with \\nAKS and can securely store container images. To improve performance and availability, \\nconsi\", \"der geo-replicating your images to a registry in each region where you have an AKS \\ncluster. Each AK\", \"S cluster then pulls container images from the local container registry in its \\nregion as shown in F\", \"igure 6-4: \\n \\nFigure 6-4. Replicated resources across regions \\n \\n120 \\nCHAPTER 6 | Cloud-native resil\", \"iency \\n \\n\\u2022 \\nImplement a DNS traffic load balancer. Azure Traffic Manager provides high-availability \", \"for \\ncritical applications by load-balancing at the DNS level. It can route traffic to different reg\", \"ions \\nbased on geography, cluster response time, and even application endpoint health. For \\nexample,\", \" Azure Traffic Manager can direct customers to the closest AKS cluster and \\napplication instance. If\", \" you have multiple AKS clusters in different regions, use Traffic \\nManager to control how traffic fl\", \"ows to the applications that run in each cluster. Figure 6-5 \\nshows this scenario. \\n \\nFigure 6-5. AK\", \"S and Azure Traffic Manager \\nDesign for scalability \\nThe cloud thrives on scaling. The ability to in\", \"crease/decrease system resources to address \\nincreasing/decreasing system load is a key tenet of the\", \" Azure cloud. But, to effectively scale an \\napplication, you need an understanding of the scaling fe\", \"atures of each Azure service that you include \\nin your application. Here are recommendations for eff\", \"ectively implementing scaling in your system. \\n\\u2022 \\nDesign for scaling. An application must be designe\", \"d for scaling. To start, services should be \\nstateless so that requests can be routed to any instanc\", \"e. Having stateless services also means \\nthat adding or removing an instance doesn\\u2019t adversely impac\", \"t current users. \\n\\u2022 \\nPartition workloads. Decomposing domains into independent, self-contained micro\", \"services \\nenable each service to scale independently of others. Typically, services will have differ\", \"ent \\nscalability needs and requirements. Partitioning enables you to scale only what needs to be \\nsc\", \"aled without the unnecessary cost of scaling an entire application. \\n\\u2022 \\nFavor scale-out. Cloud-based\", \" applications favor scaling out resources as opposed to scaling \\nup. Scaling out (also known as hori\", \"zontal scaling) involves adding more service resources to \\nan existing system to meet and share a de\", \"sired level of performance. Scaling up (also known \\n \\n121 \\nCHAPTER 6 | Cloud-native resiliency \\n \\nas\", \" vertical scaling) involves replacing existing resources with more powerful hardware (more \\ndisk, me\", \"mory, and processing cores). Scaling out can be invoked automatically with the \\nautoscaling features\", \" available in some Azure cloud resources. Scaling out across multiple \\nresources also adds redundanc\", \"y to the overall system. Finally scaling up a single resource is \\ntypically more expensive than scal\", \"ing out across many smaller resources. Figure 6-6 shows the \\ntwo approaches: \\n \\nFigure 6-6. Scale up\", \" versus scale out \\n\\u2022 \\nScale proportionally. When scaling a service, think in terms of resource sets.\", \" If you were to \\ndramatically scale out a specific service, what impact would that have on back-end \", \"data \\nstores, caches and dependent services? Some resources such as Cosmos DB can scale out \\nproport\", \"ionally, while many others can\\u2019t. You want to ensure that you don\\u2019t scale out a \\nresource to a point\", \" where it will exhaust other associated resources. \\n\\u2022 \\nAvoid affinity. A best practice is to ensure \", \"a node doesn\\u2019t require local affinity, often referred \\nto as a sticky session. A request should be a\", \"ble to route to any instance. If you need to persist \\nstate, it should be saved to a distributed cac\", \"he, such as Azure Redis cache. \\n\\u2022 \\nTake advantage of platform autoscaling features. Use built-in aut\", \"oscaling features whenever \\npossible, rather than custom or third-party mechanisms. Where possible, \", \"use scheduled \\nscaling rules to ensure that resources are available without a startup delay, but add\", \" reactive \\nautoscaling to the rules as appropriate, to cope with unexpected changes in demand. For \\n\", \"more information, see Autoscaling guidance. \\n\\u2022 \\nScale out aggressively. A final practice would be to\", \" scale out aggressively so that you can \\nquickly meet immediate spikes in traffic without losing bus\", \"iness. And, then scale in (that is, \\nremove unneeded instances) conservatively to keep the system st\", \"able. A simple way to \\nimplement this is to set the cool down period, which is the time to wait betw\", \"een scaling \\noperations, to five minutes for adding resources and up to 15 minutes for removing inst\", \"ances. \\nBuilt-in retry in services \\nWe encouraged the best practice of implementing programmatic ret\", \"ry operations in an earlier section. \\nKeep in mind that many Azure services and their corresponding \", \"client SDKs also include retry \\n \\n122 \\nCHAPTER 6 | Cloud-native resiliency \\n \\nmechanisms. The follow\", \"ing list summarizes retry features in the many of the Azure services that are \\ndiscussed in this boo\", \"k: \\n\\u2022 \\nAzure Cosmos DB. The DocumentClient class from the client API automatically retires failed \\na\", \"ttempts. The number of retries and maximum wait time are configurable. Exceptions thrown \\nby the cli\", \"ent API are either requests that exceed the retry policy or non-transient errors. \\n\\u2022 \\nAzure Redis Ca\", \"che. The Redis StackExchange client uses a connection manager class that \\nincludes retries on failed\", \" attempts. The number of retries, specific retry policy and wait time \\nare all configurable. \\n\\u2022 \\nAzu\", \"re Service Bus. The Service Bus client exposes a RetryPolicy class that can be configured \\nwith a ba\", \"ck-off interval, retry count, and TerminationTimeBuffer, which specifies the maximum \\ntime an operat\", \"ion can take. The default policy is nine maximum retry attempts with a 30-\\nsecond backoff period bet\", \"ween attempts. \\n\\u2022 \\nAzure SQL Database. Retry support is provided when using the Entity Framework Cor\", \"e library. \\n\\u2022 \\nAzure Storage. The storage client library support retry operations. The strategies va\", \"ry across \\nAzure storage tables, blobs, and queues. As well, alternate retries switch between primar\", \"y and \\nsecondary storage services locations when the geo-redundancy feature is enabled. \\n\\u2022 \\nAzure Ev\", \"ent Hubs. The Event Hub client library features a RetryPolicy property, which includes \\na configurab\", \"le exponential backoff feature. \\nResilient communications \\nThroughout this book, we\\u2019ve embraced a mi\", \"croservice-based architectural approach. While such an \\narchitecture provides important benefits, it\", \" presents many challenges: \\n\\u2022 \\nOut-of-process network communication. Each microservice communicates \", \"over a network \\nprotocol that introduces network congestion, latency, and transient faults. \\n\\u2022 \\nServ\", \"ice discovery. How do microservices discover and communicate with each other when \\nrunning across a \", \"cluster of machines with their own IP addresses and ports? \\n\\u2022 \\nResiliency. How do you manage short-l\", \"ived failures and keep the system stable? \\n\\u2022 \\nLoad balancing. How does inbound traffic get distribut\", \"ed across multiple instances of a \\nmicroservice? \\n\\u2022 \\nSecurity. How are security concerns such as tra\", \"nsport-level encryption and certificate \\nmanagement enforced? \\n\\u2022 \\nDistributed Monitoring. - How do y\", \"ou correlate and capture traceability and monitoring for a \\nsingle request across multiple consuming\", \" microservices? \\nYou can address these concerns with different libraries and frameworks, but the imp\", \"lementation can \\nbe expensive, complex, and time-consuming. You also end up with infrastructure conc\", \"erns coupled to \\nbusiness logic. \\n \\n123 \\nCHAPTER 6 | Cloud-native resiliency \\n \\nService mesh \\nA bett\", \"er approach is an evolving technology entitled Service Mesh. A service mesh is a configurable \\ninfra\", \"structure layer with built-in capabilities to handle service communication and the other \\nchallenges\", \" mentioned above. It decouples these concerns by moving them into a service proxy. The \\nproxy is dep\", \"loyed into a separate process (called a sidecar) to provide isolation from business code. \\nHowever, \", \"the sidecar is linked to the service - it\\u2019s created with it and shares its lifecycle. Figure 6-7 \\nsh\", \"ows this scenario. \\n \\nFigure 6-7. Service mesh with a side car \\nIn the previous figure, note how the\", \" proxy intercepts and manages communication among the \\nmicroservices and the cluster. \\nA service mes\", \"h is logically split into two disparate components: A data plane and control plane. Figure \\n6-8 show\", \"s these components and their responsibilities. \\n \\n124 \\nCHAPTER 6 | Cloud-native resiliency \\n \\n \\nFigu\", \"re 6-8. Service mesh control and data plane \\nOnce configured, a service mesh is highly functional. I\", \"t can retrieve a corresponding pool of instances \\nfrom a service discovery endpoint. The mesh can th\", \"en send a request to a specific instance, recording \\nthe latency and response type of the result. A \", \"mesh can choose the instance most likely to return a \\nfast response based on many factors, including\", \" its observed latency for recent requests. \\nIf an instance is unresponsive or fails, the mesh will r\", \"etry the request on another instance. If it returns \\nerrors, a mesh will evict the instance from the\", \" load-balancing pool and restate it after it heals. If a \\nrequest times out, a mesh can fail and the\", \"n retry the request. A mesh captures and emits metrics and \\ndistributed tracing to a centralized met\", \"rics system. \\nIstio and Envoy \\nWhile a few service mesh options currently exist, Istio is the most p\", \"opular at the time of this writing. \\nIstio is a joint venture from IBM, Google, and Lyft. It\\u2019s an op\", \"en-source offering that can be integrated \\ninto a new or existing distributed application. The techn\", \"ology provides a consistent and complete \\nsolution to secure, connect, and monitor microservices. It\", \"s features include: \\n\\u2022 \\nSecure service-to-service communication in a cluster with strong identity-ba\", \"sed \\nauthentication and authorization. \\n\\u2022 \\nAutomatic load balancing for HTTP, gRPC, WebSocket, and T\", \"CP traffic. \\n\\u2022 \\nFine-grained control of traffic behavior with rich routing rules, retries, failovers\", \", and fault \\ninjection. \\n\\u2022 \\nA pluggable policy layer and configuration API supporting access control\", \"s, rate limits, and \\nquotas. \\n\\u2022 \\nAutomatic metrics, logs, and traces for all traffic within a cluste\", \"r, including cluster ingress and \\negress. \\nA key component for an Istio implementation is a proxy se\", \"rvice entitled the Envoy proxy. It runs \\nalongside each service and provides a platform-agnostic fou\", \"ndation for the following features: \\n\\u2022 \\nDynamic service discovery. \\n\\u2022 \\nLoad balancing. \\n \\n125 \\nCHAPT\", \"ER 6 | Cloud-native resiliency \\n \\n\\u2022 \\nTLS termination. \\n\\u2022 \\nHTTP and gRPC proxies. \\n\\u2022 \\nCircuit breaker\", \" resiliency. \\n\\u2022 \\nHealth checks. \\n\\u2022 \\nRolling updates with canary deployments. \\nAs previously discusse\", \"d, Envoy is deployed as a sidecar to each microservice in the cluster. \\nIntegration with Azure Kuber\", \"netes Services \\nThe Azure cloud embraces Istio and provides direct support for it within Azure Kuber\", \"netes Services. \\nThe following links can help you get started: \\n\\u2022 \\nInstalling Istio in AKS \\n\\u2022 \\nUsing\", \" AKS and Istio \\nReferences \\n\\u2022 \\nPolly \\n\\u2022 \\nRetry pattern \\n\\u2022 \\nCircuit Breaker pattern \\n\\u2022 \\nResilience in\", \" Azure whitepaper \\n\\u2022 \\nnetwork latency \\n\\u2022 \\nRedundancy \\n\\u2022 \\ngeo-replication \\n\\u2022 \\nAzure Traffic Manager \\n\", \"\\u2022 \\nAutoscaling guidance \\n\\u2022 \\nIstio \\n\\u2022 \\nEnvoy proxy \\n \\n126 \\nCHAPTER 7 | Monitoring and health \\n \\nCHAPT\", \"ER 7 \\nMonitoring and health \\nMicroservices and cloud-native applications go hand in hand with good D\", \"evOps practices. DevOps is \\nmany things to many people but perhaps one of the better definitions com\", \"es from cloud advocate \\nand DevOps evangelist Donovan Brown: \\n\\u201cDevOps is the union of people, proces\", \"s, and products to enable continuous delivery of value to our \\nend users.\\u201d \\nUnfortunately, with ters\", \"e definitions, there\\u2019s always room to say more things. One of the key \\ncomponents of DevOps is ensur\", \"ing that the applications running in production are functioning \\nproperly and efficiently. To gauge \", \"the health of the application in production, it\\u2019s necessary to monitor \\nthe various logs and metrics\", \" being produced from the servers, hosts, and the application proper. The \\nnumber of different servic\", \"es running in support of a cloud-native application makes monitoring the \\nhealth of individual compo\", \"nents and the application as a whole a critical challenge. \\nObservability patterns \\nJust as patterns\", \" have been developed to aid in the layout of code in applications, there are patterns \\nfor operating\", \" applications in a reliable way. Three useful patterns in maintaining applications have \\nemerged: lo\", \"gging, monitoring, and alerts. \\nWhen to use logging \\nNo matter how careful we are, applications almo\", \"st always behave in unexpected ways in production. \\nWhen users report problems with an application, \", \"it\\u2019s useful to be able to see what was going on with \\nthe app when the problem occurred. One of the \", \"most tried and true ways of capturing information \\nabout what an application is doing while it\\u2019s run\", \"ning is to have the application write down what it\\u2019s \\ndoing. This process is known as logging. Anyti\", \"me failures or problems occur in production, the goal \\nshould be to reproduce the conditions under w\", \"hich the failures occurred, in a non-production \\nenvironment. Having good logging in place provides \", \"a roadmap for developers to follow in order to \\nduplicate problems in an environment that can be tes\", \"ted and experimented with. \\nChallenges when logging with cloud-native applications \\nIn traditional a\", \"pplications, log files are typically stored on the local machine. In fact, on Unix-like \\noperating s\", \"ystems, there\\u2019s a folder structure defined to hold any logs, typically under /var/log. \\n \\n127 \\nCHAPT\", \"ER 7 | Monitoring and health \\n \\n \\nFigure 7-1. Logging to a file in a monolithic app. \\nThe usefulness\", \" of logging to a flat file on a single machine is vastly reduced in a cloud environment. \\nApplicatio\", \"ns producing logs may not have access to the local disk or the local disk may be highly \\ntransient a\", \"s containers are shuffled around physical machines. Even simple scaling up of monolithic \\napplicatio\", \"ns across multiple nodes can make it challenging to locate the appropriate file-based log \\nfile. \\n \\n\", \"Figure 7-2. Logging to files in a scaled monolithic app. \\nCloud-native applications developed using \", \"a microservices architecture also pose some challenges for \\nfile-based loggers. User requests may no\", \"w span multiple services that are run on different machines \\n \\n128 \\nCHAPTER 7 | Monitoring and healt\", \"h \\n \\nand may include serverless functions with no access to a local file system at all. It would be \", \"very \\nchallenging to correlate the logs from a user or a session across these many services and mach\", \"ines. \\n \\nFigure 7-3. Logging to local files in a microservices app. \\nFinally, the number of users in\", \" some cloud-native applications is high. Imagine that each user \\ngenerates a hundred lines of log me\", \"ssages when they log into an application. In isolation, that is \\nmanageable, but multiply that over \", \"100,000 users and the volume of logs becomes large enough that \\nspecialized tools are needed to supp\", \"ort effective use of the logs. \\nLogging in cloud-native applications \\nEvery programming language has\", \" tooling that permits writing logs, and typically the overhead for \\nwriting these logs is low. Many \", \"of the logging libraries provide logging different kinds of criticalities, \\nwhich can be tuned at ru\", \"n time. For instance, the Serilog library is a popular structured logging library \\nfor .NET that pro\", \"vides the following logging levels: \\n\\u2022 \\nVerbose \\n\\u2022 \\nDebug \\n\\u2022 \\nInformation \\n\\u2022 \\nWarning \\n\\u2022 \\nError \\n\\u2022 \\n\", \"Fatal \\nThese different log levels provide granularity in logging. When the application is functionin\", \"g properly \\nin production, it may be configured to only log important messages. When the application\", \" is \\n \\n129 \\nCHAPTER 7 | Monitoring and health \\n \\nmisbehaving, then the log level can be increased so\", \" more verbose logs are gathered. This balances \\nperformance against ease of debugging. \\nThe high per\", \"formance of logging tools and the tunability of verbosity should encourage developers to \\nlog freque\", \"ntly. Many favor a pattern of logging the entry and exit of each method. This approach may \\nsound li\", \"ke overkill, but it\\u2019s infrequent that developers will wish for less logging. In fact, it\\u2019s not \\nunco\", \"mmon to perform deployments for the sole purpose of adding logging around a problematic \\nmethod. Err\", \" on the side of too much logging and not on too little. Some tools can be used to \\nautomatically pro\", \"vide this kind of logging. \\nBecause of the challenges associated with using file-based logs in cloud\", \"-native apps, centralized logs \\nare preferred. Logs are collected by the applications and shipped to\", \" a central logging application \\nwhich indexes and stores the logs. This class of system can ingest t\", \"ens of gigabytes of logs every day. \\nIt\\u2019s also helpful to follow some standard practices when buildi\", \"ng logging that spans many services. \\nFor instance, generating a correlation ID at the start of a le\", \"ngthy interaction, and then logging it in \\neach message that is related to that interaction, makes i\", \"t easier to search for all related messages. One \\nneed only find a single message and extract the co\", \"rrelation ID to find all the related messages. \\nAnother example is ensuring that the log format is t\", \"he same for every service, whatever the language \\nor logging library it uses. This standardization m\", \"akes reading logs much easier. Figure 7-4 \\ndemonstrates how a microservices architecture can leverag\", \"e centralized logging as part of its \\nworkflow. \\n \\nFigure 7-4. Logs from various sources are ingeste\", \"d into a centralized log store. \\n \\n130 \\nCHAPTER 7 | Monitoring and health \\n \\nChallenges with detecti\", \"ng and responding to potential app health \\nissues \\nSome applications aren\\u2019t mission critical. Maybe \", \"they\\u2019re only used internally, and when a problem \\noccurs, the user can contact the team responsible \", \"and the application can be restarted. However, \\ncustomers often have higher expectations for the app\", \"lications they consume. You should know when \\nproblems occur with your application before users do, \", \"or before users notify you. Otherwise, the first \\nyou know about a problem may be when you notice an\", \" angry deluge of social media posts deriding \\nyour application or even your organization. \\nSome scen\", \"arios you may need to consider include: \\n\\u2022 \\nOne service in your application keeps failing and restar\", \"ting, resulting in intermittent slow \\nresponses. \\n\\u2022 \\nAt some times of the day, your application\\u2019s re\", \"sponse time is slow. \\n\\u2022 \\nAfter a recent deployment, load on the database has tripled. \\nImplemented p\", \"roperly, monitoring can let you know about conditions that will lead to problems, \\nletting you addre\", \"ss underlying conditions before they result in any significant user impact. \\nMonitoring cloud-native\", \" apps \\nSome centralized logging systems take on an additional role of collecting telemetry outside o\", \"f pure \\nlogs. They can collect metrics, such as time to run a database query, average response time \", \"from a \\nweb server, and even CPU load averages and memory pressure as reported by the operating syst\", \"em. \\nIn conjunction with the logs, these systems can provide a holistic view of the health of nodes \", \"in the \\nsystem and the application as a whole. \\nThe metric-gathering capabilities of the monitoring \", \"tools can also be fed manually from within the \\napplication. Business flows that are of particular i\", \"nterest such as new users signing up or orders being \\nplaced, may be instrumented such that they inc\", \"rement a counter in the central monitoring system. \\nThis aspect unlocks the monitoring tools to not \", \"only monitor the health of the application but the \\nhealth of the business. \\nQueries can be construc\", \"ted in the log aggregation tools to look for certain statistics or patterns, which \\ncan then be disp\", \"layed in graphical form, on custom dashboards. Frequently, teams will invest in large, \\nwall-mounted\", \" displays that rotate through the statistics related to an application. This way, it\\u2019s simple \\nto se\", \"e the problems as they occur. \\nCloud-native monitoring tools provide real-time telemetry and insight\", \" into apps regardless of whether \\nthey\\u2019re single-process monolithic applications or distributed micr\", \"oservice architectures. They include \\ntools that allow collection of data from the app as well as to\", \"ols for querying and displaying \\ninformation about the app\\u2019s health. \\nChallenges with reacting to cr\", \"itical problems in cloud-native apps \\nIf you need to react to problems with your application, you ne\", \"ed some way to alert the right \\npersonnel. This is the third cloud-native application observability \", \"pattern and depends on logging and \\nmonitoring. Your application needs to have logging in place to a\", \"llow problems to be diagnosed, and \\n \\n131 \\nCHAPTER 7 | Monitoring and health \\n \\nin some cases to fee\", \"d into monitoring tools. It needs monitoring to aggregate application metrics and \\nhealth data in on\", \"e place. Once this has been established, rules can be created that will trigger alerts \\nwhen certain\", \" metrics fall outside of acceptable levels. \\nGenerally, alerts are layered on top of monitoring such\", \" that certain conditions trigger appropriate \\nalerts to notify team members of urgent problems. Some\", \" scenarios that may require alerts include: \\n\\u2022 \\nOne of your application\\u2019s services is not responding\", \" after 1 minute of downtime. \\n\\u2022 \\nYour application is returning unsuccessful HTTP responses to more t\", \"han 1% of requests. \\n\\u2022 \\nYour application\\u2019s average response time for key endpoints exceeds 2000 ms. \", \"\\nAlerts in cloud-native apps \\nYou can craft queries against the monitoring tools to look for known f\", \"ailure conditions. For instance, \\nqueries could search through the incoming logs for indications of \", \"HTTP status code 500, which \\nindicates a problem on a web server. As soon as one of these is detecte\", \"d, then an e-mail or an SMS \\ncould be sent to the owner of the originating service who can begin to \", \"investigate. \\nTypically, though, a single 500 error isn\\u2019t enough to determine that a problem has occ\", \"urred. It could \\nmean that a user mistyped their password or entered some malformed data. The alert \", \"queries can be \\ncrafted to only fire when a larger than average number of 500 errors are detected. \\n\", \"One of the most damaging patterns in alerting is to fire too many alerts for humans to investigate. \", \"\\nService owners will rapidly become desensitized to errors that they\\u2019ve previously investigated and \", \"\\nfound to be benign. Then, when true errors occur, they\\u2019ll be lost in the noise of hundreds of false\", \" \\npositives. The parable of the Boy Who Cried Wolf is frequently told to children to warn them of th\", \"is \\nvery danger. It\\u2019s important to ensure that the alerts that do fire are indicative of a real prob\", \"lem. \\nLogging with Elastic Stack \\nThere are many good centralized logging tools and they vary in cos\", \"t from being free, open-source \\ntools, to more expensive options. In many cases, the free tools are \", \"as good as or better than the paid \\nofferings. One such tool is a combination of three open-source c\", \"omponents: Elasticsearch, Logstash, \\nand Kibana. \\nCollectively these tools are known as the Elastic \", \"Stack or ELK stack. \\nElastic Stack \\nThe Elastic Stack is a powerful option for gathering information\", \" from a Kubernetes cluster. Kubernetes \\nsupports sending logs to an Elasticsearch endpoint, and for \", \"the most part, all you need to get started \\nis to set the environment variables as shown in Figure 7\", \"-5: \\nKUBE_LOGGING_DESTINATION=elasticsearch \\nKUBE_ENABLE_NODE_LOGGING=true \\nFigure 7-5. Configuratio\", \"n variables for Kubernetes \\nThis step will install Elasticsearch on the cluster and target sending a\", \"ll the cluster logs to it. \\n \\n132 \\nCHAPTER 7 | Monitoring and health \\n \\n \\nFigure 7-6. An example of \", \"a Kibana dashboard showing the results of a query against logs that are ingested from \\nKubernetes \\nW\", \"hat are the advantages of Elastic Stack? \\nElastic Stack provides centralized logging in a low-cost, \", \"scalable, cloud-friendly manner. Its user \\ninterface streamlines data analysis so you can spend your\", \" time gleaning insights from your data \\ninstead of fighting with a clunky interface. It supports a w\", \"ide variety of inputs so as your distributed \\napplication spans more and different kinds of services\", \", you can expect to continue to be able to feed \\nlog and metric data into the system. The Elastic St\", \"ack also supports fast searches even across large \\ndata sets, making it possible even for large appl\", \"ications to log detailed data and still be able to have \\nvisibility into it in a performant fashion.\", \" \\nLogstash \\nThe first component is Logstash. This tool is used to gather log information from a larg\", \"e variety of \\ndifferent sources. For instance, Logstash can read logs from disk and also receive mes\", \"sages from \\nlogging libraries like Serilog. Logstash can do some basic filtering and expansion on th\", \"e logs as they \\narrive. For instance, if your logs contain IP addresses then Logstash may be configu\", \"red to do a \\ngeographical lookup and obtain a country/region or even city of origin for that message\", \". \\nSerilog is a logging library for .NET languages, which allows for parameterized logging. Instead \", \"of \\ngenerating a textual log message that embeds fields, parameters are kept separate. This library \", \"allows \\nfor more intelligent filtering and searching. A sample Serilog configuration for writing to \", \"Logstash \\nappears in Figure 7-7. \\nvar log = new LoggerConfiguration() \\n         .WriteTo.Http(\\\"http:\", \"//localhost:8080\\\") \\n         .CreateLogger(); \\nFigure 7-7. Serilog config for writing log informatio\", \"n directly to logstash over HTTP \\nLogstash would use a configuration like the one shown in Figure 7-\", \"8. \\n \\n133 \\nCHAPTER 7 | Monitoring and health \\n \\ninput { \\n    http { \\n        #default host 0.0.0.0:8\", \"080 \\n        codec => json \\n    } \\n} \\n \\noutput { \\n    elasticsearch { \\n        hosts => \\\"elasticsear\", \"ch:9200\\\" \\n        index=>\\\"sales-%{+xxxx.ww}\\\" \\n    } \\n} \\nFigure 7-8. A Logstash configuration for con\", \"suming logs from Serilog \\nFor scenarios where extensive log manipulation isn\\u2019t needed there\\u2019s an alt\", \"ernative to Logstash known \\nas Beats. Beats is a family of tools that can gather a wide variety of d\", \"ata from logs to network data \\nand uptime information. Many applications will use both Logstash and \", \"Beats. \\nOnce the logs have been gathered by Logstash, it needs somewhere to put them. While Logstash\", \" \\nsupports many different outputs, one of the more exciting ones is Elasticsearch. \\nElasticsearch \\nE\", \"lasticsearch is a powerful search engine that can index logs as they arrive. It makes running querie\", \"s \\nagainst the logs quick. Elasticsearch can handle huge quantities of logs and, in extreme cases, c\", \"an be \\nscaled out across many nodes. \\nLog messages that have been crafted to contain parameters or t\", \"hat have had parameters split from \\nthem through Logstash processing, can be queried directly as Ela\", \"sticsearch preserves this information. \\nA query that searches for the top 10 pages visited by jill@e\", \"xample.com, appears in Figure 7-9. \\n\\\"query\\\": { \\n    \\\"match\\\": { \\n      \\\"user\\\": \\\"jill@example.com\\\" \\n  \", \"  } \\n  }, \\n  \\\"aggregations\\\": { \\n    \\\"top_10_pages\\\": { \\n      \\\"terms\\\": { \\n        \\\"field\\\": \\\"page\\\", \\n \", \"       \\\"size\\\": 10 \\n      } \\n    } \\n  } \\nFigure 7-9. An Elasticsearch query for finding top 10 pages \", \"visited by a user \\nVisualizing information with Kibana web dashboards \\nThe final component of the st\", \"ack is Kibana. This tool is used to provide interactive visualizations in a \\nweb dashboard. Dashboar\", \"ds may be crafted even by users who are non-technical. Most data that is \\nresident in the Elasticsea\", \"rch index, can be included in the Kibana dashboards. Individual users may \\n \\n134 \\nCHAPTER 7 | Monito\", \"ring and health \\n \\nhave different dashboard desires and Kibana enables this customization through al\", \"lowing user-\\nspecific dashboards. \\nInstalling Elastic Stack on Azure \\nThe Elastic stack can be insta\", \"lled on Azure in many ways. As always, it\\u2019s possible to provision virtual \\nmachines and install Elas\", \"tic Stack on them directly. This option is preferred by some experienced users \\nas it offers the hig\", \"hest degree of customizability. Deploying on infrastructure as a service introduces \\nsignificant man\", \"agement overhead forcing those who take that path to take ownership of all the tasks \\nassociated wit\", \"h infrastructure as a service such as securing the machines and keeping up-to-date with \\npatches. \\nA\", \"n option with less overhead is to make use of one of the many Docker containers on which the \\nElasti\", \"c Stack has already been configured. These containers can be dropped into an existing \\nKubernetes cl\", \"uster and run alongside application code. The sebp/elk container is a well-documented \\nand tested El\", \"astic Stack container. \\nAnother option is a recently announced ELK-as-a-service offering. \\nReference\", \"s \\n\\u2022 \\nInstall Elastic Stack on Azure \\nMonitoring in Azure Kubernetes Services \\nThe built-in logging \", \"in Kubernetes is primitive. However, there are some great options for getting the \\nlogs out of Kuber\", \"netes and into a place where they can be properly analyzed. If you need to monitor \\nyour AKS cluster\", \"s, configuring Elastic Stack for Kubernetes is a great solution. \\nAzure Monitor for Containers \\nAzur\", \"e Monitor for Containers supports consuming logs from not just Kubernetes but also from other \\norche\", \"stration engines such as DC/OS, Docker Swarm, and Red Hat OpenShift. \\n \\n135 \\nCHAPTER 7 | Monitoring \", \"and health \\n \\n \\nFigure 7-10. Consuming logs from various containers \\nPrometheus is a popular open so\", \"urce metric monitoring solution. It is part of the Cloud Native \\nCompute Foundation. Typically, usin\", \"g Prometheus requires managing a Prometheus server with its \\nown store. However, Azure Monitor for C\", \"ontainers provides direct integration with Prometheus \\nmetrics endpoints, so a separate server is no\", \"t required. \\nLog and metric information is gathered not just from the containers running in the clus\", \"ter but also \\nfrom the cluster hosts themselves. It allows correlating log information from the two \", \"making it much \\neasier to track down an error. \\nInstalling the log collectors differs on Windows and\", \" Linux clusters. But in both cases the log collection \\nis implemented as a Kubernetes DaemonSet, mea\", \"ning that the log collector is run as a container on \\neach of the nodes. \\nNo matter which orchestrat\", \"or or operating system is running the Azure Monitor daemon, the log \\ninformation is forwarded to the\", \" same Azure Monitor tools with which users are familiar. This approach \\nensures a parallel experienc\", \"e in environments that mix different log sources such as a hybrid \\nKubernetes/Azure Functions enviro\", \"nment. \\n \\n136 \\nCHAPTER 7 | Monitoring and health \\n \\n \\nFigure 7-11. A sample dashboard showing loggin\", \"g and metric information from many running containers. \\nLog.Finalize() \\nLogging is one of the most o\", \"verlooked and yet most important parts of deploying any application at \\nscale. As the size and compl\", \"exity of applications increase, then so does the difficulty of debugging \\nthem. Having top quality l\", \"ogs available makes debugging much easier and moves it from the realm of \\n\\u201cnearly impossible\\u201d to \\u201ca \", \"pleasant experience\\u201d. \\nAzure Monitor \\nNo other cloud provider has as mature of a cloud application m\", \"onitoring solution than that found in \\nAzure. Azure Monitor is an umbrella name for a collection of \", \"tools designed to provide visibility into \\nthe state of your system. It helps you understand how you\", \"r cloud-native services are performing and \\nproactively identifies issues affecting them. Figure 7-1\", \"2 presents a high level of view of Azure Monitor. \\n \\n137 \\nCHAPTER 7 | Monitoring and health \\n \\n \\nFig\", \"ure 7-12. High-level view of Azure Monitor. \\nGathering logs and metrics \\nThe first step in any monit\", \"oring solution is to gather as much data as possible. The more data \\ngathered, the deeper the insigh\", \"ts. Instrumenting systems has traditionally been difficult. Simple \\nNetwork Management Protocol (SNM\", \"P) was the gold standard protocol for collecting machine level \\ninformation, but it required a great\", \" deal of knowledge and configuration. Fortunately, much of this \\nhard work has been eliminated as th\", \"e most common metrics are gathered automatically by Azure \\nMonitor. \\nApplication level metrics and e\", \"vents aren\\u2019t possible to instrument automatically because they\\u2019re \\nspecific to the application being\", \" deployed. In order to gather these metrics, there are SDKs and APIs \\navailable to directly report s\", \"uch information, such as when a customer signs up or completes an order. \\nExceptions can also be cap\", \"tured and reported back into Azure Monitor via Application Insights. The \\nSDKs support most every la\", \"nguage found in Cloud Native Applications including Go, Python, \\nJavaScript, and the .NET languages.\", \" \\nThe ultimate goal of gathering information about the state of your application is to ensure that y\", \"our \\nend users have a good experience. What better way to tell if users are experiencing issues than\", \" doing \\noutside-in web tests? These tests can be as simple as pinging your website from locations ar\", \"ound the \\nworld or as involved as having agents log into the site and simulate user actions. \\nReport\", \"ing data \\nOnce the data is gathered, it can be manipulated, summarized, and plotted into charts, whi\", \"ch allow \\nusers to instantly see when there are problems. These charts can be gathered into dashboar\", \"ds or into \\nWorkbooks, a multi-page report designed to tell a story about some aspect of the system.\", \" \\n \\n138 \\nCHAPTER 7 | Monitoring and health \\n \\nNo modern application would be complete without some a\", \"rtificial intelligence or machine learning. To \\nthis end, data can be passed to the various machine \", \"learning tools in Azure to allow you to extract \\ntrends and information that would otherwise be hidd\", \"en. \\nApplication Insights provides a powerful (SQL-like) query language called Kusto that can query \", \"\\nrecords, summarize them, and even plot charts. For example, the following query will locate all rec\", \"ords \\nfor the month of November 2007, group them by state, and plot the top 10 as a pie chart. \\nStor\", \"mEvents \\n| where StartTime >= datetime(2007-11-01) and StartTime < datetime(2007-12-01) \\n| summarize\", \" count() by State \\n| top 10 by count_ \\n| render piechart \\nFigure 7-13 shows the results of this Appl\", \"ication Insights Query. \\n \\nFigure 7-13. Application Insights query results. \\nThere is a playground f\", \"or experimenting with Kusto queries. Reading sample queries can also be \\ninstructive. \\nDashboards \\nT\", \"here are several different dashboard technologies that may be used to surface the information from \\n\", \"Azure Monitor. Perhaps the simplest is to just run queries in Application Insights and plot the data\", \" \\ninto a chart. \\n \\n139 \\nCHAPTER 7 | Monitoring and health \\n \\n \\nFigure 7-14. An example of Applicatio\", \"n Insights charts embedded in the main Azure Dashboard. \\nThese charts can then be embedded in the Az\", \"ure portal proper through use of the dashboard feature. \\nFor users with more exacting requirements, \", \"such as being able to drill down into several tiers of data, \\nAzure Monitor data is available to Pow\", \"er BI. Power BI is an industry-leading, enterprise class, business \\nintelligence tool that can aggre\", \"gate data from many different data sources. \\n \\n140 \\nCHAPTER 7 | Monitoring and health \\n \\n \\nFigure 7-\", \"15. An example Power BI dashboard. \\nAlerts \\nSometimes, having data dashboards is insufficient. If no\", \"body is awake to watch the dashboards, then \\nit can still be many hours before a problem is addresse\", \"d, or even detected. To this end, Azure Monitor \\nalso provides a top notch alerting solution. Alerts\", \" can be triggered by a wide range of conditions \\nincluding: \\n\\u2022 \\nMetric values \\n\\u2022 \\nLog search queries\", \" \\n\\u2022 \\nActivity Log events \\n\\u2022 \\nHealth of the underlying Azure platform \\n\\u2022 \\nTests for web site availabi\", \"lity \\nWhen triggered, the alerts can perform a wide variety of tasks. On the simple side, the alerts\", \" may just \\nsend an e-mail notification to a mailing list or a text message to an individual. More in\", \"volved alerts \\n \\n141 \\nCHAPTER 7 | Monitoring and health \\n \\nmight trigger a workflow in a tool such a\", \"s PagerDuty, which is aware of who is on call for a particular \\napplication. Alerts can trigger acti\", \"ons in Microsoft Flow unlocking near limitless possibilities for \\nworkflows. \\nAs common causes of al\", \"erts are identified, the alerts can be enhanced with details about the common \\ncauses of the alerts \", \"and the steps to take to resolve them. Highly mature cloud-native application \\ndeployments may opt t\", \"o kick off self-healing tasks, which perform actions such as removing failing \\nnodes from a scale se\", \"t or triggering an autoscaling activity. Eventually it may no longer be necessary \\nto wake up on-cal\", \"l personnel at 2AM to resolve a live-site issue as the system will be able to adjust \\nitself to comp\", \"ensate or at least limp along until somebody arrives at work the next morning. \\nAzure Monitor automa\", \"tically leverages machine learning to understand the normal operating \\nparameters of deployed applic\", \"ations. This approach enables it to detect services that are operating \\noutside of their normal para\", \"meters. For instance, the typical weekday traffic on the site might be \\n10,000 requests per minute. \", \"And then, on a given week, suddenly the number of requests hits a highly \\nunusual 20,000 requests pe\", \"r minute. Smart Detection will notice this deviation from the norm and \\ntrigger an alert. At the sam\", \"e time, the trend analysis is smart enough to avoid firing false positives \\nwhen the traffic load is\", \" expected. \\nReferences \\n\\u2022 \\nAzure Monitor \\n \\n142 \\nCHAPTER 8 | Cloud-native identity \\n \\nCHAPTER 8 \\nClo\", \"ud-native identity \\nMost software applications need to have some knowledge of the user or process th\", \"at is calling them. \\nThe user or process interacting with an application is known as a security prin\", \"cipal, and the process of \\nauthenticating and authorizing these principals is known as identity mana\", \"gement, or simply identity. \\nSimple applications may include all of their identity management within\", \" the application, but this \\napproach doesn\\u2019t scale well with many applications and many kinds of sec\", \"urity principals. Windows \\nsupports the use of Active Directory to provide centralized authenticatio\", \"n and authorization. \\nWhile this solution is effective within corporate networks, it isn\\u2019t designed \", \"for use by users or \\napplications that are outside of the AD domain. With the growth of Internet-bas\", \"ed applications and \\nthe rise of cloud-native apps, security models have evolved. \\nIn today\\u2019s cloud-\", \"native identity model, architecture is assumed to be distributed. Apps can be \\ndeployed anywhere and\", \" may communicate with other apps anywhere. Clients may communicate with \\nthese apps from anywhere, a\", \"nd in fact, clients may consist of any combination of platforms and \\ndevices. Cloud-native identity \", \"solutions use open standards to achieve secure application access from \\nclients. These clients range\", \" from human users on PCs or phones, to other apps hosted anywhere \\nonline, to set-top boxes and IOT \", \"devices running any software platform anywhere in the world. \\nModern cloud-native identity solutions\", \" typically use access tokens that are issued by a secure token \\nservice/server (STS) to a security p\", \"rincipal once their identity is determined. The access token, typically \\na JSON Web Token (JWT), inc\", \"ludes claims about the security principal. These claims will minimally \\ninclude the user\\u2019s identity \", \"but may also include other claims that can be used by applications to \\ndetermine the level of access\", \" to grant the principal. \\nTypically, the STS is only responsible for authenticating the principal. D\", \"etermining their level of access \\nto resources is left to other parts of the application. \\nReference\", \"s \\n\\u2022 \\nMicrosoft identity platform \\nAuthentication and authorization in cloud-native \\napps \\nAuthentic\", \"ation is the process of determining the identity of a security principal. Authorization is the act \\n\", \"of granting an authenticated principal permission to perform an action or access a resource. \\nSometi\", \"mes authentication is shortened to AuthN and authorization is shortened to AuthZ. Cloud-\\n \\n143 \\nCHAP\", \"TER 8 | Cloud-native identity \\n \\nnative applications need to rely on open HTTP-based protocols to au\", \"thenticate security principals \\nsince both clients and applications could be running anywhere in the\", \" world on any platform or device. \\nThe only common factor is HTTP. \\nMany organizations still rely on\", \" local authentication services like Active Directory Federation Services \\n(ADFS). While this approac\", \"h has traditionally served organizations well for on premises authentication \\nneeds, cloud-native ap\", \"plications benefit from systems designed specifically for the cloud. A recent \\n2019 United Kingdom N\", \"ational Cyber Security Centre (NCSC) advisory states that \\u201corganizations using \\nAzure AD as their pr\", \"imary authentication source will actually lower their risk compared to ADFS.\\u201d \\nSome reasons outlined\", \" in this analysis include: \\n\\u2022 \\nAccess to full set of Microsoft credential protection technologies. \\n\", \"\\u2022 \\nMost organizations are already relying on Azure AD to some extent. \\n\\u2022 \\nDouble hashing of NTLM has\", \"hes ensures compromise won\\u2019t allow credentials that work in \\nlocal Active Directory. \\nReferences \\n\\u2022 \", \"\\nAuthentication basics \\n\\u2022 \\nAccess tokens and claims \\n\\u2022 \\nIt may be time to ditch your on premises aut\", \"hentication services \\nAzure Active Directory \\nMicrosoft Azure Active Directory (Azure AD) offers ide\", \"ntity and access management as a service. \\nCustomers use it to configure and maintain who users are,\", \" what information to store about them, who \\ncan access that information, who can manage it, and what\", \" apps can access it. AAD can authenticate \\nusers for applications configured to use it, providing a \", \"single sign-on (SSO) experience. It can be used \\non its own or be integrated with Windows AD running\", \" on premises. \\nAzure AD is built for the cloud. It\\u2019s truly a cloud-native identity solution that use\", \"s a REST-based Graph \\nAPI and OData syntax for queries, unlike Windows AD, which uses LDAP. On premi\", \"ses Active Directory \\ncan sync user attributes to the cloud using Identity Sync Services, allowing a\", \"ll authentication to take \\nplace in the cloud using Azure AD. Alternately, authentication can be con\", \"figured via Connect to pass \\nback to local Active Directory via ADFS to be completed by Windows AD o\", \"n premises. \\nAzure AD supports company branded sign-in screens, multi-factory authentication, and cl\", \"oud-based \\napplication proxies that are used to provide SSO for applications hosted on premises. It \", \"offers \\ndifferent kinds of security reporting and alert capabilities. \\nReferences \\n\\u2022 \\nMicrosoft iden\", \"tity platform \\n \\n144 \\nCHAPTER 8 | Cloud-native identity \\n \\nIdentityServer for cloud-native applicati\", \"ons \\nIdentityServer is an authentication server that implements OpenID Connect (OIDC) and OAuth 2.0 \", \"\\nstandards for ASP.NET Core. It\\u2019s designed to provide a common way to authenticate requests to all o\", \"f \\nyour applications, whether they\\u2019re web, native, mobile, or API endpoints. IdentityServer can be u\", \"sed to \\nimplement Single Sign-On (SSO) for multiple applications and application types. It can be us\", \"ed to \\nauthenticate actual users via sign-in forms and similar user interfaces as well as service-ba\", \"sed \\nauthentication that typically involves token issuance, verification, and renewal without any us\", \"er \\ninterface. IdentityServer is designed to be a customizable solution. Each instance is typically \", \"\\ncustomized to suit an individual organization and/or set of applications\\u2019 needs. \\nCommon web app sc\", \"enarios \\nTypically, applications need to support some or all of the following scenarios: \\n\\u2022 \\nHuman u\", \"sers accessing web applications with a browser. \\n\\u2022 \\nHuman users accessing back-end Web APIs from bro\", \"wser-based apps. \\n\\u2022 \\nHuman users on mobile/native clients accessing back-end Web APIs. \\n\\u2022 \\nOther app\", \"lications accessing back-end Web APIs (without an active user or user interface). \\n\\u2022 \\nAny applicatio\", \"n may need to interact with other Web APIs, using its own identity or \\ndelegating to the user\\u2019s iden\", \"tity. \\n \\nFigure 8-1. Application types and scenarios. \\nIn each of these scenarios, the exposed funct\", \"ionality needs to be secured against unauthorized use. At \\na minimum, this typically requires authen\", \"ticating the user or principal making a request for a resource. \\nThis authentication may use one of \", \"several common protocols such as SAML2p, WS-Fed, or OpenID \\nConnect. Communicating with APIs typical\", \"ly uses the OAuth2 protocol and its support for security \\ntokens. Separating these critical cross-cu\", \"tting security concerns and their implementation details from \\nthe applications themselves ensures c\", \"onsistency and improves security and maintainability. \\n \\n145 \\nCHAPTER 8 | Cloud-native identity \\n \\nO\", \"utsourcing these concerns to a dedicated product like IdentityServer helps the requirement for every\", \" \\napplication to solve these problems itself. \\nIdentityServer provides middleware that runs within a\", \"n ASP.NET Core application and adds support \\nfor OpenID Connect and OAuth2 (see supported specificat\", \"ions). Organizations would create their own \\nASP.NET Core app using IdentityServer middleware to act\", \" as the STS for all of their token-based \\nsecurity protocols. The IdentityServer middleware exposes \", \"endpoints to support standard \\nfunctionality, including: \\n\\u2022 \\nAuthorize (authenticate the end user) \\n\", \"\\u2022 \\nToken (request a token programmatically) \\n\\u2022 \\nDiscovery (metadata about the server) \\n\\u2022 \\nUser Info \", \"(get user information with a valid access token) \\n\\u2022 \\nDevice Authorization (used to start device flow\", \" authorization) \\n\\u2022 \\nIntrospection (token validation) \\n\\u2022 \\nRevocation (token revocation) \\n\\u2022 \\nEnd Sessi\", \"on (trigger single sign-out across all apps) \\nGetting started \\nIdentityServer4 is available under du\", \"al license: \\n\\u2022 \\nRPL - lets you use the IdentityServer4 free if used in open-source work \\n\\u2022 \\nPaid - l\", \"ets you use the IdentityServer4 in a commercial scenario \\nFor more information about pricing, see th\", \"e official product\\u2019s pricing page. \\nYou can add it to your applications using its NuGet packages. Th\", \"e main package is IdentityServer4, \\nwhich has been downloaded over four million times. The base pack\", \"age doesn\\u2019t include any user \\ninterface code and only supports in-memory configuration. To use it wi\", \"th a database, you\\u2019ll also want \\na data provider like IdentityServer4.EntityFramework, which uses En\", \"tity Framework Core to store \\nconfiguration and operational data for IdentityServer. For user interf\", \"ace, you can copy files from the \\nQuickstart UI repository into your ASP.NET Core MVC application to\", \" add support for sign in and sign \\nout using IdentityServer middleware. \\nConfiguration \\nIdentityServ\", \"er supports different kinds of protocols and social authentication providers that can be \\nconfigured\", \" as part of each custom installation. This is typically done in the ASP.NET Core application\\u2019s \\nProg\", \"ram class (or in the Startup class in the ConfigureServices method). The configuration involves \\nspe\", \"cifying the supported protocols and the paths to the servers and endpoints that will be used. \\nFigur\", \"e 8-2 shows an example configuration taken from the IdentityServer4 Quickstart UI project: \\npublic c\", \"lass Startup \\n{ \\n    public void ConfigureServices(IServiceCollection services) \\n    { \\n        serv\", \"ices.AddMvc(); \\n \\n \\n146 \\nCHAPTER 8 | Cloud-native identity \\n \\n        // some details omitted \\n     \", \"   services.AddIdentityServer(); \\n \\n          services.AddAuthentication() \\n            .AddGoogle(\\\"\", \"Google\\\", options => \\n            { \\n                options.SignInScheme = \\nIdentityServerConstants.\", \"ExternalCookieAuthenticationScheme; \\n \\n                options.ClientId = \\\"<insert here>\\\"; \\n        \", \"        options.ClientSecret = \\\"<insert here>\\\"; \\n            }) \\n            .AddOpenIdConnect(\\\"demo\", \"idsrv\\\", \\\"IdentityServer\\\", options => \\n            { \\n                options.SignInScheme = \\nIdentit\", \"yServerConstants.ExternalCookieAuthenticationScheme; \\n                options.SignOutScheme = Identi\", \"tyServerConstants.SignoutScheme; \\n \\n                options.Authority = \\\"https://demo.identityserver\", \".io/\\\"; \\n                options.ClientId = \\\"implicit\\\"; \\n                options.ResponseType = \\\"id_t\", \"oken\\\"; \\n                options.SaveTokens = true; \\n                options.CallbackPath = new PathS\", \"tring(\\\"/signin-idsrv\\\"); \\n                options.SignedOutCallbackPath = new PathString(\\\"/signout-ca\", \"llback-idsrv\\\"); \\n                options.RemoteSignOutPath = new PathString(\\\"/signout-idsrv\\\"); \\n \\n  \", \"              options.TokenValidationParameters = new TokenValidationParameters \\n                { \\n\", \"                    NameClaimType = \\\"name\\\", \\n                    RoleClaimType = \\\"role\\\" \\n           \", \"     }; \\n            }); \\n    } \\n} \\nFigure 8-2. Configuring IdentityServer. \\nJavaScript clients \\nMan\", \"y cloud-native applications use server-side APIs and rich client single page applications (SPAs) on \", \"\\nthe front end. IdentityServer ships a JavaScript client (oidc-client.js) via NPM that can be added \", \"to \\nSPAs to enable them to use IdentityServer for sign in, sign out, and token-based authentication \", \"of \\nweb APIs. \\nReferences \\n\\u2022 \\nIdentityServer documentation \\n\\u2022 \\nApplication types \\n\\u2022 \\nJavaScript OIDC\", \" client \\n \\n147 \\nCHAPTER 9 | Cloud-native security \\n \\nCHAPTER 9 \\nCloud-native security \\nNot a day goe\", \"s by where the news doesn\\u2019t contain some story about a company being hacked or \\nsomehow losing their\", \" customers\\u2019 data. Even countries/regions aren\\u2019t immune to the problems created \\nby treating security\", \" as an afterthought. For years, companies have treated the security of customer \\ndata and, in fact, \", \"their entire networks as something of a \\u201cnice to have\\u201d. Windows servers were left \\nunpatched, ancien\", \"t versions of PHP kept running, and MongoDB databases left wide open to the \\nworld. \\nHowever, there \", \"are starting to be real-world consequences for not maintaining a security mindset \\nwhen building and\", \" deploying applications. Many companies learned the hard way what can happen \\nwhen servers and deskt\", \"ops aren\\u2019t patched during the 2017 outbreak of NotPetya. The cost of these \\nattacks has easily reach\", \"ed into the billions, with some estimates putting the losses from this single \\nattack at 10 billion \", \"US dollars. \\nEven governments aren\\u2019t immune to hacking incidents. The city of Baltimore was held ran\", \"som by \\ncriminals making it impossible for citizens to pay their bills or use city services. \\nThere \", \"has also been an increase in legislation that mandates certain data protections for personal \\ndata. \", \"In Europe, GDPR has been in effect for more than a year and, more recently, California passed \\ntheir\", \" own version called CCDA, which comes into effect January 1, 2020. The fines under GDPR can be \\nso p\", \"unishing as to put companies out of business. Google has already been fined 50 million Euros for \\nvi\", \"olations, but that\\u2019s just a drop in the bucket compared with the potential fines. \\nIn short, securit\", \"y is serious business. \\nAzure security for cloud-native apps \\nCloud-native applications can be both \", \"easier and more difficult to secure than traditional applications. \\nOn the downside, you need to sec\", \"ure more smaller applications and dedicate more energy to build \\nout the security infrastructure. Th\", \"e heterogeneous nature of programming languages and styles in \\nmost service deployments also means y\", \"ou need to pay more attention to security bulletins from many \\ndifferent providers. \\nOn the flip sid\", \"e, smaller services, each with their own data store, limit the scope of an attack. If an \\nattacker c\", \"ompromises one system, it\\u2019s probably more difficult for the attacker to make the jump to \\nanother sy\", \"stem than it is in a monolithic application. Process boundaries are strong boundaries. Also, \\nif a d\", \"atabase backup gets exposed, then the damage is more limited, as that database contains only a \\nsubs\", \"et of data and is unlikely to contain personal data. \\n \\n148 \\nCHAPTER 9 | Cloud-native security \\n \\nTh\", \"reat modeling \\nNo matter if the advantages outweigh the disadvantages of cloud-native applications, \", \"the same \\nholistic security mindset must be followed. Security and secure thinking must be part of e\", \"very step of \\nthe development and operations story. When planning an application ask questions like:\", \" \\n\\u2022 \\nWhat would be the impact of this data being lost? \\n\\u2022 \\nHow can we limit the damage from bad data\", \" being injected into this service? \\n\\u2022 \\nWho should have access to this data? \\n\\u2022 \\nAre there auditing p\", \"olicies in place around the development and release process? \\nAll these questions are part of a proc\", \"ess called threat modeling. This process tries to answer the \\nquestion of what threats there are to \", \"the system, how likely the threats are, and the potential damage \\nfrom them. \\nOnce the list of threa\", \"ts has been established, you need to decide whether they\\u2019re worth mitigating. \\nSometimes a threat is\", \" so unlikely and expensive to plan for that it isn\\u2019t worth spending energy on it. \\nFor instance, som\", \"e state level actor could inject changes into the design of a process that is used by \\nmillions of d\", \"evices. Now, instead of running a certain piece of code in Ring 3, that code is run in Ring \\n0. This\", \" process allows an exploit that can bypass the hypervisor and run the attack code on the bare \\nmetal\", \" machines, allowing attacks on all the virtual machines that are running on that hardware. \\nThe alte\", \"red processors are difficult to detect without a microscope and advanced knowledge of the on \\nsilico\", \"n design of that processor. This scenario is unlikely to happen and expensive to mitigate, so \\nproba\", \"bly no threat model would recommend building exploit protection for it. \\nMore likely threats, such a\", \"s broken access controls permitting Id incrementing attacks (replacing Id=2 \\nwith Id=3 in the URL) o\", \"r SQL injection, are more attractive to build protections against. The \\nmitigations for these threat\", \"s are quite reasonable to build and prevent embarrassing security holes \\nthat smear the company\\u2019s re\", \"putation. \\nPrinciple of least privilege \\nOne of the founding ideas in computer security is the Princ\", \"iple of Least Privilege (POLP). It\\u2019s actually a \\nfoundational idea in most any form of security be i\", \"t digital or physical. In short, the principle is that \\nany user or process should have the smallest\", \" number of rights possible to execute its task. \\nAs an example, think of the tellers at a bank: acce\", \"ssing the safe is an uncommon activity. So, the \\naverage teller can\\u2019t open the safe themselves. To g\", \"ain access, they need to escalate their request \\nthrough a bank manager, who performs additional sec\", \"urity checks. \\nIn a computer system, a fantastic example is the rights of a user connecting to a dat\", \"abase. In many \\ncases, there\\u2019s a single user account used to both build the database structure and r\", \"un the application. \\nExcept in extreme cases, the account running the application doesn\\u2019t need the a\", \"bility to update \\nschema information. There should be several accounts that provide different levels\", \" of privilege. The \\napplication should only use the permission level that grants read and writes acc\", \"ess to the data in the \\ntables. This kind of protection would eliminate attacks that aimed to drop d\", \"atabase tables or \\nintroduce malicious triggers. \\n \\n149 \\nCHAPTER 9 | Cloud-native security \\n \\nAlmost\", \" every part of building a cloud-native application can benefit from remembering the principle \\nof le\", \"ast privilege. You can find it at play when setting up firewalls, network security groups, roles, an\", \"d \\nscopes in Role-based access control (RBAC). \\nPenetration testing \\nAs applications become more com\", \"plicated the number of attack vectors increases at an alarming rate. \\nThreat modeling is flawed in t\", \"hat it tends to be executed by the same people building the system. In \\nthe same way that many devel\", \"opers have trouble envisioning user interactions and then build \\nunusable user interfaces, most deve\", \"lopers have difficulty seeing every attack vector. It\\u2019s also possible \\nthat the developers building \", \"the system aren\\u2019t well versed in attack methodologies and miss \\nsomething crucial. \\nPenetration test\", \"ing or \\u201cpen testing\\u201d involves bringing in external actors to attempt to attack the \\nsystem. These at\", \"tackers may be an external consulting company or other developers with good \\nsecurity knowledge from\", \" another part of the business. They\\u2019re given carte blanche to attempt to \\nsubvert the system. Freque\", \"ntly, they\\u2019ll find extensive security holes that need to be patched. \\nSometimes the attack vector wi\", \"ll be something totally unexpected like exploiting a phishing attack \\nagainst the CEO. \\nAzure itself\", \" is constantly undergoing attacks from a team of hackers inside Microsoft. Over the years, \\nthey\\u2019ve \", \"been the first to find dozens of potentially catastrophic attack vectors, closing them before \\nthey \", \"can be exploited externally. The more tempting a target, the more likely that eternal actors will \\na\", \"ttempt to exploit it and there are a few targets in the world more tempting than Azure. \\nMonitoring \", \"\\nShould an attacker attempt to penetrate an application, there should be some warning of it. \\nFreque\", \"ntly, attacks can be spotted by examining the logs from services. Attacks leave telltale signs \\nthat\", \" can be spotted before they succeed. For instance, an attacker attempting to guess a password \\nwill \", \"make many requests to a login system. Monitoring around the login system can detect weird \\npatterns \", \"that are out of line with the typical access pattern. This monitoring can be turned into an \\nalert t\", \"hat can, in turn, alert an operations person to activate some sort of countermeasure. A highly \\nmatu\", \"re monitoring system might even take action based on these deviations proactively adding rules \\nto b\", \"lock requests or throttle responses. \\nSecuring the build \\nOne place where security is often overlook\", \"ed is around the build process. Not only should the build \\nrun security checks, such as scanning for\", \" insecure code or checked-in credentials, but the build itself \\nshould be secure. If the build serve\", \"r is compromised, then it provides a fantastic vector for introducing \\narbitrary code into the produ\", \"ct. \\nImagine that an attacker is looking to steal the passwords of people signing into a web applica\", \"tion. \\nThey could introduce a build step that modifies the checked-out code to mirror any login requ\", \"est to \\nanother server. The next time code goes through the build, it\\u2019s silently updated. The source\", \" code \\nvulnerability scanning won\\u2019t catch this vulnerability as it runs before the build. Equally, n\", \"obody will \\n \\n150 \\nCHAPTER 9 | Cloud-native security \\n \\ncatch it in a code review because the build \", \"steps live on the build server. The exploited code will go to \\nproduction where it can harvest passw\", \"ords. Probably there\\u2019s no audit log of the build process \\nchanges, or at least nobody monitoring the\", \" audit. \\nThis scenario is a perfect example of a seemingly low-value target that can be used to brea\", \"k into the \\nsystem. Once an attacker breaches the perimeter of the system, they can start working on\", \" finding \\nways to elevate their permissions to the point that they can cause real harm anywhere they\", \" like. \\nBuilding secure code \\n.NET Framework is already a quite secure framework. It avoids some of \", \"the pitfalls of unmanaged \\ncode, such as walking off the ends of arrays. Work is actively done to fi\", \"x security holes as they\\u2019re \\ndiscovered. There\\u2019s even a bug bounty program that pays researchers to \", \"find issues in the framework \\nand report them instead of exploiting them. \\nThere are many ways to ma\", \"ke .NET code more secure. Following guidelines such as the Secure coding \\nguidelines for .NET articl\", \"e is a reasonable step to take to ensure that the code is secure from the \\nground up. The OWASP top \", \"10 is another invaluable guide to build secure code. \\nThe build process is a good place to put scann\", \"ing tools to detect problems in source code before they \\nmake it into production. Most every project\", \" has dependencies on some other packages. A tool that \\ncan scan for outdated packages will catch pro\", \"blems in a nightly build. Even when building Docker \\nimages, it\\u2019s useful to check and make sure that\", \" the base image doesn\\u2019t have known vulnerabilities. \\nAnother thing to check is that nobody has accid\", \"entally checked in credentials. \\nBuilt-in security \\nAzure is designed to balance usability and secur\", \"ity for most users. Different users are going to have \\ndifferent security requirements, so they need\", \" to fine-tune their approach to cloud security. Microsoft \\npublishes a great deal of security inform\", \"ation in the Trust Center. This resource should be the first \\nstop for those professionals intereste\", \"d in understanding how the built-in attack mitigation \\ntechnologies work. \\nWithin the Azure portal, \", \"the Azure Advisor is a system that is constantly scanning an environment and \\nmaking recommendations\", \". Some of these recommendations are designed to save users money, but \\nothers are designed to identi\", \"fy potentially insecure configurations, such as having a storage container \\nopen to the world and no\", \"t protected by a Virtual Network. \\nAzure network infrastructure \\nIn an on-premises deployment enviro\", \"nment, a great deal of energy is dedicated to setting up \\nnetworking. Setting up routers, switches, \", \"and the such is complicated work. Networks allow certain \\nresources to talk to other resources and p\", \"revent access in some cases. A frequent network rule is to \\nrestrict access to the production enviro\", \"nment from the development environment on the off chance \\nthat a half-developed piece of code runs a\", \"wry and deletes a swath of data. \\nOut of the box, most PaaS Azure resources have only the most basic\", \" and permissive networking setup. \\nFor instance, anybody on the Internet can access an app service. \", \"New SQL Server instances typically \\n \\n151 \\nCHAPTER 9 | Cloud-native security \\n \\ncome restricted, so \", \"that external parties can\\u2019t access them, but the IP address ranges used by Azure \\nitself are permitt\", \"ed through. So, while the SQL server is protected from external threats, an attacker \\nonly needs to \", \"set up an Azure bridgehead from where they can launch attacks against all SQL \\ninstances on Azure. \\n\", \"Fortunately, most Azure resources can be placed into an Azure Virtual Network that allows fine-\\ngrai\", \"ned access control. Similar to the way that on-premises networks establish private networks that \\nar\", \"e protected from the wider world, virtual networks are islands of private IP addresses that are \\nloc\", \"ated within the Azure network. \\n \\nFigure 9-1. A virtual network in Azure. \\nIn the same way that on-p\", \"remises networks have a firewall governing access to the network, you can \\nestablish a similar firew\", \"all at the boundary of the virtual network. By default, all the resources on a \\nvirtual network can \", \"still talk to the Internet. It\\u2019s only incoming connections that require some form of \\nexplicit firew\", \"all exception. \\nWith the network established, internal resources like storage accounts can be set up\", \" to only allow for \\naccess by resources that are also on the Virtual Network. This firewall provides\", \" an extra level of \\nsecurity, should the keys for that storage account be leaked, attackers wouldn\\u2019t\", \" be able to connect to \\nit to exploit the leaked keys. This scenario is another example of the princ\", \"iple of least privilege. \\nThe nodes in an Azure Kubernetes cluster can participate in a virtual netw\", \"ork just like other resources \\nthat are more native to Azure. This functionality is called Azure Con\", \"tainer Networking Interface. In \\neffect, it allocates a subnet within the virtual network on which v\", \"irtual machines and container images \\nare allocated. \\nContinuing down the path of illustrating the p\", \"rinciple of least privilege, not every resource within a \\nVirtual Network needs to talk to every oth\", \"er resource. For instance, in an application that provides a \\n \\n152 \\nCHAPTER 9 | Cloud-native securi\", \"ty \\n \\nweb API over a storage account and a SQL database, it\\u2019s unlikely that the database and the sto\", \"rage \\naccount need to talk to one another. Any data sharing between them would go through the web \\na\", \"pplication. So, a network security group (NSG) could be used to deny traffic between the two \\nservic\", \"es. \\nA policy of denying communication between resources can be annoying to implement, especially \\nc\", \"oming from a background of using Azure without traffic restrictions. On some other clouds, the \\nconc\", \"ept of network security groups is much more prevalent. For instance, the default policy on AWS is \\nt\", \"hat resources can\\u2019t communicate among themselves until enabled by rules in an NSG. While slower \\nto \", \"develop this, a more restrictive environment provides a more secure default. Making use of proper \\nD\", \"evOps practices, especially using Azure Resource Manager or Terraform to manage permissions can \\nmak\", \"e controlling the rules easier. \\nVirtual Networks can also be useful when setting up communication b\", \"etween on-premises and cloud \\nresources. A virtual private network can be used to seamlessly attach \", \"the two networks together. This \\napproach allows running a virtual network without any sort of gatew\", \"ay for scenarios where all the \\nusers are on-site. There are a number of technologies that can be us\", \"ed to establish this network. The \\nsimplest is to use a site-to-site VPN that can be established bet\", \"ween many routers and Azure. Traffic \\nis encrypted and tunneled over the Internet at the same cost p\", \"er byte as any other traffic. For \\nscenarios where more bandwidth or more security is desirable, Azu\", \"re offers a service called Express \\nRoute that uses a private circuit between an on-premises network\", \" and Azure. It\\u2019s more costly and \\ndifficult to establish but also more secure. \\nRole-based access co\", \"ntrol for restricting access to Azure resources \\nRBAC is a system that provides an identity to appli\", \"cations running in Azure. Applications can access \\nresources using this identity instead of or in ad\", \"dition to using keys or passwords. \\nSecurity Principals \\nThe first component in RBAC is a security p\", \"rincipal. A security principal can be a user, group, service \\nprincipal, or managed identity. \\n \\nFig\", \"ure 9-2. Different types of security principals. \\n\\u2022 \\nUser - Any user who has an account in Azure Act\", \"ive Directory is a user. \\n\\u2022 \\nGroup - A collection of users from Azure Active Directory. As a member \", \"of a group, a user \\ntakes on the roles of that group in addition to their own. \\n\\u2022 \\nService principal\", \" - A security identity under which services or applications run. \\n \\n153 \\nCHAPTER 9 | Cloud-native se\", \"curity \\n \\n\\u2022 \\nManaged identity - An Azure Active Directory identity managed by Azure. Managed identit\", \"ies \\nare typically used when developing cloud applications that manage the credentials for \\nauthenti\", \"cating to Azure services. \\nThe security principal can be applied to most any resource. This aspect m\", \"eans that it\\u2019s possible to \\nassign a security principal to a container running within Azure Kubernet\", \"es, allowing it to access secrets \\nstored in Key Vault. An Azure Function could take on a permission\", \" allowing it to talk to an Active \\nDirectory instance to validate a JWT for a calling user. Once ser\", \"vices are enabled with a service \\nprincipal, their permissions can be managed granularly using roles\", \" and scopes. \\nRoles \\nA security principal can take on many roles or, using a more sartorial analogy,\", \" wear many hats. Each \\nrole defines a series of permissions such as \\u201cRead messages from Azure Servic\", \"e Bus endpoint\\u201d. The \\neffective permission set of a security principal is the combination of all the\", \" permissions assigned to all \\nthe roles that a security principal has. Azure has a large number of b\", \"uilt-in roles and users can define \\ntheir own roles. \\n \\nFigure 9-3. RBAC role definitions. \\nBuilt in\", \"to Azure are also a number of high-level roles such as Owner, Contributor, Reader, and User \\nAccount\", \" Administrator. With the Owner role, a security principal can access all resources and assign \\npermi\", \"ssions to others. A contributor has the same level of access to all resources but they can\\u2019t assign \", \"\\npermissions. A Reader can only view existing Azure resources and a User Account Administrator can \\n\", \"manage access to Azure resources. \\nMore granular built-in roles such as DNS Zone Contributor have ri\", \"ghts limited to a single service. \\nSecurity principals can take on any number of roles. \\n \\n154 \\nCHAP\", \"TER 9 | Cloud-native security \\n \\nScopes \\nRoles can be applied to a restricted set of resources withi\", \"n Azure. For instance, applying scope to the \\nprevious example of reading from a Service Bus queue, \", \"you can narrow the permission to a single \\nqueue: \\u201cRead messages from Azure Service Bus endpoint bla\", \"h.servicebus.windows.net/queue1\\u201d \\nThe scope can be as narrow as a single resource or it can be appli\", \"ed to an entire resource group, \\nsubscription, or even management group. \\nWhen testing if a security\", \" principal has certain permission, the combination of role and scope are \\ntaken into account. This c\", \"ombination provides a powerful authorization mechanism. \\nDeny \\nPreviously, only \\u201callow\\u201d rules were p\", \"ermitted for RBAC. This behavior made some scopes complicated \\nto build. For instance, allowing a se\", \"curity principal access to all storage accounts except one required \\ngranting explicit permission to\", \" a potentially endless list of storage accounts. Every time a new storage \\naccount was created, it w\", \"ould have to be added to this list of accounts. This added management \\noverhead that certainly wasn\\u2019\", \"t desirable. \\nDeny rules take precedence over allow rules. Now representing the same \\u201callow all but \", \"one\\u201d scope \\ncould be represented as two rules \\u201callow all\\u201d and \\u201cdeny this one specific one\\u201d. Deny rul\", \"es not only \\nease management but allow for resources that are extra secure by denying access to ever\", \"ybody. \\nChecking access \\nAs you can imagine, having a large number of roles and scopes can make figu\", \"ring out the effective \\npermission of a service principal quite difficult. Piling deny rules on top \", \"of that, only serves to increase \\nthe complexity. Fortunately, there\\u2019s a permissions calculator that\", \" can show the effective permissions \\nfor any service principal. It\\u2019s typically found under the IAM t\", \"ab in the portal, as shown in Figure 9-3. \\n \\nFigure 9-4. Permission calculator for an app service. \\n\", \" \\n155 \\nCHAPTER 9 | Cloud-native security \\n \\nSecuring secrets \\nPasswords and certificates are a commo\", \"n attack vector for attackers. Password-cracking hardware can \\ndo a brute-force attack and try to gu\", \"ess billions of passwords per second. So it\\u2019s important that the \\npasswords that are used to access \", \"resources are strong, with a large variety of characters. These \\npasswords are exactly the kind of p\", \"asswords that are near impossible to remember. Fortunately, the \\npasswords in Azure don\\u2019t actually n\", \"eed to be known by any human. \\nMany security experts suggest that using a password manager to keep y\", \"our own passwords is the \\nbest approach. While it centralizes your passwords in one location, it als\", \"o allows using highly complex \\npasswords and ensuring they\\u2019re unique for each account. The same syst\", \"em exists within Azure: a \\ncentral store for secrets. \\nAzure Key Vault \\nAzure Key Vault provides a c\", \"entralized location to store passwords for things such as databases, API \\nkeys, and certificates. On\", \"ce a secret is entered into the Vault, it\\u2019s never shown again and the \\ncommands to extract and view \", \"it are purposefully complicated. The information in the safe is \\nprotected using either software enc\", \"ryption or FIPS 140-2 Level 2 validated Hardware Security \\nModules. \\nAccess to the key vault is prov\", \"ided through RBACs, meaning that not just any user can access the \\ninformation in the vault. Say a w\", \"eb application wishes to access the database connection string stored \\nin Azure Key Vault. To gain a\", \"ccess, applications need to run using a service principal. Under this \\nassumed role, they can read t\", \"he secrets from the safe. There are a number of different security \\nsettings that can further limit \", \"the access that an application has to the vault, so that it can\\u2019t update \\nsecrets but only read them\", \". \\nAccess to the key vault can be monitored to ensure that only the expected applications are access\", \"ing \\nthe vault. The logs can be integrated back into Azure Monitor, unlocking the ability to set up \", \"alerts \\nwhen unexpected conditions are encountered. \\nKubernetes \\nWithin Kubernetes, there\\u2019s a simila\", \"r service for maintaining small pieces of secret information. \\nKubernetes Secrets can be set via the\", \" typical kubectl executable. \\nCreating a secret is as simple as finding the base64 version of the va\", \"lues to be stored: \\necho -n 'admin' | base64 \\nYWRtaW4= \\necho -n '1f2d1e2e67df' | base64 \\nMWYyZDFlMmU\", \"2N2Rm \\nThen adding it to a secrets file named secret.yml for example that looks similar to the follo\", \"wing \\nexample: \\napiVersion: v1 \\nkind: Secret \\nmetadata: \\n  name: mysecret \\n \\n156 \\nCHAPTER 9 | Cloud-\", \"native security \\n \\ntype: Opaque \\ndata: \\n  username: YWRtaW4= \\n  password: MWYyZDFlMmU2N2Rm \\nFinally,\", \" this file can be loaded into Kubernetes by running the following command: \\nkubectl apply -f ./secre\", \"t.yaml \\nThese secrets can then be mounted into volumes or exposed to container processes through \\nen\", \"vironment variables. The Twelve-factor app approach to building applications suggests using the \\nlow\", \"est common denominator to transmit settings to an application. Environment variables are the \\nlowest\", \" common denominator, because they\\u2019re supported no matter the operating system or \\napplication. \\nAn a\", \"lternative to use the built-in Kubernetes secrets is to access the secrets in Azure Key Vault from \\n\", \"within Kubernetes. The simplest way to do this is to assign an RBAC role to the container looking to\", \" \\nload secrets. The application can then use the Azure Key Vault APIs to access the secrets. However\", \", \\nthis approach requires modifications to the code and doesn\\u2019t follow the pattern of using environm\", \"ent \\nvariables. Instead, it\\u2019s possible to inject values into a container. This approach is actually \", \"more secure \\nthan using the Kubernetes secrets directly, as they can be accessed by users on the clu\", \"ster. \\nEncryption in transit and at rest \\nKeeping data safe is important whether it\\u2019s on disk or tra\", \"nsiting between various different services. \\nThe most effective way to keep data from leaking is to \", \"encrypt it into a format that can\\u2019t be easily read \\nby others. Azure supports a wide range of encryp\", \"tion options. \\nIn transit \\nThere are several ways to encrypt traffic on the network in Azure. The ac\", \"cess to Azure services is \\ntypically done over connections that use Transport Layer Security (TLS). \", \"For instance, all the \\nconnections to the Azure APIs require TLS connections. Equally, connections t\", \"o endpoints in Azure \\nstorage can be restricted to work only over TLS encrypted connections. \\nTLS is\", \" a complicated protocol and simply knowing that the connection is using TLS isn\\u2019t sufficient to \\nens\", \"ure security. For instance, TLS 1.0 is chronically insecure, and TLS 1.1 isn\\u2019t much better. Even wit\", \"hin \\nthe versions of TLS, there are various settings that can make the connections easier to decrypt\", \". The \\nbest course of action is to check and see if the server connection is using up-to-date and we\", \"ll \\nconfigured protocols. \\nThis check can be done by an external service such as SSL labs\\u2019 SSL Serve\", \"r Test. A test run against a \\ntypical Azure endpoint, in this case a service bus endpoint, yields a \", \"near perfect score of A. \\nEven services like Azure SQL databases use TLS encryption to keep data hid\", \"den. The interesting part \\nabout encrypting the data in transit using TLS is that it isn\\u2019t possible,\", \" even for Microsoft, to listen in on \\nthe connection between computers running TLS. This should prov\", \"ide comfort for companies \\nconcerned that their data may be at risk from Microsoft proper or even a \", \"state actor with more \\nresources than the standard attacker. \\n \\n157 \\nCHAPTER 9 | Cloud-native securi\", \"ty \\n \\n \\nFigure 9-5. SSL labs report showing a score of A for a Service Bus endpoint. \\nWhile this lev\", \"el of encryption isn\\u2019t going to be sufficient for all time, it should inspire confidence that \\nAzure\", \" TLS connections are quite secure. Azure will continue to evolve its security standards as \\nencrypti\", \"on improves. It\\u2019s nice to know that there\\u2019s somebody watching the security standards and \\nupdating A\", \"zure as they improve. \\nAt rest \\nIn any application, there are a number of places where data rests on\", \" the disk. The application code \\nitself is loaded from some storage mechanism. Most applications als\", \"o use some kind of a database \\nsuch as SQL Server, Cosmos DB, or even the amazingly price-efficient \", \"Table Storage. These databases \\nall use heavily encrypted storage to ensure that nobody other than t\", \"he applications with proper \\npermissions can read your data. Even the system operators can\\u2019t read da\", \"ta that has been encrypted. \\nSo customers can remain confident their secret information remains secr\", \"et. \\nStorage \\nThe underpinning of much of Azure is the Azure Storage engine. Virtual machine disks a\", \"re mounted \\non top of Azure Storage. Azure Kubernetes Service runs on virtual machines that, themsel\", \"ves, are \\nhosted on Azure Storage. Even serverless technologies, such as Azure Functions Apps and Az\", \"ure \\nContainer Instances, run out of disk that is part of Azure Storage. \\nIf Azure Storage is well e\", \"ncrypted, then it provides for a foundation for most everything else to also \\nbe encrypted. Azure St\", \"orage is encrypted with FIPS 140-2 compliant 256-bit AES. This is a well-\\nregarded encryption techno\", \"logy having been the subject of extensive academic scrutiny over the last \\n20 or so years. At presen\", \"t, there\\u2019s no known practical attack that would allow someone without \\nknowledge of the key to read \", \"data encrypted by AES. \\nBy default, the keys used for encrypting Azure Storage are managed by Micros\", \"oft. There are extensive \\nprotections in place to ensure to prevent malicious access to these keys. \", \"However, users with \\nparticular encryption requirements can also provide their own storage keys that\", \" are managed in Azure \\n \\n158 \\nCHAPTER 9 | Cloud-native security \\n \\nKey Vault. These keys can be revo\", \"ked at any time, which would effectively render the contents of the \\nStorage account using them inac\", \"cessible. \\nVirtual machines use encrypted storage, but it\\u2019s possible to provide another layer of enc\", \"ryption by \\nusing technologies like BitLocker on Windows or DM-Crypt on Linux. These technologies me\", \"an that \\neven if the disk image was leaked off of storage, it would remain near impossible to read i\", \"t. \\nAzure SQL \\nDatabases hosted on Azure SQL use a technology called Transparent Data Encryption (TD\", \"E) to ensure \\ndata remains encrypted. It\\u2019s enabled by default on all newly created SQL databases, bu\", \"t must be \\nenabled manually for legacy databases. TDE executes real-time encryption and decryption o\", \"f not just \\nthe database, but also the backups and transaction logs. \\nThe encryption parameters are \", \"stored in the master database and, on startup, are read into memory \\nfor the remaining operations. T\", \"his means that the master database must remain unencrypted. The \\nactual key is managed by Microsoft.\", \" However, users with exacting security requirements may provide \\ntheir own key in Key Vault in much \", \"the same way as is done for Azure Storage. The Key Vault provides \\nfor such services as key rotation\", \" and revocation. \\nThe \\u201cTransparent\\u201d part of TDS comes from the fact that there aren\\u2019t client changes\", \" needed to use an \\nencrypted database. While this approach provides for good security, leaking the d\", \"atabase password is \\nenough for users to be able to decrypt the data. There\\u2019s another approach that \", \"encrypts individual \\ncolumns or tables in a database. Always Encrypted ensures that at no point the \", \"encrypted data \\nappears in plain text inside the database. \\nSetting up this tier of encryption requi\", \"res running through a wizard in SQL Server Management Studio \\nto select the sort of encryption and w\", \"here in Key Vault to store the associated keys. \\n \\n159 \\nCHAPTER 9 | Cloud-native security \\n \\n \\nFigur\", \"e 9-6. Selecting columns in a table to be encrypted using Always Encrypted. \\nClient applications tha\", \"t read information from these encrypted columns need to make special \\nallowances to read encrypted d\", \"ata. Connection strings need to be updated with Column Encryption \\nSetting=Enabled and client creden\", \"tials must be retrieved from the Key Vault. The SQL Server client \\nmust then be primed with the colu\", \"mn encryption keys. Once that is done, the remaining actions use \\nthe standard interfaces to SQL Cli\", \"ent. That is, tools like Dapper and Entity Framework, which are built \\non top of SQL Client, will co\", \"ntinue to work without changes. Always Encrypted may not yet be \\navailable for every SQL Server driv\", \"er on every language. \\nThe combination of TDE and Always Encrypted, both of which can be used with c\", \"lient-specific keys, \\nensures that even the most exacting encryption requirements are supported. \\nCo\", \"smos DB \\nCosmos DB is the newest database provided by Microsoft in Azure. It has been built from the\", \" ground \\nup with security and cryptography in mind. AES-256bit encryption is standard for all Cosmos\", \" DB \\n \\n160 \\nCHAPTER 9 | Cloud-native security \\n \\ndatabases and can\\u2019t be disabled. Coupled with the T\", \"LS 1.2 requirement for communication, the entire \\nstorage solution is encrypted. \\n \\nFigure 9-7. The \", \"flow of data encryption within Cosmos DB. \\nWhile Cosmos DB doesn\\u2019t provide for supplying customer en\", \"cryption keys, there has been significant \\nwork done by the team to ensure it remains PCI-DSS compli\", \"ant without that. Cosmos DB also doesn\\u2019t \\nsupport any sort of single column encryption similar to Az\", \"ure SQL\\u2019s Always Encrypted yet. \\nKeeping secure \\nAzure has all the tools necessary to release a high\", \"ly secure product. However, a chain is only as strong \\nas its weakest link. If the applications depl\", \"oyed on top of Azure aren\\u2019t developed with a proper \\nsecurity mindset and good security audits, then\", \" they become the weak link in the chain. There are \\nmany great static analysis tools, encryption lib\", \"raries, and security practices that can be used to ensure \\nthat the software installed on Azure is a\", \"s secure as Azure itself. Examples include static analysis tools, \\nencryption libraries, and securit\", \"y practices. \\n \\n161 \\nCHAPTER 10 | DevOps \\n \\nCHAPTER 10 \\nDevOps \\nThe favorite mantra of software cons\", \"ultants is to answer \\u201cIt depends\\u201d to any question posed. It isn\\u2019t \\nbecause software consultants are \", \"fond of not taking a position. It\\u2019s because there\\u2019s no one true \\nanswer to any questions in software\", \". There\\u2019s no absolute right and wrong, but rather a balance \\nbetween opposites. \\nTake, for instance,\", \" the two major schools of developing web applications: Single Page Applications \\n(SPAs) versus serve\", \"r-side applications. On the one hand, the user experience tends to be better with \\nSPAs and the amou\", \"nt of traffic to the web server can be minimized making it possible to host them \\non something as si\", \"mple as static hosting. On the other hand, SPAs tend to be slower to develop and \\nmore difficult to \", \"test. Which one is the right choice? Well, it depends on your situation. \\nCloud-native applications \", \"aren\\u2019t immune to that same dichotomy. They have clear advantages in \\nterms of speed of development, \", \"stability, and scalability, but managing them can be quite a bit more \\ndifficult. \\nYears ago, it was\", \"n\\u2019t uncommon for the process of moving an application from development to \\nproduction to take a mont\", \"h, or even more. Companies released software on a 6-month or even every \\nyear cadence. One needs to \", \"look no further than Microsoft Windows to get an idea for the cadence of \\nreleases that were accepta\", \"ble before the ever-green days of Windows 10. Five years passed between \\nWindows XP and Vista, a fur\", \"ther three between Vista and Windows 7. \\nIt\\u2019s now fairly well established that being able to release\", \" software rapidly gives fast-moving companies \\na huge market advantage over their more sloth-like co\", \"mpetitors. It\\u2019s for that reason that major \\nupdates to Windows 10 are now approximately every six mo\", \"nths. \\nThe patterns and practices that enable faster, more reliable releases to deliver value to the\", \" business \\nare collectively known as DevOps. They consist of a wide range of ideas spanning the enti\", \"re software \\ndevelopment life cycle from specifying an application all the way up to delivering and \", \"operating that \\napplication. \\nDevOps emerged before microservices and it\\u2019s likely that the movement \", \"towards smaller, more fit to \\npurpose services wouldn\\u2019t have been possible without DevOps to make re\", \"leasing and operating not \\njust one but many applications in production easier. \\n \\n162 \\nCHAPTER 10 |\", \" DevOps \\n \\n \\nFigure 10-1 - DevOps and microservices. \\nThrough good DevOps practices, it\\u2019s possible t\", \"o realize the advantages of cloud-native applications \\nwithout suffocating under a mountain of work \", \"actually operating the applications. \\nThere\\u2019s no golden hammer when it comes to DevOps. Nobody can s\", \"ell a complete and all-\\nencompassing solution for releasing and operating high-quality applications.\", \" This is because each \\napplication is wildly different from all others. However, there are tools tha\", \"t can make DevOps a far less \\ndaunting proposition. One of these tools is known as Azure DevOps. \\nAz\", \"ure DevOps \\nAzure DevOps has a long pedigree. It can trace its roots back to when Team Foundation Se\", \"rver first \\nmoved online and through the various name changes: Visual Studio Online and Visual Studi\", \"o Team \\nServices. Through the years, however, it has become far more than its predecessors. \\nAzure D\", \"evOps is divided into five major components: \\n \\nFigure 10-2 - Azure DevOps. \\nAzure Repos - Source co\", \"de management that supports the venerable Team Foundation Version \\nControl (TFVC) and the industry f\", \"avorite Git. Pull requests provide a way to enable social coding by \\nfostering discussion of changes\", \" as they\\u2019re made. \\n \\n163 \\nCHAPTER 10 | DevOps \\n \\nAzure Boards - Provides an issue and work item trac\", \"king tool that strives to allow users to pick the \\nworkflows that work best for them. It comes with \", \"a number of pre-configured templates including \\nones to support SCRUM and Kanban styles of developme\", \"nt. \\nAzure Pipelines - A build and release management system that supports tight integration with Az\", \"ure. \\nBuilds can be run on various platforms from Windows to Linux to macOS. Build agents may be \\npr\", \"ovisioned in the cloud or on-premises. \\nAzure Test Plans - No QA person will be left behind with the\", \" test management and exploratory \\ntesting support offered by the Test Plans feature. \\nAzure Artifact\", \"s - An artifact feed that allows companies to create their own, internal, versions of \\nNuGet, npm, a\", \"nd others. It serves a double purpose of acting as a cache of upstream packages if \\nthere\\u2019s a failur\", \"e of a centralized repository. \\nThe top-level organizational unit in Azure DevOps is known as a Proj\", \"ect. Within each project the \\nvarious components, such as Azure Artifacts, can be turned on and off.\", \" Each of these components \\nprovides different advantages for cloud-native applications. The three mo\", \"st useful are repositories, \\nboards, and pipelines. If users want to manage their source code in ano\", \"ther repository stack, such as \\nGitHub, but still take advantage of Azure Pipelines and other compon\", \"ents, that\\u2019s perfectly possible. \\nFortunately, development teams have many options when selecting a \", \"repository. One of them is \\nGitHub. \\nGitHub Actions \\nFounded in 2009, GitHub is a widely popular web\", \"-based repository for hosting projects, \\ndocumentation, and code. Many large tech companies, such as\", \" Apple, Amazon, Google, and \\nmainstream corporations use GitHub. GitHub uses the open-source, distri\", \"buted version control system \\nnamed Git as its foundation. On top, it then adds its own set of featu\", \"res, including defect tracking, \\nfeature and pull requests, tasks management, and wikis for each cod\", \"e base. \\nAs GitHub evolves, it too is adding DevOps features. For example, GitHub has its own contin\", \"uous \\nintegration/continuous delivery (CI/CD) pipeline, called GitHub Actions. GitHub Actions is a \\n\", \"community-powered workflow automation tool. It lets DevOps teams integrate with their existing \\ntool\", \"ing, mix and match new products, and hook into their software lifecycle, including existing CI/CD \\np\", \"artners.\\u201d \\nGitHub has over 40 million users, making it the largest host of source code in the world.\", \" In October of \\n2018, Microsoft purchased GitHub. Microsoft has pledged that GitHub will remain an o\", \"pen platform \\nthat any developer can plug into and extend. It continues to operate as an independent\", \" company. \\nGitHub offers plans for enterprise, team, professional, and free accounts. \\nSource contro\", \"l \\nOrganizing the code for a cloud-native application can be challenging. Instead of a single giant \", \"\\napplication, the cloud-native applications tend to be made up of a web of smaller applications that\", \" \\ntalk with one another. As with all things in computing, the best arrangement of code remains an op\", \"en \\n \\n164 \\nCHAPTER 10 | DevOps \\n \\nquestion. There are examples of successful applications using diff\", \"erent kinds of layouts, but two \\nvariants seem to have the most popularity. \\nBefore getting down int\", \"o the actual source control itself, it\\u2019s probably worth deciding on how many \\nprojects are appropria\", \"te. Within a single project, there\\u2019s support for multiple repositories, and build \\npipelines. Boards\", \" are a little more complicated, but there too, the tasks can easily be assigned to \\nmultiple teams w\", \"ithin a single project. It\\u2019s possible to support hundreds, even thousands of \\ndevelopers, out of a s\", \"ingle Azure DevOps project. Doing so is likely the best approach as it provides a \\nsingle place for \", \"all developer to work out of and reduces the confusion of finding that one application \\nwhen develop\", \"ers are unsure in which project in which it resides. \\nSplitting up code for microservices within the\", \" Azure DevOps project can be slightly more challenging. \\n \\nFigure 10-3 - One vs. many repositories. \", \"\\nRepository per microservice \\nAt first glance, this approach seems like the most logical approach to\", \" splitting up the source code for \\nmicroservices. Each repository can contain the code needed to bui\", \"ld the one microservice. The \\nadvantages to this approach are readily visible: \\n1. \\nInstructions for\", \" building and maintaining the application can be added to a README file at \\nthe root of each reposit\", \"ory. When flipping through the repositories, it\\u2019s easy to find these \\ninstructions, reducing spin-up\", \" time for developers. \\n2. \\nEvery service is located in a logical place, easily found by knowing the \", \"name of the service. \\n3. \\nBuilds can easily be set up such that they\\u2019re only triggered when a change\", \" is made to the \\nowning repository. \\n4. \\nThe number of changes coming into a repository is limited t\", \"o the small number of developers \\nworking on the project. \\n \\n165 \\nCHAPTER 10 | DevOps \\n \\n5. \\nSecurit\", \"y is easy to set up by restricting the repositories to which developers have read and \\nwrite permiss\", \"ions. \\n6. \\nRepository level settings can be changed by the owning team with a minimum of discussion \", \"\\nwith others. \\nOne of the key ideas behind microservices is that services should be siloed and separ\", \"ated from each \\nother. When using Domain Driven Design to decide on the boundaries for services the \", \"services act as \\ntransactional boundaries. Database updates shouldn\\u2019t span multiple services. This c\", \"ollection of related \\ndata is referred to as a bounded context. This idea is reflected by the isolat\", \"ion of microservice data to \\na database separate and autonomous from the rest of the services. It ma\", \"kes a great deal of sense to \\ncarry this idea all the way through to the source code. \\nHowever, this\", \" approach isn\\u2019t without its issues. One of the more gnarly development problems of our \\ntime is mana\", \"ging dependencies. Consider the number of files that make up the average \\nnode_modules directory. A \", \"fresh install of something like create-react-app is likely to bring with it \\nthousands of packages. \", \"The question of how to manage these dependencies is a difficult one. \\nIf a dependency is updated, th\", \"en downstream packages must also update this dependency. \\nUnfortunately, that takes development work\", \" so, invariably, the node_modules directory ends up with \\nmultiple versions of a single package, eac\", \"h one a dependency of some other package that is \\nversioned at a slightly different cadence. When de\", \"ploying an application, which version of a \\ndependency should be used? The version that is currently\", \" in production? The version that is currently \\nin Beta but is likely to be in production by the time\", \" the consumer makes it to production? Difficult \\nproblems that aren\\u2019t resolved by just using microse\", \"rvices. \\nThere are libraries that are depended upon by a wide variety of projects. By dividing the m\", \"icroservices \\nup with one in each repository the internal dependencies can best be resolved by using\", \" the internal \\nrepository, Azure Artifacts. Builds for libraries will push their latest versions int\", \"o Azure Artifacts for \\ninternal consumption. The downstream project must still be manually updated t\", \"o take a dependency \\non the newly updated packages. \\nAnother disadvantage presents itself when movin\", \"g code between services. Although it would be nice \\nto believe that the first division of an applica\", \"tion into microservices is 100% correct, the reality is that \\nrarely we\\u2019re so prescient as to make n\", \"o service division mistakes. Thus, functionality and the code that \\ndrives it will need to move from\", \" service to service: repository to repository. When leaping from one \\nrepository to another, the cod\", \"e loses its history. There are many cases, especially in the event of an \\naudit, where having full h\", \"istory on a piece of code is invaluable. \\nThe final and most important disadvantage is coordinating \", \"changes. In a true microservices \\napplication, there should be no deployment dependencies between se\", \"rvices. It should be possible to \\ndeploy services A, B, and C in any order as they have loose coupli\", \"ng. In reality, however, there are \\ntimes when it\\u2019s desirable to make a change that crosses multiple\", \" repositories at the same time. Some \\nexamples include updating a library to close a security hole o\", \"r changing a communication protocol \\nused by all services. \\nTo do a cross-repository change requires\", \" a commit to each repository be made in succession. Each \\nchange in each repository will need to be \", \"pull-requested and reviewed separately. This activity can be \\ndifficult to coordinate. \\n \\n166 \\nCHAPT\", \"ER 10 | DevOps \\n \\nAn alternative to using many repositories is to put all the source code together i\", \"n a giant, all knowing, \\nsingle repository. \\nSingle repository \\nIn this approach, sometimes referred\", \" to as a monorepository, all the source code for every service is \\nput into the same repository. At \", \"first, this approach seems like a terrible idea likely to make dealing \\nwith source code unwieldy. T\", \"here are, however, some marked advantages to working this way. \\nThe first advantage is that it\\u2019s eas\", \"ier to manage dependencies between projects. Instead of relying on \\nsome external artifact feed, pro\", \"jects can directly import one another. This means that updates are \\ninstant, and conflicting version\", \"s are likely to be found at compile time on the developer\\u2019s workstation. \\nIn effect, shifting some o\", \"f the integration testing left. \\nWhen moving code between projects, it\\u2019s now easier to preserve the \", \"history as the files will be \\ndetected as having been moved rather than being rewritten. \\nAnother ad\", \"vantage is that wide ranging changes that cross service boundaries can be made in a \\nsingle commit. \", \"This activity reduces the overhead of having potentially dozens of changes to review \\nindividually. \", \"\\nThere are many tools that can perform static analysis of code to detect insecure programming \\npract\", \"ices or problematic use of APIs. In a multi-repository world, each repository will need to be \\nitera\", \"ted over to find the problems in them. The single repository allows running the analysis all in one \", \"\\nplace. \\nThere are also many disadvantages to the single repository approach. One of the most worryi\", \"ng ones \\nis that having a single repository raises security concerns. If the contents of a repositor\", \"y are leaked in \\na repository per service model, the amount of code lost is minimal. With a single r\", \"epository, \\neverything the company owns could be lost. There have been many examples in the past of \", \"this \\nhappening and derailing entire game development efforts. Having multiple repositories exposes \", \"less \\nsurface area, which is a desirable trait in most security practices. \\nThe size of the single r\", \"epository is likely to become unmanageable rapidly. This presents some \\ninteresting performance impl\", \"ications. It may become necessary to use specialized tools such as Virtual \\nFile System for Git, whi\", \"ch was originally designed to improve the experience for developers on the \\nWindows team. \\nFrequentl\", \"y the argument for using a single repository boils down to an argument that Facebook or \\nGoogle use \", \"this method for source code arrangement. If the approach is good enough for these \\ncompanies, then, \", \"surely, it\\u2019s the correct approach for all companies. The truth of the matter is that few \\ncompanies \", \"operate on anything like the scale of Facebook or Google. The problems that occur at \\nthose scales a\", \"re different from those most developers will face. What is good for the goose may not \\nbe good for t\", \"he gander. \\nIn the end, either solution can be used to host the source code for microservices. Howev\", \"er, in most \\ncases, the management, and engineering overhead of operating in a single repository isn\", \"\\u2019t worth the \\nmeager advantages. Splitting code up over multiple repositories encourages better sepa\", \"ration of \\nconcerns and encourages autonomy among development teams. \\n \\n167 \\nCHAPTER 10 | DevOps \\n \\n\", \"Standard directory structure \\nRegardless of the single versus multiple repositories debate each serv\", \"ice will have its own directory. \\nOne of the best optimizations to allow developers to cross between\", \" projects quickly is to maintain a \\nstandard directory structure. \\n \\nFigure 10-4 - Standard director\", \"y structure. \\nWhenever a new project is created, a template that puts in place the correct structure\", \" should be used. \\nThis template can also include such useful items as a skeleton README file and an \", \"azure-\\npipelines.yml. In any microservice architecture, a high degree of variance between projects m\", \"akes bulk \\noperations against the services more difficult. \\nThere are many tools that can provide te\", \"mplating for an entire directory, containing several source \\ncode directories. Yeoman is popular in \", \"the JavaScript world and GitHub have recently released \\nRepository Templates, which provide much of \", \"the same functionality. \\nTask management \\nManaging tasks in any project can be difficult. Up front t\", \"here are countless questions to be answered \\nabout the sort of workflows to set up to ensure optimal\", \" developer productivity. \\nCloud-native applications tend to be smaller than traditional software pro\", \"ducts or at least they\\u2019re \\ndivided into smaller services. Tracking of issues or tasks related to the\", \"se services remains as important \\nas with any other software project. Nobody wants to lose track of \", \"some work item or explain to a \\ncustomer that their issue wasn\\u2019t properly logged. Boards are configu\", \"red at the project level but within \\neach project, areas can be defined. These allow breaking down i\", \"ssues across several components. The \\nadvantage to keeping all the work for the entire application i\", \"n one place is that it\\u2019s easy to move work \\nitems from one team to another as they\\u2019re understood bet\", \"ter. \\n \\n168 \\nCHAPTER 10 | DevOps \\n \\nAzure DevOps comes with a number of popular templates pre-config\", \"ured. In the most basic \\nconfiguration, all that is needed to know is what\\u2019s in the backlog, what pe\", \"ople are working on, and \\nwhat\\u2019s done. It\\u2019s important to have this visibility into the process of bu\", \"ilding software, so that work \\ncan be prioritized and completed tasks reported to the customer. Of c\", \"ourse, few software projects \\nstick to a process as simple as to do, doing, and done. It doesn\\u2019t tak\", \"e long for people to start adding \\nsteps like QA or Detailed Specification to the process. \\nOne of t\", \"he more important parts of Agile methodologies is self-introspection at regular intervals. \\nThese re\", \"views are meant to provide insight into what problems the team is facing and how they can \\nbe improv\", \"ed. Frequently, this means changing the flow of issues and features through the \\ndevelopment process\", \". So, it\\u2019s perfectly healthy to expand the layouts of the boards with additional \\nstages. \\nThe stage\", \"s in the boards aren\\u2019t the only organizational tool. Depending on the configuration of the \\nboard, t\", \"here\\u2019s a hierarchy of work items. The most granular item that can appear on a board is a task. \\nOut \", \"of the box a task contains fields for a title, description, a priority, an estimate of the amount of\", \" \\nwork remaining and the ability to link to other work items or development items (branches, commits\", \", \\npull requests, builds, and so forth). Work items can be classified into different areas of the ap\", \"plication \\nand different iterations (sprints) to make finding them easier. \\n \\nFigure 10-5 - Task in \", \"Azure DevOps. \\nThe description field supports the normal styles you\\u2019d expect (bold, italic underscor\", \"e and strike \\nthrough) and the ability to insert images. This makes it a powerful tool for use when \", \"specifying work \\nor bugs. \\nTasks can be rolled up into features, which define a larger unit of work.\", \" Features, in turn, can be rolled \\nup into epics. Classifying tasks in this hierarchy makes it much \", \"easier to understand how close a large \\nfeature is to rolling out. \\n \\n169 \\nCHAPTER 10 | DevOps \\n \\n \\n\", \"Figure 10-6 - Work item in Azure DevOps. \\nThere are different kinds of views into the issues in Azur\", \"e Boards. Items that aren\\u2019t yet scheduled \\nappear in the backlog. From there, they can be assigned t\", \"o a sprint. A sprint is a time box during \\nwhich it\\u2019s expected some quantity of work will be complet\", \"ed. This work can include tasks but also the \\nresolution of tickets. Once there, the entire sprint c\", \"an be managed from the Sprint board section. This \\nview shows how work is progressing and includes a\", \" burn down chart to give an ever-updating \\nestimate of if the sprint will be successful. \\n \\nFigure 1\", \"0-7 - Board in Azure DevOps. \\nBy now, it should be apparent that there\\u2019s a great deal of power in th\", \"e Boards in Azure DevOps. For \\ndevelopers, there are easy views of what is being worked on. For proj\", \"ect managers views into \\nupcoming work as well as an overview of existing work. For managers, there \", \"are plenty of reports \\nabout resourcing and capacity. Unfortunately, there\\u2019s nothing magical about c\", \"loud-native applications \\nthat eliminate the need to track work. But if you must track work, there a\", \"re a few places where the \\nexperience is better than in Azure DevOps. \\nCI/CD pipelines \\nAlmost no ch\", \"ange in the software development life cycle has been so revolutionary as the advent of \\ncontinuous i\", \"ntegration (CI) and continuous delivery (CD). Building and running automated tests \\nagainst the sour\", \"ce code of a project as soon as a change is checked in catches mistakes early. Prior to \\nthe advent \", \"of continuous integration builds, it wouldn\\u2019t be uncommon to pull code from the \\n \\n170 \\nCHAPTER 10 |\", \" DevOps \\n \\nrepository and find that it didn\\u2019t pass tests or couldn\\u2019t even be built. This resulted in\", \" tracking down \\nthe source of the breakage. \\nTraditionally shipping software to the production envir\", \"onment required extensive documentation and \\na list of steps. Each one of these steps needed to be m\", \"anually completed in a very error prone \\nprocess. \\n \\nFigure 10-8 - Checklist. \\nThe sister of continu\", \"ous integration is continuous delivery in which the freshly built packages are \\ndeployed to an envir\", \"onment. The manual process can\\u2019t scale to match the speed of development so \\nautomation becomes more\", \" important. Checklists are replaced by scripts that can execute the same \\ntasks faster and more accu\", \"rately than any human. \\nThe environment to which continuous delivery delivers might be a test enviro\", \"nment or, as is being \\ndone by many major technology companies, it could be the production environme\", \"nt. The latter \\nrequires an investment in high-quality tests that can give confidence that a change \", \"isn\\u2019t going to \\nbreak production for users. In the same way that continuous integration caught issue\", \"s in the code \\nearly continuous delivery catches issues in the deployment process early. \\nThe import\", \"ance of automating the build and delivery process is accentuated by cloud-native \\napplications. Depl\", \"oyments happen more frequently and to more environments so manually deploying \\nborders on impossible\", \". \\nAzure Builds \\nAzure DevOps provides a set of tools to make continuous integration and deployment \", \"easier than \\never. These tools are located under Azure Pipelines. The first of them is Azure Builds,\", \" which is a tool \\nfor running YAML-based build definitions at scale. Users can either bring their ow\", \"n build machines \\n(great for if the build requires a meticulously set up environment) or use a machi\", \"ne from a constantly \\nrefreshed pool of Azure hosted virtual machines. These hosted build agents com\", \"e pre-installed with a \\n \\n171 \\nCHAPTER 10 | DevOps \\n \\nwide range of development tools for not just .\", \"NET development but for everything from Java to \\nPython to iPhone development. \\nDevOps includes a wi\", \"de range of out of the box build definitions that can be customized for any build. \\nThe build defini\", \"tions are defined in a file called azure-pipelines.yml and checked into the repository so \\nthey can \", \"be versioned along with the source code. This makes it much easier to make changes to the \\nbuild pip\", \"eline in a branch as the changes can be checked into just that branch. An example azure-\\npipelines.y\", \"ml for building an ASP.NET web application on full framework is show in Figure 10-9. \\nname: $(rev:r)\", \" \\n \\nvariables: \\n  version: 9.2.0.$(Build.BuildNumber) \\n  solution: Portals.sln \\n  artifactName: drop\", \" \\n  buildPlatform: any cpu \\n  buildConfiguration: release \\n \\npool: \\n  name: Hosted VisualStudio \\n  d\", \"emands: \\n  - msbuild \\n  - visualstudio \\n  - vstest \\n \\nsteps: \\n- task: NuGetToolInstaller@0 \\n  displa\", \"yName: 'Use NuGet 4.4.1' \\n  inputs: \\n    versionSpec: 4.4.1 \\n \\n- task: NuGetCommand@2 \\n  displayName\", \": 'NuGet restore' \\n  inputs: \\n    restoreSolution: '$(solution)' \\n \\n- task: VSBuild@1 \\n  displayName\", \": 'Build solution' \\n  inputs: \\n    solution: '$(solution)' \\n    msbuildArgs: '-p:DeployOnBuild=true \", \"-p:WebPublishMethod=Package -\\np:PackageAsSingleFile=true -p:SkipInvalidConfigurations=true -\\np:Packa\", \"geLocation=\\\"$(build.artifactstagingdirectory)\\\\\\\\\\\"' \\n    platform: '$(buildPlatform)' \\n    configurati\", \"on: '$(buildConfiguration)' \\n \\n- task: VSTest@2 \\n  displayName: 'Test Assemblies' \\n  inputs: \\n    te\", \"stAssemblyVer2: | \\n     **\\\\$(buildConfiguration)\\\\**\\\\*test*.dll \\n     !**\\\\obj\\\\** \\n     !**\\\\*testadapt\", \"er.dll \\n    platform: '$(buildPlatform)' \\n    configuration: '$(buildConfiguration)' \\n \\n- task: Copy\", \"Files@2 \\n  displayName: 'Copy UI Test Files to: $(build.artifactstagingdirectory)' \\n  inputs: \\n \\n172\", \" \\nCHAPTER 10 | DevOps \\n \\n    SourceFolder: UITests \\n    TargetFolder: '$(build.artifactstagingdirect\", \"ory)/uitests' \\n \\n- task: PublishBuildArtifacts@1 \\n  displayName: 'Publish Artifact' \\n  inputs: \\n    \", \"PathtoPublish: '$(build.artifactstagingdirectory)' \\n    ArtifactName: '$(artifactName)' \\n  condition\", \": succeededOrFailed() \\nFigure 10-9 - A sample azure-pipelines.yml \\nThis build definition uses a numb\", \"er of built-in tasks that make creating builds as simple as building a \\nLego set (simpler than the g\", \"iant Millennium Falcon). For instance, the NuGet task restores NuGet \\npackages, while the VSBuild ta\", \"sk calls the Visual Studio build tools to perform the actual compilation. \\nThere are hundreds of dif\", \"ferent tasks available in Azure DevOps, with thousands more that are \\nmaintained by the community. I\", \"t\\u2019s likely that no matter what build tasks you\\u2019re looking to run, \\nsomebody has built one already. \\n\", \"Builds can be triggered manually, by a check-in, on a schedule, or by the completion of another buil\", \"d. \\nIn most cases, building on every check-in is desirable. Builds can be filtered so that different\", \" builds run \\nagainst different parts of the repository or against different branches. This allows fo\", \"r scenarios like \\nrunning fast builds with reduced testing on pull requests and running a full regre\", \"ssion suite against \\nthe trunk on a nightly basis. \\nThe end result of a build is a collection of fil\", \"es known as build artifacts. These artifacts can be passed \\nalong to the next step in the build proc\", \"ess or added to an Azure Artifacts feed, so they can be \\nconsumed by other builds. \\nAzure DevOps rel\", \"eases \\nBuilds take care of compiling the software into a shippable package, but the artifacts still \", \"need to be \\npushed out to a testing environment to complete continuous delivery. For this, Azure Dev\", \"Ops uses a \\nseparate tool called Releases. The Releases tool makes use of the same tasks\\u2019 library th\", \"at were \\navailable to the Build but introduce a concept of \\u201cstages\\u201d. A stage is an isolated environm\", \"ent into \\nwhich the package is installed. For instance, a product might make use of a development, a\", \" QA, and a \\nproduction environment. Code is continuously delivered into the development environment \", \"where \\nautomated tests can be run against it. Once those tests pass the release moves onto the QA \\ne\", \"nvironment for manual testing. Finally, the code is pushed to production where it\\u2019s visible to \\never\", \"ybody. \\n \\nFigure 10-10 - Release pipeline \\nEach stage in the build can be automatically triggered by\", \" the completion of the previous phase. In \\nmany cases, however, this isn\\u2019t desirable. Moving code in\", \"to production might require approval from \\nsomebody. The Releases tool supports this by allowing app\", \"rovers at each step of the release pipeline. \\nRules can be set up such that a specific person or gro\", \"up of people must sign off on a release before it \\n \\n173 \\nCHAPTER 10 | DevOps \\n \\nmakes into producti\", \"on. These gates allow for manual quality checks and also for compliance with any \\nregulatory require\", \"ments related to control what goes into production. \\nEverybody gets a build pipeline \\nThere\\u2019s no cos\", \"t to configuring many build pipelines, so it\\u2019s advantageous to have at least one build \\npipeline per\", \" microservice. Ideally, microservices are independently deployable to any environment so \\nhaving eac\", \"h one able to be released via its own pipeline without releasing a mass of unrelated code is \\nperfec\", \"t. Each pipeline can have its own set of approvals allowing for variations in build process for \\neac\", \"h service. \\nVersioning releases \\nOne drawback to using the Releases functionality is that it can\\u2019t b\", \"e defined in a checked-in azure-\\npipelines.yml file. There are many reasons you might want to do tha\", \"t from having per-branch release \\ndefinitions to including a release skeleton in your project templa\", \"te. Fortunately, work is ongoing to \\nshift some of the stages support into the Build component. This\", \" will be known as multi-stage build \\nand the first version is available now! \\nFeature flags \\nIn chap\", \"ter 1, we affirmed that cloud native is much about speed and agility. Users expect rapid \\nresponsive\", \"ness, innovative features, and zero downtime. Feature flags are a modern deployment \\ntechnique that \", \"helps increase agility for cloud-native applications. They enable you to deploy new \\nfeatures into a\", \" production environment, but restrict their availability. With the flick of a switch, you can \\nactiv\", \"ate a new feature for specific users without restarting the app or deploying new code. They \\nseparat\", \"e the release of new features from their code deployment. \\nFeature flags are built upon conditional \", \"logic that control visibility of functionality for users at run \\ntime. In modern cloud-native system\", \"s, it\\u2019s common to deploy new features into production early, but \\ntest them with a limited audience.\", \" As confidence increases, the feature can be incrementally rolled out \\nto wider audiences. \\nOther us\", \"e cases for feature flags include: \\n\\u2022 \\nRestrict premium functionality to specific customer groups wi\", \"lling to pay higher subscription \\nfees. \\n\\u2022 \\nStabilize a system by quickly deactivating a problem fea\", \"ture, avoiding the risks of a rollback or \\nimmediate hotfix. \\n\\u2022 \\nDisable an optional feature with hi\", \"gh resource consumption during peak usage periods. \\n\\u2022 \\nConduct experimental feature releases to smal\", \"l user segments to validate feasibility and \\npopularity. \\nFeature flags also promote trunk-based dev\", \"elopment. It\\u2019s a source-control branching model where \\ndevelopers collaborate on features in a singl\", \"e branch. The approach minimizes the risk and complexity \\nof merging large numbers of long-running f\", \"eature branches. Features are unavailable until activated. \\n \\n174 \\nCHAPTER 10 | DevOps \\n \\nImplementi\", \"ng feature flags \\nAt its core, a feature flag is a reference to a simple decision object. It returns\", \" a Boolean state of on or \\noff. The flag typically wraps a block of code that encapsulates a feature\", \" capability. The state of the flag \\ndetermines whether that code block executes for a given user. Fi\", \"gure 10-11 shows the \\nimplementation. \\nif (featureFlag) { \\n    // Run this code block if the feature\", \"Flag value is true \\n} else { \\n    // Run this code block if the featureFlag value is false \\n} \\nFigur\", \"e 10-11 - Simple feature flag implementation. \\nNote how this approach separates the decision logic f\", \"rom the feature code. \\nIn chapter 1, we discussed the Twelve-Factor App. The guidance recommended ke\", \"eping configuration \\nsettings external from application executable code. When needed, settings can b\", \"e read in from the \\nexternal source. Feature flag configuration values should also be independent fr\", \"om their codebase. By \\nexternalizing flag configuration in a separate repository, you can change fla\", \"g state without modifying \\nand redeploying the application. \\nAzure App Configuration provides a cent\", \"ralized repository for feature flags. With it, you define \\ndifferent kinds of feature flags and mani\", \"pulate their states quickly and confidently. You add the App \\nConfiguration client libraries to your\", \" application to enable feature flag functionality. Various \\nprogramming language frameworks are supp\", \"orted. \\nFeature flags can be easily implemented in an ASP.NET Core service. Installing the .NET Feat\", \"ure \\nManagement libraries and App Configuration provider enable you to declaratively add feature fla\", \"gs to \\nyour code. They enable FeatureGate attributes so that you don\\u2019t have to manually write if sta\", \"tements \\nacross your codebase. \\nOnce configured in your Startup class, you can add feature flag func\", \"tionality at the controller, action, \\nor middleware level. Figure 10-12 presents controller and acti\", \"on implementation: \\n[FeatureGate(MyFeatureFlags.FeatureA)] \\npublic class ProductController : Control\", \"ler \\n{ \\n    ... \\n} \\n \\n \\n[FeatureGate(MyFeatureFlags.FeatureA)] \\npublic IActionResult UpdateProductSt\", \"atus() \\n{ \\n    return ObjectResult(ProductDto); \\n} \\nFigure 10-12 - Feature flag implementation in a \", \"controller and action. \\nIf a feature flag is disabled, the user will receive a 404 (Not Found) statu\", \"s code with no response body. \\nFeature flags can also be injected directly into C# classes. Figure 1\", \"0-13 shows feature flag injection: \\n \\n175 \\nCHAPTER 10 | DevOps \\n \\npublic class ProductController : C\", \"ontroller \\n{ \\n    private readonly IFeatureManager _featureManager; \\n \\n    public ProductController(\", \"IFeatureManager featureManager) \\n    { \\n        _featureManager = featureManager; \\n    } \\n} \\nFigure \", \"10-13 - Feature flag injection into a class. \\nThe Feature Management libraries manage the feature fl\", \"ag lifecycle behind the scenes. For example, \\nto minimize high numbers of calls to the configuration\", \" store, the libraries cache flag states for a \\nspecified duration. They can guarantee the immutabili\", \"ty of flag states during a request call. They also \\noffer a Point-in-time snapshot. You can reconstr\", \"uct the history of any key-value and provide its past \\nvalue at any moment within the previous seven\", \" days. \\nInfrastructure as code \\nCloud-native systems embrace microservices, containers, and modern s\", \"ystem design to achieve speed \\nand agility. They provide automated build and release stages to ensur\", \"e consistent and quality code. \\nBut, that\\u2019s only part of the story. How do you provision the cloud e\", \"nvironments upon which these \\nsystems run? \\nModern cloud-native applications embrace the widely acce\", \"pted practice of Infrastructure as Code, or \\nIaC. With IaC, you automate platform provisioning. You \", \"essentially apply software engineering \\npractices such as testing and versioning to your DevOps prac\", \"tices. Your infrastructure and \\ndeployments are automated, consistent, and repeatable. Just as conti\", \"nuous delivery automated the \\ntraditional model of manual deployments, Infrastructure as Code (IaC) \", \"is evolving how application \\nenvironments are managed. \\nTools like Azure Resource Manager (ARM), Ter\", \"raform, and the Azure Command Line Interface (CLI) \\nenable you to declaratively script the cloud inf\", \"rastructure you require. \\nAzure Resource Manager templates \\nARM stands for Azure Resource Manager. I\", \"t\\u2019s an API provisioning engine that is built into Azure and \\nexposed as an API service. ARM enables \", \"you to deploy, update, delete, and manage the resources \\ncontained in Azure resource group in a sing\", \"le, coordinated operation. You provide the engine with a \\nJSON-based template that specifies the res\", \"ources you require and their configuration. ARM \\nautomatically orchestrates the deployment in the co\", \"rrect order respecting dependencies. The engine \\nensures idempotency. If a desired resource already \", \"exists with the same configuration, provisioning \\nwill be ignored. \\nAzure Resource Manager templates\", \" are a JSON-based language for defining various resources in \\nAzure. The basic schema looks somethin\", \"g like Figure 10-14. \\n{ \\n  \\\"$schema\\\": \\\"https://schema.management.azure.com/schemas/2015-01-\\n \\n176 \\nC\", \"HAPTER 10 | DevOps \\n \\n01/deploymentTemplate.json#\\\", \\n  \\\"contentVersion\\\": \\\"\\\", \\n  \\\"apiProfile\\\": \\\"\\\", \\n \", \" \\\"parameters\\\": {  }, \\n  \\\"variables\\\": {  }, \\n  \\\"functions\\\": [  ], \\n  \\\"resources\\\": [  ], \\n  \\\"outputs\\\":\", \" {  } \\n} \\nFigure 10-14 - The schema for a Resource Manager template \\nWithin this template, one might\", \" define a storage container inside the resources section like so: \\n\\\"resources\\\": [ \\n    { \\n      \\\"typ\", \"e\\\": \\\"Microsoft.Storage/storageAccounts\\\", \\n      \\\"name\\\": \\\"[variables('storageAccountName')]\\\", \\n      \", \"\\\"location\\\": \\\"[parameters('location')]\\\", \\n      \\\"apiVersion\\\": \\\"2018-07-01\\\", \\n      \\\"sku\\\": { \\n        \", \"\\\"name\\\": \\\"[parameters('storageAccountType')]\\\" \\n      }, \\n      \\\"kind\\\": \\\"StorageV2\\\", \\n      \\\"propertie\", \"s\\\": {} \\n    } \\n  ], \\nFigure 10-15 - An example of a storage account defined in a Resource Manager te\", \"mplate \\nAn ARM template can be parameterized with dynamic environment and configuration information.\", \" \\nDoing so enables it to be reused to define different environments, such as development, QA, or \\npr\", \"oduction. Normally, the template creates all resources within a single Azure resource group. It\\u2019s \\np\", \"ossible to define multiple resource groups in a single Resource Manager template, if needed. You \\nca\", \"n delete all resources in an environment by deleting the resource group itself. Cost analysis can al\", \"so \\nbe run at the resource group level, allowing for quick accounting of how much each environment i\", \"s \\ncosting. \\nThere are many examples of ARM templates available in the Azure Quickstart Templates pr\", \"oject on \\nGitHub. They can help accelerate creating a new template or modifying an existing one. \\nRe\", \"source Manager templates can be run in many of ways. Perhaps the simplest way is to simply paste \\nth\", \"em into the Azure portal. For experimental deployments, this method can be quick. They can also be \\n\", \"run as part of a build or release process in Azure DevOps. There are tasks that will leverage \\nconne\", \"ctions into Azure to run the templates. Changes to Resource Manager templates are applied \\nincrement\", \"ally, meaning that to add a new resource requires just adding it to the template. The tooling \\nwill \", \"reconcile differences between the current resources and those defined in the template. Resources \\nwi\", \"ll then be created or altered so they match what is defined in the template. \\nTerraform \\nCloud-nativ\", \"e applications are often constructed to be cloud agnostic. Being so means the application \\nisn\\u2019t tig\", \"htly coupled to a particular cloud vendor and can be deployed to any public cloud. \\n \\n177 \\nCHAPTER 1\", \"0 | DevOps \\n \\nTerraform is a commercial templating tool that can provision cloud-native applications\", \" across all the \\nmajor cloud players: Azure, Google Cloud Platform, AWS, and AliCloud. Instead of us\", \"ing JSON as the \\ntemplate definition language, it uses the slightly more terse HCL (Hashicorp Config\", \"uration Language). \\nAn example Terraform file that does the same as the previous Resource Manager te\", \"mplate (Figure 10-\\n15) is shown in Figure 10-16: \\nprovider \\\"azurerm\\\" { \\n  version = \\\"=1.28.0\\\" \\n} \\n \\n\", \"resource \\\"azurerm_resource_group\\\" \\\"testrg\\\" { \\n  name     = \\\"production\\\" \\n  location = \\\"West US\\\" \\n} \\n\", \" \\nresource \\\"azurerm_storage_account\\\" \\\"testsa\\\" { \\n  name                     = \\\"${var.storageAccountN\", \"ame}\\\" \\n  resource_group_name      = \\\"${azurerm_resource_group.testrg.name}\\\" \\n  location             \", \"    = \\\"${var.region}\\\" \\n  account_tier             = \\\"${var.tier}\\\" \\n  account_replication_type = \\\"${v\", \"ar.replicationType}\\\" \\n \\n} \\nFigure 10-16 - An example of a Resource Manager template \\nTerraform also \", \"provides intuitive error messages for problem templates. There\\u2019s also a handy validate \\ntask that ca\", \"n be used in the build phase to catch template errors early. \\nAs with Resource Manager templates, co\", \"mmand-line tools are available to deploy Terraform \\ntemplates. There are also community-created task\", \"s in Azure Pipelines that can validate and apply \\nTerraform templates. \\nSometimes Terraform and ARM \", \"templates output meaningful values, such as a connection string to a \\nnewly created database. This i\", \"nformation can be captured in the build pipeline and used in \\nsubsequent tasks. \\nAzure CLI Scripts a\", \"nd Tasks \\nFinally, you can leverage Azure CLI to declaratively script your cloud infrastructure. Azu\", \"re CLI scripts \\ncan be created, found, and shared to provision and configure almost any Azure resour\", \"ce. The CLI is \\nsimple to use with a gentle learning curve. Scripts are executed within either Power\", \"Shell or Bash. \\nThey\\u2019re also straightforward to debug, especially when compared with ARM templates. \", \"\\nAzure CLI scripts work well when you need to tear down and redeploy your infrastructure. Updating \\n\", \"an existing environment can be tricky. Many CLI commands aren\\u2019t idempotent. That means they\\u2019ll \\nrecr\", \"eate the resource each time they\\u2019re run, even if the resource already exists. It\\u2019s always possible t\", \"o \\nadd code that checks for the existence of each resource before creating it. But, doing so, your s\", \"cript \\ncan become bloated and difficult to manage. \\nThese scripts can also be embedded in Azure DevO\", \"ps pipelines as Azure CLI tasks. Executing the \\npipeline invokes the script. \\n \\n178 \\nCHAPTER 10 | De\", \"vOps \\n \\nFigure 10-17 shows a YAML snippet that lists the version of Azure CLI and the details of the\", \" \\nsubscription. Note how Azure CLI commands are included in an inline script. \\n- task: AzureCLI@2 \\n \", \" displayName: Azure CLI \\n  inputs: \\n    azureSubscription: <Name of the Azure Resource Manager servi\", \"ce connection> \\n    scriptType: ps \\n    scriptLocation: inlineScript \\n    inlineScript: | \\n      az \", \"--version \\n      az account show \\nFigure 10-17 - Azure CLI script \\nIn the article, What is Infrastru\", \"cture as Code, Author Sam Guckenheimer describes how, \\u201cTeams who \\nimplement IaC can deliver stable e\", \"nvironments rapidly and at scale. Teams avoid manual \\nconfiguration of environments and enforce cons\", \"istency by representing the desired state of their \\nenvironments via code. Infrastructure deployment\", \"s with IaC are repeatable and prevent runtime issues \\ncaused by configuration drift or missing depen\", \"dencies. DevOps teams can work together with a \\nunified set of practices and tools to deliver applic\", \"ations and their supporting infrastructure rapidly, \\nreliably, and at scale.\\u201d \\nCloud Native Applicat\", \"ion Bundles \\nA key property of cloud-native applications is that they leverage the capabilities of t\", \"he cloud to speed \\nup development. This design often means that a full application uses different ki\", \"nds of technologies. \\nApplications may be shipped in Docker containers, some services may use Azure \", \"Functions, while \\nother parts may run directly on virtual machines allocated on large metal servers \", \"with hardware GPU \\nacceleration. No two cloud-native applications are the same, so it\\u2019s been difficu\", \"lt to provide a single \\nmechanism for shipping them. \\nThe Docker containers may run on Kubernetes us\", \"ing a Helm Chart for deployment. The Azure \\nFunctions may be allocated using Terraform templates. Fi\", \"nally, the virtual machines may be allocated \\nusing Terraform but built out using Ansible. This is a\", \" large variety of technologies and there has been \\nno way to package them all together into a reason\", \"able package. Until now. \\nCloud Native Application Bundles (CNABs) are a joint effort by many commun\", \"ity-minded companies \\nsuch as Microsoft, Docker, and HashiCorp to develop a specification to package\", \" distributed \\napplications. \\nThe effort was announced in December of 2018, so there\\u2019s still a fair b\", \"it of work to do to expose the \\neffort to the greater community. However, there\\u2019s already an open sp\", \"ecification and a reference \\nimplementation known as Duffle. This tool, which was written in Go, is \", \"a joint effort between Docker \\nand Microsoft. \\nThe CNABs can contain different kinds of installation\", \" technologies. This aspect allows things like Helm \\nCharts, Terraform templates, and Ansible Playboo\", \"ks to coexist in the same package. Once built, the \\npackages are self-contained and portable; they c\", \"an be installed from a USB stick. The packages are \\ncryptographically signed to ensure they originat\", \"e from the party they claim. \\n \\n179 \\nCHAPTER 10 | DevOps \\n \\nThe core of a CNAB is a file called bund\", \"le.json. This file defines the contents of the bundle, be they \\nTerraform or images or anything else\", \". Figure 11-9 defines a CNAB that invokes some Terraform. \\nNotice, however, that it actually defines\", \" an invocation image that is used to invoke the Terraform. \\nWhen packaged up, the Docker file that i\", \"s located in the cnab directory is built into a Docker image, \\nwhich will be included in the bundle.\", \" Having Terraform installed inside a Docker container in the \\nbundle means that users don\\u2019t need to \", \"have Terraform installed on their machine to run the bundling. \\n{ \\n    \\\"name\\\": \\\"terraform\\\", \\n    \\\"ve\", \"rsion\\\": \\\"0.1.0\\\", \\n    \\\"schemaVersion\\\": \\\"v1.0.0-WD\\\", \\n    \\\"parameters\\\": { \\n        \\\"backend\\\": { \\n    \", \"        \\\"type\\\": \\\"boolean\\\", \\n            \\\"defaultValue\\\": false, \\n            \\\"destination\\\": { \\n      \", \"          \\\"env\\\": \\\"TF_VAR_backend\\\" \\n            } \\n        } \\n    }, \\n    \\\"invocationImages\\\": [ \\n    \", \"    { \\n        \\\"imageType\\\": \\\"docker\\\", \\n        \\\"image\\\": \\\"cnab/terraform:latest\\\" \\n        } \\n    ], \\n\", \"    \\\"credentials\\\": { \\n        \\\"tenant_id\\\": { \\n            \\\"env\\\": \\\"TF_VAR_tenant_id\\\" \\n        }, \\n   \", \"     \\\"client_id\\\": { \\n            \\\"env\\\": \\\"TF_VAR_client_id\\\" \\n        }, \\n        \\\"client_secret\\\": { \\n\", \"            \\\"env\\\": \\\"TF_VAR_client_secret\\\" \\n        }, \\n        \\\"subscription_id\\\": { \\n            \\\"en\", \"v\\\": \\\"TF_VAR_subscription_id\\\" \\n        }, \\n        \\\"ssh_authorized_key\\\": { \\n            \\\"env\\\": \\\"TF_VA\", \"R_ssh_authorized_key\\\" \\n        } \\n    }, \\n    \\\"actions\\\": { \\n        \\\"status\\\": { \\n            \\\"modifi\", \"es\\\": true \\n        } \\n    } \\n} \\nFigure 10-18 - An example Terraform file \\nThe bundle.json also defin\", \"es a set of parameters that are passed down into the Terraform. \\nParameterization of the bundle allo\", \"ws for installation in various different environments. \\nThe CNAB format is also flexible, allowing i\", \"t to be used against any cloud. It can even be used against \\non-premises solutions such as OpenStack\", \". \\n \\n180 \\nCHAPTER 10 | DevOps \\n \\nDevOps Decisions \\nThere are so many great tools in the DevOps space\", \" these days and even more fantastic books and \\npapers on how to succeed. A favorite book to get star\", \"ted on the DevOps journey is The Phoenix \\nProject, which follows the transformation of a fictional c\", \"ompany from NoOps to DevOps. One thing is \\nfor certain: DevOps is no longer a \\u201cnice to have\\u201d when de\", \"ploying complex, Cloud Native Applications. \\nIt\\u2019s a requirement and should be planned for and resour\", \"ced at the start of any project. \\nReferences \\n\\u2022 \\nAzure DevOps \\n\\u2022 \\nAzure Resource Manager \\n\\u2022 \\nTerrafo\", \"rm \\n\\u2022 \\nAzure CLI \\n \\n181 \\nCHAPTER 11 | Summary: Architecting cloud-native apps \\n \\nCHAPTER 11 \\nSummary\", \": Architecting \\ncloud-native apps \\nIn summary, here are important conclusions from this guide: \\n\\u2022 \\nC\", \"loud-native is about designing modern applications that embrace rapid change, large scale, \\nand resi\", \"lience, in modern, dynamic environments such as public, private, and hybrid clouds. \\n\\u2022 \\nThe Cloud Na\", \"tive Computing Foundation (CNCF) is an influential open-source consortium \\nof over 300 major corpora\", \"tions. It\\u2019s responsible for driving the adoption of cloud-native \\ncomputing across technology and cl\", \"oud stacks. \\n\\u2022 \\nCNCF guidelines recommend that cloud-native applications embrace six important pilla\", \"rs as \\nshown in Figure 11-1: \\n \\nFigure 11-1. Cloud-native foundational pillars \\n\\u2022 \\nThese cloud-nativ\", \"e pillars include: \\n\\u2013 \\nThe cloud and its underlying service model \\n\\u2013 \\nModern design principles \\n\\u2013 \\nM\", \"icroservices \\n\\u2013 \\nContainerization and container orchestration \\n\\u2013 \\nCloud-based backing services, such\", \" as databases and message brokers \\n\\u2013 \\nAutomation, including Infrastructure as Code and code deployme\", \"nt \\n \\n182 \\nCHAPTER 11 | Summary: Architecting cloud-native apps \\n \\n\\u2022 \\nKubernetes is the hosting envi\", \"ronment of choice for most cloud-native applications. Smaller, \\nsimple services are sometimes hosted\", \" in serverless platforms, such as Azure Functions. Among \\nmany key automation features, both environ\", \"ments provide automatic scaling to handle \\nfluctuating workload volumes. \\n\\u2022 \\nService communication b\", \"ecomes a significant design decision when constructing a cloud-\\nnative application. Applications typ\", \"ically expose an API gateway to manage front-end client \\ncommunication. Then backend microservices s\", \"trive to communicate with each other \\nimplementing asynchronous communication patterns, when possibl\", \"e. \\n\\u2022 \\ngRPC is a modern, high-performance framework that evolves the age-old remote procedure \\ncall \", \"(RPC) protocol. Cloud-native applications often embrace gRPC to streamline messaging \\nbetween back-e\", \"nd services. gRPC uses HTTP/2 for its transport protocol. It can be up to 8x \\nfaster than JSON seria\", \"lization with message sizes 60-80% smaller. gRPC is open source and \\nmanaged by the Cloud Native Com\", \"puting Foundation (CNCF). \\n\\u2022 \\nDistributed data is a model often implemented by cloud-native applicat\", \"ions. Applications \\nsegregate business functionality into small, independent microservices. Each mic\", \"roservice \\nencapsulates its own dependencies, data, and state. The classic shared database model \\nev\", \"olves into one of many smaller databases, each aligning with a microservice. When the \\nsmoke clears,\", \" we emerge with a design that exposes a database-per-microservice model. \\n\\u2022 \\nNo-SQL databases refer \", \"to high-performance, non-relational data stores. They excel in their \\nease-of-use, scalability, resi\", \"lience, and availability characteristics. High volume services that \\nrequire sub second response tim\", \"e favor NoSQL datastores. The proliferation of NoSQL \\ntechnologies for distributed cloud-native syst\", \"ems can\\u2019t be overstated. \\n\\u2022 \\nNewSQL is an emerging database technology that combines the distributed\", \" scalability of \\nNoSQL and the ACID guarantees of a relational database. NewSQL databases target bus\", \"iness \\nsystems that must process high-volumes of data, across distributed environments, with full \\nt\", \"ransactional/ACID compliance. The Cloud Native Computing Foundation (CNCF) features \\nseveral NewSQL \", \"database projects. \\n\\u2022 \\nResiliency is the ability of your system to react to failure and still remain\", \" functional. Cloud-\\nnative systems embrace distributed architecture where failure is inevitable. App\", \"lications must \\nbe constructed to respond elegantly to failure and quickly return to a fully functio\", \"ning state. \\n\\u2022 \\nService meshes are a configurable infrastructure layer with built-in capabilities to\", \" handle \\nservice communication and other cross-cutting challenges. They decouple cross-cutting \\nresp\", \"onsibilities from your business code. These responsibilities move into a service proxy. \\nReferred to\", \" as the Sidecar pattern, the proxy is deployed into a separate process to provide \\nisolation from yo\", \"ur business code. \\n\\u2022 \\nObservability is a key design consideration for cloud-native applications. As \", \"services are \\ndistributed across a cluster of nodes, centralized logging, monitoring, and alerts, be\", \"come \\nmandatory. Azure Monitor is a collection of cloud-based tools designed to provide visibility \\n\", \"into the state of your system. \\n \\n183 \\nCHAPTER 11 | Summary: Architecting cloud-native apps \\n \\n\\u2022 \\nIn\", \"frastructure as Code is a widely accepted practice that automates platform provisioning. \\nYour infra\", \"structure and deployments are automated, consistent, and repeatable. Tools like \\nAzure Resource Mana\", \"ger, Terraform, and the Azure CLI, enable you to declaratively script the \\ncloud infrastructure you \", \"require. \\n\\u2022 \\nCode automation is a requirement for cloud-native applications. Modern CI/CD systems he\", \"lp \\nfulfill this principle. They provide separate build and deployment steps that help ensure \\nconsi\", \"stent and quality code. The build stage transforms the code into a binary artifact. The \\nrelease sta\", \"ge picks up the binary artifact, applies external environment configuration, and \\ndeploys it to a sp\", \"ecified environment. Azure DevOps and GitHub are full-featured DevOps \\nenvironments. \\n\"]"