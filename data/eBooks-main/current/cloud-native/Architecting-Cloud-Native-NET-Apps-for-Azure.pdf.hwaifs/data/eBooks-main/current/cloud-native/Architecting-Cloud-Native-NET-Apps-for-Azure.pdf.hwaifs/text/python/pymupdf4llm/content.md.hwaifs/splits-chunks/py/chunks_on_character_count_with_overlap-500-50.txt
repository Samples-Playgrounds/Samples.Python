['**EDITION v1.0.3**\n\n\n[Refer changelog](https://aka.ms/cn-ebook-changelog) for the book updates and community contributions.\n\n\nPUBLISHED BY\n\n\nMicrosoft Developer Division, .NET, and Visual Studio product teams\n\n\nA division of Microsoft Corporation\n\n\nOne Microsoft Way\n\n\nRedmond, Washington 98052-6399\n\n\nCopyright © 2023 by Microsoft Corporation\n\n\nAll rights reserved. No part of the contents of this book may be reproduced or transmitted in any\nform o', 'r by any means without the written permission of the publisher.\n\n\nThis book is provided “as-is” and expresses the author’s views and opinions. The views, opinions, and\ninformation expressed in this book, including URL and other Internet website references, may change\nwithout notice.\n\n\nSome examples depicted herein are provided for illustration only and are fictitious. No real association\nor connection is intended or should be inferred.\n\n\n[Microso', 'ft and the trademarks listed at https://www.microsoft.com](https://www.microsoft.com/) on the “Trademarks” webpage are\ntrademarks of the Microsoft group of companies.\n\n\nMac and macOS are trademarks of Apple Inc.\n\n\nThe Docker whale logo is a registered trademark of Docker, Inc. Used by permission.\n\n\nAll other marks and logos are property of their respective owners.\n\n\nAuthors:\n\n\n**Rob Vettor**, Principal MTC (Microsoft Technology Center) Architect ', 'for Cloud App Innovation [thinkingincloudnative.com, Microsoft](https://thinkingincloudnative.com/about/)\n\n\n**Steve “ardalis” Smith** [, Software Architect and Trainer - Ardalis.com](https://ardalis.com/)\n\n\nParticipants and Reviewers:\n\n\n**Cesar De la Torre**, Principal Program Manager, .NET team, Microsoft\n\n\n**Nish Anil**, Senior Program Manager, .NET team, Microsoft\n\n\n**Jeremy Likness**, Senior Program Manager, .NET team, Microsoft\n\n\n**Cecil Phi', 'llip**, Senior Cloud Advocate, Microsoft\n\n\n**Sumit Ghosh**, Principal Consultant at Neudesic\n\n\nEditors:\n\n\n**Maira Wenzel**, Program Manager, .NET team, Microsoft\n\n\n**David Pine**, Senior Content Developer, .NET docs, Microsoft\n### Version\n\n\nThis guide has been written to cover **.NET 7** version along with many additional updates related to\nthe same “wave” of technologies (that is, Azure and additional third-party technologies) coinciding in\ntime', ' with the .NET 7 release.\n### Who should use this guide\n\n\nThe audience for this guide is mainly developers, development leads, and architects who are\ninterested in learning how to build applications designed for the cloud.\n\n\nA secondary audience is technical decision-makers who plan to choose whether to build their\napplications using a cloud-native approach.\n### How you can use this guide\n\n\nThis guide begins by defining cloud native and introduci', 'ng a reference application built using cloudnative principles and technologies. Beyond these first two chapters, the rest of the book is broken up\ninto specific chapters focused on topics common to most cloud-native applications. You can jump to\nany of these chapters to learn about cloud-native approaches to:\n\n\n  - Data and data access\n\n  - Communication patterns\n\n  - Scaling and scalability\n\n  - Application resiliency\n\n  - Monitoring and health\n', '\n  - Identity and security\n\n  - DevOps\n\n\n[This guide is available both in PDF](https://dotnet.microsoft.com/download/e-book/cloud-native-azure/pdf) form and online. Feel free to forward this document or links to its\nonline version to your team to help ensure common understanding of these topics. Most of these\ntopics benefit from a consistent understanding of the underlying principles and patterns, as well as\nthe trade-offs involved in decisions r', 'elated to these topics. Our goal with this document is to equip\nteams and their leaders with the information they need to make well-informed decisions for their\napplications’ architecture, development, and hosting.\n\n\n## Contents\n\n**Introduction to cloud-native applications ............................................................................ 1**\n\n\nCloud-native computing ......................................................................', '............................................................................ 3\n\n\nWhat is Cloud Native? ............................................................................................................................................................ 4\n\n\nThe pillars of cloud native ................................................................................................................................................ 5\n\n\nThe cloud', ' ................................................................................................................................................................................ 5\n\n\nModern design ..................................................................................................................................................................... 6\n\n\nMicroservices ......................................................................', '.................................................................................................. 9\n\n\nContainers ........................................................................................................................................................................... 12\n\n\nBacking services ..............................................................................................................................................', '.................. 15\n\n\nAutomation ......................................................................................................................................................................... 17\n\n\nCandidate apps for cloud native ..................................................................................................................................... 19\n\n\nModernizing legacy apps .............................................', '................................................................................................. 19\n\n\nSummary .............................................................................................................................................................................. 21\n\n\n**Introducing eShopOnContainers reference app ................................................................ 22**\n\n\nFeatures and requirements ...............', '................................................................................................................................ 23\n\n\nOverview of the code .......................................................................................................................................................... 25\n\n\nUnderstanding microservices ...........................................................................................................', '................................ 27\n\n\nMapping eShopOnContainers to Azure Services ..................................................................................................... 27\n\n\nContainer orchestration and clustering ................................................................................................................... 28\n\n\nAPI Gateway .........................................................................................', '.............................................................................. 28\n\n\nData ....................................................................................................................................................................................... 29\n\n\nEvent Bus ..................................................................................................................................................................', '........... 30\n\n\nResiliency ............................................................................................................................................................................. 30\n\n\nDeploying eShopOnContainers to Azure .................................................................................................................... 30\n\n\nAzure Kubernetes Service ...........................................................', '.................................................................................. 30\n\n\nDeploying to Azure Kubernetes Service using Helm ......................................................................................... 30\n\n\nAzure Functions and Logic Apps (Serverless) ....................................................................................................... 32\n\n\nCentralized configuration .......................................', '........................................................................................................... 33\n\n\ni Contents\n\n\nAzure App Configuration .............................................................................................................................................. 33\n\n\nAzure Key Vault ........................................................................................................................................', '......................... 34\n\n\nConfiguration in eShop .................................................................................................................................................. 34\n\n\nReferences ........................................................................................................................................................................... 34\n\n\n**Scaling cloud-native applications ....................', '.................................................................... 36**\n\n\nLeveraging containers and orchestrators .................................................................................................................... 36\n\n\nChallenges with monolithic deployments .............................................................................................................. 36\n\n\nWhat are the benefits of containers and orchestrators? ..', '................................................................................ 38\n\n\nWhat are the scaling benefits? .................................................................................................................................... 40\n\n\nWhat scenarios are ideal for containers and orchestrators?........................................................................... 42\n\n\nWhen should you avoid using containers and orchestrators', '? ....................................................................... 42\n\n\nDevelopment resources ................................................................................................................................................. 42\n\n\nLeveraging serverless functions ...................................................................................................................................... 46\n\n\nWhat is serverless? ......', '..................................................................................................................................................... 47\n\n\nWhat challenges are solved by serverless? ............................................................................................................ 47\n\n\nWhat is the difference between a microservice and a serverless function? ............................................. 47\n\n\nWhat scenarios ', 'are appropriate for serverless? ................................................................................................... 47\n\n\nWhen should you avoid serverless? .......................................................................................................................... 48\n\n\nCombining containers and serverless approaches .................................................................................................. 49\n\n\n', 'When does it make sense to use containers with serverless? ........................................................................ 49\n\n\nWhen should you avoid using containers with Azure Functions? ................................................................ 49\n\n\nHow to combine serverless and Docker containers ........................................................................................... 49\n\n\nHow to combine serverless and Kuberne', 'tes with KEDA .................................................................................. 50\n\n\nDeploying containers in Azure ........................................................................................................................................ 50\n\n\nAzure Container Registry .............................................................................................................................................. 50\n\n\nACR', ' Tasks ............................................................................................................................................................................ 52\n\n\nAzure Kubernetes Service ............................................................................................................................................. 52\n\n\nAzure Bridge to Kubernetes ..................................................................', '....................................................................... 53\n\n\nScaling containers and serverless applications ......................................................................................................... 53\n\n\nThe simple solution: scaling up .................................................................................................................................. 53\n\n\nScaling out cloud-native apps .................', '................................................................................................................... 54\n\n\nOther container deployment options ........................................................................................................................... 55\n\n\nii Contents\n\n\nWhen does it make sense to deploy to App Service for Containers? ......................................................... 55\n\n\nHow to deploy to App Se', 'rvice for Containers ...................................................................................................... 55\n\n\nWhen does it make sense to deploy to Azure Container Instances? .......................................................... 55\n\n\nHow to deploy an app to Azure Container Instances ........................................................................................ 55\n\n\nReferences ......................................', '..................................................................................................................................... 56\n\n\n**Cloud-native communication patterns ............................................................................... 58**\n\n\nCommunication considerations ...................................................................................................................................... 58\n\n\nFront-end client ', 'communication .................................................................................................................................... 60\n\n\nSimple Gateways .............................................................................................................................................................. 62\n\n\nAzure Application Gateway ............................................................................................', '.............................................. 63\n\n\nAzure API Management ................................................................................................................................................. 63\n\n\nReal-time communication ............................................................................................................................................ 66\n\n\nService-to-service communication .......................', '......................................................................................................... 67\n\n\nQueries ................................................................................................................................................................................. 68\n\n\nCommands ...........................................................................................................................................', '............................... 71\n\n\nEvents .................................................................................................................................................................................... 74\n\n\ngRPC ........................................................................................................................................................................................... 80\n\n\nWhat is gRPC? ........', '........................................................................................................................................................... 80\n\n\ngRPC Benefits .................................................................................................................................................................... 80\n\n\nProtocol Buffers ........................................................................................', '........................................................................ 81\n\n\ngRPC support in .NET ..................................................................................................................................................... 81\n\n\ngRPC usage ......................................................................................................................................................................... 82\n\n\ngRPC imple', 'mentation .................................................................................................................................................... 83\n\n\nLooking ahead ................................................................................................................................................................... 85\n\n\nService Mesh communication infrastructure .............................................................', '................................................ 85\n\n\nSummary .............................................................................................................................................................................. 86\n\n\n**Cloud-native data patterns .................................................................................................. 88**\n\n\nDatabase-per-microservice, why? .........................................', '......................................................................................... 89\n\n\nCross-service queries ........................................................................................................................................................... 90\n\n\nDistributed transactions ...................................................................................................................................................', '.. 91\n\n\nHigh volume data ................................................................................................................................................................. 93\n\n\nCQRS ..................................................................................................................................................................................... 93\n\n\niii Contents\n\n\nEvent sourcing ....................................', '............................................................................................................................... 94\n\n\nRelational vs. NoSQL data ................................................................................................................................................. 96\n\n\nThe CAP theorem ............................................................................................................................', '................................. 97\n\n\nConsiderations for relational vs. NoSQL systems ................................................................................................ 99\n\n\nDatabase as a Service ..................................................................................................................................................... 99\n\n\nAzure relational databases .........................................................', '................................................................................ 100\n\n\nAzure SQL Database...................................................................................................................................................... 100\n\n\nOpen-source databases in Azure ............................................................................................................................. 101\n\n\nNoSQL data in Azure .....', '............................................................................................................................................... 102\n\n\nNewSQL databases ........................................................................................................................................................ 106\n\n\nData migration to the cloud ................................................................................................', '...................................... 108\n\n\nCaching in a cloud-native app ....................................................................................................................................... 108\n\n\nWhy? .................................................................................................................................................................................... 108\n\n\nCaching architecture ....................', '................................................................................................................................. 109\n\n\nAzure Cache for Redis .................................................................................................................................................. 110\n\n\nElasticsearch in a cloud-native app .......................................................................................................', '...................... 110\n\n\nSummary ............................................................................................................................................................................ 111\n\n\n**Cloud-native resiliency ....................................................................................................... 113**\n\n\nApplication resiliency patterns ................................................................', '...................................................................... 114\n\n\nCircuit breaker pattern ................................................................................................................................................. 116\n\n\nTesting for resiliency ..................................................................................................................................................... 117\n\n\nAzure platform res', 'iliency ................................................................................................................................................. 117\n\n\nDesign with resiliency ................................................................................................................................................... 118\n\n\nDesign with redundancy...........................................................................................', '................................................... 118\n\n\nDesign for scalability ..................................................................................................................................................... 120\n\n\nBuilt-in retry in services ............................................................................................................................................... 121\n\n\nResilient communications ...........', '..................................................................................................................................... 122\n\n\nService mesh .................................................................................................................................................................... 123\n\n\nIstio and Envoy ..............................................................................................................', '.................................................. 124\n\n\nIntegration with Azure Kubernetes Services ....................................................................................................... 125\n\n\n**Monitoring and health ........................................................................................................ 126**\n\n\nObservability patterns ................................................................................', '....................................................................... 126\n\n\niv Contents\n\n\nWhen to use logging .................................................................................................................................................... 126\n\n\nChallenges with detecting and responding to potential app health issues ........................................... 130\n\n\nChallenges with reacting to critical problems in cloud-native', ' apps .......................................................... 130\n\n\nLogging with Elastic Stack ............................................................................................................................................... 131\n\n\nElastic Stack ...................................................................................................................................................................... 131\n\n\nWhat are the ad', 'vantages of Elastic Stack? .......................................................................................................... 132\n\n\nLogstash ............................................................................................................................................................................. 132\n\n\nElasticsearch ...........................................................................................................', '.......................................................... 133\n\n\nVisualizing information with Kibana web dashboards .................................................................................... 133\n\n\nInstalling Elastic Stack on Azure ............................................................................................................................... 134\n\n\nReferences ................................................................', '......................................................................................................... 134\n\n\nMonitoring in Azure Kubernetes Services ................................................................................................................. 134\n\n\nAzure Monitor for Containers ................................................................................................................................... 134\n\n\nLog.Finaliz', 'e() .................................................................................................................................................................... 136\n\n\nAzure Monitor ...................................................................................................................................................................... 136\n\n\nGathering logs and metrics .............................................................', '........................................................................... 137\n\n\nReporting data ................................................................................................................................................................ 137\n\n\nDashboards ....................................................................................................................................................................... 138\n\n\nA', 'lerts ................................................................................................................................................................................... 140\n\n\nReferences ......................................................................................................................................................................... 141\n\n\n**Cloud-native identity ...............................................', '........................................................... 142**\n\n\nReferences ............................................................................................................................................................................. 142\n\n\nAuthentication and authorization in cloud-native apps ..................................................................................... 142\n\n\nReferences ..................................', '....................................................................................................................................... 143\n\n\nAzure Active Directory ...................................................................................................................................................... 143\n\n\nReferences .....................................................................................................................', '.................................................... 143\n\n\nIdentityServer for cloud-native applications ............................................................................................................ 144\n\n\nCommon web app scenarios ..................................................................................................................................... 144\n\n\nGetting started ..................................................', '.............................................................................................................. 145\n\n\nConfiguration ................................................................................................................................................................... 145\n\n\nJavaScript clients ..................................................................................................................................', '.......................... 146\n\n\nReferences ......................................................................................................................................................................... 146\n\n\nv Contents\n\n\n**Cloud-native security .......................................................................................................... 147**\n\n\nAzure security for cloud-native apps .........................................', '................................................................................. 147\n\n\nThreat modeling ............................................................................................................................................................. 148\n\n\nPrinciple of least privilege ........................................................................................................................................... 148\n\n\nPenetra', 'tion testing ........................................................................................................................................................ 149\n\n\nMonitoring ........................................................................................................................................................................ 149\n\n\nSecuring the build .........................................................................', '................................................................................. 149\n\n\nBuilding secure code .................................................................................................................................................... 150\n\n\nBuilt-in security ............................................................................................................................................................... 150\n\n\nAz', 'ure network infrastructure ..................................................................................................................................... 150\n\n\nRole-based access control for restricting access to Azure resources........................................................ 152\n\n\nSecurity Principals .....................................................................................................................................', '..................... 152\n\n\nRoles .................................................................................................................................................................................... 153\n\n\nScopes ................................................................................................................................................................................ 154\n\n\nDeny ..................................', '.................................................................................................................................................. 154\n\n\nChecking access .............................................................................................................................................................. 154\n\n\nSecuring secrets ...................................................................................................', '........................................................... 155\n\n\nAzure Key Vault ............................................................................................................................................................... 155\n\n\nKubernetes ........................................................................................................................................................................ 155\n\n\nEncryption in tr', 'ansit and at rest ............................................................................................................................... 156\n\n\nKeeping secure ................................................................................................................................................................ 160\n\n\n**DevOps ...........................................................................................................', '...................... 161**\n\n\nAzure DevOps ....................................................................................................................................................................... 162\n\n\nGitHub Actions ..................................................................................................................................................................... 163\n\n\nSource control ..............................', '........................................................................................................................................ 163\n\n\nRepository per microservice ...................................................................................................................................... 164\n\n\nSingle repository ........................................................................................................................', '.................................... 166\n\n\nStandard directory structure ...................................................................................................................................... 167\n\n\nTask management .............................................................................................................................................................. 167\n\n\nCI/CD pipelines ........................................', '............................................................................................................................ 169\n\n\nAzure Builds ...................................................................................................................................................................... 170\n\n\nAzure DevOps releases ...............................................................................................................', '................................. 172\n\n\nvi Contents\n\n\nEverybody gets a build pipeline ............................................................................................................................... 173\n\n\nVersioning releases ........................................................................................................................................................ 173\n\n\nFeature flags .....................................', '.................................................................................................................................... 173\n\n\nImplementing feature flags ........................................................................................................................................ 174\n\n\nInfrastructure as code ......................................................................................................................', '................................. 175\n\n\nAzure Resource Manager templates ...................................................................................................................... 175\n\n\nTerraform ........................................................................................................................................................................... 176\n\n\nAzure CLI Scripts and Tasks.....................................', '................................................................................................... 177\n\n\nCloud Native Application Bundles ............................................................................................................................... 178\n\n\nDevOps Decisions .......................................................................................................................................................... 180\n\n', '\nReferences ......................................................................................................................................................................... 180\n\n\n**Summary: Architecting cloud-native apps ....................................................................... 181**\n\n\nvii Contents\n\n\n**CHAPTER**\n# 1\n\n## Introduction to cloud- native applications\n\n\nAnother day, at the office, working on “the next big thing.”', '\n\n\nYour cellphone rings. It’s your friendly recruiter - the one who calls daily with exciting new\nopportunities.\n\n\nBut this time it’s different: Start-up, equity, and plenty of funding.\n\n\nThe mention of the cloud, microservices, and cutting-edge technology pushes you over the edge.\n\n\nFast forward a few weeks and you’re now a new employee in a design session architecting a major\neCommerce application. You’re going to compete with the leading eComm', 'erce sites.\n\n\nHow will you build it?\n\n\nIf you follow the guidance from past 15 years, you’ll most likely build the system shown in Figure 1.1.\n\n\n_Figure 1-1. Traditional monolithic design_\n\n\nYou construct a large core application containing all of your domain logic. It includes modules such as\nIdentity, Catalog, Ordering, and more. They directly communicate with each other within a single\nserver process. The modules share a large relational datab', 'ase. The core exposes functionality via an\nHTML interface and a mobile app.\n\n\nCongratulations! You just created a monolithic application.\n\n\nNot all is bad. Monoliths offer some distinct advantages. For example, they’re straightforward to…\n\n\n1 CHAPTER 1 | Introduction to cloud-native applications\n\n\n  - build\n\n  - test\n\n  - deploy\n\n  - troubleshoot\n\n  - vertically scale\n\n\nMany successful apps that exist today were created as monoliths. The app is a', ' hit and continues to\nevolve, iteration after iteration, adding more functionality.\n\n\nAt some point, however, you begin to feel uncomfortable. You find yourself losing control of the\napplication. As time goes on, the feeling becomes more intense, and you eventually enter a state\nknown as the Fear Cycle:\n\n\n  - The app has become so overwhelmingly complicated that no single person understands it.\n\n  - You fear making changes - each change has unint', 'ended and costly side effects.\n\n  - New features/fixes become tricky, time-consuming, and expensive to implement.\n\n  - Each release becomes as small as possible and requires a full deployment of the entire\napplication.\n\n  - One unstable component can crash the entire system.\n\n  - New technologies and frameworks aren’t an option.\n\n  - It’s difficult to implement agile delivery methodologies.\n\n  - Architectural erosion sets in as the code base dete', 'riorates with never-ending “quick fixes.”\n\n  - Finally, the _consultants_ come in and tell you to rewrite it.\n\n\nSound familiar?\n\n\nMany organizations have addressed this monolithic fear cycle by adopting a cloud-native approach to\nbuilding systems. Figure 1-2 shows the same system built applying cloud-native techniques and\npractices.\n\n\n2 CHAPTER 1 | Introduction to cloud-native applications\n\n\n_Figure 1-2. Cloud-native design_\n\n\nNote how the applic', 'ation is decomposed across a set of small isolated microservices. Each service is\nself-contained and encapsulates its own code, data, and dependencies. Each is deployed in a software\ncontainer and managed by a container orchestrator. Instead of a large relational database, each\nservice owns it own datastore, the type of which vary based upon the data needs. Note how some\nservices depend on a relational database, but other on NoSQL databases. One ', 'service stores its state\nin a distributed cache. Note how all traffic routes through an API Gateway service that is responsible\nfor routing traffic to the core back-end services and enforcing many cross-cutting concerns. Most\nimportantly, the application takes full advantage of the scalability, availability, and resiliency features\nfound in modern cloud platforms.\n\n#### **Cloud-native computing**\n\n\nHmm… We just used the term, _Cloud Native_ . You', 'r first thought might be, “What exactly does that\nmean?” Another industry buzzword concocted by software vendors to market more stuff?”\n\n\nFortunately it’s far different, and hopefully this book will help convince you.\n\n\nWithin a short time, cloud native has become a driving trend in the software industry. It’s a new way\nto construct large, complex systems. The approach takes full advantage of modern software\ndevelopment practices, technologies, a', 'nd cloud infrastructure. Cloud native changes the way you\ndesign, implement, deploy, and operationalize systems.\n\n\nUnlike the continuous hype that drives our industry, cloud native is _for-real_ [. Consider the Cloud Native](https://www.cncf.io/)\n[Computing Foundation](https://www.cncf.io/) (CNCF), a consortium of over 400 major corporations. Its charter is to make\n\n\n3 CHAPTER 1 | Introduction to cloud-native applications\n\n\ncloud-native computing', ' ubiquitous across technology and cloud stacks. As one of the most influential\nopen-source groups, it hosts many of the fastest-growing open source-projects in GitHub. These\n[projects include Kubernetes, Prometheus,](https://kubernetes.io/) [Helm, Envoy, and gRPC.](https://helm.sh/)\n\n\nThe CNCF fosters an ecosystem of open-source and vendor-neutrality. Following that lead, this book\npresents cloud-native principles, patterns, and best practices th', 'at are technology agnostic. At the\nsame time, we discuss the services and infrastructure available in the Microsoft Azure cloud for\nconstructing cloud-native systems.\n\n\nSo, what exactly is Cloud Native? Sit back, relax, and let us help you explore this new world.\n\n### What is Cloud Native?\n\n\nStop what you’re doing and ask your colleagues to define the term “Cloud Native”. There’s a good\nchance you’ll get several different answers.\n\n\nLet’s start w', 'ith a simple definition:\n\n\n_Cloud-native architecture and technologies are an approach to designing, constructing, and operating_\n_workloads that are built in the cloud and take full advantage of the cloud computing model._\n\n\n[The Cloud Native Computing Foundation provides the official definition:](https://www.cncf.io/)\n\n\n_Cloud-native technologies empower organizations to build and run scalable applications in modern,_\n_dynamic environments such', ' as public, private, and hybrid clouds. Containers, service meshes,_\n_microservices, immutable infrastructure, and declarative APIs exemplify this approach._\n\n\n_These techniques enable loosely coupled systems that are resilient, manageable, and observable._\n_Combined with robust automation, they allow engineers to make high-impact changes frequently and_\n_predictably with minimal toil._\n\n\nCloud native is about _speed_ and _agility_ . Business sys', 'tems are evolving from enabling business\ncapabilities to weapons of strategic transformation that accelerate business velocity and growth. It’s\nimperative to get new ideas to market immediately.\n\n\nAt the same time, business systems have also become increasingly complex with users demanding\nmore. They expect rapid responsiveness, innovative features, and zero downtime. Performance\nproblems, recurring errors, and the inability to move fast are no l', 'onger acceptable. Your users will visit\nyour competitor. Cloud-native systems are designed to embrace rapid change, large scale, and\nresilience.\n\n\nHere are some companies who have implemented cloud-native techniques. Think about the speed,\nagility, and scalability they’ve achieved.\n\n|Company|Experience|\n|---|---|\n|Netflix|Has 600+ services in production. Deploys 100<br>times per day.|\n|Uber|Has 1,000+ services in production. Deploys<br>several th', 'ousand times each week.|\n\n\n\n4 CHAPTER 1 | Introduction to cloud-native applications\n\n\n|Company|Experience|\n|---|---|\n|WeChat|Has 3,000+ services in production. Deploys<br>1,000 times a day.|\n\n\nAs you can see, Netflix, Uber, and, WeChat expose cloud-native systems that consist of many\nindependent services. This architectural style enables them to rapidly respond to market conditions.\nThey instantaneously update small areas of a live, complex appli', 'cation, without a full redeployment.\nThey individually scale services as needed.\n\n#### **The pillars of cloud native**\n\n\nThe speed and agility of cloud native derive from many factors. Foremost is _cloud infrastructure_ . But\nthere’s more: Five other foundational pillars shown in Figure 1-3 also provide the bedrock for cloudnative systems.\n\n\n_Figure 1-3. Cloud-native foundational pillars_\n\n\nLet’s take some time to better understand the significan', 'ce of each pillar.\n\n#### **The cloud**\n\n\nCloud-native systems take full advantage of the cloud service model.\n\n\nDesigned to thrive in a dynamic, virtualized cloud environment, these systems make extensive use of\n[Platform as a Service (PaaS) compute infrastructure and managed services. They treat the underlying](https://azure.microsoft.com/overview/what-is-paas/)\ninfrastructure as _disposable_ - provisioned in minutes and resized, scaled, or dest', 'royed on demand –\nvia automation.\n\n\n[Consider the widely accepted DevOps concept of Pets vs. Cattle. In a traditional data center, servers](https://medium.com/@Joachim8675309/devops-concepts-pets-vs-cattle-2380b5aab313)\nare treated as _Pets_ : a physical machine, given a meaningful name, and _cared_ for. You scale by adding\nmore resources to the same machine (scaling up). If the server becomes sick, you nurse it back to\nhealth. Should the server ', 'become unavailable, everyone notices.\n\n\nThe _Cattle_ service model is different. You provision each instance as a virtual machine or container.\nThey’re identical and assigned a system identifier such as Service-01, Service-02, and so on. You scale\nby creating more of them (scaling out). When one becomes unavailable, nobody notices.\n\n\n5 CHAPTER 1 | Introduction to cloud-native applications\n\n\nThe cattle model embraces _immutable infrastructure_ . S', 'ervers aren’t repaired or modified. If one fails or\nrequires updating, it’s destroyed and a new one is provisioned – all done via automation.\n\n\nCloud-native systems embrace the Cattle service model. They continue to run as the infrastructure\nscales in or out with no regard to the machines upon which they’re running.\n\n\nThe Azure cloud platform supports this type of highly elastic infrastructure with automatic scaling,\nself-healing, and monitoring ', 'capabilities.\n\n#### **Modern design**\n\n\nHow would you design a cloud-native app? What would your architecture look like? To what\nprinciples, patterns, and best practices would you adhere? What infrastructure and operational\nconcerns would be important?\n\n\n**The Twelve-Factor Application**\n\n\n[A widely accepted methodology for constructing cloud-based applications is the Twelve-Factor](https://12factor.net/)\n[Application. It describes a set of princ', 'iples and practices that developers follow to construct](https://12factor.net/)\napplications optimized for modern cloud environments. Special attention is given to portability across\nenvironments and declarative automation.\n\n\nWhile applicable to any web-based application, many practitioners consider Twelve-Factor a solid\nfoundation for building cloud-native apps. Systems built upon these principles can deploy and scale\nrapidly and add features to', ' react quickly to market changes.\n\n\nThe following table highlights the Twelve-Factor methodology:\n\n|Factor|Explanation|\n|---|---|\n|1 - Code Base|A single code base for each microservice, stored<br>in its own repository. Tracked with version<br>control, it can deploy to multiple environments<br>(QA, Staging, Production).|\n|2 - Dependencies|Each microservice isolates and packages its own<br>dependencies, embracing changes without<br>impacting the e', 'ntire system.|\n|3 - Configurations|Configuration information is moved out of the<br>microservice and externalized through a<br>configuration management tool outside of the<br>code. The same deployment can propagate<br>across environments with the correct<br>configuration applied.|\n|4 - Backing Services|Ancillary resources (data stores, caches,<br>message brokers) should be exposed via an<br>addressable URL. Doing so decouples the<br>resource from', ' the application, enabling it to be<br>interchangeable.|\n\n\n\n6 CHAPTER 1 | Introduction to cloud-native applications\n\n\n|Factor|Explanation|\n|---|---|\n|5 - Build, Release, Run|Each release must enforce a strict separation<br>across the build, release, and run stages. Each<br>should be tagged with a unique ID and support<br>the ability to roll back. Modern CI/CD systems<br>help fulfill this principle.|\n|6 - Processes|Each microservice should execute', ' in its own<br>process, isolated from other running services.<br>Externalize required state to a backing service<br>such as a distributed cache or data store.|\n|7 - Port Binding|Each microservice should be self-contained with<br>its interfaces and functionality exposed on its<br>own port. Doing so provides isolation from<br>other microservices.|\n|8 - Concurrency|When capacity needs to increase, scale out<br>services horizontally across multiple i', 'dentical<br>processes (copies) as opposed to scaling-up a<br>single large instance on the most powerful<br>machine available. Develop the application to be<br>concurrent making scaling out in cloud<br>environments seamless.|\n|9 - Disposability|Service instances should be disposable. Favor<br>fast startup to increase scalability opportunities<br>and graceful shutdowns to leave the system in a<br>correct state. Docker containers along with an<br>or', 'chestrator inherently satisfy this requirement.|\n|10 - Dev/Prod Parity|Keep environments across the application<br>lifecycle as similar as possible, avoiding costly<br>shortcuts. Here, the adoption of containers can<br>greatly contribute by promoting the same<br>execution environment.|\n|11 - Logging|Treat logs generated by microservices as event<br>streams. Process them with an event<br>aggregator. Propagate log data to data-<br>mining/log manage', 'ment tools like Azure<br>Monitor or Splunk and eventually to long-term<br>archival.|\n|12 - Admin Processes|Run administrative/management tasks, such as<br>data cleanup or computing analytics, as one-off<br>processes. Use independent tools to invoke<br>these tasks from the production environment,<br>but separately from the application.|\n\n\n7 CHAPTER 1 | Introduction to cloud-native applications\n\n\n[In the book, Beyond the Twelve-Factor App, author K', 'evin Hoffman details each of the original 12](https://content.pivotal.io/blog/beyond-the-twelve-factor-app)\nfactors (written in 2011). Additionally, he discusses three extra factors that reflect today’s modern\ncloud application design.\n\n|New Factor|Explanation|\n|---|---|\n|13 - API First|Make everything a service. Assume your code<br>will be consumed by a front-end client, gateway,<br>or another service.|\n|14 - Telemetry|On a workstation, you have', ' deep visibility into<br>your application and its behavior. In the cloud,<br>you don’t. Make sure your design includes the<br>collection of monitoring, domain-specific, and<br>health/system data.|\n|15 - Authentication/ Authorization|Implement identity from the start. Consider<br>RBAC (role-based access control) features<br>available in public clouds.|\n\n\n\nWe’ll refer to many of the 12+ factors in this chapter and throughout the book.\n\n\n**Azure Wel', 'l-Architected Framework**\n\n\nDesigning and deploying cloud-based workloads can be challenging, especially when implementing\ncloud-native architecture. Microsoft provides industry standard best practices to help you and your\nteam deliver robust cloud solutions.\n\n\n[The Microsoft Well-Architected Framework provides a set of guiding tenets that can be used to](https://docs.microsoft.com/azure/architecture/framework/)\nimprove the quality of a cloud-nat', 'ive workload. The framework consists of five pillars of architecture\nexcellence:\n\n|Tenets|Description|\n|---|---|\n|Cost management|Focus on generating incremental value early.<br>Apply_Build-Measure-Learn_ principles to<br>accelerate time to market while avoiding<br>capital-intensive solutions. Using a pay-as-you-<br>go strategy, invest as you scale out, rather than<br>delivering a large investment up front.|\n|Operational excellence|Automate the e', 'nvironment and operations to<br>increase speed and reduce human error. Roll<br>problem updates back or forward quickly.<br>Implement monitoring and diagnostics from the<br>start.|\n|Performance efficiency|Efficiently meet demands placed on your<br>workloads. Favor horizontal scaling (scaling out)<br>and design it into your systems. Continually<br>conduct performance and load testing to<br>identify potential bottlenecks.|\n\n\n\n8 CHAPTER 1 | Introduct', 'ion to cloud-native applications\n\n\n|Tenets|Description|\n|---|---|\n|Reliability|Build workloads that are both resilient and<br>available. Resiliency enables workloads to<br>recover from failures and continue functioning.<br>Availability ensures users access to your<br>workload at all times. Design applications to<br>expect failures and recover from them.|\n|Security|Implement security across the entire lifecycle of<br>an application, from design an', 'd implementation<br>to deployment and operations. Pay close<br>attention to identity management, infrastructure<br>access, application security, and data<br>sovereignty and encryption.|\n\n\n[To get started, Microsoft provides a set of online assessments](https://docs.microsoft.com/assessments/?mode=pre-assessment&session=local) to help you assess your current cloud\nworkloads against the five well-architected pillars.\n\n#### **Microservices**\n\n\nCloud', '-native systems embrace microservices, a popular architectural style for constructing modern\napplications.\n\n\nBuilt as a distributed set of small, independent services that interact through a shared fabric,\nmicroservices share the following characteristics:\n\n\n  - Each implements a specific business capability within a larger domain context.\n\n\n  - Each is developed autonomously and can be deployed independently.\n\n\n  - Each is self-contained encapsu', 'lating its own data storage technology, dependencies, and\nprogramming platform.\n\n\n  - Each runs in its own process and communicates with others using standard communication\n[protocols such as HTTP/HTTPS, gRPC, WebSockets, or AMQP.](https://en.wikipedia.org/wiki/Advanced_Message_Queuing_Protocol)\n\n\n  - They compose together to form an application.\n\n\nFigure 1-4 contrasts a monolithic application approach with a microservices approach. Note how the\n', 'monolith is composed of a layered architecture, which executes in a single process. It typically\nconsumes a relational database. The microservice approach, however, segregates functionality into\nindependent services, each with its own logic, state, and data. Each microservice hosts its own\ndatastore.\n\n\n9 CHAPTER 1 | Introduction to cloud-native applications\n\n\n_Figure 1-4. Monolithic versus microservices architecture_\n\n\nNote how microservices prom', 'ote the **Processes** [principle from the Twelve-Factor Application,](https://12factor.net/)\ndiscussed earlier in the chapter.\n\n\n_Factor #6 specifies “Each microservice should execute in its own process, isolated from other running_\n_services.”_\n\n\n**Why microservices?**\n\n\nMicroservices provide agility.\n\n\nEarlier in the chapter, we compared an eCommerce application built as a monolith to that with\nmicroservices. In the example, we saw some clear b', 'enefits:\n\n\n  - Each microservice has an autonomous lifecycle and can evolve independently and deploy\nfrequently. You don’t have to wait for a quarterly release to deploy a new feature or update.\nYou can update a small area of a live application with less risk of disrupting the entire system.\nThe update can be made without a full redeployment of the application.\n\n\n  - Each microservice can scale independently. Instead of scaling the entire applica', 'tion as a single\nunit, you scale out only those services that require more processing power to meet desired\nperformance levels and service-level agreements. Fine-grained scaling provides for greater\ncontrol of your system and helps reduce overall costs as you scale portions of your system,\nnot everything.\n\n\n[An excellent reference guide for understanding microservices is .NET Microservices: Architecture for](https://dotnet.microsoft.com/download/', 'thank-you/microservices-architecture-ebook)\n[Containerized .NET Applications. The book deep dives into microservices design and architecture. It’s](https://dotnet.microsoft.com/download/thank-you/microservices-architecture-ebook)\n[a companion for a full-stack microservice reference architecture](https://github.com/dotnet-architecture/eShopOnContainers) available as a free download from\nMicrosoft.\n\n\n**Developing microservices**\n\n\nMicroservices can', ' be created upon any modern development platform.\n\n\n10 CHAPTER 1 | Introduction to cloud-native applications\n\n\nThe Microsoft .NET platform is an excellent choice. Free and open source, it has many built-in features\nthat simplify microservice development. .NET is cross-platform. Applications can be built and run on\nWindows, macOS, and most flavors of Linux.\n\n\n.NET is highly performant and has scored well in comparison to Node.js and other competin', 'g\n[platforms. Interestingly, TechEmpower conducted an extensive set of performance benchmarks across](https://www.techempower.com/)\nmany web application platforms and frameworks. .NET scored in the top 10 - well above Node.js and\nother competing platforms.\n\n\n[.NET](https://github.com/dotnet/core) is maintained by Microsoft and the .NET community on GitHub.\n\n\n**Microservice challenges**\n\n\nWhile distributed cloud-native microservices can provide im', 'mense agility and speed, they present\nmany challenges:\n\n\n_**Communication**_\n\nHow will front-end client applications communicate with backed-end core microservices? Will you\nallow direct communication? Or, might you abstract the back-end microservices with a gateway\nfacade that provides flexibility, control, and security?\n\n\nHow will back-end core microservices communicate with each other? Will you allow direct HTTP calls\nthat can increase couplin', 'g and impact performance and agility? Or might you consider decoupled\nmessaging with queue and topic technologies?\n\n\nCommunication is covered in the Cloud-native communication patterns chapter.\n\n\n_**Resiliency**_\n\n\nA microservices architecture moves your system from in-process to out-of-process network\ncommunication. In a distributed architecture, what happens when Service B isn’t responding to a\nnetwork call from Service A? Or, what happens when', ' Service C becomes temporarily unavailable and\nother services calling it become blocked?\n\n\nResiliency is covered in the Cloud-native resiliency chapter.\n\n\n_**Distributed Data**_\n\n\nBy design, each microservice encapsulates its own data, exposing operations via its public interface. If\nso, how do you query data or implement a transaction across multiple services?\n\n\nDistributed data is covered in the Cloud-native data patterns chapter.\n\n\n_**Secrets*', '*_\n\n\nHow will your microservices securely store and manage secrets and sensitive configuration data?\n\n\nSecrets are covered in detail Cloud-native security.\n\n\n11 CHAPTER 1 | Introduction to cloud-native applications\n\n\n**Manage Complexity with Dapr**\n\n\n[Dapr](https://dapr.io/) is a distributed, open-source application runtime. Through an architecture of pluggable\ncomponents, it dramatically simplifies the _plumbing_ behind distributed applications.', ' It provides a\n**dynamic glue** that binds your application with pre-built infrastructure capabilities and components\nfrom the Dapr runtime. Figure 1-5 shows Dapr from 20,000 feet.\n\n\n_Figure 1-5. Dapr at 20,000 feet._\n\n\n[In the top row of the figure, note how Dapr provides language-specific SDKs for popular development](https://docs.dapr.io/developing-applications/sdks/)\nplatforms. Dapr v1 includes support for .NET, Go, Node.js, Python, PHP, Java', ', and JavaScript.\n\n\nWhile language-specific SDKs enhance the developer experience, Dapr is platform agnostic. Under the\nhood, Dapr’s programming model exposes capabilities through standard HTTP/gRPC communication\nprotocols. Any programming platform can call Dapr via its native HTTP and gRPC APIs.\n\n\nThe blue boxes across the center of the figure represent the Dapr building blocks. Each exposes prebuilt plumbing code for a distributed application c', 'apability that your application can consume.\n\n\nThe components row represents a large set of pre-defined infrastructure components that your\napplication can consume. Think of components as infrastructure code you don’t have to write.\n\n\nThe bottom row highlights the portability of Dapr and the diverse environments across which it can\nrun.\n\n\n[Microsoft features a free ebook Dapr for .NET Developers](https://docs.microsoft.com/dotnet/architecture/dap', 'r-for-net-developers/) for learning Dapr.\n\n\nLooking ahead, Dapr has the potential to have a profound impact on cloud-native application\ndevelopment.\n\n#### **Containers**\n\n\nIt’s natural to hear the term _container_ mentioned in any _cloud native_ [conversation. In the book, Cloud](https://www.manning.com/books/cloud-native-patterns)\n[Native Patterns, author Cornelia Davis observes that, “Containers are a great enabler of cloud-native](https://www.', 'manning.com/books/cloud-native-patterns)\n\n\n12 CHAPTER 1 | Introduction to cloud-native applications\n\n\nsoftware.” The Cloud Native Computing Foundation places microservice containerization as the first\n[step in their Cloud-Native Trail Map - guidance for enterprises beginning their cloud-native journey.](https://raw.githubusercontent.com/cncf/trailmap/master/CNCF_TrailMap_latest.png)\n\n\nContainerizing a microservice is simple and straightforward. T', 'he code, its dependencies, and runtime\n[are packaged into a binary called a container image. Images are stored in a container registry, which](https://docs.docker.com/glossary/?term=image)\nacts as a repository or library for images. A registry can be located on your development computer, in\n[your data center, or in a public cloud. Docker itself maintains a public registry via Docker Hub. The](https://hub.docker.com/)\n[Azure cloud features a priva', 'te container registry](https://azure.microsoft.com/services/container-registry/) to store container images close to the cloud\napplications that will run them.\n\n\nWhen an application starts or scales, you transform the container image into a running container\n[instance. The instance runs on any computer that has a container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes/) engine installed. You can\nhave as many i', 'nstances of the containerized service as needed.\n\n\nFigure 1-6 shows three different microservices, each in its own container, all running on a single host.\n\n\n_Figure 1-6. Multiple containers running on a container host_\n\n\nNote how each container maintains its own set of dependencies and runtime, which can be different\nfrom one another. Here, we see different versions of the Product microservice running on the same\nhost. Each container shares a sl', 'ice of the underlying host operating system, memory, and processor,\nbut is isolated from one another.\n\n\nNote how well the container model embraces the **Dependencies** [principle from the Twelve-Factor](https://12factor.net/)\n[Application.](https://12factor.net/)\n\n\n_Factor #2 specifies that “Each microservice isolates and packages its own dependencies, embracing_\n_changes without impacting the entire system.”_\n\n\nContainers support both Linux and ', 'Windows workloads. The Azure cloud openly embraces both.\nInterestingly, it’s Linux, not Windows Server, that has become the more popular operating system in\nAzure.\n\n\n13 CHAPTER 1 | Introduction to cloud-native applications\n\n\n[While several container vendors exist, Docker](https://www.docker.com/) has captured the lion’s share of the market. The\ncompany has been driving the software container movement. It has become the de facto standard for\npacka', 'ging, deploying, and running cloud-native applications.\n\n\n**Why containers?**\n\n\nContainers provide portability and guarantee consistency across environments. By encapsulating\neverything into a single package, you _isolate_ the microservice and its dependencies from the\nunderlying infrastructure.\n\n\nYou can deploy the container in any environment that hosts the Docker runtime engine. Containerized\nworkloads also eliminate the expense of pre-configu', 'ring each environment with frameworks, software\nlibraries, and runtime engines.\n\n\nBy sharing the underlying operating system and host resources, a container has a much smaller\nfootprint than a full virtual machine. The smaller size increases the _density_, or number of\nmicroservices, that a given host can run at one time.\n\n\n**Container orchestration**\n\n\nWhile tools such as Docker create images and run containers, you also need tools to manage the', 'm.\nContainer management is done with a special software program called a **container orchestrator** .\nWhen operating at scale with many independent running containers, orchestration is essential.\n\n\nFigure 1-7 shows management tasks that container orchestrators automate.\n\n\n_Figure 1-7. What container orchestrators do_\n\n\nThe following table describes common orchestration tasks.\n\n|Tasks|Explanation|\n|---|---|\n|Scheduling|Automatically provision cont', 'ainer instances.|\n|Affinity/anti-affinity|Provision containers nearby or far apart from<br>each other, helping availability and<br>performance.|\n|Health monitoring|Automatically detect and correct failures.|\n|Failover|Automatically reprovision a failed instance to a<br>healthy machine.|\n\n\n\n14 CHAPTER 1 | Introduction to cloud-native applications\n\n\n|Tasks|Explanation|\n|---|---|\n|Scaling|Automatically add or remove a container<br>instance to meet d', 'emand.|\n|Networking|Manage a networking overlay for container<br>communication.|\n|Service Discovery|Enable containers to locate each other.|\n|Rolling Upgrades|Coordinate incremental upgrades with zero<br>downtime deployment. Automatically roll back<br>problematic changes.|\n\n\nNote how container orchestrators embrace the **Disposability** and **Concurrency** principles from the\n[Twelve-Factor Application.](https://12factor.net/)\n\n\n_Factor #9 specif', 'ies that “Service instances should be disposable, favoring fast startups to increase_\n_scalability opportunities and graceful shutdowns to leave the system in a correct state.”_ Docker\ncontainers along with an orchestrator inherently satisfy this requirement.”\n\n\n_Factor #8 specifies that “Services scale out across a large number of small identical processes (copies) as_\n_opposed to scaling-up a single large instance on the most powerful machine a', 'vailable.”_\n\n\n[While several container orchestrators exist, Kubernetes](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/) has become the de facto standard for the\ncloud-native world. It’s a portable, extensible, open-source platform for managing containerized\nworkloads.\n\n\nYou could host your own instance of Kubernetes, but then you’d be responsible for provisioning and\nmanaging its resources - which can be complex. The Azure cloud', ' features Kubernetes as a managed\n[service. Both Azure Kubernetes Service (AKS) and Azure Red Hat OpenShift (ARO)](https://azure.microsoft.com/services/kubernetes-service/) enable you to fully\nleverage the features and power of Kubernetes as a managed service, without having to install and\nmaintain it.\n\n\nContainer orchestration is covered in detail in Scaling Cloud-Native Applications.\n\n#### **Backing services**\n\n\nCloud-native systems depend upon', ' many different ancillary resources, such as data stores, message\n[brokers, monitoring, and identity services. These services are known as backing services.](https://12factor.net/backing-services)\n\n\nFigure 1-8 shows many common backing services that cloud-native systems consume.\n\n\n15 CHAPTER 1 | Introduction to cloud-native applications\n\n\n_Figure 1-8. Common backing services_\n\n\nYou could host your own backing services, but then you’d be responsib', 'le for licensing, provisioning,\nand managing those resources.\n\n\nCloud providers offer a rich assortment of _managed backing services._ Instead of owning the service,\nyou simply consume it. The cloud provider operates the resource at scale and bears the responsibility\nfor performance, security, and maintenance. Monitoring, redundancy, and availability are built into the\nservice. Providers guarantee service level performance and fully support their', ' managed services open a ticket and they fix your issue.\n\n\nCloud-native systems favor managed backing services from cloud vendors. The savings in time and\nlabor can be significant. The operational risk of hosting your own and experiencing trouble can get\nexpensive fast.\n\n\nA best practice is to treat a backing service as an **attached resource**, dynamically bound to a\nmicroservice with configuration information (a URL and credentials) stored in a', 'n external\n[configuration. This guidance is spelled out in the Twelve-Factor Application, discussed earlier in the](https://12factor.net/)\nchapter.\n\n\n_Factor #4_ specifies that backing services “should be exposed via an addressable URL. Doing so\ndecouples the resource from the application, enabling it to be interchangeable.”\n\n\n_Factor #3_ specifies that “Configuration information is moved out of the microservice and externalized\nthrough a configu', 'ration management tool outside of the code.”\n\n\nWith this pattern, a backing service can be attached and detached without code changes. You might\npromote a microservice from QA to a staging environment. You update the microservice\nconfiguration to point to the backing services in staging and inject the settings into your container\nthrough an environment variable.\n\n\n16 CHAPTER 1 | Introduction to cloud-native applications\n\n\nCloud vendors provide AP', 'Is for you to communicate with their proprietary backing services. These\nlibraries encapsulate the proprietary plumbing and complexity. However, communicating directly with\nthese APIs will tightly couple your code to that specific backing service. It’s a widely accepted practice\nto insulate the implementation details of the vendor API. Introduce an intermediation layer, or\nintermediate API, exposing generic operations to your service code and wra', 'p the vendor code inside\nit. This loose coupling enables you to swap out one backing service for another or move your code to\na different cloud environment without having to make changes to the mainline service code. Dapr,\n[discussed earlier, follows this model with its set of prebuilt building blocks.](https://docs.dapr.io/developing-applications/building-blocks/)\n\n\nOn a final thought, backing services also promote the **Statelessness** [princip', 'le from the Twelve-Factor](https://12factor.net/)\n[Application, discussed earlier in the chapter.](https://12factor.net/)\n\n\n_Factor #6_ specifies that, “Each microservice should execute in its own process, isolated from other\nrunning services. Externalize required state to a backing service such as a distributed cache or data\nstore.”\n\n\nBacking services are discussed in Cloud-native data patterns and Cloud-native communication\npatterns.\n\n#### **Au', 'tomation**\n\n\nAs you’ve seen, cloud-native systems embrace microservices, containers, and modern system design\nto achieve speed and agility. But, that’s only part of the story. How do you provision the cloud\nenvironments upon which these systems run? How do you rapidly deploy app features and updates?\nHow do you round out the full picture?\n\n\n[Enter the widely accepted practice of Infrastructure as Code, or IaC.](https://docs.microsoft.com/devops/d', 'eliver/what-is-infrastructure-as-code)\n\n\nWith IaC, you automate platform provisioning and application deployment. You essentially apply\nsoftware engineering practices such as testing and versioning to your DevOps practices. Your\ninfrastructure and deployments are automated, consistent, and repeatable.\n\n\n**Automating infrastructure**\n\n\n[Tools like Azure Resource Manager, Azure Bicep,](https://docs.microsoft.com/azure/azure-resource-manager/managem', 'ent/overview) [Terraform](https://www.terraform.io/) [from HashiCorp, and the Azure CLI, enable](https://docs.microsoft.com/cli/azure/)\nyou to declaratively script the cloud infrastructure you require. Resource names, locations, capacities,\nand secrets are parameterized and dynamic. The script is versioned and checked into source control\nas an artifact of your project. You invoke the script to provision a consistent and repeatable\ninfrastructure ', 'across system environments, such as QA, staging, and production.\n\n\nUnder the hood, IaC is idempotent, meaning that you can run the same script over and over without\nside effects. If the team needs to make a change, they edit and rerun the script. Only the updated\nresources are affected.\n\n\n[In the article, What is Infrastructure as Code, Author Sam Guckenheimer describes how, “Teams who](https://docs.microsoft.com/devops/deliver/what-is-infrastruc', 'ture-as-code)\nimplement IaC can deliver stable environments rapidly and at scale. They avoid manual configuration\nof environments and enforce consistency by representing the desired state of their environments via\ncode. Infrastructure deployments with IaC are repeatable and prevent runtime issues caused by\nconfiguration drift or missing dependencies. DevOps teams can work together with a unified set of\n\n\n17 CHAPTER 1 | Introduction to cloud-nativ', 'e applications\n\n\npractices and tools to deliver applications and their supporting infrastructure rapidly, reliably, and at\nscale.”\n\n\n**Automating deployments**\n\n\n[The Twelve-Factor Application, discussed earlier, calls for separate steps when transforming](https://12factor.net/)\ncompleted code into a running application.\n\n\n_Factor #5_ specifies that “Each release must enforce a strict separation across the build, release and run\nstages. Each shou', 'ld be tagged with a unique ID and support the ability to roll back.”\n\n\nModern CI/CD systems help fulfill this principle. They provide separate build and delivery steps that\nhelp ensure consistent and quality code that’s readily available to users.\n\n\nFigure 1-9 shows the separation across the deployment process.\n\n\n_Figure 1-9. Deployment steps in a CI/CD Pipeline_\n\n\nIn the previous figure, pay special attention to separation of tasks:\n\n\n1. The dev', 'eloper constructs a feature in their development environment, iterating through what\nis called the “inner loop” of code, run, and debug.\n\n2. When complete, that code is _pushed_ into a code repository, such as GitHub, Azure DevOps, or\nBitBucket.\n\n3. The push triggers a build stage that transforms the code into a binary artifact. The work is\n[implemented with a Continuous Integration (CI)](https://martinfowler.com/articles/continuousIntegration.ht', 'ml) pipeline. It automatically builds, tests, and\npackages the application.\n\n4. The release stage picks up the binary artifact, applies external application and environment\nconfiguration information, and produces an immutable release. The release is deployed to a\n[specified environment. The work is implemented with a Continuous Delivery (CD) pipeline.](https://martinfowler.com/bliki/ContinuousDelivery.html)\nEach release should be identifiable. Yo', 'u can say, “This deployment is running Release 2.1.1 of\nthe application.”\n\n\n18 CHAPTER 1 | Introduction to cloud-native applications\n\n\n5. Finally, the released feature is run in the target execution environment. Releases are\nimmutable meaning that any change must create a new release.\n\n\nApplying these practices, organizations have radically evolved how they ship software. Many have\nmoved from quarterly releases to on-demand updates. The goal is t', 'o catch problems early in the\ndevelopment cycle when they’re less expensive to fix. The longer the duration between integrations,\nthe more expensive problems become to resolve. With consistency in the integration process, teams\ncan commit code changes more frequently, leading to better collaboration and software quality.\n\n\nInfrastructure as code and deployment automation, along with GitHub and Azure DevOps are\ndiscussed in detail in DevOps.\n\n### ', 'Candidate apps for cloud native\n\n\nThink about the apps your organization needs to build. Then, look at the existing apps in your\nportfolio. How many of them warrant a cloud-native architecture? All of them? Perhaps some?\n\n\nApplying cost/benefit analysis, there’s a good chance some wouldn’t support the effort. The cost of\nbecoming cloud native would far exceed the business value of the application.\n\n\nWhat type of application might be a candidate f', 'or cloud native?\n\n\n  - Strategic enterprise systems that need to constantly evolve business capabilities/features\n\n\n  - An application that requires a high release velocity - with high confidence\n\n\n  - A system where individual features must release _without_ a full redeployment of the entire\nsystem\n\n\n  - An application developed by teams with expertise in different technology stacks\n\n\n  - An application with components that must scale independen', 'tly\n\n\nSmaller, less impactful line-of-business applications might fare well with a simple monolithic\narchitecture hosted in a Cloud PaaS environment.\n\n\nThen there are legacy systems. While we’d all like to build new applications, we’re often responsible\nfor modernizing legacy workloads that are critical to the business.\n\n#### **Modernizing legacy apps**\n\n\n[The free Microsoft e-book Modernize existing .NET applications with Azure cloud and Windows', '](https://dotnet.microsoft.com/download/thank-you/modernizing-existing-net-apps-ebook)\n[Containers](https://dotnet.microsoft.com/download/thank-you/modernizing-existing-net-apps-ebook) provides guidance about migrating on-premises workloads into cloud. Figure 1-10 shows\nthat there isn’t a single, one-size-fits-all strategy for modernizing legacy applications.\n\n\n19 CHAPTER 1 | Introduction to cloud-native applications\n\n\n_Figure 1-10. Strategies fo', 'r migrating legacy workloads_\n\n\nMonolithic apps that are non-critical might benefit from a quick **lift-and-shift** [(Cloud Infrastructure-](https://docs.microsoft.com/dotnet/architecture/modernize-with-azure-containers/lift-and-shift-existing-apps-azure-iaas)\n[Ready) migration. Here, the on-premises workload is rehosted to a cloud-based VM, without changes.](https://docs.microsoft.com/dotnet/architecture/modernize-with-azure-containers/lift-and-', 'shift-existing-apps-azure-iaas)\n[This approach uses the IaaS (Infrastructure as a Service) model. Azure includes several tools such as](https://azure.microsoft.com/overview/what-is-iaas/)\n[Azure Migrate,](https://azure.microsoft.com/services/azure-migrate/) [Azure Site Recovery, and Azure Database Migration Service](https://azure.microsoft.com/services/site-recovery/) to help streamline the\nmove. While this strategy can yield some cost savings, s', 'uch applications typically weren’t designed to\nunlock and leverage the benefits of cloud computing.\n\n\nLegacy apps that are critical to the business often benefit from an enhanced **Cloud Optimized**\nmigration. This approach includes deployment optimizations that enable key cloud services - without\n[changing the core architecture of the application. For example, you might containerize the application](https://docs.microsoft.com/virtualization/wind', 'owscontainers/about/)\n[and deploy it to a container orchestrator, like Azure Kubernetes Services, discussed later in this book.](https://azure.microsoft.com/services/kubernetes-service/)\nOnce in the cloud, the application can consume cloud backing services such as databases, message\nqueues, monitoring, and distributed caching.\n\n\nFinally, monolithic apps that provide strategic enterprise functions might best benefit from a _Cloud-_\n_Native_ approa', 'ch, the subject of this book. This approach provides agility and velocity. But, it comes at\na cost of replatforming, rearchitecting, and rewriting code. Over time, a legacy application could be\ndecomposed into microservices, containerized, and ultimately _replatformed_ into a cloud-native\narchitecture.\n\n\nIf you and your team believe a cloud-native approach is appropriate, it behooves you to rationalize\nthe decision with your organization. What ex', 'actly is the business problem that a cloud-native\napproach will solve? How would it align with business needs?\n\n\n  - Rapid releases of features with increased confidence?\n\n\n  - Fine-grained scalability - more efficient usage of resources?\n\n\n20 CHAPTER 1 | Introduction to cloud-native applications\n\n\n  - Improved system resiliency?\n\n\n  - Improved system performance?\n\n\n  - More visibility into operations?\n\n\n  - Blend development platforms and data s', 'tores to arrive at the best tool for the job?\n\n\n  - Future-proof application investment?\n\n\nThe right migration strategy depends on organizational priorities and the systems you’re targeting.\nFor many, it may be more cost effective to cloud-optimize a monolithic application or add coarsegrained services to an N-Tier app. In these cases, you can still make full use of cloud PaaS capabilities\nlike the ones offered by Azure App Service.\n\n#### **Summa', 'ry**\n\n\nIn this chapter, we introduced cloud-native computing. We provided a definition along with the key\ncapabilities that drive a cloud-native application. We looked at the types of applications that might\njustify this investment and effort.\n\n\nWith the introduction behind, we now dive into a much more detailed look at cloud native.\n\n\n**References**\n\n\n  - [Cloud Native Computing Foundation](https://www.cncf.io/)\n\n\n  - [.NET Microservices: Archit', 'ecture for Containerized .NET applications](https://dotnet.microsoft.com/download/thank-you/microservices-architecture-ebook)\n\n\n  - [Microsoft Azure Well-Architected Framework](https://docs.microsoft.com/azure/architecture/framework/)\n\n\n  - [Modernize existing .NET applications with Azure cloud and Windows Containers](https://dotnet.microsoft.com/download/thank-you/modernizing-existing-net-apps-ebook)\n\n\n  - [Cloud Native Patterns by Cornelia Davi', 's](https://www.manning.com/books/cloud-native-patterns)\n\n\n  - [Cloud native applications: Ship faster, reduce risk, and grow your business](https://tanzu.vmware.com/cloud-native)\n\n\n  - [Dapr for .NET Developers](https://docs.microsoft.com/dotnet/architecture/dapr-for-net-developers/)\n\n\n  - [Dapr documents](https://dapr.io/)\n\n\n  - [Beyond the Twelve-Factor Application](https://content.pivotal.io/blog/beyond-the-twelve-factor-app)\n\n\n  - [What is In', 'frastructure as Code](https://docs.microsoft.com/devops/deliver/what-is-infrastructure-as-code)\n\n\n  - [Uber Engineering’s Micro Deploy: Deploying Daily with Confidence](https://www.uber.com/blog/micro-deploy-code/)\n\n\n  - [How Netflix Deploys Code](https://www.infoq.com/news/2013/06/netflix/)\n\n\n  - [Overload Control for Scaling WeChat Microservices](https://www.cs.columbia.edu/~ruigu/papers/socc18-final100.pdf)\n\n\n21 CHAPTER 1 | Introduction to clo', 'ud-native applications\n\n\n**CHAPTER**\n# 2\n\n## Introducing eShopOnContainers reference app\n\n\nMicrosoft, in partnership with leading community experts, has produced a full-featured cloud-native\nmicroservices reference application, eShopOnContainers. This application is built to showcase using\n.NET and Docker, and optionally Azure, Kubernetes, and Visual Studio, to build an online storefront.\n\n\n_Figure 2-1. eShopOnContainers Sample App Screenshot._\n\n', '\n22 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n[Before starting this chapter, we recommend that you download the eShopOnContainers reference](https://github.com/dotnet-architecture/eShopOnContainers)\n[application. If you do so, it should be easier for you to follow along with the information presented.](https://github.com/dotnet-architecture/eShopOnContainers)\n\n### Features and requirements\n\n\nLet’s start with a review of the applic', 'ation’s features and requirements. The eShopOnContainers\napplication represents an online store that sells various physical products like t-shirts and coffee\nmugs. If you’ve bought anything online before, the experience of using the store should be relatively\nfamiliar. Here are some of the basic features the store implements:\n\n\n  - List catalog items\n\n  - Filter items by type\n\n  - Filter items by brand\n\n  - Add items to the shopping basket\n\n  - E', 'dit or remove items from the basket\n\n  - Checkout\n\n  - Register an account\n\n  - Sign in\n\n  - Sign out\n\n  - Review orders\n\nThe application also has the following non-functional requirements:\n\n\n  - It needs to be highly available and it must scale automatically to meet increased traffic (and\nscale back down once traffic subsides).\n\n  - It should provide easy-to-use monitoring of its health and diagnostic logs to help\ntroubleshoot any issues it enco', 'unters.\n\n  - It should support an agile development process, including support for continuous integration\nand deployment (CI/CD).\n\n  - In addition to the two web front ends (traditional and Single Page Application), the\napplication must also support mobile client apps running different kinds of operating\nsystems.\n\n  - It should support cross-platform hosting and cross-platform development.\n\n\n23 CHAPTER 2 | Introducing eShopOnContainers reference ', 'app\n\n\n_Figure 2-2. eShopOnContainers reference application development architecture._\n\n\nThe eShopOnContainers application is accessible from web or mobile clients that access the\napplication over HTTPS targeting either the ASP.NET Core MVC server application or an appropriate\nAPI Gateway. API Gateways offer several advantages, such as decoupling back-end services from\nindividual front-end clients and providing better security. The application als', 'o makes use of a related\npattern known as Backends-for-Frontends (BFF), which recommends creating separate API gateways\nfor each front-end client. The reference architecture demonstrates breaking up the API gateways\nbased on whether the request is coming from a web or mobile client.\n\n\nThe application’s functionality is broken up into many distinct microservices. There are services\nresponsible for authentication and identity, listing items from th', 'e product catalog, managing users’\nshopping baskets, and placing orders. Each of these separate services has its own persistent storage.\nThere’s no single primary data store with which all services interact. Instead, coordination and\ncommunication between the services is done on an as-needed basis and by using a message bus.\n\n\nEach of the different microservices is designed differently, based on their individual requirements. This\naspect means th', 'eir technology stack may differ, although they’re all built using .NET and designed for\nthe cloud. Simpler services provide basic Create-Read-Update-Delete (CRUD) access to the underlying\ndata stores, while more advanced services use Domain-Driven Design approaches and patterns to\nmanage business complexity.\n\n\n24 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n_Figure 2-3. Different kinds of microservices._\n\n### Overview of the code\n\n\nB', 'ecause it uses microservices, the eShopOnContainers app includes quite a few separate projects and\nsolutions in its GitHub repository. In addition to separate solutions and executable files, the various\nservices are designed to run inside their own containers, both during local development and at run\ntime in production. Figure 2-4 shows the full Visual Studio solution, in which the various different\nprojects are organized.\n\n\n25 CHAPTER 2 | Introd', 'ucing eShopOnContainers reference app\n\n\n26 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n_Figure 2-4. Projects in Visual Studio solution._\n\n\nThe code is organized to support the different microservices, and within each microservice, the code is\nbroken up into domain logic, infrastructure concerns, and user interface or service endpoint. In many\ncases, each service’s dependencies can be fulfilled by Azure services in production, and al', 'ternative\noptions for local development. Let’s examine how the application’s requirements map to Azure\nservices.\n\n### Understanding microservices\n\n\nThis book focuses on cloud-native applications built using Azure technology. To learn more about\nmicroservices best practices and how to architect microservice-based applications, read the\n[companion book, .NET Microservices: Architecture for Containerized .NET Applications.](https://dotnet.microsoft.', 'com/download/thank-you/microservices-architecture-ebook)\n\n### Mapping eShopOnContainers to Azure Services\n\n\nAlthough not required, Azure is well-suited to supporting the eShopOnContainers because the project\nwas built to be a cloud-native application. The application is built with .NET, so it can run on Linux or\nWindows containers depending on the Docker host. The application is made up of multiple\nautonomous microservices, each with its own data', '. The different microservices showcase different\napproaches, ranging from simple CRUD operations to more complex DDD and CQRS patterns.\nMicroservices communicate with clients over HTTP and with one another via message-based\ncommunication. The application supports multiple platforms for clients as well, since it adopts HTTP\nas a standard communication protocol and includes ASP.NET Core and Xamarin mobile apps that run\non Android, iOS, and Windows ', 'platforms.\n\n\nThe application’s architecture is shown in Figure 2-5. On the left are the client apps, broken up into\nmobile, traditional Web, and Web Single Page Application (SPA) flavors. On the right are the serverside components that make up the system, each of which can be hosted in Docker containers and\nKubernetes clusters. The traditional web app is powered by the ASP.NET Core MVC application shown\nin yellow. This app and the mobile and web ', 'SPA applications communicate with the individual\nmicroservices through one or more API gateways. The API gateways follow the “backends for front\nends” (BFF) pattern, meaning that each gateway is designed to support a given front-end client. The\nindividual microservices are listed to the right of the API gateways and include both business logic\nand some kind of persistence store. The different services make use of SQL Server databases, Redis\ncache', ' instances, and MongoDB/CosmosDB stores. On the far right is the system’s Event Bus, which is\nused for communication between the microservices.\n\n\n27 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n_Figure 2-5. The eShopOnContainers Architecture._\n\n\nThe server-side components of this architecture all map easily to Azure services.\n\n#### **Container orchestration and clustering**\n\n\nThe application’s container-hosted services, from ASP.NET ', 'Core MVC apps to individual Catalog and\nOrdering microservices, can be hosted and managed in Azure Kubernetes Service (AKS). The\napplication can run locally on Docker and Kubernetes, and the same containers can then be deployed\nto staging and production environments hosted in AKS. This process can be automated as we’ll see in\nthe next section.\n\n\nAKS provides management services for individual clusters of containers. The application will deploy\nse', 'parate containers for each microservice in the AKS cluster, as shown in the architecture diagram\nabove. This approach allows each individual service to scale independently according to its resource\ndemands. Each microservice can also be deployed independently, and ideally such deployments\nshould incur zero system downtime.\n\n#### **API Gateway**\n\n\nThe eShopOnContainers application has multiple front-end clients and multiple different back-end\nserv', 'ices. There’s no one-to-one correspondence between the client applications and the microservices\nthat support them. In such a scenario, there may be a great deal of complexity when writing client\nsoftware to interface with the various back-end services in a secure manner. Each client would need to\naddress this complexity on its own, resulting in duplication and many places in which to make updates\nas services change or new policies are implemente', 'd.\n\n\nAzure API Management (APIM) helps organizations publish APIs in a consistent, manageable fashion.\nAPIM consists of three components: the API Gateway, and administration portal (the Azure portal),\nand a developer portal.\n\n\n28 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\nThe API Gateway accepts API calls and routes them to the appropriate back-end API. It can also\nprovide additional services like verification of API keys or JWT to', 'kens and API transformation on the\nfly without code modifications (for instance, to accommodate clients expecting an older interface).\n\n\nThe Azure portal is where you define the API schema and package different APIs into products. You\nalso configure user access, view reports, and configure policies for quotas or transformations.\n\n\nThe developer portal serves as the main resource for developers. It provides developers with API\ndocumentation, an in', 'teractive test console, and reports on their own usage. Developers also use the\nportal to create and manage their own accounts, including subscription and API key support.\n\n\nUsing APIM, applications can expose several different groups of services, each providing a back end\nfor a particular front-end client. APIM is recommended for complex scenarios. For simpler needs, the\nlightweight API Gateway Ocelot can be used. The eShopOnContainers app uses ', 'Ocelot because of its\nsimplicity and because it can be deployed into the same application environment as the application\n[itself. Learn more about eShopOnContainers, APIM, and Ocelot.](https://docs.microsoft.com/dotnet/architecture/microservices/architect-microservice-container-applications/direct-client-to-microservice-communication-versus-the-api-gateway-pattern#azure-api-management)\n\n\nAnother option if your application is using AKS is to deplo', 'y the Azure Gateway Ingress Controller as a\npod within your AKS cluster. This approach allows your cluster to integrate with an Azure Application\n[Gateway, allowing the gateway to load-balance traffic to the AKS pods. Learn more about the Azure](https://github.com/Azure/application-gateway-kubernetes-ingress)\n[Gateway Ingress Controller for AKS.](https://github.com/Azure/application-gateway-kubernetes-ingress)\n\n#### **Data**\n\n\nThe various back-en', 'd services used by eShopOnContainers have different storage requirements.\nSeveral microservices use SQL Server databases. The Basket microservice leverages a Redis cache for\nits persistence. The Locations microservice expects a MongoDB API for its data. Azure supports each\nof these data formats.\n\n\nFor SQL Server database support, Azure has products for everything from single databases up to\nhighly scalable SQL Database elastic pools. Individual m', 'icroservices can be configured to\ncommunicate with their own individual SQL Server databases quickly and easily. These databases can\nbe scaled as needed to support each separate microservice according to its needs.\n\n\nThe eShopOnContainers application stores the user’s current shopping basket between requests. This\naspect is managed by the Basket microservice that stores the data in a Redis cache. In development,\nthis cache can be deployed in a co', 'ntainer, while in production it can utilize Azure Cache for Redis.\nAzure Cache for Redis is a fully managed service offering high performance and reliability without the\nneed to deploy and manage Redis instances or containers on your own.\n\n\nThe Locations microservice uses a MongoDB NoSQL database for its persistence. During\ndevelopment, the database can be deployed in its own container, while in production the service can\n[leverage Azure Cosmos D', 'B’s API for MongoDB. One of the benefits of Azure Cosmos DB is its ability](https://docs.microsoft.com/azure/cosmos-db/mongodb-introduction)\nto leverage multiple different communication protocols, including a SQL API and common NoSQL\nAPIs including MongoDB, Cassandra, Gremlin, and Azure Table Storage. Azure Cosmos DB offers a\nfully managed and globally distributed database as a service that can scale to meet the needs of the\nservices that use it.', '\n\n\nDistributed data in cloud-native applications is covered in more detail in chapter 5.\n\n\n29 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n#### **Event Bus**\n\nThe application uses events to communicate changes between different services. This functionality can\nbe implemented with various implementations, and locally the eShopOnContainers application uses\n[RabbitMQ. When hosted in Azure, the application would leverage Azure Service Bu', 's for its messaging.](https://www.rabbitmq.com/)\nAzure Service Bus is a fully managed integration message broker that allows applications and services\nto communicate with one another in a decoupled, reliable, asynchronous manner. Azure Service Bus\nsupports individual queues as well as separate _topics_ to support publisher-subscriber scenarios. The\neShopOnContainers application would leverage topics with Azure Service Bus to support distributing\n', 'messages from one microservice to any other microservice that needed to react to a given message.\n\n#### **Resiliency**\n\n\nOnce deployed to production, the eShopOnContainers application would be able to take advantage\nof several Azure services available to improve its resiliency. The application publishes health checks,\nwhich can be integrated with Application Insights to provide reporting and alerts based on the app’s\navailability. Azure resources', ' also provide diagnostic logs that can be used to identify and correct bugs\nand performance issues. Resource logs provide detailed information on when and how different Azure\nresources are used by the application. You’ll learn more about cloud-native resiliency features in\nchapter 6.\n\n### Deploying eShopOnContainers to Azure\n\n\nThe eShopOnContainers application can be deployed to various Azure platforms. The recommended\napproach is to deploy the a', 'pplication to Azure Kubernetes Services (AKS). Helm, a Kubernetes\ndeployment tool, is available to reduce deployment complexity. Optionally, developers may\nimplement Azure Dev Spaces for Kubernetes to streamline their development process.\n\n#### **Azure Kubernetes Service**\n\n\nTo host eShop in AKS, the first step is to create an AKS cluster. To do so, you might use the Azure\nportal, which will walk you through the required steps. You could also cre', 'ate a cluster from the Azure\nCLI, taking care to enable Role-Based Access Control (RBAC) and application routing. The\neShopOnContainers’ documentation details the steps for creating your own AKS cluster. Once created,\nyou can access and manage the cluster from the Kubernetes dashboard.\n\n\nYou can now deploy the eShop application to the cluster using Helm.\n\n#### **Deploying to Azure Kubernetes Service using Helm**\n\n\nHelm is an application package m', 'anager tool that works directly with Kubernetes. It helps you define,\ninstall, and upgrade Kubernetes applications. While simple apps can be deployed to AKS with custom\nCLI scripts or simple deployment files, complex apps can contain many Kubernetes objects and benefit\nfrom Helm.\n\n\nUsing Helm, applications include text-based configuration files, called Helm charts, which declaratively\ndescribe the application and configuration in Helm packages. C', 'harts use standard YAML-formatted\n\n\n30 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\nfiles to describe a related set of Kubernetes resources. They’re versioned alongside the application\ncode they describe. Helm Charts range from simple to complex depending on the requirements of the\ninstallation they describe.\n\n\nHelm is composed of a command-line client tool, which consumes helm charts and launches\ncommands to a server component named', ', Tiller. Tiller communicates with the Kubernetes API to\nensure the correct provisioning of your containerized workloads. Helm is maintained by the Cloudnative Computing Foundation.\n\n\nThe following yaml file presents a Helm template:\n\n\nNote how the template describes a dynamic set of key/value pairs. When the template is invoked,\nvalues that enclosed in curly braces are pulled in from other yaml-based configuration files.\n\n\nYou’ll find the eShopO', 'nContainers helm charts in the /k8s/helm folder. Figure 2-6 shows how the\ndifferent components of the application are organized into a folder structure used by helm to define\nand managed deployments.\n\n\n31 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n_Figure 2-6. The eShopOnContainers helm folder._\n\n\nEach individual component is installed using a helm install command. eShop includes a “deploy all”\nscript that loops through and install', 's the components using their respective helm charts. The result is\na repeatable process, versioned with the application in source control, that anyone on the team can\ndeploy to an AKS cluster with a one-line script command.\n\n\nNote that version 3 of Helm officially removes the need for the Tiller server component. More\n[information on this enhancement can be found here.](https://medium.com/better-programming/why-is-tiller-missing-in-helm-3-2347c44', '6714)\n\n#### **Azure Functions and Logic Apps (Serverless)**\n\n\nThe eShopOnContainers sample includes support for tracking online marketing campaigns. An Azure\nFunction is used to track marketing campaign details for a given campaign ID. Rather than creating a\n\n\n32 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\nfull microservice, a single Azure Function is simpler and sufficient. Azure Functions have a simple build\nand deployment model, ', 'especially when configured to run in Kubernetes. Deploying the function is\nscripted using Azure Resource Manager (ARM) templates and the Azure CLI. This campaign service\nisn’t customer-facing and invokes a single operation, making it a great candidate for Azure Functions.\nThe function requires minimal configuration, including a database connection string data and image\nbase URI settings. You configure Azure Functions in the Azure portal.\n\n### Cen', 'tralized configuration\n\n\nUnlike a monolithic app in which everything runs within a single instance, a cloud-native application\nconsists of independent services distributed across virtual machines, containers, and geographic\nregions. Managing configuration settings for dozens of interdependent services can be challenging.\nDuplicate copies of configuration settings across different locations are error prone and difficult to\nmanage. Centralized conf', 'iguration is a critical requirement for distributed cloud-native applications.\n\n\nAs discussed in Chapter 1, the Twelve-Factor App recommendations require strict separation between\ncode and configuration. Configuration must be stored externally from the application and read-in as\nneeded. Storing configuration values as constants or literal values in code is a violation. The same\nconfiguration values are often be used by many services in the same a', 'pplication. Additionally, we\nmust support the same values across multiple environments, such as dev, testing, and production. The\nbest practice is store them in a centralized configuration store.\n\n\nThe Azure cloud presents several great options.\n\n#### **Azure App Configuration**\n\n\n[Azure App Configuration](https://docs.microsoft.com/azure/azure-app-configuration/overview) is a fully managed Azure service that stores non-secret configuration\nsetti', 'ngs in a secure, centralized location. Stored values can be shared among multiple services and\napplications.\n\n\nThe service is simple to use and provides several benefits:\n\n\n  - Flexible key/value representations and mappings\n\n  - Tagging with Azure labels\n\n  - Dedicated UI for management\n\n  - Encryption of sensitive information\n\n  - Querying and batch retrieval\n\n\nAzure App Configuration maintains changes made to key-value settings for seven days.', ' The point-intime snapshot feature enables you to reconstruct the history of a setting and even rollback for a failed\ndeployment.\n\n\nApp Configuration automatically caches each setting to avoid excessive calls to the configuration\nstore. The refresh operation waits until the cached value of a setting expires to update that setting,\neven when its value changes in the configuration store. The default cache expiration time is 30\nseconds. You can over', 'ride the expiration time.\n\n\n33 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\nApp Configuration encrypts all configuration values in transit and at rest. Key names and labels are\nused as indexes for retrieving configuration data and aren’t encrypted.\n\n\nAlthough App Configuration provides hardened security, Azure Key Vault is still the best place for\nstoring application secrets. Key Vault provides hardware-level encryption, granular acc', 'ess policies, and\nmanagement operations such as certificate rotation. You can create App Configuration values that\nreference secrets stored in a Key Vault.\n\n#### **Azure Key Vault**\n\n\nKey Vault is a managed service for securely storing and accessing secrets. A secret is anything that you\nwant to tightly control access to, such as API keys, passwords, or certificates. A vault is a logical group\nof secrets.\n\n\nKey Vault greatly reduces the chances t', 'hat secrets may be accidentally leaked. When using Key Vault,\napplication developers no longer need to store security information in their application. This practice\neliminates the need to store this information inside your code. For example, an application may need\nto connect to a database. Instead of storing the connection string in the app’s code, you can store it\nsecurely in Key Vault.\n\n\nYour applications can securely access the information t', 'hey need by using URIs. These URIs allow the\napplications to retrieve specific versions of a secret. There’s no need to write custom code to protect\nany of the secret information stored in Key Vault.\n\n\nAccess to Key Vault requires proper caller authentication and authorization. Typically, each cloudnative microservice uses a ClientId/ClientSecret combination. It’s important to keep these credentials\noutside source control. A best practice is to s', 'et them in the application’s environment. Direct access to\n[Key Vault from AKS can be achieved using Key Vault FlexVolume.](https://github.com/Azure/kubernetes-keyvault-flexvol)\n\n#### **Configuration in eShop**\n\n\nThe eShopOnContainers application includes local application settings files with each microservice.\nThese files are checked into source control, but don’t include production secrets such as connection\nstrings or API keys. In production, ', 'individual settings may be overwritten with per-service environment\nvariables. Injecting secrets in environment variables is a common practice for hosted applications, but\ndoesn’t provide a central configuration store. To support centralized management of configuration\nsettings, each microservice includes a setting to toggle between its use of local settings or Azure Key\nVault settings.\n\n#### **References**\n\n\n  - [The eShopOnContainers Architectu', 're](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Architecture)\n\n  - [Orchestrating microservices and multi-container applications for high scalability and](https://docs.microsoft.com/dotnet/architecture/microservices/architect-microservice-container-applications/scalable-available-multi-container-microservice-applications)\n[availability](https://docs.microsoft.com/dotnet/architecture/microservices/architect-microservice-container', '-applications/scalable-available-multi-container-microservice-applications)\n\n  - [Azure API Management](https://docs.microsoft.com/azure/api-management/api-management-key-concepts)\n\n  - [Azure SQL Database Overview](https://docs.microsoft.com/azure/sql-database/sql-database-technical-overview)\n\n  - [Azure Cache for Redis](https://azure.microsoft.com/services/cache/)\n\n  - [Azure Cosmos DB’s API for MongoDB](https://docs.microsoft.com/azure/cosmos-', 'db/mongodb-introduction)\n\n\n34 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n  - [Azure Service Bus](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview)\n\n  - [Azure Monitor overview](https://docs.microsoft.com/azure/azure-monitor/overview)\n\n  - [eShopOnContainers: Create Kubernetes cluster in AKS](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Deploy-to-Azure-Kubernetes-Service-(AKS)#cr', 'eate-kubernetes-cluster-in-aks)\n\n  - [eShopOnContainers: Azure Dev Spaces](https://github.com/dotnet-architecture/eShopOnContainers/wiki/Azure-Dev-Spaces)\n\n  - [Azure Dev Spaces](https://docs.microsoft.com/azure/dev-spaces/about)\n\n\n35 CHAPTER 2 | Introducing eShopOnContainers reference app\n\n\n**CHAPTER**\n# 3\n\n## Scaling cloud-native applications\n\n\nOne of the most-often touted advantages of moving to a cloud hosting environment is scalability.\nScal', 'ability, or the ability for an application to accept additional user load without compromising\nperformance for each user. It’s most often achieved by breaking up an application into small pieces\nthat can each be given whatever resources they require. Cloud vendors enable massive scalability\nanytime and anywhere in the world.\n\n\nIn this chapter, we discuss technologies that enable cloud-native applications to scale to meet user\ndemand. These techno', 'logies include:\n\n\n  - Containers\n\n  - Orchestrators\n\n  - Serverless computing\n\n### Leveraging containers and orchestrators\n\n\nContainers and orchestrators are designed to solve problems common to monolithic deployment\napproaches.\n\n#### **Challenges with monolithic deployments**\n\n\nTraditionally, most applications have been deployed as a single unit. Such applications are referred to\nas a monolith. This general approach of deploying applications as ', 'single units even if they’re\ncomposed of multiple modules or assemblies is known as monolithic architecture, as shown in Figure\n3-1.\n\n\n36 CHAPTER 3 | Scaling cloud-native applications\n\n\n_Figure 3-1. Monolithic architecture._\n\n\nAlthough they have the benefit of simplicity, monolithic architectures face many challenges:\n\n\n**Deployment**\n\n\nAdditionally, they require a restart of the application, which may temporarily impact availability if\nzero-down', 'time techniques are not applied while deploying.\n\n\n**Scaling**\n\n\nA monolithic application is hosted entirely on a single machine instance, often requiring highcapability hardware. If any part of the monolith requires scaling, another copy of the entire application\nmust be deployed to another machine. With a monolith, you can’t scale application components\nindividually - it’s all or nothing. Scaling components that don’t require scaling results in', ' inefficient and\ncostly resource usage.\n\n\n**Environment**\n\n\nMonolithic applications are typically deployed to a hosting environment with a pre-installed operating\nsystem, runtime, and library dependencies. This environment may not match that upon which the\napplication was developed or tested. Inconsistencies across application environments are a common\nsource of problems for monolithic deployments.\n\n\n**Coupling**\n\n\nA monolithic application is lik', 'ely to experience high coupling across its functional components.\nWithout hard boundaries, system changes often result in unintended and costly side effects. New\nfeatures/fixes become tricky, time-consuming, and expensive to implement. Updates require extensive\ntesting. Coupling also makes it difficult to refactor components or swap in alternative\nimplementations. Even when constructed with a strict separation of concerns, architectural erosion\ns', 'ets in as the monolithic code base deteriorates with never-ending “special cases.”\n\n\n37 CHAPTER 3 | Scaling cloud-native applications\n\n\n**Platform lock-in**\n\n\nA monolithic application is constructed with a single technology stack. While offering uniformity, this\ncommitment can become a barrier to innovation. New features and components will be built using\nthe application’s current stack - even when more modern technologies may be a better choice.', ' A\nlonger-term risk is your technology stack becoming outdated and obsolete. Rearchitecting an entire\napplication to a new, more modern platform is at best expensive and risky.\n\n#### **What are the benefits of containers and orchestrators?**\n\n\nWe introduced containers in Chapter 1. We highlighted how the Cloud Native Computing Foundation\n[(CNCF) ranks containerization as the first step in their Cloud-Native Trail Map](https://raw.githubuserconten', 't.com/cncf/trailmap/master/CNCF_TrailMap_latest.png) - guidance for\nenterprises beginning their cloud-native journey. In this section, we discuss the benefits of containers.\n\n\nDocker is the most popular container management platform. It works with containers on both Linux or\nWindows. Containers provide separate but reproducible application environments that run the same\nway on any system. This aspect makes them perfect for developing and hosting ', 'cloud-native services.\nContainers are isolated from one another. Two containers on the same host hardware can have\ndifferent versions of software, without causing conflicts.\n\n\nContainers are defined by simple text-based files that become project artifacts and are checked into\nsource control. While full servers and virtual machines require manual effort to update, containers are\neasily version-controlled. Apps built to run in containers can be dev', 'eloped, tested, and deployed\nusing automated tools as part of a build pipeline.\n\n\nContainers are immutable. Once you define a container, you can recreate and run it exactly the same\nway. This immutability lends itself to component-based design. If some parts of an application evolve\ndifferently than others, why redeploy the entire app when you can just deploy the parts that change\nmost frequently? Different features and cross-cutting concerns of ', 'an app can be broken up into\nseparate units. Figure 3-2 shows how a monolithic app can take advantage of containers and\nmicroservices by delegating certain features or functionality. The remaining functionality in the app\nitself has also been containerized.\n\n\n38 CHAPTER 3 | Scaling cloud-native applications\n\n\n_Figure 3-2. Decomposing a monolithic app to embrace microservices._\n\n\nEach cloud-native service is built and deployed in a separate contai', 'ner. Each can update as needed.\nIndividual services can be hosted on nodes with resources appropriate to each service. The\nenvironment each service runs in is immutable, shared across dev, test, and production environments,\nand easily versioned. Coupling between different areas of the application occurs explicitly as calls or\nmessages between services, not compile-time dependencies within the monolith. You can also choose\nthe technology that best', ' suites a given capability without requiring changes to the rest of the app.\n\n\nContainerized services require automated management. It wouldn’t be feasible to manually administer\na large set of independently deployed containers. For example, consider the following tasks:\n\n\n  - How will container instances be provisioned across a cluster of many machines?\n\n  - Once deployed, how will containers discover and communicate with each other?\n\n  - How ca', 'n containers scale in or out on-demand?\n\n  - How do you monitor the health of each container?\n\n  - How do you protect a container against hardware and software failures?\n\n  - How do upgrade containers for a live application with zero downtime?\n\n\nContainer orchestrators address and automate these and other concerns.\n\n\nIn the cloud-native eco-system, Kubernetes has become the de facto container orchestrator. It’s an\nopen-source platform managed by ', 'the Cloud Native Computing Foundation (CNCF). Kubernetes\nautomates the deployment, scaling, and operational concerns of containerized workloads across a\nmachine cluster. However, installing and managing Kubernetes is notoriously complex.\n\n\n39 CHAPTER 3 | Scaling cloud-native applications\n\n\nA much better approach is to leverage Kubernetes as a managed service from a cloud vendor. The\n[Azure cloud features a fully managed Kubernetes platform entitl', 'ed Azure Kubernetes Service (AKS).](https://azure.microsoft.com/services/kubernetes-service/)\nAKS abstracts the complexity and operational overhead of managing Kubernetes. You consume\nKubernetes as a cloud service; Microsoft takes responsibility for managing and supporting it. AKS also\ntightly integrates with other Azure services and dev tools.\n\n\nAKS is a cluster-based technology. A pool of federated virtual machines, or nodes, is deployed to the', '\nAzure cloud. Together they form a highly available environment, or cluster. The cluster appears as a\nseamless, single entity to your cloud-native application. Under the hood, AKS deploys your\ncontainerized services across these nodes following a predefined strategy that evenly distributes the\nload.\n\n#### **What are the scaling benefits?**\n\n\nServices built on containers can leverage scaling benefits provided by orchestration tools like\nKubernetes', '. By design containers only know about themselves. Once you have multiple containers\nthat need to work together, you should organize them at a higher level. Organizing large numbers of\ncontainers and their shared dependencies, such as network configuration, is where orchestration tools\ncome in to save the day! Kubernetes creates an abstraction layer over groups of containers and\norganizes them into _pods_ . Pods run on worker machines referred to', ' as _nodes_ . This organized structure\nis referred to as a _cluster_ . Figure 3-3 shows the different components of a Kubernetes cluster.\n\n\n_Figure 3-3. Kubernetes cluster components._\n\n\nScaling containerized workloads is a key feature of container orchestrators. AKS supports automatic\nscaling across two dimensions: Container instances and compute nodes. Together they give AKS the\nability to quickly and efficiently respond to spikes in demand and', ' add additional resources. We\ndiscuss scaling in AKS later in this chapter.\n\n\n40 CHAPTER 3 | Scaling cloud-native applications\n\n\n**Declarative versus imperative**\n\n\nKubernetes supports both declarative and imperative configuration. The imperative approach involves\nrunning various commands that tell Kubernetes what to do each step of the way. Run this image.\nDelete this pod. Expose this port. With the declarative approach, you create a configurati', 'on file, called\na manifest, to describe what you want instead of what to do. Kubernetes reads the manifest and\ntransforms your desired end state into actual end state.\n\n\nImperative commands are great for learning and interactive experimentation. However, you’ll want to\ndeclaratively create Kubernetes manifest files to embrace an infrastructure as code approach,\nproviding for reliable and repeatable deployments. The manifest file becomes a project', ' artifact and is\nused in your CI/CD pipeline for automating Kubernetes deployments.\n\n\nIf you’ve already configured your cluster using imperative commands, you can export a declarative\nmanifest by using kubectl get svc SERVICENAME -o yaml > service.yaml. This command produces a\nmanifest similar to one shown below:\n\n\nWhen using declarative configuration, you can preview the changes that will be made before\ncommitting them by using kubectl diff -f F', 'OLDERNAME against the folder where your configuration\nfiles are located. Once you’re sure you want to apply the changes, run kubectl apply -f FOLDERNAME.\nAdd -R to recursively process a folder hierarchy.\n\n\nYou can also use declarative configuration with other Kubernetes features, one of which being\ndeployments. Declarative deployments help manage releases, updates, and scaling. They instruct the\nKubernetes deployment controller on how to deploy n', 'ew changes, scale out load, or roll back to a\nprevious revision. If a cluster is unstable, a declarative deployment will automatically return the cluster\nback to a desired state. For example, if a node should crash, the deployment mechanism will redeploy\na replacement to achieve your desired state\n\n\n41 CHAPTER 3 | Scaling cloud-native applications\n\n\nUsing declarative configuration allows infrastructure to be represented as code that can be checke', 'd in\nand versioned alongside the application code. It provides improved change control and better\nsupport for continuous deployment using a build and deploy pipeline.\n\n#### **What scenarios are ideal for containers and orchestrators?**\n\n\nThe following scenarios are ideal for using containers and orchestrators.\n\n\n**Applications requiring high uptime and scalability**\n\n\nIndividual applications that have high uptime and scalability requirements are ', 'ideal candidates for\ncloud-native architectures using microservices, containers, and orchestrators. They can be developed\nin containers, tested across versioned environments, and deployed into production with zero\ndowntime. The use of Kubernetes clusters ensures such apps can also scale on demand and recover\nautomatically from node failures.\n\n\n**Large numbers of applications**\n\n\nOrganizations that deploy and maintain large numbers of applications', ' benefit from containers and\norchestrators. The up front effort of setting up containerized environments and Kubernetes clusters is\nprimarily a fixed cost. Deploying, maintaining, and updating individual applications has a cost that\nvaries with the number of applications. Beyond a few applications, the complexity of maintaining\ncustom applications manually exceeds the cost of implementing a solution using containers and\norchestrators.\n\n#### **Whe', 'n should you avoid using containers and orchestrators?**\n\n\nIf you’re unable to build your application following the Twelve-Factor App principles, you should\nconsider avoiding containers and orchestrators. In these cases, consider a VM-based hosting platform,\nor possibly some hybrid system. With it, you can always spin off certain pieces of functionality into\nseparate containers or even serverless functions.\n\n#### **Development resources**\n\n\nThis ', 'section shows a short list of development resources that may help you get started using\ncontainers and orchestrators for your next application. If you’re looking for guidance on how to\ndesign your cloud-native microservices architecture app, read this book’s companion, .NET\n[Microservices: Architecture for Containerized .NET Applications.](https://dotnet.microsoft.com/download/thank-you/microservices-architecture-ebook)\n\n\n**Local Kubernetes Devel', 'opment**\n\n\nKubernetes deployments provide great value in production environments, but can also run locally on\nyour development machine. While you may work on individual microservices independently, there\nmay be times when you’ll need to run the entire system locally - just as it will run when deployed to\nproduction. There are several tools that can help: Minikube and Docker Desktop. Visual Studio also\nprovides tooling for Docker development.\n\n\n42', ' CHAPTER 3 | Scaling cloud-native applications\n\n\n**Minikube**\n\n\nWhat is Minikube? The Minikube project says “Minikube implements a local Kubernetes cluster on\nmacOS, Linux, and Windows.” Its primary goals are “to be the best tool for local Kubernetes\napplication development and to support all Kubernetes features that fit.” Installing Minikube is\nseparate from Docker, but Minikube supports different hypervisors than Docker Desktop supports.\nThe fo', 'llowing Kubernetes features are currently supported by Minikube:\n\n\n  - DNS\n\n  - NodePorts\n\n  - ConfigMaps and secrets\n\n  - Dashboards\n\n  - Container runtimes: Docker, rkt, CRI-O, and containerd\n\n  - Enabling Container Network Interface (CNI)\n\n  - Ingress\n\n\nAfter installing Minikube, you can quickly start using it by running the minikube start command, which\ndownloads an image and start the local Kubernetes cluster. Once the cluster is started, yo', 'u interact\nwith it using the standard Kubernetes kubectl commands.\n\n\n**Docker Desktop**\n\n\nYou can also work with Kubernetes directly from Docker Desktop on Windows. It is your only option if\nyou’re using Windows Containers, and is a great choice for non-Windows containers as well. Figure 34 shows how to enable local Kubernetes support when running Docker Desktop.\n\n\n_Figure 3-4. Configuring Kubernetes in Docker Desktop._\n\n\n43 CHAPTER 3 | Scaling c', 'loud-native applications\n\n\nDocker Desktop is the most popular tool for configuring and running containerized apps locally.\nWhen you work with Docker Desktop, you can develop locally against the exact same set of Docker\ncontainer images that you’ll deploy to production. Docker Desktop is designed to “build, test, and\nship” containerized apps locally. It supports both Linux and Windows containers. Once you push your\nimages to an image registry, lik', 'e Azure Container Registry or Docker Hub, AKS can pull and deploy\nthem to production.\n\n\n**Visual Studio Docker Tooling**\n\n\nVisual Studio supports Docker development for web-based applications. When you create a new\nASP.NET Core application, you have an option to configure it with Docker support, as shown in Figure\n3-5.\n\n\n_Figure 3-5. Visual Studio Enable Docker Support_\n\n\nWhen this option is selected, the project is created with a Dockerfile in i', 'ts root, which can be used to\nbuild and host the app in a Docker container. An example Dockerfile is shown in Figure 3-6.\n\n\n44 CHAPTER 3 | Scaling cloud-native applications\n\n\n_Figure 3-6. Visual Studio generated Dockerfile_\n\n\nOnce support is added, you can run your application in a Docker container in Visual Studio. Figure 3-7\nshows the different run options available from a new ASP.NET Core project created with Docker\nsupport added.\n\n\n_Figure 3-', '7. Visual Studio Docker Run Options_\n\n\nAlso, at any time you can add Docker support to an existing ASP.NET Core application. From the\nVisual Studio Solution Explorer, right-click on the project and select **Add** - **Docker Support**, as shown\nin Figure 3-8.\n\n\n45 CHAPTER 3 | Scaling cloud-native applications\n\n\n_Figure 3-8. Adding Docker support to Visual Studio_\n\n\n**Visual Studio Code Docker Tooling**\n\n\nThere are many extensions available for Vis', 'ual Studio Code that support Docker development.\n\n\n[Microsoft provides the Docker for Visual Studio Code extension. This extension simplifies the process](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker)\nof adding container support to applications. It scaffolds required files, builds Docker images, and\nenables you to debug your app inside a container. The extension features a visual explorer that makes\nit easy to t', 'ake actions on containers and images such as start, stop, inspect, remove, and more. The\nextension also supports Docker Compose enabling you to manage multiple running containers as a\nsingle unit.\n\n### Leveraging serverless functions\n\n\nIn the spectrum from managing physical machines to leveraging cloud capabilities, serverless lives at\nthe extreme end. Your only responsibility is your code, and you only pay when your code runs. Azure\nFunctions pr', 'ovides a way to build serverless capabilities into your cloud-native applications.\n\n\n46 CHAPTER 3 | Scaling cloud-native applications\n\n\n#### **What is serverless?**\n\nServerless is a relatively new service model of cloud computing. It doesn’t mean that servers are\noptional - your code still runs on a server somewhere. The distinction is that the application team no\nlonger concerns itself with managing server infrastructure. Instead, the cloud vend', 'or own this\nresponsibility. The development team increases its productivity by delivering business solutions to\ncustomers, not plumbing.\n\n\nServerless computing uses event-triggered stateless containers to host your services. They can scale\nout and in to meet demand as-needed. Serverless platforms like Azure Functions have tight\nintegration with other Azure services like queues, events, and storage.\n\n#### **What challenges are solved by serverless', '?**\n\n\nServerless platforms address many time-consuming and expensive concerns:\n\n\n  - Purchasing machines and software licenses\n\n  - Housing, securing, configuring, and maintaining the machines and their networking, power,\nand A/C requirements\n\n  - Patching and upgrading operating systems and software\n\n  - Configuring web servers or machine services to host application software\n\n  - Configuring application software within its platform\n\n\nMany compa', 'nies allocate large budgets to support hardware infrastructure concerns. Moving to the\ncloud can help reduce these costs; shifting applications to serverless can help eliminate them.\n\n#### **What is the difference between a microservice and a serverless** **function?**\n\n\nTypically, a microservice encapsulates a business capability, such as a shopping cart for an online\neCommerce site. It exposes multiple operations that enable a user to manage th', 'eir shopping\nexperience. A function, however, is a small, lightweight block of code that executes a single-purpose\noperation in response to an event. Microservices are typically constructed to respond to requests,\noften from an interface. Requests can be HTTP Rest- or gRPC-based. Serverless services respond to\nevents. Its event-driven architecture is ideal for processing short-running, background tasks.\n\n#### **What scenarios are appropriate for ', 'serverless?**\n\n\nServerless exposes individual short-running functions that are invoked in response to a trigger. This\nmakes them ideal for processing background tasks.\n\n\nAn application might need to send an email as a step in a workflow. Instead of sending the\nnotification as part of a microservice request, place the message details onto a queue. An Azure\nFunction can dequeue the message and asynchronously send the email. Doing so could improve t', 'he\n[performance and scalability of the microservice. Queue-based load leveling can be implemented to](https://docs.microsoft.com/azure/architecture/patterns/queue-based-load-leveling)\navoid bottlenecks related to sending the emails. Additionally, this stand-alone service could be reused\nas a utility across many different applications.\n\n\n47 CHAPTER 3 | Scaling cloud-native applications\n\n\nAsynchronous messaging from queues and topics is a common pa', 'ttern to trigger serverless functions.\nHowever, Azure Functions can be triggered by other events, such as changes to Azure Blob Storage. A\nservice that supports image uploads could have an Azure Function responsible for optimizing the\nimage size. The function could be triggered directly by inserts into Azure Blob Storage, keeping\ncomplexity out of the microservice operations.\n\n\nMany services have long-running processes as part of their workflows.', ' Often these tasks are done as\npart of the user’s interaction with the application. These tasks can force the user to wait, negatively\nimpacting their experience. Serverless computing provides a great way to move slower tasks outside\nof the user interaction loop. These tasks can scale with demand without requiring the entire\napplication to scale.\n\n#### **When should you avoid serverless?**\n\n\nServerless solutions provision and scale on demand. Whe', 'n a new instance is invoked, cold starts are a\ncommon issue. A cold start is the period of time it takes to provision this instance. Normally, this delay\nmight be a few seconds, but can be longer depending on various factors. Once provisioned, a single\ninstance is kept alive as long as it receives periodic requests. But, if a service is called less frequently,\nAzure may remove it from memory and require a cold start when reinvoked. Cold starts ar', 'e also\nrequired when a function scales out to a new instance.\n\n\nFigure 3-9 shows a cold-start pattern. Note the extra steps required when the app is cold.\n\n\n_Figure 3-9. Cold start versus warm start._\n\n\n[To avoid cold starts entirely, you might switch from a consumption plan to a dedicated plan. You can](https://azure.microsoft.com/blog/understanding-serverless-cold-start/)\n[also configure one or more pre-warmed instances](https://docs.microsoft.', 'com/azure/azure-functions/functions-premium-plan#pre-warmed-instances) with the premium plan upgrade. In these cases,\nwhen you need to add another instance, it’s already up and ready to go. These options can help\nmitigate the cold start issue associated with serverless computing.\n\n\n48 CHAPTER 3 | Scaling cloud-native applications\n\n\nCloud providers bill for serverless based on compute execution time and consumed memory. Long\nrunning operations or ', 'high memory consumption workloads aren’t always the best candidates for\nserverless. Serverless functions favor small chunks of work that can complete quickly. Most serverless\nplatforms require individual functions to complete within a few minutes. Azure Functions defaults to a\n5-minute time-out duration, which can be configured up to 10 minutes. The Azure Functions premium\nplan can mitigate this issue as well, defaulting time-outs to 30 minutes w', 'ith an unbounded higher\nlimit that can be configured. Compute time isn’t calendar time. More advanced functions using the\n[Azure Durable Functions framework may pause execution over a course of several days. The billing is](https://docs.microsoft.com/azure/azure-functions/durable/durable-functions-overview?tabs=csharp)\nbased on actual execution time - when the function wakes up and resumes processing.\n\n\nFinally, leveraging Azure Functions for app', 'lication tasks adds complexity. It’s wise to first architect\nyour application with a modular, loosely coupled design. Then, identify if there are benefits serverless\nwould offer that justify the additional complexity.\n\n### Combining containers and serverless approaches\n\n\nCloud-native applications typically implement services leveraging containers and orchestration. There\nare often opportunities to expose some of the application’s services as Azur', 'e Functions. However,\nwith a cloud-native app deployed to Kubernetes, it would be nice to leverage Azure Functions within\nthis same toolset. Fortunately, you can wrap Azure Functions inside Docker containers and deploy\nthem using the same processes and tools as the rest of your Kubernetes-based app.\n\n#### **When does it make sense to use containers with serverless?**\n\n\nYour Azure Function has no knowledge of the platform on which it’s deployed. F', 'or some scenarios,\nyou may have specific requirements and need to customize the environment on which your function\ncode will run. You’ll need a custom image that supports dependencies or a configuration not\nsupported by the default image. In these cases, it makes sense to deploy your function in a custom\nDocker container.\n\n#### **When should you avoid using containers with Azure Functions?**\n\n\nIf you want to use consumption billing, you can’t run', ' your function in a container. What’s more, if you\ndeploy your function to a Kubernetes cluster, you’ll no longer benefit from the built-in scaling\nprovided by Azure Functions. You’ll need to use Kubernetes’ scaling features, described earlier in this\nchapter.\n\n#### **How to combine serverless and Docker containers**\n\n\n[To wrap an Azure Function in a Docker container, install the Azure Functions Core Tools and then run](https://github.com/Azure/a', 'zure-functions-core-tools)\nthe following command:\n\n```\nfunc init ProjectName --worker-runtime dotnet --docker\n\n```\n\nWhen the project is created, it will include a Dockerfile and the worker runtime configured to dotnet.\nNow, you can create and test your function locally. Build and run it using the docker build and docker\n\n\n49 CHAPTER 3 | Scaling cloud-native applications\n\n\nrun commands. For detailed steps to get started building Azure Functions wi', 'th Docker support, see\n[the Create a function on Linux using a custom image tutorial.](https://docs.microsoft.com/azure/azure-functions/functions-create-function-linux-custom-image)\n\n#### **How to combine serverless and Kubernetes with KEDA**\n\n\nIn this chapter, you’ve seen that the Azure Functions’ platform automatically scales out to meet\ndemand. When deploying containerized functions to AKS, however, you lose the built-in scaling\n[functionality', '. To the rescue comes Kubernetes-based Event Driven (KEDA). It enables fine-grained](https://docs.microsoft.com/azure/azure-functions/functions-kubernetes-keda)\nautoscaling for event-driven Kubernetes workloads, including containerized functions.\n\n\nKEDA provides event-driven scaling functionality to the Functions’ runtime in a Docker container.\nKEDA can scale from zero instances (when no events are occurring) out to n instances, based on load.\nIt', ' enables autoscaling by exposing custom metrics to the Kubernetes autoscaler (Horizontal Pod\nAutoscaler). Using Functions containers with KEDA makes it possible to replicate serverless function\ncapabilities in any Kubernetes cluster.\n\n\nIt’s worth noting that the KEDA project is now managed by the Cloud Native Computing Foundation\n(CNCF).\n\n### Deploying containers in Azure\n\n\nWe’ve discussed containers in this chapter and in chapter 1. We’ve seen t', 'hat containers provide many\nbenefits to cloud-native applications, including portability. In the Azure cloud, you can deploy the\nsame containerized services across staging and production environments. Azure provides several\noptions for hosting these containerized workloads:\n\n\n  - Azure Kubernetes Services (AKS)\n\n  - Azure Container Instance (ACI)\n\n  - Azure Web Apps for Containers\n\n#### **Azure Container Registry**\n\n\nWhen containerizing a microse', 'rvice, you first build a container “image.” The image is a binary\nrepresentation of the service code, dependencies, and runtime. While you can manually create an\nimage using the Docker Build command from the Docker API, a better approach is to create it as part\nof an automated build process.\n\n\nOnce created, container images are stored in container registries. They enable you to build, store, and\nmanage container images. There are many registries ', 'available, both public and private. Azure\nContainer Registry (ACR) is a fully managed container registry service in the Azure cloud. It persists\nyour images inside the Azure network, reducing the time to deploy them to Azure container hosts.\nYou can also secure them using the same security and identity procedures that you use for other\nAzure resources.\n\n\n[You create an Azure Container Registry using the Azure portal,](https://docs.microsoft.com/a', 'zure/container-registry/container-registry-get-started-portal) [Azure CLI, or PowerShell tools.](https://docs.microsoft.com/azure/container-registry/container-registry-get-started-azure-cli)\nCreating a registry in Azure is simple. It requires an Azure subscription, resource group, and a unique\n\n\n50 CHAPTER 3 | Scaling cloud-native applications\n\n\nname. Figure 3-10 shows the basic options for creating a registry, which will be hosted at\nregistrynam', 'e.azurecr.io.\n\n\n_Figure 3-10. Create container registry_\n\n\nOnce you’ve created the registry, you’ll need to authenticate with it before you can use it. Typically,\nyou’ll log into the registry using the Azure CLI command:\n\n```\naz acr login --name *registryname*\n\n```\n\nOnce authenticated, you can use docker commands to push container images to it. Before you can do\nso, however, you must tag your image with the fully qualified name (URL) of your ACR ', 'login server. It\nwill have the format _registryname_ .azurecr.io.\n\n```\ndocker tag mycontainer myregistry.azurecr.io/mycontainer:v1\n\n```\n\nAfter you’ve tagged the image, you use the docker push command to push the image to your ACR\ninstance.\n\n```\ndocker push myregistry.azurecr.io/mycontainer:v1\n\n```\n\n51 CHAPTER 3 | Scaling cloud-native applications\n\n\nAfter you push an image to the registry, it’s a good idea to remove the image from your local Docke', 'r\nenvironment, using this command:\n\n```\ndocker rmi myregistry.azurecr.io/mycontainer:v1\n\n```\n\nAs a best practice, you shouldn’t manually push images to a container registry. Instead, use a build\npipeline defined in a tool like GitHub or Azure DevOps. Learn more in the Cloud-Native DevOps\nchapter.\n\n#### **ACR Tasks**\n\n\n[ACR Tasks](https://docs.microsoft.com/azure/container-registry/container-registry-tasks-overview) [is a set of features available', ' from the Azure Container Registry. It extends your inner-loop](https://docs.microsoft.com/dotnet/architecture/containerized-lifecycle/design-develop-containerized-apps/docker-apps-inner-loop-workflow)\n[development cycle](https://docs.microsoft.com/dotnet/architecture/containerized-lifecycle/design-develop-containerized-apps/docker-apps-inner-loop-workflow) by building and managing container images in the Azure cloud. Instead of\ninvoking a docker', ' build and docker push locally on your development machine, they’re automatically\nhandled by ACR Tasks in the cloud.\n\n\nThe following AZ CLI command both builds a container image and pushes it to ACR:\n\n```\n# create a container registry\naz acr create --resource-group myResourceGroup --name myContainerRegistry008 --sku\nBasic\n\n# build container image in ACR and push it into your container registry\naz acr build --image sample/hello-world:v1 --registry', ' myContainerRegistry008 --file\nDockerfile .\n\n```\n\nAs you can see from the previous command block, there’s no need to install Docker Desktop on your\ndevelopment machine. Additionally, you can configure ACR Task triggers to rebuild containers images\non both source code and base image updates.\n\n#### **Azure Kubernetes Service**\n\n\nWe discussed Azure Kubernetes Service (AKS) at length in this chapter. We’ve seen that it’s the de\nfacto container orches', 'trator managing containerized cloud-native applications.\n\n\nOnce you deploy an image to a registry, such as ACR, you can configure AKS to automatically pull and\n[deploy it. With a CI/CD pipeline in place, you might configure a canary release](https://martinfowler.com/bliki/CanaryRelease.html) strategy to minimize\nthe risk involved when rapidly deploying updates. The new version of the app is initially configured in\nproduction with no traffic route', 'd to it. Then, the system will route a small percentage of users to the\nnewly deployed version. As the team gains confidence in the new version, it can roll out more\ninstances and retire the old. AKS easily supports this style of deployment.\n\n\nAs with most resources in Azure, you can create an Azure Kubernetes Service cluster using the portal,\ncommand-line, or automation tools like Helm or Terraform. To get started with a new cluster, you\nneed to', ' provide the following information:\n\n\n  - Azure subscription\n\n  - Resource group\n\n  - Kubernetes cluster name\n\n  - Region\n\n\n52 CHAPTER 3 | Scaling cloud-native applications\n\n\n  - Kubernetes version\n\n  - DNS name prefix\n\n  - Node size\n\n  - Node count\n\nThis information is sufficient to get started. As part of the creation process in the Azure portal, you can\nalso configure options for the following features of your cluster:\n\n\n  - Scale\n\n  - Authent', 'ication\n\n  - Networking\n\n  - Monitoring\n\n  - Tags\n\n[This quickstart walks through deploying an AKS cluster using the Azure portal.](https://docs.microsoft.com/azure/aks/kubernetes-walkthrough-portal)\n\n#### **Azure Bridge to Kubernetes**\n\n\nCloud-native applications can grow large and complex, requiring significant compute resources to\nrun. In these scenarios, the entire application can’t be hosted on a development machine (especially a\n[laptop). A', 'zure Bridge to Kubernetes addresses the shortcoming. It enables developers to work with a](https://docs.microsoft.com/visualstudio/bridge/overview-bridge-to-kubernetes)\nlocal version of their service while hosting the entire application in an AKS development cluster.\n\n\nWhen ready, developers test their changes locally while running against the full application in the AKS\ncluster - without replicating dependencies. Under the hood, the bridge merge', 's code from the local\nmachine with services in AKS. Developers can rapidly iterate and debug code directly in Kubernetes\nusing Visual Studio or Visual Studio Code.\n\n\nGabe Monroy, former VP of Product Management at Microsoft, describes it well:\n\n\nImagine you’re a new employee trying to fix a bug in a complex microservices application consisting\nof dozens of components, each with their own configuration and backing services. To get started, you\nmus', 't configure your local development environment so that it can mimic production including setting\nup your IDE, building tool chain, containerized service dependencies, a local Kubernetes environment,\nmocks for backing services, and more. With all the time involved setting up your development\nenvironment, fixing that first bug could take days! Or you could just use Bridge to Kubernetes and\nAKS.\n\n### Scaling containers and serverless applications\n\n\n', 'There are two ways to scale an application: up or out. The former refers to adding capacity to a single\nresource, while the latter refers to adding more resources to increase capacity.\n\n#### **The simple solution: scaling up**\n\n\nUpgrading an existing host server with increased CPU, memory, disk I/O speed, and network I/O\nspeed is known as _scaling up_ . Scaling up a cloud-native application involves choosing more capable\n\n\n53 CHAPTER 3 | Scaling ', 'cloud-native applications\n\n\nresources from the cloud vendor. For example, you can create a new node pool with larger VMs in\nyour Kubernetes cluster. Then, migrate your containerized services to the new pool.\n\n\n[Serverless apps scale up by choosing the premium Functions plan](https://docs.microsoft.com/azure/azure-functions/functions-scale) or premium instance sizes from a\ndedicated app service plan.\n\n#### **Scaling out cloud-native apps**\n\n\nCloud', '-native applications often experience large fluctuations in demand and require scale on a\nmoment’s notice. They favor scaling out. Scaling out is done horizontally by adding additional\nmachines (called nodes) or application instances to an existing cluster. In Kubernetes, you can scale\n[manually by adjusting configuration settings for the app (for example, scaling a node pool), or](https://docs.microsoft.com/azure/aks/use-multiple-node-pools#scal', 'e-a-node-pool-manually)\nthrough autoscaling.\n\n\nAKS clusters can autoscale in one of two ways:\n\n\n[First, the Horizontal Pod Autoscaler monitors resource demand and automatically scales your POD](https://docs.microsoft.com/azure/aks/tutorial-kubernetes-scale#autoscale-pods)\nreplicas to meet it. When traffic increases, additional replicas are automatically provisioned to scale\nout your services. Likewise, when demand decreases, they’re removed to sc', 'ale-in your services. You\ndefine the metric on which to scale, for example, CPU usage. You can also specify the minimum and\nmaximum number of replicas to run. AKS monitors that metric and scales accordingly.\n\n\n[Next, the AKS Cluster Autoscaler feature enables you to automatically scale compute nodes across a](https://docs.microsoft.com/azure/aks/cluster-autoscaler)\nKubernetes cluster to meet demand. With it, you can automatically add new VMs to t', 'he underlying\nAzure Virtual Machine Scale Set whenever more compute capacity of is required. It also removes\nnodes when no longer required.\n\n\nFigure 3-11 shows the relationship between these two scaling services.\n\n\n_Figure 3-11. Scaling out an App Service plan._\n\n\n54 CHAPTER 3 | Scaling cloud-native applications\n\n\nWorking together, both ensure an optimal number of container instances and compute nodes to\nsupport fluctuating demand. The horizontal', ' pod autoscaler optimizes the number of pods required.\nThe cluster autoscaler optimizes the number of nodes required.\n\n\n**Scaling Azure Functions**\n\n\nAzure Functions automatically scale out upon demand. Server resources are dynamically allocated and\nremoved based on the number of triggered events. You’re only charged for compute resources\nconsumed when your functions run. Billing is based upon the number of executions, execution time,\nand memory ', 'used.\n\n\nWhile the default consumption plan provides an economical and scalable solution for most apps, the\npremium option allows developers flexibility for custom Azure Functions requirements. Upgrading to\nthe premium plan provides control over instance sizes, pre-warmed instances (to avoid cold start\ndelays), and dedicated VMs.\n\n### Other container deployment options\n\n\nAside from Azure Kubernetes Service (AKS), you can also deploy containers to ', 'Azure App Service for\nContainers and Azure Container Instances.\n\n#### **When does it make sense to deploy to App Service for Containers?**\n\n\nSimple production applications that don’t require orchestration are well suited to Azure App Service\nfor Containers.\n\n#### **How to deploy to App Service for Containers**\n\n\n[To deploy to Azure App Service for Containers, you’ll need an Azure Container Registry (ACR) instance](https://azure.microsoft.com/serv', 'ices/app-service/containers/)\nand credentials to access it. Push your container image to the ACR repository so that your Azure App\nService can pull it when needed. Once complete, you can configure the app for Continuous\nDeployment. Doing so will automatically deploy updates whenever the image changes in ACR.\n\n#### **When does it make sense to deploy to Azure Container Instances?**\n\n\n[Azure Container Instances (ACI) enables you to run Docker conta', 'iners in a managed, serverless cloud](https://azure.microsoft.com/services/container-instances/)\nenvironment, without having to set up virtual machines or clusters. It’s a great solution for shortrunning workloads that can run in an isolated container. Consider ACI for simple services, testing\nscenarios, task automation, and build jobs. ACI spins-up a container instance, performs the task, and\nthen spins it down.\n\n#### **How to deploy an app to A', 'zure Container Instances**\n\n\n[To deploy to Azure Container Instances (ACI), you need an Azure Container Registry (ACR) and](https://docs.microsoft.com/azure/container-instances/)\ncredentials for accessing it. Once you push your container image to the repository, it’s available to pull\ninto ACI. You can work with ACI using the Azure portal or command-line interface. ACR provides tight\nintegration with ACI. Figure 3-12 shows how to push an individu', 'al container image to ACR.\n\n\n55 CHAPTER 3 | Scaling cloud-native applications\n\n\n_Figure 3-12. Azure Container Registry Run Instance_\n\n\nCreating an instance in ACI can be done quickly. Specify the image registry, Azure resource group\n[information, the amount of memory to allocate, and the port on which to listen. This quickstart shows](https://docs.microsoft.com/azure/container-instances/container-instances-quickstart-portal)\n[how to deploy a cont', 'ainer instance to ACI using the Azure portal.](https://docs.microsoft.com/azure/container-instances/container-instances-quickstart-portal)\n\n\nOnce the deployment completes, find the newly deployed container’s IP address and communicate\nwith it over the port you specified.\n\n\nAzure Container Instances offers the fastest way to run simple container workloads in Azure. You don’t\nneed to configure an app service, orchestrator, or virtual machine. For s', 'cenarios where you require full\ncontainer orchestration, service discovery, automatic scaling, or coordinated upgrades, we\nrecommend Azure Kubernetes Service (AKS).\n\n#### **References**\n\n\n  - [What is Kubernetes?](https://blog.newrelic.com/engineering/what-is-kubernetes/)\n\n  - [Installing Kubernetes with Minikube](https://kubernetes.io/docs/setup/learning-environment/minikube/)\n\n  - [MiniKube vs Docker Desktop](https://medium.com/containers-101/l', 'ocal-kubernetes-for-windows-minikube-vs-docker-desktop-25a1c6d3b766)\n\n  - [Visual Studio Tools for Docker](https://docs.microsoft.com/dotnet/standard/containerized-lifecycle-architecture/design-develop-containerized-apps/visual-studio-tools-for-docker)\n\n\n56 CHAPTER 3 | Scaling cloud-native applications\n\n\n  - [Understanding serverless cold start](https://azure.microsoft.com/blog/understanding-serverless-cold-start/)\n\n  - [Pre-warmed Azure Function', 's instances](https://docs.microsoft.com/azure/azure-functions/functions-premium-plan#pre-warmed-instances)\n\n  - [Create a function on Linux using a custom image](https://docs.microsoft.com/azure/azure-functions/functions-create-function-linux-custom-image)\n\n  - [Run Azure Functions in a Docker Container](https://markheath.net/post/azure-functions-docker)\n\n  - [Create a function on Linux using a custom image](https://docs.microsoft.com/azure/azure', '-functions/functions-create-function-linux-custom-image)\n\n  - [Azure Functions with Kubernetes Event Driven Autoscaling](https://docs.microsoft.com/azure/azure-functions/functions-kubernetes-keda)\n\n  - [Canary Release](https://martinfowler.com/bliki/CanaryRelease.html)\n\n  - [Azure Dev Spaces with VS Code](https://docs.microsoft.com/azure/dev-spaces/quickstart-netcore)\n\n  - [Azure Dev Spaces with Visual Studio](https://docs.microsoft.com/azure/dev', '-spaces/quickstart-netcore-visualstudio)\n\n  - [AKS Multiple Node Pools](https://docs.microsoft.com/azure/aks/use-multiple-node-pools)\n\n  - [AKS Cluster Autoscaler](https://docs.microsoft.com/azure/aks/cluster-autoscaler)\n\n  - [Tutorial: Scale applications in AKS](https://docs.microsoft.com/azure/aks/tutorial-kubernetes-scale)\n\n  - [Azure Functions scale and hosting](https://docs.microsoft.com/azure/azure-functions/functions-scale)\n\n  - [Azure Con', 'tainer Instances Docs](https://docs.microsoft.com/azure/container-instances/)\n\n  - [Deploy Container Instance from ACR](https://docs.microsoft.com/azure/container-instances/container-instances-using-azure-container-registry#deploy-with-azure-portal)\n\n\n57 CHAPTER 3 | Scaling cloud-native applications\n\n\n**CHAPTER**\n# 4\n\n## Cloud-native communication patterns\n\n\nWhen constructing a cloud-native system, communication becomes a significant design decis', 'ion. How\ndoes a front-end client application communicate with a back-end microservice? How do back-end\nmicroservices communicate with each other? What are the principles, patterns, and best practices to\nconsider when implementing communication in cloud-native applications?\n\n### Communication considerations\n\n\nIn a monolithic application, communication is straightforward. The code modules execute together in\nthe same executable space (process) on a', ' server. This approach can have performance advantages as\neverything runs together in shared memory, but results in tightly coupled code that becomes difficult\nto maintain, evolve, and scale.\n\n\nCloud-native systems implement a microservice-based architecture with many small, independent\nmicroservices. Each microservice executes in a separate process and typically runs inside a container\nthat is deployed to a _cluster_ .\n\n\nA cluster groups a pool ', 'of virtual machines together to form a highly available environment. They’re\nmanaged with an orchestration tool, which is responsible for deploying and managing the\n[containerized microservices. Figure 4-1 shows a Kubernetes](https://kubernetes.io/) cluster deployed into the Azure cloud\n[with the fully managed Azure Kubernetes Services.](https://docs.microsoft.com/azure/aks/intro-kubernetes)\n\n\n58 CHAPTER 4 | Cloud-native communication patterns\n\n\n', '_Figure 4-1. A Kubernetes cluster in Azure_\n\n\nAcross the cluster, microservices communicate with each other through APIs and messaging\ntechnologies.\n\n\nWhile they provide many benefits, microservices are no free lunch. Local in-process method calls\nbetween components are now replaced with network calls. Each microservice must communicate over\na network protocol, which adds complexity to your system:\n\n\n  - Network congestion, latency, and transient', ' faults are a constant concern.\n\n\n  - Resiliency (that is, retrying failed requests) is essential.\n\n\n  - [Some calls must be idempotent](https://www.restapitutorial.com/lessons/idempotency.html) as to keep consistent state.\n\n\n  - Each microservice must authenticate and authorize calls.\n\n\n  - Each message must be serialized and then deserialized - which can be expensive.\n\n\n  - Message encryption/decryption becomes important.\n\n\n[The book .NET Micro', 'services: Architecture for Containerized .NET Applications, available for free from](https://dotnet.microsoft.com/download/thank-you/microservices-architecture-ebook)\nMicrosoft, provides an in-depth coverage of communication patterns for microservice applications. In\nthis chapter, we provide a high-level overview of these patterns along with implementation options\navailable in the Azure cloud.\n\n\nIn this chapter, we’ll first address communication ', 'between front-end applications and back-end\nmicroservices. We’ll then look at back-end microservices communicate with each other. We’ll explore\n\n\n59 CHAPTER 4 | Cloud-native communication patterns\n\n\nthe up and gRPC communication technology. Finally, we’ll look new innovative communication\npatterns using service mesh technology. We’ll also see how the Azure cloud provides different kinds\nof _backing services_ to support cloud-native communication.', '\n\n### Front-end client communication\n\n\nIn a cloud-native system, front-end clients (mobile, web, and desktop applications) require a\ncommunication channel to interact with independent back-end microservices.\n\n\nWhat are the options?\n\n\nTo keep things simple, a front-end client could _directly communicate_ with the back-end microservices,\nshown in Figure 4-2.\n\n\n_Figure 4-2. Direct client to service communication_\n\n\nWith this approach, each microserv', 'ice has a public endpoint that is accessible by front-end clients. In\na production environment, you’d place a load balancer in front of the microservices, routing traffic\nproportionately.\n\n\nWhile simple to implement, direct client communication would be acceptable only for simple\nmicroservice applications. This pattern tightly couples front-end clients to core back-end services,\nopening the door for many problems, including:\n\n\n  - Client suscepti', 'bility to back-end service refactoring.\n\n  - A wider attack surface as core back-end services are directly exposed.\n\n  - Duplication of cross-cutting concerns across each microservice.\n\n  - Overly complex client code - clients must keep track of multiple endpoints and handle failures\nin a resilient way.\n\n\n60 CHAPTER 4 | Cloud-native communication patterns\n\n\n[Instead, a widely accepted cloud design pattern is to implement an API Gateway Service be', 'tween the](https://docs.microsoft.com/dotnet/architecture/microservices/architect-microservice-container-applications/direct-client-to-microservice-communication-versus-the-api-gateway-pattern)\nfront-end applications and back-end services. The pattern is shown in Figure 4-3.\n\n\n_Figure 4-3. API gateway pattern_\n\n\nIn the previous figure, note how the API Gateway service abstracts the back-end core microservices.\nImplemented as a web API, it acts as', ' a _reverse proxy_, routing incoming traffic to the internal\nmicroservices.\n\n\nThe gateway insulates the client from internal service partitioning and refactoring. If you change a\nback-end service, you accommodate for it in the gateway without breaking the client. It’s also your\nfirst line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and\nthrottling. Many of these cross-cutting concerns can be off-loaded f', 'rom the back-end core services to\nthe gateway, simplifying the back-end services.\n\n\nCare must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of\nthe gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger\nsystems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or\n[back-end functionality. The Backend for Frontends](https://docs.micros', 'oft.com/azure/architecture/patterns/backends-for-frontends) pattern provides direction for implementing\nmultiple gateways. The pattern is shown in Figure 4-4.\n\n\n61 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-4. Backend for frontend pattern_\n\n\nNote in the previous figure how incoming traffic is sent to a specific API gateway - based upon client\ntype: web, mobile, or desktop app. This approach makes sense as the capabilities of each', ' device differ\nsignificantly across form factor, performance, and display limitations. Typically mobile applications\nexpose less functionality than a browser or desktop applications. Each gateway can be optimized to\nmatch the capabilities and functionality of the corresponding device.\n\n#### **Simple Gateways**\n\n\nTo start, you could build your own API Gateway service. A quick search of GitHub will provide many\nexamples.\n\n\n[For simple .NET cloud-na', 'tive applications, you might consider the Ocelot Gateway. Open source and](https://github.com/ThreeMammals/Ocelot)\ncreated for .NET microservices, it’s lightweight, fast, scalable. Like any API Gateway, its primary\nfunctionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a\nwide variety of capabilities that are configurable in a .NET middleware pipeline.\n\n\n[YARP](https://github.com/microsoft/reverse-prox', 'y) (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft\nproduct teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as\n[middleware and is highly customizable. You’ll find YARP well-documented](https://microsoft.github.io/reverse-proxy/articles/getting-started.html) with various usage\nexamples.\n\n\nFor enterprise cloud-native applications, there are several managed Azure services that', ' can help\njump-start your efforts.\n\n\n62 CHAPTER 4 | Cloud-native communication patterns\n\n\n#### **Azure Application Gateway**\n\n[For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure](https://docs.microsoft.com/azure/application-gateway/overview)\n[PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web](https://azure.microsoft.com/overview/what-is-paas/)\n[Applicat', 'ion Firewall. The service supports Layer-7 load balancing](https://www.nginx.com/resources/glossary/layer-7-load-balancing/) capabilities. With Layer 7, you can\nroute requests based on the actual content of an HTTP message, not just low-level TCP network\npackets.\n\n\n[Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container](https://www.infoworld.com/article/3268073/what-is-kubernetes-your-next-application-platfor', 'm.html)\norchestrator, Kubernetes automates the deployment, scaling, and operational concerns of\n[containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure](https://azure.microsoft.com/services/kubernetes-service/)\n[Kubernetes Service](https://azure.microsoft.com/services/kubernetes-service/) cluster.\n\n\n[The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with](https:', '//azure.github.io/application-gateway-kubernetes-ingress/)\n[Azure Kubernetes Service. Figure 4.5 shows the architecture.](https://azure.microsoft.com/services/kubernetes-service/)\n\n\n_Figure 4-5. Application Gateway Ingress Controller_\n\n\n[Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress.](https://kubernetes.io/docs/concepts/services-networking/ingress/)\nIngress defines a set of rules for how micros', 'ervice instances inside AKS can be exposed to the outside\nworld. In the previous image, the ingress controller interprets the ingress rules configured for the\ncluster and automatically configures the Azure Application Gateway. Based on those rules, the\nApplication Gateway routes traffic to microservices running inside AKS. The ingress controller listens\nfor changes to ingress rules and makes the appropriate changes to the Azure Application Gatewa', 'y.\n\n#### **Azure API Management**\n\n\n[For moderate to large-scale cloud-native systems, you may consider Azure API Management. It’s a](https://azure.microsoft.com/services/api-management/)\ncloud-based service that not only solves your API Gateway needs, but provides a full-featured\ndeveloper and administrative experience. API Management is shown in Figure 4-6.\n\n\n63 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-6. Azure API Management', '_\n\n\nTo start, API Management exposes a gateway server that allows controlled access to back-end\nservices based upon configurable rules and policies. These services can be in the Azure cloud, your\non-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All\ntraffic is logged for analytical purposes.\n\n\nFor developers, API Management offers a developer portal that provides access to services,\ndocumentation, and', ' sample code for invoking them. Developers can use Swagger/Open API to\ninspect service endpoints and analyze their usage. The service works across the major development\nplatforms: .NET, Java, Golang, and more.\n\n\nThe publisher portal exposes a management dashboard where administrators expose APIs and\nmanage their behavior. Service access can be granted, service health monitored, and service telemetry\ngathered. Administrators apply _policies_ [to e', 'ach endpoint to affect behavior. Policies](https://docs.microsoft.com/azure/api-management/api-management-howto-policies) are pre-built\nstatements that execute sequentially for each service call. Policies are configured for an inbound call,\noutbound call, or invoked upon an error. Policies can be applied at different service scopes as to\nenable deterministic ordering when combining policies. The product ships with a large number of\n[prebuilt poli', 'cies.](https://docs.microsoft.com/azure/api-management/api-management-policies)\n\n\nHere are examples of how policies can affect the behavior of your cloud-native services:\n\n\n  - Restrict service access.\n\n  - Enforce authentication.\n\n\n  - Throttle calls from a single source, if necessary.\n\n  - Enable caching.\n\n  - Block calls from specific IP addresses.\n\n\n64 CHAPTER 4 | Cloud-native communication patterns\n\n\n  - Control the flow of the service.\n\n  -', ' Convert requests from SOAP to REST or between different data formats, such as from XML to\nJSON.\n\n\nAzure API Management can expose back-end services that are hosted anywhere – in the cloud or your\ndata center. For legacy services that you may expose in your cloud-native systems, it supports both\nREST and SOAP APIs. Even other Azure services can be exposed through API Management. You could\n[place a managed API on top of an Azure backing service li', 'ke Azure Service Bus or Azure Logic Apps.](https://azure.microsoft.com/services/service-bus/)\nAzure API Management doesn’t include built-in load-balancing support and should be used in\nconjunction with a load-balancing service.\n\n\n[Azure API Management is available across four different tiers:](https://azure.microsoft.com/pricing/details/api-management/)\n\n\n  - Developer\n\n  - Basic\n\n  - Standard\n\n  - Premium\n\n\nThe Developer tier is meant for non-pr', 'oduction workloads and evaluation. The other tiers offer\nprogressively more power, features, and higher service level agreements (SLAs). The Premium tier\n[provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.](https://docs.microsoft.com/azure/virtual-network/virtual-networks-overview)\n\n\n[The Azure cloud also offers a serverless tier](https://azure.microsoft.com/blog/announcing-azure-api-management-for-ser', 'verless-architectures/) for Azure API Management. Referred to as the\n_consumption pricing tier_, the service is a variant of API Management designed around the serverless\ncomputing model. Unlike the “pre-allocated” pricing tiers previously shown, the consumption tier\nprovides instant provisioning and pay-per-action pricing.\n\n\nIt enables API Gateway features for the following use cases:\n\n\n  - [Microservices implemented using serverless technologie', 's such as Azure Functions and Azure](https://docs.microsoft.com/azure/azure-functions/functions-overview)\n[Logic Apps.](https://azure.microsoft.com/services/logic-apps/)\n\n  - Azure backing service resources such as Service Bus queues and topics, Azure storage, and\nothers.\n\n  - Microservices where traffic has occasional large spikes but remains low most the time.\n\n\nThe consumption tier uses the same underlying service API Management components, bu', 't employs\nan entirely different architecture based on dynamically allocated resources. It aligns perfectly with the\nserverless computing model:\n\n\n  - No infrastructure to manage.\n\n  - No idle capacity.\n\n  - High-availability.\n\n  - Automatic scaling.\n\n  - Cost is based on actual usage.\n\nThe new consumption tier is a great choice for cloud-native systems that expose serverless resources\nas APIs.\n\n\n65 CHAPTER 4 | Cloud-native communication patterns\n', '\n\n#### **Real-time communication**\n\nReal-time, or push, communication is another option for front-end applications that communicate\nwith back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online\neducation, gaming, and job-progress updates, require instantaneous, real-time responses from the\nback-end. With normal HTTP communication, there’s no way for the client to know when new data is\navailable. The client must con', 'tinually _poll_ or send requests to the server. With _real-time_\ncommunication, the server can push new data to the client at any time.\n\n\nReal-time systems are often characterized by high-frequency data flows and large numbers of\nconcurrent client connections. Manually implementing real-time connectivity can quickly become\ncomplex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected\nclients. You could fi', 'nd yourself managing an instance of Azure Redis Cache and a set of load\nbalancers configured with sticky sessions for client affinity.\n\n\n[Azure SignalR Service](https://azure.microsoft.com/services/signalr-service/) is a fully managed Azure service that simplifies real-time communication for\nyour cloud-native applications. Technical implementation details like capacity provisioning, scaling,\nand persistent connections are abstracted away. They’re', ' handled for you with a 99.9% service-level\nagreement. You focus on application features, not infrastructure plumbing.\n\n\nOnce enabled, a cloud-based HTTP service can push content updates directly to connected clients,\nincluding browser, mobile and desktop applications. Clients are updated without the need to poll the\nserver. Azure SignalR abstracts the transport technologies that create real-time connectivity, including\nWebSockets, Server-Side Ev', 'ents, and Long Polling. Developers focus on sending messages to all or\nspecific subsets of connected clients.\n\n\nFigure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR\nenabled.\n\n\n66 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-7. Azure SignalR_\n\n\nAnother advantage of Azure SignalR Service comes with implementing Serverless cloud-native\nservices. Perhaps your code is executed on demand with', ' Azure Functions triggers. This scenario can\nbe tricky because your code doesn’t maintain long connections with clients. Azure SignalR Service can\nhandle this situation since the service already manages connections for you.\n\n\nAzure SignalR Service closely integrates with other Azure services, such as Azure SQL Database,\nService Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.\n\n### Service-to-service communica', 'tion\n\n\nMoving from the front-end client, we now address back-end microservices communicate with each\nother.\n\n\nWhen constructing a cloud-native application, you’ll want to be sensitive to how back-end services\ncommunicate with each other. Ideally, the less inter-service communication, the better. However,\navoidance isn’t always possible as back-end services often rely on one another to complete an\noperation.\n\n\nThere are several widely accepted app', 'roaches to implementing cross-service communication. The _type_\n_of communication interaction_ will often determine the best approach.\n\n\nConsider the following interaction types:\n\n\n  - _Query_  - when a calling microservice requires a response from a called microservice, such as,\n“Hey, give me the buyer information for a given customer Id.”\n\n\n67 CHAPTER 4 | Cloud-native communication patterns\n\n\n  - _Command_  - when the calling microservice needs', ' another microservice to execute an action\nbut doesn’t require a response, such as, “Hey, just ship this order.”\n\n\n  - _Event_  - when a microservice, called the publisher, raises an event that state has changed or an\naction has occurred. Other microservices, called subscribers, who are interested, can react to\nthe event appropriately. The publisher and the subscribers aren’t aware of each other.\n\n\nMicroservice systems typically use a combination', ' of these interaction types when executing\noperations that require cross-service interaction. Let’s take a close look at each and how you might\nimplement them.\n\n#### **Queries**\n\n\nMany times, one microservice might need to _query_ another, requiring an immediate response to\ncomplete an operation. A shopping basket microservice may need product information and a price to\nadd an item to its basket. There are many approaches for implementing query o', 'perations.\n\n\n**Request/Response Messaging**\n\n\nOne option for implementing this scenario is for the calling back-end microservice to make direct\nHTTP requests to the microservices it needs to query, shown in Figure 4-8.\n\n\n_Figure 4-8. Direct HTTP communication_\n\n\nWhile direct HTTP calls between microservices are relatively simple to implement, care should be\ntaken to minimize this practice. To start, these calls are always _synchronous_ and will b', 'lock the\noperation until a result is returned or the request times outs. What were once self-contained,\nindependent services, able to evolve independently and deploy frequently, now become coupled to\neach other. As coupling among microservices increase, their architectural benefits diminish.\n\n\n68 CHAPTER 4 | Cloud-native communication patterns\n\n\nExecuting an infrequent request that makes a single direct HTTP call to another microservice might be\n', 'acceptable for some systems. However, high-volume calls that invoke direct HTTP calls to multiple\nmicroservices aren’t advisable. They can increase latency and negatively impact the performance,\nscalability, and availability of your system. Even worse, a long series of direct HTTP communication can\nlead to deep and complex chains of synchronous microservices calls, shown in Figure 4-9:\n\n\n_Figure 4-9. Chaining HTTP queries_\n\n\nYou can certainly ima', 'gine the risk in the design shown in the previous image. What happens if Step\n#3 fails? Or Step #8 fails? How do you recover? What if Step #6 is slow because the underlying service\nis busy? How do you continue? Even if all works correctly, think of the latency this call would incur,\nwhich is the sum of the latency of each step.\n\n\nThe large degree of coupling in the previous image suggests the services weren’t optimally modeled.\nIt would behoove t', 'he team to revisit their design.\n\n\n**Materialized View pattern**\n\n\n[A popular option for removing microservice coupling is the Materialized View pattern. With this](https://docs.microsoft.com/azure/architecture/patterns/materialized-view)\npattern, a microservice stores its own local, denormalized copy of data that’s owned by other services.\nInstead of the Shopping Basket microservice querying the Product Catalog and Pricing microservices,\nit main', 'tains its own local copy of that data. This pattern eliminates unnecessary coupling and\nimproves reliability and response time. The entire operation executes inside a single process. We\nexplore this pattern and other data concerns in Chapter 5.\n\n\n**Service Aggregator Pattern**\n\n\n[Another option for eliminating microservice-to-microservice coupling is an Aggregator microservice,](https://devblogs.microsoft.com/cesardelatorre/designing-and-implemen', 'ting-api-gateways-with-ocelot-in-a-microservices-and-container-based-architecture/)\nshown in purple in Figure 4-10.\n\n\n69 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-10. Aggregator microservice_\n\n\nThe pattern isolates an operation that makes calls to multiple back-end microservices, centralizing its\nlogic into a specialized microservice. The purple checkout aggregator microservice in the previous\nfigure orchestrates the workflow fo', 'r the Checkout operation. It includes calls to several back-end\nmicroservices in a sequenced order. Data from the workflow is aggregated and returned to the caller.\nWhile it still implements direct HTTP calls, the aggregator microservice reduces direct dependencies\namong back-end microservices.\n\n\n**Request/Reply Pattern**\n\n\n[Another approach for decoupling synchronous HTTP messages is a Request-Reply Pattern, which uses](https://www.enterpriseint', 'egrationpatterns.com/patterns/messaging/RequestReply.html)\nqueuing communication. Communication using a queue is always a one-way channel, with a producer\nsending the message and consumer receiving it. With this pattern, both a request queue and response\nqueue are implemented, shown in Figure 4-11.\n\n\n70 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-11. Request-reply pattern_\n\n\nHere, the message producer creates a query-based message', ' that contains a unique correlation ID and\nplaces it into a request queue. The consuming service dequeues the messages, processes it and places\nthe response into the response queue with the same correlation ID. The producer service dequeues\nthe message, matches it with the correlation ID and continues processing. We cover queues in detail\nin the next section.\n\n#### **Commands**\n\n\nAnother type of communication interaction is a _command_ . A micros', 'ervice may need another\nmicroservice to perform an action. The Ordering microservice may need the Shipping microservice to\ncreate a shipment for an approved order. In Figure 4-12, one microservice, called a Producer, sends a\nmessage to another microservice, the Consumer, commanding it to do something.\n\n\n71 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-12. Command interaction with a queue_\n\n\nMost often, the Producer doesn’t require a', ' response and can _fire-and-forget_ the message. If a reply is\nneeded, the Consumer sends a separate message back to Producer on another channel. A command\nmessage is best sent asynchronously with a message queue. supported by a lightweight message\nbroker. In the previous diagram, note how a queue separates and decouples both services.\n\n\nA message queue is an intermediary construct through which a producer and consumer pass a\nmessage. Queues impl', 'ement an asynchronous, point-to-point messaging pattern. The Producer\nknows where a command needs to be sent and routes appropriately. The queue guarantees that a\nmessage is processed by exactly one of the consumer instances that are reading from the channel. In\nthis scenario, either the producer or consumer service can scale out without affecting the other. As\nwell, technologies can be disparate on each side, meaning that we might have a Java mi', 'croservice\n[calling a Golang](https://golang.org/) microservice.\n\n\nIn chapter 1, we talked about _backing services_ . Backing services are ancillary resources upon which\ncloud-native systems depend. Message queues are backing services. The Azure cloud supports two\ntypes of message queues that your cloud-native systems can consume to implement command\nmessaging: Azure Storage Queues and Azure Service Bus Queues.\n\n\n**Azure Storage Queues**\n\n\nAzure ', 'storage queues offer a simple queueing infrastructure that is fast, affordable, and backed by\nAzure storage accounts.\n\n\n[Azure Storage Queues](https://docs.microsoft.com/azure/storage/queues/storage-queues-introduction) feature a REST-based queuing mechanism with reliable and persistent\nmessaging. They provide a minimal feature set, but are inexpensive and store millions of messages.\nTheir capacity ranges up to 500 TB. A single message can be up ', 'to 64 KB in size.\n\n\nYou can access messages from anywhere in the world via authenticated calls using HTTP or HTTPS.\nStorage queues can scale out to large numbers of concurrent clients to handle traffic spikes.\n\n\n72 CHAPTER 4 | Cloud-native communication patterns\n\n\nThat said, there are limitations with the service:\n\n\n  - Message order isn’t guaranteed.\n\n\n  - A message can only persist for seven days before it’s automatically removed.\n\n\n  - Support', ' for state management, duplicate detection, or transactions isn’t available.\n\n\nFigure 4-13 shows the hierarchy of an Azure Storage Queue.\n\n\n_Figure 4-13. Storage queue hierarchy_\n\n\nIn the previous figure, note how storage queues store their messages in the underlying Azure Storage\naccount.\n\n\nFor developers, Microsoft provides several client and server-side libraries for Storage queue\nprocessing. Most major platforms are supported including .NET, ', 'Java, JavaScript, Ruby, Python, and\nGo. Developers should never communicate directly with these libraries. Doing so will tightly couple\nyour microservice code to the Azure Storage Queue service. It’s a better practice to insulate the\nimplementation details of the API. Introduce an intermediation layer, or intermediate API, that exposes\ngeneric operations and encapsulates the concrete library. This loose coupling enables you to swap out\none queuin', 'g service for another without having to make changes to the mainline service code.\n\n\nAzure Storage queues are an economical option to implement command messaging in your cloudnative applications. Especially when a queue size will exceed 80 GB, or a simple feature set is\nacceptable. You only pay for the storage of the messages; there are no fixed hourly charges.\n\n\n**Azure Service Bus Queues**\n\n\nFor more complex messaging requirements, consider Azu', 're Service Bus queues.\n\n\n[Sitting atop a robust message infrastructure, Azure Service Bus](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview) supports a _brokered messaging model_ .\nMessages are reliably stored in a broker (the queue) until received by the consumer. The queue\nguarantees First-In/First-Out (FIFO) message delivery, respecting the order in which messages were\nadded to the queue.\n\n\nThe size of a me', 'ssage can be much larger, up to 256 KB. Messages are persisted in the queue for an\nunlimited period of time. Service Bus supports not only HTTP-based calls, but also provides full\n\n\n73 CHAPTER 4 | Cloud-native communication patterns\n\n\n[support for the AMQP protocol. AMQP is an open-standard across vendors that supports a binary](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-amqp-overview)\nprotocol and higher degrees of reliab', 'ility.\n\n\n[Service Bus provides a rich set of features, including transaction support](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-transactions) [and a duplicate detection](https://docs.microsoft.com/azure/service-bus-messaging/duplicate-detection)\n[feature. The queue guarantees “at most once delivery” per message. It automatically discards a](https://docs.microsoft.com/azure/service-bus-messaging/duplicate-detection)\nmessag', 'e that has already been sent. If a producer is in doubt, it can resend the same message, and\nService Bus guarantees that only one copy will be processed. Duplicate detection frees you from\nhaving to build additional infrastructure plumbing.\n\n\nTwo more enterprise features are partitioning and sessions. A conventional Service Bus queue is\n[handled by a single message broker and stored in a single message store. But, Service Bus Partitioning](https:', '//docs.microsoft.com/azure/service-bus-messaging/service-bus-partitioning)\nspreads the queue across multiple message brokers and message stores. The overall throughput is no\nlonger limited by the performance of a single message broker or messaging store. A temporary\noutage of a messaging store doesn’t render a partitioned queue unavailable.\n\n\n[Service Bus Sessions](https://codingcanvas.com/azure-service-bus-sessions/) provide a way to group-relat', 'ed messages. Imagine a workflow scenario where\nmessages must be processed together and the operation completed at the end. To take advantage,\nsessions must be explicitly enabled for the queue and each related messaged must contain the same\nsession ID.\n\n\nHowever, there are some important caveats: Service Bus queues size is limited to 80 GB, which is much\nsmaller than what’s available from store queues. Additionally, Service Bus queues incur a base', ' cost\nand charge per operation.\n\n\nFigure 4-14 outlines the high-level architecture of a Service Bus queue.\n\n\n_Figure 4-14. Service Bus queue_\n\n\nIn the previous figure, note the point-to-point relationship. Two instances of the same provider are\nenqueuing messages into a single Service Bus queue. Each message is consumed by only one of three\nconsumer instances on the right. Next, we discuss how to implement messaging where different\nconsumers may ', 'all be interested the same message.\n\n#### **Events**\n\n\nMessage queuing is an effective way to implement communication where a producer can\nasynchronously send a consumer a message. However, what happens when _many different consumers_\n\n\n74 CHAPTER 4 | Cloud-native communication patterns\n\n\nare interested in the same message? A dedicated message queue for each consumer wouldn’t scale\nwell and would become difficult to manage.\n\n\nTo address this scen', 'ario, we move to the third type of message interaction, the _event_ . One\nmicroservice announces that an action had occurred. Other microservices, if interested, react to the\n[action, or event. This is also known as the event-driven architectural style.](https://docs.microsoft.com/azure/architecture/guide/architecture-styles/event-driven)\n\n\nEventing is a two-step process. For a given state change, a microservice publishes an event to a\nmessage br', 'oker, making it available to any other interested microservice. The interested microservice\n[is notified by subscribing to the event in the message broker. You use the Publish/Subscribe pattern](https://docs.microsoft.com/azure/architecture/patterns/publisher-subscriber)\n[to implement event-based communication.](https://docs.microsoft.com/dotnet/standard/microservices-architecture/multi-container-microservice-net-applications/integration-event-ba', 'sed-microservice-communications)\n\n\nFigure 4-15 shows a shopping basket microservice publishing an event with two other microservices\nsubscribing to it.\n\n\n_Figure 4-15. Event-Driven messaging_\n\n\nNote the _event bus_ component that sits in the middle of the communication channel. It’s a custom\nclass that encapsulates the message broker and decouples it from the underlying application. The\nordering and inventory microservices independently operate t', 'he event with no knowledge of each\nother, nor the shopping basket microservice. When the registered event is published to the event bus,\nthey act upon it.\n\n\nWith eventing, we move from queuing technology to _topics_ [. A topic is similar to a queue, but supports](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-dotnet-how-to-use-topics-subscriptions)\na one-to-many messaging pattern. One microservice publishes a message. Multiple', ' subscribing\nmicroservices can choose to receive and act upon that message. Figure 4-16 shows a topic\narchitecture.\n\n\n75 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-16. Topic architecture_\n\n\nIn the previous figure, publishers send messages to the topic. At the end, subscribers receive\nmessages from subscriptions. In the middle, the topic forwards messages to subscriptions based on a\nset of rules, shown in dark blue boxes. Rules ac', 't as a filter that forward specific messages to a\nsubscription. Here, a “GetPrice” event would be sent to the price and logging subscriptions as the\nlogging subscription has chosen to receive all messages. A “GetInformation” event would be sent to\nthe information and logging subscriptions.\n\n\nThe Azure cloud supports two different topic services: Azure Service Bus Topics and Azure EventGrid.\n\n\n**Azure Service Bus Topics**\n\n\n[Sitting on top of the ', 'same robust brokered message model of Azure Service Bus queues are Azure](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-dotnet-how-to-use-topics-subscriptions)\n[Service Bus Topics. A topic can receive messages from multiple independent publishers and send](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-dotnet-how-to-use-topics-subscriptions)\nmessages to up to 2,000 subscribers. Subscriptions can be dynamic', 'ally added or removed at run time\nwithout stopping the system or recreating the topic.\n\n\nMany advanced features from Azure Service Bus queues are also available for topics, including\n[Duplicate Detection](https://docs.microsoft.com/azure/service-bus-messaging/duplicate-detection) [and Transaction support. By default, Service Bus topics are handled by a single](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-transactions)\n[messa', 'ge broker and stored in a single message store. But, Service Bus Partitioning](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-partitioning) scales a topic by\nspreading it across many message brokers and message stores.\n\n\n[Scheduled Message Delivery tags a message with a specific time for processing. The message won’t](https://docs.microsoft.com/azure/service-bus-messaging/message-sequencing)\n[appear in the topic before that ti', 'me. Message Deferral](https://docs.microsoft.com/azure/service-bus-messaging/message-deferral) enables you to defer a retrieval of a message\nto a later time. Both are commonly used in workflow processing scenarios where operations are\nprocessed in a particular order. You can postpone processing of received messages until prior work\nhas been completed.\n\n\nService Bus topics are a robust and proven technology for enabling publish/subscribe communica', 'tion\nin your cloud-native systems.\n\n\n**Azure Event Grid**\n\n\nWhile Azure Service Bus is a battle-tested messaging broker with a full set of enterprise features,\n[Azure Event Grid](https://docs.microsoft.com/azure/event-grid/overview) is the new kid on the block.\n\n\n76 CHAPTER 4 | Cloud-native communication patterns\n\n\nAt first glance, Event Grid may look like just another topic-based messaging system. However, it’s\ndifferent in many ways. Focused on', ' event-driven workloads, it enables real-time event processing,\ndeep Azure integration, and an open-platform - all on serverless infrastructure. It’s designed for\ncontemporary cloud-native and serverless applications\n\n\nAs a centralized _eventing backplane_, or pipe, Event Grid reacts to events inside Azure resources and\nfrom your own services.\n\n\nEvent notifications are published to an Event Grid Topic, which, in turn, routes each event to a\nsubsc', 'ription. Subscribers map to subscriptions and consume the events. Like Service Bus, Event Grid\nsupports a _filtered subscriber model_ where a subscription sets rule for the events it wishes to receive.\nEvent Grid provides fast throughput with a guarantee of 10 million events per second enabling near\nreal-time delivery - far more than what Azure Service Bus can generate.\n\n\nA sweet spot for Event Grid is its deep integration into the fabric of Azur', 'e infrastructure. An Azure\nresource, such as Cosmos DB, can publish built-in events directly to other interested Azure resources without the need for custom code. Event Grid can publish events from an Azure Subscription,\nResource Group, or Service, giving developers fine-grained control over the lifecycle of cloud\nresources. However, Event Grid isn’t limited to Azure. It’s an open platform that can consume custom\nHTTP events published from applic', 'ations or third-party services and route events to external\nsubscribers.\n\n\nWhen publishing and subscribing to native events from Azure resources, no coding is required. With\nsimple configuration, you can integrate events from one Azure resource to another leveraging built-in\nplumbing for Topics and Subscriptions. Figure 4-17 shows the anatomy of Event Grid.\n\n\n_Figure 4-17. Event Grid anatomy_\n\n\n77 CHAPTER 4 | Cloud-native communication patterns\n\n', '\nA major difference between EventGrid and Service Bus is the underlying _message exchange pattern_ .\n\n\nService Bus implements an older style _pull model_ in which the downstream subscriber actively polls\nthe topic subscription for new messages. On the upside, this approach gives the subscriber full control\nof the pace at which it processes messages. It controls when and how many messages to process at\nany given time. Unread messages remain in the', ' subscription until processed. A significant\nshortcoming is the latency between the time the event is generated and the polling operation that\npulls that message to the subscriber for processing. Also, the overhead of constant polling for the\nnext event consumes resources and money.\n\n\nEventGrid, however, is different. It implements a _push model_ in which events are sent to the\nEventHandlers as received, giving near real-time event delivery. It a', 'lso reduces cost as the service is\ntriggered only when it’s needed to consume an event – not continually as with polling. That said, an\nevent handler must handle the incoming load and provide throttling mechanisms to protect itself\nfrom becoming overwhelmed. Many Azure services that consume these events, such as Azure\nFunctions and Logic Apps provide automatic autoscaling capabilities to handle increased loads.\n\n\nEvent Grid is a fully managed ser', 'verless cloud service. It dynamically scales based on your traffic and\ncharges you only for your actual usage, not pre-purchased capacity. The first 100,000 operations per\nmonth are free – operations being defined as event ingress (incoming event notifications),\nsubscription delivery attempts, management calls, and filtering by subject. With 99.99% availability,\nEventGrid guarantees the delivery of an event within a 24-hour period, with built-in ', 'retry functionality\nfor unsuccessful delivery. Undelivered messages can be moved to a “dead-letter” queue for resolution.\nUnlike Azure Service Bus, Event Grid is tuned for fast performance and doesn’t support features like\nordered messaging, transactions, and sessions.\n\n\n**Streaming messages in the Azure cloud**\n\n\nAzure Service Bus and Event Grid provide great support for applications that expose single, discrete\nevents like a new document has be', 'en inserted into a Cosmos DB. But, what if your cloud-native\nsystem needs to process a _stream of related events_ [? Event streams](https://docs.microsoft.com/archive/msdn-magazine/2015/february/microsoft-azure-the-rise-of-event-stream-oriented-systems) are more complex. They’re typically\ntime-ordered, interrelated, and must be processed as a group.\n\n\n[Azure Event Hub](https://azure.microsoft.com/services/event-hubs/) is a data streaming platform', ' and event ingestion service that collects, transforms,\nand stores events. It’s fine-tuned to capture streaming data, such as continuous event notifications\n[emitted from a telemetry context. The service is highly scalable and can store and process millions of](https://docs.microsoft.com/azure/event-hubs/event-hubs-about)\n[events per second. Shown in Figure 4-18, it’s often a front door for an event pipeline, decoupling](https://docs.microsoft.co', 'm/azure/event-hubs/event-hubs-about)\ningest stream from event consumption.\n\n\n78 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-18. Azure Event Hub_\n\n\nEvent Hub supports low latency and configurable time retention. Unlike queues and topics, Event\nHubs keep event data after it’s been read by a consumer. This feature enables other data analytic\nservices, both internal and external, to replay the data for further analysis. Events stored ', 'in event hub\nare only deleted upon expiration of the retention period, which is one day by default, but\nconfigurable.\n\n\nEvent Hub supports common event publishing protocols including HTTPS and AMQP. It also supports\n[Kafka 1.0. Existing Kafka applications can communicate with Event Hub](https://docs.microsoft.com/azure/event-hubs/event-hubs-for-kafka-ecosystem-overview) using the Kafka protocol\nproviding an alternative to managing large Kafka clu', 'sters. Many open-source cloud-native systems\nembrace Kafka.\n\n\n[Event Hubs implements message streaming through a partitioned consumer model](https://docs.microsoft.com/azure/event-hubs/event-hubs-features) in which each\nconsumer only reads a specific subset, or partition, of the message stream. This pattern enables\ntremendous horizontal scale for event processing and provides other stream-focused features that are\nunavailable in queues and topics', '. A partition is an ordered sequence of events that is held in an event\nhub. As newer events arrive, they’re added to the end of this sequence. Figure 4-19 shows partitioning\nin an Event Hub.\n\n\n_Figure 4-19. Event Hub partitioning_\n\n\nInstead of reading from the same resource, each consumer group reads across a subset, or partition,\nof the message stream.\n\n\n79 CHAPTER 4 | Cloud-native communication patterns\n\n\nFor cloud-native applications that mus', 't stream large numbers of events, Azure Event Hub can be a\nrobust and affordable solution.\n\n### gRPC\n\n\n[So far in this book, we’ve focused on REST-based](https://docs.microsoft.com/azure/architecture/best-practices/api-design) communication. We’ve seen that REST is a flexible\narchitectural style that defines CRUD-based operations against entity resources. Clients interact with\nresources across HTTP with a request/response communication model. Whi', 'le REST is widely\nimplemented, a newer communication technology, gRPC, has gained tremendous momentum across\nthe cloud-native community.\n\n#### **What is gRPC?**\n\n\n[gRPC is a modern, high-performance framework that evolves the age-old remote procedure call (RPC)](https://en.wikipedia.org/wiki/Remote_procedure_call)\nprotocol. At the application level, gRPC streamlines messaging between clients and back-end services.\n[Originating from Google, gRPC i', 's open source and part of the Cloud Native Computing Foundation](https://www.cncf.io/)\n[(CNCF)](https://www.cncf.io/) [ecosystem of cloud-native offerings. CNCF considers gRPC an incubating project. Incubating](https://github.com/cncf/toc/blob/main/process/graduation_criteria.md)\nmeans end users are using the technology in production applications, and the project has a healthy\nnumber of contributors.\n\n\nA typical gRPC client app will expose a loca', 'l, in-process function that implements a business\noperation. Under the covers, that local function invokes another function on a remote machine. What\nappears to be a local call essentially becomes a transparent out-of-process call to a remote service.\nThe RPC plumbing abstracts the point-to-point networking communication, serialization, and\nexecution between computers.\n\n\nIn cloud-native applications, developers often work across programming langu', 'ages, frameworks, and\ntechnologies. This _interoperability_ complicates message contracts and the plumbing required for\ncross-platform communication. gRPC provides a “uniform horizontal layer” that abstracts these\nconcerns. Developers code in their native platform focused on business functionality, while gRPC\nhandles communication plumbing.\n\n\ngRPC offers comprehensive support across most popular development stacks, including Java,\nJavaScript, C#,', ' Go, Swift, and NodeJS.\n\n#### **gRPC Benefits**\n\n\ngRPC uses HTTP/2 for its transport protocol. While compatible with HTTP 1.1, HTTP/2 features many\nadvanced capabilities:\n\n\n  - A binary framing protocol for data transport - unlike HTTP 1.1, which is text based.\n\n  - Multiplexing support for sending multiple parallel requests over the same connection - HTTP\n1.1 limits processing to one request/response message at a time.\n\n  - Bidirectional full-du', 'plex communication for sending both client requests and server responses\nsimultaneously.\n\n  - Built-in streaming enabling requests and responses to asynchronously stream large data sets.\n\n  - Header compression that reduces network usage.\n\n\n80 CHAPTER 4 | Cloud-native communication patterns\n\n\ngRPC is lightweight and highly performant. It can be up to 8x faster than JSON serialization with\n[messages 60-80% smaller. In Microsoft Windows Communicati', 'on Foundation (WCF)](https://docs.microsoft.com/dotnet/framework/wcf/whats-wcf) parlance, gRPC\n[performance exceeds the speed and efficiency of the highly optimized NetTCP bindings. Unlike](https://docs.microsoft.com/dotnet/api/system.servicemodel.nettcpbinding?view=netframework-4.8&preserve-view=true)\nNetTCP, which favors the Microsoft stack, gRPC is cross-platform.\n\n#### **Protocol Buffers**\n\n\n[gRPC embraces an open-source technology called Pro', 'tocol Buffers. They provide a highly efficient](https://developers.google.com/protocol-buffers/docs/overview)\nand platform-neutral serialization format for serializing structured messages that services send to\neach other. Using a cross-platform Interface Definition Language (IDL), developers define a service\ncontract for each microservice. The contract, implemented as a text-based .proto file, describes the\nmethods, inputs, and outputs for each s', 'ervice. The same contract file can be used for gRPC clients and\nservices built on different development platforms.\n\n\nUsing the proto file, the Protobuf compiler, protoc, generates both client and service code for your\ntarget platform. The code includes the following components:\n\n\n  - Strongly typed objects, shared by the client and service, that represent the service operations\nand data elements for a message.\n\n  - A strongly typed base class wit', 'h the required network plumbing that the remote gRPC service\ncan inherit and extend.\n\n  - A client stub that contains the required plumbing to invoke the remote gRPC service.\n\nAt run time, each message is serialized as a standard Protobuf representation and exchanged between\nthe client and remote service. Unlike JSON or XML, Protobuf messages are serialized as compiled\nbinary bytes.\n\n\n[The book, gRPC for WCF Developers, available from the Microso', 'ft Architecture site, provides in-depth](https://docs.microsoft.com/dotnet/architecture/grpc-for-wcf-developers/)\ncoverage of gRPC and Protocol Buffers.\n\n#### **gRPC support in .NET**\n\n\ngRPC is integrated into .NET Core 3.0 SDK and later. The following tools support it:\n\n\n  - Visual Studio 2022 with the ASP.NET and web development workload installed\n\n  - Visual Studio Code\n\n  - The dotnet CLI\n\n\nThe SDK includes tooling for endpoint routing, built', '-in IoC, and logging. The open-source Kestrel web\nserver supports HTTP/2 connections. Figure 4-20 shows a Visual Studio 2022 template that scaffolds a\nskeleton project for a gRPC service. Note how .NET fully supports Windows, Linux, and macOS.\n\n\n81 CHAPTER 4 | Cloud-native communication patterns\n\n\n_Figure 4-20. gRPC support in Visual Studio 2022_\n\n\nFigure 4-21 shows the skeleton gRPC service generated from the built-in scaffolding included in\nVis', 'ual Studio 2022.\n\n\n_Figure 4-21. gRPC project in Visual Studio 2022_\n\n\nIn the previous figure, note the proto description file and service code. As you’ll see shortly, Visual\nStudio generates additional configuration in both the Startup class and underlying project file.\n\n#### **gRPC usage**\n\n\nFavor gRPC for the following scenarios:\n\n\n  - Synchronous backend microservice-to-microservice communication where an immediate\nresponse is required to con', 'tinue processing.\n\n  - Polyglot environments that need to support mixed programming platforms.\n\n  - Low latency and high throughput communication where performance is critical.\n\n  - Point-to-point real-time communication - gRPC can push messages in real time without\npolling and has excellent support for bi-directional streaming.\n\n\n82 CHAPTER 4 | Cloud-native communication patterns\n\n\n  - Network constrained environments – binary gRPC messages are ', 'always smaller than an\nequivalent text-based JSON message.\n\n\nAt the time, of this writing, gRPC is primarily used with backend services. Modern browsers can’t\nprovide the level of HTTP/2 control required to support a front-end gRPC client. That said, there’s\n[support for gRPC-Web with .NET that enables gRPC communication from browser-based apps built](https://devblogs.microsoft.com/aspnet/grpc-web-for-net-now-available/)\n[with JavaScript or Blazo', 'r WebAssembly technologies. gRPC-Web](https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-WEB.md) enables an ASP.NET Core gRPC app\nto support gRPC features in browser apps:\n\n\n  - Strongly typed, code-generated clients\n\n  - Compact Protobuf messages\n\n  - Server streaming\n\n#### **gRPC implementation**\n\n\n[The microservice reference architecture, eShop on Containers, from Microsoft, shows how to](https://github.com/dotnet-architecture/eShopOnContai', 'ners)\nimplement gRPC services in .NET applications. Figure 4-22 presents the back-end architecture.\n\n\n_Figure 4-22. Backend architecture for eShop on Containers_\n\n\n[In the previous figure, note how eShop embraces the Backend for Frontends pattern](https://docs.microsoft.com/azure/architecture/patterns/backends-for-frontends) (BFF) by\nexposing multiple API gateways. We discussed the BFF pattern earlier in this chapter. Pay close\n\n\n83 CHAPTER 4 | C', 'loud-native communication patterns\n\n\nattention to the Aggregator microservice (in gray) that sits between the Web-Shopping API Gateway\nand backend Shopping microservices. The Aggregator receives a single request from a client,\ndispatches it to various microservices, aggregates the results, and sends them back to the requesting\nclient. Such operations typically require synchronous communication as to produce an immediate\nresponse. In eShop, backen', 'd calls from the Aggregator are performed using gRPC as shown in Figure\n4-23.\n\n\n_Figure 4-23. gRPC in eShop on Containers_\n\n\ngRPC communication requires both client and server components. In the previous figure, note how\nthe Shopping Aggregator implements a gRPC client. The client makes synchronous gRPC calls (in red)\nto backend microservices, each of which implement a gRPC server. Both the client and server take\nadvantage of the built-in gRPC pl', 'umbing from the .NET SDK. Client-side _stubs_ provide the plumbing\nto invoke remote gRPC calls. Server-side components provide gRPC plumbing that custom service\nclasses can inherit and consume.\n\n\nMicroservices that expose both a RESTful API and gRPC communication require multiple endpoints to\nmanage traffic. You would open an endpoint that listens for HTTP traffic for the RESTful calls and\nanother for gRPC calls. The gRPC endpoint must be configu', 'red for the HTTP/2 protocol that is\nrequired for gRPC communication.\n\n\nWhile we strive to decouple microservices with asynchronous communication patterns, some\noperations require direct calls. gRPC should be the primary choice for direct synchronous\ncommunication between microservices. Its high-performance communication protocol, based on\nHTTP/2 and protocol buffers, make it a perfect choice.\n\n\n84 CHAPTER 4 | Cloud-native communication patterns\n\n', '\n#### **Looking ahead**\n\nLooking ahead, gRPC will continue to gain traction for cloud-native systems. The performance\nbenefits and ease of development are compelling. However, REST will likely be around for a long time.\nIt excels for publicly exposed APIs and for backward compatibility reasons.\n\n### Service Mesh communication infrastructure\n\n\nThroughout this chapter, we’ve explored the challenges of microservice communication. We said that\ndevelo', 'pment teams need to be sensitive to how back-end services communicate with each other.\nIdeally, the less inter-service communication, the better. However, avoidance isn’t always possible as\nback-end services often rely on one another to complete operations.\n\n\nWe explored different approaches for implementing synchronous HTTP communication and\nasynchronous messaging. In each of the cases, the developer is burdened with implementing\ncommunication c', 'ode. Communication code is complex and time intensive. Incorrect decisions can\nlead to significant performance issues.\n\n\nA more modern approach to microservice communication centers around a new and rapidly evolving\ntechnology entitled _Service Mesh_ [. A service mesh](https://www.nginx.com/blog/what-is-a-service-mesh/) is a configurable infrastructure layer with built-in\ncapabilities to handle service-to-service communication, resiliency, and ma', 'ny cross-cutting concerns.\nIt moves the responsibility for these concerns out of the microservices and into service mesh layer.\nCommunication is abstracted away from your microservices.\n\n\nA key component of a service mesh is a proxy. In a cloud-native application, an instance of a proxy is\ntypically colocated with each microservice. While they execute in separate processes, the two are\n[closely linked and share the same lifecycle. This pattern, k', 'nown as the Sidecar pattern, and is shown in](https://docs.microsoft.com/azure/architecture/patterns/sidecar)\nFigure 4-24.\n\n\n_Figure 4-24. Service mesh with a side car_\n\n\n85 CHAPTER 4 | Cloud-native communication patterns\n\n\nNote in the previous figure how messages are intercepted by a proxy that runs alongside each\nmicroservice. Each proxy can be configured with traffic rules specific to the microservice. It\nunderstands messages and can route the', 'm across your services and the outside world.\n\n\nAlong with managing service-to-service communication, the Service Mesh provides support for\nservice discovery and load balancing.\n\n\nOnce configured, a service mesh is highly functional. The mesh retrieves a corresponding pool of\ninstances from a service discovery endpoint. It sends a request to a specific service instance, recording\nthe latency and response type of the result. It chooses the instanc', 'e most likely to return a fast\nresponse based on different factors, including the observed latency for recent requests.\n\n\nA service mesh manages traffic, communication, and networking concerns at the application level. It\nunderstands messages and requests. A service mesh typically integrates with a container orchestrator.\nKubernetes supports an extensible architecture in which a service mesh can be added.\n\n\nIn chapter 6, we deep-dive into Service', ' Mesh technologies including a discussion on its architecture\nand available open-source implementations.\n\n#### **Summary**\n\n\nIn this chapter, we discussed cloud-native communication patterns. We started by examining how\nfront-end clients communicate with back-end microservices. Along the way, we talked about API\nGateway platforms and real-time communication. We then looked at how microservices communicate\nwith other back-end services. We looked a', 't both synchronous HTTP communication and\nasynchronous messaging across services. We covered gRPC, an upcoming technology in the cloudnative world. Finally, we introduced a new and rapidly evolving technology entitled Service Mesh that\ncan streamline microservice communication.\n\n\nSpecial emphasis was on managed Azure services that can help implement communication in cloudnative systems:\n\n\n  - [Azure Application Gateway](https://docs.microsoft.com', '/azure/application-gateway/overview)\n\n  - [Azure API Management](https://azure.microsoft.com/services/api-management/)\n\n  - [Azure SignalR Service](https://azure.microsoft.com/services/signalr-service/)\n\n  - [Azure Storage Queues](https://docs.microsoft.com/azure/storage/queues/storage-queues-introduction)\n\n  - [Azure Service Bus](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-messaging-overview)\n\n  - [Azure Event Grid](https:', '//docs.microsoft.com/azure/event-grid/overview)\n\n  - [Azure Event Hub](https://azure.microsoft.com/services/event-hubs/)\n\n\nWe next move to distributed data in cloud-native systems and the benefits and challenges that it\npresents.\n\n\n**References**\n\n\n  - [.NET Microservices: Architecture for Containerized .NET applications](https://dotnet.microsoft.com/download/thank-you/microservices-architecture-ebook)\n\n\n  - [Designing Interservice Communication ', 'for Microservices](https://docs.microsoft.com/azure/architecture/microservices/design/interservice-communication)\n\n\n  - [Azure SignalR Service, a fully managed service to add real-time functionality](https://azure.microsoft.com/blog/azure-signalr-service-a-fully-managed-service-to-add-real-time-functionality/)\n\n\n86 CHAPTER 4 | Cloud-native communication patterns\n\n\n  - [Azure API Gateway Ingress Controller](https://azure.github.io/application-gate', 'way-kubernetes-ingress/)\n\n\n  - [gRPC Documentation](https://grpc.io/docs/guides/)\n\n\n  - [gRPC for WCF Developers](https://docs.microsoft.com/dotnet/architecture/grpc-for-wcf-developers/)\n\n\n  - [Comparing gRPC Services with HTTP APIs](https://docs.microsoft.com/aspnet/core/grpc/comparison?view=aspnetcore-3.0&preserve-view=false)\n\n\n  - [Building gRPC Services with .NET video](https://docs.microsoft.com/Shows/The-Cloud-Native-Show/Building-Microserv', 'ices-with-gRPC-and-NET)\n\n\n87 CHAPTER 4 | Cloud-native communication patterns\n\n\n**CHAPTER**\n# 5\n\n## Cloud-native data patterns\n\n\nAs we’ve seen throughout this book, a cloud-native approach changes the way you design, deploy,\nand manage applications. It also changes the way you manage and store data.\n\n\nFigure 5-1 contrasts the differences.\n\n\n_Figure 5-1. Data management in cloud-native applications_\n\n\nExperienced developers will easily recognize th', 'e architecture on the left-side of figure 5-1. In this\n_monolithic application_, business service components collocate together in a shared services tier,\nsharing data from a single relational database.\n\n\nIn many ways, a single database keeps data management simple. Querying data across multiple tables\n[is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee](https://docs.microsoft.com/windows/desktop', '/cossdk/acid-properties)\nstrong and immediate consistency.\n\n\nDesigning for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how\nbusiness functionality segregates into small, independent microservices. Each microservice\nencapsulates a specific business capability and its own data. The monolithic database decomposes\n\n\n88 CHAPTER 5 | Cloud-native data patterns\n\n\ninto a distributed data model with many smaller databas', 'es, each aligning with a microservice. When\nthe smoke clears, we emerge with a design that exposes a _database per microservice_ .\n\n### Database-per-microservice, why?\n\n\nThis database per microservice provides many benefits, especially for systems that must evolve rapidly\nand support massive scale. With this model…\n\n\n  - Domain data is encapsulated within the service\n\n  - Data schema can evolve without directly impacting other services\n\n  - Each ', 'data store can independently scale\n\n  - A data store failure in one service won’t directly impact other services\n\n\nSegregating data also enables each microservice to implement the data store type that is best\noptimized for its workload, storage needs, and read/write patterns. Choices include relational,\ndocument, key-value, and even graph-based data stores.\n\n\nFigure 5-2 presents the principle of polyglot persistence in a cloud-native system.\n\n\n_F', 'igure 5-2. Polyglot data persistence_\n\n\nNote in the previous figure how each microservice supports a different type of data store.\n\n\n  - The product catalog microservice consumes a relational database to accommodate the rich\nrelational structure of its underlying data.\n\n  - The shopping cart microservice consumes a distributed cache that supports its simple, keyvalue data store.\n\n  - The ordering microservice consumes both a NoSql document databa', 'se for write operations\nalong with a highly denormalized key/value store to accommodate high-volumes of read\noperations.\n\n\n89 CHAPTER 5 | Cloud-native data patterns\n\n\nWhile relational databases remain relevant for microservices with complex data, NoSQL databases\nhave gained considerable popularity. They provide massive scale and high availability. Their\nschemaless nature allows developers to move away from an architecture of typed data classes an', 'd\nORMs that make change expensive and time-consuming. We cover NoSQL databases later in this\nchapter.\n\n\nWhile encapsulating data into separate microservices can increase agility, performance, and scalability,\nit also presents many challenges. In the next section, we discuss these challenges along with patterns\nand practices to help overcome them.\n\n### Cross-service queries\n\n\nWhile microservices are independent and focus on specific functional cap', 'abilities, like inventory,\nshipping, or ordering, they frequently require integration with other microservices. Often the\nintegration involves one microservice _querying_ another for data. Figure 5-3 shows the scenario.\n\n\n_Figure 5-3. Querying across microservices_\n\n\nIn the preceding figure, we see a shopping basket microservice that adds an item to a user’s shopping\nbasket. While the data store for this microservice contains basket and line item', ' data, it doesn’t\nmaintain product or pricing data. Instead, those data items are owned by the catalog and pricing\nmicroservices. This aspect presents a problem. How can the shopping basket microservice add a\nproduct to the user’s shopping basket when it doesn’t have product nor pricing data in its database?\n\n\nOne option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and\npricing microservices. However, in cha', 'pter 4, we said synchronous HTTP calls _couple_ microservices\ntogether, reducing their autonomy and diminishing their architectural benefits.\n\n\nWe could also implement a request-reply pattern with separate inbound and outbound queues for\neach service. However, this pattern is complicated and requires plumbing to correlate request and\nresponse messages. While it does decouple the backend microservice calls, the calling service must\nstill synchrono', 'usly wait for the call to complete. Network congestion, transient faults, or an\noverloaded microservice and can result in long-running and even failed operations.\n\n\n90 CHAPTER 5 | Cloud-native data patterns\n\n\n[Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View](https://docs.microsoft.com/azure/architecture/patterns/materialized-view)\n[Pattern, shown in Figure 5-4.](https://docs.microsoft.com/azure/', 'architecture/patterns/materialized-view)\n\n\n_Figure 5-4. Materialized View Pattern_\n\n\nWith this pattern, you place a local data table (known as a _read model_ ) in the shopping basket service.\nThis table contains a denormalized copy of the data needed from the product and pricing\nmicroservices. Copying the data directly into the shopping basket microservice eliminates the need for\nexpensive cross-service calls. With the data local to the service, ', 'you improve the service’s response\ntime and reliability. Additionally, having its own copy of the data makes the shopping basket service\nmore resilient. If the catalog service should become unavailable, it wouldn’t directly impact the\nshopping basket service. The shopping basket can continue operating with the data from its own\nstore.\n\n\nThe catch with this approach is that you now have duplicate data in your system. However,\n_strategically_ dupli', 'cating data in cloud-native systems is an established practice and not considered an\nanti-pattern, or bad practice. Keep in mind that _one and only one service_ can own a data set and have\nauthority over it. You’ll need to synchronize the read models when the system of record is updated.\nSynchronization is typically implemented via asynchronous messaging with a publish/subscribe\npattern, as shown in Figure 5.4.\n\n### Distributed transactions\n\n\nWhi', 'le querying data across microservices is difficult, implementing a transaction across several\nmicroservices is even more complex. The inherent challenge of maintaining data consistency across\nindependent data sources in different microservices can’t be understated. The lack of distributed\ntransactions in cloud-native applications means that you must manage distributed transactions\nprogrammatically. You move from a world of _immediate consistency_', ' to that of _eventual consistency_ .\n\n\nFigure 5-5 shows the problem.\n\n\n91 CHAPTER 5 | Cloud-native data patterns\n\n\n_Figure 5-5. Implementing a transaction across microservices_\n\n\nIn the preceding figure, five independent microservices participate in a distributed transaction that\ncreates an order. Each microservice maintains its own data store and implements a local transaction\nfor its store. To create the order, the local transaction for _each_ ', 'individual microservice must succeed,\nor _all_ must abort and roll back the operation. While built-in transactional support is available inside\neach of the microservices, there’s no support for a distributed transaction that would span across all\nfive services to keep data consistent.\n\n\nInstead, you must construct this distributed transaction _programmatically_ .\n\n\nA popular pattern for adding distributed transactional support is the Saga pattern', '. It’s implemented\nby grouping local transactions together programmatically and sequentially invoking each one. If any\n[of the local transactions fail, the Saga aborts the operation and invokes a set of compensating](https://docs.microsoft.com/azure/architecture/patterns/compensating-transaction)\n[transactions. The compensating transactions undo the changes made by the preceding local](https://docs.microsoft.com/azure/architecture/patterns/compen', 'sating-transaction)\ntransactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.\n\n\n_Figure 5-6. Rolling back a transaction_\n\n\n92 CHAPTER 5 | Cloud-native data patterns\n\n\nIn the previous figure, the _Update Inventory_ operation has failed in the Inventory microservice. The\nSaga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the\npayment and the order, and return ', 'the data for each microservice back to a consistent state.\n\n\nSaga patterns are typically choreographed as a series of related events, or orchestrated as a set of\nrelated commands. In Chapter 4, we discussed the service aggregator pattern that would be the\nfoundation for an orchestrated saga implementation. We also discussed eventing along with Azure\nService Bus and Azure Event Grid topics that would be a foundation for a choreographed saga\nimplem', 'entation.\n\n### High volume data\n\n\nLarge cloud-native applications often support high-volume data requirements. In these scenarios,\ntraditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large\nscale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve\napplication performance.\n\n#### **CQRS**\n\n\n[CQRS, is an architectural pattern that can help maximize performance, scalabili', 'ty, and security. The](https://docs.microsoft.com/azure/architecture/patterns/cqrs)\npattern separates operations that read data from those operations that write data.\n\n\nFor normal scenarios, the same entity model and data repository object are used for _both_ read and\nwrite operations.\n\n\nHowever, a high volume data scenario can benefit from separate models and data tables for reads\nand writes. To improve performance, the read operation could quer', 'y against a highly denormalized\nrepresentation of the data to avoid expensive repetitive table joins and table locks. The _write_\noperation, known as a _command_, would update against a fully normalized representation of the data\nthat would guarantee consistency. You then need to implement a mechanism to keep both\nrepresentations in sync. Typically, whenever the write table is modified, it publishes an event that\nreplicates the modification to th', 'e read table.\n\n\nFigure 5-7 shows an implementation of the CQRS pattern.\n\n\n_Figure 5-7. CQRS implementation_\n\n\n93 CHAPTER 5 | Cloud-native data patterns\n\n\nIn the previous figure, separate command and query models are implemented. Each data write\noperation is saved to the write store and then propagated to the read store. Pay close attention to\n[how the data propagation process operates on the principle of eventual consistency. The read model](http', 's://www.cloudcomputingpatterns.org/eventual_consistency/)\neventually synchronizes with the write model, but there may be some lag in the process. We discuss\neventual consistency in the next section.\n\n\nThis separation enables reads and writes to scale independently. Read operations use a schema\noptimized for queries, while the writes use a schema optimized for updates. Read queries go against\ndenormalized data, while complex business logic can be ', 'applied to the write model. As well, you\nmight impose tighter security on write operations than those exposing reads.\n\n\nImplementing CQRS can improve application performance for cloud-native services. However, it does\nresult in a more complex design. Apply this principle carefully and strategically to those sections of\n[your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET](https://docs.microsoft.c', 'om/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/apply-simplified-microservice-cqrs-ddd-patterns)\n[Microservices: Architecture for Containerized .NET Applications.](https://docs.microsoft.com/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/apply-simplified-microservice-cqrs-ddd-patterns)\n\n#### **Event sourcing**\n\n\n[Another approach to optimizing high volume data scenarios involves Event Sourcing.](https://docs.', 'microsoft.com/azure/architecture/patterns/event-sourcing)\n\n\nA system typically stores the current state of a data entity. If a user changes their phone number, for\nexample, the customer record is updated with the new number. We always know the current state of a\ndata entity, but each update overwrites the previous state.\n\n\nIn most cases, this model works fine. In high volume systems, however, overhead from transactional\nlocking and frequent updat', 'e operations can impact database performance, responsiveness, and limit\nscalability.\n\n\nEvent Sourcing takes a different approach to capturing data. Each operation that affects data is\npersisted to an event store. Instead of updating the state of a data record, we append each change to\na sequential list of past events - similar to an accountant’s ledger. The Event Store becomes the\nsystem of record for the data. It’s used to propagate various mate', 'rialized views within the bounded\ncontext of a microservice. Figure 5.8 shows the pattern.\n\n\n94 CHAPTER 5 | Cloud-native data patterns\n\n\n_Figure 5-8. Event Sourcing_\n\n\nIn the previous figure, note how each entry (in blue) for a user’s shopping cart is appended to an\nunderlying event store. In the adjoining materialized view, the system projects the current state by\nreplaying all the events associated with each shopping cart. This view, or read mo', 'del, is then exposed\nback to the UI. Events can also be integrated with external systems and applications or queried to\ndetermine the current state of an entity. With this approach, you maintain history. You know not only\nthe current state of an entity, but also how you reached this state.\n\n\nMechanically speaking, event sourcing simplifies the write model. There are no updates or deletes.\nAppending each data entry as an immutable event minimizes ', 'contention, locking, and concurrency\nconflicts associated with relational databases. Building read models with the materialized view pattern\nenables you to decouple the view from the write model and choose the best data store to optimize\nthe needs of your application UI.\n\n\nFor this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB,\nMongoDB, Cassandra, CouchDB, and RavenDB are good candidates.\n\n\nAs with all patt', 'erns and technologies, implement strategically and when needed. While event sourcing\ncan provide increased performance and scalability, it comes at the expense of complexity and a\nlearning curve.\n\n\n95 CHAPTER 5 | Cloud-native data patterns\n\n\n### Relational vs. NoSQL data\n\nRelational and NoSQL are two types of database systems commonly implemented in cloud-native\napps. They’re built differently, store data differently, and accessed differently. In', ' this section, we’ll look\nat both. Later in this chapter, we’ll look at an emerging database technology called _NewSQL_ .\n\n\n_Relational databases_ have been a prevalent technology for decades. They’re mature, proven, and\nwidely implemented. Competing database products, tooling, and expertise abound. Relational\ndatabases provide a store of related data tables. These tables have a fixed schema, use SQL\n(Structured Query Language) to manage data, an', 'd support ACID guarantees.\n\n\n_No-SQL databases_ refer to high-performance, non-relational data stores. They excel in their ease-ofuse, scalability, resilience, and availability characteristics. Instead of joining tables of normalized data,\nNoSQL stores unstructured or semi-structured data, often in key-value pairs or JSON documents. NoSQL databases typically don’t provide ACID guarantees beyond the scope of a single database\npartition. High volum', 'e services that require sub second response time favor NoSQL datastores.\n\n\n[The impact of NoSQL technologies for distributed cloud-native systems can’t be overstated. The](https://www.geeksforgeeks.org/introduction-to-nosql/)\nproliferation of new data technologies in this space has disrupted solutions that once exclusively\nrelied on relational databases.\n\n\nNoSQL databases include several different models for accessing and managing data, each suit', 'ed to\nspecific use cases. Figure 5-9 presents four common models.\n\n\n_Figure 5-9: Data models for NoSQL databases_\n\n|Model|Characteristics|\n|---|---|\n|Document Store|Data and metadata are stored hierarchically in<br>JSON-based documents inside the database.|\n|Key Value Store|The simplest of the NoSQL databases, data is<br>represented as a collection of key-value pairs.|\n|Wide-Column Store|Related data is stored as a set of nested-<br>key/value pai', 'rs within a single column.|\n|Graph Store|Data is stored in a graph structure as node,<br>edge, and data properties.|\n\n\n\n96 CHAPTER 5 | Cloud-native data patterns\n\n\n#### **The CAP theorem**\n\nAs a way to understand the differences between these types of databases, consider the CAP theorem,\na set of principles applied to distributed systems that store state. Figure 5-10 shows the three\nproperties of the CAP theorem.\n\n\n_Figure 5-10. The CAP theorem_\n', '\n\nThe theorem states that distributed data systems will offer a trade-off between consistency,\navailability, and partition tolerance. And, that any database can only guarantee _two_ of the three\nproperties:\n\n\n  - _Consistency._ Every node in the cluster responds with the most recent data, even if the system\nmust block the request until all replicas update. If you query a “consistent system” for an item\nthat is currently updating, you’ll wait for ', 'that response until all replicas successfully update.\nHowever, you’ll receive the most current data.\n\n\n  - _Availability._ Every node returns an immediate response, even if that response isn’t the most\nrecent data. If you query an “available system” for an item that is updating, you’ll get the best\npossible answer the service can provide at that moment.\n\n\n  - _Partition Tolerance._ Guarantees the system continues to operate even if a replicated d', 'ata\nnode fails or loses connectivity with other replicated data nodes.\n\n\nCAP theorem explains the tradeoffs associated with managing consistency and availability during a\nnetwork partition; however tradeoffs with respect to consistency and performance also exist with the\n[absence of a network partition. CAP theorem is often further extended to PACELC](http://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf) to explain the\ntradeoffs more comprehensiv', 'ely.\n\n\nRelational databases typically provide consistency and availability, but not partition tolerance. They’re\ntypically provisioned to a single server and scale vertically by adding more resources to the machine.\n\n\n97 CHAPTER 5 | Cloud-native data patterns\n\n\nMany relational database systems support built-in replication features where copies of the primary\ndatabase can be made to other secondary server instances. Write operations are made to th', 'e primary\ninstance and replicated to each of the secondaries. Upon a failure, the primary instance can fail over\nto a secondary to provide high availability. Secondaries can also be used to distribute read operations.\nWhile writes operations always go against the primary replica, read operations can be routed to any of\nthe secondaries to reduce system load.\n\n\n[Data can also be horizontally partitioned across multiple nodes, such as with sharding.', ' But, sharding](https://docs.microsoft.com/azure/sql-database/sql-database-elastic-scale-introduction)\ndramatically increases operational overhead by spitting data across many pieces that cannot easily\ncommunicate. It can be costly and time consuming to manage. Relational features that include table\njoins, transactions, and referential integrity require steep performance penalties in sharded\ndeployments.\n\n\nReplication consistency and recovery poi', 'nt objectives can be tuned by configuring whether replication\noccurs synchronously or asynchronously. If data replicas were to lose network connectivity in a “highly\nconsistent” or synchronous relational database cluster, you wouldn’t be able to write to the database.\nThe system would reject the write operation as it can’t replicate that change to the other data replica.\nEvery data replica has to update before the transaction can complete.\n\n\nNoSQ', 'L databases typically support high availability and partition tolerance. They scale out\nhorizontally, often across commodity servers. This approach provides tremendous availability, both\nwithin and across geographical regions at a reduced cost. You partition and replicate data across\nthese machines, or nodes, providing redundancy and fault tolerance. Consistency is typically tuned\nthrough consensus protocols or quorum mechanisms. They provide mor', 'e control when navigating\ntradeoffs between tuning synchronous versus asynchronous replication in relational systems.\n\n\nIf data replicas were to lose connectivity in a “highly available” NoSQL database cluster, you could still\ncomplete a write operation to the database. The database cluster would allow the write operation and\nupdate each data replica as it becomes available. NoSQL databases that support multiple writable\nreplicas can further stre', 'ngthen high availability by avoiding the need for failover when optimizing\nrecovery time objective.\n\n\nModern NoSQL databases typically implement partitioning capabilities as a feature of their system\ndesign. Partition management is often built-in to the database, and routing is achieved through\nplacement hints - often called partition keys. A flexible data models enables the NoSQL databases to\nlower the burden of schema management and improve ava', 'ilability when deploying application\nupdates that require data model changes.\n\n\nHigh availability and massive scalability are often more critical to the business than relational table\njoins and referential integrity. Developers can implement techniques and patterns such as Sagas,\nCQRS, and asynchronous messaging to embrace eventual consistency.\n\n\nNowadays, care must be taken when considering the CAP theorem constraints. A new type of\ndatabase, ca', 'lled NewSQL, has emerged which extends the relational database engine to support both\nhorizontal scalability and the scalable performance of NoSQL systems.\n\n\n98 CHAPTER 5 | Cloud-native data patterns\n\n\n#### **Considerations for relational vs. NoSQL systems**\n\nBased upon specific data requirements, a cloud-native-based microservice can implement a relational,\nNoSQL datastore or both.\n\n\n\n\n\n\n\n\n\n\n\n\n|Consider a NoSQL datastore when:|Consider a relatio', 'nal database when:|\n|---|---|\n|You have high volume workloads that require<br>predictable latency at large scale (for example,<br>latency measured in milliseconds while<br>performing millions of transactions per second)|Your workload volume generally fits within<br>thousands of transactions per second|\n|Your data is dynamic and frequently changes|Your data is highly structured and requires<br>referential integrity|\n|Relationships can be de-normal', 'ized data<br>models|Relationships are expressed through table joins<br>on normalized data models|\n|Data retrieval is simple and expressed without<br>table joins|You work with complex queries and reports|\n|Data is typically replicated across geographies<br>and requires finer control over consistency,<br>availability, and performance|Data is typically centralized, or can be replicated<br>regions asynchronously|\n|Your application will be deployed to', ' commodity<br>hardware, such as with public clouds|Your application will be deployed to large, high-<br>end hardware|\n\n\n\nIn the next sections, we’ll explore the options available in the Azure cloud for storing and managing\nyour cloud-native data.\n\n#### **Database as a Service**\n\n\nTo start, you could provision an Azure virtual machine and install your database of choice for each\nservice. While you’d have full control over the environment, you’d fo', 'rgo many built-in features of the\ncloud platform. You’d also be responsible for managing the virtual machine and database for each\nservice. This approach could quickly become time-consuming and expensive.\n\n\nInstead, cloud-native applications favor data services exposed as a Database as a Service (DBaaS).\nFully managed by a cloud vendor, these services provide built-in security, scalability, and monitoring.\nInstead of owning the service, you simpl', 'y consume it as a backing service. The provider operates the\nresource at scale and bears the responsibility for performance and maintenance.\n\n\nThey can be configured across cloud availability zones and regions to achieve high availability. They\nall support just-in-time capacity and a pay-as-you-go model. Azure features different kinds of\nmanaged data service options, each with specific benefits.\n\n\nWe’ll first look at relational DBaaS services ava', 'ilable in Azure. You’ll see that Microsoft’s flagship SQL\nServer database is available along with several open-source options. Then, we’ll talk about the NoSQL\ndata services in Azure.\n\n\n99 CHAPTER 5 | Cloud-native data patterns\n\n\n#### **Azure relational databases**\n\nFor cloud-native microservices that require relational data, Azure offers four managed relational\ndatabases as a service (DBaaS) offerings, shown in Figure 5-11.\n\n\n_Figure 5-11. Manag', 'ed relational databases available in Azure_\n\n\nIn the previous figure, note how each sits upon a common DBaaS infrastructure which features key\ncapabilities at no additional cost.\n\n\nThese features are especially important to organizations who provision large numbers of databases,\nbut have limited resources to administer them. You can provision an Azure database in minutes by\nselecting the amount of processing cores, memory, and underlying storage.', ' You can scale the\ndatabase on-the-fly and dynamically adjust resources with little to no downtime.\n\n#### **Azure SQL Database**\n\n\n[Development teams with expertise in Microsoft SQL Server should consider Azure SQL Database. It’s a](https://docs.microsoft.com/azure/sql-database/)\nfully managed relational database-as-a-service (DBaaS) based on the Microsoft SQL Server Database\nEngine. The service shares many features found in the on-premises versi', 'on of SQL Server and runs the\nlatest stable version of the SQL Server Database Engine.\n\n\nFor use with a cloud-native microservice, Azure SQL Database is available with three deployment\noptions:\n\n\n  - [A Single Database represents a fully managed SQL Database running on an Azure SQL](https://docs.microsoft.com/azure/sql-database/sql-database-servers)\n[Database server](https://docs.microsoft.com/azure/sql-database/sql-database-servers) in the Azure', ' cloud. The database is considered _[contained](https://docs.microsoft.com/sql/relational-databases/databases/contained-databases)_ as it has no\nconfiguration dependencies on the underlying database server.\n\n\n  - [A Managed Instance is a fully managed instance of the Microsoft SQL Server Database Engine](https://docs.microsoft.com/azure/sql-database/sql-database-managed-instance)\nthat provides near-100% compatibility with an on-premises SQL Serve', 'r. This option supports\n[larger databases, up to 35 TB and is placed in an Azure Virtual Network](https://docs.microsoft.com/azure/virtual-network/virtual-networks-overview) for better isolation.\n\n\n100 CHAPTER 5 | Cloud-native data patterns\n\n\n  - [Azure SQL Database serverless is a compute tier for a single database that automatically](https://docs.microsoft.com/azure/sql-database/sql-database-serverless)\nscales based on workload demand. It bills', ' only for the amount of compute used per second.\nThe service is well suited for workloads with intermittent, unpredictable usage patterns,\ninterspersed with periods of inactivity. The serverless compute tier also automatically pauses\ndatabases during inactive periods so that only storage charges are billed. It automatically\nresumes when activity returns.\n\n\nBeyond the traditional Microsoft SQL Server stack, Azure also features managed versions of ', 'three\npopular open-source databases.\n\n#### **Open-source databases in Azure**\n\n\nOpen-source relational databases have become a popular choice for cloud-native applications. Many\nenterprises favor them over commercial database products, especially for cost savings. Many\ndevelopment teams enjoy their flexibility, community-backed development, and ecosystem of tools\nand extensions. Open-source databases can be deployed across multiple cloud provider', 's, helping\nminimize the concern of “vendor lock-in.”\n\n\nDevelopers can easily self-host any open-source database on an Azure VM. While providing full\ncontrol, this approach puts you on the hook for the management, monitoring, and maintenance of\nthe database and VM.\n\n\nHowever, Microsoft continues its commitment to keeping Azure an “open platform” by offering\nseveral popular open-source databases as _fully managed_ DBaaS services.\n\n\n**Azure Database', ' for MySQL**\n\n\n[MySQL](https://en.wikipedia.org/wiki/MySQL) [is an open-source relational database and a pillar for applications built on the LAMP software](https://en.wikipedia.org/wiki/LAMP_(software_bundle))\n[stack. Widely chosen for](https://en.wikipedia.org/wiki/LAMP_(software_bundle)) _read heavy_ workloads, it’s used by many large organizations, including\nFacebook, Twitter, and YouTube. The community edition is available for free, while th', 'e enterprise\nedition requires a license purchase. Originally created in 1995, the product was purchased by Sun\nMicrosystems in 2008. Oracle acquired Sun and MySQL in 2010.\n\n\n[Azure Database for MySQL is a managed relational database service based on the open-source](https://azure.microsoft.com/services/mysql/)\nMySQL Server engine. It uses the MySQL Community edition. The Azure MySQL server is the\nadministrative point for the service. It’s the sam', 'e MySQL server engine used for on-premises\ndeployments. The engine can create a single database per server or multiple databases per server that\nshare resources. You can continue to manage data using the same open-source tools without having\nto learn new skills or manage virtual machines.\n\n\n**Azure Database for MariaDB**\n\n\n[MariaDB](https://mariadb.com/) Server is another popular open-source database server. It was created as a _fork_ of MySQL\nwh', 'en Oracle purchased Sun Microsystems, who owned MySQL. The intent was to ensure that MariaDB\nremained open-source. As MariaDB is a fork of MySQL, the data and table definitions are compatible,\nand the client protocols, structures, and APIs, are close-knit.\n\n\n101 CHAPTER 5 | Cloud-native data patterns\n\n\nMariaDB has a strong community and is used by many large enterprises. While Oracle continues to\nmaintain, enhance, and support MySQL, the MariaDB ', 'foundation manages MariaDB, allowing public\ncontributions to the product and documentation.\n\n\n[Azure Database for MariaDB is a fully managed relational database as a service in the Azure cloud. The](https://azure.microsoft.com/services/mariadb/)\nservice is based on the MariaDB community edition server engine. It can handle mission-critical\nworkloads with predictable performance and dynamic scalability.\n\n\n**Azure Database for PostgreSQL**\n\n\n[Postg', 'reSQL](https://www.postgresql.org/) is an open-source relational database with over 30 years of active development.\nPostgreSQL has a strong reputation for reliability and data integrity. It’s feature rich, SQL compliant,\nand considered more performant than MySQL - especially for workloads with complex queries and\nheavy writes. Many large enterprises including Apple, Red Hat, and Fujitsu have built products using\nPostgreSQL.\n\n\n[Azure Database for ', 'PostgreSQL is a fully managed relational database service, based on the open-](https://azure.microsoft.com/services/postgresql/)\nsource Postgres database engine. The service supports many development platforms, including C++,\nJava, Python, Node, C#, and PHP. You can migrate PostgreSQL databases to it using the commandline interface tool or Azure Data Migration Service.\n\n\nAzure Database for PostgreSQL is available with two deployment options:\n\n\n  ', '- [The Single Server deployment option is a central administrative point for multiple databases](https://docs.microsoft.com/azure/postgresql/concepts-servers)\nto which you can deploy many databases. The pricing is structured per-server based upon\ncores and storage.\n\n\n  - [The Hyperscale (Citus) option is powered by Citus Data technology. It enables high](https://azure.microsoft.com/blog/get-high-performance-scaling-for-your-azure-database-workloa', 'ds-with-hyperscale/)\nperformance by _horizontally scaling_ a single database across hundreds of nodes to deliver fast\nperformance and scale. This option allows the engine to fit more data in memory, parallelize\nqueries across hundreds of nodes, and index data faster.\n\n#### **NoSQL data in Azure**\n\n\nCosmos DB is a fully managed, globally distributed NoSQL database service in the Azure cloud. It has\nbeen adopted by many large companies across the w', 'orld, including Coca-Cola, Skype, ExxonMobil,\nand Liberty Mutual.\n\n\nIf your services require fast response from anywhere in the world, high availability, or elastic\nscalability, Cosmos DB is a great choice. Figure 5-12 shows Cosmos DB.\n\n\n102 CHAPTER 5 | Cloud-native data patterns\n\n\n_Figure 5-12: Overview of Azure Cosmos DB_\n\n\nThe previous figure presents many of the built-in cloud-native capabilities available in Cosmos DB. In\nthis section, we’ll', ' take a closer look at them.\n\n\n**Global support**\n\n\nCloud-native applications often have a global audience and require global scale.\n\n\nYou can distribute Cosmos databases across regions or around the world, placing data close to your\nusers, improving response time, and reducing latency. You can add or remove a database from a\nregion without pausing or redeploying your services. In the background, Cosmos DB transparently\nreplicates the data to eac', 'h of the configured regions.\n\n\n[Cosmos DB supports active/active clustering at the global level, enabling you to configure any of your](https://kemptechnologies.com/white-papers/unfog-confusion-active-passive-activeactive-load-balancing/)\ndatabase regions to support _both writes and reads_ .\n\n\n[The Multi-region write](https://docs.microsoft.com/azure/cosmos-db/conflict-resolution-policies) protocol is an important feature in Cosmos DB that enable', 's the following\nfunctionality:\n\n\n  - Unlimited elastic write and read scalability.\n\n\n  - 99.999% read and write availability all around the world.\n\n\n  - Guaranteed reads and writes served in less than 10 milliseconds at the 99th percentile.\n\n\n[With the Cosmos DB Multi-Homing APIs, your microservice is automatically aware of the nearest](https://docs.microsoft.com/azure/cosmos-db/distribute-data-globally)\nAzure region and sends requests to it. The', ' nearest region is identified by Cosmos DB without any\nconfiguration changes. Should a region become unavailable, the Multi-Homing feature will\nautomatically route requests to the next nearest available region.\n\n\n**Multi-model support**\n\n\nWhen replatforming monolithic applications to a cloud-native architecture, development teams\nsometimes have to migrate open-source, NoSQL data stores. Cosmos DB can help you preserve your\n\n\n103 CHAPTER 5 | Cloud', '-native data patterns\n\n\ninvestment in these NoSQL datastores with its _multi-model_ data platform. The following table shows\n[the supported NoSQL compatibility APIs.](https://www.wikiwand.com/en/Cosmos_DB)\n\n|Provider|Description|\n|---|---|\n|NoSQL API|API for NoSQL stores data in document format|\n|Mongo DB API|Supports Mongo DB APIs and JSON documents|\n|Gremlin API|Supports Gremlin API with graph-based nodes<br>and edge data representations|\n|Cass', 'andra API|Supports Casandra API for wide-column data<br>representations|\n|Table API|Supports Azure Table Storage with premium<br>enhancements|\n|PostgreSQL API|Managed service for running PostgreSQL at any<br>scale|\n\n\n\nDevelopment teams can migrate existing Mongo, Gremlin, or Cassandra databases into Cosmos DB\nwith minimal changes to data or code. For new apps, development teams can choose among opensource options or the built-in SQL API model.\n\n\n', 'Internally, Cosmos stores the data in a simple struct format made up of primitive data types. For each\nrequest, the database engine translates the primitive data into the model representation you’ve\nselected.\n\n\n[In the previous table, note the Table API](https://docs.microsoft.com/azure/cosmos-db/table-introduction) option. This API is an evolution of Azure Table Storage. Both\nshare the same underlying table model, but the Cosmos DB Table API add', 's premium enhancements\nnot available in the Azure Storage API. The following table contrasts the features.\n\n\n\n\n\n\n\n\n\n\n\n\n|Feature|Azure Table Storage|Azure Cosmos DB|\n|---|---|---|\n|Latency|Fast|Single-digit millisecond latency for reads and<br>writes anywhere in the world|\n|Throughp<br>ut|Limit of 20,000 operations per table|Unlimited operations per table|\n|Global<br>Distributio<br>n|Single region with optional single<br>secondary read region|Turn', 'key distributions to all regions with<br>automatic failover|\n|Indexing|Available for partition and row key<br>properties only|Automatic indexing of all properties|\n|Pricing|Optimized for cold workloads (low<br>throughput : storage ratio)|Optimized for hot workloads (high<br>throughput : storage ratio)|\n\n\n\nMicroservices that consume Azure Table storage can easily migrate to the Cosmos DB Table API. No\ncode changes are required.\n\n\n104 CHAPTER 5 | C', 'loud-native data patterns\n\n\n**Tunable consistency**\n\n\nEarlier in the _Relational vs. NoSQL_ section, we discussed the subject of _data consistency_ . Data\nconsistency refers to the _integrity_ of your data. Cloud-native services with distributed data rely on\nreplication and must make a fundamental tradeoff between read consistency, availability, and latency.\n\n\nMost distributed databases allow developers to choose between two consistency\nmodels: s', 'trong consistency and eventual consistency. _Strong consistency_ is the gold standard of data\nprogrammability. It guarantees that a query will always return the most current data - even if the\nsystem must incur latency waiting for an update to replicate across all database copies. While a\ndatabase configured for _eventual consistency_ will return data immediately, even if that data isn’t the\nmost current copy. The latter option enables higher ava', 'ilability, greater scale, and increased\nperformance.\n\n\n[Azure Cosmos DB offers five well-defined consistency models](https://docs.microsoft.com/azure/cosmos-db/consistency-levels) shown in Figure 5-13.\n\n\n_Figure 5-13: Cosmos DB Consistency Levels_\n\n\nThese options enable you to make precise choices and granular tradeoffs for consistency, availability,\nand the performance for your data. The levels are presented in the following table.\n\n|Consistency', ' Level|Description|\n|---|---|\n|Eventual|No ordering guarantee for reads. Replicas will<br>eventually converge.|\n|Constant Prefix|Reads are still eventual, but data is returned in<br>the ordering in which it is written.|\n|Session|Guarantees you can read any data written<br>during the current session. It is the default<br>consistency level.|\n|Bounded Staleness|Reads trail writes by interval that you specify.|\n|Strong|Reads are guaranteed to return ', 'most recent<br>committed version of an item. A client never<br>sees an uncommitted or partial read.|\n\n\n\n[In the article Getting Behind the 9-Ball: Cosmos DB Consistency Levels Explained, Microsoft Program](https://blog.jeremylikness.com/blog/2018-03-23_getting-behind-the-9ball-cosmosdb-consistency-levels/)\nManager Jeremy Likness provides an excellent explanation of the five models.\n\n\n**Partitioning**\n\n\n[Azure Cosmos DB embraces automatic partitio', 'ning](https://docs.microsoft.com/azure/cosmos-db/partitioning-overview) to scale a database to meet the performance\nneeds of your cloud-native services.\n\n\nYou manage data in Cosmos DB data by creating databases, containers, and items.\n\n\n105 CHAPTER 5 | Cloud-native data patterns\n\n\nContainers live in a Cosmos DB database and represent a schema-agnostic grouping of items. Items\nare the data that you add to the container. They’re represented as docu', 'ments, rows, nodes, or edges.\nAll items added to a container are automatically indexed.\n\n\nTo partition the container, items are divided into distinct subsets called logical partitions. Logical\npartitions are populated based on the value of a partition key that is associated with each item in a\ncontainer. Figure 5-14 shows two containers each with a logical partition based on a partition key\nvalue.\n\n\n_Figure 5-14: Cosmos DB partitioning mechanics_', '\n\n\nNote in the previous figure how each item includes a partition key of either ‘city’ or ‘airport’. The key\ndetermines the item’s logical partition. Items with a city code are assigned to the container on the left,\nand items with an airport code, to the container on the right. Combining the partition key value with\nthe ID value creates an item’s index, which uniquely identifies the item.\n\n\n[Internally, Cosmos DB automatically manages the placeme', 'nt of logical partitions](https://docs.microsoft.com/azure/cosmos-db/partition-data) on physical\npartitions to satisfy the scalability and performance needs of the container. As application throughput\nand storage requirements increase, Azure Cosmos DB redistributes logical partitions across a greater\nnumber of servers. Redistribution operations are managed by Cosmos DB and invoked without\ninterruption or downtime.\n\n#### **NewSQL databases**\n\n\n_Ne', 'wSQL_ is an emerging database technology that combines the distributed scalability of NoSQL with\nthe ACID guarantees of a relational database. NewSQL databases are important for business systems\nthat must process high-volumes of data, across distributed environments, with full transactional\nsupport and ACID compliance. While a NoSQL database can provide massive scalability, it does not\nguarantee data consistency. Intermittent problems from incons', 'istent data can place a burden on the\ndevelopment team. Developers must construct safeguards into their microservice code to manage\nproblems caused by inconsistent data.\n\n\nThe Cloud Native Computing Foundation (CNCF) features several NewSQL database projects.\n\n\n106 CHAPTER 5 | Cloud-native data patterns\n\n\n|Project|Characteristics|\n|---|---|\n|Cockroach DB|An ACID-compliant, relational database that<br>scales globally. Add a new node to a cluster a', 'nd<br>CockroachDB takes care of balancing the data<br>across instances and geographies. It creates,<br>manages, and distributes replicas to ensure<br>reliability. It’s open source and freely available.|\n|TiDB|An open-source database that supports Hybrid<br>Transactional and Analytical Processing (HTAP)<br>workloads. It is MySQL-compatible and features<br>horizontal scalability, strong consistency, and<br>high availability. TiDB acts like a MySQL ', 'server.<br>You can continue to use existing MySQL client<br>libraries, without requiring extensive code<br>changes to your application.|\n|YugabyteDB|An open source, high-performance, distributed<br>SQL database. It supports low query latency,<br>resilience against failures, and global data<br>distribution. YugabyteDB is PostgreSQL-<br>compatible and handles scale-out RDBMS and<br>internet-scale OLTP workloads. The product also<br>supports NoSQL a', 'nd is compatible with<br>Cassandra.|\n|Vitess|Vitess is a database solution for deploying,<br>scaling, and managing large clusters of MySQL<br>instances. It can run in a public or private cloud<br>architecture. Vitess combines and extends many<br>important MySQL features and features both<br>vertical and horizontal sharding support.<br>Originated by YouTube, Vitess has been serving<br>all YouTube database traffic since 2011.|\n\n\nThe open-source pro', 'jects in the previous figure are available from the Cloud Native Computing\nFoundation. Three of the offerings are full database products, which include .NET support. The other,\nVitess, is a database clustering system that horizontally scales large clusters of MySQL instances.\n\n\nA key design goal for NewSQL databases is to work natively in Kubernetes, taking advantage of the\nplatform’s resiliency and scalability.\n\n\nNewSQL databases are designed to', ' thrive in ephemeral cloud environments where underlying virtual\nmachines can be restarted or rescheduled at a moment’s notice. The databases are designed to\nsurvive node failures without data loss nor downtime. CockroachDB, for example, is able to survive a\nmachine loss by maintaining three consistent replicas of any data across the nodes in a cluster.\n\n\nKubernetes uses a _Services construct_ to allow a client to address a group of identical New', 'SQL\ndatabases processes from a single DNS entry. By decoupling the database instances from the address\n\n\n107 CHAPTER 5 | Cloud-native data patterns\n\n\nof the service with which it’s associated, we can scale without disrupting existing application instances.\nSending a request to any service at a given time will always yield the same result.\n\n\nIn this scenario, all database instances are equal. There are no primary or secondary relationships.\nTechni', 'ques like _consensus replication_ found in CockroachDB allow any database node to handle any\nrequest. If the node that receives a load-balanced request has the data it needs locally, it responds\nimmediately. If not, the node becomes a gateway and forwards the request to the appropriate nodes\nto get the correct answer. From the client’s perspective, every database node is the same: They appear\nas a single _logical_ database with the consistency gu', 'arantees of a single-machine system, despite\nhaving dozens or even hundreds of nodes that are working behind the scenes.\n\n\n[For a detailed look at the mechanics behind NewSQL databases, see the DASH: Four Properties of](https://thenewstack.io/dash-four-properties-of-kubernetes-native-databases/)\n[Kubernetes-Native Databases article.](https://thenewstack.io/dash-four-properties-of-kubernetes-native-databases/)\n\n#### **Data migration to the cloud**', '\n\n\nOne of the more time-consuming tasks is migrating data from one data platform to another. The\n[Azure Data Migration Service can help expedite such efforts. It can migrate data from several external](https://azure.microsoft.com/services/database-migration/)\ndatabase sources into Azure Data platforms with minimal downtime. Target platforms include the\nfollowing services:\n\n\n  - Azure SQL Database\n\n  - Azure Database for MySQL\n\n  - Azure Database ', 'for MariaDB\n\n  - Azure Database for PostgreSQL\n\n  - Azure Cosmos DB\n\n\nThe service provides recommendations to guide you through the changes required to execute a\nmigration, both small or large.\n\n### Caching in a cloud-native app\n\n\nThe benefits of caching are well understood. The technique works by temporarily copying frequently\naccessed data from a backend data store to _fast storage_ that’s located closer to the application.\nCaching is often imp', 'lemented where…\n\n\n  - Data remains relatively static.\n\n  - Data access is slow, especially compared to the speed of the cache.\n\n  - Data is subject to high levels of contention.\n\n#### **Why?**\n\n\n[As discussed in the Microsoft caching guidance, caching can increase performance, scalability, and](https://docs.microsoft.com/azure/architecture/best-practices/caching)\navailability for individual microservices and the system as a whole. It reduces the ', 'latency and\ncontention of handling large volumes of concurrent requests to a data store. As data volume and the\nnumber of users increase, the greater the benefits of caching become.\n\n\n108 CHAPTER 5 | Cloud-native data patterns\n\n\nCaching is most effective when a client repeatedly reads data that is immutable or that changes\ninfrequently. Examples include reference information such as product and pricing information, or\nshared static resources that', ' are costly to construct.\n\n\nWhile microservices should be stateless, a distributed cache can support concurrent access to session\nstate data when absolutely required.\n\n\nAlso consider caching to avoid repetitive computations. If an operation transforms data or performs a\ncomplicated calculation, cache the result for subsequent requests.\n\n#### **Caching architecture**\n\n\nCloud native applications typically implement a distributed caching architectur', 'e. The cache is hosted\nas a cloud-based backing service, separate from the microservices. Figure 5-15 shows the architecture.\n\n\n_Figure 5-15: Caching in a cloud native app_\n\n\nIn the previous figure, note how the cache is independent of and shared by the microservices. In this\nscenario, the cache is invoked by the API Gateway. As discussed in chapter 4, the gateway serves as a\nfront end for all incoming requests. The distributed cache increases sy', 'stem responsiveness by\nreturning cached data whenever possible. Additionally, separating the cache from the services allows\nthe cache to scale up or out independently to meet increased traffic demands.\n\n\n[The previous figure presents a common caching pattern known as the cache-aside pattern. For an](https://docs.microsoft.com/azure/architecture/patterns/cache-aside)\nincoming request, you first query the cache (step #1) for a response. If found, t', 'he data is returned\n[immediately. If the data doesn’t exist in the cache (known as a cache miss), it’s retrieved from a local](https://www.techopedia.com/definition/6308/cache-miss)\ndatabase in a downstream service (step #2). It’s then written to the cache for future requests (step #3),\nand returned to the caller. Care must be taken to periodically evict cached data so that the system\nremains timely and consistent.\n\n\nAs a shared cache grows, it m', 'ight prove beneficial to partition its data across multiple nodes. Doing\nso can help minimize contention and improve scalability. Many Caching services support the ability to\n\n\n109 CHAPTER 5 | Cloud-native data patterns\n\n\ndynamically add and remove nodes and rebalance data across partitions. This approach typically\ninvolves clustering. Clustering exposes a collection of federated nodes as a seamless, single cache.\nInternally, however, the data is', ' dispersed across the nodes following a predefined distribution strategy\nthat balances the load evenly.\n\n#### **Azure Cache for Redis**\n\n\n[Azure Cache for Redis](https://azure.microsoft.com/services/cache/) is a secure data caching and messaging broker service, fully managed by\nMicrosoft. Consumed as a Platform as a Service (PaaS) offering, it provides high throughput and lowlatency access to data. The service is accessible to any application wit', 'hin or outside of Azure.\n\n\nThe Azure Cache for Redis service manages access to open-source Redis servers hosted across Azure\ndata centers. The service acts as a facade providing management, access control, and security. The\nservice natively supports a rich set of data structures, including strings, hashes, lists, and sets. If your\napplication already uses Redis, it will work as-is with Azure Cache for Redis.\n\n\nAzure Cache for Redis is more than a', ' simple cache server. It can support a number of scenarios to\nenhance a microservices architecture:\n\n\n  - An in-memory data store\n\n  - A distributed non-relational database\n\n  - A message broker\n\n  - A configuration or discovery server\n\n\n[For advanced scenarios, a copy of the cached data can be persisted to disk. If a catastrophic event](https://docs.microsoft.com/azure/azure-cache-for-redis/cache-how-to-premium-persistence)\ndisables both the pri', 'mary and replica caches, the cache is reconstructed from the most recent\nsnapshot.\n\n\nAzure Redis Cache is available across a number of predefined configurations and pricing tiers. The\n[Premium tier](https://docs.microsoft.com/azure/azure-cache-for-redis/cache-overview#service-tiers) features many enterprise-level features such as clustering, data persistence, georeplication, and virtual-network isolation.\n\n### Elasticsearch in a cloud-native app\n', '\n\nElasticsearch is a distributed search and analytics system that enables complex search capabilities\nacross diverse types of data. It’s open source and widely popular. Consider how the following\ncompanies integrate Elasticsearch into their application:\n\n\n  - [Wikipedia](https://blog.wikimedia.org/2014/01/06/wikimedia-moving-to-elasticsearch/) for full-text and incremental (search as you type) searching.\n\n  - [GitHub](https://www.elastic.co/custo', 'mers/github) to index and expose over 8 million code repositories.\n\n\n  - [Docker](https://www.elastic.co/customers/docker) for making its container library discoverable.\n\n\n[Elasticsearch is built on top of the Apache Lucene](https://lucene.apache.org/core/) full-text search engine. Lucene provides highperformance document indexing and querying. It indexes data with an inverted indexing scheme –\ninstead of mapping pages to keywords, it maps keywor', 'ds to pages just like a glossary at the end of a\nbook. Lucene has powerful query syntax capabilities and can query data by:\n\n\n110 CHAPTER 5 | Cloud-native data patterns\n\n\n  - Term (a full word)\n\n  - Prefix (starts-with word)\n\n  - Wildcard (using “*” or “?” filters)\n\n  - Phrase (a sequence of text in a document)\n\n  - Boolean value (complex searches combining queries)\n\n\nWhile Lucene provides low-level plumbing for searching, Elasticsearch provides ', 'the server that sits on\ntop of Lucene. Elasticsearch adds higher-level functionality to simplify working Lucene, including a\nRESTful API to access Lucene’s indexing and searching functionality. It also provides a distributed\ninfrastructure capable of massive scalability, fault tolerance, and high availability.\n\n\nFor larger cloud-native applications with complex search requirements, Elasticsearch is available as\nmanaged service in Azure. The Micro', 'soft Azure Marketplace features preconfigured templates which\ndevelopers can use to deploy an Elasticsearch cluster on Azure.\n\n\nFrom the Microsoft Azure Marketplace, developers can use preconfigured templates built to quickly\ndeploy an Elasticsearch cluster on Azure. Using the Azure-managed offering, you can deploy up to 50\ndata nodes, 20 coordinating nodes, and three dedicated master nodes.\n\n#### **Summary**\n\n\nThis chapter presented a detailed l', 'ook at data in cloud-native systems. We started by contrasting data\nstorage in monolithic applications with data storage patterns in cloud-native systems. We looked at\ndata patterns implemented in cloud-native systems, including cross-service queries, distributed\ntransactions, and patterns to deal with high-volume systems. We contrasted SQL with NoSQL data.\nWe looked at data storage options available in Azure that include both Microsoft-centric a', 'nd opensource options. Finally, we discussed caching and Elasticsearch in a cloud-native application.\n\n\n**References**\n\n\n  - [Command and Query Responsibility Segregation (CQRS) pattern](https://docs.microsoft.com/azure/architecture/patterns/cqrs)\n\n\n  - [Event Sourcing pattern](https://docs.microsoft.com/azure/architecture/patterns/event-sourcing)\n\n\n  - [Why isn’t RDBMS Partition Tolerant in CAP Theorem and why is it Available?](https://stackover', 'flow.com/questions/36404765/why-isnt-rdbms-partition-tolerant-in-cap-theorem-and-why-is-it-available)\n\n\n  - [Materialized View](https://docs.microsoft.com/azure/architecture/patterns/materialized-view)\n\n\n  - [All you really need to know about open source databases](https://www.ibm.com/blogs/systems/all-you-really-need-to-know-about-open-source-databases/)\n\n\n  - [Compensating Transaction pattern](https://docs.microsoft.com/azure/architecture/patte', 'rns/compensating-transaction)\n\n\n  - [Saga Pattern](https://microservices.io/patterns/data/saga.html)\n\n\n  - [Saga Patterns | How to implement business transactions using microservices](https://blog.couchbase.com/saga-pattern-implement-business-transactions-using-microservices-part/)\n\n\n  - [Compensating Transaction pattern](https://docs.microsoft.com/azure/architecture/patterns/compensating-transaction)\n\n\n  - [Getting Behind the 9-Ball: Cosmos DB C', 'onsistency Levels Explained](https://blog.jeremylikness.com/blog/2018-03-23_getting-behind-the-9ball-cosmosdb-consistency-levels/)\n\n\n  - [On RDBMS, NoSQL and NewSQL databases. Interview with John Ryan](http://www.odbms.org/blog/2018/03/on-rdbms-nosql-and-newsql-databases-interview-with-john-ryan/)\n\n\n111 CHAPTER 5 | Cloud-native data patterns\n\n\n  - [SQL vs NoSQL vs NewSQL: The Full Comparison](https://www.xenonstack.com/blog/sql-vs-nosql-vs-newsql', '/)\n\n\n  - [DASH: Four Properties of Kubernetes-Native Databases](https://thenewstack.io/dash-four-properties-of-kubernetes-native-databases/)\n\n\n  - [CockroachDB](https://www.cockroachlabs.com/)\n\n\n  - [TiDB](https://pingcap.com/en/)\n\n\n  - [YugabyteDB](https://www.yugabyte.com/)\n\n\n  - [Vitess](https://vitess.io/)\n\n\n  - [Elasticsearch: The Definitive Guide](https://shop.oreilly.com/product/0636920028505.do)\n\n\n  - [Introduction to Apache Lucene](https', '://www.baeldung.com/lucene)\n\n\n112 CHAPTER 5 | Cloud-native data patterns\n\n\n**CHAPTER**\n# 6\n\n## Cloud-native resiliency\n\n\nResiliency is the ability of your system to react to failure and still remain functional. It’s not about\navoiding failure, but accepting failure and constructing your cloud-native services to respond to it.\nYou want to return to a fully functioning state quickly as possible.\n\n\nUnlike traditional monolithic applications, where e', 'verything runs together in a single process, cloudnative systems embrace a distributed architecture as shown in Figure 6-1:\n\n\n_Figure 6-1. Distributed cloud-native environment_\n\n\n[In the previous figure, each microservice and cloud-based backing service](https://12factor.net/backing-services) execute in a separate\nprocess, across server infrastructure, communicating via network-based calls.\n\n\nOperating in this environment, a service must be sensi', 'tive to many different challenges:\n\n\n  - Unexpected network latency - the time for a service request to travel to the receiver and back.\n\n\n  - [Transient faults](https://docs.microsoft.com/azure/architecture/best-practices/transient-faults)  - short-lived network connectivity errors.\n\n\n  - Blockage by a long-running synchronous operation.\n\n\n  - A host process that has crashed and is being restarted or moved.\n\n\n  - An overloaded microservice that ', 'can’t respond for a short time.\n\n\n  - An in-flight orchestrator operation such as a rolling upgrade or moving a service from one\nnode to another.\n\n\n113 CHAPTER 6 | Cloud-native resiliency\n\n\n  - Hardware failures.\n\n\nCloud platforms can detect and mitigate many of these infrastructure issues. It may restart, scale out,\nand even redistribute your service to a different node. However, to take full advantage of this built-in\nprotection, you must desig', 'n your services to react to it and thrive in this dynamic environment.\n\n\nIn the following sections, we’ll explore defensive techniques that your service and managed cloud\nresources can leverage to minimize downtime and disruption.\n\n### Application resiliency patterns\n\n\nThe first line of defense is application resiliency.\n\n\nWhile you could invest considerable time writing your own resiliency framework, such products\n[already exist. Polly](https://', 'old.dotnetfoundation.org/projects/polly) is a comprehensive .NET resilience and transient-fault-handling library that allows\ndevelopers to express resiliency policies in a fluent and thread-safe manner. Polly targets applications\nbuilt with either .NET Framework or .NET 7. The following table describes the resiliency features, called\npolicies, available in the Polly Library. They can be applied individually or grouped together.\n\n|Policy|Experienc', 'e|\n|---|---|\n|Retry|Configures retry operations on designated<br>operations.|\n|Circuit Breaker|Blocks requested operations for a predefined<br>period when faults exceed a configured<br>threshold|\n|Timeout|Places limit on the duration for which a caller<br>can wait for a response.|\n|Bulkhead|Constrains actions to fixed-size resource pool to<br>prevent failing calls from swamping a resource.|\n|Cache|Stores responses automatically.|\n|Fallback|Define', 's structured behavior upon a failure.|\n\n\n\nNote how in the previous figure the resiliency policies apply to request messages, whether coming\nfrom an external client or back-end service. The goal is to compensate the request for a service that\nmight be momentarily unavailable. These short-lived interruptions typically manifest themselves with\nthe HTTP status codes shown in the following table.\n\n|HTTP Status Code|Cause|\n|---|---|\n|404|Not Found|\n|40', '8|Request timeout|\n|429|Too many requests (you’ve most likely been throttled)|\n|502|Bad gateway|\n|503|Service unavailable|\n\n\n\n114 CHAPTER 6 | Cloud-native resiliency\n\n\n|HTTP Status Code|Cause|\n|---|---|\n|504|Gateway timeout|\n\n\nQuestion: Would you retry an HTTP Status Code of 403 - Forbidden? No. Here, the system is\nfunctioning properly, but informing the caller that they aren’t authorized to perform the requested\noperation. Care must be taken to ', 'retry only those operations caused by failures.\n\n\nAs recommended in Chapter 1, Microsoft developers constructing cloud-native applications should\n[target the .NET platform. Version 2.1 introduced the HTTPClientFactory](https://www.stevejgordon.co.uk/introduction-to-httpclientfactory-aspnetcore) library for creating HTTP Client\ninstances for interacting with URL-based resources. Superseding the original HTTPClient class, the\n[factory class support', 's many enhanced features, one of which is tight integration](https://docs.microsoft.com/dotnet/architecture/microservices/implement-resilient-applications/implement-http-call-retries-exponential-backoff-polly) with the Polly\nresiliency library. With it, you can easily define resiliency policies in the application Startup class to\nhandle partial failures and connectivity issues.\n\n\nNext, let’s expand on retry and circuit breaker patterns.\n\n\n**Retry', ' pattern**\n\n\nIn a distributed cloud-native environment, calls to services and cloud resources can fail because of\ntransient (short-lived) failures, which typically correct themselves after a brief period of time.\nImplementing a retry strategy helps a cloud-native service mitigate these scenarios.\n\n\n[The Retry pattern](https://docs.microsoft.com/azure/architecture/patterns/retry) enables a service to retry a failed request operation a (configurabl', 'e) number of\ntimes with an exponentially increasing wait time. Figure 6-2 shows a retry in action.\n\n\n_Figure 6-2. Retry pattern in action_\n\n\nIn the previous figure, a retry pattern has been implemented for a request operation. It’s configured to\nallow up to four retries before failing with a backoff interval (wait time) starting at two seconds, which\nexponentially doubles for each subsequent attempt.\n\n\n  - The first invocation fails and returns a', 'n HTTP status code of 500. The application waits for two\nseconds and retries the call.\n\n\n115 CHAPTER 6 | Cloud-native resiliency\n\n\n  - The second invocation also fails and returns an HTTP status code of 500. The application now\ndoubles the backoff interval to four seconds and retries the call.\n\n  - Finally, the third call succeeds.\n\n  - In this scenario, the retry operation would have attempted up to four retries while doubling\nthe backoff durati', 'on before failing the call.\n\n  - Had the 4th retry attempt failed, a fallback policy would be invoked to gracefully handle the\nproblem.\n\n\nIt’s important to increase the backoff period before retrying the call to allow the service time to selfcorrect. It’s a best practice to implement an exponentially increasing backoff (doubling the period on\neach retry) to allow adequate correction time.\n\n#### **Circuit breaker pattern**\n\n\nWhile the retry patter', 'n can help salvage a request entangled in a partial failure, there are situations\nwhere failures can be caused by unanticipated events that will require longer periods of time to\nresolve. These faults can range in severity from a partial loss of connectivity to the complete failure of\na service. In these situations, it’s pointless for an application to continually retry an operation that is\nunlikely to succeed.\n\n\nTo make things worse, executing c', 'ontinual retry operations on a non-responsive service can move\nyou into a self-imposed denial of service scenario where you flood your service with continual calls\nexhausting resources such as memory, threads and database connections, causing failure in unrelated\nparts of the system that use the same resources.\n\n\nIn these situations, it would be preferable for the operation to fail immediately and only attempt to\ninvoke the service if it’s likely', ' to succeed.\n\n\n[The Circuit Breaker pattern can prevent an application from repeatedly trying to execute an operation](https://docs.microsoft.com/azure/architecture/patterns/circuit-breaker)\nthat’s likely to fail. After a pre-defined number of failed calls, it blocks all traffic to the service.\nPeriodically, it will allow a trial call to determine whether the fault has resolved. Figure 6-3 shows the\nCircuit Breaker pattern in action.\n\n\n116 CHAPTE', 'R 6 | Cloud-native resiliency\n\n\n_Figure 6-3. Circuit breaker pattern in action_\n\n\nIn the previous figure, a Circuit Breaker pattern has been added to the original retry pattern. Note how\nafter 100 failed requests, the circuit breakers opens and no longer allows calls to the service. The\nCheckCircuit value, set at 30 seconds, specifies how often the library allows one request to proceed to\nthe service. If that call succeeds, the circuit closes and', ' the service is once again available to traffic.\n\n\nKeep in mind that the intent of the Circuit Breaker pattern is _different_ than that of the Retry pattern.\nThe Retry pattern enables an application to retry an operation in the expectation that it will succeed.\nThe Circuit Breaker pattern prevents an application from doing an operation that is likely to fail.\nTypically, an application will _combine_ these two patterns by using the Retry pattern t', 'o invoke an\noperation through a circuit breaker.\n\n#### **Testing for resiliency**\n\n\nTesting for resiliency cannot always be done the same way that you test application functionality (by\nrunning unit tests, integration tests, and so on). Instead, you must test how the end-to-end workload\nperforms under failure conditions, which only occur intermittently. For example: inject failures by\ncrashing processes, expired certificates, make dependent servi', 'ces unavailable etc. Frameworks like\n[chaos-monkey](https://github.com/Netflix/chaosmonkey) can be used for such chaos testing.\n\n\nApplication resiliency is a must for handling problematic requested operations. But, it’s only half of the\nstory. Next, we cover resiliency features available in the Azure cloud.\n\n### Azure platform resiliency\n\n\nBuilding a reliable application in the cloud is different from traditional on-premises application\ndevelopme', 'nt. While historically you purchased higher-end hardware to scale up, in a cloud\nenvironment you scale out. Instead of trying to prevent failures, the goal is to minimize their effects\nand keep the system stable.\n\n\n117 CHAPTER 6 | Cloud-native resiliency\n\n\nThat said, reliable cloud applications display distinct characteristics:\n\n\n  - They’re resilient, recover gracefully from problems, and continue to function.\n\n  - They’re highly available (HA) ', 'and run as designed in a healthy state with no significant\ndowntime.\n\n\nUnderstanding how these characteristics work together - and how they affect cost - is essential to\nbuilding a reliable cloud-native application. We’ll next look at ways that you can build resiliency and\navailability into your cloud-native applications leveraging features from the Azure cloud.\n\n#### **Design with resiliency**\n\n\nWe’ve said resiliency enables your application to ', 'react to failure and still remain functional. The\n[whitepaper, Resilience in Azure whitepaper, provides guidance for achieving resilience in the Azure](https://azure.microsoft.com/mediahandler/files/resourcefiles/resilience-in-azure-whitepaper/Resilience%20in%20Azure.pdf)\nplatform. Here are some key recommendations:\n\n\n  - _Hardware failure._ Build redundancy into the application by deploying components across\ndifferent fault domains. For example,', ' ensure that Azure VMs are placed in different racks by\nusing Availability Sets.\n\n\n  - _Datacenter failure._ Build redundancy into the application with fault isolation zones across\ndatacenters. For example, ensure that Azure VMs are placed in different fault-isolated\ndatacenters by using Azure Availability Zones.\n\n\n  - _Regional failure._ Replicate the data and components into another region so that applications\ncan be quickly recovered. For exam', 'ple, use Azure Site Recovery to replicate Azure VMs to\nanother Azure region.\n\n\n  - _Heavy load._ Load balance across instances to handle spikes in usage. For example, put two or\nmore Azure VMs behind a load balancer to distribute traffic to all VMs.\n\n\n  - _Accidental data deletion or corruption._ Back up data so it can be restored if there’s any\ndeletion or corruption. For example, use Azure Backup to periodically back up your Azure\nVMs.\n\n#### **', 'Design with redundancy**\n\n\nFailures vary in scope of impact. A hardware failure, such as a failed disk, can affect a single node in a\ncluster. A failed network switch could affect an entire server rack. Less common failures, such as loss of\npower, could disrupt a whole datacenter. Rarely, an entire region becomes unavailable.\n\n\n[Redundancy](https://docs.microsoft.com/azure/architecture/guide/design-principles/redundancy) is one way to provide app', 'lication resilience. The exact level of redundancy needed\ndepends upon your business requirements and will affect both the cost and complexity of your\nsystem. For example, a multi-region deployment is more expensive and more complex to manage\nthan a single-region deployment. You’ll need operational procedures to manage failover and failback.\nThe additional cost and complexity might be justified for some business scenarios, but not others.\n\n\nTo ar', 'chitect redundancy, you need to identify the critical paths in your application, and then\ndetermine if there’s redundancy at each point in the path? If a subsystem should fail, will the\napplication fail over to something else? Finally, you need a clear understanding of those features built\n\n\n118 CHAPTER 6 | Cloud-native resiliency\n\n\ninto the Azure cloud platform that you can leverage to meet your redundancy requirements. Here are\nrecommendations ', 'for architecting redundancy:\n\n\n  - _Deploy multiple instances of services._ If your application depends on a single instance of a\nservice, it creates a single point of failure. Provisioning multiple instances improves both\nresiliency and scalability. When hosting in Azure Kubernetes Service, you can declaratively\nconfigure redundant instances (replica sets) in the Kubernetes manifest file. The replica count\nvalue can be managed programmatically, ', 'in the portal, or through autoscaling features.\n\n\n  - _Leveraging a load balancer._ Load-balancing distributes your application’s requests to healthy\nservice instances and automatically removes unhealthy instances from rotation. When\ndeploying to Kubernetes, load balancing can be specified in the Kubernetes manifest file in\nthe Services section.\n\n\n  - _Plan for multiregion deployment._ If you deploy your application to a single region, and that\nr', 'egion becomes unavailable, your application will also become unavailable. This may be\nunacceptable under the terms of your application’s service level agreements. Instead, consider\ndeploying your application and its services across multiple regions. For example, an Azure\nKubernetes Service (AKS) cluster is deployed to a single region. To protect your system from a\nregional failure, you might deploy your application to multiple AKS clusters across', ' different\n[regions and use the Paired Regions](https://docs.microsoft.com/azure/virtual-machines/regions#region-pairs) feature to coordinate platform updates and prioritize\nrecovery efforts.\n\n\n  - _[Enable geo-replication.](https://docs.microsoft.com/azure/sql-database/sql-database-active-geo-replication)_ Geo-replication for services such as Azure SQL Database and Cosmos\nDB will create secondary replicas of your data across multiple regions. Wh', 'ile both services will\nautomatically replicate data within the same region, geo-replication protects you against a\nregional outage by enabling you to fail over to a secondary region. Another best practice for\ngeo-replication centers around storing container images. To deploy a service in AKS, you\nneed to store and pull the image from a repository. Azure Container Registry integrates with\nAKS and can securely store container images. To improve per', 'formance and availability,\nconsider geo-replicating your images to a registry in each region where you have an AKS\ncluster. Each AKS cluster then pulls container images from the local container registry in its\nregion as shown in Figure 6-4:\n\n\n_Figure 6-4. Replicated resources across regions_\n\n\n119 CHAPTER 6 | Cloud-native resiliency\n\n\n  - _Implement a DNS traffic load balancer._ [Azure Traffic Manager](https://docs.microsoft.com/azure/traffic-man', 'ager/traffic-manager-overview) provides high-availability for\ncritical applications by load-balancing at the DNS level. It can route traffic to different regions\nbased on geography, cluster response time, and even application endpoint health. For\nexample, Azure Traffic Manager can direct customers to the closest AKS cluster and\napplication instance. If you have multiple AKS clusters in different regions, use Traffic\nManager to control how traffic', ' flows to the applications that run in each cluster. Figure 6-5\nshows this scenario.\n\n\n_Figure 6-5. AKS and Azure Traffic Manager_\n\n#### **Design for scalability**\n\n\nThe cloud thrives on scaling. The ability to increase/decrease system resources to address\nincreasing/decreasing system load is a key tenet of the Azure cloud. But, to effectively scale an\napplication, you need an understanding of the scaling features of each Azure service that you i', 'nclude\nin your application. Here are recommendations for effectively implementing scaling in your system.\n\n\n  - _Design for scaling._ An application must be designed for scaling. To start, services should be\nstateless so that requests can be routed to any instance. Having stateless services also means\nthat adding or removing an instance doesn’t adversely impact current users.\n\n\n  - _Partition workloads_ . Decomposing domains into independent, sel', 'f-contained microservices\nenable each service to scale independently of others. Typically, services will have different\nscalability needs and requirements. Partitioning enables you to scale only what needs to be\nscaled without the unnecessary cost of scaling an entire application.\n\n\n  - _Favor scale-out._ Cloud-based applications favor scaling out resources as opposed to scaling\nup. Scaling out (also known as horizontal scaling) involves adding m', 'ore service resources to\nan existing system to meet and share a desired level of performance. Scaling up (also known\n\n\n120 CHAPTER 6 | Cloud-native resiliency\n\n\nas vertical scaling) involves replacing existing resources with more powerful hardware (more\ndisk, memory, and processing cores). Scaling out can be invoked automatically with the\nautoscaling features available in some Azure cloud resources. Scaling out across multiple\nresources also adds', ' redundancy to the overall system. Finally scaling up a single resource is\ntypically more expensive than scaling out across many smaller resources. Figure 6-6 shows the\ntwo approaches:\n\n\n_Figure 6-6. Scale up versus scale out_\n\n\n  - _Scale proportionally._ When scaling a service, think in terms of _resource sets_ . If you were to\ndramatically scale out a specific service, what impact would that have on back-end data\nstores, caches and dependent s', 'ervices? Some resources such as Cosmos DB can scale out\nproportionally, while many others can’t. You want to ensure that you don’t scale out a\nresource to a point where it will exhaust other associated resources.\n\n\n  - _Avoid affinity._ A best practice is to ensure a node doesn’t require local affinity, often referred\nto as a _sticky session_ . A request should be able to route to any instance. If you need to persist\n[state, it should be saved to', ' a distributed cache, such as Azure Redis cache.](https://azure.microsoft.com/services/cache/)\n\n\n  - _Take advantage of platform autoscaling features._ Use built-in autoscaling features whenever\npossible, rather than custom or third-party mechanisms. Where possible, use scheduled\nscaling rules to ensure that resources are available without a startup delay, but add reactive\nautoscaling to the rules as appropriate, to cope with unexpected changes i', 'n demand. For\n[more information, see Autoscaling guidance.](https://docs.microsoft.com/azure/architecture/best-practices/auto-scaling)\n\n\n  - _Scale out aggressively._ A final practice would be to scale out aggressively so that you can\nquickly meet immediate spikes in traffic without losing business. And, then scale in (that is,\nremove unneeded instances) conservatively to keep the system stable. A simple way to\nimplement this is to set the cool d', 'own period, which is the time to wait between scaling\noperations, to five minutes for adding resources and up to 15 minutes for removing instances.\n\n#### **Built-in retry in services**\n\n\nWe encouraged the best practice of implementing programmatic retry operations in an earlier section.\nKeep in mind that many Azure services and their corresponding client SDKs also include retry\n\n\n121 CHAPTER 6 | Cloud-native resiliency\n\n\nmechanisms. The following', ' list summarizes retry features in the many of the Azure services that are\ndiscussed in this book:\n\n\n  - _Azure Cosmos DB._ [The DocumentClient](https://docs.microsoft.com/dotnet/api/microsoft.azure.documents.client.documentclient) class from the client API automatically retires failed\nattempts. The number of retries and maximum wait time are configurable. Exceptions thrown\nby the client API are either requests that exceed the retry policy or non', '-transient errors.\n\n\n  - _Azure Redis Cache._ The Redis StackExchange client uses a connection manager class that\nincludes retries on failed attempts. The number of retries, specific retry policy and wait time\nare all configurable.\n\n\n  - _Azure Service Bus._ [The Service Bus client exposes a RetryPolicy class](xref:Microsoft.ServiceBus.RetryPolicy) that can be configured\n[with a back-off interval, retry count, and TerminationTimeBuffer, which spe', 'cifies the maximum](https://docs.microsoft.com/dotnet/api/microsoft.servicebus.retryexponential.terminationtimebuffer)\ntime an operation can take. The default policy is nine maximum retry attempts with a 30second backoff period between attempts.\n\n\n  - _Azure SQL Database._ [Retry support is provided when using the Entity Framework Core library.](https://docs.microsoft.com/ef/core/miscellaneous/connection-resiliency)\n\n\n  - _Azure Storage._ The sto', 'rage client library support retry operations. The strategies vary across\nAzure storage tables, blobs, and queues. As well, alternate retries switch between primary and\nsecondary storage services locations when the geo-redundancy feature is enabled.\n\n\n  - _Azure Event Hubs._ The Event Hub client library features a RetryPolicy property, which includes\na configurable exponential backoff feature.\n\n### Resilient communications\n\n\nThroughout this book, ', 'we’ve embraced a microservice-based architectural approach. While such an\narchitecture provides important benefits, it presents many challenges:\n\n\n  - _Out-of-process network communication._ Each microservice communicates over a network\nprotocol that introduces network congestion, latency, and transient faults.\n\n\n  - _Service discovery._ How do microservices discover and communicate with each other when\nrunning across a cluster of machines with t', 'heir own IP addresses and ports?\n\n\n  - _Resiliency._ How do you manage short-lived failures and keep the system stable?\n\n\n  - _Load balancing._ How does inbound traffic get distributed across multiple instances of a\nmicroservice?\n\n\n  - _Security._ How are security concerns such as transport-level encryption and certificate\nmanagement enforced?\n\n\n  - _Distributed Monitoring._  - How do you correlate and capture traceability and monitoring for a\nsi', 'ngle request across multiple consuming microservices?\n\n\nYou can address these concerns with different libraries and frameworks, but the implementation can\nbe expensive, complex, and time-consuming. You also end up with infrastructure concerns coupled to\nbusiness logic.\n\n\n122 CHAPTER 6 | Cloud-native resiliency\n\n\n#### **Service mesh**\n\nA better approach is an evolving technology entitled _Service Mesh_ [. A service mesh](https://www.nginx.com/blog', '/what-is-a-service-mesh/) is a configurable\ninfrastructure layer with built-in capabilities to handle service communication and the other\nchallenges mentioned above. It decouples these concerns by moving them into a service proxy. The\n[proxy is deployed into a separate process (called a sidecar) to provide isolation from business code.](https://docs.microsoft.com/azure/architecture/patterns/sidecar)\nHowever, the sidecar is linked to the service -', ' it’s created with it and shares its lifecycle. Figure 6-7\nshows this scenario.\n\n\n_Figure 6-7. Service mesh with a side car_\n\n\nIn the previous figure, note how the proxy intercepts and manages communication among the\nmicroservices and the cluster.\n\n\n[A service mesh is logically split into two disparate components: A data plane](https://blog.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc) [and control plane. Figure](https://bl', 'og.envoyproxy.io/service-mesh-data-plane-vs-control-plane-2774e720f7fc)\n6-8 shows these components and their responsibilities.\n\n\n123 CHAPTER 6 | Cloud-native resiliency\n\n\n_Figure 6-8. Service mesh control and data plane_\n\n\nOnce configured, a service mesh is highly functional. It can retrieve a corresponding pool of instances\nfrom a service discovery endpoint. The mesh can then send a request to a specific instance, recording\nthe latency and respo', 'nse type of the result. A mesh can choose the instance most likely to return a\nfast response based on many factors, including its observed latency for recent requests.\n\n\nIf an instance is unresponsive or fails, the mesh will retry the request on another instance. If it returns\nerrors, a mesh will evict the instance from the load-balancing pool and restate it after it heals. If a\nrequest times out, a mesh can fail and then retry the request. A mes', 'h captures and emits metrics and\ndistributed tracing to a centralized metrics system.\n\n#### **Istio and Envoy**\n\n\n[While a few service mesh options currently exist, Istio](https://istio.io/docs/concepts/what-is-istio/) is the most popular at the time of this writing.\nIstio is a joint venture from IBM, Google, and Lyft. It’s an open-source offering that can be integrated\ninto a new or existing distributed application. The technology provides a con', 'sistent and complete\nsolution to secure, connect, and monitor microservices. Its features include:\n\n\n  - Secure service-to-service communication in a cluster with strong identity-based\nauthentication and authorization.\n\n  - [Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic.](https://grpc.io/)\n\n  - Fine-grained control of traffic behavior with rich routing rules, retries, failovers, and fault\ninjection.\n\n  - A pluggable policy l', 'ayer and configuration API supporting access controls, rate limits, and\nquotas.\n\n  - Automatic metrics, logs, and traces for all traffic within a cluster, including cluster ingress and\negress.\n\n\n[A key component for an Istio implementation is a proxy service entitled the Envoy proxy. It runs](https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy)\nalongside each service and provides a platform-agnostic foundation for the following featur', 'es:\n\n\n  - Dynamic service discovery.\n\n  - Load balancing.\n\n\n124 CHAPTER 6 | Cloud-native resiliency\n\n\n  - TLS termination.\n\n  - HTTP and gRPC proxies.\n\n  - Circuit breaker resiliency.\n\n  - Health checks.\n\n  - [Rolling updates with canary](https://martinfowler.com/bliki/CanaryRelease.html) deployments.\n\n\nAs previously discussed, Envoy is deployed as a sidecar to each microservice in the cluster.\n\n#### **Integration with Azure Kubernetes Services**', '\n\n\nThe Azure cloud embraces Istio and provides direct support for it within Azure Kubernetes Services.\nThe following links can help you get started:\n\n\n  - [Installing Istio in AKS](https://docs.microsoft.com/azure/aks/istio-install)\n\n  - [Using AKS and Istio](https://docs.microsoft.com/azure/aks/istio-scenario-routing)\n\n\n**References**\n\n\n  - [Polly](https://old.dotnetfoundation.org/projects/polly)\n\n\n  - [Retry pattern](https://docs.microsoft.com/', 'azure/architecture/patterns/retry)\n\n\n  - [Circuit Breaker pattern](https://docs.microsoft.com/azure/architecture/patterns/circuit-breaker)\n\n\n  - [Resilience in Azure whitepaper](https://azure.microsoft.com/mediahandler/files/resourcefiles/resilience-in-azure-whitepaper/Resilience%20in%20Azure.pdf)\n\n\n  - [network latency](https://www.techopedia.com/definition/8553/network-latency)\n\n\n  - [Redundancy](https://docs.microsoft.com/azure/architecture/gu', 'ide/design-principles/redundancy)\n\n\n  - [geo-replication](https://docs.microsoft.com/azure/sql-database/sql-database-active-geo-replication)\n\n\n  - [Azure Traffic Manager](https://docs.microsoft.com/azure/traffic-manager/traffic-manager-overview)\n\n\n  - [Autoscaling guidance](https://docs.microsoft.com/azure/architecture/best-practices/auto-scaling)\n\n\n  - [Istio](https://istio.io/docs/concepts/what-is-istio/)\n\n\n  - [Envoy proxy](https://www.envoypr', 'oxy.io/docs/envoy/latest/intro/what_is_envoy)\n\n\n125 CHAPTER 6 | Cloud-native resiliency\n\n\n**CHAPTER**\n# 7\n\n## Monitoring and health\n\n\nMicroservices and cloud-native applications go hand in hand with good DevOps practices. DevOps is\nmany things to many people but perhaps one of the better definitions comes from cloud advocate\nand DevOps evangelist Donovan Brown:\n\n\n“DevOps is the union of people, process, and products to enable continuous delivery ', 'of value to our\nend users.”\n\n\nUnfortunately, with terse definitions, there’s always room to say more things. One of the key\ncomponents of DevOps is ensuring that the applications running in production are functioning\nproperly and efficiently. To gauge the health of the application in production, it’s necessary to monitor\nthe various logs and metrics being produced from the servers, hosts, and the application proper. The\nnumber of different servic', 'es running in support of a cloud-native application makes monitoring the\nhealth of individual components and the application as a whole a critical challenge.\n\n### Observability patterns\n\n\nJust as patterns have been developed to aid in the layout of code in applications, there are patterns\nfor operating applications in a reliable way. Three useful patterns in maintaining applications have\nemerged: **logging**, **monitoring**, and **alerts** .\n\n###', '# **When to use logging**\n\n\nNo matter how careful we are, applications almost always behave in unexpected ways in production.\nWhen users report problems with an application, it’s useful to be able to see what was going on with\nthe app when the problem occurred. One of the most tried and true ways of capturing information\nabout what an application is doing while it’s running is to have the application write down what it’s\ndoing. This process is kn', 'own as logging. Anytime failures or problems occur in production, the goal\nshould be to reproduce the conditions under which the failures occurred, in a non-production\nenvironment. Having good logging in place provides a roadmap for developers to follow in order to\nduplicate problems in an environment that can be tested and experimented with.\n\n\n**Challenges when logging with cloud-native applications**\n\n\nIn traditional applications, log files are', ' typically stored on the local machine. In fact, on Unix-like\noperating systems, there’s a folder structure defined to hold any logs, typically under /var/log.\n\n\n126 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-1. Logging to a file in a monolithic app._\n\n\nThe usefulness of logging to a flat file on a single machine is vastly reduced in a cloud environment.\nApplications producing logs may not have access to the local disk or the local disk may be', ' highly\ntransient as containers are shuffled around physical machines. Even simple scaling up of monolithic\napplications across multiple nodes can make it challenging to locate the appropriate file-based log\nfile.\n\n\n_Figure 7-2. Logging to files in a scaled monolithic app._\n\n\nCloud-native applications developed using a microservices architecture also pose some challenges for\nfile-based loggers. User requests may now span multiple services that ar', 'e run on different machines\n\n\n127 CHAPTER 7 | Monitoring and health\n\n\nand may include serverless functions with no access to a local file system at all. It would be very\nchallenging to correlate the logs from a user or a session across these many services and machines.\n\n\n_Figure 7-3. Logging to local files in a microservices app._\n\n\nFinally, the number of users in some cloud-native applications is high. Imagine that each user\ngenerates a hundred ', 'lines of log messages when they log into an application. In isolation, that is\nmanageable, but multiply that over 100,000 users and the volume of logs becomes large enough that\nspecialized tools are needed to support effective use of the logs.\n\n\n**Logging in cloud-native applications**\n\n\nEvery programming language has tooling that permits writing logs, and typically the overhead for\nwriting these logs is low. Many of the logging libraries provide', ' logging different kinds of criticalities,\n[which can be tuned at run time. For instance, the Serilog library](https://serilog.net/) is a popular structured logging library\nfor .NET that provides the following logging levels:\n\n\n  - Verbose\n\n  - Debug\n\n  - Information\n\n  - Warning\n\n  - Error\n\n  - Fatal\n\n\nThese different log levels provide granularity in logging. When the application is functioning properly\nin production, it may be configured to on', 'ly log important messages. When the application is\n\n\n128 CHAPTER 7 | Monitoring and health\n\n\nmisbehaving, then the log level can be increased so more verbose logs are gathered. This balances\nperformance against ease of debugging.\n\n\nThe high performance of logging tools and the tunability of verbosity should encourage developers to\nlog frequently. Many favor a pattern of logging the entry and exit of each method. This approach may\nsound like overk', 'ill, but it’s infrequent that developers will wish for less logging. In fact, it’s not\nuncommon to perform deployments for the sole purpose of adding logging around a problematic\nmethod. Err on the side of too much logging and not on too little. Some tools can be used to\nautomatically provide this kind of logging.\n\n\nBecause of the challenges associated with using file-based logs in cloud-native apps, centralized logs\nare preferred. Logs are colle', 'cted by the applications and shipped to a central logging application\nwhich indexes and stores the logs. This class of system can ingest tens of gigabytes of logs every day.\n\n\nIt’s also helpful to follow some standard practices when building logging that spans many services.\n[For instance, generating a correlation ID](https://blog.rapid7.com/2016/12/23/the-value-of-correlation-ids/) at the start of a lengthy interaction, and then logging it in\nea', 'ch message that is related to that interaction, makes it easier to search for all related messages. One\nneed only find a single message and extract the correlation ID to find all the related messages.\nAnother example is ensuring that the log format is the same for every service, whatever the language\nor logging library it uses. This standardization makes reading logs much easier. Figure 7-4\ndemonstrates how a microservices architecture can levera', 'ge centralized logging as part of its\nworkflow.\n\n\n_Figure 7-4. Logs from various sources are ingested into a centralized log store._\n\n\n129 CHAPTER 7 | Monitoring and health\n\n\n#### **Challenges with detecting and responding to potential app health** **issues**\n\nSome applications aren’t mission critical. Maybe they’re only used internally, and when a problem\noccurs, the user can contact the team responsible and the application can be restarted. How', 'ever,\ncustomers often have higher expectations for the applications they consume. You should know when\nproblems occur with your application _before_ users do, or before users notify you. Otherwise, the first\nyou know about a problem may be when you notice an angry deluge of social media posts deriding\nyour application or even your organization.\n\n\nSome scenarios you may need to consider include:\n\n\n  - One service in your application keeps failing ', 'and restarting, resulting in intermittent slow\nresponses.\n\n  - At some times of the day, your application’s response time is slow.\n\n  - After a recent deployment, load on the database has tripled.\n\n\nImplemented properly, monitoring can let you know about conditions that will lead to problems,\nletting you address underlying conditions before they result in any significant user impact.\n\n\n**Monitoring cloud-native apps**\n\n\nSome centralized logging s', 'ystems take on an additional role of collecting telemetry outside of pure\nlogs. They can collect metrics, such as time to run a database query, average response time from a\nweb server, and even CPU load averages and memory pressure as reported by the operating system.\nIn conjunction with the logs, these systems can provide a holistic view of the health of nodes in the\nsystem and the application as a whole.\n\n\nThe metric-gathering capabilities of t', 'he monitoring tools can also be fed manually from within the\napplication. Business flows that are of particular interest such as new users signing up or orders being\nplaced, may be instrumented such that they increment a counter in the central monitoring system.\nThis aspect unlocks the monitoring tools to not only monitor the health of the application but the\nhealth of the business.\n\n\nQueries can be constructed in the log aggregation tools to loo', 'k for certain statistics or patterns, which\ncan then be displayed in graphical form, on custom dashboards. Frequently, teams will invest in large,\nwall-mounted displays that rotate through the statistics related to an application. This way, it’s simple\nto see the problems as they occur.\n\n\nCloud-native monitoring tools provide real-time telemetry and insight into apps regardless of whether\nthey’re single-process monolithic applications or distribu', 'ted microservice architectures. They include\ntools that allow collection of data from the app as well as tools for querying and displaying\ninformation about the app’s health.\n\n#### **Challenges with reacting to critical problems in cloud-native apps**\n\n\nIf you need to react to problems with your application, you need some way to alert the right\npersonnel. This is the third cloud-native application observability pattern and depends on logging and\n', 'monitoring. Your application needs to have logging in place to allow problems to be diagnosed, and\n\n\n130 CHAPTER 7 | Monitoring and health\n\n\nin some cases to feed into monitoring tools. It needs monitoring to aggregate application metrics and\nhealth data in one place. Once this has been established, rules can be created that will trigger alerts\nwhen certain metrics fall outside of acceptable levels.\n\n\nGenerally, alerts are layered on top of monit', 'oring such that certain conditions trigger appropriate\nalerts to notify team members of urgent problems. Some scenarios that may require alerts include:\n\n\n  - One of your application’s services is not responding after 1 minute of downtime.\n\n  - Your application is returning unsuccessful HTTP responses to more than 1% of requests.\n\n  - Your application’s average response time for key endpoints exceeds 2000 ms.\n\n\n**Alerts in cloud-native apps**\n\n\nY', 'ou can craft queries against the monitoring tools to look for known failure conditions. For instance,\nqueries could search through the incoming logs for indications of HTTP status code 500, which\nindicates a problem on a web server. As soon as one of these is detected, then an e-mail or an SMS\ncould be sent to the owner of the originating service who can begin to investigate.\n\n\nTypically, though, a single 500 error isn’t enough to determine that ', 'a problem has occurred. It could\nmean that a user mistyped their password or entered some malformed data. The alert queries can be\ncrafted to only fire when a larger than average number of 500 errors are detected.\n\n\nOne of the most damaging patterns in alerting is to fire too many alerts for humans to investigate.\nService owners will rapidly become desensitized to errors that they’ve previously investigated and\nfound to be benign. Then, when true', ' errors occur, they’ll be lost in the noise of hundreds of false\n[positives. The parable of the Boy Who Cried Wolf](https://en.wikipedia.org/wiki/The_Boy_Who_Cried_Wolf) is frequently told to children to warn them of this\nvery danger. It’s important to ensure that the alerts that do fire are indicative of a real problem.\n\n### Logging with Elastic Stack\n\n\nThere are many good centralized logging tools and they vary in cost from being free, open-sou', 'rce\ntools, to more expensive options. In many cases, the free tools are as good as or better than the paid\nofferings. One such tool is a combination of three open-source components: Elasticsearch, Logstash,\nand Kibana.\n\n\nCollectively these tools are known as the Elastic Stack or ELK stack.\n\n#### **Elastic Stack**\n\n\nThe Elastic Stack is a powerful option for gathering information from a Kubernetes cluster. Kubernetes\n[supports sending logs to an E', 'lasticsearch endpoint, and for the most part, all you need to get started](https://www.elastic.co/guide/en/kibana/master/logging-configuration.html)\nis to set the environment variables as shown in Figure 7-5:\n\n```\nKUBE_LOGGING_DESTINATION=elasticsearch\nKUBE_ENABLE_NODE_LOGGING=true\n\n```\n\n_Figure 7-5. Configuration variables for Kubernetes_\n\n\nThis step will install Elasticsearch on the cluster and target sending all the cluster logs to it.\n\n\n131 C', 'HAPTER 7 | Monitoring and health\n\n\n_Figure 7-6. An example of a Kibana dashboard showing the results of a query against logs that are ingested from_\n_Kubernetes_\n\n#### **What are the advantages of Elastic Stack?**\n\n\nElastic Stack provides centralized logging in a low-cost, scalable, cloud-friendly manner. Its user\ninterface streamlines data analysis so you can spend your time gleaning insights from your data\ninstead of fighting with a clunky inte', 'rface. It supports a wide variety of inputs so as your distributed\napplication spans more and different kinds of services, you can expect to continue to be able to feed\nlog and metric data into the system. The Elastic Stack also supports fast searches even across large\ndata sets, making it possible even for large applications to log detailed data and still be able to have\nvisibility into it in a performant fashion.\n\n#### **Logstash**\n\n\n[The first', ' component is Logstash. This tool is used to gather log information from a large variety of](https://www.elastic.co/products/logstash)\ndifferent sources. For instance, Logstash can read logs from disk and also receive messages from\n[logging libraries like Serilog. Logstash can do some basic filtering and expansion on the logs as they](https://serilog.net/)\narrive. For instance, if your logs contain IP addresses then Logstash may be configured to ', 'do a\ngeographical lookup and obtain a country/region or even city of origin for that message.\n\n\nSerilog is a logging library for .NET languages, which allows for parameterized logging. Instead of\ngenerating a textual log message that embeds fields, parameters are kept separate. This library allows\nfor more intelligent filtering and searching. A sample Serilog configuration for writing to Logstash\nappears in Figure 7-7.\n\n\n_Figure 7-7. Serilog conf', 'ig for writing log information directly to logstash over HTTP_\n\n\nLogstash would use a configuration like the one shown in Figure 7-8.\n\n\n132 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-8. A Logstash configuration for consuming logs from Serilog_\n\n\nFor scenarios where extensive log manipulation isn’t needed there’s an alternative to Logstash known\n[as Beats. Beats is a family of tools that can gather a wide variety of data from logs to network da', 'ta](https://www.elastic.co/products/beats)\nand uptime information. Many applications will use both Logstash and Beats.\n\n\nOnce the logs have been gathered by Logstash, it needs somewhere to put them. While Logstash\nsupports many different outputs, one of the more exciting ones is Elasticsearch.\n\n#### **Elasticsearch**\n\n\nElasticsearch is a powerful search engine that can index logs as they arrive. It makes running queries\nagainst the logs quick. El', 'asticsearch can handle huge quantities of logs and, in extreme cases, can be\nscaled out across many nodes.\n\n\nLog messages that have been crafted to contain parameters or that have had parameters split from\nthem through Logstash processing, can be queried directly as Elasticsearch preserves this information.\n\n\nA query that searches for the top 10 pages visited by jill@example.com, appears in Figure 7-9.\n\n\n_Figure 7-9. An Elasticsearch query for fi', 'nding top 10 pages visited by a user_\n\n#### **Visualizing information with Kibana web dashboards**\n\n\nThe final component of the stack is Kibana. This tool is used to provide interactive visualizations in a\nweb dashboard. Dashboards may be crafted even by users who are non-technical. Most data that is\nresident in the Elasticsearch index, can be included in the Kibana dashboards. Individual users may\n\n\n133 CHAPTER 7 | Monitoring and health\n\n\nhave d', 'ifferent dashboard desires and Kibana enables this customization through allowing userspecific dashboards.\n\n#### **Installing Elastic Stack on Azure**\n\n\n[The Elastic stack can be installed on Azure in many ways. As always, it’s possible to provision virtual](https://docs.microsoft.com/azure/virtual-machines/linux/tutorial-elasticsearch)\n[machines and install Elastic Stack on them directly. This option is preferred by some experienced users](https', '://docs.microsoft.com/azure/virtual-machines/linux/tutorial-elasticsearch)\nas it offers the highest degree of customizability. Deploying on infrastructure as a service introduces\nsignificant management overhead forcing those who take that path to take ownership of all the tasks\nassociated with infrastructure as a service such as securing the machines and keeping up-to-date with\npatches.\n\n\nAn option with less overhead is to make use of one of the ', 'many Docker containers on which the\nElastic Stack has already been configured. These containers can be dropped into an existing\n[Kubernetes cluster and run alongside application code. The sebp/elk](https://elk-docker.readthedocs.io/) container is a well-documented\nand tested Elastic Stack container.\n\n\n[Another option is a recently announced ELK-as-a-service offering.](https://devops.com/logz-io-unveils-azure-open-source-elk-monitoring-solution/)\n', '\n#### **References**\n\n\n  - [Install Elastic Stack on Azure](https://docs.microsoft.com/azure/virtual-machines/linux/tutorial-elasticsearch)\n\n### Monitoring in Azure Kubernetes Services\n\n\nThe built-in logging in Kubernetes is primitive. However, there are some great options for getting the\nlogs out of Kubernetes and into a place where they can be properly analyzed. If you need to monitor\nyour AKS clusters, configuring Elastic Stack for Kubernetes ', 'is a great solution.\n\n#### **Azure Monitor for Containers**\n\n\n[Azure Monitor for Containers supports consuming logs from not just Kubernetes but also from other](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-overview)\norchestration engines such as DC/OS, Docker Swarm, and Red Hat OpenShift.\n\n\n134 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-10. Consuming logs from various containers_\n\n\n[Prometheus](https://prometheus', '.io/) is a popular open source metric monitoring solution. It is part of the Cloud Native\nCompute Foundation. Typically, using Prometheus requires managing a Prometheus server with its\n[own store. However, Azure Monitor for Containers provides direct integration with Prometheus](https://docs.microsoft.com/azure/azure-monitor/insights/container-insights-prometheus-integration)\n[metrics endpoints, so a separate server is not required.](https://docs', '.microsoft.com/azure/azure-monitor/insights/container-insights-prometheus-integration)\n\n\nLog and metric information is gathered not just from the containers running in the cluster but also\nfrom the cluster hosts themselves. It allows correlating log information from the two making it much\neasier to track down an error.\n\n\n[Installing the log collectors differs on Windows](https://docs.microsoft.com/azure/azure-monitor/insights/containers#configure', '-a-log-analytics-windows-agent-for-kubernetes) [and Linux](https://docs.microsoft.com/azure/azure-monitor/insights/containers#configure-a-log-analytics-linux-agent-for-kubernetes) clusters. But in both cases the log collection\n[is implemented as a Kubernetes DaemonSet, meaning that the log collector is run as a container on](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/)\neach of the nodes.\n\n\nNo matter which orchestrator or ', 'operating system is running the Azure Monitor daemon, the log\ninformation is forwarded to the same Azure Monitor tools with which users are familiar. This approach\nensures a parallel experience in environments that mix different log sources such as a hybrid\nKubernetes/Azure Functions environment.\n\n\n135 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-11. A sample dashboard showing logging and metric information from many running containers._\n\n#### *', '*Log.Finalize()**\n\n\nLogging is one of the most overlooked and yet most important parts of deploying any application at\nscale. As the size and complexity of applications increase, then so does the difficulty of debugging\nthem. Having top quality logs available makes debugging much easier and moves it from the realm of\n“nearly impossible” to “a pleasant experience”.\n\n### Azure Monitor\n\n\nNo other cloud provider has as mature of a cloud application m', 'onitoring solution than that found in\nAzure. Azure Monitor is an umbrella name for a collection of tools designed to provide visibility into\nthe state of your system. It helps you understand how your cloud-native services are performing and\nproactively identifies issues affecting them. Figure 7-12 presents a high level of view of Azure Monitor.\n\n\n136 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-12. High-level view of Azure Monitor._\n\n#### **Gath', 'ering logs and metrics**\n\n\nThe first step in any monitoring solution is to gather as much data as possible. The more data\ngathered, the deeper the insights. Instrumenting systems has traditionally been difficult. Simple\nNetwork Management Protocol (SNMP) was the gold standard protocol for collecting machine level\ninformation, but it required a great deal of knowledge and configuration. Fortunately, much of this\nhard work has been eliminated as th', 'e most common metrics are gathered automatically by Azure\nMonitor.\n\n\nApplication level metrics and events aren’t possible to instrument automatically because they’re\n[specific to the application being deployed. In order to gather these metrics, there are SDKs and APIs](https://docs.microsoft.com/azure/azure-monitor/app/api-custom-events-metrics)\n[available](https://docs.microsoft.com/azure/azure-monitor/app/api-custom-events-metrics) to directly ', 'report such information, such as when a customer signs up or completes an order.\nExceptions can also be captured and reported back into Azure Monitor via Application Insights. The\nSDKs support most every language found in Cloud Native Applications including Go, Python,\nJavaScript, and the .NET languages.\n\n\nThe ultimate goal of gathering information about the state of your application is to ensure that your\nend users have a good experience. What b', 'etter way to tell if users are experiencing issues than doing\n[outside-in web tests? These tests can be as simple as pinging your website from locations around the](https://docs.microsoft.com/azure/azure-monitor/app/monitor-web-app-availability)\nworld or as involved as having agents log into the site and simulate user actions.\n\n#### **Reporting data**\n\n\nOnce the data is gathered, it can be manipulated, summarized, and plotted into charts, which a', 'llow\nusers to instantly see when there are problems. These charts can be gathered into dashboards or into\nWorkbooks, a multi-page report designed to tell a story about some aspect of the system.\n\n\n137 CHAPTER 7 | Monitoring and health\n\n\nNo modern application would be complete without some artificial intelligence or machine learning. To\n[this end, data can be passed to the various machine learning tools in Azure to allow you to extract](https://ww', 'w.youtube.com/watch?v=Cuza-I1g9tw)\ntrends and information that would otherwise be hidden.\n\n\nApplication Insights provides a powerful (SQL-like) query language called _Kusto_ that can query\nrecords, summarize them, and even plot charts. For example, the following query will locate all records\nfor the month of November 2007, group them by state, and plot the top 10 as a pie chart.\n\n\nFigure 7-13 shows the results of this Application Insights Query.\n', '\n\n_Figure 7-13. Application Insights query results._\n\n\n[There is a playground for experimenting with Kusto queries. Reading sample queries can also be](https://dataexplorer.azure.com/clusters/help/databases/Samples)\ninstructive.\n\n#### **Dashboards**\n\n\nThere are several different dashboard technologies that may be used to surface the information from\n[Azure Monitor. Perhaps the simplest is to just run queries in Application Insights and plot the d', 'ata](https://docs.microsoft.com/azure/azure-monitor/learn/tutorial-app-dashboards)\n[into a chart.](https://docs.microsoft.com/azure/azure-monitor/learn/tutorial-app-dashboards)\n\n\n138 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-14. An example of Application Insights charts embedded in the main Azure Dashboard._\n\n\nThese charts can then be embedded in the Azure portal proper through use of the dashboard feature.\nFor users with more exacting requir', 'ements, such as being able to drill down into several tiers of data,\n[Azure Monitor data is available to Power BI. Power BI is an industry-leading, enterprise class, business](https://powerbi.microsoft.com/)\nintelligence tool that can aggregate data from many different data sources.\n\n\n139 CHAPTER 7 | Monitoring and health\n\n\n_Figure 7-15. An example Power BI dashboard._\n\n#### **Alerts**\n\n\nSometimes, having data dashboards is insufficient. If nobod', 'y is awake to watch the dashboards, then\nit can still be many hours before a problem is addressed, or even detected. To this end, Azure Monitor\n[also provides a top notch alerting solution. Alerts can be triggered by a wide range of conditions](https://docs.microsoft.com/azure/azure-monitor/platform/alerts-overview)\nincluding:\n\n\n  - Metric values\n\n  - Log search queries\n\n  - Activity Log events\n\n  - Health of the underlying Azure platform\n\n  - Te', 'sts for web site availability\n\n\nWhen triggered, the alerts can perform a wide variety of tasks. On the simple side, the alerts may just\nsend an e-mail notification to a mailing list or a text message to an individual. More involved alerts\n\n\n140 CHAPTER 7 | Monitoring and health\n\n\nmight trigger a workflow in a tool such as PagerDuty, which is aware of who is on call for a particular\n[application. Alerts can trigger actions in Microsoft Flow](https', '://flow.microsoft.com/) unlocking near limitless possibilities for\nworkflows.\n\n\nAs common causes of alerts are identified, the alerts can be enhanced with details about the common\ncauses of the alerts and the steps to take to resolve them. Highly mature cloud-native application\ndeployments may opt to kick off self-healing tasks, which perform actions such as removing failing\nnodes from a scale set or triggering an autoscaling activity. Eventually', ' it may no longer be necessary\nto wake up on-call personnel at 2AM to resolve a live-site issue as the system will be able to adjust\nitself to compensate or at least limp along until somebody arrives at work the next morning.\n\n\nAzure Monitor automatically leverages machine learning to understand the normal operating\nparameters of deployed applications. This approach enables it to detect services that are operating\noutside of their normal paramete', 'rs. For instance, the typical weekday traffic on the site might be\n10,000 requests per minute. And then, on a given week, suddenly the number of requests hits a highly\n[unusual 20,000 requests per minute. Smart Detection](https://docs.microsoft.com/azure/azure-monitor/app/proactive-diagnostics) will notice this deviation from the norm and\ntrigger an alert. At the same time, the trend analysis is smart enough to avoid firing false positives\nwhen t', 'he traffic load is expected.\n\n#### **References**\n\n\n  - [Azure Monitor](https://docs.microsoft.com/azure/azure-monitor/overview)\n\n\n141 CHAPTER 7 | Monitoring and health\n\n\n**CHAPTER**\n# 8\n\n## Cloud-native identity\n\n\nMost software applications need to have some knowledge of the user or process that is calling them.\nThe user or process interacting with an application is known as a security principal, and the process of\nauthenticating and authorizing', ' these principals is known as identity management, or simply _identity_ .\nSimple applications may include all of their identity management within the application, but this\napproach doesn’t scale well with many applications and many kinds of security principals. Windows\nsupports the use of Active Directory to provide centralized authentication and authorization.\n\n\nWhile this solution is effective within corporate networks, it isn’t designed for us', 'e by users or\napplications that are outside of the AD domain. With the growth of Internet-based applications and\nthe rise of cloud-native apps, security models have evolved.\n\n\nIn today’s cloud-native identity model, architecture is assumed to be distributed. Apps can be\ndeployed anywhere and may communicate with other apps anywhere. Clients may communicate with\nthese apps from anywhere, and in fact, clients may consist of any combination of platf', 'orms and\ndevices. Cloud-native identity solutions use open standards to achieve secure application access from\nclients. These clients range from human users on PCs or phones, to other apps hosted anywhere\nonline, to set-top boxes and IOT devices running any software platform anywhere in the world.\n\n\nModern cloud-native identity solutions typically use access tokens that are issued by a secure token\nservice/server (STS) to a security principal onc', 'e their identity is determined. The access token, typically\na JSON Web Token (JWT), includes _claims_ about the security principal. These claims will minimally\ninclude the user’s identity but may also include other claims that can be used by applications to\ndetermine the level of access to grant the principal.\n\n\nTypically, the STS is only responsible for authenticating the principal. Determining their level of access\nto resources is left to other', ' parts of the application.\n\n### References\n\n\n  - [Microsoft identity platform](https://docs.microsoft.com/azure/active-directory/develop/)\n\n### Authentication and authorization in cloud-native apps\n\n\n_Authentication_ is the process of determining the identity of a security principal. _Authorization_ is the act\nof granting an authenticated principal permission to perform an action or access a resource.\nSometimes authentication is shortened to Auth', 'N and authorization is shortened to AuthZ. Cloud\n\n142 CHAPTER 8 | Cloud-native identity\n\n\nnative applications need to rely on open HTTP-based protocols to authenticate security principals\nsince both clients and applications could be running anywhere in the world on any platform or device.\nThe only common factor is HTTP.\n\n\nMany organizations still rely on local authentication services like Active Directory Federation Services\n(ADFS). While this ap', 'proach has traditionally served organizations well for on premises authentication\nneeds, cloud-native applications benefit from systems designed specifically for the cloud. A recent\n2019 United Kingdom National Cyber Security Centre (NCSC) advisory states that “organizations using\nAzure AD as their primary authentication source will actually lower their risk compared to ADFS.”\n[Some reasons outlined in this analysis](https://oxfordcomputergroup.c', 'om/resources/o365-security-native-cloud-authentication/) include:\n\n\n  - Access to full set of Microsoft credential protection technologies.\n\n  - Most organizations are already relying on Azure AD to some extent.\n\n  - Double hashing of NTLM hashes ensures compromise won’t allow credentials that work in\nlocal Active Directory.\n\n#### **References**\n\n\n  - [Authentication basics](https://docs.microsoft.com/azure/active-directory/develop/authentication', '-scenarios)\n\n  - [Access tokens and claims](https://docs.microsoft.com/azure/active-directory/develop/access-tokens)\n\n  - [It may be time to ditch your on premises authentication services](https://oxfordcomputergroup.com/resources/o365-security-native-cloud-authentication/)\n\n### Azure Active Directory\n\n\nMicrosoft Azure Active Directory (Azure AD) offers identity and access management as a service.\nCustomers use it to configure and maintain who us', 'ers are, what information to store about them, who\ncan access that information, who can manage it, and what apps can access it. AAD can authenticate\nusers for applications configured to use it, providing a single sign-on (SSO) experience. It can be used\non its own or be integrated with Windows AD running on premises.\n\n\nAzure AD is built for the cloud. It’s truly a cloud-native identity solution that uses a REST-based Graph\nAPI and OData syntax fo', 'r queries, unlike Windows AD, which uses LDAP. On premises Active Directory\ncan sync user attributes to the cloud using Identity Sync Services, allowing all authentication to take\nplace in the cloud using Azure AD. Alternately, authentication can be configured via Connect to pass\nback to local Active Directory via ADFS to be completed by Windows AD on premises.\n\n\nAzure AD supports company branded sign-in screens, multi-factory authentication, and', ' cloud-based\napplication proxies that are used to provide SSO for applications hosted on premises. It offers\ndifferent kinds of security reporting and alert capabilities.\n\n#### **References**\n\n\n  - [Microsoft identity platform](https://docs.microsoft.com/azure/active-directory/develop/)\n\n\n143 CHAPTER 8 | Cloud-native identity\n\n\n### IdentityServer for cloud-native applications\n\nIdentityServer is an authentication server that implements OpenID Conn', 'ect (OIDC) and OAuth 2.0\nstandards for ASP.NET Core. It’s designed to provide a common way to authenticate requests to all of\nyour applications, whether they’re web, native, mobile, or API endpoints. IdentityServer can be used to\nimplement Single Sign-On (SSO) for multiple applications and application types. It can be used to\nauthenticate actual users via sign-in forms and similar user interfaces as well as service-based\nauthentication that typic', 'ally involves token issuance, verification, and renewal without any user\ninterface. IdentityServer is designed to be a customizable solution. Each instance is typically\ncustomized to suit an individual organization and/or set of applications’ needs.\n\n#### **Common web app scenarios**\n\n\nTypically, applications need to support some or all of the following scenarios:\n\n\n  - Human users accessing web applications with a browser.\n\n  - Human users acces', 'sing back-end Web APIs from browser-based apps.\n\n  - Human users on mobile/native clients accessing back-end Web APIs.\n\n  - Other applications accessing back-end Web APIs (without an active user or user interface).\n\n  - Any application may need to interact with other Web APIs, using its own identity or\ndelegating to the user’s identity.\n\n\n_Figure 8-1. Application types and scenarios._\n\n\nIn each of these scenarios, the exposed functionality needs ', 'to be secured against unauthorized use. At\na minimum, this typically requires authenticating the user or principal making a request for a resource.\nThis authentication may use one of several common protocols such as SAML2p, WS-Fed, or OpenID\nConnect. Communicating with APIs typically uses the OAuth2 protocol and its support for security\ntokens. Separating these critical cross-cutting security concerns and their implementation details from\nthe app', 'lications themselves ensures consistency and improves security and maintainability.\n\n\n144 CHAPTER 8 | Cloud-native identity\n\n\nOutsourcing these concerns to a dedicated product like IdentityServer helps the requirement for every\napplication to solve these problems itself.\n\n\nIdentityServer provides middleware that runs within an ASP.NET Core application and adds support\n[for OpenID Connect and OAuth2 (see supported specifications). Organizations wo', 'uld create their own](https://docs.duendesoftware.com/identityserver/v6/overview/specs/)\nASP.NET Core app using IdentityServer middleware to act as the STS for all of their token-based\nsecurity protocols. The IdentityServer middleware exposes endpoints to support standard\nfunctionality, including:\n\n\n  - Authorize (authenticate the end user)\n\n  - Token (request a token programmatically)\n\n  - Discovery (metadata about the server)\n\n  - User Info (ge', 't user information with a valid access token)\n\n  - Device Authorization (used to start device flow authorization)\n\n  - Introspection (token validation)\n\n  - Revocation (token revocation)\n\n  - End Session (trigger single sign-out across all apps)\n\n#### **Getting started**\n\n\nIdentityServer4 is available under dual license:\n\n\n  - RPL - lets you use the IdentityServer4 free if used in open-source work\n\n  - Paid - lets you use the IdentityServer4 in a', ' commercial scenario\n\n\n[For more information about pricing, see the official product’s pricing page.](https://duendesoftware.com/products/identityserver)\n\n\n[You can add it to your applications using its NuGet packages. The main package is IdentityServer4,](https://www.nuget.org/packages/IdentityServer4/)\nwhich has been downloaded over four million times. The base package doesn’t include any user\ninterface code and only supports in-memory configur', 'ation. To use it with a database, you’ll also want\n[a data provider like IdentityServer4.EntityFramework, which uses Entity Framework Core to store](https://www.nuget.org/packages/IdentityServer4.EntityFramework)\nconfiguration and operational data for IdentityServer. For user interface, you can copy files from the\n[Quickstart UI repository](https://github.com/IdentityServer/IdentityServer4.Quickstart.UI) into your ASP.NET Core MVC application to ', 'add support for sign in and sign\nout using IdentityServer middleware.\n\n#### **Configuration**\n\n\nIdentityServer supports different kinds of protocols and social authentication providers that can be\nconfigured as part of each custom installation. This is typically done in the ASP.NET Core application’s\nProgram class (or in the Startup class in the ConfigureServices method). The configuration involves\nspecifying the supported protocols and the paths', ' to the servers and endpoints that will be used.\nFigure 8-2 shows an example configuration taken from the IdentityServer4 Quickstart UI project:\n\n\n145 CHAPTER 8 | Cloud-native identity\n\n\n_Figure 8-2. Configuring IdentityServer._\n\n#### **JavaScript clients**\n\n\nMany cloud-native applications use server-side APIs and rich client single page applications (SPAs) on\n[the front end. IdentityServer ships a JavaScript client](https://docs.duendesoftware.c', 'om/identityserver/v6/quickstarts/js_clients/) (oidc-client.js) via NPM that can be added to\nSPAs to enable them to use IdentityServer for sign in, sign out, and token-based authentication of\nweb APIs.\n\n#### **References**\n\n\n  - [IdentityServer documentation](https://docs.duendesoftware.com/identityserver/v6/)\n\n  - [Application types](https://docs.microsoft.com/azure/active-directory/develop/app-types)\n\n  - [JavaScript OIDC client](https://docs.du', 'endesoftware.com/identityserver/v6/quickstarts/js_clients/)\n\n\n146 CHAPTER 8 | Cloud-native identity\n\n\n**CHAPTER**\n# 9\n\n## Cloud-native security\n\n\nNot a day goes by where the news doesn’t contain some story about a company being hacked or\nsomehow losing their customers’ data. Even countries/regions aren’t immune to the problems created\nby treating security as an afterthought. For years, companies have treated the security of customer\ndata and, in ', 'fact, their entire networks as something of a “nice to have”. Windows servers were left\nunpatched, ancient versions of PHP kept running, and MongoDB databases left wide open to the\nworld.\n\n\nHowever, there are starting to be real-world consequences for not maintaining a security mindset\nwhen building and deploying applications. Many companies learned the hard way what can happen\n[when servers and desktops aren’t patched during the 2017 outbreak of', ' NotPetya. The cost of these](https://www.wired.com/story/notpetya-cyberattack-ukraine-russia-code-crashed-the-world/)\nattacks has easily reached into the billions, with some estimates putting the losses from this single\nattack at 10 billion US dollars.\n\n\nEven governments aren’t immune to hacking incidents. The city of Baltimore was held ransom by\n[criminals](https://www.vox.com/recode/2019/5/21/18634505/baltimore-ransom-robbinhood-mayor-jack-you', 'ng-hackers) making it impossible for citizens to pay their bills or use city services.\n\n\nThere has also been an increase in legislation that mandates certain data protections for personal\ndata. In Europe, GDPR has been in effect for more than a year and, more recently, California passed\ntheir own version called CCDA, which comes into effect January 1, 2020. The fines under GDPR can be\nso punishing as to put companies out of business. Google has a', 'lready been fined 50 million Euros for\nviolations, but that’s just a drop in the bucket compared with the potential fines.\n\n\nIn short, security is serious business.\n\n### Azure security for cloud-native apps\n\n\nCloud-native applications can be both easier and more difficult to secure than traditional applications.\nOn the downside, you need to secure more smaller applications and dedicate more energy to build\nout the security infrastructure. The het', 'erogeneous nature of programming languages and styles in\nmost service deployments also means you need to pay more attention to security bulletins from many\ndifferent providers.\n\n\nOn the flip side, smaller services, each with their own data store, limit the scope of an attack. If an\nattacker compromises one system, it’s probably more difficult for the attacker to make the jump to\nanother system than it is in a monolithic application. Process bound', 'aries are strong boundaries. Also,\nif a database backup gets exposed, then the damage is more limited, as that database contains only a\nsubset of data and is unlikely to contain personal data.\n\n\n147 CHAPTER 9 | Cloud-native security\n\n\n#### **Threat modeling**\n\nNo matter if the advantages outweigh the disadvantages of cloud-native applications, the same\nholistic security mindset must be followed. Security and secure thinking must be part of every ', 'step of\nthe development and operations story. When planning an application ask questions like:\n\n\n  - What would be the impact of this data being lost?\n\n  - How can we limit the damage from bad data being injected into this service?\n\n  - Who should have access to this data?\n\n  - Are there auditing policies in place around the development and release process?\n\n\n[All these questions are part of a process called threat modeling. This process tries to', ' answer the](https://docs.microsoft.com/azure/security/azure-security-threat-modeling-tool)\nquestion of what threats there are to the system, how likely the threats are, and the potential damage\nfrom them.\n\n\nOnce the list of threats has been established, you need to decide whether they’re worth mitigating.\nSometimes a threat is so unlikely and expensive to plan for that it isn’t worth spending energy on it.\nFor instance, some state level actor co', 'uld inject changes into the design of a process that is used by\n[millions of devices. Now, instead of running a certain piece of code in Ring 3, that code is run in Ring](https://en.wikipedia.org/wiki/Protection_ring)\n0. This process allows an exploit that can bypass the hypervisor and run the attack code on the bare\nmetal machines, allowing attacks on all the virtual machines that are running on that hardware.\n\n\nThe altered processors are diffic', 'ult to detect without a microscope and advanced knowledge of the on\nsilicon design of that processor. This scenario is unlikely to happen and expensive to mitigate, so\nprobably no threat model would recommend building exploit protection for it.\n\n\nMore likely threats, such as broken access controls permitting Id incrementing attacks (replacing Id=2\nwith Id=3 in the URL) or SQL injection, are more attractive to build protections against. The\nmitiga', 'tions for these threats are quite reasonable to build and prevent embarrassing security holes\nthat smear the company’s reputation.\n\n#### **Principle of least privilege**\n\n\nOne of the founding ideas in computer security is the Principle of Least Privilege (POLP). It’s actually a\nfoundational idea in most any form of security be it digital or physical. In short, the principle is that\nany user or process should have the smallest number of rights pos', 'sible to execute its task.\n\n\nAs an example, think of the tellers at a bank: accessing the safe is an uncommon activity. So, the\naverage teller can’t open the safe themselves. To gain access, they need to escalate their request\nthrough a bank manager, who performs additional security checks.\n\n\nIn a computer system, a fantastic example is the rights of a user connecting to a database. In many\ncases, there’s a single user account used to both build ', 'the database structure and run the application.\nExcept in extreme cases, the account running the application doesn’t need the ability to update\nschema information. There should be several accounts that provide different levels of privilege. The\napplication should only use the permission level that grants read and writes access to the data in the\ntables. This kind of protection would eliminate attacks that aimed to drop database tables or\nintroduc', 'e malicious triggers.\n\n\n148 CHAPTER 9 | Cloud-native security\n\n\nAlmost every part of building a cloud-native application can benefit from remembering the principle\nof least privilege. You can find it at play when setting up firewalls, network security groups, roles, and\nscopes in Role-based access control (RBAC).\n\n#### **Penetration testing**\n\n\nAs applications become more complicated the number of attack vectors increases at an alarming rate.\nThr', 'eat modeling is flawed in that it tends to be executed by the same people building the system. In\nthe same way that many developers have trouble envisioning user interactions and then build\nunusable user interfaces, most developers have difficulty seeing every attack vector. It’s also possible\nthat the developers building the system aren’t well versed in attack methodologies and miss\nsomething crucial.\n\n\nPenetration testing or “pen testing” invol', 'ves bringing in external actors to attempt to attack the\nsystem. These attackers may be an external consulting company or other developers with good\nsecurity knowledge from another part of the business. They’re given carte blanche to attempt to\nsubvert the system. Frequently, they’ll find extensive security holes that need to be patched.\nSometimes the attack vector will be something totally unexpected like exploiting a phishing attack\nagainst the', ' CEO.\n\n\n[Azure itself is constantly undergoing attacks from a team of hackers inside Microsoft. Over the years,](https://azure.microsoft.com/resources/videos/red-vs-blue-internal-security-penetration-testing-of-microsoft-azure/)\nthey’ve been the first to find dozens of potentially catastrophic attack vectors, closing them before\nthey can be exploited externally. The more tempting a target, the more likely that eternal actors will\nattempt to explo', 'it it and there are a few targets in the world more tempting than Azure.\n\n#### **Monitoring**\n\n\nShould an attacker attempt to penetrate an application, there should be some warning of it.\nFrequently, attacks can be spotted by examining the logs from services. Attacks leave telltale signs\nthat can be spotted before they succeed. For instance, an attacker attempting to guess a password\nwill make many requests to a login system. Monitoring around th', 'e login system can detect weird\npatterns that are out of line with the typical access pattern. This monitoring can be turned into an\nalert that can, in turn, alert an operations person to activate some sort of countermeasure. A highly\nmature monitoring system might even take action based on these deviations proactively adding rules\nto block requests or throttle responses.\n\n#### **Securing the build**\n\n\nOne place where security is often overlooked', ' is around the build process. Not only should the build\nrun security checks, such as scanning for insecure code or checked-in credentials, but the build itself\nshould be secure. If the build server is compromised, then it provides a fantastic vector for introducing\narbitrary code into the product.\n\n\nImagine that an attacker is looking to steal the passwords of people signing into a web application.\nThey could introduce a build step that modifies ', 'the checked-out code to mirror any login request to\nanother server. The next time code goes through the build, it’s silently updated. The source code\nvulnerability scanning won’t catch this vulnerability as it runs before the build. Equally, nobody will\n\n\n149 CHAPTER 9 | Cloud-native security\n\n\ncatch it in a code review because the build steps live on the build server. The exploited code will go to\nproduction where it can harvest passwords. Proba', 'bly there’s no audit log of the build process\nchanges, or at least nobody monitoring the audit.\n\n\nThis scenario is a perfect example of a seemingly low-value target that can be used to break into the\nsystem. Once an attacker breaches the perimeter of the system, they can start working on finding\nways to elevate their permissions to the point that they can cause real harm anywhere they like.\n\n#### **Building secure code**\n\n\n.NET Framework is alrea', 'dy a quite secure framework. It avoids some of the pitfalls of unmanaged\ncode, such as walking off the ends of arrays. Work is actively done to fix security holes as they’re\n[discovered. There’s even a bug bounty program](https://www.microsoft.com/msrc/bounty) that pays researchers to find issues in the framework\nand report them instead of exploiting them.\n\n\n[There are many ways to make .NET code more secure. Following guidelines such as the Secu', 're coding](https://docs.microsoft.com/dotnet/standard/security/secure-coding-guidelines)\n[guidelines for .NET](https://docs.microsoft.com/dotnet/standard/security/secure-coding-guidelines) article is a reasonable step to take to ensure that the code is secure from the\n[ground up. The OWASP top 10 is another invaluable guide to build secure code.](https://owasp.org/www-project-top-ten/)\n\n\nThe build process is a good place to put scanning tools to ', 'detect problems in source code before they\nmake it into production. Most every project has dependencies on some other packages. A tool that\ncan scan for outdated packages will catch problems in a nightly build. Even when building Docker\nimages, it’s useful to check and make sure that the base image doesn’t have known vulnerabilities.\nAnother thing to check is that nobody has accidentally checked in credentials.\n\n#### **Built-in security**\n\n\nAzure', ' is designed to balance usability and security for most users. Different users are going to have\ndifferent security requirements, so they need to fine-tune their approach to cloud security. Microsoft\n[publishes a great deal of security information in the Trust Center. This resource should be the first](https://azure.microsoft.com/support/trust-center/)\nstop for those professionals interested in understanding how the built-in attack mitigation\ntec', 'hnologies work.\n\n\n[Within the Azure portal, the Azure Advisor](https://azure.microsoft.com/services/advisor/) is a system that is constantly scanning an environment and\nmaking recommendations. Some of these recommendations are designed to save users money, but\nothers are designed to identify potentially insecure configurations, such as having a storage container\nopen to the world and not protected by a Virtual Network.\n\n#### **Azure network infra', 'structure**\n\n\nIn an on-premises deployment environment, a great deal of energy is dedicated to setting up\nnetworking. Setting up routers, switches, and the such is complicated work. Networks allow certain\nresources to talk to other resources and prevent access in some cases. A frequent network rule is to\nrestrict access to the production environment from the development environment on the off chance\nthat a half-developed piece of code runs awry a', 'nd deletes a swath of data.\n\n\nOut of the box, most PaaS Azure resources have only the most basic and permissive networking setup.\nFor instance, anybody on the Internet can access an app service. New SQL Server instances typically\n\n\n150 CHAPTER 9 | Cloud-native security\n\n\ncome restricted, so that external parties can’t access them, but the IP address ranges used by Azure\nitself are permitted through. So, while the SQL server is protected from exte', 'rnal threats, an attacker\nonly needs to set up an Azure bridgehead from where they can launch attacks against all SQL\ninstances on Azure.\n\n\nFortunately, most Azure resources can be placed into an Azure Virtual Network that allows finegrained access control. Similar to the way that on-premises networks establish private networks that\nare protected from the wider world, virtual networks are islands of private IP addresses that are\nlocated within th', 'e Azure network.\n\n\n_Figure 9-1. A virtual network in Azure._\n\n\nIn the same way that on-premises networks have a firewall governing access to the network, you can\nestablish a similar firewall at the boundary of the virtual network. By default, all the resources on a\nvirtual network can still talk to the Internet. It’s only incoming connections that require some form of\nexplicit firewall exception.\n\n\nWith the network established, internal resources', ' like storage accounts can be set up to only allow for\naccess by resources that are also on the Virtual Network. This firewall provides an extra level of\nsecurity, should the keys for that storage account be leaked, attackers wouldn’t be able to connect to\nit to exploit the leaked keys. This scenario is another example of the principle of least privilege.\n\n\nThe nodes in an Azure Kubernetes cluster can participate in a virtual network just like ot', 'her resources\n[that are more native to Azure. This functionality is called Azure Container Networking Interface. In](https://github.com/Azure/azure-container-networking/blob/master/docs/cni.md)\neffect, it allocates a subnet within the virtual network on which virtual machines and container images\nare allocated.\n\n\nContinuing down the path of illustrating the principle of least privilege, not every resource within a\nVirtual Network needs to talk to', ' every other resource. For instance, in an application that provides a\n\n\n151 CHAPTER 9 | Cloud-native security\n\n\nweb API over a storage account and a SQL database, it’s unlikely that the database and the storage\naccount need to talk to one another. Any data sharing between them would go through the web\n[application. So, a network security group (NSG) could be used to deny traffic between the two](https://docs.microsoft.com/azure/virtual-network/s', 'ecurity-overview)\nservices.\n\n\nA policy of denying communication between resources can be annoying to implement, especially\ncoming from a background of using Azure without traffic restrictions. On some other clouds, the\nconcept of network security groups is much more prevalent. For instance, the default policy on AWS is\nthat resources can’t communicate among themselves until enabled by rules in an NSG. While slower\nto develop this, a more restrict', 'ive environment provides a more secure default. Making use of proper\nDevOps practices, especially using Azure Resource Manager or Terraform to manage permissions can\nmake controlling the rules easier.\n\n\nVirtual Networks can also be useful when setting up communication between on-premises and cloud\nresources. A virtual private network can be used to seamlessly attach the two networks together. This\napproach allows running a virtual network without', ' any sort of gateway for scenarios where all the\nusers are on-site. There are a number of technologies that can be used to establish this network. The\n[simplest is to use a site-to-site VPN that can be established between many routers and Azure. Traffic](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpngateways?toc=%252fazure%252fvirtual-network%252ftoc.json#s2smulti)\nis encrypted and tunneled over the Internet at the same cost p', 'er byte as any other traffic. For\n[scenarios where more bandwidth or more security is desirable, Azure offers a service called Express](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpngateways?toc=%252fazure%252fvirtual-network%252ftoc.json#ExpressRoute)\n[Route](https://docs.microsoft.com/azure/vpn-gateway/vpn-gateway-about-vpngateways?toc=%252fazure%252fvirtual-network%252ftoc.json#ExpressRoute) that uses a private circuit betw', 'een an on-premises network and Azure. It’s more costly and\ndifficult to establish but also more secure.\n\n#### **Role-based access control for restricting access to Azure resources**\n\n\nRBAC is a system that provides an identity to applications running in Azure. Applications can access\nresources using this identity instead of or in addition to using keys or passwords.\n\n#### **Security Principals**\n\n\nThe first component in RBAC is a security princip', 'al. A security principal can be a user, group, service\nprincipal, or managed identity.\n\n\n_Figure 9-2. Different types of security principals._\n\n\n  - User - Any user who has an account in Azure Active Directory is a user.\n\n  - Group - A collection of users from Azure Active Directory. As a member of a group, a user\ntakes on the roles of that group in addition to their own.\n\n  - Service principal - A security identity under which services or applic', 'ations run.\n\n\n152 CHAPTER 9 | Cloud-native security\n\n\n  - Managed identity - An Azure Active Directory identity managed by Azure. Managed identities\nare typically used when developing cloud applications that manage the credentials for\nauthenticating to Azure services.\n\n\nThe security principal can be applied to most any resource. This aspect means that it’s possible to\nassign a security principal to a container running within Azure Kubernetes, all', 'owing it to access secrets\nstored in Key Vault. An Azure Function could take on a permission allowing it to talk to an Active\nDirectory instance to validate a JWT for a calling user. Once services are enabled with a service\nprincipal, their permissions can be managed granularly using roles and scopes.\n\n#### **Roles**\n\n\nA security principal can take on many roles or, using a more sartorial analogy, wear many hats. Each\nrole defines a series of per', 'missions such as “Read messages from Azure Service Bus endpoint”. The\neffective permission set of a security principal is the combination of all the permissions assigned to all\nthe roles that a security principal has. Azure has a large number of built-in roles and users can define\ntheir own roles.\n\n\n_Figure 9-3. RBAC role definitions._\n\n\nBuilt into Azure are also a number of high-level roles such as Owner, Contributor, Reader, and User\nAccount Ad', 'ministrator. With the Owner role, a security principal can access all resources and assign\npermissions to others. A contributor has the same level of access to all resources but they can’t assign\npermissions. A Reader can only view existing Azure resources and a User Account Administrator can\nmanage access to Azure resources.\n\n\n[More granular built-in roles such as DNS Zone Contributor](https://docs.microsoft.com/azure/role-based-access-control/b', 'uilt-in-roles#dns-zone-contributor) have rights limited to a single service.\nSecurity principals can take on any number of roles.\n\n\n153 CHAPTER 9 | Cloud-native security\n\n\n#### **Scopes**\n\nRoles can be applied to a restricted set of resources within Azure. For instance, applying scope to the\nprevious example of reading from a Service Bus queue, you can narrow the permission to a single\nqueue: “Read messages from Azure Service Bus endpoint blah.se', 'rvicebus.windows.net/queue1”\n\n\nThe scope can be as narrow as a single resource or it can be applied to an entire resource group,\nsubscription, or even management group.\n\n\nWhen testing if a security principal has certain permission, the combination of role and scope are\ntaken into account. This combination provides a powerful authorization mechanism.\n\n#### **Deny**\n\n\nPreviously, only “allow” rules were permitted for RBAC. This behavior made some s', 'copes complicated\nto build. For instance, allowing a security principal access to all storage accounts except one required\ngranting explicit permission to a potentially endless list of storage accounts. Every time a new storage\naccount was created, it would have to be added to this list of accounts. This added management\noverhead that certainly wasn’t desirable.\n\n\nDeny rules take precedence over allow rules. Now representing the same “allow all b', 'ut one” scope\ncould be represented as two rules “allow all” and “deny this one specific one”. Deny rules not only\nease management but allow for resources that are extra secure by denying access to everybody.\n\n#### **Checking access**\n\n\nAs you can imagine, having a large number of roles and scopes can make figuring out the effective\npermission of a service principal quite difficult. Piling deny rules on top of that, only serves to increase\n[the co', 'mplexity. Fortunately, there’s a permissions calculator](https://docs.microsoft.com/azure/role-based-access-control/check-access) that can show the effective permissions\nfor any service principal. It’s typically found under the IAM tab in the portal, as shown in Figure 9-3.\n\n\n_Figure 9-4. Permission calculator for an app service._\n\n\n154 CHAPTER 9 | Cloud-native security\n\n\n#### **Securing secrets**\n\nPasswords and certificates are a common attack v', 'ector for attackers. Password-cracking hardware can\ndo a brute-force attack and try to guess billions of passwords per second. So it’s important that the\npasswords that are used to access resources are strong, with a large variety of characters. These\npasswords are exactly the kind of passwords that are near impossible to remember. Fortunately, the\npasswords in Azure don’t actually need to be known by any human.\n\n\n[Many security experts suggest t', 'hat using a password manager to keep your own passwords is the](https://www.troyhunt.com/password-managers-dont-have-to-be-perfect-they-just-have-to-be-better-than-not-having-one/)\nbest approach. While it centralizes your passwords in one location, it also allows using highly complex\npasswords and ensuring they’re unique for each account. The same system exists within Azure: a\ncentral store for secrets.\n\n#### **Azure Key Vault**\n\n\nAzure Key Vault', ' provides a centralized location to store passwords for things such as databases, API\nkeys, and certificates. Once a secret is entered into the Vault, it’s never shown again and the\ncommands to extract and view it are purposefully complicated. The information in the safe is\nprotected using either software encryption or FIPS 140-2 Level 2 validated Hardware Security\nModules.\n\n\nAccess to the key vault is provided through RBACs, meaning that not jus', 't any user can access the\ninformation in the vault. Say a web application wishes to access the database connection string stored\nin Azure Key Vault. To gain access, applications need to run using a service principal. Under this\nassumed role, they can read the secrets from the safe. There are a number of different security\nsettings that can further limit the access that an application has to the vault, so that it can’t update\nsecrets but only read', ' them.\n\n\nAccess to the key vault can be monitored to ensure that only the expected applications are accessing\nthe vault. The logs can be integrated back into Azure Monitor, unlocking the ability to set up alerts\nwhen unexpected conditions are encountered.\n\n#### **Kubernetes**\n\n\nWithin Kubernetes, there’s a similar service for maintaining small pieces of secret information.\nKubernetes Secrets can be set via the typical kubectl executable.\n\n\nCreati', 'ng a secret is as simple as finding the base64 version of the values to be stored:\n\n\nThen adding it to a secrets file named secret.yml for example that looks similar to the following\nexample:\n\n\n155 CHAPTER 9 | Cloud-native security\n\n\nFinally, this file can be loaded into Kubernetes by running the following command:\n\n```\nkubectl apply -f ./secret.yaml\n\n```\n\nThese secrets can then be mounted into volumes or exposed to container processes through\n[e', 'nvironment variables. The Twelve-factor app](https://12factor.net/) approach to building applications suggests using the\nlowest common denominator to transmit settings to an application. Environment variables are the\nlowest common denominator, because they’re supported no matter the operating system or\napplication.\n\n\nAn alternative to use the built-in Kubernetes secrets is to access the secrets in Azure Key Vault from\nwithin Kubernetes. The simpl', 'est way to do this is to assign an RBAC role to the container looking to\nload secrets. The application can then use the Azure Key Vault APIs to access the secrets. However,\nthis approach requires modifications to the code and doesn’t follow the pattern of using environment\nvariables. Instead, it’s possible to inject values into a container. This approach is actually more secure\nthan using the Kubernetes secrets directly, as they can be accessed b', 'y users on the cluster.\n\n#### **Encryption in transit and at rest**\n\n\nKeeping data safe is important whether it’s on disk or transiting between various different services.\nThe most effective way to keep data from leaking is to encrypt it into a format that can’t be easily read\nby others. Azure supports a wide range of encryption options.\n\n\n**In transit**\n\n\nThere are several ways to encrypt traffic on the network in Azure. The access to Azure serv', 'ices is\ntypically done over connections that use Transport Layer Security (TLS). For instance, all the\nconnections to the Azure APIs require TLS connections. Equally, connections to endpoints in Azure\nstorage can be restricted to work only over TLS encrypted connections.\n\n\nTLS is a complicated protocol and simply knowing that the connection is using TLS isn’t sufficient to\nensure security. For instance, TLS 1.0 is chronically insecure, and TLS 1.', '1 isn’t much better. Even within\nthe versions of TLS, there are various settings that can make the connections easier to decrypt. The\nbest course of action is to check and see if the server connection is using up-to-date and well\nconfigured protocols.\n\n\nThis check can be done by an external service such as SSL labs’ SSL Server Test. A test run against a\ntypical Azure endpoint, in this case a service bus endpoint, yields a near perfect score of A.', '\n\n\nEven services like Azure SQL databases use TLS encryption to keep data hidden. The interesting part\nabout encrypting the data in transit using TLS is that it isn’t possible, even for Microsoft, to listen in on\nthe connection between computers running TLS. This should provide comfort for companies\nconcerned that their data may be at risk from Microsoft proper or even a state actor with more\nresources than the standard attacker.\n\n\n156 CHAPTER 9 ', '| Cloud-native security\n\n\n_Figure 9-5. SSL labs report showing a score of A for a Service Bus endpoint._\n\n\nWhile this level of encryption isn’t going to be sufficient for all time, it should inspire confidence that\nAzure TLS connections are quite secure. Azure will continue to evolve its security standards as\nencryption improves. It’s nice to know that there’s somebody watching the security standards and\nupdating Azure as they improve.\n\n\n**At res', 't**\n\n\nIn any application, there are a number of places where data rests on the disk. The application code\nitself is loaded from some storage mechanism. Most applications also use some kind of a database\nsuch as SQL Server, Cosmos DB, or even the amazingly price-efficient Table Storage. These databases\nall use heavily encrypted storage to ensure that nobody other than the applications with proper\npermissions can read your data. Even the system ope', 'rators can’t read data that has been encrypted.\nSo customers can remain confident their secret information remains secret.\n\n\n**Storage**\n\n\nThe underpinning of much of Azure is the Azure Storage engine. Virtual machine disks are mounted\non top of Azure Storage. Azure Kubernetes Service runs on virtual machines that, themselves, are\nhosted on Azure Storage. Even serverless technologies, such as Azure Functions Apps and Azure\nContainer Instances, ru', 'n out of disk that is part of Azure Storage.\n\n\nIf Azure Storage is well encrypted, then it provides for a foundation for most everything else to also\n[be encrypted. Azure Storage is encrypted](https://docs.microsoft.com/azure/storage/common/storage-service-encryption) [with FIPS 140-2](https://en.wikipedia.org/wiki/FIPS_140) [compliant 256-bit AES. This is a well-](https://en.wikipedia.org/wiki/Advanced_Encryption_Standard)\nregarded encryption te', 'chnology having been the subject of extensive academic scrutiny over the last\n20 or so years. At present, there’s no known practical attack that would allow someone without\nknowledge of the key to read data encrypted by AES.\n\n\nBy default, the keys used for encrypting Azure Storage are managed by Microsoft. There are extensive\nprotections in place to ensure to prevent malicious access to these keys. However, users with\n[particular encryption requi', 'rements can also provide their own storage keys that are managed in Azure](https://docs.microsoft.com/azure/storage/common/storage-encryption-keys-powershell)\n\n\n157 CHAPTER 9 | Cloud-native security\n\n\nKey Vault. These keys can be revoked at any time, which would effectively render the contents of the\nStorage account using them inaccessible.\n\n\nVirtual machines use encrypted storage, but it’s possible to provide another layer of encryption by\nusing', ' technologies like BitLocker on Windows or DM-Crypt on Linux. These technologies mean that\neven if the disk image was leaked off of storage, it would remain near impossible to read it.\n\n\n**Azure SQL**\n\n\n[Databases hosted on Azure SQL use a technology called Transparent Data Encryption (TDE) to ensure](https://docs.microsoft.com/sql/relational-databases/security/encryption/transparent-data-encryption)\ndata remains encrypted. It’s enabled by defaul', 't on all newly created SQL databases, but must be\nenabled manually for legacy databases. TDE executes real-time encryption and decryption of not just\nthe database, but also the backups and transaction logs.\n\n\nThe encryption parameters are stored in the master database and, on startup, are read into memory\nfor the remaining operations. This means that the master database must remain unencrypted. The\nactual key is managed by Microsoft. However, use', 'rs with exacting security requirements may provide\ntheir own key in Key Vault in much the same way as is done for Azure Storage. The Key Vault provides\nfor such services as key rotation and revocation.\n\n\nThe “Transparent” part of TDS comes from the fact that there aren’t client changes needed to use an\nencrypted database. While this approach provides for good security, leaking the database password is\nenough for users to be able to decrypt the da', 'ta. There’s another approach that encrypts individual\n[columns or tables in a database. Always Encrypted](https://docs.microsoft.com/azure/sql-database/sql-database-always-encrypted-azure-key-vault) ensures that at no point the encrypted data\nappears in plain text inside the database.\n\n\nSetting up this tier of encryption requires running through a wizard in SQL Server Management Studio\nto select the sort of encryption and where in Key Vault to st', 'ore the associated keys.\n\n\n158 CHAPTER 9 | Cloud-native security\n\n\n_Figure 9-6. Selecting columns in a table to be encrypted using Always Encrypted._\n\n\nClient applications that read information from these encrypted columns need to make special\nallowances to read encrypted data. Connection strings need to be updated with Column Encryption\nSetting=Enabled and client credentials must be retrieved from the Key Vault. The SQL Server client\nmust then b', 'e primed with the column encryption keys. Once that is done, the remaining actions use\nthe standard interfaces to SQL Client. That is, tools like Dapper and Entity Framework, which are built\non top of SQL Client, will continue to work without changes. Always Encrypted may not yet be\navailable for every SQL Server driver on every language.\n\n\nThe combination of TDE and Always Encrypted, both of which can be used with client-specific keys,\nensures t', 'hat even the most exacting encryption requirements are supported.\n\n\n**Cosmos DB**\n\n\nCosmos DB is the newest database provided by Microsoft in Azure. It has been built from the ground\nup with security and cryptography in mind. AES-256bit encryption is standard for all Cosmos DB\n\n\n159 CHAPTER 9 | Cloud-native security\n\n\ndatabases and can’t be disabled. Coupled with the TLS 1.2 requirement for communication, the entire\nstorage solution is encrypted.', '\n\n\n_Figure 9-7. The flow of data encryption within Cosmos DB._\n\n\nWhile Cosmos DB doesn’t provide for supplying customer encryption keys, there has been significant\nwork done by the team to ensure it remains PCI-DSS compliant without that. Cosmos DB also doesn’t\nsupport any sort of single column encryption similar to Azure SQL’s Always Encrypted yet.\n\n#### **Keeping secure**\n\n\nAzure has all the tools necessary to release a highly secure product. H', 'owever, a chain is only as strong\nas its weakest link. If the applications deployed on top of Azure aren’t developed with a proper\nsecurity mindset and good security audits, then they become the weak link in the chain. There are\nmany great static analysis tools, encryption libraries, and security practices that can be used to ensure\n[that the software installed on Azure is as secure as Azure itself. Examples include static analysis tools,](https:', '//www.mend.io/sca/)\n[encryption libraries, and security practices.](https://www.libressl.org/)\n\n\n160 CHAPTER 9 | Cloud-native security\n\n\n**CHAPTER**\n# 10\n\n## DevOps\n\n\nThe favorite mantra of software consultants is to answer “It depends” to any question posed. It isn’t\nbecause software consultants are fond of not taking a position. It’s because there’s no one true\nanswer to any questions in software. There’s no absolute right and wrong, but rather', ' a balance\nbetween opposites.\n\n\nTake, for instance, the two major schools of developing web applications: Single Page Applications\n(SPAs) versus server-side applications. On the one hand, the user experience tends to be better with\nSPAs and the amount of traffic to the web server can be minimized making it possible to host them\non something as simple as static hosting. On the other hand, SPAs tend to be slower to develop and\nmore difficult to tes', 't. Which one is the right choice? Well, it depends on your situation.\n\n\nCloud-native applications aren’t immune to that same dichotomy. They have clear advantages in\nterms of speed of development, stability, and scalability, but managing them can be quite a bit more\ndifficult.\n\n\nYears ago, it wasn’t uncommon for the process of moving an application from development to\nproduction to take a month, or even more. Companies released software on a 6-mo', 'nth or even every\nyear cadence. One needs to look no further than Microsoft Windows to get an idea for the cadence of\nreleases that were acceptable before the ever-green days of Windows 10. Five years passed between\nWindows XP and Vista, a further three between Vista and Windows 7.\n\n\nIt’s now fairly well established that being able to release software rapidly gives fast-moving companies\na huge market advantage over their more sloth-like competito', 'rs. It’s for that reason that major\nupdates to Windows 10 are now approximately every six months.\n\n\nThe patterns and practices that enable faster, more reliable releases to deliver value to the business\nare collectively known as DevOps. They consist of a wide range of ideas spanning the entire software\ndevelopment life cycle from specifying an application all the way up to delivering and operating that\napplication.\n\n\nDevOps emerged before microse', 'rvices and it’s likely that the movement towards smaller, more fit to\npurpose services wouldn’t have been possible without DevOps to make releasing and operating not\njust one but many applications in production easier.\n\n\n161 CHAPTER 10 | DevOps\n\n\n_Figure 10-1 - DevOps and microservices._\n\n\nThrough good DevOps practices, it’s possible to realize the advantages of cloud-native applications\nwithout suffocating under a mountain of work actually opera', 'ting the applications.\n\n\nThere’s no golden hammer when it comes to DevOps. Nobody can sell a complete and allencompassing solution for releasing and operating high-quality applications. This is because each\napplication is wildly different from all others. However, there are tools that can make DevOps a far less\ndaunting proposition. One of these tools is known as Azure DevOps.\n\n### Azure DevOps\n\n\nAzure DevOps has a long pedigree. It can trace its', ' roots back to when Team Foundation Server first\nmoved online and through the various name changes: Visual Studio Online and Visual Studio Team\nServices. Through the years, however, it has become far more than its predecessors.\n\n\nAzure DevOps is divided into five major components:\n\n\n_Figure 10-2 - Azure DevOps._\n\n\n**Azure Repos** - Source code management that supports the venerable Team Foundation Version\n[Control (TFVC) and the industry favorite', ' Git. Pull requests provide a way to enable social coding by](https://en.wikipedia.org/wiki/Git)\nfostering discussion of changes as they’re made.\n\n\n162 CHAPTER 10 | DevOps\n\n\n**Azure Boards** - Provides an issue and work item tracking tool that strives to allow users to pick the\nworkflows that work best for them. It comes with a number of pre-configured templates including\nones to support SCRUM and Kanban styles of development.\n\n\n**Azure Pipelines', '** - A build and release management system that supports tight integration with Azure.\nBuilds can be run on various platforms from Windows to Linux to macOS. Build agents may be\nprovisioned in the cloud or on-premises.\n\n\n**Azure Test Plans** - No QA person will be left behind with the test management and exploratory\ntesting support offered by the Test Plans feature.\n\n\n**Azure Artifacts** - An artifact feed that allows companies to create their ow', 'n, internal, versions of\nNuGet, npm, and others. It serves a double purpose of acting as a cache of upstream packages if\nthere’s a failure of a centralized repository.\n\n\nThe top-level organizational unit in Azure DevOps is known as a Project. Within each project the\nvarious components, such as Azure Artifacts, can be turned on and off. Each of these components\nprovides different advantages for cloud-native applications. The three most useful are ', 'repositories,\nboards, and pipelines. If users want to manage their source code in another repository stack, such as\nGitHub, but still take advantage of Azure Pipelines and other components, that’s perfectly possible.\n\n\nFortunately, development teams have many options when selecting a repository. One of them is\nGitHub.\n\n### GitHub Actions\n\n\nFounded in 2009, GitHub is a widely popular web-based repository for hosting projects,\ndocumentation, and co', 'de. Many large tech companies, such as Apple, Amazon, Google, and\nmainstream corporations use GitHub. GitHub uses the open-source, distributed version control system\nnamed Git as its foundation. On top, it then adds its own set of features, including defect tracking,\nfeature and pull requests, tasks management, and wikis for each code base.\n\n\nAs GitHub evolves, it too is adding DevOps features. For example, GitHub has its own continuous\nintegrati', 'on/continuous delivery (CI/CD) pipeline, called GitHub Actions. GitHub Actions is a\ncommunity-powered workflow automation tool. It lets DevOps teams integrate with their existing\ntooling, mix and match new products, and hook into their software lifecycle, including existing CI/CD\npartners.”\n\n\nGitHub has over 40 million users, making it the largest host of source code in the world. In October of\n[2018, Microsoft purchased GitHub. Microsoft has ple', 'dged that GitHub will remain an open platform](https://techcrunch.com/2018/06/04/microsoft-promises-to-keep-github-independent-and-open/)\nthat any developer can plug into and extend. It continues to operate as an independent company.\nGitHub offers plans for enterprise, team, professional, and free accounts.\n\n### Source control\n\n\nOrganizing the code for a cloud-native application can be challenging. Instead of a single giant\napplication, the cloud', '-native applications tend to be made up of a web of smaller applications that\ntalk with one another. As with all things in computing, the best arrangement of code remains an open\n\n\n163 CHAPTER 10 | DevOps\n\n\nquestion. There are examples of successful applications using different kinds of layouts, but two\nvariants seem to have the most popularity.\n\n\nBefore getting down into the actual source control itself, it’s probably worth deciding on how many\n', 'projects are appropriate. Within a single project, there’s support for multiple repositories, and build\npipelines. Boards are a little more complicated, but there too, the tasks can easily be assigned to\nmultiple teams within a single project. It’s possible to support hundreds, even thousands of\ndevelopers, out of a single Azure DevOps project. Doing so is likely the best approach as it provides a\nsingle place for all developer to work out of and', ' reduces the confusion of finding that one application\nwhen developers are unsure in which project in which it resides.\n\n\nSplitting up code for microservices within the Azure DevOps project can be slightly more challenging.\n\n\n_Figure 10-3 - One vs. many repositories._\n\n#### **Repository per microservice**\n\n\nAt first glance, this approach seems like the most logical approach to splitting up the source code for\nmicroservices. Each repository can co', 'ntain the code needed to build the one microservice. The\nadvantages to this approach are readily visible:\n\n\n1. Instructions for building and maintaining the application can be added to a README file at\nthe root of each repository. When flipping through the repositories, it’s easy to find these\ninstructions, reducing spin-up time for developers.\n\n2. Every service is located in a logical place, easily found by knowing the name of the service.\n\n3. B', 'uilds can easily be set up such that they’re only triggered when a change is made to the\nowning repository.\n\n4. The number of changes coming into a repository is limited to the small number of developers\nworking on the project.\n\n\n164 CHAPTER 10 | DevOps\n\n\n5. Security is easy to set up by restricting the repositories to which developers have read and\nwrite permissions.\n\n6. Repository level settings can be changed by the owning team with a minimum ', 'of discussion\nwith others.\n\nOne of the key ideas behind microservices is that services should be siloed and separated from each\nother. When using Domain Driven Design to decide on the boundaries for services the services act as\ntransactional boundaries. Database updates shouldn’t span multiple services. This collection of related\ndata is referred to as a bounded context. This idea is reflected by the isolation of microservice data to\na database s', 'eparate and autonomous from the rest of the services. It makes a great deal of sense to\ncarry this idea all the way through to the source code.\n\n\nHowever, this approach isn’t without its issues. One of the more gnarly development problems of our\ntime is managing dependencies. Consider the number of files that make up the average\nnode_modules directory. A fresh install of something like create-react-app is likely to bring with it\nthousands of pack', 'ages. The question of how to manage these dependencies is a difficult one.\n\n\nIf a dependency is updated, then downstream packages must also update this dependency.\nUnfortunately, that takes development work so, invariably, the node_modules directory ends up with\nmultiple versions of a single package, each one a dependency of some other package that is\nversioned at a slightly different cadence. When deploying an application, which version of a\ndep', 'endency should be used? The version that is currently in production? The version that is currently\nin Beta but is likely to be in production by the time the consumer makes it to production? Difficult\nproblems that aren’t resolved by just using microservices.\n\n\nThere are libraries that are depended upon by a wide variety of projects. By dividing the microservices\nup with one in each repository the internal dependencies can best be resolved by usin', 'g the internal\nrepository, Azure Artifacts. Builds for libraries will push their latest versions into Azure Artifacts for\ninternal consumption. The downstream project must still be manually updated to take a dependency\non the newly updated packages.\n\n\nAnother disadvantage presents itself when moving code between services. Although it would be nice\nto believe that the first division of an application into microservices is 100% correct, the reality', ' is that\nrarely we’re so prescient as to make no service division mistakes. Thus, functionality and the code that\ndrives it will need to move from service to service: repository to repository. When leaping from one\nrepository to another, the code loses its history. There are many cases, especially in the event of an\naudit, where having full history on a piece of code is invaluable.\n\n\nThe final and most important disadvantage is coordinating chang', 'es. In a true microservices\napplication, there should be no deployment dependencies between services. It should be possible to\ndeploy services A, B, and C in any order as they have loose coupling. In reality, however, there are\ntimes when it’s desirable to make a change that crosses multiple repositories at the same time. Some\nexamples include updating a library to close a security hole or changing a communication protocol\nused by all services.\n\n', '\nTo do a cross-repository change requires a commit to each repository be made in succession. Each\nchange in each repository will need to be pull-requested and reviewed separately. This activity can be\ndifficult to coordinate.\n\n\n165 CHAPTER 10 | DevOps\n\n\nAn alternative to using many repositories is to put all the source code together in a giant, all knowing,\nsingle repository.\n\n#### **Single repository**\n\n\n[In this approach, sometimes referred to ', 'as a monorepository, all the source code for every service is](https://danluu.com/monorepo/)\nput into the same repository. At first, this approach seems like a terrible idea likely to make dealing\nwith source code unwieldy. There are, however, some marked advantages to working this way.\n\n\nThe first advantage is that it’s easier to manage dependencies between projects. Instead of relying on\nsome external artifact feed, projects can directly import', ' one another. This means that updates are\ninstant, and conflicting versions are likely to be found at compile time on the developer’s workstation.\nIn effect, shifting some of the integration testing left.\n\n\nWhen moving code between projects, it’s now easier to preserve the history as the files will be\ndetected as having been moved rather than being rewritten.\n\n\nAnother advantage is that wide ranging changes that cross service boundaries can be ma', 'de in a\nsingle commit. This activity reduces the overhead of having potentially dozens of changes to review\nindividually.\n\n\nThere are many tools that can perform static analysis of code to detect insecure programming\npractices or problematic use of APIs. In a multi-repository world, each repository will need to be\niterated over to find the problems in them. The single repository allows running the analysis all in one\nplace.\n\n\nThere are also many ', 'disadvantages to the single repository approach. One of the most worrying ones\nis that having a single repository raises security concerns. If the contents of a repository are leaked in\na repository per service model, the amount of code lost is minimal. With a single repository,\neverything the company owns could be lost. There have been many examples in the past of this\nhappening and derailing entire game development efforts. Having multiple repo', 'sitories exposes less\nsurface area, which is a desirable trait in most security practices.\n\n\nThe size of the single repository is likely to become unmanageable rapidly. This presents some\n[interesting performance implications. It may become necessary to use specialized tools such as Virtual](https://github.com/Microsoft/VFSForGit)\n[File System for Git, which was originally designed to improve the experience for developers on the](https://github.c', 'om/Microsoft/VFSForGit)\nWindows team.\n\n\nFrequently the argument for using a single repository boils down to an argument that Facebook or\nGoogle use this method for source code arrangement. If the approach is good enough for these\ncompanies, then, surely, it’s the correct approach for all companies. The truth of the matter is that few\ncompanies operate on anything like the scale of Facebook or Google. The problems that occur at\nthose scales are di', 'fferent from those most developers will face. What is good for the goose may not\nbe good for the gander.\n\n\nIn the end, either solution can be used to host the source code for microservices. However, in most\ncases, the management, and engineering overhead of operating in a single repository isn’t worth the\nmeager advantages. Splitting code up over multiple repositories encourages better separation of\nconcerns and encourages autonomy among developm', 'ent teams.\n\n\n166 CHAPTER 10 | DevOps\n\n\n#### **Standard directory structure**\n\nRegardless of the single versus multiple repositories debate each service will have its own directory.\nOne of the best optimizations to allow developers to cross between projects quickly is to maintain a\nstandard directory structure.\n\n\n_Figure 10-4 - Standard directory structure._\n\n\nWhenever a new project is created, a template that puts in place the correct structure s', 'hould be used.\nThis template can also include such useful items as a skeleton README file and an azurepipelines.yml. In any microservice architecture, a high degree of variance between projects makes bulk\noperations against the services more difficult.\n\n\nThere are many tools that can provide templating for an entire directory, containing several source\n[code directories. Yeoman is popular in the JavaScript world and GitHub have recently released]', '(https://yeoman.io/)\n[Repository Templates, which provide much of the same functionality.](https://github.blog/2019-06-06-generate-new-repositories-with-repository-templates/)\n\n### Task management\n\n\nManaging tasks in any project can be difficult. Up front there are countless questions to be answered\nabout the sort of workflows to set up to ensure optimal developer productivity.\n\n\nCloud-native applications tend to be smaller than traditional softw', 'are products or at least they’re\ndivided into smaller services. Tracking of issues or tasks related to these services remains as important\nas with any other software project. Nobody wants to lose track of some work item or explain to a\ncustomer that their issue wasn’t properly logged. Boards are configured at the project level but within\neach project, areas can be defined. These allow breaking down issues across several components. The\nadvantage ', 'to keeping all the work for the entire application in one place is that it’s easy to move work\nitems from one team to another as they’re understood better.\n\n\n167 CHAPTER 10 | DevOps\n\n\nAzure DevOps comes with a number of popular templates pre-configured. In the most basic\nconfiguration, all that is needed to know is what’s in the backlog, what people are working on, and\nwhat’s done. It’s important to have this visibility into the process of buildi', 'ng software, so that work\ncan be prioritized and completed tasks reported to the customer. Of course, few software projects\nstick to a process as simple as to do, doing, and done. It doesn’t take long for people to start adding\nsteps like QA or Detailed Specification to the process.\n\n\nOne of the more important parts of Agile methodologies is self-introspection at regular intervals.\nThese reviews are meant to provide insight into what problems the', ' team is facing and how they can\nbe improved. Frequently, this means changing the flow of issues and features through the\ndevelopment process. So, it’s perfectly healthy to expand the layouts of the boards with additional\nstages.\n\n\nThe stages in the boards aren’t the only organizational tool. Depending on the configuration of the\nboard, there’s a hierarchy of work items. The most granular item that can appear on a board is a task.\nOut of the box ', 'a task contains fields for a title, description, a priority, an estimate of the amount of\nwork remaining and the ability to link to other work items or development items (branches, commits,\npull requests, builds, and so forth). Work items can be classified into different areas of the application\nand different iterations (sprints) to make finding them easier.\n\n\n_Figure 10-5 - Task in Azure DevOps._\n\n\nThe description field supports the normal style', 's you’d expect (bold, italic underscore and strike\nthrough) and the ability to insert images. This makes it a powerful tool for use when specifying work\nor bugs.\n\n\n[Tasks can be rolled up into features, which define a larger unit of work. Features, in turn, can be rolled](https://docs.microsoft.com/azure/devops/boards/backlogs/define-features-epics?view=azure-devops&preserve-view=true)\n[up into epics. Classifying tasks in this hierarchy makes it ', 'much easier to understand how close a large](https://docs.microsoft.com/azure/devops/boards/backlogs/define-features-epics?view=azure-devops&preserve-view=true)\nfeature is to rolling out.\n\n\n168 CHAPTER 10 | DevOps\n\n\n_Figure 10-6 - Work item in Azure DevOps._\n\n\nThere are different kinds of views into the issues in Azure Boards. Items that aren’t yet scheduled\nappear in the backlog. From there, they can be assigned to a sprint. A sprint is a time b', 'ox during\nwhich it’s expected some quantity of work will be completed. This work can include tasks but also the\nresolution of tickets. Once there, the entire sprint can be managed from the Sprint board section. This\nview shows how work is progressing and includes a burn down chart to give an ever-updating\nestimate of if the sprint will be successful.\n\n\n_Figure 10-7 - Board in Azure DevOps._\n\n\nBy now, it should be apparent that there’s a great dea', 'l of power in the Boards in Azure DevOps. For\ndevelopers, there are easy views of what is being worked on. For project managers views into\nupcoming work as well as an overview of existing work. For managers, there are plenty of reports\nabout resourcing and capacity. Unfortunately, there’s nothing magical about cloud-native applications\nthat eliminate the need to track work. But if you must track work, there are a few places where the\nexperience i', 's better than in Azure DevOps.\n\n### CI/CD pipelines\n\n\nAlmost no change in the software development life cycle has been so revolutionary as the advent of\ncontinuous integration (CI) and continuous delivery (CD). Building and running automated tests\nagainst the source code of a project as soon as a change is checked in catches mistakes early. Prior to\nthe advent of continuous integration builds, it wouldn’t be uncommon to pull code from the\n\n\n169 C', 'HAPTER 10 | DevOps\n\n\nrepository and find that it didn’t pass tests or couldn’t even be built. This resulted in tracking down\nthe source of the breakage.\n\n\nTraditionally shipping software to the production environment required extensive documentation and\na list of steps. Each one of these steps needed to be manually completed in a very error prone\nprocess.\n\n\n_Figure 10-8 - Checklist._\n\n\nThe sister of continuous integration is continuous delivery i', 'n which the freshly built packages are\ndeployed to an environment. The manual process can’t scale to match the speed of development so\nautomation becomes more important. Checklists are replaced by scripts that can execute the same\ntasks faster and more accurately than any human.\n\n\nThe environment to which continuous delivery delivers might be a test environment or, as is being\ndone by many major technology companies, it could be the production en', 'vironment. The latter\nrequires an investment in high-quality tests that can give confidence that a change isn’t going to\nbreak production for users. In the same way that continuous integration caught issues in the code\nearly continuous delivery catches issues in the deployment process early.\n\n\nThe importance of automating the build and delivery process is accentuated by cloud-native\napplications. Deployments happen more frequently and to more env', 'ironments so manually deploying\nborders on impossible.\n\n#### **Azure Builds**\n\n\nAzure DevOps provides a set of tools to make continuous integration and deployment easier than\never. These tools are located under Azure Pipelines. The first of them is Azure Builds, which is a tool\nfor running YAML-based build definitions at scale. Users can either bring their own build machines\n(great for if the build requires a meticulously set up environment) or u', 'se a machine from a constantly\nrefreshed pool of Azure hosted virtual machines. These hosted build agents come pre-installed with a\n\n\n170 CHAPTER 10 | DevOps\n\n\nwide range of development tools for not just .NET development but for everything from Java to\nPython to iPhone development.\n\n\nDevOps includes a wide range of out of the box build definitions that can be customized for any build.\nThe build definitions are defined in a file called azure-pipe', 'lines.yml and checked into the repository so\nthey can be versioned along with the source code. This makes it much easier to make changes to the\nbuild pipeline in a branch as the changes can be checked into just that branch. An example azurepipelines.yml for building an ASP.NET web application on full framework is show in Figure 10-9.\n\n\n171 CHAPTER 10 | DevOps\n\n\n_Figure 10-9 - A sample azure-pipelines.yml_\n\n\nThis build definition uses a number of ', 'built-in tasks that make creating builds as simple as building a\nLego set (simpler than the giant Millennium Falcon). For instance, the NuGet task restores NuGet\npackages, while the VSBuild task calls the Visual Studio build tools to perform the actual compilation.\nThere are hundreds of different tasks available in Azure DevOps, with thousands more that are\nmaintained by the community. It’s likely that no matter what build tasks you’re looking to', ' run,\nsomebody has built one already.\n\n\nBuilds can be triggered manually, by a check-in, on a schedule, or by the completion of another build.\nIn most cases, building on every check-in is desirable. Builds can be filtered so that different builds run\nagainst different parts of the repository or against different branches. This allows for scenarios like\nrunning fast builds with reduced testing on pull requests and running a full regression suite a', 'gainst\nthe trunk on a nightly basis.\n\n\nThe end result of a build is a collection of files known as build artifacts. These artifacts can be passed\nalong to the next step in the build process or added to an Azure Artifacts feed, so they can be\nconsumed by other builds.\n\n#### **Azure DevOps releases**\n\n\nBuilds take care of compiling the software into a shippable package, but the artifacts still need to be\npushed out to a testing environment to compl', 'ete continuous delivery. For this, Azure DevOps uses a\nseparate tool called Releases. The Releases tool makes use of the same tasks’ library that were\navailable to the Build but introduce a concept of “stages”. A stage is an isolated environment into\nwhich the package is installed. For instance, a product might make use of a development, a QA, and a\nproduction environment. Code is continuously delivered into the development environment where\nauto', 'mated tests can be run against it. Once those tests pass the release moves onto the QA\nenvironment for manual testing. Finally, the code is pushed to production where it’s visible to\neverybody.\n\n\n_Figure 10-10 - Release pipeline_\n\n\nEach stage in the build can be automatically triggered by the completion of the previous phase. In\nmany cases, however, this isn’t desirable. Moving code into production might require approval from\nsomebody. The Releas', 'es tool supports this by allowing approvers at each step of the release pipeline.\nRules can be set up such that a specific person or group of people must sign off on a release before it\n\n\n172 CHAPTER 10 | DevOps\n\n\nmakes into production. These gates allow for manual quality checks and also for compliance with any\nregulatory requirements related to control what goes into production.\n\n#### **Everybody gets a build pipeline**\n\n\nThere’s no cost to con', 'figuring many build pipelines, so it’s advantageous to have at least one build\npipeline per microservice. Ideally, microservices are independently deployable to any environment so\nhaving each one able to be released via its own pipeline without releasing a mass of unrelated code is\nperfect. Each pipeline can have its own set of approvals allowing for variations in build process for\neach service.\n\n#### **Versioning releases**\n\n\nOne drawback to usi', 'ng the Releases functionality is that it can’t be defined in a checked-in azurepipelines.yml file. There are many reasons you might want to do that from having per-branch release\ndefinitions to including a release skeleton in your project template. Fortunately, work is ongoing to\nshift some of the stages support into the Build component. This will be known as multi-stage build\n[and the first version is available now!](https://devblogs.microsoft.c', 'om/devops/whats-new-with-azure-pipelines/)\n\n### Feature flags\n\n\nIn chapter 1, we affirmed that cloud native is much about speed and agility. Users expect rapid\nresponsiveness, innovative features, and zero downtime. Feature flags are a modern deployment\ntechnique that helps increase agility for cloud-native applications. They enable you to deploy new\nfeatures into a production environment, but restrict their availability. With the flick of a swit', 'ch, you can\nactivate a new feature for specific users without restarting the app or deploying new code. They\nseparate the release of new features from their code deployment.\n\n\nFeature flags are built upon conditional logic that control visibility of functionality for users at run\ntime. In modern cloud-native systems, it’s common to deploy new features into production early, but\ntest them with a limited audience. As confidence increases, the featu', 're can be incrementally rolled out\nto wider audiences.\n\n\nOther use cases for feature flags include:\n\n\n  - Restrict premium functionality to specific customer groups willing to pay higher subscription\nfees.\n\n  - Stabilize a system by quickly deactivating a problem feature, avoiding the risks of a rollback or\nimmediate hotfix.\n\n  - Disable an optional feature with high resource consumption during peak usage periods.\n\n  - Conduct experimental featur', 'e releases to small user segments to validate feasibility and\npopularity.\n\n\nFeature flags also promote trunk-based development. It’s a source-control branching model where\ndevelopers collaborate on features in a single branch. The approach minimizes the risk and complexity\nof merging large numbers of long-running feature branches. Features are unavailable until activated.\n\n\n173 CHAPTER 10 | DevOps\n\n\n#### **Implementing feature flags**\n\nAt its cor', 'e, a feature flag is a reference to a simple decision object. It returns a Boolean state of on or\noff. The flag typically wraps a block of code that encapsulates a feature capability. The state of the flag\ndetermines whether that code block executes for a given user. Figure 10-11 shows the\nimplementation.\n\n\n_Figure 10-11 - Simple feature flag implementation._\n\n\nNote how this approach separates the decision logic from the feature code.\n\n\nIn chapte', 'r 1, we discussed the Twelve-Factor App. The guidance recommended keeping configuration\nsettings external from application executable code. When needed, settings can be read in from the\nexternal source. Feature flag configuration values should also be independent from their codebase. By\nexternalizing flag configuration in a separate repository, you can change flag state without modifying\nand redeploying the application.\n\n\n[Azure App Configuration', '](https://docs.microsoft.com/azure/azure-app-configuration/overview) provides a centralized repository for feature flags. With it, you define\ndifferent kinds of feature flags and manipulate their states quickly and confidently. You add the App\nConfiguration client libraries to your application to enable feature flag functionality. Various\nprogramming language frameworks are supported.\n\n\n[Feature flags can be easily implemented in an ASP.NET Core ', 'service. Installing the .NET Feature](https://docs.microsoft.com/azure/azure-app-configuration/use-feature-flags-dotnet-core)\nManagement libraries and App Configuration provider enable you to declaratively add feature flags to\nyour code. They enable FeatureGate attributes so that you don’t have to manually write if statements\nacross your codebase.\n\n\nOnce configured in your Startup class, you can add feature flag functionality at the controller, a', 'ction,\nor middleware level. Figure 10-12 presents controller and action implementation:\n\n\n_Figure 10-12 - Feature flag implementation in a controller and action._\n\n\nIf a feature flag is disabled, the user will receive a 404 (Not Found) status code with no response body.\n\n\nFeature flags can also be injected directly into C# classes. Figure 10-13 shows feature flag injection:\n\n\n174 CHAPTER 10 | DevOps\n\n\n_Figure 10-13 - Feature flag injection into a', ' class._\n\n\nThe Feature Management libraries manage the feature flag lifecycle behind the scenes. For example,\nto minimize high numbers of calls to the configuration store, the libraries cache flag states for a\nspecified duration. They can guarantee the immutability of flag states during a request call. They also\noffer a Point-in-time snapshot. You can reconstruct the history of any key-value and provide its past\nvalue at any moment within the pre', 'vious seven days.\n\n### Infrastructure as code\n\n\nCloud-native systems embrace microservices, containers, and modern system design to achieve speed\nand agility. They provide automated build and release stages to ensure consistent and quality code.\nBut, that’s only part of the story. How do you provision the cloud environments upon which these\nsystems run?\n\n\n[Modern cloud-native applications embrace the widely accepted practice of Infrastructure as ', 'Code, or](https://docs.microsoft.com/devops/deliver/what-is-infrastructure-as-code)\nIaC. With IaC, you automate platform provisioning. You essentially apply software engineering\npractices such as testing and versioning to your DevOps practices. Your infrastructure and\ndeployments are automated, consistent, and repeatable. Just as continuous delivery automated the\ntraditional model of manual deployments, Infrastructure as Code (IaC) is evolving ho', 'w application\nenvironments are managed.\n\n\nTools like Azure Resource Manager (ARM), Terraform, and the Azure Command Line Interface (CLI)\nenable you to declaratively script the cloud infrastructure you require.\n\n#### **Azure Resource Manager templates**\n\n\n[ARM stands for Azure Resource Manager. It’s an API provisioning engine that is built into Azure and](https://docs.microsoft.com/azure/azure-resource-manager/management/overview)\nexposed as an AP', 'I service. ARM enables you to deploy, update, delete, and manage the resources\ncontained in Azure resource group in a single, coordinated operation. You provide the engine with a\nJSON-based template that specifies the resources you require and their configuration. ARM\nautomatically orchestrates the deployment in the correct order respecting dependencies. The engine\nensures idempotency. If a desired resource already exists with the same configurat', 'ion, provisioning\nwill be ignored.\n\n\nAzure Resource Manager templates are a JSON-based language for defining various resources in\nAzure. The basic schema looks something like Figure 10-14.\n\n\n175 CHAPTER 10 | DevOps\n\n\n_Figure 10-14 - The schema for a Resource Manager template_\n\n\nWithin this template, one might define a storage container inside the resources section like so:\n\n\n_Figure 10-15 - An example of a storage account defined in a Resource Ma', 'nager template_\n\n\nAn ARM template can be parameterized with dynamic environment and configuration information.\nDoing so enables it to be reused to define different environments, such as development, QA, or\nproduction. Normally, the template creates all resources within a single Azure resource group. It’s\npossible to define multiple resource groups in a single Resource Manager template, if needed. You\ncan delete all resources in an environment by ', 'deleting the resource group itself. Cost analysis can also\nbe run at the resource group level, allowing for quick accounting of how much each environment is\ncosting.\n\n\n[There are many examples of ARM templates available in the Azure Quickstart Templates project on](https://github.com/Azure/azure-quickstart-templates)\nGitHub. They can help accelerate creating a new template or modifying an existing one.\n\n\nResource Manager templates can be run in m', 'any of ways. Perhaps the simplest way is to simply paste\nthem into the Azure portal. For experimental deployments, this method can be quick. They can also be\nrun as part of a build or release process in Azure DevOps. There are tasks that will leverage\nconnections into Azure to run the templates. Changes to Resource Manager templates are applied\nincrementally, meaning that to add a new resource requires just adding it to the template. The tooling\n', 'will reconcile differences between the current resources and those defined in the template. Resources\nwill then be created or altered so they match what is defined in the template.\n\n#### **Terraform**\n\n\nCloud-native applications are often constructed to be cloud agnostic. Being so means the application\nisn’t tightly coupled to a particular cloud vendor and can be deployed to any public cloud.\n\n\n176 CHAPTER 10 | DevOps\n\n\n[Terraform](https://www.te', 'rraform.io/) is a commercial templating tool that can provision cloud-native applications across all the\nmajor cloud players: Azure, Google Cloud Platform, AWS, and AliCloud. Instead of using JSON as the\ntemplate definition language, it uses the slightly more terse HCL (Hashicorp Configuration Language).\n\n\nAn example Terraform file that does the same as the previous Resource Manager template (Figure 1015) is shown in Figure 10-16:\n\n\n_Figure 10-16', ' - An example of a Resource Manager template_\n\n\nTerraform also provides intuitive error messages for problem templates. There’s also a handy validate\ntask that can be used in the build phase to catch template errors early.\n\n\nAs with Resource Manager templates, command-line tools are available to deploy Terraform\ntemplates. There are also community-created tasks in Azure Pipelines that can validate and apply\nTerraform templates.\n\n\nSometimes Terraf', 'orm and ARM templates output meaningful values, such as a connection string to a\nnewly created database. This information can be captured in the build pipeline and used in\nsubsequent tasks.\n\n#### **Azure CLI Scripts and Tasks**\n\n\n[Finally, you can leverage Azure CLI](https://docs.microsoft.com/cli/azure/) to declaratively script your cloud infrastructure. Azure CLI scripts\ncan be created, found, and shared to provision and configure almost any Az', 'ure resource. The CLI is\nsimple to use with a gentle learning curve. Scripts are executed within either PowerShell or Bash.\nThey’re also straightforward to debug, especially when compared with ARM templates.\n\n\nAzure CLI scripts work well when you need to tear down and redeploy your infrastructure. Updating\nan existing environment can be tricky. Many CLI commands aren’t idempotent. That means they’ll\nrecreate the resource each time they’re run, ev', 'en if the resource already exists. It’s always possible to\nadd code that checks for the existence of each resource before creating it. But, doing so, your script\ncan become bloated and difficult to manage.\n\n\nThese scripts can also be embedded in Azure DevOps pipelines as Azure CLI tasks. Executing the\npipeline invokes the script.\n\n\n177 CHAPTER 10 | DevOps\n\n\nFigure 10-17 shows a YAML snippet that lists the version of Azure CLI and the details of t', 'he\nsubscription. Note how Azure CLI commands are included in an inline script.\n\n\n_Figure 10-17 - Azure CLI script_\n\n\n[In the article, What is Infrastructure as Code, Author Sam Guckenheimer describes how, “Teams who](https://docs.microsoft.com/devops/deliver/what-is-infrastructure-as-code)\nimplement IaC can deliver stable environments rapidly and at scale. Teams avoid manual\nconfiguration of environments and enforce consistency by representing th', 'e desired state of their\nenvironments via code. Infrastructure deployments with IaC are repeatable and prevent runtime issues\ncaused by configuration drift or missing dependencies. DevOps teams can work together with a\nunified set of practices and tools to deliver applications and their supporting infrastructure rapidly,\nreliably, and at scale.”\n\n### Cloud Native Application Bundles\n\n\nA key property of cloud-native applications is that they lever', 'age the capabilities of the cloud to speed\nup development. This design often means that a full application uses different kinds of technologies.\nApplications may be shipped in Docker containers, some services may use Azure Functions, while\nother parts may run directly on virtual machines allocated on large metal servers with hardware GPU\nacceleration. No two cloud-native applications are the same, so it’s been difficult to provide a single\nmechan', 'ism for shipping them.\n\n\nThe Docker containers may run on Kubernetes using a Helm Chart for deployment. The Azure\nFunctions may be allocated using Terraform templates. Finally, the virtual machines may be allocated\nusing Terraform but built out using Ansible. This is a large variety of technologies and there has been\nno way to package them all together into a reasonable package. Until now.\n\n\nCloud Native Application Bundles (CNABs) are a joint ef', 'fort by many community-minded companies\nsuch as Microsoft, Docker, and HashiCorp to develop a specification to package distributed\napplications.\n\n\nThe effort was announced in December of 2018, so there’s still a fair bit of work to do to expose the\n[effort to the greater community. However, there’s already an open specification and a reference](https://github.com/deislabs/cnab-spec)\n[implementation known as Duffle. This tool, which was written in', ' Go, is a joint effort between Docker](https://duffle.sh/)\nand Microsoft.\n\n\nThe CNABs can contain different kinds of installation technologies. This aspect allows things like Helm\nCharts, Terraform templates, and Ansible Playbooks to coexist in the same package. Once built, the\npackages are self-contained and portable; they can be installed from a USB stick. The packages are\ncryptographically signed to ensure they originate from the party they cl', 'aim.\n\n\n178 CHAPTER 10 | DevOps\n\n\nThe core of a CNAB is a file called bundle.json. This file defines the contents of the bundle, be they\nTerraform or images or anything else. Figure 11-9 defines a CNAB that invokes some Terraform.\nNotice, however, that it actually defines an invocation image that is used to invoke the Terraform.\nWhen packaged up, the Docker file that is located in the _cnab_ directory is built into a Docker image,\nwhich will be in', 'cluded in the bundle. Having Terraform installed inside a Docker container in the\nbundle means that users don’t need to have Terraform installed on their machine to run the bundling.\n\n\n_Figure 10-18 - An example Terraform file_\n\n\nThe bundle.json also defines a set of parameters that are passed down into the Terraform.\nParameterization of the bundle allows for installation in various different environments.\n\n\nThe CNAB format is also flexible, allo', 'wing it to be used against any cloud. It can even be used against\n[on-premises solutions such as OpenStack.](https://www.openstack.org/)\n\n\n179 CHAPTER 10 | DevOps\n\n\n#### **DevOps Decisions**\n\nThere are so many great tools in the DevOps space these days and even more fantastic books and\n[papers on how to succeed. A favorite book to get started on the DevOps journey is The Phoenix](https://www.oreilly.com/library/view/the-phoenix-project/9781457191', '350/)\n[Project, which follows the transformation of a fictional company from NoOps to DevOps. One thing is](https://www.oreilly.com/library/view/the-phoenix-project/9781457191350/)\nfor certain: DevOps is no longer a “nice to have” when deploying complex, Cloud Native Applications.\nIt’s a requirement and should be planned for and resourced at the start of any project.\n\n#### **References**\n\n\n  - [Azure DevOps](https://azure.microsoft.com/services/d', 'evops/)\n\n  - [Azure Resource Manager](https://docs.microsoft.com/azure/azure-resource-manager/management/overview)\n\n  - [Terraform](https://www.terraform.io/)\n\n  - [Azure CLI](https://docs.microsoft.com/cli/azure/)\n\n\n180 CHAPTER 10 | DevOps\n\n\n**CHAPTER**\n# 11\n\n## Summary: Architecting cloud-native apps\n\n\nIn summary, here are important conclusions from this guide:\n\n\n  - **Cloud-native** is about designing modern applications that embrace rapid cha', 'nge, large scale,\nand resilience, in modern, dynamic environments such as public, private, and hybrid clouds.\n\n\n  - The **[Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/)** is an influential open-source consortium\nof over 300 major corporations. It’s responsible for driving the adoption of cloud-native\ncomputing across technology and cloud stacks.\n\n\n  - **CNCF guidelines** recommend that cloud-native applications embrace six impor', 'tant pillars as\nshown in Figure 11-1:\n\n\n**Figure 11-1** . Cloud-native foundational pillars\n\n\n  - These cloud-native pillars include:\n\n\n     - The cloud and its underlying service model\n\n     - Modern design principles\n\n     - Microservices\n\n     - Containerization and container orchestration\n\n     - Cloud-based backing services, such as databases and message brokers\n\n     - Automation, including Infrastructure as Code and code deployment\n\n\n181 C', 'HAPTER 11 | Summary: Architecting cloud-native apps\n\n\n  - **Kubernetes** is the hosting environment of choice for most cloud-native applications. Smaller,\nsimple services are sometimes hosted in serverless platforms, such as Azure Functions. Among\nmany key automation features, both environments provide automatic scaling to handle\nfluctuating workload volumes.\n\n\n  - **Service communication** becomes a significant design decision when constructing ', 'a cloudnative application. Applications typically expose an API gateway to manage front-end client\ncommunication. Then backend microservices strive to communicate with each other\nimplementing asynchronous communication patterns, when possible.\n\n\n  - **gRPC** is a modern, high-performance framework that evolves the age-old remote procedure\ncall (RPC) protocol. Cloud-native applications often embrace gRPC to streamline messaging\nbetween back-end se', 'rvices. gRPC uses HTTP/2 for its transport protocol. It can be up to 8x\nfaster than JSON serialization with message sizes 60-80% smaller. gRPC is open source and\nmanaged by the Cloud Native Computing Foundation (CNCF).\n\n\n  - **Distributed data** is a model often implemented by cloud-native applications. Applications\nsegregate business functionality into small, independent microservices. Each microservice\nencapsulates its own dependencies, data, a', 'nd state. The classic shared database model\nevolves into one of many smaller databases, each aligning with a microservice. When the\nsmoke clears, we emerge with a design that exposes a database-per-microservice model.\n\n\n  - **No-SQL databases** refer to high-performance, non-relational data stores. They excel in their\nease-of-use, scalability, resilience, and availability characteristics. High volume services that\nrequire sub second response time', ' favor NoSQL datastores. The proliferation of NoSQL\ntechnologies for distributed cloud-native systems can’t be overstated.\n\n\n  - **NewSQL** is an emerging database technology that combines the distributed scalability of\nNoSQL and the ACID guarantees of a relational database. NewSQL databases target business\nsystems that must process high-volumes of data, across distributed environments, with full\ntransactional/ACID compliance. The Cloud Native Co', 'mputing Foundation (CNCF) features\nseveral NewSQL database projects.\n\n\n  - **Resiliency** is the ability of your system to react to failure and still remain functional. Cloudnative systems embrace distributed architecture where failure is inevitable. Applications must\nbe constructed to respond elegantly to failure and quickly return to a fully functioning state.\n\n\n  - **Service meshes** are a configurable infrastructure layer with built-in capabi', 'lities to handle\nservice communication and other cross-cutting challenges. They decouple cross-cutting\nresponsibilities from your business code. These responsibilities move into a service proxy.\nReferred to as the Sidecar pattern, the proxy is deployed into a separate process to provide\nisolation from your business code.\n\n\n  - **Observability** is a key design consideration for cloud-native applications. As services are\ndistributed across a clust', 'er of nodes, centralized logging, monitoring, and alerts, become\nmandatory. Azure Monitor is a collection of cloud-based tools designed to provide visibility\ninto the state of your system.\n\n\n182 CHAPTER 11 | Summary: Architecting cloud-native apps\n\n\n  - **Infrastructure as Code** is a widely accepted practice that automates platform provisioning.\nYour infrastructure and deployments are automated, consistent, and repeatable. Tools like\nAzure Resou', 'rce Manager, Terraform, and the Azure CLI, enable you to declaratively script the\ncloud infrastructure you require.\n\n\n  - **Code automation** is a requirement for cloud-native applications. Modern CI/CD systems help\nfulfill this principle. They provide separate build and deployment steps that help ensure\nconsistent and quality code. The build stage transforms the code into a binary artifact. The\nrelease stage picks up the binary artifact, applies', ' external environment configuration, and\ndeploys it to a specified environment. Azure DevOps and GitHub are full-featured DevOps\nenvironments.\n\n\n183 CHAPTER 11 | Summary: Architecting cloud-native apps\n\n\n']