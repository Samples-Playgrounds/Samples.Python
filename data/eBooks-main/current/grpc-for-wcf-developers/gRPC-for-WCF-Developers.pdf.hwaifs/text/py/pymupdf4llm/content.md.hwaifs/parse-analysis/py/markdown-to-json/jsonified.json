{"1": {"Introduction to gRPC for WCF developers": {"History": "The fundamental principle of a computer network as nothing more than a group of computers\nexchanging data with each other to achieve a set of interrelated tasks hasn\u2019t changed since its\ninception. But the complexity, scale, and expectations have grown exponentially.\n\nDuring the 1990s, the emphasis was mainly on improving internal networks that used the same\nlanguage and platforms. TCP/IP became the gold standard for this type of communication.\n\nThe focus soon shifted to how best to optimize communication across multiple platforms by\npromoting a language-agnostic approach. Service-oriented architecture (SOA) provided a structure\nfor loosely coupling a broad collection of services that could be provided to an application.\n\nThe development of _web services_ occurred when all major platforms could access the internet, but\nthey still couldn\u2019t interact with each other. Web services have open standards and protocols,\nincluding:\n\n['XML to tag and code data.']\n\n1 CHAPTER 1 | Introduction to gRPC for WCF developers\n\n['Simple Object Access Protocol (SOAP) to transfer data.', 'Web Services Definition Language (WSDL) to describe and connect web services to client\\napplications.', 'Universal Description, Discovery, and Integration (UDDI) to make web services discoverable by\\nother services.']\n\nSOAP defines the rules by which distributed elements of an application can communicate with each\nother, even if they\u2019re on different platforms. SOAP is based on XML, so it\u2019s human-readable. The\nsacrifice for making SOAP easily understood is size; SOAP messages are larger than messages in\ncomparable protocols. SOAP was designed to break monolithic applications into multicomponent\nform without losing security or control. So WCF was designed to work with that kind of system, unlike\ngRPC, which began as a distributed system. WCF addressed some of these limitations by developing\nand documenting proprietary extension protocols for the SOAP stack, but at the cost of a lack of\nsupport from other platforms.\n\nWindows Communication Foundation is a framework for building services. It was designed in the early\n2000s to help developers using early SOA to manage the complexities of working with SOAP.\nAlthough it removes the requirement for the developers to write their own SOAP protocols, WCF still\nuses SOAP to enable interoperability with other systems. WCF was also designed to deliver solutions\nacross multiple protocols (HTTP/1.1, Net.TCP, and so on).", "Microservices": "In microservice architectures, large applications are built as a collection of smaller modular services.\nEach component does a specific task or process, and components are designed to work interoperably\nbut can be isolated as necessary.\n\nAdvantages to microservices include:\n\n['Changes and upgrades can be handled independently.', 'Error handling becomes more efficient because problems can be traced to specific services\\nthat are then isolated, rebuilt, tested, and redeployed independently of the other services.', 'Scalability can be confined to specific instances or services rather than the whole application.', 'Development can happen across multiple teams, with less friction than occurs when many\\nteams work on a single codebase.']\n\nThe move towards increasing virtualization, cloud computing, containers, and the Internet of Things\nhas contributed to the ongoing rise of microservices. But microservices aren\u2019t without their challenges.\nThe fragmented/decentralized infrastructure put more emphasis on the need for simplicity and speed\nwhen communicating between services. This in turn drew attention to the sometimes laborious and\ncontorted nature of SOAP.\n\nIt was into this environment that gRPC was launched, 10 years after Microsoft first released WCF.\nEvolved directly from Google\u2019s internal infrastructure RPC (Stubby), gRPC was never based on the\nsame standards and protocols that had informed the parameters of many earlier RPCs. And gRPC was\nonly ever based on HTTP/2. That\u2019s why it could draw on the new capabilities that advanced transport\nprotocol provided. In particular, bidirectional streaming, binary messaging, and multiplexing.\n\n2 CHAPTER 1 | Introduction to gRPC for WCF developers", "About this guide": "This guide covers the key features of gRPC. The early chapters take a high-level look at the main\nfeatures of WCF and compare them to those of gRPC. It identifies where there are direct correlations\nbetween WCF and gRPC and also where gRPC offers an advantage. When there\u2019s no correlation\nbetween WCF and gRPC, or when gRPC isn\u2019t able to offer an equivalent solution to WCF, this guide\nwill suggest workarounds or where to go for more information.\n\nUsing a set of sample WCF applications, Chapter 5 is a deep-dive look at converting the main types of\nWCF service (simple request-reply, one-way, and streaming) to their equivalents in gRPC.\n\nThe final section of the book looks at how to get the best from gRPC in practice. This section includes\ninformation on using additional tools, like Docker containers or Kubernetes, to take advantage of the\nefficiency of gRPC. It also includes a detailed look at monitoring with logging, metrics, and distributed\ntracing.", "Who this guide is for": "This guide was written for developers working in .NET Framework or .NET Core who have used WCF\nand who are seeking to migrate their applications to a modern RPC environment for .NET Core 3.0\nand later versions. The guide might also be useful more generally for developers upgrading or\nconsidering upgrading to .NET and who want to use the built-in gRPC tools.\n\n3 CHAPTER 1 | Introduction to gRPC for WCF developers\n\n**CHAPTER**"}}, "2": {"gRPC overview": {"Key principles": "As discussed in chapter 1, Google wanted to use the introduction of HTTP/2 to replace Stubby, its\ninternal, general purpose RPC infrastructure. gRPC, based on Stubby, now can take advantage of\nstandardization and would extend its applicability to mobile computing, the cloud, and the Internet of\nThings.\n\n[To achieve this standardization, the Cloud Native Computing Foundation (CNCF)](https://www.cncf.io/) established a set of\nprinciples that would govern gRPC. The following list shows the most relevant ones, which are\nprimarily concerned with maximizing accessibility and usability:\n\n['**Free and open**  - All artifacts should be open source, with licensing that doesn\u2019t constrain\\ndevelopers from adopting gRPC.', '**Coverage and simplicity**  - gRPC should be available across every popular platform, and\\nsimple enough to build on any platform.', '**Interoperability and reach**  - It should be possible to use gRPC on any network, regardless of\\nbandwidth or latency, by using widely available network standards.', '**General purpose and performant**  - The framework should be usable by as broad a range of\\nuse-cases as possible, without compromising performance.', '**Streaming**  - The protocol should provide streaming semantics for large datasets or\\nasynchronous messaging.', '**Metadata exchange**  - The protocol allows non-business data, such as authentication tokens,\\nto be handled separately from actual business data.', '**Standardized status codes**  - The variability of error codes should be reduced to make error\\nhandling decisions clearer. Where additional, richer error handling is required, a mechanism\\nshould be provided for managing behavior within the metadata exchange.']\n\n4 CHAPTER 2 | gRPC overview", "How gRPC approaches RPC": "Windows Communication Foundation (WCF) and gRPC are both implementations of the _Remote_\n_Procedure Call_ (RPC) pattern. This pattern aims to make calls to services that run on a different\nmachine, or in a different process, work seamlessly, like method calls in the client application. While\nthe aims of WCF and gRPC are the same, the details of the implementation are quite different.\n\nThe following table sets out how the key features of WCF relate to gRPC, and where you can find\n\n|more detailed explanations.|Col2|Col3|\n|---|---|---|\n|**Features**|**WCF**|**gRPC**|\n|Objective|Separate business<br>code from<br>networking<br>implementation.|Separate business code<br>from interface definition<br>and networking<br>implementation.|\n|Define services and messages (chapters 3-4)|Service Contract,<br>Operation<br>Contract, and<br>Data Contract.|Uses proto file to<br>declare services and<br>messages.|\n|Language (chapters 3-5)|Contracts written<br>in C# or Visual<br>Basic.|Protocol Buffer<br>language.|\n|Wire format (chapter 3)|Configurable,<br>including<br>SOAP/XML, Plain<br>XML, JSON, and<br>.NET Binary.|Protocol Buffer binary<br>format (although it\u2019s<br>possible to use other<br>formats).|\n|Interoperability (chapter 4)|When using<br>SOAP over HTTP.|Official support: .NET,<br>Java, Python, JavaScript,<br>C/C++, Go, Rust, Ruby,<br>Swift, Dart, PHP.<br>Unofficial support for<br>other languages from<br>the community.|\n|Networking (chapter 4)|Configured at run<br>time. Switch<br>between NetTCP,<br>HTTP, and<br>MSMQ.|HTTP/2, currently over<br>TCP only with ASP.NET<br>Core gRPC.|\n\n5 CHAPTER 2 | gRPC overview\n\n|Features|WCF|gRPC|\n|---|---|---|\n|Approach (chapter 4)|Runtime<br>generation of<br>serialization,<br>deserialization,<br>and networking<br>code in base<br>classes.|Build-time generation of<br>serialization,<br>deserialization, and<br>networking code in base<br>classes.|\n|Security (chapter 6)|Authentication,<br>WS-Security,<br>message<br>encryption.|Credentials, ASP.NET<br>Core security, TLS<br>networking.|", "Interface Definition Language": "With Windows Communication Foundation (WCF), services can expose description metadata by using\nthe Web Service Definition Language (WSDL). WSDL is generated dynamically by using .NET reflection\nat run time. Developers can use this metadata to generate clients for those services, potentially in\nother languages if they\u2019re using a platform-neutral binding such as SOAP over HTTP.\n\ngRPC uses the Interface Definition Language (IDL) from Protocol Buffers. The Protocol Buffers IDL is a\ncustom, platform-neutral language with an open specification. Developers author .proto files to\ndescribe services, along with their inputs and outputs. These .proto files can then be used to generate\nlanguage- or platform-specific stubs for clients and servers, allowing multiple different platforms to\ncommunicate. By sharing .proto files, teams can generate code to use each others\u2019 services, without\nneeding to take a code dependency.\n\nOne of the advantages of the Protobuf IDL is that as a custom language, it enables gRPC to be\ncompletely language and platform agnostic, not favoring any technology over another.\n\nThe Protobuf IDL is also designed for humans to both read and write, whereas WSDL is intended as a\nmachine-readable/writable format. Changing the WSDL of a WCF service typically requires changing\nthe service, running the service, and regenerating the WSDL file from the server. By contrast, with a\n.proto file, changes are simple to apply with a text editor, and automatically flow through the\ngenerated code. Visual Studio 2022 builds .proto files in the background when they are saved. With\nother editors, such as VS Code, the changes are applied when the project is built.\n\nWhen compared with XML, and particularly SOAP, messages encoded by using Protobuf have many\nadvantages. Protobuf messages tend to be smaller than the same data serialized as SOAP XML, and\nencoding, decoding, and transmitting them over a network can be faster.\n\nThe potential disadvantage of Protobuf compared to SOAP is that, because the messages aren\u2019t\nreadable by humans, additional tooling is required to debug message content.\n\n6 CHAPTER 2 | gRPC overview", "Network protocols": {"**Key features of HTTP/2**": "This list shows some of the key features and advantages of HTTP/2:\n\n**Binary protocol**\n\nRequest/response cycles no longer need text commands. This activity simplifies and speeds up the\nimplementation of commands. Specifically, parsing data is faster and uses less memory, network\nlatency is reduced with obvious related improvements to speed, and there\u2019s an overall better use of\nnetwork resources.\n\n**Streams**\n\nStreams allow you to create long-lived connections between sender and receiver, over which multiple\nmessages or frames can be sent asynchronously. Multiple streams can operate independently over a\nsingle HTTP/2 connection.\n\n**Request multiplexing over a single TCP connection**\n\nThis feature is one of the most important innovations of HTTP/2. Because it allows multiple parallel\nrequests for data, it\u2019s now possible to download web files concurrently from a single server. Websites\nload faster, and the need for optimization is reduced. Head-of-line (HOL) blocking, where responses\n\n7 CHAPTER 2 | gRPC overview\n\nthat are ready must wait to be sent until an earlier request is completed, is also mitigated (although\nHOL blocking can still occur at the TCP-transport level).\n\n**Net.TCP-like performance, cross-platform**\n\nFundamentally, the combination of gRPC and HTTP/2 offers developers at least the equivalent speed\nand efficiency of Net.TCP bindings for WCF, and in some cases even greater speed and efficiency. But,\nunlike Net.TCP, gRPC over HTTP/2 isn\u2019t constrained to .NET applications."}, "Why we recommend gRPC for WCF developers": {"**Similarity to WCF**": "Although the implementation and approach are different for gRPC, the experience of developing and\nconsuming services with gRPC should be intuitive for WCF developers. The underlying goal is the\nsame: make it possible to code as though the client and server are on the same platform, without\nneeding to worry about networking.\n\nBoth platforms share the principle of declaring and then implementing an interface, even though the\nprocess for declaring that interface is different. And as you\u2019ll see in chapter 5, the different types of\nRPC calls that gRPC supports map well to the bindings available to WCF services.", "**Benefits of gRPC**": "gRPC stands above other solutions for the following reasons.\n\n**Performance**\n\nUsing HTTP/2 rather than HTTP/1.1 removes the requirement for human-readable messages and\ninstead uses the smaller, faster binary protocol. This is more efficient for computers to parse. HTTP/2\nalso supports multiplexing requests over a single connection. This support enables responses to be\nsent as soon as they\u2019re ready without the need to wait in a queue. (In HTTP/1.1, this issue is known as\n\u201chead-of-line (HOL) blocking.\u201d) You need fewer resources when using gRPC, which makes it a good\nsolution to use for mobile devices and over slower networks.\n\n**Interoperability**\n\nThere are gRPC tools and libraries for all major programming languages and platforms, including\n.NET, Java, Python, Go, C++, Node.js, Swift, Dart, Ruby, and PHP. Thanks to the Protocol Buffers binary\nwire format and the efficient code generation for each platform, developers can build performant\napps while still enjoying full cross-platform support.\n\n8 CHAPTER 2 | gRPC overview\n\n**Usability and productivity**\n\ngRPC is a comprehensive RPC solution. It works consistently across multiple languages and platforms.\nIt also provides excellent tooling, with much of the necessary boilerplate code automatically\ngenerated. So more developer time is freed up to focus on business logic.\n\n**Streaming**\n\ngRPC has full bidirectional streaming, which provides similar functionality to WCF\u2019s full-duplex\nservices. gRPC streaming can operate over regular internet connections, load balancers, and service\nmeshes.\n\n**Deadline/timeouts and cancellation**\n\ngRPC allows clients to specify a maximum time for an RPC to finish. If the specified deadline is\nexceeded, the server can cancel the operation independently of the client. Deadlines and cancellations\ncan be propagated through further gRPC calls to help enforce resource usage limits. Clients can also\nstop operations when a deadline is exceeded, or earlier if necessary (for example, because of a user\ninteraction).\n\n**Security**\n\ngRPC is implicitly secure when it\u2019s using HTTP/2 over a TLS end-to-end encrypted connection. Support\nfor client certificate authentication (see chapter 6) further increases security and trust between client\nand server.", "**Comparison with CoreWCF**": "[A notable alternative to gRPC for replacing WCF services when migrating to .NET is CoreWCF. Both](https://github.com/corewcf/corewcf)\ngRPC and CoreWCF are Microsoft endorsed paths forward for WCF applications and each comes with\nits own benefits and drawbacks.\n\nCoreWCF is a community-owned .NET Foundation project supported by Microsoft that implements\nWCF server APIs for .NET. CoreWCF is an effort to allow existing WCF services to work with minimal\nchanges on .NET. Your Data Contracts for WCF are unchanged with CoreWCF, and it supports many of\nthe bindings and APIs from WCF. The main differences are around the patterns for starting WCF\nservices, and not all configuration options are available (some configuration must now be done in\ncode).\n\nServices and interfaces can often migrate with few changes. Because of this, a key benefit of CoreWCF\nis its very high compatibility with WCF. Where changes have been made, they are to adapt to the\nprogramming style of modern .NET, for example hosting now through ASP.NET Core, and APIs now\nuse the Task based async patterns usable with await rather than the older BeginXXX / EndXXX pattern.\n\nOn the other hand, gRPC is a modern remote communication solution with a number of features, as\ndiscussed previously. Benefits of using gRPC include better interoperability across languages, its\nrelatively simple modern API, and a broad community ecosystem.\n\nWhen deciding whether to use CoreWCF or gRPC to migrate a WCF application to .NET, CoreWCF is\ntypically a better fit if the goal is to migrate the application with minimal changes whereas gRPC may\n\n9 CHAPTER 2 | gRPC overview\n\nbe a better fit if the goal is to modernize the application while retargeting to .NET. The remainder of\nthis guide focuses on that modernization with gRPC.\n\n10 CHAPTER 2 | gRPC overview\n\n**CHAPTER**"}}}, "3": {"Protocol buffers": {"How Protobuf works": "Most .NET object serialization techniques, including WCF\u2019s data contracts, work by using reflection to\nanalyze the object structure at run time. By contrast, most Protobuf libraries require you to define the\nstructure up front by using a dedicated language ( _Protocol Buffer Language_ ) in a .proto file. A compiler\nthen uses this file to generate code for any of the supported platforms. Supported platforms include\n.NET, Java, C/C++, JavaScript, and many more.\n\nThe Protobuf compiler, protoc, is maintained by Google, although alternative implementations are\navailable. The generated code is efficient and optimized for fast serialization and deserialization of\ndata.\n\nThe Protobuf wire format is a binary encoding. It uses some clever tricks to minimize the number of\nbytes used to represent messages. Knowledge of the binary encoding format isn\u2019t necessary to use\n[Protobuf. But if you\u2019re interested, you can learn more about it on the Protocol Buffers website.](https://developers.google.com/protocol-buffers/docs/encoding)", "Protobuf messages": {"**Declaring a message**": "In Windows Communication Foundation (WCF), a Stock class for a stock market trading application\nmight be defined like the following example:\n\n11 CHAPTER 3 | Protocol buffers\n\nTo implement the equivalent class in Protobuf, you must declare it in the .proto file. The protoc\ncompiler will then generate the .NET class as part of the build process.\n\nThe first line declares the syntax version being used. Version 3 of the language was released in 2016.\nIt\u2019s the version that we recommend for gRPC services.\n\nThe option csharp_namespace line specifies the namespace to be used for the generated C# types.\nThis option will be ignored when the .proto file is compiled for other languages. Protobuf files often\ncontain language-specific options for several languages.\n\nThe Stock message definition specifies four fields. Each has a type, a name, and a field number.", "**Field numbers**": "Field numbers are an important part of Protobuf. They\u2019re used to identify fields in the binary encoded\ndata, which means they can\u2019t change from version to version of your service. The advantage is that\nbackward compatibility and forward compatibility are possible. Clients and services will ignore field\nnumbers that they don\u2019t know about, as long as the possibility of missing values is handled.\n\nIn the binary format, the field number is combined with a type identifier. Field numbers from 1 to 15\ncan be encoded with their type as a single byte. Numbers from 16 to 2,047 take 2 bytes. You can go\nhigher if you need more than 2,047 fields on a message for any reason. The single-byte identifiers for\nfield numbers 1 to 15 offer better performance, so you should use them for the most basic, frequently\nused fields.\n\n12 CHAPTER 3 | Protocol buffers", "**Types**": "The type declarations are using Protobuf\u2019s native scalar data types, which are discussed in more detail\nin the next section. The rest of this chapter will cover Protobuf\u2019s built-in types and show how they\nrelate to common .NET types.", "**The generated code**": "When you build your application, Protobuf creates classes for each of your messages, mapping its\nnative types to C# types. The generated Stock type would have the following signature:\n\nThe actual code that\u2019s generated is far more complicated than this. The reason is that each class\ncontains all the code necessary to serialize and deserialize itself to the binary wire format.\n\n**Property names**\n\nNote that the Protobuf compiler applied PascalCase to the property names, although they were\n[snake_case in the .proto file. The Protobuf style guide](https://developers.google.com/protocol-buffers/docs/style) recommends using snake_case in your message\ndefinitions so that the code generation for other platforms produces the expected case for their\nconventions."}, "Protobuf scalar data types": {"**Other .NET primitive types**": "**Dates and times**\n\n[The native scalar types don\u2019t provide for date and time values, equivalent to C#\u2019s DateTimeOffset,](https://docs.microsoft.com/dotnet/api/system.datetimeoffset)\n[DateTime, and TimeSpan. You can specify these types by using some of Google\u2019s \u201cWell Known Types\u201d](https://docs.microsoft.com/dotnet/api/system.datetime)\nextensions. These extensions provide code generation and runtime support for complex field types\nacross the supported platforms.\n\nThe following table shows the date and time types:\n\n|C# type|Protobuf well-known type|\n|---|---|\n|DateTimeOffset|google.protobuf.Timestamp|\n|DateTime|google.protobuf.Timestamp|\n|TimeSpan|google.protobuf.Duration|\n\n14 CHAPTER 3 | Protocol buffers\n\nThe generated properties in the C# class aren\u2019t the .NET date and time types. The properties use the\nTimestamp and Duration classes in the Google.Protobuf.WellKnownTypes namespace. These classes\nprovide methods for converting to and from DateTimeOffset, DateTime, and TimeSpan.\n\n**System.Guid**\n\n[Protobuf doesn\u2019t directly support the Guid](https://docs.microsoft.com/dotnet/api/system.guid) type, known as UUID on other platforms. There\u2019s no wellknown type for it.\n\nThe best approach is to handle Guid values as a string field, by using the standard 8-4-4-4-12\nhexadecimal format (for example, 45a9fda3-bd01-47a9-8460-c1cd7484b0b3). All languages and\nplatforms can parse that format.\n\nDon\u2019t use a bytes field for Guid values. Problems with _endianness_ [(Wikipedia definition) can result in](https://en.wikipedia.org/wiki/Endianness)\nerratic behavior when Protobuf is interacting with other platforms, such as Java.\n\n**Nullable types**\n\nThe Protobuf code generation for C# uses the native types, such as int for int32. So the values are\nalways included and can\u2019t be null.\n\nFor values that require explicit null, such as using int? in your C# code, Protobuf\u2019s \u201cWell Known Types\u201d\ninclude wrappers that are compiled to nullable C# types. To use them, import wrappers.proto into\nyour .proto file, like this:\n\n15 CHAPTER 3 | Protocol buffers\n\nProtobuf will use the simple T? (for example, int?) for the generated message property.\n\nThe following table shows the complete list of wrapper types with their equivalent C# type:\n\n|C# type|Well Known Type wrapper|\n|---|---|\n|double?|google.protobuf.DoubleValue|\n|float?|google.protobuf.FloatValue|\n|int?|google.protobuf.Int32Value|\n|long?|google.protobuf.Int64Value|\n|uint?|google.protobuf.UInt32Value|\n|ulong?|google.protobuf.UInt64Value|\n\nThe well-known types Timestamp and Duration are represented in .NET as classes. In C# 8 and\nbeyond, you can use nullable reference types. But it\u2019s important to check for null on properties of\nthose types when you\u2019re converting to DateTimeOffset or TimeSpan.", "**Decimals**": "Protobuf doesn\u2019t natively support the .NET decimal type, just double and float. There\u2019s an ongoing\ndiscussion in the Protobuf project about the possibility of adding a standard Decimal type to the wellknown types, with platform support for languages and frameworks that support it. Nothing has been\nimplemented yet.\n\nIt\u2019s possible to create a message definition to represent the decimal type that would work for safe\nserialization between .NET clients and servers. But developers on other platforms would have to\nunderstand the format being used and implement their own handling for it.\n\n**Creating a custom decimal type for Protobuf**\n\nA simple implementation might be similar to the nonstandard Money type that some Google APIs\nuse, without the currency field.\n\n16 CHAPTER 3 | Protocol buffers\n\nThe nanos field represents values from 0.999_999_999 to -0.999_999_999. For example, the decimal\nvalue 1.5m would be represented as { units = 1, nanos = 500_000_000 }. This is why the nanos field in\nthis example uses the sfixed32 type, which encodes more efficiently than int32 for larger values. If the\nunits field is negative, the nanos field should also be negative.\n\nConversion between this type and the BCL decimal type might be implemented in C# like this:"}, "Protobuf nested types": "Just as C# allows you to declare classes inside other classes, Protocol Buffer (Protobuf) allows you to\nnest message definitions within other messages. The following example shows how to create nested\nmessage types:\n\n17 CHAPTER 3 | Protocol buffers\n\nIn the generated C# code, the Inner type will be declared in a nested static Types class within the\nHelloRequest class:\n\n```\nvar inner = new Outer.Types.Inner { Text = \"Hello\" };\n\n### Repeated fields for lists and arrays\n\n```\n\nYou specify lists in Protocol Buffer (Protobuf) by using the repeated prefix keyword. The following\nexample shows how to create a list:\n\nIn the generated code, repeated fields are represented by read-only properties of the\n[Google.Protobuf.Collections.RepeatedField<T> type rather than any of the built-in .NET collection](https://developers.google.cn/protocol-buffers/docs/reference/csharp/class/google/protobuf/collections/repeated-field-t-)\n[types. This type implements all the standard .NET collection interfaces, such as IList](https://docs.microsoft.com/dotnet/api/system.collections.generic.ilist-1) [and IEnumerable.](https://docs.microsoft.com/dotnet/api/system.collections.generic.ienumerable-1)\nSo you can use LINQ queries or convert it to an array or a list easily.\n\nThe RepeatedField<T> type includes the code required to serialize and deserialize the list to the\nbinary wire format.", "Protobuf reserved fields": "The backward-compatibility guarantees in Protocol Buffer (Protobuf) rely on field numbers always\nrepresenting the same data item. If a field is removed from a message in a new version of the service,\nthat field number should never be reused. You can enforce this behavior by using the reserved\nkeyword.\n\nIf the displayName and marketId fields were removed from the Stock message defined earlier, their\nfield numbers should be reserved as in the following example.\n\nYou can also use the reserved keyword as a placeholder for fields that might be added in the future.\nYou can express contiguous field numbers as a range by using the to keyword.\n\n18 CHAPTER 3 | Protocol buffers", "Protobuf Any and Oneof fields for variant types": {"**Any**": "Any is one of Protobuf\u2019s \u201cwell-known types\u201d: a collection of useful, reusable message types with\nimplementations in all supported languages. To use the Any type, you must import the\ngoogle/protobuf/any.proto definition.\n\nIn the C# code, the Any class provides methods for setting the field, extracting the message, and\nchecking the type.\n\n19 CHAPTER 3 | Protocol buffers\n\nProtobuf\u2019s internal reflection code uses the Descriptor static field on each generated type to resolve\nAny field types. There\u2019s also a TryUnpack<T> method, but that creates an uninitialized instance of T\neven when it fails. It\u2019s better to use the Is method as shown earlier.", "**Oneof**": "Oneof fields are a language feature: the compiler handles the oneof keyword when it generates the\nmessage class. Using oneof to specify the ChangeNotification message might look like this:\n\nFields within the oneof set must have unique field numbers in the overall message declaration.\n\nWhen you use oneof, the generated C# code includes an enum that specifies which of the fields has\nbeen set. You can test the enum to find which field is set. Fields that aren\u2019t set return null or the\ndefault value, rather than throwing an exception.\n\nSetting any field that\u2019s part of a oneof set will automatically clear any other fields in the set. You can\u2019t\nuse repeated with oneof. Instead, you can create a nested message with either the repeated field or\nthe oneof set to work around this limitation.\n\n20 CHAPTER 3 | Protocol buffers"}, "Protobuf enumerations": "Protobuf supports enumeration types. You saw this support in the previous section, where an enum\nwas used to determine the type of a Oneof field. You can define your own enumeration types, and\nProtobuf will compile them to C# enum types.\n\nBecause you can use Protobuf with various languages, the naming conventions for enumerations are\ndifferent from the C# conventions. However, the code generator converts the names to the traditional\nC# case. If the Pascal-case equivalent of the field name starts with the enumeration name, then it\u2019s\nremoved.\n\nFor example, in the following Protobuf enumeration, the fields are prefixed with ACCOUNT_STATUS.\nThis prefix is equivalent to the Pascal-case enum name, AccountStatus.\n\nThe generator creates a C# enum equivalent to the following code:\n\nProtobuf enumeration definitions _must_ have a zero constant as their first field. As in C#, you can\ndeclare multiple fields with the same value. But you must explicitly enable this option by using the\nallow_alias option in the enum:\n\nYou can declare enumerations at the top level in a .proto file, or nested within a message definition.\nNested enumerations\u2014like nested messages\u2014will be declared within the .Types static class in the\ngenerated message class.\n\n[There\u2019s no way to apply the [Flags]](https://docs.microsoft.com/dotnet/api/system.flagsattribute) attribute to a Protobuf-generated enum, and Protobuf doesn\u2019t\nunderstand bitwise enum combinations. Look at the following example:\n\n21 CHAPTER 3 | Protocol buffers\n\nIf you set product.AvailableIn to Region.NorthAmerica | Region.SouthAmerica, it\u2019s serialized as the\ninteger value 3. When a client or server tries to deserialize the value, it won\u2019t find a match in the enum\ndefinition for 3. The result will be Region.None.\n\nThe best way to work with multiple enum values in Protobuf is to use a repeated field of the enum\ntype.", "Protobuf maps for dictionaries": {"**Using MapField properties in code**": "The MapField properties generated from map fields are read-only, and will never be null. To set a map\nproperty, use the Add(IDictionary<TKey,TValue> values) method on the empty MapField property to\ncopy values from any .NET dictionary.\n\n22 CHAPTER 3 | Protocol buffers", "**Further reading**": "[For more information about Protobuf, see the official Protobuf documentation.](https://developers.google.com/protocol-buffers/docs/overview)\n\n23 CHAPTER 3 | Protocol buffers\n\n**CHAPTER**"}}}, "4": {"Comparing WCF to gRPC": {"gRPC example": "When you create a new ASP.NET Core 7.0 gRPC project from Visual Studio 2022 or the command line,\nthe gRPC equivalent of \u201cHello World\u201d is generated for you. It consists of a greeter.proto file that\ndefines the service and its messages, and a GreeterService.cs file with an implementation of the\nservice.\n\n24 CHAPTER 4 | Comparing WCF to gRPC\n\nThis chapter will refer to this example code when explaining different concepts and features of gRPC.", "WCF endpoints and gRPC methods": {"**OperationContract properties**": "[The OperationContract](https://docs.microsoft.com/dotnet/api/system.servicemodel.operationcontractattribute) attribute has properties to control or refine how it works. gRPC methods don\u2019t\noffer this type of control. The following table lists those OperationContract properties and describes\nhow the functionality that they specify is (or isn\u2019t) dealt with in gRPC:\n\n25 CHAPTER 4 | Comparing WCF to gRPC\n\n|OperationContract property|gRPC|\n|---|---|\n|Action|A URI identifies the operation. gRPC uses the name of<br>package, service, and rpc from the .proto file.|\n|AsyncPattern|All gRPC service methods return Task objects.|\n|IsInitiating|See the paragraph after this table.|\n|IsOneWay|One-way gRPC methods return Empty results or use client<br>streaming.|\n|IsTerminating|See the paragraph after this table.|\n|Name|This property is SOAP related and has no meaning in gRPC.|\n|ProtectionLevel|There\u2019s no message encryption. Network encryption is<br>handled at the transport layer (TLS over HTTP/2).|\n|ReplyAction|This property is SOAP related and has no meaning in gRPC.|\n\n[The IsInitiating property lets you indicate that a method within ServiceContract](https://docs.microsoft.com/dotnet/api/system.servicemodel.servicecontractattribute) can\u2019t be the first\nmethod called as part of a session. The IsTerminating property causes the server to close the session\nafter an operation is called (or the client, if the property is used on a callback client). In gRPC, streams\nare created by single methods and closed explicitly. See gRPC streaming.\n\nFor more information on gRPC security and encryption, see chapter 6."}, "WCF bindings and transports": {"**NetTCP**": "WCF\u2019s NetTCP binding allows for persistent connections, small messages, and two-way messaging.\nBut it works only between .NET clients and servers. gRPC allows the same functionality but is\nsupported across multiple programming languages and platforms.\n\ngRPC has many features of WCF\u2019s NetTCP binding, but they\u2019re not always implemented in the same\nway. For example, in WCF, encryption is controlled through configuration and handled in the\nframework. In gRPC, encryption is achieved at the connection level through HTTP/2 over TLS.\n\n26 CHAPTER 4 | Comparing WCF to gRPC", "**HTTP**": "The WCF binding called BasicHttpBinding is usually text-based and uses SOAP as the wire format. It\u2019s\nslow compared to the NetTCP binding. It\u2019s used to provide cross-platform interoperability, or\nconnection over internet infrastructure.\n\nThe equivalent in gRPC uses HTTP/2 as the underlying transport layer with the binary Protobuf wire\nformat for messages. So it can offer performance at the NetTCP service level and full cross-platform\ninteroperability with all modern programming languages and frameworks.", "**Named pipes**": "WCF provided a _named pipes_ binding for communication between processes on the same physical\nmachine. ASP.NET Core gRPC doesn\u2019t support named pipes. For inter-process communication (IPC)\nusing gRPC instead supports Unix domain sockets. Unix domain sockets are supported on Linux and\n[modern versions of Windows.](https://devblogs.microsoft.com/commandline/af_unix-comes-to-windows/)\n\n[For more information, see Inter-process communication with gRPC.](https://docs.microsoft.com/aspnet/core/grpc/interprocess)", "**MSMQ**": "MSMQ is a proprietary Windows message queue. WCF\u2019s binding to MSMQ enables \u201cfire and forget\u201d\nrequests from clients that might be processed at any time in the future. gRPC doesn\u2019t natively provide\nany message queue functionality.\n\nThe best alternative is to directly use a messaging system like Azure Service Bus, RabbitMQ, or Kafka.\nYou can implement this functionality with the client placing messages directly onto the queue, or a\ngRPC client streaming service that enqueues the messages.", "**WebHttpBinding**": "WebHttpBinding (also known as WCF REST), with the WebGet and WebInvoke attributes, enabled you\nto develop RESTful APIs that could speak JSON at a time when this behavior was less common. If you\nhave a RESTful API built with WCF REST, consider migrating it to a regular ASP.NET Core MVC Web\nAPI application. This migration would provide the same functionality as a conversion to gRPC."}, "Types of RPC": {"**Request/reply**": "For simple request/reply methods that take and return small amounts of data, use the simplest gRPC\npattern, the unary RPC.\n\nAs you can see, implementing a gRPC unary RPC service method is similar to implementing a WCF\noperation. The difference is that with gRPC, you override a base class method instead of\n[implementing an interface. On the server, gRPC base methods always return Task, although the client](https://docs.microsoft.com/dotnet/api/system.threading.tasks.task-1)\nprovides both async and blocking methods to call the service.", "**WCF duplex, one way to client**": "WCF applications (with certain bindings) can create a persistent connection between client and server.\nThe server can asynchronously send data to the client until the connection is closed, by using a\n_callback interface_ [specified in the ServiceContractAttribute.CallbackContract](https://docs.microsoft.com/dotnet/api/system.servicemodel.servicecontractattribute.callbackcontract) property.\n\ngRPC services provide similar functionality with message streams. Streams don\u2019t map _exactly_ to WCF\nduplex services in terms of implementation, but you can achieve the same results.\n\n28 CHAPTER 4 | Comparing WCF to gRPC\n\n**gRPC streaming**\n\ngRPC supports the creation of persistent streams from client to server, and from server to client. Both\ntypes of stream can be active concurrently. This ability is called bidirectional streaming.\n\nYou can use streams for arbitrary, asynchronous messaging over time. Or you can use them for\npassing large datasets that are too big to generate and send in a single request or response.\n\nThe following example shows a server-streaming RPC.\n\nThis server stream can be consumed from a client application, as shown in the following code:\n\n29 CHAPTER 4 | Comparing WCF to gRPC\n\n**Differences from WCF**\n\nA WCF duplex service uses a client callback interface that can have multiple methods. A gRPC serverstreaming service can only send messages over a single stream. If you need multiple methods, use a\nmessage type with either an Any field or a oneof field to send different messages, and write code in\nthe client to handle them.\n\n[In WCF, the ServiceContract class with the session is kept alive until the connection is closed. Multiple](https://docs.microsoft.com/dotnet/api/system.servicemodel.servicecontractattribute)\nmethods can be called within the session. In gRPC, the Task that the implementation method returns\nshouldn\u2019t finish until the connection is closed.", "**WCF one-way operations and gRPC client streaming**": "WCF provides one-way operations (marked with [OperationContract(IsOneWay = true)]) that return a\ntransport-specific acknowledgment. gRPC service methods always return a response, even if it\u2019s\nempty. The client should always await that response. For the \u201cfire-and-forget\u201d style of messaging in\ngRPC, you can create a client streaming service.\n\n**thing_log.proto**\n\n**ThingLogService.cs**\n\n**ThingLog client example**\n\n30 CHAPTER 4 | Comparing WCF to gRPC\n\nYou can use client-streaming RPCs for fire-and-forget messaging, as shown in the previous example.\nYou can also use them for sending very large datasets to the server. The same warning about\nperformance applies: for smaller datasets, use repeated fields in regular messages.", "**WCF full-duplex services**": "WCF duplex binding supports multiple one-way operations on both the service interface and the\nclient callback interface. This support allows ongoing conversations between client and server. gRPC\nsupports something similar with bidirectional streaming RPCs, where both parameters are marked\nwith the stream modifier.\n\n**chat.proto**\n\n**ChatterService.cs**\n\n31 CHAPTER 4 | Comparing WCF to gRPC\n\nIn the previous example, you can see that the implementation method receives both a request stream\n(IAsyncStreamReader<MessageRequest>) and a response stream\n(IServerStreamWriter<MessageResponse>). The method can read and write messages until the\nconnection is closed.\n\n**Chatter client**"}, "Metadata": "_Metadata_ refers to additional data that might be useful during the processing of requests and\nresponses but that\u2019s not part of the actual application data. Metadata might include authentication\ntokens, request identifiers and tags for monitoring purposes, and information about the data, like the\nnumber of records in a dataset.\n\n32 CHAPTER 4 | Comparing WCF to gRPC\n\nIt\u2019s possible to add generic key/value headers to Windows Communication Foundation (WCF)\n[messages by using an OperationContextScope and the OperationContext.OutgoingMessageHeaders](https://docs.microsoft.com/dotnet/api/system.servicemodel.operationcontextscope)\n[property and handle them by using MessageProperties.](https://docs.microsoft.com/dotnet/api/system.servicemodel.channels.messageproperties)\n\ngRPC calls and responses can also include metadata that\u2019s similar to HTTP headers. This metadata is\nmostly invisible to gRPC itself and is passed through to be processed by your application code or\nmiddleware. Metadata is represented as key/value pairs, where the key is a string and the value is\neither a string or binary data. You don\u2019t need to specify metadata in the .proto file.\n\n[Metadata is handled by the Metadata class of the Grpc.Core.Api](https://www.nuget.org/packages/Grpc.Core.Api/) NuGet package. This class can be\nused with collection initializer syntax.\n\nThis example shows how to add metadata to a call from a C# client:\n\ngRPC services can access metadata from the ServerCallContext argument\u2019s RequestHeaders property:\n\nServices can send metadata to clients by using the ResponseTrailers property of ServerCallContext:", "Error handling": {"**Raise errors in ASP.NET Core gRPC**": "An ASP.NET Core gRPC service can send an error response by throwing an RpcException, which can be\ncaught by the client as if it were in the same process. The RpcException must include a status code\nand description, and can optionally include metadata and a longer exception message. The metadata\ncan be used to send supporting data, similar to how FaultContract objects can carry additional data\nfor WCF errors.", "**Catch errors in gRPC clients**": "[Just like WCF clients can catch FaultException](https://docs.microsoft.com/dotnet/api/system.servicemodel.faultexception-1) errors, a gRPC client can catch an RpcException to\nhandle errors. Because RpcException isn\u2019t a generic type, you can\u2019t catch different error types in\ndifferent blocks. But you can use C#\u2019s _exception filters_ feature to declare separate catch blocks for\ndifferent status codes, as shown in the following example:\n\n34 CHAPTER 4 | Comparing WCF to gRPC", "**gRPC richer error model**": "[Google has developed a richer error model that\u2019s more like WCF\u2019s FaultContract, but this model isn\u2019t](https://cloud.google.com/apis/design/errors#error_model)\nsupported in C# yet. Currently, it\u2019s only available for Go, Java, Python, and C++."}, "WS-* protocols": {"**Metadata exchange: WS-Policy, WS-Discovery, and so on**": "SOAP services expose Web Services Description Language (WSDL) schema documents with\ninformation such as data formats, operations, or communication options. You can use this schema to\ngenerate the client code.\n\ngRPC works best when servers and clients are generated from the same .proto files, but a Server\nReflection optional extension does provide a way to expose dynamic information from a running\n[server. For more information, see the Grpc.Reflection](https://nuget.org/packages/Grpc.Reflection) NuGet package.\n\nThe WS-Discovery protocol is used to locate services on a local network. gRPC services are located\nthrough DNS or a service registry such as Consul or ZooKeeper.", "**Security: WS-Security, WS-Federation, XML Encryption, and so on**": "Security, authentication, and authorization are covered in much more detail in chapter 6. But it\u2019s worth\nnoting here that, unlike WCF, gRPC doesn\u2019t support WS-Security, WS-Federation, or XML Encryption.\nEven so, gRPC provides excellent security. All gRPC network traffic is automatically encrypted when it\u2019s\nusing HTTP/2 over TLS. You can use X509 certificates for mutual client/server authentication.\n\n35 CHAPTER 4 | Comparing WCF to gRPC", "**WS-ReliableMessaging**": "gRPC does not provide an equivalent to WS-ReliableMessaging. Retry semantics should be handled in\n[code, possibly with a library like Polly. When you\u2019re running in Kubernetes or similar orchestration](https://github.com/App-vNext/Polly)\nenvironments, service meshes can also help to provide reliable messaging between services.", "**WS-Transaction, WS-Coordination**": "WCF\u2019s implementation of distributed transactions uses Microsoft Distributed Transaction Coordinator\n(MSDTC). It works with resource managers that specifically support it, like SQL Server, MSMQ, or\nWindows file systems. There\u2019s no equivalent yet in the modern microservices world, in part due to the\nwider range of technologies in use. For a discussion of transactions, see Appendix A.\n\n36 CHAPTER 4 | Comparing WCF to gRPC\n\n**CHAPTER**"}}}, "5": {"Migrate a WCF solution to gRPC": {"Create a new ASP.NET Core gRPC project": {"**Create the project by using Visual Studio**": "37 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nCreate an empty solution called **TraderSys** from the _Blank Solution_ template. Add a solution folder\ncalled src. Then, right-click on the folder and choose **Add** - **New Project** . Enter grpc in the template\nsearch box, and you should see a project template called gRPC Service.\n\nSelect **Next** to continue to the **Configure your new project** dialog box. Name the project\nTraderSys.Portfolios and add an src subdirectory to the **Location** .\n\n38 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nSelect **Next** to continue to the **Create a new gRPC service** dialog box.\n\n39 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nAt present, you have limited options for the service creation. Docker will be introduced later, so for\nnow, leave that option unselected. Just select **Create** . Your first ASP.NET Core 7.0 gRPC project is\ngenerated and added to the solution. If you don\u2019t want to know about working with the dotnet CLI,\nskip to the Clean up the example code section.", "**Create the project by using the .NET CLI**": "This section covers the creation of solutions and projects from the command line.\n\nCreate the solution as shown in the following command. The -o (or --output) flag specifies the output\ndirectory, which is created in the current directory if it doesn\u2019t already exist. The solution has the same\nname as the directory: TraderSys.sln. You can provide a different name by using the -n (or --name)\nflag.\n\nASP.NET Core 7.0 comes with a CLI template for gRPC services. Create the new project by using this\ntemplate, putting it into an src subdirectory as is conventional for ASP.NET Core projects. The project\nis named after the directory (TraderSys.Portfolios.csproj), unless you specify a different name with the\n-n flag.\n\n```\ndotnet new grpc -o src/TraderSys.Portfolios\n\n```\n\nFinally, add the project to the solution by using the dotnet sln command:\n\n```\ndotnet sln add src/TraderSys.Portfolios\n\n```\n\n40 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nYou can now open this solution in Visual Studio 2022, Visual Studio Code, or whatever editor you\nprefer.", "**Clean up the example code**": "You\u2019ve now created an example service by using the gRPC template, which was reviewed earlier in the\nbook. This code isn\u2019t useful in our stock trading context, so we\u2019ll edit things for our first project.\n\n**Rename and edit the proto file**\n\nGo ahead and rename the Protos/greet.proto file to Protos/portfolios.proto, and open it in your\neditor. Delete everything after the package line. Then change the option csharp_namespace, package\nand service names, and remove the default SayHello service. The code now looks like the following:\n\nIf you rename the greet.proto file in an integrated development environment (IDE) like Visual Studio, a\nreference to this file is automatically updated in the .csproj file. But in some other editor, such as\nVisual Studio Code, this reference isn\u2019t updated automatically, so you need to edit the project file\nmanually.\n\nIn the gRPC build targets, there\u2019s a Protobuf item element that lets you specify which .proto files\nshould be compiled, and which form of code generation is required (that is, \u201cServer\u201d or \u201cClient\u201d).\n\n**Rename the GreeterService class**\n\nThe GreeterService class is in the Services folder and inherits from Greeter.GreeterBase. Rename it to\nPortfolioService, and change the base class to Portfolios.PortfoliosBase. Delete the override methods.\n\n41 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThere was a reference to the GreeterService class in the _Program.cs_ . If you used refactoring to rename\nthe class, this reference should have been updated automatically. However, if you didn\u2019t, you need to\nedit it manually.\n\nIn the next section, we\u2019ll add functionality to this new service."}, "Migrate a WCF request-reply service to a gRPC unary RPC": {"**The WCF solution**": "[The PortfoliosSample solution includes a simple request-reply Portfolio service to download either a](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/PortfoliosSample/wcf/TraderSys)\nsingle portfolio or all portfolios for a given trader. The service is defined in the interface\nIPortfolioService with a ServiceContract attribute:\n\n42 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n[The Portfolio model is a simple C# class marked with DataContract](https://docs.microsoft.com/dotnet/api/system.runtime.serialization.datacontractattribute) and including a list of\nPortfolioItem objects. These models are defined in the TraderSys.PortfolioData project along with a\nrepository class that represents a data access abstraction.\n\nThe ServiceContract implementation uses a repository class provided via dependency injection that\nreturns instances of the DataContract types:\n\n43 CHAPTER 5 | Migrate a WCF solution to gRPC", "**The portfolios.proto file**": "If you followed the instructions in the previous section, you should have a gRPC project with a\nportfolios.proto file that looks like this:\n\nThe first step is to migrate the DataContract classes to their Protobuf equivalents.", "**Convert the DataContract classes to gRPC messages**": "The PortfolioItem class will be converted to a Protobuf message first, because the Portfolio class\ndepends on it. The class is simple, and three of the properties map directly to gRPC data types. The\nCost property, which represents the price paid for the shares at purchase, is a decimal field. gRPC\nsupports only float or double for real numbers, which aren\u2019t suitable for currency. Because share\nprices vary by a minimum of one cent, the cost can be expressed as an int32 of cents.\n\nThe Portfolio class is a little more complicated. In the WCF code, the developer used a Guid for the\nTraderId property, and contains a List<PortfolioItem>. In Protobuf, which doesn\u2019t have a first-class\nUUID type, you should use a string for the traderId field and parse it in your own code. For the list of\nitems, use the repeated keyword on the field.\n\nNow that you have the data messages, you can declare the service RPC endpoints.\n\n44 CHAPTER 5 | Migrate a WCF solution to gRPC", "**Convert ServiceContract to a gRPC service**": "The WCF Get method takes two parameters: Guid traderId and int portfolioId. gRPC service methods\ncan take only a single parameter, so you need to create a message to hold the two values. It\u2019s\ncommon practice to name these request objects with the same name as the method followed by the\nsuffix Request. Again, string is being used for the traderId field instead of Guid.\n\nThe service could just return a Portfolio message directly, but again, this could affect backward\ncompatibility in the future. It\u2019s a good practice to define separate Request and Response messages for\nevery method in a service, even if many of them are the same right now. So declare a GetResponse\nmessage with a single Portfolio field.\n\nThis example shows the declaration of the gRPC service method with the GetRequest message:\n\nThe WCF GetAll method takes only a single parameter, traderId, so it might seem that you could\nspecify string as the parameter type. But gRPC requires a defined message type. This requirement\nhelps to enforce the practice of using custom messages for all inputs and outputs, for future backward\ncompatibility.\n\nThe WCF method also returns a List<Portfolio>, but for the same reason it doesn\u2019t allow simple\nparameter types, gRPC won\u2019t allow repeated Portfolio as a return type. Instead, create a\nGetAllResponse type to wrap the list.\n\n45 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nIf you save your project with these changes, the gRPC build target will run in the background and\ngenerate all the Protobuf message types and a base class that you can inherit to implement the\nservice.\n\nOpen the Services/GreeterService.cs class and delete the example code. Now you can add the\nPortfolio service implementation. The generated base class will be in the Protos namespace and is\ngenerated as a nested class. gRPC creates a static class with the same name as the service in the\n.proto file and a base class with the suffix Base inside that static class, so the full identifier for the base\ntype is TraderSys.Portfolios.Protos.Portfolios.PortfoliosBase.\n\nThe base class declares virtual methods for Get and GetAll that can be overridden to implement the\nservice. The methods are virtual rather than abstract so that if you don\u2019t implement them, the service\ncan return an explicit gRPC Unimplemented status code, much like you might throw a\nNotImplementedException in regular C# code.\n\nThe signature for all gRPC unary service methods in ASP.NET Core is consistent. There are two\nparameters: the first is the message type declared in the .proto file, and the second is a\nServerCallContext that works similarly to the HttpContext from ASP.NET Core. In fact, there\u2019s an\nextension method called GetHttpContext on the ServerCallContext class that you can use to get the\nunderlying HttpContext, although you shouldn\u2019t need to use it often. We\u2019ll take a look at\nServerCallContext later in this chapter, and also in the chapter that discusses authentication.\n\nThe method\u2019s return type is a Task<T>, where T is the response message type. All gRPC service\nmethods are asynchronous.", "**Migrate the PortfolioData library to .NET**": "At this point, the project needs the Portfolio repository and models contained in the\nTraderSys.PortfolioData class library in the WCF solution. The easiest way to bring them across is to\ncreate a new class library by using either the Visual Studio **New project** dialog box with the Class\nLibrary (.NET Standard) template, or from the command line by using the .NET CLI, running these\ncommands from the directory that contains the TraderSys.sln file:\n\nAfter you\u2019ve created the library and added it to the solution, delete the generated Class1.cs file and\ncopy the files from the WCF solution\u2019s library into the new class library\u2019s folder, keeping the folder\nstructure:\n\n46 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nSDK-style .NET projects automatically include any .cs files in or under their own directory, so you don\u2019t\nneed to explicitly add them to the project. The only step remaining is to remove the DataContract and\nDataMember attributes from the Portfolio and PortfolioItem classes so they\u2019re plain old C# classes:", "**Use ASP.NET Core dependency injection**": "Now you can add a reference to this library to the gRPC application project and consume the\nPortfolioRepository class by using dependency injection in the gRPC service implementation. In the\nWCF application, dependency injection was provided by the Autofac IoC container. ASP.NET Core has\ndependency injection baked in. You can register the repository in the _Program.cs_ itself:\n\nThe IPortfolioRepository implementation can now be specified as a constructor parameter in the\nPortfolioService class, as follows:\n\n```\npublic class PortfolioService : Protos.Portfolios.PortfoliosBase\n{\n  private readonly IPortfolioRepository _repository;\n\n  public PortfolioService(IPortfolioRepository repository)\n  {\n\n```\n\n47 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n```\n_repository = repository;\n  }\n}\n\n#### **Implement the gRPC service**\n\n```\n\nNow that you\u2019ve declared your messages and your service in the portfolios.proto file, you have to\nimplement the service methods in the PortfolioService class that inherits from the gRPC-generated\nPortfolios.PortfoliosBase class. The methods are declared as virtual in the base class. If you don\u2019t\noverride them, they\u2019ll return a gRPC \u201cNot Implemented\u201d status code by default.\n\nStart by implementing the Get method. The default override looks like this example:\n\nThe first problem is that request.TraderId is a string, and the service requires a Guid. Even though the\nexpected format for the string is UUID, the code has to deal with the possibility that a caller has sent\nan invalid value and respond appropriately. The service can respond with errors by throwing an\nRpcException and use the standard InvalidArgument status code to express the problem:\n\nAfter there\u2019s a proper Guid value for traderId, you can use the repository to retrieve the Portfolio and\nreturn it to the client:\n\n**Map internal models to gRPC messages**\n\nThe previous code doesn\u2019t actually work because the repository is returning its own POCO model\nPortfolio, but gRPC needs its own Protobuf message Portfolio. As when you map Entity Framework\ntypes to data transfer types, the best solution is to provide a conversion between the two. A good\nplace to put the code for this conversion is in the Protobuf-generated class, which is declared as a\npartial class so it can be extended:\n\n48 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nNow that you have the conversion code in place, you can complete the Get method implementation:\n\n49 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThe implementation of the GetAll method is similar. Note that the repeated fields on Protobuf\nmessages are generated as readonly properties of type RepeatedField<T>, so you have to add items\nto them by using the AddRange method, like in this example:\n\nHaving successfully migrated the WCF request-reply service to gRPC, let\u2019s look at creating a client for\nit from the .proto file.", "**Generate client code**": "Create a .NET Standard class library in the same solution to contain the client. This is primarily an\nexample of creating client code, but you could package such a library by using NuGet and distribute it\non an internal repository for other .NET teams to consume. Go ahead and add a new .NET Standard\nclass library called TraderSys.Portfolios.Client to the solution and delete the Class1.cs file.\n\nIn Visual Studio 2022, you can add references to gRPC services in a way that\u2019s similar to how you\u2019d\nadd service references to WCF projects in earlier versions of Visual Studio. Service references and\nconnected services are all managed under the same UI now. You can access the UI by right-clicking\nthe **Dependencies** node in the TraderSys.Portfolios.Client project in Solution Explorer and selecting\n**Manage Connected Service** . In the tool window that appears, select the **Connected Services** section,\nthen select **Add a service reference** in Service References section, select gRPC and click Next:\n\n50 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nBrowse to the portfolios.proto file in the TraderSys.Portfolios project, leave **Client** under **Select the**\n**type of class to be generated**, and then select **OK** :\n\n51 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nWhen you use the Visual Studio **Add Connected Service** feature, the portfolios.proto file is added to\nthe class library project as a _linked file_ rather than copied, so changes to the file in the service project\nwill automatically be applied in the client project. The <Protobuf> element in the csproj file looks like\nthis:\n\n**Use the Portfolios service from a client application**\n\nThe following code is a brief example of how to use the generated client in a console application. A\nmore detailed exploration of the gRPC client code is at the end of this chapter.\n\n52 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nYou\u2019ve now migrated a basic WCF application to an ASP.NET Core gRPC service and created a client to\nconsume the service from a .NET application. The next section will cover the more involved duplex\nservices."}, "Migrate WCF duplex services to gRPC": {"**Server streaming RPC**": "[In the sample SimpleStockTicker WCF solution, SimpleStockPriceTicker, there\u2019s a duplex service for](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/SimpleStockTickerSample/wcf/SimpleStockTicker)\nwhich the client starts the connection with a list of stock symbols, and the server uses the _callback_\n_interface_ to send updates as they become available. The client implements that interface to respond\nto calls from the server.\n\n**The WCF solution**\n\nThe WCF solution is implemented as a self-hosted Net.TCP server in a .NET Framework 4. _x_ console\napplication.\n\n53 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n**ServiceContract**\n\nThe service has a single method with no return type because it uses the callback interface\nISimpleStockTickerCallback to send data to the client in real time.\n\n**The callback interface**\n\nYou can find the implementations of these interfaces in the solution, along with faked external\ndependencies to provide test data.\n\n**gRPC streaming**\n\nThe gRPC process for handling real-time data is different from the WCF process. A call from client to\nserver can create a persistent stream, which can be monitored for messages that arrive\nasynchronously. Despite the difference, streams can be a more intuitive way of dealing with this data\nand are more relevant in modern programming, which emphasizes LINQ, Reactive Streams, functional\nprogramming, and so on.\n\nThe service definition needs two messages: one for the request and one for the stream. The service\nreturns a stream of the StockTickerUpdate message with the stream keyword in its return declaration.\nWe recommend that you add a Timestamp to the update to show the exact time of the price change.\n\n**simple_stock_ticker.proto**\n\n54 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n**Implement SimpleStockTicker**\n\nReuse the fake StockPriceSubscriber from the WCF project by copying the three classes from the\nTraderSys.StockMarket class library into a new .NET Standard class library in the target solution. To\nbetter follow best practices, add a Factory type to create instances of it, and register the\nIStockPriceSubscriberFactory with the ASP.NET Core dependency injection services.\n\n**The factory implementation**\n\n**Register the factory**\n\nThis class can now be used to implement the gRPC StockTickerService.\n\n55 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n**StockTickerService.cs**\n\nAs you can see, although the declaration in the .proto file says the method returns a stream of\nStockTickerUpdate messages, it actually returns a Task. The job of creating the stream is handled by\nthe generated code and the gRPC runtime libraries, which provide the\nIServerStreamWriter<StockTickerUpdate> response stream, ready to use.\n\nUnlike a WCF duplex service, where the instance of the service class is kept alive while the connection\nis open, the gRPC service uses the returned task to keep the service alive. The task shouldn\u2019t complete\nuntil the connection is closed.\n\n56 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThe service can tell when the client has closed the connection by using the CancellationToken from\nthe ServerCallContext. A simple static method, AwaitCancellation, is used to create a task that\ncompletes when the token is canceled.\n\nIn the Subscribe method, then, get a StockPriceSubscriber and add an event handler that writes to the\nresponse stream. Then wait for the connection to be closed before immediately disposing the\nsubscriber to prevent it from trying to write data to the closed stream.\n\nThe WriteUpdateAsync method has a try/catch block to handle any errors that might happen when a\nmessage is written to the stream. This consideration is important in persistent connections over\nnetworks, which could be broken at any millisecond, whether intentionally or because of a failure\nsomewhere.\n\n**Use StockTickerService from a client application**\n\nFollow the same steps in the previous section to create a shareable client class library from the .proto\nfile. In the sample, there\u2019s a .NET console application that demonstrates how to use the client.\n\n**Example Program.cs**\n\nIn this case, the Subscribe method on the generated client isn\u2019t asynchronous. The stream is created\nand usable right away because its MoveNext method is asynchronous and the first time it\u2019s called it\nwon\u2019t complete until the connection is alive.\n\nThe stream is passed to an asynchronous DisplayAsync method. The application then waits for the\nuser to press a key, and then cancels the DisplayAsync method and waits for the task to complete\nbefore exiting.\n\n57 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n**Consume the stream**\n\nWCF uses callback interfaces to allow the server to call methods directly on the client. gRPC streams\nwork differently. The client iterates over the returned stream and processes messages, just as though\nthey were returned from a local method returning an IEnumerable.\n\nThe IAsyncStreamReader<T> type works much like an IEnumerator<T>. There\u2019s a MoveNext method\nthat returns true as long as there\u2019s more data, and a Current property that returns the latest value. The\nonly difference is that the MoveNext method returns a Task<bool> instead of just a bool. The\nReadAllAsync extension method wraps the stream in a standard C# 8 IAsyncEnumerable that can be\nused with the new await foreach syntax.\n\nAgain, be sure to catch exceptions here because of the possibility of network failure, and because of\n[the OperationCanceledException that will inevitably be thrown because the code is using a](https://docs.microsoft.com/dotnet/api/system.operationcanceledexception)\n[CancellationToken](https://docs.microsoft.com/dotnet/api/system.threading.cancellationtoken) to break the loop. The RpcException type has a lot of useful information about\ngRPC runtime errors, including the StatusCode. For more information, see _Error handling_ in Chapter 4.", "**Bidirectional streaming**": "A WCF full-duplex service allows for asynchronous, real-time messaging in both directions. In the\nserver streaming example, the client starts a request and then receives a stream of updates. A better\n\n58 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nversion of that service would allow the client to add and remove stocks from the list without having to\n[stop and create a new subscription. That functionality has been implemented in the FullStockTicker](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/FullStockTickerSample/wcf/FullStockTicker)\n[sample solution.](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/FullStockTickerSample/wcf/FullStockTicker)\n\nThe IFullStockTickerService interface provides three methods:\n\n['Subscribe starts the connection.', 'AddSymbol adds a stock symbol to watch.', 'RemoveSymbol removes a symbol from the watched list.']\n\nThe callback interface remains the same.\n\nImplementing this pattern in gRPC is less straightforward because there are now two streams of data\nwith messages being passed: one from client to server and another from server to client. It isn\u2019t\npossible to use multiple methods to implement the add and remove operations, but you can pass\nmore than one type of message on a single stream by using either the Any type or the oneof\nkeyword, which were covered in Chapter 3.\n\nIn a case where there\u2019s a specific set of types that are acceptable, oneof is a better way to go. Use an\nActionMessage that can hold either an AddSymbolRequest or a RemoveSymbolRequest:\n\nDeclare a bidirectional streaming service that takes a stream of ActionMessage messages:\n\n59 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThe implementation for this service is similar to that of the previous example, except the first\nparameter of the Subscribe method is now an IAsyncStreamReader<ActionMessage>, which can be\nused to handle the Add and Remove requests:\n\nThe ActionMessage class that gRPC has generated guarantees that only one of the Add and Remove\nproperties can be set. Finding which one isn\u2019t null is a valid way to determine which type of message\nis used, but there\u2019s a better way. The code generation also created an enum ActionOneOfCase in the\nActionMessage class, which looks like this:\n\n60 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThe property ActionCase on the ActionMessage object can be used with a switch statement to\ndetermine which field is set:\n\n**Use FullStockTickerService from a client application**\n\nThere\u2019s a simple .NET WPF application that demonstrates the use of this more complex client. You can\n[find the full application on GitHub.](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/FullStockTickerSample/grpc/FullStockTicker)\n\nThe client is used in the MainWindowViewModel class, which gets an instance of the\nFullStockTicker.FullStockTickerClient type from dependency injection:\n\n61 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThe object returned by the client.Subscribe() method is now an instance of the gRPC library type\nAsyncDuplexStreamingCall<TRequest, TResponse>, which provides a RequestStream for sending\nrequests to the server and a ResponseStream for handling responses.\n\nThe request stream is used from some WPF ICommand methods to add and remove symbols. For\neach operation, set the relevant field on an ActionMessage object:\n\nThe stream of responses is handled in an async method. The Task it returns is held to be disposed\nwhen the window is closed:\n\n62 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n**Client cleanup**\n\nWhen the window is closed and the MainWindowViewModel is disposed (from the Closed event of\nMainWindow), we recommend that you properly dispose the AsyncDuplexStreamingCall object. In\nparticular, the CompleteAsync method on the RequestStream should be called to gracefully close the\nstream on the server. This example shows the DisposeAsync method from the sample view-model:\n\nClosing request streams enables the server to dispose of its own resources in a timely way. This\nimproves the efficiency and scalability of services and prevents exceptions."}, "gRPC streaming services vs. repeated fields": {"**When to use repeated fields**": "For any dataset that\u2019s constrained in size and that can be generated in its entirety in a short time\u2014\nsay, under one second\u2014you should use a repeated field in a regular Protobuf message. For example,\nin an e-commerce system, to build a list of items within an order is probably quick and the list won\u2019t\nbe very large. Returning a single message with a repeated field is an order of magnitude faster than\nusing stream and incurs less network overhead.\n\n63 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nIf the client needs all the data before starting to process it and the dataset is small enough to\nconstruct in memory, then consider using a repeated field. Consider it even if the creation of the\ndataset in memory on the server is slower.", "**When to use stream methods**": "When the message objects in your datasets are potentially very large, it\u2019s best for you transfer them\nby using streaming requests or responses. It\u2019s more efficient to construct a large object in memory,\nwrite it to the network, and then free up the resources. This approach will improve the scalability of\nyour service.\n\nSimilarly, you should send datasets of unconstrained size over streams to avoid running out of\nmemory while constructing them.\n\nFor datasets where the consumer can separately process each item, you should consider using a\nstream if it means that progress can be indicated to the user. Using a stream can improve the\nresponsiveness of an application, but you should balance it against the overall performance of the\napplication.\n\nAnother scenario where streams can be useful is where a message is being processed across multiple\nservices. If each service in a chain returns a stream, then the terminal service (that is, the last one in\nthe chain) can start returning messages. These messages can be processed and passed back along the\nchain to the original requestor. The requestor can either return a stream or aggregate the results into\na single response message. This approach lends itself well to patterns like MapReduce."}, "Create gRPC client libraries": {"**Useful extensions**": "[There are two commonly used interfaces in .NET for dealing with streams of objects: IEnumerable](https://docs.microsoft.com/dotnet/api/system.collections.generic.ienumerable-1) and\n[IObservable. Starting with .NET Core 3.0 and C# 8.0, there\u2019s an IAsyncEnumerable](https://docs.microsoft.com/dotnet/api/system.iobservable-1) interface for\nprocessing streams asynchronously, and an await foreach syntax for using the interface. This section\npresents reusable code for applying these interfaces to gRPC streams.\n\nWith the .NET gRPC client libraries, there\u2019s a ReadAllAsync extension method for\nIAsyncStreamReader<T> that creates an IAsyncEnumerable<T> interface. For developers using\nreactive programming, an equivalent extension method to create an IObservable<T> interface might\nlook like the example in the following section.\n\n**IObservable**\n\nThe IObservable<T> interface is the \u201creactive\u201d inverse of IEnumerable<T>. Rather than pulling items\nfrom a stream, the reactive approach lets the stream push items to a subscriber. This behavior is very\nsimilar to gRPC streams, and it\u2019s easy to wrap an IObservable<T> interface around an\nIAsyncStreamReader<T> interface.\n\nThis code is longer than the IAsyncEnumerable<T> code, because C# doesn\u2019t have built-in support for\nworking with observables. You have to create the implementation class manually. It\u2019s a generic class,\nthough, so a single implementation works across all types.\n\n65 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nThe GrpcStreamSubscription class handles the enumeration of the IAsyncStreamReader:\n\n66 CHAPTER 5 | Migrate a WCF solution to gRPC\n\nAll that is required now is a simple extension method to create the observable from the stream reader.", "**Summary**": "[The IAsyncEnumerable](https://docs.microsoft.com/dotnet/api/system.collections.generic.iasyncenumerable-1) [and IObservable](https://docs.microsoft.com/dotnet/api/system.iobservable-1) models are both well-supported and well-documented ways\nof dealing with asynchronous streams of data in .NET. gRPC streams map well to both paradigms,\noffering close integration with .NET, and reactive and asynchronous programming styles.\n\n67 CHAPTER 5 | Migrate a WCF solution to gRPC\n\n**CHAPTER**"}}}, "6": {"Security in gRPC applications": {"WCF authentication and authorization": "In Windows Communication Foundation (WCF), authentication and authorization were handled in\ndifferent ways, depending on the transports and bindings being used. WCF supported various WS-*\nsecurity standards. It also supported Windows authentication for HTTP services running in IIS or\nNetTCP services between Windows systems.", "gRPC authentication and authorization": "gRPC authentication and authorization works on two levels:\n\n['Call-level authentication/authorization is usually handled through tokens that are applied in\\nmetadata when the call is made.', 'Channel-level authentication uses a client certificate that\u2019s applied at the connection level. It\\ncan also include call-level authentication/authorization credentials to be applied to every call\\non the channel automatically.']\n\nYou can use either or both of these mechanisms to help secure your service.\n\n68 CHAPTER 6 | Security in gRPC applications\n\nThe ASP.NET Core implementation of gRPC supports authentication and authorization through most\nof the standard ASP.NET Core mechanisms:\n\n['Call authentication', ['Azure Active Directory', 'IdentityServer', 'JWT Bearer Token', 'OAuth 2.0', 'OpenID Connect', 'WS-Federation'], 'Channel authentication', ['Client certificate']]\n\nThe call authentication methods are all based on _tokens_ . The only real difference is how the tokens are\ngenerated and the libraries that are used to validate the tokens in the ASP.NET Core service.\n\n[For more information, see the Authentication and authorization](https://docs.microsoft.com/aspnet/core/grpc/authn-and-authz) article.\n\nThis chapter will show how to apply call credentials and channel credentials to a gRPC service. It will\nalso show how to use credentials from a .NET gRPC client to authenticate with the service.", "Call credentials": {"**WS-Federation**": "[ASP.NET Core supports WS-Federation using the WsFederation](https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.WsFederation) NuGet package. WS-Federation is the\nclosest available alternative to Windows Authentication, which isn\u2019t supported over HTTP/2. Users are\nauthenticated by using Active Directory Federation Services (AD FS), which provides a token that can\nbe used to authenticate with ASP.NET Core.\n\n[For more information on how to get started with this authentication method, see Authenticate users](https://docs.microsoft.com/aspnet/core/security/authentication/ws-federation)\n[with WS-Federation in ASP.NET Core.](https://docs.microsoft.com/aspnet/core/security/authentication/ws-federation)", "**JWT Bearer tokens**": "[The JSON Web Token](https://jwt.io/) (JWT) standard provides a way to encode information about a user and their\nclaims in an encoded string. It also provides a way to sign that token, so that the consumer can verify\nthe integrity of the token by using public key cryptography. You can use various services, such as\nIdentityServer4, to authenticate users and generate OpenID Connect (OIDC) tokens to use with gRPC\nand HTTP APIs.\n\n69 CHAPTER 6 | Security in gRPC applications\n\nASP.NET Core 7.0 can handle JWTs by using the JWT Bearer package. The configuration is exactly the\nsame for a gRPC application as it is for an ASP.NET Core MVC application. Here, we\u2019ll focus on JWT\nBearer tokens, because they\u2019re easier to develop with than WS-Federation.", "**Add authentication and authorization to the server**": "The JWT Bearer package isn\u2019t included in ASP.NET Core 7.0 by default. Install the\n[Microsoft.AspNetCore.Authentication.JwtBearer NuGet package in your app.](https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.JwtBearer)\n\nAdd the Authentication service in the _Program.cs_ class, and configure the JWT Bearer handler:\n\nThe IssuerSigningKey property requires an implementation of\nMicrosoft.IdentityModels.Tokens.SecurityKey with the cryptographic data necessary to validate the\nsigned tokens. Store this token securely in a _secrets server_, like Azure Key Vault.\n\nNext, add the Authorization service, which controls access to the system:\n\n70 CHAPTER 6 | Security in gRPC applications\n\nNow add the authentication and authorization middleware to the ASP.NET Core pipeline in the\n_Program.cs_ :\n\nFinally, apply the [Authorize] attribute to any services or methods to be secured, and use the User\nproperty from the underlying HttpContext to verify permissions.", "**Provide call credentials in the client application**": "After you\u2019ve obtained a JWT token from an identity server, you can use it to authenticate gRPC calls\nfrom the client by adding it as a metadata header on the call, as follows:\n\nNow you\u2019ve secured your gRPC service by using JWT bearer tokens as call credentials. A version of the\n[portfolios sample gRPC application with authentication and authorization added](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/PortfoliosSample/grpc/TraderSysAuth) is on GitHub.\n\n71 CHAPTER 6 | Security in gRPC applications"}, "Channel credentials": {"**Add certificate authentication to the server**": "Configure certificate authentication both at the host level (for example, on the Kestrel server), and in\nthe ASP.NET Core pipeline.\n\n**Configure certificate validation on Kestrel**\n\nYou can configure Kestrel (the ASP.NET Core HTTP server) to require a client certificate, and optionally\nto carry out some validation of the supplied certificate, before accepting incoming connections. You\nspecify this configuration in the _Program.cs_ :\n\nThe ClientCertificateMode.RequireCertificate setting causes Kestrel to immediately reject any\nconnection request that doesn\u2019t provide a client certificate, but this setting by itself won\u2019t validate a\ncertificate that is provided. Add the ClientCertificateValidation callback to enable Kestrel to validate\nthe client certificate at the point the connection is made, before the ASP.NET Core pipeline is\nengaged. (In this case, the callback ensures that it was issued by the same _Certificate Authority_ as the\nserver certificate.)\n\n72 CHAPTER 6 | Security in gRPC applications\n\n**Add ASP.NET Core certificate authentication**\n\n[The Microsoft.AspNetCore.Authentication.Certificate NuGet package provides certificate](https://www.nuget.org/packages/Microsoft.AspNetCore.Authentication.Certificate)\nauthentication.\n\nAdd the certificate authentication service in the _Program.cs_, and add authentication and authorization\nto the ASP.NET Core pipeline.", "**Provide channel credentials in the client application**": "[With the Grpc.Net.Client package, you configure certificates on an HttpClient](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient) instance that is provided\nto the GrpcChannel used for the connection.\n\n**Load a client certificate from a .PFX file**\n\nA certificate can be loaded from a _.pfx_ file.\n\n73 CHAPTER 6 | Security in gRPC applications\n\n**Load a client certificate from certificate and private key .PEM files**\n\nA certificate can be loaded from a certificate and private key _.pem_ file.", "**Combine ChannelCredentials and CallCredentials**": "You can configure your server to use both certificate and token authentication. To do this, apply the\ncertificate changes to the Kestrel server, and use the JWT bearer middleware in ASP.NET Core.\n\nTo provide both ChannelCredentials and CallCredentials on the client, use the\nChannelCredentials.Create method to apply the call credentials. You still need to apply certificate\n[authentication by using the HttpClient](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient) instance. If you pass any arguments to the SslCredentials\n\n74 CHAPTER 6 | Security in gRPC applications\n\nconstructor, the internal client code throws an exception. The SslCredentials parameter is only\nincluded in the Grpc.Net.Client package\u2019s Create method to maintain compatibility with the Grpc.Core\npackage.\n\n[A version of the FullStockTicker sample gRPC application with certificate authentication added is on](https://github.com/dotnet-architecture/grpc-for-wcf-developers/tree/main/FullStockTickerSample/grpc/FullStockTickerAuth/FullStockTicker)\nGitHub."}, "Encryption and network security": "The network security model for Windows Communication Foundation (WCF) is extensive and complex.\nIt includes transport-level security by using HTTPS or TLS-over-TCP, and message-level security by\nusing the WS-Security specification to encrypt individual messages.\n\ngRPC leaves secure networking to the underlying HTTP/2 protocol, which you can secure by using TLS\ncertificates.\n\nWeb browsers insist on using TLS connections for HTTP/2, but most programmatic clients, including\n.NET\u2019s HttpClient, can use HTTP/2 over unencrypted connections.\n\nFor public APIs, you should always use TLS connections, and provide valid certificates for your services\n[from a proper SSL authority. LetsEncrypt](https://letsencrypt.org/) provides free, automated SSL certificates, and most hosting\ninfrastructure today supports the LetsEncrypt standard with common plug-ins or extensions.\n\nFor internal services across a corporate network, you should still consider using TLS to secure network\ntraffic to and from your gRPC services.\n\n75 CHAPTER 6 | Security in gRPC applications\n\nIf you need to use explicit TLS between services running in Kubernetes, consider using an in-cluster\n[certificate authority and a certificate manager controller like cert-manager. You can then automatically](https://docs.cert-manager.io/en/latest/)\nassign certificates to services at deployment time.\n\n76 CHAPTER 6 | Security in gRPC applications\n\n**CHAPTER**"}}, "7": {"gRPC in production": {"Self-hosted gRPC applications": {"**Run your app as a Windows service**": "To configure your ASP.NET Core application to run as a Windows service, install the\n[Microsoft.Extensions.Hosting.WindowsServices package from NuGet. Then add a call to](https://www.nuget.org/packages/Microsoft.Extensions.Hosting.WindowsServices)\nUseWindowsService to the CreateHostBuilder method in Program.cs.\n\nNow publish your application by using one of these methods:\n\n['From Visual Studio by right-clicking the project and selecting **Publish** on the shortcut menu.', 'From the .NET CLI.']\n\nWhen you publish a .NET application, you can choose to create a _framework-dependent_ deployment\nor a _self-contained_ deployment. Framework-dependent deployments require the .NET Shared Runtime\nto be installed on the host where they\u2019re run. Self-contained deployments are published with a\ncomplete copy of the .NET runtime and framework and can be run on any host. For more information,\n\n77 CHAPTER 7 | gRPC in production\n\n[including the advantages and disadvantages of each approach, see the .NET application deployment](https://docs.microsoft.com/dotnet/core/deploying/)\ndocumentation.\n\nTo publish a self-contained build of the application that doesn\u2019t require the .NET 5 runtime to be\ninstalled on the host, specify the runtime to be included with the application. Use the -r (or --runtime)\nflag.\n\n```\ndotnet publish -c Release -r win-x64 -o ./publish\n\n```\n\nTo publish a framework-dependent build, omit the -r flag.\n\n```\ndotnet publish -c Release -o ./publish\n\n```\n\n[Copy the complete contents of the publish directory to an installation folder. Then, use the sc tool](https://docs.microsoft.com/windows/desktop/services/controlling-a-service-using-sc) to\ncreate a Windows service for the executable file.\n\n```\nsc create MyService binPath=C:\\MyService\\MyService.exe\n\n```\n\n**Log to the Windows event log**\n\n[The UseWindowsService method automatically adds a logging](https://docs.microsoft.com/aspnet/core/fundamentals/logging/) provider that writes log messages to\nthe Windows event log. You can configure logging for this provider by adding an EventLog entry to\nthe Logging section of appsettings.json or another configuration source.\n\nYou can override the source name used in the event log by setting a SourceName property in these\nsettings. If you don\u2019t specify a name, the default application name (normally the executable assembly\nname) will be used.\n\nMore information on logging is at the end of this chapter.", "**Run your app as a Linux service with systemd**": "To configure your ASP.NET Core application to run as a Linux service (or _daemon_ in Linux parlance),\n[install the Microsoft.Extensions.Hosting.Systemd package from NuGet. Then add a call to UseSystemd](https://www.nuget.org/packages/Microsoft.Extensions.Hosting.Systemd)\nto the CreateHostBuilder method in Program.cs.\n\nNow publish your application. The application can be either framework dependent or self-contained\nfor the relevant Linux runtime (for example, linux-x64). You can publish by using one of these\nmethods:\n\n78 CHAPTER 7 | gRPC in production\n\n['From Visual Studio by right-clicking the project and selecting **Publish** on the shortcut menu.']\n\n['From the .NET CLI, by using the following command:']\n\n```\n    dotnet publish -c Release -r linux-x64 -o ./publish\n\n```\n\nCopy the complete contents of the publish directory to an installation folder on the Linux host.\nRegistering the service requires a special file, called a _unit file_, to be added to the /etc/systemd/system\ndirectory. You\u2019ll need root permission to create a file in this folder. Name the file with the identifier\nthat you want systemd to use and the .service extension. For example, use\n/etc/systemd/system/myapp.service.\n\nThe service file uses INI format, as shown in this example:\n\nThe Type=notify property tells systemd that the application will notify it on startup and shutdown. The\nWantedBy=multi-user.target setting will cause the service to start when the Linux system reaches\n\u201crunlevel 2,\u201d which means a nongraphical, multi-user shell is active.\n\nBefore systemd will recognize the service, it needs to reload its configuration. You control systemd by\nusing the systemctl command. After reloading, use the status subcommand to confirm that the\napplication has registered successfully.\n\nIf you\u2019ve configured the service correctly, you\u2019ll get the following output:\n\n```\nmyapp.service - My gRPC Application\nLoaded: loaded (/etc/systemd/system/myapp.service; disabled; vendor preset: enabled)\nActive: inactive (dead)\n\n```\n\nUse the start command to start the service.\n\n```\nsudo systemctl start myapp.service\n\n```\n\nTo tell systemd to start the service automatically on system startup, use the enable command.\n\n```\nsudo systemctl enable myapp\n\n```\n\n79 CHAPTER 7 | gRPC in production\n\n**Log to journald**\n\nThe Linux equivalent of the Windows event log is journald, a structured logging system service that\u2019s\npart of systemd. Log messages written to the standard output by a Linux daemon are automatically\nwritten to journald. To configure logging levels, use the Console section of the logging configuration.\nThe UseSystemd host builder method automatically configures the console output format to suit the\njournal.\n\nBecause journald is the standard for Linux logs, a variety of tools integrate with it. You can easily route\nlogs from journald to an external logging system. Working locally on the host, you can use the\njournalctl command to view logs from the command line.\n\n```\nsudo journalctl -u myapp\n\n```\n\n[To learn more about querying the systemd journal from the command line by using journalctl, see the](https://manpages.debian.org/buster/systemd/journalctl.1)\n[manpages.](https://manpages.debian.org/buster/systemd/journalctl.1)", "**HTTPS certificates for self-hosted applications**": "When you\u2019re running a gRPC application in production, you should use a TLS certificate from a trusted\ncertificate authority (CA). This CA might be a public CA, or an internal one for your organization.\n\n[On Windows hosts, you can load the certificate from a secure certificate store by using the X509Store](https://docs.microsoft.com/windows/win32/seccrypto/managing-certificates-with-certificate-stores)\nclass. You can also use the X509Store class with the OpenSSL key store on some Linux hosts.\n\n[You can also create certificates by using one of the X509Certificate2 constructors, from either:](https://docs.microsoft.com/dotnet/api/system.security.cryptography.x509certificates.x509certificate2.-ctor)\n\n['A file, such as a .pfx file protected by a strong password', '[Binary data retrieved from a secure storage service such as Azure Key Vault](https://azure.microsoft.com/services/key-vault/)']\n\nYou can configure Kestrel to use a certificate in two ways: from configuration or in code.\n\n**Set HTTPS certificates by using configuration**\n\nThe configuration approach requires setting the password and path to the certificate .pfx file in the\nKestrel configuration section. In appsettings.json, that looks like this:\n\n80 CHAPTER 7 | gRPC in production\n\nProvide the password by using a secure configuration source such as Azure Key Vault or Hashicorp\nVault.\n\n**Set HTTPS certificates in code**\n\nTo configure HTTPS on Kestrel in code, use the ConfigureKestrel method on IWebHostBuilder in the\nProgram class.\n\nAgain, be sure to store the password for the .pfx file in, and retrieve it from, a secure configuration\nsource."}, "Create Docker images": {"**Microsoft base images for ASP.NET Core applications**": "Microsoft provides a range of base images for building and running .NET applications. To create an\nASP.NET Core 7.0 image, you use two base images:\n\n['An SDK image to build and publish the application.', 'A runtime image for deployment.']\n\n|Image|Description|\n|---|---|\n|mcr.microsoft.com/dotnet/sd<br>k|For building applications with docker build. Not to be used in<br>production.|\n|mcr.microsoft.com/dotnet/as<br>pnet|Contains the runtime and ASP.NET Core dependencies. For<br>production.|\n\nFor each image, there are four variants based on different Linux distributions, distinguished by tags.\n\n81 CHAPTER 7 | gRPC in production\n\n|Image tag(s)|Linux|Notes|\n|---|---|---|\n|7.0-bullseye-slim, 7.0|Debian 11|The default image if no<br>OS variant is specified.|\n|7.0-alpine|Alpine 3.17|Alpine base images are<br>much smaller than<br>Debian or Ubuntu ones.|\n\nThe Alpine base image is around 100 MB, compared to 200 MB for the Debian and Ubuntu images.\nSome software packages or libraries might not be available in Alpine\u2019s package management. If you\u2019re\nnot sure which image to use, you should probably choose the default Debian.", "**Create a Docker image**": "A Docker image is defined by a _Dockerfile_ . This _Dockerfile_ is a text file that contains all the commands\nneeded to build the application and install any dependencies that are required for either building or\nrunning the application. The following example shows the simplest Dockerfile for an ASP.NET Core 7.0\napplication:\n\nThe Dockerfile has two parts: the first uses the sdk base image to build and publish the application;\nthe second creates a runtime image from the aspnet base. This is because the sdk image is around\n900 MB, compared to around 200 MB for the runtime image, and most of its contents are unnecessary\nat run time.\n\n82 CHAPTER 7 | gRPC in production\n\n|The build steps|Col2|\n|---|---|\n|**Step**|**Description**|\n|FROM ...|Declares the base image and assigns the builder alias.|\n|WORKDIR /src|Creates the /src directory and sets it as the current working directory.|\n|COPY . .|Copies everything below the current directory on the host into the<br>current directory on the image.|\n|RUN dotnet restore|Restores any external packages (ASP.NET Core 3.0 framework is<br>preinstalled with the SDK).|\n|RUN dotnet publish ...|Builds and publishes a Release build. The --runtime flag isn\u2019t required.|\n\n**The runtime image steps**\n\n|Step|Description|\n|---|---|\n|FROM ...|Declares a new base image.|\n|WORKDIR /app|Creates the /app directory and sets it as the current working directory.|\n|COPY --from=builder ...|Copies the published application from the previous image, by using the<br>builder alias from the first FROM line.|\n|ENTRYPOINT [ ... ]|Sets the command to run when the container starts. The dotnet<br>command in the runtime image can only run DLL files.|\n\n**HTTPS in Docker**\n\nMicrosoft base images for Docker set the ASPNETCORE_URLS environment variable to http://+:80,\nmeaning that Kestrel runs without HTTPS on that port. If you\u2019re using HTTPS with a custom certificate\n(as described in Self-hosted gRPC applications), you should override this configuration. Set the\nenvironment variable in the runtime image creation part of your Dockerfile.\n\n**The .dockerignore file**\n\nMuch like .gitignore files that exclude certain files and directories from source control, the\n.dockerignore file can be used to exclude files and directories from being copied to the image during\nbuild. This file not only saves time copying, but can also avoid some errors that arise from having the\nobj directory from your PC copied into the image. At a minimum, you should add entries for bin and\nobj to your .dockerignore file.\n\n```\nbin/\nobj/\n\n```\n\n83 CHAPTER 7 | gRPC in production", "**Build the image**": "For a StockKube.sln solution containing two different applications StockData and StockWeb, it\u2019s\nsimplest to put the Dockerfile for each one of them in the base directory. In that case, to build the\nimage, use the following docker build command from the same directory where .sln file resides.\n\n```\ndocker build -t stockdata:1.0.0 -f ./src/StockData/Dockerfile .\n\n```\n\nThe confusingly named --tag flag (which can be shortened to -t) specifies the whole name of the\nimage, including the actual tag if specified. The . at the end specifies the context in which the build\nwill be run; the current working directory for the COPY commands in the Dockerfile.\n\nIf you have multiple applications within a single solution, you can keep the Dockerfile for each\napplication in its own folder, beside the .csproj file. You should still run the docker build command\nfrom the base directory to ensure that the solution and all the projects are copied into the image. You\ncan specify a Dockerfile below the current directory by using the --file (or -f) flag.\n\n```\ndocker build -t stockdata:1.0.0 -f ./src/StockData/Dockerfile .\n\n#### **Run the image in a container on your machine**\n\n```\n\nTo run the image in your local Docker instance, use the docker run command.\n\n```\ndocker run -ti -p 5000:80 stockdata:1.0.0\n\n```\n\nThe -ti flag connects your current terminal to the container\u2019s terminal, and runs in interactive mode.\nThe -p 5000:80 publishes (links) port 80 on the container to port 5000 on the localhost network\ninterface.", "**Push the image to a registry**": "After you\u2019ve verified that the image works, push it to a Docker registry to make it available on other\nsystems. Internal networks will need to provision a Docker registry. This activity can be as simple as\n[running Docker\u2019s own registry image](https://docs.docker.com/registry/deploying/) (the Docker registry runs in a Docker container), but there are\nvarious more comprehensive solutions available. For external sharing and cloud use, there are various\n[managed registries available, such as Azure Container Registry](https://docs.microsoft.com/azure/container-registry/) [or Docker Hub.](https://docs.docker.com/docker-hub/repos/)\n\nTo push to Docker Hub, prefix the image name with your user or organization name.\n\nTo push to a private registry, prefix the image name with the registry host name and the organization\nname.\n\nAfter the image is in a registry, you can deploy it to individual Docker hosts, or to a container\norchestration engine like Kubernetes.\n\n84 CHAPTER 7 | gRPC in production"}, "Kubernetes": {"**Kubernetes terminology**": "Kubernetes uses _desired state configuration_ : the API is used to describe objects like _Pods_, _Deployments_,\nand _Services_, and the _Control Plane_ takes care of implementing the desired state across all the _nodes_\nin a _cluster_ . A Kubernetes cluster has a _Master_ node that runs the _Kubernetes API_, which you can\ncommunicate with programmatically or by using the kubectl command-line tool. kubectl can create\nand manage objects through command-line arguments, but it works best with YAML files that contain\ndeclaration data for Kubernetes objects.\n\n**Kubernetes YAML files**\n\nEvery Kubernetes YAML file will have at least three top-level properties:\n\nThe apiVersion property is used to specify which version (and which API) the file is intended for. The\nkind property specifies the kind of object the YAML represents. The metadata property contains\nobject properties like name, namespace, and labels.\n\nMost Kubernetes YAML files will also have a spec section that describes the resources and\nconfiguration necessary to create the object.\n\n85 CHAPTER 7 | gRPC in production\n\n**Pods**\n\nPods are the basic units of execution in Kubernetes. They can run multiple containers, but they\u2019re also\nused to run single containers. The pod also includes any storage resources required by the containers,\nand the network IP address.\n\n**Services**\n\nServices are meta-objects that describe Pods (or sets of Pods) and provide a way to access them\nwithin the cluster, such as mapping a service name to a set of pod IP addresses by using the cluster\nDNS service.\n\n**Deployments**\n\nDeployments are the _desired state_ objects for Pods. If you create a pod manually, it won\u2019t be restarted\nwhen it terminates. Deployments are used to tell the cluster which Pods, and how many replicas of\nthose Pods, should be running at the present time.\n\n**Other objects**\n\nPods, Services, and Deployments are just three of the most basic object types. There are dozens of\nother object types that are managed by Kubernetes clusters. For more information, see the\n[Kubernetes Concepts](https://kubernetes.io/docs/concepts/) documentation.\n\n**Namespaces**\n\nKubernetes clusters are designed to scale to hundreds or thousands of nodes and to run similar\nnumbers of services. To avoid clashes between object names, namespaces are used to group objects\ntogether as part of larger applications. Kubernetes\u2019s own services run in a default namespace. All user\nobjects should be created in their own namespaces to avoid potential clashes with default objects or\nother tenants in the cluster.", "**Get started with Kubernetes**": "If you\u2019re running Docker Desktop for Windows or Docker Desktop for Mac, Kubernetes is already\navailable. Just enable it in the **Kubernetes** section of the **Settings** window:\n\n86 CHAPTER 7 | gRPC in production\n\n[To run a local Kubernetes cluster on Linux, consider minikube, or MicroK8s](https://github.com/kubernetes/minikube) if your Linux distribution\n[supports snaps.](https://snapcraft.io/)\n\nTo confirm that your cluster is running and accessible, run the kubectl version command:\n\nIn this example, both the kubectl CLI and the Kubernetes server are running version 1.14.6. Each\nversion of kubectl is supposed to support the previous and next version of the server, so kubectl 1.14\nshould work with server versions 1.13 and 1.15 as well.", "**Run services on Kubernetes**": "The sample application has a kube directory that contains three YAML files. The namespace.yml file\ndeclares a custom namespace: stocks. The stockdata.yml file declares the Deployment and the Service\nfor the gRPC application, and the stockweb.yml file declares the Deployment and Service for an\nASP.NET Core 7.0 MVC web application that consumes the gRPC service.\n\nTo use a YAML file with kubectl, run the apply -f command:\n\n```\nkubectl apply -f object.yml\n\n```\n\nThe apply command will check the validity of the YAML file and display any errors received from the\nAPI, but doesn\u2019t wait until all the objects declared in the file have been created because this step can\n\n87 CHAPTER 7 | gRPC in production\n\ntake some time. Use the kubectl get command with the relevant object types to check on object\ncreation in the cluster.\n\n**The namespace declaration**\n\nNamespace declaration is simple and requires only assigning a name:\n\nUse kubectl to apply the namespace.yml file and to confirm the namespace is created successfully:\n\n**The StockData application**\n\nThe stockdata.yml file declares two objects: a Deployment and a Service.\n\n**The StockData Deployment**\n\nThe Deployment part of the YAML file provides the spec for the deployment itself, including the\nnumber of replicas required, and a template for the Pod objects to be created and managed by the\ndeployment. Note that Deployment objects are managed by the apps API, as specified in apiVersion,\nrather than the main Kubernetes API.\n\n88 CHAPTER 7 | gRPC in production\n\nThe spec.selector property is used to match running Pods to the Deployment. The Pod\u2019s\nmetadata.labels property must match the matchLabels property or the API call will fail.\n\nThe template.spec section declares the container to be run. When you\u2019re working with a local\nKubernetes cluster, such as the one provided by Docker Desktop, you can specify images that were\nbuilt locally as long as they have a version tag.\n\nThe ports property specifies which container ports should be published on the Pod. The stockservice\nimage runs the service on the standard HTTP port, so port 80 is published.\n\nThe resources section applies resource limits to the container running within the Pod. This is a good\npractice because it prevents an individual Pod from consuming all the available CPU or memory on a\nnode.\n\n**The StockData Service**\n\nThe Service part of the YAML file declares the service that provides access to the Pods within the\ncluster.\n\nThe Service spec uses the selector property to match running Pods, in this case looking for Pods that\nhave a label run: stockdata. The specified port on matching Pods is published by the named service.\nOther Pods running in the stocks namespace can access HTTP on this service by using\nhttp://stockdata as the address. Pods running in other namespaces can use the http://stockdata.stocks\n[host name. You can control cross-namespace service access by using Network Policies.](https://kubernetes.io/docs/concepts/services-networking/network-policies/)\n\n**Deploy the StockData application**\n\nUse kubectl to apply the stockdata.yml file and confirm that the Deployment and Service were\ncreated:\n\n89 CHAPTER 7 | gRPC in production\n\n**The StockWeb application**\n\nThe stockweb.yml file declares the Deployment and Service for the MVC application.\n\n90 CHAPTER 7 | gRPC in production\n\n**Environment variables**\n\nThe env section of the Deployment object specifies environment variables to be set in the container\nthat\u2019s running the stockweb:1.0.0 images.\n\nThe **StockData__Address** environment variable will map to the StockData:Address configuration\nsetting thanks to the EnvironmentVariables configuration provider. This setting uses double\nunderscores between names to separate sections. The address uses the service name of the stockdata\nService, which is running in the same Kubernetes namespace.\n\nThe **DOTNET_SYSTEM_NET_HTTP_SOCKETSHTTPHANDLER_HTTP2UNENCRYPTEDSUPPORT**\n[environment variable sets an AppContext](https://docs.microsoft.com/dotnet/api/system.appcontext) switch that enables unencrypted HTTP/2 connections for\n[HttpClient. This environment variable does the same thing as setting the switch in code, as shown](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient)\nhere:\n\n```\nAppContext.SetSwitch(\"System.Net.Http.SocketsHttpHandler.Http2UnencryptedSupport\", true );\n\n```\n\nIf you use an environment variable for the switch, you can easily change the context depending on the\ncontext in which the application is running.\n\n**Service types**\n\nThe type: NodePort property is used to make the web application accessible from outside the cluster.\nThis property type causes Kubernetes to publish port 80 on the Service to an arbitrary port on the\ncluster\u2019s external network sockets. You can find the assigned port by using the kubectl get service\ncommand.\n\nThe stockdata Service shouldn\u2019t be accessible from outside the cluster, so it uses the default type,\nClusterIP.\n\nProduction systems will most likely use an integrated load balancer to expose public applications to\nexternal consumers. Services exposed in this way should use the LoadBalancer type.\n\n[For more information on Service types, see the Kubernetes Publishing Services](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types) documentation.\n\n**Deploy the StockWeb application**\n\nUse kubectl to apply the stockweb.yml file and confirm that the Deployment and Service were created:\n\nThe output of the get service command shows that the HTTP port has been published to port 32564\non the external network. For Docker Desktop, this IP address will be localhost. You can access the\napplication by browsing to http://localhost:32564.\n\n91 CHAPTER 7 | gRPC in production\n\n**Test the application**\n\nThe StockWeb application displays a list of NASDAQ stocks that are retrieved from a simple requestreply service. For this demonstration, each line also shows the unique ID of the Service instance that\nreturned it.\n\nIf the number of replicas of the stockdata Service were increased, you might expect the **Server** value\nto change from line to line, but in fact all 100 records are always returned from the same instance. If\nyou refresh the page every few seconds, the server ID remains the same. Why does this happen?\nThere are two factors at play here.\n\nFirst, the Kubernetes Service discovery system uses round-robin load balancing by default. The first\ntime the DNS server is queried, it will return the first matching IP address for the Service. The next\ntime, it will return the next IP address in the list, and so on, until the end. At that point, it loops back to\nthe start.\n\nSecond, the HttpClient used for the StockWeb application\u2019s gRPC client is created and managed by\n[the ASP.NET Core HttpClientFactory, and a single instance of this client is used for every call to the](https://docs.microsoft.com/dotnet/architecture/microservices/implement-resilient-applications/use-httpclientfactory-to-implement-resilient-http-requests)\npage. The client only does one DNS lookup, so all requests are routed to the same IP address. And\nbecause the HttpClientHandler is cached for performance reasons, multiple requests in quick\nsuccession will _all_ use the same IP address, until the cached DNS entry expires or the handler instance\nis disposed for some reason.\n\nThe result is that by default requests to a gRPC Service aren\u2019t balanced across all instances of that\nService in the cluster. Different consumers will use different instances, but that doesn\u2019t guarantee a\ngood distribution of requests or a balanced use of resources.\n\nThe next chapter, Service meshes, will address this problem.\n\n92 CHAPTER 7 | gRPC in production"}, "Service meshes": {"**Service mesh options**": "Three general-purpose service mesh implementations are currently available for use with Kubernetes:\n[Istio,](https://istio.io/) [Linkerd, and Consul Connect. All three provide request routing/proxying, traffic encryption,](https://linkerd.io/)\nresilience, host-to-host authentication, and traffic control.\n\nChoosing a service mesh depends on multiple factors:\n\n['The organization\u2019s specific requirements around costs, compliance, paid support plans, and so\\non.', 'The nature of the cluster, its size, the number of services deployed, and the volume of traffic\\nwithin the cluster network.', 'Ease of deploying and managing the mesh and using it with services.']", "**Example: Add Linkerd to a deployment**": "In this example, you\u2019ll learn how to use the Linkerd service mesh with the _StockKube_ application from\n[the previous section. To follow this example, you\u2019ll need to install the Linkerd CLI. You can download](https://linkerd.io/2/getting-started/#step-1-install-the-cli)\n\n94 CHAPTER 7 | gRPC in production\n\nWindows binaries from the section that lists GitHub releases. Be sure to use the most recent _stable_\nrelease and not one of the edge releases.\n\n[With the Linkerd CLI installed, follow the Getting Started](https://linkerd.io/2/getting-started/index.html) instructions to install the Linkerd\ncomponents on your Kubernetes cluster. The instructions are straightforward, and the installation\nshould take only a couple of minutes on a local Kubernetes instance.\n\n**Add Linkerd to Kubernetes deployments**\n\nThe Linkerd CLI provides an inject command to add the necessary sections and properties to\nKubernetes files. You can run the command and write the output to a new file.\n\n```\nlinkerd inject stockdata.yml > stockdata-with-mesh.yml\nlinkerd inject stockweb.yml > stockweb-with-mesh.yml\n\n```\n\nYou can inspect the new files to see what changes have been made. For deployment objects, a\nmetadata annotation is added to tell Linkerd to inject a sidecar proxy container into the pod when it\u2019s\ncreated.\n\nIt\u2019s also possible to pipe the output of the linkerd inject command to kubectl directly. The following\ncommands will work in PowerShell or any Linux shell.\n\n**Inspect services in the Linkerd dashboard**\n\nOpen the Linkerd dashboard by using the linkerd CLI.\n\n```\nlinkerd dashboard\n\n```\n\nThe dashboard provides detailed information about all services that are connected to the mesh.\n\n95 CHAPTER 7 | gRPC in production\n\nIf you increase the number of replicas of the StockData gRPC service as shown in the following\nexample, and refresh the StockWeb page in the browser, you should see a mix of IDs in the **Server**\ncolumn. This mix indicates that all the available instances are serving requests.\n\n96 CHAPTER 7 | gRPC in production"}, "Load balancing gRPC": {"**L4 load balancers**": "An L4 load balancer accepts a TCP connection request from a client, opens another connection to one\nof the back-end instances, and copies data between the two connections with no real processing. L4\noffers excellent performance and low latency, but with little control or intelligence. As long as the\nclient keeps the connection open, all requests will be directed to the same back-end instance.\n\n[Azure Load Balancer](https://azure.microsoft.com/services/load-balancer/) is an example of an L4 load balancer.", "**L7 load balancers**": "An L7 load balancer parses incoming HTTP/2 requests and passes them on to back-end instances on a\nrequest-by-request basis, no matter how long the connection is held by the client.\n\nExamples of L7 load balancers:\n\n['[NGINX](https://www.nginx.com/)', '[HAProxy](https://www.haproxy.com/)', '[Traefik](https://traefik.io/)']\n\nAs a rule of thumb, L7 load balancers are the best choice for gRPC and other HTTP/2 applications (and\nfor HTTP applications generally, in fact). L4 load balancers will _work_ with gRPC applications, but\nthey\u2019re primarily useful when low latency and low overhead are important.\n\nIf you\u2019re using TLS encryption, load balancers can terminate the TLS connection and pass unencrypted\nrequests to the back-end application, or they can pass the encrypted request along. Either way, the\nload balancer will need to be configured with the server\u2019s public and private key so it can decrypt\nrequests for processing.\n\nSee to the documentation for your preferred load balancer to find out how to configure it to handle\nHTTP/2 requests with your back-end services.\n\n97 CHAPTER 7 | gRPC in production", "**Load balancing within Kubernetes**": "See the section on service meshes for a discussion of load balancing across internal services on\nKubernetes."}, "Application Performance Management": {"**The difference between logging and metrics**": "_Logging_ is concerned with text messages that record detailed information about things that have\nhappened in the system. Log messages might include exception data, like stack traces, or structured\ndata that provide context about the message. Logging output is commonly written to a searchable\ntext store.\n\n_Metrics_ refers to numeric data designed to be aggregated and presented by using charts and graphs\nin a dashboard. The dashboard provides a view of the overall health and performance of an\napplication. Metrics data can also be used to trigger automated alerts when a threshold is exceeded.\nHere are some examples of metrics data:\n\n['Time taken to process requests.', 'The number of requests per second being handled by an instance of a service.', 'The number of failed requests on an instance.']", "**Logging in ASP.NET Core gRPC**": "[ASP.NET Core provides built-in support for logging, in the form of Microsoft.Extensions.Logging](https://www.nuget.org/packages/Microsoft.Extensions.Logging)\nNuGet package. The core parts of this library are included with the Web SDK, so there\u2019s no need to\ninstall it manually. By default, log messages are written to the standard output (the \u201cconsole\u201d) and to\nany attached debugger. To write logs to persistent external data stores, you might need to import\n[optional logging sink packages.](https://docs.microsoft.com/aspnet/core/fundamentals/logging/#third-party-logging-providers)\n\nThe ASP.NET Core gRPC framework writes detailed diagnostic logging messages to this logging\nframework, so they can be processed and stored along with your application\u2019s own messages.\n\n**Produce log messages**\n\nThe logging extension is automatically registered with ASP.NET Core\u2019s dependency injection system,\nso you can specify loggers as a constructor parameter on gRPC service types.\n\n98 CHAPTER 7 | gRPC in production\n\nMany log messages, such as requests and exceptions, are provided by the ASP.NET Core and gRPC\nframework components. Add your own log messages to provide detail and context about application\nlogic, rather than lower-level concerns.\n\nFor more information about writing log messages and available logging sinks and targets, see\n[Logging in .NET Core and ASP.NET Core.](https://docs.microsoft.com/aspnet/core/fundamentals/logging/)", "**Metrics in ASP.NET Core gRPC**": "The .NET Core runtime provides a set of components for emitting and observing metrics. These\n[include APIs such as the EventSource and EventCounter](https://docs.microsoft.com/dotnet/api/system.diagnostics.tracing.eventsource) classes. These APIs can emit basic numeric\n[data that can be consumed by external processes, like the dotnet-counters global tool, or Event](https://docs.microsoft.com/dotnet/core/diagnostics/dotnet-counters)\nTracing for Windows. For more information about using EventCounter in your own code, see\n[EventCounter introduction.](https://github.com/dotnet/runtime/blob/main/src/libraries/System.Diagnostics.Tracing/documentation/EventCounterTutorial.md)\n\nFor more advanced metrics and for writing metric data to a wider range of data stores, you might try\n[an open-source project called App Metrics. This suite of libraries provides an extensive set of types to](https://www.app-metrics.io/)\ninstrument your code. It also offers packages to write metrics to different kinds of targets that include\n[time-series databases, such as Prometheus and InfluxDB, and Application Insights. The](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview)\n[App.Metrics.AspNetCore.Mvc NuGet package even adds a comprehensive set of basic metrics that are](https://www.nuget.org/packages/App.Metrics.AspNetCore.Mvc/)\nautomatically generated via integration with the ASP.NET Core framework. The project website\n[provides templates](https://www.app-metrics.io/samples/grafana/) [for displaying those metrics with the Grafana](https://grafana.com/) visualization platform.\n\n**Produce metrics**\n\nMost metrics platforms support the following types:\n\n|Metric type|Description|\n|---|---|\n|Counter|Tracks how often something happens, such as<br>requests and errors.|\n|Gauge|Records a single value that changes over time,<br>such as active connections.|\n|Histogram|Measures a distribution of values across<br>arbitrary limits. For example, a histogram can<br>track dataset size, counting how many<br>contained <10 records, how many contained<br>11-100 records, how many contained 101-1000<br>records, and how many contained >1000<br>records.|\n|Meter|Measures the rate at which an event occurs in<br>various time spans.|\n\n99 CHAPTER 7 | gRPC in production\n\n|Metric type|Description|\n|---|---|\n|Timer|Tracks the duration of events and the rate at<br>which it occurs, stored as a histogram.|\n\nBy using _App Metrics_, an IMetrics interface can be obtained via dependency injection, and used to\nrecord any of these metrics for a gRPC service. The following example shows how to count the\nnumber of Get requests made over time:\n\n**Store and visualize metrics data**\n\nThe best way to store metrics data is in a _time-series database_, a specialized data store designed to\nrecord numerical data series marked with timestamps. The most popular of these databases are\n[Prometheus](https://prometheus.io/) [and InfluxDB. Microsoft Azure also provides dedicated metrics storage through the Azure](https://www.influxdata.com/products/influxdb-overview/)\n[Monitor](https://docs.microsoft.com/azure/azure-monitor/overview) service.\n\n[The current go-to solution for visualizing metrics data is Grafana, which works with a wide range of](https://grafana.com/)\nstorage providers. The following image shows an example Grafana dashboard that displays metrics\nfrom the Linkerd service mesh running the StockData sample:\n\n100 CHAPTER 7 | gRPC in production\n\n**Metrics-based alerting**\n\nThe numerical nature of metrics data means that it\u2019s ideally suited to drive alerting systems, notifying\ndevelopers or support engineers when a value falls outside of some defined tolerance. The platforms\nalready mentioned all provide support for alerting via a range of options, including emails, text\nmessages, or in-dashboard visualizations.", "**Distributed tracing**": "Distributed tracing is a relatively recent development in monitoring, which has arisen from the\nincreasing use of microservices and distributed architectures. A single request from a client browser,\napplication, or device can be broken down into many steps and sub-requests, and involve the use of\nmany services across a network. This activity makes it difficult to correlate log messages and metrics\nwith the specific request that triggered them. Distributed tracing applies identifiers to requests, and\n[allows logs and metrics to be correlated with a particular operation. This tracing is similar to WCF\u2019s](https://docs.microsoft.com/dotnet/framework/wcf/diagnostics/tracing/end-to-end-tracing)\n[end-to-end tracing, but it\u2019s applied across multiple platforms.](https://docs.microsoft.com/dotnet/framework/wcf/diagnostics/tracing/end-to-end-tracing)\n\nDistributed tracing has grown quickly in popularity and is beginning to standardize. The Cloud Native\n[Computing Foundation created the Open Tracing standard, attempting to provide vendor-neutral](https://opentracing.io/)\n[libraries for working with back ends like Jaeger](https://www.jaegertracing.io/) [and Elastic APM. At the same time, Google created the](https://www.elastic.co/products/apm)\n[OpenCensus project](https://opencensus.io/) to address the same set of problems. These two projects are merging into a new\n[project, OpenTelemetry, which aims to be the industry standard of the future.](https://opentelemetry.io/)\n\n**How distributed tracing works**\n\nDistributed tracing is based on the concept of _spans_ : named, timed operations that are part of a single\n_trace_, which can involve processing on multiple nodes of a system. When a new operation is initiated,\na trace is created with a unique identifier. For each sub-operation, a span is created with its own\n\n101 CHAPTER 7 | gRPC in production\n\nidentifier and trace identifier. As the request passes around the system, various components can\ncreate _child_ spans that include the identifier of their _parent_ . A span has a _context_, which contains the\ntrace and span identifiers, as well as useful data in the form of key and value pairs (called _baggage_ ).\n\n**Distributed tracing with DiagnosticSource**\n\n[.NET has an internal module that maps well to distributed traces and spans: DiagnosticSource. As well](https://github.com/dotnet/runtime/blob/main/src/libraries/System.Diagnostics.DiagnosticSource/src/DiagnosticSourceUsersGuide.md#diagnosticsource-users-guide)\nas providing a simple way to produce and consume diagnostics within a process, the\nDiagnosticSource module has the concept of an _activity_ . An activity is effectively an implementation of\na distributed trace, or a span within a trace. The internals of the module take care of parent/child\nactivities, including allocating identifiers. For more information about using the Activity type, see the\n[Activity User Guide on GitHub.](https://github.com/dotnet/runtime/blob/main/src/libraries/System.Diagnostics.DiagnosticSource/src/ActivityUserGuide.md#activity-user-guide)\n\nBecause DiagnosticSource is a part of the core framework and later, it\u2019s supported by several core\n[components. These include HttpClient, Entity Framework Core, and ASP.NET Core, including explicit](https://docs.microsoft.com/dotnet/api/system.net.http.httpclient)\nsupport in the gRPC framework. When ASP.NET Core receives a request, it checks for a pair of HTTP\n[headers matching the W3C Trace Context standard. If the headers are found, an activity is started by](https://www.w3.org/TR/trace-context)\nusing the identity values and context from the headers. If no headers are found, an activity is started\nwith generated identity values that match the standard format. Any diagnostics generated by the\nframework or by application code during the lifetime of this activity can be tagged with the trace and\nspan identifiers. The HttpClient support extends this functionality further by checking for a current\nactivity on every request, and automatically adding the trace headers to the outgoing request.\n\nThe ASP.NET Core gRPC client and server libraries include explicit support for DiagnosticSource and\nActivity, and create activities and apply and use header information automatically.\n\n**Add your own DiagnosticSource and Activity**\n\nTo add your own diagnostics or create explicit spans within your application code, see the\n[DiagnosticSource User Guide and Activity User Guide.](https://github.com/dotnet/runtime/blob/main/src/libraries/System.Diagnostics.DiagnosticSource/src/DiagnosticSourceUsersGuide.md#instrumenting-with-diagnosticsourcediagnosticlistener)\n\n**Store distributed trace data**\n\nAt the time of writing, the OpenTelemetry project is still in the early stages, and only alpha-quality\npackages are available for .NET applications. The OpenTracing project currently offers more mature\nlibraries.\n\nThe OpenTracing API is described in the following section. If you want to use the OpenTelemetry API\n[in your application instead, refer to the OpenTelemetry .NET SDK repository](https://github.com/open-telemetry/opentelemetry-dotnet) on GitHub.\n\n**Use the OpenTracing package to store distributed trace data**\n\n[The OpenTracing NuGet package supports all OpenTracing-compliant back ends (which can be used](https://www.nuget.org/packages/OpenTracing/)\nindependently of DiagnosticSource). There\u2019s an additional package from the OpenTracing API\n\n102 CHAPTER 7 | gRPC in production\n\n[Contributions project, OpenTracing.Contrib.NetCore. This package adds a DiagnosticSource listener,](https://www.nuget.org/packages/OpenTracing.Contrib.NetCore/)\nand writes events and activities to a back end automatically. Enabling this package is as simple as\ninstalling it from NuGet and adding it as a service in your Program class.\n\nThe OpenTracing package is an abstraction layer, and as such it requires implementation specific to\nthe back end. OpenTracing API implementations are available for the following open source back\n\n|ends.|Col2|Col3|\n|---|---|---|\n|**Name**|**Package**|**Website**|\n|Jaeger|Jaeger|jaegertracing.io|\n|Elastic APM|Elastic.Apm.NetCoreAll|elastic.co/products/apm|\n\n[For more information on the OpenTracing API for .NET, see the OpenTracing for C#](https://github.com/opentracing/opentracing-csharp) and the\n[OpenTracing Contrib C#/.NET Core repositories on GitHub.](https://github.com/opentracing-contrib/csharp-netcore)\n\n103 CHAPTER 7 | gRPC in production\n\n**CHAPTER**"}}}, "8": {"Appendix A - Transactions": "Windows Communication Foundation (WCF) supports distributed transactions, allowing you to\n[perform atomic operations across multiple services. This functionality is based on the Microsoft](https://docs.microsoft.com/previous-versions/windows/desktop/ms684146(v=vs.85))\n[Distributed Transaction Coordinator.](https://docs.microsoft.com/previous-versions/windows/desktop/ms684146(v=vs.85))\n\nIn the newer microservices landscape, this type of automated distributed transaction processing isn\u2019t\npossible. There are too many different technologies involved, including relational databases, NoSQL\ndata stores, and messaging systems. There might also be a mix of operating systems, programming\nlanguages, and frameworks in use in a single environment.\n\n[WCF distributed transaction is an implementation of what is known as a two-phase commit (2PC). You](https://en.wikipedia.org/wiki/Two-phase_commit_protocol)\ncan implement 2PC transactions manually by coordinating messages across services, creating open\ntransactions within each service, and sending commit or rollback messages, depending upon success\nor failure. However, the complexity involved in managing 2PC can increase exponentially as systems\nevolve. Open transactions hold database locks that can negatively affect performance, or, worse, cause\ncross-service deadlocks.\n\nIf possible, it\u2019s best to avoid distributed transactions altogether. If two items of data are so linked as to\nrequire atomic updates, consider handling them both with the same service. Apply those atomic\nchanges by using a single request or message to that service.\n\n[If that isn\u2019t possible, then one alternative is to use the Saga pattern. In a saga, updates are processed](https://microservices.io/patterns/data/saga.html)\nsequentially; as each update succeeds, the next one is triggered. These triggers can be propagated\nfrom service to service, or managed by a saga coordinator or orchestrator. If an update fails at any\npoint during the process, the services that have already completed their updates apply specific logic\nto reverse them.\n\nAnother option is to use Domain Driven Design (DDD) and Command/Query Responsibility\n[Segregation (CQRS), as described in the .NET Microservices e-book. In particular, using domain events](https://docs.microsoft.com/dotnet/architecture/microservices/microservice-ddd-cqrs-patterns/)\n[or event sourcing](https://martinfowler.com/eaaDev/EventSourcing.html) can help to ensure that updates are consistently, if not immediately, applied.\n\n104 CHAPTER 8 | Appendix A - Transactions"}}