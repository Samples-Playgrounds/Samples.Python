{"Runtime image creation": {"The .dockerignore file": "Much like .gitignore files that exclude certain files and directories from source control, the .dockerignore file can be used to exclude files and directories from being copied to the image during build. This file not only saves time copying, but can also avoid some errors that arise from having the obj directory from your PC copied into the image. At a minimum, you should add entries for bin and obj to your .dockerignore file.\n\nbin/\n\nobj/", "Build the image": "For a StockKube.sln solution containing two different applications StockData and StockWeb, it's simplest to put the Dockerfile for each one of them in the base directory. In that case, to build the image, use the following docker build command from the same directory where .sln file resides.\n\ndocker build -t stockdata:1.0.0 -f ./src/StockData/Dockerfile .\n\nThe confusingly named --tag flag (which can be shortened to -t) specifies the whole name of the image, including the actual tag if specified. The . at the end specifies the context in which the build will be run; the current working directory for the COPY commands in the Dockerfile.\n\nIf you have multiple applications within a single solution, you can keep the Dockerfile for each application in its own folder, beside the .csproj file. You should still run the docker build command from the base directory to ensure that the solution and all the projects are copied into the image. You can specify a Dockerfile below the current directory by using the --file (or -f) flag.\n\ndocker build -t stockdata:1.0.0 -f ./src/StockData/Dockerfile .", "Run the image in a container on your machine": "To run the image in your local Docker instance, use the docker run command.\n\n```\ndocker run -ti -p 5000:80 stockdata:1.0.0\n```\n\nThe -ti flag connects your current terminal to the container's terminal, and runs in interactive mode. The -p 5000:80 publishes (links) port 80 on the container to port 5000 on the localhost network interface.", "Push the image to a registry": "After you've verified that the image works, push it to a Docker registry to make it available on other systems. Internal networks will need to provision a Docker registry. This activity can be as simple as running Docker's own registry image (the Docker registry runs in a Docker container), but there are various more comprehensive solutions available. For external sharing and cloud use, there are various managed registries available, such as Azure Container Registry or Docker Hub .\n\nTo push to Docker Hub, prefix the image name with your user or organization name.\n\n```\ndocker tag stockdata:1.0.0 <myorg>/stockdata:1.0.0 docker push <myorg>/stockdata:1.0.0\n```\n\nTo push to a private registry, prefix the image name with the registry host name and the organization name.\n\n```\ndocker tag stockdata <internal-registry:5000>/<myorg>/stockdata:1.0.0 docker push <internal-registry:5000>/<myorg>/stockdata:1.0.0\n```\n\nAfter the image is in a registry, you can deploy it to individual Docker hosts, or to a container orchestration engine like Kubernetes.", "Kubernetes": "Although it's possible to run containers manually on Docker hosts, for reliable production systems it's better to use a container orchestration engine to manage multiple instances running across several servers in a cluster. There are various container orchestration engines available, including Kubernetes, Docker Swarm, and Apache Mesos. But of these engines, Kubernetes is far and away the most widely used, so it will be the focus of this chapter.\n\nKubernetes includes the following functionality:\n\n['Scheduling runs containers on multiple nodes within a cluster, ensuring balanced usage of the available resource, keeping containers running if there are outages, and handling rolling updates to new versions of images or new configurations.', 'Health checks monitor containers to ensure continued service.', 'DNS &amp; service discovery handles routing between services within a cluster.', 'Ingress exposes selected services externally and generally provides load-balancing across instances of those services.', 'Resource management attaches external resources like storage to containers.']\n\nThis chapter will detail how to deploy an ASP.NET Core gRPC service and a website that consumes the service into a Kubernetes cluster. The sample application used is available in the dotnetarchitecture/grpc-for-wcf-developers repository on GitHub.", "Kubernetes terminology": "Kubernetes uses desired state configuration: the API is used to describe objects like Pods , Deployments , and Services, and the Control Plane takes care of implementing the desired state across all the nodes in a cluster. A Kubernetes cluster has a Master node that runs the Kubernetes API, which you can communicate with programmatically or by using the kubectl command-line tool. kubectl can create and manage objects through command-line arguments, but it works best with YAML files that contain declaration data for Kubernetes objects.", "Kubernetes YAML files": "Every Kubernetes YAML file will have at least three top-level properties:\n\n| apiVersion:  v1     |\n|---------------------|\n| kind:  Namespace    |\n| metadata:           |\n| # Object properties |\n\nThe apiVersion property is used to specify which version (and which API) the file is intended for. The kind property specifies the kind of object the YAML represents. The metadata property contains object properties like name, namespace, and labels.\n\nMost Kubernetes YAML files will also have a spec section that describes the resources and configuration necessary to create the object.", "Pods": "Pods are the basic units of execution in Kubernetes. They can run multiple containers, but they're also used to run single containers. The pod also includes any storage resources required by the containers, and the network IP address.", "Services": "Services are meta -objects that describe Pods (or sets of Pods) and provide a way to access them within the cluster, such as mapping a service name to a set of pod IP addresses by using the cluster DNS service.", "Deployments": "Deployments are the desired state objects for Pods. If you create a pod manually, it won't be restarted when it terminates. Deployments are used to tell the cluster which Pods, and how many replicas of those Pods, should be running at the present time.", "Other objects": "Pods, Services, and Deployments are just three of the most basic object types. There are dozens of other object types that are managed by Kubernetes clusters. For more information, see the Kubernetes Concepts documentation.", "Namespaces": "Kubernetes clusters are designed to scale to hundreds or thousands of nodes and to run similar numbers of services. To avoid clashes between object names, namespaces are used to group objects together as part of larger applications. Kubernetes's own services run in a default namespace. All user objects should be created in their own namespaces to avoid potential clashes with default objects or other tenants in the cluster.", "Get started with Kubernetes": "If you're running Docker Desktop for Windows or Docker Desktop for Mac, Kubernetes is already available. Just enable it in the Kubernetes section of the Settings window:\n\nSettings\n\n\u2022 Docker running\n\nGeneral\n\nX\n\nX\n\n<!-- image -->\n\nKubernetes\n\nTo run a local Kubernetes cluster on Linux, consider minikube, or MicroK8s if your Linux distribution supports snaps .\n\nTo confirm that your cluster is running and accessible, run the kubectl version command:\n\n```\nkubectl version Client Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020 -10 -14T12:50:19Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"windows/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\", GitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\", BuildDate:\"2020 -10 -14T12:41:49Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n```\n\nIn this example, both the kubectl CLI and the Kubernetes server are running version 1.14.6. Each version of kubectl is supposed to support the previous and next version of the server, so kubectl 1.14 should work with server versions 1.13 and 1.15 as well.", "Run services on Kubernetes": "The sample application has a kube directory that contains three YAML files. The namespace.yml file declares a custom namespace: stocks. The stockdata.yml file declares the Deployment and the Service for the gRPC application, and the stockweb.yml file declares the Deployment and Service for an ASP.NET Core 7.0 MVC web application that consumes the gRPC service.\n\nTo use a YAML file with kubectl, run the apply -f command:\n\n```\nkubectl apply\n```\n\n```\n-f object.yml\n```\n\nThe apply command will check the validity of the YAML file and display any errors received from the API, but doesn't wait until all the objects declared in the file have been created because this step can sir docker"}, "O sughosnea": {"The namespace declaration": "Namespace declaration is simple and requires only assigning a name:\n\n```\napiVersion: v1 kind: Namespace metadata: name: stocks\n```\n\nUse kubectl to apply the namespace.yml file and to confirm the namespace is created successfully:\n\n```\n> kubectl apply -f namespace.yml namespace/stocks created > kubectl get namespaces NAME STATUS AGE stocks Active 2m53s\n```", "The StockData application": "The stockdata.yml file declares two objects: a Deployment and a Service.", "The StockData Deployment": "The Deployment part of the YAML file provides the spec for the deployment itself, including the number of replicas required, and a template for the Pod objects to be created and managed by the deployment. Note that Deployment objects are managed by the apps API, as specified in apiVersion, rather than the main Kubernetes API.\n\n```\napiVersion: apps/v1 kind: Deployment metadata: name: stockdata namespace: stocks spec: selector: matchLabels: run: stockdata replicas: 1 template: metadata: labels: run: stockdata spec: containers: -name: stockdata image: stockdata:1.0.0 imagePullPolicy: Never resources: limits: cpu: 100m memory: 100Mi ports: -containerPort: 80\n```\n\nThe spec.selector property is used to match running Pods to the Deployment. The Pod's metadata.labels property must match the matchLabels property or the API call will fail.\n\nThe template.spec section declares the container to be run. When you're working with a local Kubernetes cluster, such as the one provided by Docker Desktop, you can specify images that were built locally as long as they have a version tag.", "Important": "By default, Kubernetes will always check for and try to pull a new image. If it can't find the image in any of its known repositories, the Pod creation will fail. To work with local images, set the imagePullPolicy to Never.\n\nThe ports property specifies which container ports should be published on the Pod. The stockservice image runs the service on the standard HTTP port, so port 80 is published.\n\nThe resources section applies resource limits to the container running within the Pod. This is a good practice because it prevents an individual Pod from consuming all the available CPU or memory on a node.", "Note": "All of this happens only if a listener is consuming the diagnostic information. If there's no listener, no diagnostics are written and no activities are created.", "The StockData Service": "The Service part of the YAML file declares the service that provides access to the Pods within the cluster.\n\n```\napiVersion: v1 kind: Service metadata: name: stockdata namespace: stocks spec: ports: -port: 80 selector: run: stockdata\n```\n\nThe Service spec uses the selector property to match running Pods, in this case looking for Pods that have a label run: stockdata. The specified port on matching Pods is published by the named service. Other Pods running in the stocks namespace can access HTTP on this service by using http://stockdata as the address. Pods running in other namespaces can use the http://stockdata.stocks host name. You can control cross -namespace service access by using Network Policies .", "Deploy the StockData application": "Use kubectl to apply the stockdata.yml file and confirm that the Deployment and Service were created:\n\n```\n> kubectl apply -f .\\stockdata.yml deployment.apps/stockdata created service/stockdata created > kubectl get deployment stockdata --namespace stocks NAME READY UP -TO -DATE AVAILABLE AGE stockdata 1/1 1 1 17s > kubectl get service stockdata --namespace stocks NAME TYPE CLUSTER -IP EXTERNAL -IP PORT(S) AGE stockdata ClusterIP 10.97.132.103 <none> 80/TCP 33s\n```", "The StockWeb application": "The stockweb.yml file declares the Deployment and Service for the MVC application.\n\n```\napiVersion: apps/v1 kind: Deployment metadata: name: stockweb namespace: stocks spec: selector: matchLabels: run: stockweb replicas: 1 template: metadata: labels: run: stockweb spec: containers: -name: stockweb image: stockweb:1.0.0 imagePullPolicy: Never resources: limits: cpu: 100m memory: 100Mi ports: -containerPort: 80 env: -name: StockData__Address value: \"http://stockdata\" -name: DOTNET_SYSTEM_NET_HTTP_SOCKETSHTTPHANDLER_HTTP2UNENCRYPTEDSUPPORT value: \"true\" ---apiVersion: v1 kind: Service metadata: name: stockweb namespace: stocks spec: type: NodePort ports: -port: 80 selector: run: stockweb\n```", "Environment variables": "The env section of the Deployment object specifies environment variables to be set in the container that's running the stockweb:1.0.0 images.\n\nThe StockData\\_\\_Address environment variable will map to the StockData:Address configuration setting thanks to the EnvironmentVariables configuration provider. This setting uses double underscores between names to separate sections. The address uses the service name of the stockdata Service, which is running in the same Kubernetes namespace.\n\nThe DOTNET\\_SYSTEM\\_NET\\_HTTP\\_SOCKETSHTTPHANDLER\\_HTTP2UNENCRYPTEDSUPPORT environment variable sets an AppContext switch that enables unencrypted HTTP/2 connections for HttpClient. This environment variable does the same thing as setting the switch in code, as shown here:\n\n```\nAppContext . SetSwitch(\"System.Net.Http.SocketsHttpHandler.Http2UnencryptedSupport\" , true);\n```\n\nIf you use an environment variable for the switch, you can easily change the context depending on the context in which the application is running.", "Service types": "The type: NodePort property is used to make the web application accessible from outside the cluster. This property type causes Kubernetes to publish port 80 on the Service to an arbitrary port on the cluster's external network sockets. You can find the assigned port by using the kubectl get service command.\n\nThe stockdata Service shouldn't be accessible from outside the cluster, so it uses the default type, ClusterIP.\n\nProduction systems will most likely use an integrated load balancer to expose public applications to external consumers. Services exposed in this way should use the LoadBalancer type.\n\nFor more information on Service types, see the Kubernetes Publishing Services documentation.", "Deploy the StockWeb application": "Use kubectl to apply the stockweb.yml file and confirm that the Deployment and Service were created:\n\n```\n> kubectl apply -f .\\stockweb.yml deployment.apps/stockweb created service/stockweb created > kubectl get deployment stockweb --namespace stocks NAME READY UP -TO -DATE AVAILABLE AGE stockweb 1/1 1 1 8s > kubectl get service stockweb --namespace stocks NAME TYPE CLUSTER -IP EXTERNAL -IP PORT(S) AGE stockweb NodePort 10.106.141.5 <none> 80:32564/TCP 13s\n```\n\nThe output of the get service command shows that the HTTP port has been published to port 32564 on the external network. For Docker Desktop, this IP address will be localhost. You can access the application by browsing to http://localhost:32564.\n\nHome Page - StockWeb x+\n\n&lt; &gt;\n\n\u2022 localhost:32564\n\nStockWeb\n\nHome Privacy", "Test the application": "The StockWeb application displays a list of NASDAQ stocks that are retrieved from a simple requestreply service. For this demonstration, each line also shows the unique ID of the Service instance that returned it. American Airlines Group Inc 83345e3b-7b5e-4ad6-aa84-c2c0a1d83495\n\n<!-- image -->\n\nIf the number of replicas of the stockdata Service were increased, you might expect the Server value to change from line to line, but in fact all 100 records are always returned from the same instance. If you refresh the page every few seconds, the server ID remains the same. Why does this happen? There are two factors at play here.\n\nFirst, the Kubernetes Service discovery system uses round-robin load balancing by default. The first time the DNS server is queried, it will return the first matching IP address for the Service. The next time, it will return the next IP address in the list, and so on, until the end. At that point, it loops back to the start.\n\nSecond, the HttpClient used for the StockWeb application's gRPC client is created and managed by the ASP.NET Core HttpClientFactory, and a single instance of this client is used for every call to the page. The client only does one DNS lookup, so all requests are routed to the same IP address. And because the HttpClientHandler is cached for performance reasons, multiple requests in quick succession will all use the same IP address, until the cached DNS entry expires or the handler instance is disposed for some reason.\n\nThe result is that by default requests to a gRPC Service aren't balanced across all instances of that Service in the cluster. Different consumers will use different instances, but that doesn't guarantee a good distribution of requests or a balanced use of resources.\n\nThe next chapter, Service meshes, will address this problem.\n\nGuest\n\n[]", "Service meshes": "A service mesh is an infrastructure component that takes control of routing service requests within a network. Service meshes can handle all kinds of network -level concerns within a Kubernetes cluster, including:\n\n['Service discovery', 'Load balancing', 'Fault tolerance', 'Encryption', 'Monitoring']\n\nKubernetes service meshes work by adding an extra container, called a sidecar proxy, to each pod included in the mesh. The proxy takes over handling all inbound and outbound network requests. You can then keep the configuration and management of networking matters separate from the application containers. In many cases, this separation doesn't require any changes to the application code.\n\nIn the previous chapter's example, the gRPC requests from the web application were all routed to a single instance of the gRPC service. This happens because the service's host name is resolved to an IP address, and that IP address is cached for the lifetime of the HttpClientHandler instance. It might be possible to work around this behavior by handling DNS lookups manually or creating multiple clients. But this workaround would complicate the application code without adding any business or customer value.\n\nWhen you use a service mesh, the requests from the application container are sent to the sidecar proxy. The sidecar proxy can then distribute them intelligently across all instances of the other service. The mesh can also:\n\n['Respond seamlessly to failures of individual instances of a service.', 'Handle retry semantics for failed calls or timeouts.', 'Reroute failed requests to an alternate instance without returning to the client application.']\n\nThe following screenshot shows the StockWeb application running with the Linkerd service mesh. There are no changes to the application code, and the Docker image isn't being used. The only change required was the addition of an annotation to the deployment in the YAML files for the stockdata and stockweb services.\n\nE Home Page - StockWeb\n\n&lt; \u2192\n\nStockWeb\n\n[]\n\nlocalhost:31700\n\nHome Privacy\n\n<!-- image -->\n\nYou can see from the Server column that the requests from the StockWeb application have been routed to both replicas of the StockData service, despite originating from a single HttpClient instance in the application code. In fact, if you review the code, you'll see that all 100 requests to the StockData service are made simultaneously by using the same HttpClient instance. With the service mesh, those requests will be balanced across however many service instances are available.\n\nService meshes apply only to traffic within a cluster. For external clients, see the next chapter, Load Balancing .", "Service mesh options": "Three general-purpose service mesh implementations are currently available for use with Kubernetes: Istio , Linkerd, and Consul Connect. All three provide request routing/proxying, traffic encryption, resilience, host-to-host authentication, and traffic control.\n\nChoosing a service mesh depends on multiple factors:\n\n[\"The organization's specific requirements around costs, compliance, paid support plans, and so on.\", 'The nature of the cluster, its size, the number of services deployed, and the volume of traffic within the cluster network.', 'Ease of deploying and managing the mesh and using it with services.']", "Example: Add Linkerd to a deployment": "In this example, you'll learn how to use the Linkerd service mesh with the StockKube application from the previous section. To follow this example, you'll need to install the Linkerd CLI. You can download\n\nGuest 8)\n\no a ..\n\nWindows binaries from the section that lists GitHub releases. Be sure to use the most recent stable release and not one of the edge releases.\n\nWith the Linkerd CLI installed, follow the Getting Started instructions to install the Linkerd components on your Kubernetes cluster. The instructions are straightforward, and the installation should take only a couple of minutes on a local Kubernetes instance.", "Add Linkerd to Kubernetes deployments": "The Linkerd CLI provides an inject command to add the necessary sections and properties to Kubernetes files. You can run the command and write the output to a new file.\n\n```\nlinkerd inject stockdata.yml > stockdata-with-mesh.yml linkerd inject stockweb.yml > stockweb-with-mesh.yml\n```\n\nYou can inspect the new files to see what changes have been made. For deployment objects, a metadata annotation is added to tell Linkerd to inject a sidecar proxy container into the pod when it's created.\n\nIt's also possible to pipe the output of the linkerd inject command to kubectl directly. The following commands will work in PowerShell or any Linux shell.\n\n| linkerd inject stockdata.yml &#124; kubectl apply  -f  -   |\n|------------------------------------------------------------|\n| linkerd inject stockweb.yml &#124; kubectl apply  -f  -    |", "Inspect services in the Linkerd dashboard": "Open the Linkerd dashboard by using the linkerd CLI.\n\nlinkerd dashboard\n\nThe dashboard provides detailed information about all services that are connected to the mesh.\n\nLINKERD &lt;\n\nOverview\n\nTap\n\nTop\n\n=\n\nTop Routes\n\nService Mesh\n\nResources\n\nDocumentation\n\nCommunity\n\nJoin the Mailing List\n\nJoin us on Slack\n\nFile an Issue\n\nRunning Linkerd 2.5.0 (stable).\n\nLinkerd is up to date.\n\nstocks\n\nNamespace: stocks\n\n<!-- image -->\n\nIf you increase the number of replicas of the StockData gRPC service as shown in the following example, and refresh the StockWeb page in the browser, you should see a mix of IDs in the Server column. This mix indicates that all the available instances are serving requests.\n\n```\napiVersion: apps/v1 kind: Deployment metadata: name: stockdata namespace: stocks spec: selector: matchLabels: run: stockdata replicas: 2 # Increase the target number of instances template: metadata: annotations: linkerd.io/inject: enabled creationTimestamp: null labels: run: stockdata spec: containers: -name: stockdata image: stockdata:1.0.0 imagePullPolicy: Never resources: limits: cpu: 100m memory: 100Mi ports: -containerPort: 80\n```\n\nmeshed", "Load balancing gRPC": "A typical deployment of a gRPC application includes a number of identical instances of the service, providing resilience and horizontal scalability. Load balancing distributes incoming requests across these instances to provide full usage of all available resources. To make this load balancing invisible to the client, it's common to use a proxy load balancer server to handle requests from clients and route them to back -end instances.\n\nLoad balancers are classified according to the layer they operate on. Layer 4 load balancers work on the transport level, for example, with TCP sockets, connections, and packets. Layer 7 load balancers work at the application level, specifically handling HTTP/2 requests for gRPC applications.", "L4 load balancers": "An L4 load balancer accepts a TCP connection request from a client, opens another connection to one of the back -end instances, and copies data between the two connections with no real processing. L4 offers excellent performance and low latency, but with little control or intelligence. As long as the client keeps the connection open, all requests will be directed to the same back-end instance.\n\nAzure Load Balancer is an example of an L4 load balancer.", "L7 load balancers": "An L7 load balancer parses incoming HTTP/2 requests and passes them on to back-end instances on a request -by-request basis, no matter how long the connection is held by the client.\n\nExamples of L7 load balancers:\n\n['NGINX', 'HAProxy', 'Traefik']\n\nAs a rule of thumb, L7 load balancers are the best choice for gRPC and other HTTP/2 applications (and for HTTP applications generally, in fact). L4 load balancers will work with gRPC applications, but they're primarily useful when low latency and low overhead are important.\n\n| Important                                                                                                                                                                 |\n|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| At the time of this writing, some L7 load balancers don\u2019t support all the parts of the HTTP/2 specification that are required by gRPC services, such as trailing headers. |\n\nIf you're using TLS encryption, load balancers can terminate the TLS connection and pass unencrypted requests to the back-end application, or they can pass the encrypted request along. Either way, the load balancer will need to be configured with the server's public and private key so it can decrypt requests for processing.\n\nSee to the documentation for your preferred load balancer to find out how to configure it to handle HTTP/2 requests with your back-end services.", "Load balancing within Kubernetes": "See the section on service meshes for a discussion of load balancing across internal services on Kubernetes.", "Application Performance Management": "In production environments like Kubernetes, it's important to monitor applications to ensure they're running optimally. Logging and metrics are important in particular. ASP.NET Core, including gRPC, provides built-in support for producing and managing log messages and metrics data, as well as tracing data.", "The difference between logging and metrics": "Logging is concerned with text messages that record detailed information about things that have happened in the system. Log messages might include exception data, like stack traces, or structured data that provide context about the message. Logging output is commonly written to a searchable text store.\n\nMetrics refers to numeric data designed to be aggregated and presented by using charts and graphs in a dashboard. The dashboard provides a view of the overall health and performance of an application. Metrics data can also be used to trigger automated alerts when a threshold is exceeded. Here are some examples of metrics data:\n\n['Time taken to process requests.', 'The number of requests per second being handled by an instance of a service.', 'The number of failed requests on an instance.']", "Logging in ASP.NET Core gRPC": "ASP.NET Core provides built-in support for logging, in the form of Microsoft.Extensions.Logging NuGet package. The core parts of this library are included with the Web SDK, so there's no need to install it manually. By default, log messages are written to the standard output (the \"console\") and to any attached debugger. To write logs to persistent external data stores, you might need to import optional logging sink packages .\n\nThe ASP.NET Core gRPC framework writes detailed diagnostic logging messages to this logging framework, so they can be processed and stored along with your application's own messages.", "Produce log messages": "The logging extension is automatically registered with ASP.NET Core's dependency injection system, so you can specify loggers as a constructor parameter on gRPC service types.\n\n```\npublic class StockData : Stocks . StocksBase { private readonly ILogger<StockData> _logger; public StockData(ILogger<StockData> logger)\n```\n\n```\n{ _logger = logger; } }\n```\n\nMany log messages, such as requests and exceptions, are provided by the ASP.NET Core and gRPC framework components. Add your own log messages to provide detail and context about application logic, rather than lower-level concerns.\n\nFor more information about writing log messages and available logging sinks and targets, see Logging in .NET Core and ASP.NET Core .", "Metrics in ASP.NET Core gRPC": "The .NET Core runtime provides a set of components for emitting and observing metrics. These include APIs such as the EventSource and EventCounter classes. These APIs can emit basic numeric data that can be consumed by external processes, like the dotnet-counters global tool, or Event Tracing for Windows. For more information about using EventCounter in your own code, see EventCounter introduction .\n\nFor more advanced metrics and for writing metric data to a wider range of data stores, you might try an open -source project called App Metrics. This suite of libraries provides an extensive set of types to instrument your code. It also offers packages to write metrics to different kinds of targets that include time -series databases, such as Prometheus and InfluxDB, and Application Insights. The App.Metrics.AspNetCore.Mvc NuGet package even adds a comprehensive set of basic metrics that are automatically generated via integration with the ASP.NET Core framework. The project website provides templates for displaying those metrics with the Grafana visualization platform.", "Produce metrics": "Most metrics platforms support the following types:\n\n| Metric type   | Description                                                                                                                                                                                                                                                          |\n|---------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Counter       | Tracks how often something happens, such as requests and errors.                                                                                                                                                                                                     |\n| Gauge         | Records a single value that changes over time, such as active connections.                                                                                                                                                                                           |\n| Histogram     | Measures a distribution of values across arbitrary limits. For example, a histogram can track dataset size, counting how many contained <10 records, how many contained 11 - 100 records, how many contained 101-1000 records, and how many contained >1000 records. |\n| Meter         | Measures the rate at which an event occurs in various time spans.                                                                                                                                                                                                    |\n\n| Metric type   | Description                                                                           |\n|---------------|---------------------------------------------------------------------------------------|\n| Timer         | Tracks the duration of events and the rate at which it occurs, stored as a histogram. |\n\nBy using App Metrics, an IMetrics interface can be obtained via dependency injection, and used to record any of these metrics for a gRPC service. The following example shows how to count the number of Get requests made over time:\n\n```\npublic class StockData : Stocks . StocksBase { private static readonly CounterOptions GetRequestCounter = new CounterOptions { Name = \"StockData_Get_Requests\" , MeasurementUnit = Unit . Calls }; private readonly IStockRepository _repository; private readonly IMetrics _metrics; public StockData(IStockRepository repository , IMetrics metrics) { _repository = repository; _metrics = metrics; } public override async Task<GetResponse> Get(GetRequest request , ServerCallContext context) { _metrics . Measure . Counter . Increment(GetRequestCounter); // Serve request... } }\n```", "Store and visualize metrics data": "The best way to store metrics data is in a time-series database, a specialized data store designed to record numerical data series marked with timestamps. The most popular of these databases are Prometheus and InfluxDB. Microsoft Azure also provides dedicated metrics storage through the Azure Monitor service.\n\nThe current go-to solution for visualizing metrics data is Grafana, which works with a wide range of storage providers. The following image shows an example Grafana dashboard that displays metrics from the Linkerd service mesh running the StockData sample:\n\n[]\n\n88 Linkerd Deployment - ral deploy/stockdata\n\nSUCCESS RATE\n\nREQUEST RATE\n\nthit\n\nINBOUND DEPLOYMENTS\n\n\u00a9 Last 5 minutes\n\nOUTBOUND DEPLOYMENTS\n\n<!-- image -->\n\n100.00%\n\n75.00%\n\n50.00%\n\n25.00%\n\no%\n\n17:43\n\n17:44\n\n&gt; Inbound TCP Metrics (panels)\n\n&gt; deploy/stockweb (4 panels)", "Metrics -based alerting": "The numerical nature of metrics data means that it's ideally suited to drive alerting systems, notifying developers or support engineers when a value falls outside of some defined tolerance. The platforms already mentioned all provide support for alerting via a range of options, including emails, text messages, or in-dashboard visualizations.", "Distributed tracing": "Distributed tracing is a relatively recent development in monitoring, which has arisen from the increasing use of microservices and distributed architectures. A single request from a client browser, application, or device can be broken down into many steps and sub-requests, and involve the use of many services across a network. This activity makes it difficult to correlate log messages and metrics with the specific request that triggered them. Distributed tracing applies identifiers to requests, and allows logs and metrics to be correlated with a particular operation. This tracing is similar to WCF's end -to -end tracing, but it's applied across multiple platforms.\n\nDistributed tracing has grown quickly in popularity and is beginning to standardize. The Cloud Native Computing Foundation created the Open Tracing standard, attempting to provide vendor-neutral libraries for working with back ends like Jaeger and Elastic APM. At the same time, Google created the OpenCensus project to address the same set of problems. These two projects are merging into a new project, OpenTelemetry, which aims to be the industry standard of the future.", "How distributed tracing works": "Distributed tracing is based on the concept of spans: named, timed operations that are part of a single trace, which can involve processing on multiple nodes of a system. When a new operation is initiated, a trace is created with a unique identifier. For each sub-operation, a span is created with its own\n\nidentifier and trace identifier. As the request passes around the system, various components can create child spans that include the identifier of their parent. A span has a context, which contains the trace and span identifiers, as well as useful data in the form of key and value pairs (called baggage).", "Distributed tracing with DiagnosticSource": ".NET has an internal module that maps well to distributed traces and spans: DiagnosticSource. As well as providing a simple way to produce and consume diagnostics within a process, the DiagnosticSource module has the concept of an activity. An activity is effectively an implementation of a distributed trace, or a span within a trace. The internals of the module take care of parent/child activities, including allocating identifiers. For more information about using the Activity type, see the Activity User Guide on GitHub .\n\nBecause DiagnosticSource is a part of the core framework and later, it's supported by several core components. These include HttpClient, Entity Framework Core, and ASP.NET Core, including explicit support in the gRPC framework. When ASP.NET Core receives a request, it checks for a pair of HTTP headers matching the W3C Trace Context standard. If the headers are found, an activity is started by using the identity values and context from the headers. If no headers are found, an activity is started with generated identity values that match the standard format. Any diagnostics generated by the framework or by application code during the lifetime of this activity can be tagged with the trace and span identifiers. The HttpClient support extends this functionality further by checking for a current activity on every request, and automatically adding the trace headers to the outgoing request.\n\nThe ASP.NET Core gRPC client and server libraries include explicit support for DiagnosticSource and Activity, and create activities and apply and use header information automatically.", "Add your own DiagnosticSource and Activity": "To add your own diagnostics or create explicit spans within your application code, see the DiagnosticSource User Guide and Activity User Guide .", "Store distributed trace data": "At the time of writing, the OpenTelemetry project is still in the early stages, and only alpha-quality packages are available for .NET applications. The OpenTracing project currently offers more mature libraries.\n\nThe OpenTracing API is described in the following section. If you want to use the OpenTelemetry API in your application instead, refer to the OpenTelemetry .NET SDK repository on GitHub.", "Use the OpenTracing package to store distributed trace data": "The OpenTracing NuGet package supports all OpenTracing-compliant back ends (which can be used independently of DiagnosticSource). There's an additional package from the OpenTracing API\n\nContributions project, OpenTracing.Contrib.NetCore. This package adds a DiagnosticSource listener, and writes events and activities to a back end automatically. Enabling this package is as simple as installing it from NuGet and adding it as a service in your Program class.\n\n```\n// builder . Services . AddOpenTracing(); //\n```\n\nThe OpenTracing package is an abstraction layer, and as such it requires implementation specific to the back end. OpenTracing API implementations are available for the following open source back ends.\n\n| Name        | Package                | Website                 |\n|-------------|------------------------|-------------------------|\n| Jaeger      | Jaeger                 | jaegertracing.io        |\n| Elastic APM | Elastic.Apm.NetCoreAll | elastic.co/products/apm |\n\nFor more information on the OpenTracing API for .NET, see the OpenTracing for C# and the OpenTracing Contrib C#/.NET Core repositories on GitHub.", "Appendix A -Transactions": "Windows Communication Foundation (WCF) supports distributed transactions, allowing you to perform atomic operations across multiple services. This functionality is based on the Microsoft Distributed Transaction Coordinator .\n\nIn the newer microservices landscape, this type of automated distributed transaction processing isn't possible. There are too many different technologies involved, including relational databases, NoSQL data stores, and messaging systems. There might also be a mix of operating systems, programming languages, and frameworks in use in a single environment.\n\nWCF distributed transaction is an implementation of what is known as a two-phase commit (2PC). You can implement 2PC transactions manually by coordinating messages across services, creating open transactions within each service, and sending commit or rollback messages, depending upon success or failure. However, the complexity involved in managing 2PC can increase exponentially as systems evolve. Open transactions hold database locks that can negatively affect performance, or, worse, cause cross-service deadlocks.\n\nIf possible, it's best to avoid distributed transactions altogether. If two items of data are so linked as to require atomic updates, consider handling them both with the same service. Apply those atomic changes by using a single request or message to that service.\n\nIf that isn't possible, then one alternative is to use the Saga pattern. In a saga, updates are processed sequentially; as each update succeeds, the next one is triggered. These triggers can be propagated from service to service, or managed by a saga coordinator or orchestrator. If an update fails at any point during the process, the services that have already completed their updates apply specific logic to reverse them.\n\nAnother option is to use Domain Driven Design (DDD) and Command/Query Responsibility Segregation (CQRS), as described in the .NET Microservices e-book. In particular, using domain events or event sourcing can help to ensure that updates are consistently, if not immediately, applied."}}