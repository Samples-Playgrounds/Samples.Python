{"Uncomment the line below if running with HTTPS": "", "ENV ASPNETCORE_URLS=https://+:443": "WORKDIR /app\n\nCOPY --from=build /published .\n\nENTRYPOINT [ \"dotnet\", \"StockData.dll\" ]\n\nThe Dockerfile has two parts: the first uses the sdk base image to build and publish the application;\nthe second creates a runtime image from the aspnet base. This is because the sdk image is around\n900 MB, compared to around 200 MB for the runtime image, and most of its contents are unnecessary\nat run time.\n\n82\n\nCHAPTER 7 | gRPC in production\n\n\fThe build steps\n\nStep\n\nDescription\n\nFROM ...\n\nDeclares the base image and assigns the builder alias.\n\nWORKDIR /src\n\nCreates the /src directory and sets it as the current working directory.\n\nCOPY . .\n\nCopies everything below the current directory on the host into the\ncurrent directory on the image.\n\nRUN dotnet restore\n\nRestores any external packages (ASP.NET Core 3.0 framework is\npreinstalled with the SDK).\n\nRUN dotnet publish ...\n\nBuilds and publishes a Release build. The --runtime flag isn\u2019t required.\n\nThe runtime image steps\n\nStep\n\nDescription\n\nFROM ...\n\nDeclares a new base image.\n\nWORKDIR /app\n\nCreates the /app directory and sets it as the current working directory.\n\nCOPY --from=builder ...\n\nCopies the published application from the previous image, by using the\nbuilder alias from the first FROM line.\n\nENTRYPOINT [ ... ]\n\nSets the command to run when the container starts. The dotnet\ncommand in the runtime image can only run DLL files.\n\nHTTPS in Docker\n\nMicrosoft base images for Docker set the ASPNETCORE_URLS environment variable to http://+:80,\nmeaning that Kestrel runs without HTTPS on that port. If you\u2019re using HTTPS with a custom certificate\n(as described in Self-hosted gRPC applications), you should override this configuration. Set the\nenvironment variable in the runtime image creation part of your Dockerfile.", "Runtime image creation": "FROM mcr.microsoft.com/dotnet/aspnet:7.0\n\nENV ASPNETCORE_URLS=https://+:443\n\nThe .dockerignore file\n\nMuch like .gitignore files that exclude certain files and directories from source control, the\n.dockerignore file can be used to exclude files and directories from being copied to the image during\nbuild. This file not only saves time copying, but can also avoid some errors that arise from having the\nobj directory from your PC copied into the image. At a minimum, you should add entries for bin and\nobj to your .dockerignore file.\n\nbin/\nobj/\n\n83\n\nCHAPTER 7 | gRPC in production\n\n\fBuild the image\n\nFor a StockKube.sln solution containing two different applications StockData and StockWeb, it\u2019s\nsimplest to put the Dockerfile for each one of them in the base directory. In that case, to build the\nimage, use the following docker build command from the same directory where .sln file resides.\n\ndocker build -t stockdata:1.0.0 -f ./src/StockData/Dockerfile .\n\nThe confusingly named --tag flag (which can be shortened to -t) specifies the whole name of the\nimage, including the actual tag if specified. The . at the end specifies the context in which the build\nwill be run; the current working directory for the COPY commands in the Dockerfile.\n\nIf you have multiple applications within a single solution, you can keep the Dockerfile for each\napplication in its own folder, beside the .csproj file. You should still run the docker build command\nfrom the base directory to ensure that the solution and all the projects are copied into the image. You\ncan specify a Dockerfile below the current directory by using the --file (or -f) flag.\n\ndocker build -t stockdata:1.0.0 -f ./src/StockData/Dockerfile .\n\nRun the image in a container on your machine\n\nTo run the image in your local Docker instance, use the docker run command.\n\ndocker run -ti -p 5000:80 stockdata:1.0.0\n\nThe -ti flag connects your current terminal to the container\u2019s terminal, and runs in interactive mode.\nThe -p 5000:80 publishes (links) port 80 on the container to port 5000 on the localhost network\ninterface.\n\nPush the image to a registry\n\nAfter you\u2019ve verified that the image works, push it to a Docker registry to make it available on other\nsystems. Internal networks will need to provision a Docker registry. This activity can be as simple as\nrunning Docker\u2019s own registry image (the Docker registry runs in a Docker container), but there are\nvarious more comprehensive solutions available. For external sharing and cloud use, there are various\nmanaged registries available, such as Azure Container Registry or Docker Hub.\n\nTo push to Docker Hub, prefix the image name with your user or organization name.\n\ndocker tag stockdata:1.0.0 <myorg>/stockdata:1.0.0\ndocker push <myorg>/stockdata:1.0.0\n\nTo push to a private registry, prefix the image name with the registry host name and the organization\nname.\n\ndocker tag stockdata <internal-registry:5000>/<myorg>/stockdata:1.0.0\ndocker push <internal-registry:5000>/<myorg>/stockdata:1.0.0\n\nAfter the image is in a registry, you can deploy it to individual Docker hosts, or to a container\norchestration engine like Kubernetes.\n\n84\n\nCHAPTER 7 | gRPC in production\n\n\fKubernetes\n\nAlthough it\u2019s possible to run containers manually on Docker hosts, for reliable production systems it\u2019s\nbetter to use a container orchestration engine to manage multiple instances running across several\nservers in a cluster. There are various container orchestration engines available, including Kubernetes,\nDocker Swarm, and Apache Mesos. But of these engines, Kubernetes is far and away the most widely\nused, so it will be the focus of this chapter.\n\nKubernetes includes the following functionality:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nScheduling runs containers on multiple nodes within a cluster, ensuring balanced usage of\nthe available resource, keeping containers running if there are outages, and handling rolling\nupdates to new versions of images or new configurations.\n\nHealth checks monitor containers to ensure continued service.\n\nDNS & service discovery handles routing between services within a cluster.\n\nIngress exposes selected services externally and generally provides load-balancing across\ninstances of those services.\n\nResource management attaches external resources like storage to containers.\n\nThis chapter will detail how to deploy an ASP.NET Core gRPC service and a website that consumes the\nservice into a Kubernetes cluster. The sample application used is available in the dotnet-\narchitecture/grpc-for-wcf-developers repository on GitHub.\n\nKubernetes terminology\n\nKubernetes uses desired state configuration: the API is used to describe objects like Pods, Deployments,\nand Services, and the Control Plane takes care of implementing the desired state across all the nodes\nin a cluster. A Kubernetes cluster has a Master node that runs the Kubernetes API, which you can\ncommunicate with programmatically or by using the kubectl command-line tool. kubectl can create\nand manage objects through command-line arguments, but it works best with YAML files that contain\ndeclaration data for Kubernetes objects.\n\nKubernetes YAML files\n\nEvery Kubernetes YAML file will have at least three top-level properties:\n\napiVersion: v1\nkind: Namespace\nmetadata:", "Object properties": "The apiVersion property is used to specify which version (and which API) the file is intended for. The\nkind property specifies the kind of object the YAML represents. The metadata property contains\nobject properties like name, namespace, and labels.\n\nMost Kubernetes YAML files will also have a spec section that describes the resources and\nconfiguration necessary to create the object.\n\n85\n\nCHAPTER 7 | gRPC in production\n\n\fPods\n\nPods are the basic units of execution in Kubernetes. They can run multiple containers, but they\u2019re also\nused to run single containers. The pod also includes any storage resources required by the containers,\nand the network IP address.\n\nServices\n\nServices are meta-objects that describe Pods (or sets of Pods) and provide a way to access them\nwithin the cluster, such as mapping a service name to a set of pod IP addresses by using the cluster\nDNS service.\n\nDeployments\n\nDeployments are the desired state objects for Pods. If you create a pod manually, it won\u2019t be restarted\nwhen it terminates. Deployments are used to tell the cluster which Pods, and how many replicas of\nthose Pods, should be running at the present time.\n\nOther objects\n\nPods, Services, and Deployments are just three of the most basic object types. There are dozens of\nother object types that are managed by Kubernetes clusters. For more information, see the\nKubernetes Concepts documentation.\n\nNamespaces\n\nKubernetes clusters are designed to scale to hundreds or thousands of nodes and to run similar\nnumbers of services. To avoid clashes between object names, namespaces are used to group objects\ntogether as part of larger applications. Kubernetes\u2019s own services run in a default namespace. All user\nobjects should be created in their own namespaces to avoid potential clashes with default objects or\nother tenants in the cluster.\n\nGet started with Kubernetes\n\nIf you\u2019re running Docker Desktop for Windows or Docker Desktop for Mac, Kubernetes is already\navailable. Just enable it in the Kubernetes section of the Settings window:\n\n86\n\nCHAPTER 7 | gRPC in production\n\n\fTo run a local Kubernetes cluster on Linux, consider minikube, or MicroK8s if your Linux distribution\nsupports snaps.\n\nTo confirm that your cluster is running and accessible, run the kubectl version command:\n\nkubectl version\nClient Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\",\nGitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\",\nBuildDate:\"2020-10-14T12:50:19Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\",\nPlatform:\"windows/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.3\",\nGitCommit:\"1e11e4a2108024935ecfcb2912226cedeafd99df\", GitTreeState:\"clean\",\nBuildDate:\"2020-10-14T12:41:49Z\", GoVersion:\"go1.15.2\", Compiler:\"gc\",\nPlatform:\"linux/amd64\"}\n\nIn this example, both the kubectl CLI and the Kubernetes server are running version 1.14.6. Each\nversion of kubectl is supposed to support the previous and next version of the server, so kubectl 1.14\nshould work with server versions 1.13 and 1.15 as well.\n\nRun services on Kubernetes\n\nThe sample application has a kube directory that contains three YAML files. The namespace.yml file\ndeclares a custom namespace: stocks. The stockdata.yml file declares the Deployment and the Service\nfor the gRPC application, and the stockweb.yml file declares the Deployment and Service for an\nASP.NET Core 7.0 MVC web application that consumes the gRPC service.\n\nTo use a YAML file with kubectl, run the apply -f command:\n\nkubectl apply -f object.yml\n\nThe apply command will check the validity of the YAML file and display any errors received from the\nAPI, but doesn\u2019t wait until all the objects declared in the file have been created because this step can\n\n87\n\nCHAPTER 7 | gRPC in production\n\n\ftake some time. Use the kubectl get command with the relevant object types to check on object\ncreation in the cluster.\n\nThe namespace declaration\n\nNamespace declaration is simple and requires only assigning a name:\n\napiVersion: v1\nkind: Namespace\nmetadata:\nname: stocks\n\nUse kubectl to apply the namespace.yml file and to confirm the namespace is created successfully:\n\n['kubectl apply -f namespace.yml\\nnamespace/stocks created']\n\n['kubectl get namespaces\\nNAME              STATUS   AGE\\nstocks            Active   2m53s']\n\nThe StockData application\n\nThe stockdata.yml file declares two objects: a Deployment and a Service.\n\nThe StockData Deployment\n\nThe Deployment part of the YAML file provides the spec for the deployment itself, including the\nnumber of replicas required, and a template for the Pod objects to be created and managed by the\ndeployment. Note that Deployment objects are managed by the apps API, as specified in apiVersion,\nrather than the main Kubernetes API.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: stockdata\nnamespace: stocks\nspec:\nselector:\nmatchLabels:\nrun: stockdata\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\nrun: stockdata\nspec:\ncontainers:\n- name: stockdata\nimage: stockdata:1.0.0\nimagePullPolicy: Never\nresources:\nlimits:\ncpu: 100m\nmemory: 100Mi\nports:\n- containerPort: 80\n\n88\n\nCHAPTER 7 | gRPC in production\n\n\fThe spec.selector property is used to match running Pods to the Deployment. The Pod\u2019s\nmetadata.labels property must match the matchLabels property or the API call will fail.\n\nThe template.spec section declares the container to be run. When you\u2019re working with a local\nKubernetes cluster, such as the one provided by Docker Desktop, you can specify images that were\nbuilt locally as long as they have a version tag.\n\nImportant\n\nBy default, Kubernetes will always check for and try to pull a new image. If it can\u2019t find the image in\nany of its known repositories, the Pod creation will fail. To work with local images, set the\nimagePullPolicy to Never.\n\nThe ports property specifies which container ports should be published on the Pod. The stockservice\nimage runs the service on the standard HTTP port, so port 80 is published.\n\nThe resources section applies resource limits to the container running within the Pod. This is a good\npractice because it prevents an individual Pod from consuming all the available CPU or memory on a\nnode.\n\nNote\n\nASP.NET Core 7.0 has been optimized and tuned to run in resource-limited containers. The\ndotnet/core/aspnet Docker image sets an environment variable to tell the dotnet runtime that it\u2019s in a\ncontainer.\n\nThe StockData Service\n\nThe Service part of the YAML file declares the service that provides access to the Pods within the\ncluster.\n\napiVersion: v1\nkind: Service\nmetadata:\nname: stockdata\nnamespace: stocks\nspec:\nports:\n\n['port: 80\\nselector:\\nrun: stockdata']\n\nThe Service spec uses the selector property to match running Pods, in this case looking for Pods that\nhave a label run: stockdata. The specified port on matching Pods is published by the named service.\nOther Pods running in the stocks namespace can access HTTP on this service by using\nhttp://stockdata as the address. Pods running in other namespaces can use the http://stockdata.stocks\nhost name. You can control cross-namespace service access by using Network Policies.\n\nDeploy the StockData application\n\nUse kubectl to apply the stockdata.yml file and confirm that the Deployment and Service were\ncreated:\n\n89\n\nCHAPTER 7 | gRPC in production\n\n\f> kubectl apply -f .\\stockdata.yml\ndeployment.apps/stockdata created\nservice/stockdata created\n\n['kubectl get deployment stockdata --namespace stocks\\nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\\nstockdata   1/1     1            1           17s']\n\n['kubectl get service stockdata --namespace stocks\\nNAME        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\\nstockdata   ClusterIP   10.97.132.103   <none>        80/TCP    33s']\n\nThe StockWeb application\n\nThe stockweb.yml file declares the Deployment and Service for the MVC application.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: stockweb\nnamespace: stocks\nspec:\nselector:\nmatchLabels:\nrun: stockweb\nreplicas: 1\ntemplate:\nmetadata:\nlabels:\nrun: stockweb\nspec:\ncontainers:\n- name: stockweb\nimage: stockweb:1.0.0\nimagePullPolicy: Never\nresources:\nlimits:\ncpu: 100m\nmemory: 100Mi\nports:\n- containerPort: 80\nenv:\n- name: StockData__Address\nvalue: \"http://stockdata\"\n- name: DOTNET_SYSTEM_NET_HTTP_SOCKETSHTTPHANDLER_HTTP2UNENCRYPTEDSUPPORT\nvalue: \"true\"\n\n[]\n\napiVersion: v1\nkind: Service\nmetadata:\nname: stockweb\nnamespace: stocks\nspec:\ntype: NodePort\nports:\n\n['port: 80\\nselector:\\nrun: stockweb']\n\n90\n\nCHAPTER 7 | gRPC in production\n\n\fEnvironment variables\n\nThe env section of the Deployment object specifies environment variables to be set in the container\nthat\u2019s running the stockweb:1.0.0 images.\n\nThe StockData__Address environment variable will map to the StockData:Address configuration\nsetting thanks to the EnvironmentVariables configuration provider. This setting uses double\nunderscores between names to separate sections. The address uses the service name of the stockdata\nService, which is running in the same Kubernetes namespace.\n\nThe DOTNET_SYSTEM_NET_HTTP_SOCKETSHTTPHANDLER_HTTP2UNENCRYPTEDSUPPORT\nenvironment variable sets an AppContext switch that enables unencrypted HTTP/2 connections for\nHttpClient. This environment variable does the same thing as setting the switch in code, as shown\nhere:\n\nAppContext.SetSwitch(\"System.Net.Http.SocketsHttpHandler.Http2UnencryptedSupport\", true);\n\nIf you use an environment variable for the switch, you can easily change the context depending on the\ncontext in which the application is running.\n\nService types\n\nThe type: NodePort property is used to make the web application accessible from outside the cluster.\nThis property type causes Kubernetes to publish port 80 on the Service to an arbitrary port on the\ncluster\u2019s external network sockets. You can find the assigned port by using the kubectl get service\ncommand.\n\nThe stockdata Service shouldn\u2019t be accessible from outside the cluster, so it uses the default type,\nClusterIP.\n\nProduction systems will most likely use an integrated load balancer to expose public applications to\nexternal consumers. Services exposed in this way should use the LoadBalancer type.\n\nFor more information on Service types, see the Kubernetes Publishing Services documentation.\n\nDeploy the StockWeb application\n\nUse kubectl to apply the stockweb.yml file and confirm that the Deployment and Service were created:\n\n['kubectl apply -f .\\\\stockweb.yml\\ndeployment.apps/stockweb created\\nservice/stockweb created']\n\n['kubectl get deployment stockweb --namespace stocks\\nNAME       READY   UP-TO-DATE   AVAILABLE   AGE\\nstockweb   1/1     1            1           8s']\n\n['kubectl get service stockweb --namespace stocks\\nNAME       TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE\\nstockweb   NodePort   10.106.141.5   <none>        80:32564/TCP   13s']\n\nThe output of the get service command shows that the HTTP port has been published to port 32564\non the external network. For Docker Desktop, this IP address will be localhost. You can access the\napplication by browsing to http://localhost:32564.\n\n91\n\nCHAPTER 7 | gRPC in production\n\n\fTest the application\n\nThe StockWeb application displays a list of NASDAQ stocks that are retrieved from a simple request-\nreply service. For this demonstration, each line also shows the unique ID of the Service instance that\nreturned it.\n\nIf the number of replicas of the stockdata Service were increased, you might expect the Server value\nto change from line to line, but in fact all 100 records are always returned from the same instance. If\nyou refresh the page every few seconds, the server ID remains the same. Why does this happen?\nThere are two factors at play here.\n\nFirst, the Kubernetes Service discovery system uses round-robin load balancing by default. The first\ntime the DNS server is queried, it will return the first matching IP address for the Service. The next\ntime, it will return the next IP address in the list, and so on, until the end. At that point, it loops back to\nthe start.\n\nSecond, the HttpClient used for the StockWeb application\u2019s gRPC client is created and managed by\nthe ASP.NET Core HttpClientFactory, and a single instance of this client is used for every call to the\npage. The client only does one DNS lookup, so all requests are routed to the same IP address. And\nbecause the HttpClientHandler is cached for performance reasons, multiple requests in quick\nsuccession will all use the same IP address, until the cached DNS entry expires or the handler instance\nis disposed for some reason.\n\nThe result is that by default requests to a gRPC Service aren\u2019t balanced across all instances of that\nService in the cluster. Different consumers will use different instances, but that doesn\u2019t guarantee a\ngood distribution of requests or a balanced use of resources.\n\nThe next chapter, Service meshes, will address this problem.\n\n92\n\nCHAPTER 7 | gRPC in production\n\n\fService meshes\n\nA service mesh is an infrastructure component that takes control of routing service requests within a\nnetwork. Service meshes can handle all kinds of network-level concerns within a Kubernetes cluster,\nincluding:\n\n\u2022\n\n\u2022\n\n\u2022\n\n\u2022\n\nService discovery\n\nLoad balancing\n\nFault tolerance\n\nEncryption\n\n\u2022  Monitoring\n\nKubernetes service meshes work by adding an extra container, called a sidecar proxy, to each pod\nincluded in the mesh. The proxy takes over handling all inbound and outbound network requests. You\ncan then keep the configuration and management of networking matters separate from the\napplication containers. In many cases, this separation doesn\u2019t require any changes to the application\ncode.\n\nIn the previous chapter\u2019s example, the gRPC requests from the web application were all routed to a\nsingle instance of the gRPC service. This happens because the service\u2019s host name is resolved to an IP\naddress, and that IP address is cached for the lifetime of the HttpClientHandler instance. It might be\npossible to work around this behavior by handling DNS lookups manually or creating multiple clients.\nBut this workaround would complicate the application code without adding any business or customer\nvalue.\n\nWhen you use a service mesh, the requests from the application container are sent to the sidecar\nproxy. The sidecar proxy can then distribute them intelligently across all instances of the other service.\nThe mesh can also:\n\n\u2022\n\n\u2022\n\n\u2022\n\nRespond seamlessly to failures of individual instances of a service.\n\nHandle retry semantics for failed calls or timeouts.\n\nReroute failed requests to an alternate instance without returning to the client application.\n\nThe following screenshot shows the StockWeb application running with the Linkerd service mesh.\nThere are no changes to the application code, and the Docker image isn\u2019t being used. The only\nchange required was the addition of an annotation to the deployment in the YAML files for the\nstockdata and stockweb services.\n\n93\n\nCHAPTER 7 | gRPC in production\n\n\fYou can see from the Server column that the requests from the StockWeb application have been\nrouted to both replicas of the StockData service, despite originating from a single HttpClient instance\nin the application code. In fact, if you review the code, you\u2019ll see that all 100 requests to the StockData\nservice are made simultaneously by using the same HttpClient instance. With the service mesh, those\nrequests will be balanced across however many service instances are available.\n\nService meshes apply only to traffic within a cluster. For external clients, see the next chapter, Load\nBalancing.\n\nService mesh options\n\nThree general-purpose service mesh implementations are currently available for use with Kubernetes:\nIstio, Linkerd, and Consul Connect. All three provide request routing/proxying, traffic encryption,\nresilience, host-to-host authentication, and traffic control.\n\nChoosing a service mesh depends on multiple factors:\n\n\u2022\n\n\u2022\n\n\u2022\n\nThe organization\u2019s specific requirements around costs, compliance, paid support plans, and so\non.\n\nThe nature of the cluster, its size, the number of services deployed, and the volume of traffic\nwithin the cluster network.\n\nEase of deploying and managing the mesh and using it with services.\n\nExample: Add Linkerd to a deployment\n\nIn this example, you\u2019ll learn how to use the Linkerd service mesh with the StockKube application from\nthe previous section. To follow this example, you\u2019ll need to install the Linkerd CLI. You can download\n\n94\n\nCHAPTER 7 | gRPC in production\n\n\fWindows binaries from the section that lists GitHub releases. Be sure to use the most recent stable\nrelease and not one of the edge releases.\n\nWith the Linkerd CLI installed, follow the Getting Started instructions to install the Linkerd\ncomponents on your Kubernetes cluster. The instructions are straightforward, and the installation\nshould take only a couple of minutes on a local Kubernetes instance.\n\nAdd Linkerd to Kubernetes deployments\n\nThe Linkerd CLI provides an inject command to add the necessary sections and properties to\nKubernetes files. You can run the command and write the output to a new file.\n\nlinkerd inject stockdata.yml > stockdata-with-mesh.yml\nlinkerd inject stockweb.yml > stockweb-with-mesh.yml\n\nYou can inspect the new files to see what changes have been made. For deployment objects, a\nmetadata annotation is added to tell Linkerd to inject a sidecar proxy container into the pod when it\u2019s\ncreated.\n\nIt\u2019s also possible to pipe the output of the linkerd inject command to kubectl directly. The following\ncommands will work in PowerShell or any Linux shell.\n\nlinkerd inject stockdata.yml | kubectl apply -f -\nlinkerd inject stockweb.yml | kubectl apply -f -\n\nInspect services in the Linkerd dashboard\n\nOpen the Linkerd dashboard by using the linkerd CLI.\n\nlinkerd dashboard\n\nThe dashboard provides detailed information about all services that are connected to the mesh.\n\n95\n\nCHAPTER 7 | gRPC in production\n\n\fIf you increase the number of replicas of the StockData gRPC service as shown in the following\nexample, and refresh the StockWeb page in the browser, you should see a mix of IDs in the Server\ncolumn. This mix indicates that all the available instances are serving requests.\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\nname: stockdata\nnamespace: stocks\nspec:\nselector:\nmatchLabels:\nrun: stockdata\nreplicas: 2 # Increase the target number of instances\ntemplate:\nmetadata:\nannotations:\nlinkerd.io/inject: enabled\ncreationTimestamp: null\nlabels:\nrun: stockdata\nspec:\ncontainers:\n- name: stockdata\nimage: stockdata:1.0.0\nimagePullPolicy: Never\nresources:\nlimits:\ncpu: 100m\nmemory: 100Mi\nports:\n- containerPort: 80\n\n96\n\nCHAPTER 7 | gRPC in production\n\n\fLoad balancing gRPC\n\nA typical deployment of a gRPC application includes a number of identical instances of the service,\nproviding resilience and horizontal scalability. Load balancing distributes incoming requests across\nthese instances to provide full usage of all available resources. To make this load balancing invisible to\nthe client, it\u2019s common to use a proxy load balancer server to handle requests from clients and route\nthem to back-end instances.\n\nLoad balancers are classified according to the layer they operate on. Layer 4 load balancers work on\nthe transport level, for example, with TCP sockets, connections, and packets. Layer 7 load balancers\nwork at the application level, specifically handling HTTP/2 requests for gRPC applications.\n\nL4 load balancers\n\nAn L4 load balancer accepts a TCP connection request from a client, opens another connection to one\nof the back-end instances, and copies data between the two connections with no real processing. L4\noffers excellent performance and low latency, but with little control or intelligence. As long as the\nclient keeps the connection open, all requests will be directed to the same back-end instance.\n\nAzure Load Balancer is an example of an L4 load balancer.\n\nL7 load balancers\n\nAn L7 load balancer parses incoming HTTP/2 requests and passes them on to back-end instances on a\nrequest-by-request basis, no matter how long the connection is held by the client.\n\nExamples of L7 load balancers:\n\n\u2022\n\n\u2022\n\n\u2022\n\nNGINX\n\nHAProxy\n\nTraefik\n\nAs a rule of thumb, L7 load balancers are the best choice for gRPC and other HTTP/2 applications (and\nfor HTTP applications generally, in fact). L4 load balancers will work with gRPC applications, but\nthey\u2019re primarily useful when low latency and low overhead are important.\n\nImportant\n\nAt the time of this writing, some L7 load balancers don\u2019t support all the parts of the HTTP/2\nspecification that are required by gRPC services, such as trailing headers.\n\nIf you\u2019re using TLS encryption, load balancers can terminate the TLS connection and pass unencrypted\nrequests to the back-end application, or they can pass the encrypted request along. Either way, the\nload balancer will need to be configured with the server\u2019s public and private key so it can decrypt\nrequests for processing.\n\nSee to the documentation for your preferred load balancer to find out how to configure it to handle\nHTTP/2 requests with your back-end services.\n\n97\n\nCHAPTER 7 | gRPC in production\n\n\fLoad balancing within Kubernetes\n\nSee the section on service meshes for a discussion of load balancing across internal services on\nKubernetes.\n\nApplication Performance Management\n\nIn production environments like Kubernetes, it\u2019s important to monitor applications to ensure they\u2019re\nrunning optimally. Logging and metrics are important in particular. ASP.NET Core, including gRPC,\nprovides built-in support for producing and managing log messages and metrics data, as well as\ntracing data.\n\nThe difference between logging and metrics\n\nLogging is concerned with text messages that record detailed information about things that have\nhappened in the system. Log messages might include exception data, like stack traces, or structured\ndata that provide context about the message. Logging output is commonly written to a searchable\ntext store.\n\nMetrics refers to numeric data designed to be aggregated and presented by using charts and graphs\nin a dashboard. The dashboard provides a view of the overall health and performance of an\napplication. Metrics data can also be used to trigger automated alerts when a threshold is exceeded.\nHere are some examples of metrics data:\n\n\u2022\n\n\u2022\n\n\u2022\n\nTime taken to process requests.\n\nThe number of requests per second being handled by an instance of a service.\n\nThe number of failed requests on an instance.\n\nLogging in ASP.NET Core gRPC\n\nASP.NET Core provides built-in support for logging, in the form of Microsoft.Extensions.Logging\nNuGet package. The core parts of this library are included with the Web SDK, so there\u2019s no need to\ninstall it manually. By default, log messages are written to the standard output (the \u201cconsole\u201d) and to\nany attached debugger. To write logs to persistent external data stores, you might need to import\noptional logging sink packages.\n\nThe ASP.NET Core gRPC framework writes detailed diagnostic logging messages to this logging\nframework, so they can be processed and stored along with your application\u2019s own messages.\n\nProduce log messages\n\nThe logging extension is automatically registered with ASP.NET Core\u2019s dependency injection system,\nso you can specify loggers as a constructor parameter on gRPC service types.\n\npublic class StockData : Stocks.StocksBase\n{\nprivate readonly ILogger<StockData> _logger;\n\npublic StockData(ILogger<StockData> logger)\n\n\n98\n\nCHAPTER 7 | gRPC in production\n\n\f    {\n_logger = logger;\n}\n}\n\nMany log messages, such as requests and exceptions, are provided by the ASP.NET Core and gRPC\nframework components. Add your own log messages to provide detail and context about application\nlogic, rather than lower-level concerns.\n\nFor more information about writing log messages and available logging sinks and targets, see\nLogging in .NET Core and ASP.NET Core.\n\nMetrics in ASP.NET Core gRPC\n\nThe .NET Core runtime provides a set of components for emitting and observing metrics. These\ninclude APIs such as the EventSource and EventCounter classes. These APIs can emit basic numeric\ndata that can be consumed by external processes, like the dotnet-counters global tool, or Event\nTracing for Windows. For more information about using EventCounter in your own code, see\nEventCounter introduction.\n\nFor more advanced metrics and for writing metric data to a wider range of data stores, you might try\nan open-source project called App Metrics. This suite of libraries provides an extensive set of types to\ninstrument your code. It also offers packages to write metrics to different kinds of targets that include\ntime-series databases, such as Prometheus and InfluxDB, and Application Insights. The\nApp.Metrics.AspNetCore.Mvc NuGet package even adds a comprehensive set of basic metrics that are\nautomatically generated via integration with the ASP.NET Core framework. The project website\nprovides templates for displaying those metrics with the Grafana visualization platform.\n\nProduce metrics\n\nMost metrics platforms support the following types:\n\nMetric type\n\nDescription\n\nCounter\n\nGauge\n\nHistogram\n\nMeter\n\n99\n\nTracks how often something happens, such as\nrequests and errors.\n\nRecords a single value that changes over time,\nsuch as active connections.\n\nMeasures a distribution of values across\narbitrary limits. For example, a histogram can\ntrack dataset size, counting how many\ncontained <10 records, how many contained\n11-100 records, how many contained 101-1000\nrecords, and how many contained >1000\nrecords.\n\nMeasures the rate at which an event occurs in\nvarious time spans.\n\nCHAPTER 7 | gRPC in production\n\n\fMetric type\n\nDescription\n\nTimer\n\nTracks the duration of events and the rate at\nwhich it occurs, stored as a histogram.\n\nBy using App Metrics, an IMetrics interface can be obtained via dependency injection, and used to\nrecord any of these metrics for a gRPC service. The following example shows how to count the\nnumber of Get requests made over time:\n\npublic class StockData : Stocks.StocksBase\n{\nprivate static readonly CounterOptions GetRequestCounter = new CounterOptions\n{\nName = \"StockData_Get_Requests\",\nMeasurementUnit = Unit.Calls\n};\n\nprivate readonly IStockRepository _repository;\nprivate readonly IMetrics _metrics;\n\npublic StockData(IStockRepository repository, IMetrics metrics)\n{\n    _repository = repository;\n    _metrics = metrics;\n}\n\npublic override async Task<GetResponse> Get(GetRequest request, ServerCallContext\n\ncontext)\n{\n_metrics.Measure.Counter.Increment(GetRequestCounter);\n\n    // Serve request...\n}\n\n}\n\nStore and visualize metrics data\n\nThe best way to store metrics data is in a time-series database, a specialized data store designed to\nrecord numerical data series marked with timestamps. The most popular of these databases are\nPrometheus and InfluxDB. Microsoft Azure also provides dedicated metrics storage through the Azure\nMonitor service.\n\nThe current go-to solution for visualizing metrics data is Grafana, which works with a wide range of\nstorage providers. The following image shows an example Grafana dashboard that displays metrics\nfrom the Linkerd service mesh running the StockData sample:\n\n100\n\nCHAPTER 7 | gRPC in production\n\n\fMetrics-based alerting\n\nThe numerical nature of metrics data means that it\u2019s ideally suited to drive alerting systems, notifying\ndevelopers or support engineers when a value falls outside of some defined tolerance. The platforms\nalready mentioned all provide support for alerting via a range of options, including emails, text\nmessages, or in-dashboard visualizations.\n\nDistributed tracing\n\nDistributed tracing is a relatively recent development in monitoring, which has arisen from the\nincreasing use of microservices and distributed architectures. A single request from a client browser,\napplication, or device can be broken down into many steps and sub-requests, and involve the use of\nmany services across a network. This activity makes it difficult to correlate log messages and metrics\nwith the specific request that triggered them. Distributed tracing applies identifiers to requests, and\nallows logs and metrics to be correlated with a particular operation. This tracing is similar to WCF\u2019s\nend-to-end tracing, but it\u2019s applied across multiple platforms.\n\nDistributed tracing has grown quickly in popularity and is beginning to standardize. The Cloud Native\nComputing Foundation created the Open Tracing standard, attempting to provide vendor-neutral\nlibraries for working with back ends like Jaeger and Elastic APM. At the same time, Google created the\nOpenCensus project to address the same set of problems. These two projects are merging into a new\nproject, OpenTelemetry, which aims to be the industry standard of the future.\n\nHow distributed tracing works\n\nDistributed tracing is based on the concept of spans: named, timed operations that are part of a single\ntrace, which can involve processing on multiple nodes of a system. When a new operation is initiated,\na trace is created with a unique identifier. For each sub-operation, a span is created with its own\n\n101\n\nCHAPTER 7 | gRPC in production\n\n\fidentifier and trace identifier. As the request passes around the system, various components can\ncreate child spans that include the identifier of their parent. A span has a context, which contains the\ntrace and span identifiers, as well as useful data in the form of key and value pairs (called baggage).\n\nDistributed tracing with DiagnosticSource\n\n.NET has an internal module that maps well to distributed traces and spans: DiagnosticSource. As well\nas providing a simple way to produce and consume diagnostics within a process, the\nDiagnosticSource module has the concept of an activity. An activity is effectively an implementation of\na distributed trace, or a span within a trace. The internals of the module take care of parent/child\nactivities, including allocating identifiers. For more information about using the Activity type, see the\nActivity User Guide on GitHub.\n\nBecause DiagnosticSource is a part of the core framework and later, it\u2019s supported by several core\ncomponents. These include HttpClient, Entity Framework Core, and ASP.NET Core, including explicit\nsupport in the gRPC framework. When ASP.NET Core receives a request, it checks for a pair of HTTP\nheaders matching the W3C Trace Context standard. If the headers are found, an activity is started by\nusing the identity values and context from the headers. If no headers are found, an activity is started\nwith generated identity values that match the standard format. Any diagnostics generated by the\nframework or by application code during the lifetime of this activity can be tagged with the trace and\nspan identifiers. The HttpClient support extends this functionality further by checking for a current\nactivity on every request, and automatically adding the trace headers to the outgoing request.\n\nThe ASP.NET Core gRPC client and server libraries include explicit support for DiagnosticSource and\nActivity, and create activities and apply and use header information automatically.\n\nNote\n\nAll of this happens only if a listener is consuming the diagnostic information. If there\u2019s no listener, no\ndiagnostics are written and no activities are created.\n\nAdd your own DiagnosticSource and Activity\n\nTo add your own diagnostics or create explicit spans within your application code, see the\nDiagnosticSource User Guide and Activity User Guide.\n\nStore distributed trace data\n\nAt the time of writing, the OpenTelemetry project is still in the early stages, and only alpha-quality\npackages are available for .NET applications. The OpenTracing project currently offers more mature\nlibraries.\n\nThe OpenTracing API is described in the following section. If you want to use the OpenTelemetry API\nin your application instead, refer to the OpenTelemetry .NET SDK repository on GitHub.\n\nUse the OpenTracing package to store distributed trace data\n\nThe OpenTracing NuGet package supports all OpenTracing-compliant back ends (which can be used\nindependently of DiagnosticSource). There\u2019s an additional package from the OpenTracing API\n\n102\n\nCHAPTER 7 | gRPC in production\n\n\fContributions project, OpenTracing.Contrib.NetCore. This package adds a DiagnosticSource listener,\nand writes events and activities to a back end automatically. Enabling this package is as simple as\ninstalling it from NuGet and adding it as a service in your Program class.\n\n//\n\nbuilder.Services.AddOpenTracing();\n\n//\n\nThe OpenTracing package is an abstraction layer, and as such it requires implementation specific to\nthe back end. OpenTracing API implementations are available for the following open source back\nends.\n\nName\n\nPackage\n\nWebsite\n\nJaeger\n\nJaeger\n\njaegertracing.io\n\nElastic APM\n\nElastic.Apm.NetCoreAll\n\nelastic.co/products/apm\n\nFor more information on the OpenTracing API for .NET, see the OpenTracing for C# and the\nOpenTracing Contrib C#/.NET Core repositories on GitHub.\n\n103\n\nCHAPTER 7 | gRPC in production\n\n\fCHAPTER  8\n\nAppendix A - Transactions\n\nWindows Communication Foundation (WCF) supports distributed transactions, allowing you to\nperform atomic operations across multiple services. This functionality is based on the Microsoft\nDistributed Transaction Coordinator.\n\nIn the newer microservices landscape, this type of automated distributed transaction processing isn\u2019t\npossible. There are too many different technologies involved, including relational databases, NoSQL\ndata stores, and messaging systems. There might also be a mix of operating systems, programming\nlanguages, and frameworks in use in a single environment.\n\nWCF distributed transaction is an implementation of what is known as a two-phase commit (2PC). You\ncan implement 2PC transactions manually by coordinating messages across services, creating open\ntransactions within each service, and sending commit or rollback messages, depending upon success\nor failure. However, the complexity involved in managing 2PC can increase exponentially as systems\nevolve. Open transactions hold database locks that can negatively affect performance, or, worse, cause\ncross-service deadlocks.\n\nIf possible, it\u2019s best to avoid distributed transactions altogether. If two items of data are so linked as to\nrequire atomic updates, consider handling them both with the same service. Apply those atomic\nchanges by using a single request or message to that service.\n\nIf that isn\u2019t possible, then one alternative is to use the Saga pattern. In a saga, updates are processed\nsequentially; as each update succeeds, the next one is triggered. These triggers can be propagated\nfrom service to service, or managed by a saga coordinator or orchestrator. If an update fails at any\npoint during the process, the services that have already completed their updates apply specific logic\nto reverse them.\n\nAnother option is to use Domain Driven Design (DDD) and Command/Query Responsibility\nSegregation (CQRS), as described in the .NET Microservices e-book. In particular, using domain events\nor event sourcing can help to ensure that updates are consistently, if not immediately, applied.\n\n104\n\nCHAPTER 8 | Appendix A - Transactions"}